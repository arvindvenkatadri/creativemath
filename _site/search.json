[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Hi, I’m Arvind Venkatadri.",
    "section": "",
    "text": "Hi, I’m Arvind Venkatadri.\nI’m external Faculty at the Srishti Manipal Institute of Art and Design, both in Bangalore, INDIA, and a Certified Level #1 TRIZ Professional.\nI have a passion for coding in R / ObservableJS / p5.js, Data Visualization, Complexity Science, Literature, and Creative Thinking / Problem Solving with TRIZ. On this website, I share and teach what I learn.\nTo get started, you can check out my courses. You can also find me on Twitter, GitHub, and on Medium. Feel free to reach out to me via email !\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Arvind V.",
    "section": "",
    "text": "I’m an an external Faculty Member at the Srishti Manipal Institute of Art, Design, and Technology (SMI), in Bangalore, INDIA. I am passionate about working on R, Data Visualization, Complexity Science, and Creative Thinking and Problem Solving with TRIZ. On this website, I share my course materials and methods. I also blog about TRIZ and Data Science on occasion.\nTo get started, you can check out my courses on this website.\nMy teaching websites are:\n\n\nMaths for Creative Coders(This one!!)\nModel Thinking in Design\n\nThe Mad Hatter’s Guide to Data Viz and Stats in R \nTRIZ for Problem Solvers\n\nLiterary Jukebox: Design Thinking from Short Fiction\n\n\nMy student portfolios are here:\nhttps://we-r-us.netlify.app/portfolio/\nhttps://form-and-structure.netlify.app/portfolio/\n\nYou can find me on Twitter, or GitHub, and on LinkedIn! Feel free to reach out to me via mail too!\n\n\n Back to top"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/500-Tech/20-AddingLibrariesToP5Js/index.html",
    "href": "content/courses/MathModelsDesign/Modules/500-Tech/20-AddingLibrariesToP5Js/index.html",
    "title": "Adding Libraries to p5.js",
    "section": "",
    "text": "When we wish to use some feature that is not directly available in p5.js, say for example creating Voronoi Tesselations using the c2.js library, then we need to import that JavaScript library into our p5.js session that allows us to do that.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Tech",
      "Adding Libraries to p5.js"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/500-Tech/20-AddingLibrariesToP5Js/index.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/500-Tech/20-AddingLibrariesToP5Js/index.html#introduction",
    "title": "Adding Libraries to p5.js",
    "section": "",
    "text": "When we wish to use some feature that is not directly available in p5.js, say for example creating Voronoi Tesselations using the c2.js library, then we need to import that JavaScript library into our p5.js session that allows us to do that.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Tech",
      "Adding Libraries to p5.js"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/500-Tech/20-AddingLibrariesToP5Js/index.html#simple-instructions",
    "href": "content/courses/MathModelsDesign/Modules/500-Tech/20-AddingLibrariesToP5Js/index.html#simple-instructions",
    "title": "Adding Libraries to p5.js",
    "section": "Simple Instructions!",
    "text": "Simple Instructions!\n\nOn your p5.js web editor, click the arrow on the left side and access the index.html file.\n\n\n\n\n\n\n\nFigure 1: Accessing index.html on p5js web editor\n\n\n\n\nGo to either jsDeliver or cdnjs and get the &lt;script&gt; for your chosen library:\n\n\n\n\n\n\n\nFigure 2: Script for tonejs @ cdnjs website\n\n\n\n\nPaste that &lt;script&gt; tag and its contents into your index.html.\nThe library will now be available in your sketch.js and you can use the commands and features from that library therein.\n\nHere is a shortish video that tells us how to do this:",
    "crumbs": [
      "Math Models for Creative Coders",
      "Tech",
      "Adding Libraries to p5.js"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/500-Tech/20-AddingLibrariesToP5Js/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/500-Tech/20-AddingLibrariesToP5Js/index.html#references",
    "title": "Adding Libraries to p5.js",
    "section": "References",
    "text": "References\n\np5.js Documentation: How to use a JS library in your p5.js sketch. https://archive.p5js.org/libraries/\np5.js Community Libraries. https://p5js.org/libraries/\njsDeliver https://www.jsdelivr.com/?query=p5play\ncdnJS.https://cdnjs.com",
    "crumbs": [
      "Math Models for Creative Coders",
      "Tech",
      "Adding Libraries to p5.js"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/500-Tech/50-OSC/index.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/500-Tech/50-OSC/index.html#introduction",
    "title": "The Open Sound Protocol",
    "section": "Introduction",
    "text": "Introduction",
    "crumbs": [
      "Math Models for Creative Coders",
      "Tech",
      "The Open Sound Protocol"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/500-Tech/50-OSC/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/500-Tech/50-OSC/index.html#references",
    "title": "The Open Sound Protocol",
    "section": "References",
    "text": "References\n\nTramontana: A platform for interactive spaces, interactive objects and prototyping. https://tramontana.xyz/\nThe Coding Train set of video tutorials https://www.youtube.com/@TheCodingTrain\nDan Shiffman. The Nature of Code book. https://natureofcode.com\nCodeAcademy. p5.js short cheatsheet. https://www.codecademy.com/learn/learn-p5js/modules/p5js-introduction-to-creative-coding/cheatsheet",
    "crumbs": [
      "Math Models for Creative Coders",
      "Tech",
      "The Open Sound Protocol"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/30-Fractals/index.html",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/30-Fractals/index.html",
    "title": "Fractals",
    "section": "",
    "text": "What is in us that must reach the top, that longs to look down upon the world as if a god? Don’t we know that in this infinite space the same rocks at the seashore know the secret of each peak?\nUnderneath the surface are caverns, caves soaring cathedrals the earth has made. What arias does she sing to dripping water, bats and other seekers of wisdom? What prayers echo while the ceilings reach slowly to the floor?\nThe open window houses everything: a cat lounging in the sunlight, the call of neighbors, the breath of possibility.\n- Robin Walthery Allen",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "Fractals"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/30-Fractals/index.html#inspiration-a-geometric-jewellery-store",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/30-Fractals/index.html#inspiration-a-geometric-jewellery-store",
    "title": "Fractals",
    "section": "\n Inspiration: A Geometric Jewellery Store",
    "text": "Inspiration: A Geometric Jewellery Store\n\n\nPhoto by Mykola/Kolya Korzh on Unsplash\n\nExplore some jewels here: https://math.hws.edu/eck/js/mandelbrot/MB.html",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "Fractals"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/30-Fractals/index.html#an-introduction-to-fractals",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/30-Fractals/index.html#an-introduction-to-fractals",
    "title": "Fractals",
    "section": "An Introduction to Fractals",
    "text": "An Introduction to Fractals\nLet us listen to the late great Benoit Mandelbrot, on the Art of Roughness.\n\nJulia, and Julia, and still more Julia….\nLet us head off to https://mathigon.org/course/fractals/mandelbrot and play with some iterated functions in the Complex Plane. This will lead us into an intuitive understanding of Julia and Mandelbrot Fractals.\nNow we are ready(?) to understand this video on Julia fractals!",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "Fractals"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/30-Fractals/index.html#designing-with-juliamandelbrot-fractals",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/30-Fractals/index.html#designing-with-juliamandelbrot-fractals",
    "title": "Fractals",
    "section": "Designing with Julia/Mandelbrot fractals",
    "text": "Designing with Julia/Mandelbrot fractals\nLet us play with this interactive Mandelbrot-Julia combination diagram!\n\n\n\nWe now that the Mandelbrot fractal uses the Iterated Function \\(z \\leftarrow ~ z^2 + C\\), where \\(C\\) is a starting complex constant.\nThe Mandelbrot (at left) is plotted in a coordinate space for \\(C\\).\nThe Julia(s) (at right) are plotted in a coordinate space for \\(z\\).\nDepending upon the starting value of \\(C\\) in the Mandelbrot we get a different choice of Julia fractal in the right side diagram.\nIn this “single-colour” Mandelbrot, we see that for those values of \\(C\\) where the corresponding Julia is connected, the \\(C\\) point is coloured black in the Mandelbrot. Else it is coloured white and appears to be “outside”.\nPlace your cursor inside the Julia diagram. A connected Julia always shows an iterative trace that lies within itself. A dis-connected Julia has points going off to infinity…\nVarying \\(C\\) we see a systematic variation of the attainable Julia fractals.\n\nJewellery Shopping with Mandelbrot\nEver gone shopping for jewellery? How is the place organized? That is a good metaphor for how Mandelbrot Set is generated!! Here is an image to complete the Jewellery Store metaphor:\n\n\n\n\n\nFigure 1: Mandelbrot and Julia Mapping",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "Fractals"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/30-Fractals/index.html#fractals-with-code",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/30-Fractals/index.html#fractals-with-code",
    "title": "Fractals",
    "section": "Fractals with Code",
    "text": "Fractals with Code\n\n\nUsing p5.js\nUsing XaOS\n\n\n\n\n\nAnd here is the Julia set:\n\n\nSee also this sketch for a static Julia, set by a user choice of \\(C\\). https://editor.p5js.org/AhmadMoussa/full/nDrd9EfHr\n\n\nLet us use the XaOS software to make different kinds of fractals. A sample screen to explore the Mandelbrot fractal is here:",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "Fractals"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/30-Fractals/index.html#wait-but-why",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/30-Fractals/index.html#wait-but-why",
    "title": "Fractals",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nFractal shapes are all around us in nature\nThese mathematically created fractal shapes can have uses that are limited only by your imagination: want to design footwear that does not slip?\nDifferent parts of the Mandelbrot fractal allow us to contemplate a variety of fractal shapes\nAnd if we choose a different IFS function (rather than \\(z \\rightarrow z^2 + C\\)), we can think of entire families of shapes!",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "Fractals"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/30-Fractals/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/30-Fractals/index.html#references",
    "title": "Fractals",
    "section": "\n References",
    "text": "References\n\nAn Intrepid Tour of the Complex Fractal World using Dark Heart Package 2.2.0 for Mac. https://dhushara.com/DarkHeart/#Anchor-Introduction-35326\n\nGary William Flake.(28 August 1988) The Computational Beauty of Nature. ISBN: 978-0262062008. MIT Press.\nBatty, M. and Longley, P. A. (1994) Fractal Cities: A Geometry of Form and Function, London: Academic Press, 1994.\nWang H, Luo S, Luo T. Fractal characteristics of urban surface transit and road networks: Case study of Strasbourg, France. Advances in Mechanical Engineering. 2017;9(2). doi:10.1177/1687814017692289\n\nLarry Riddle.Classic Iterated Function Systems. https://larryriddle.agnesscott.org/ifs/ifs.htm\n\nTurtle Graphics in R: https://cran.r-project.org/web/packages/TurtleGraphics/vignettes/TurtleGraphics.pdf\n\nhttps://www.reddit.com/r/CitiesSkylines/comments/9r0y4e/grid_idea_im_working_on_fractal_squares/\nMenger Sponge: https://blogs.scientificamerican.com/roots-of-unity/a-few-of-my-favorite-spaces-the-menger-sponge/\n\nhttp://fractalfoundation.org/resources/fractal-software/",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "Fractals"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/20-ComplexNumbers/index.html",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/20-ComplexNumbers/index.html",
    "title": "\n Complex Numbers",
    "section": "",
    "text": "One of the most basic kinds of numbers we will need are, of course, Complex Numbers. But what are they?",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"tabler:math-xy\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Complex Numbers"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/20-ComplexNumbers/index.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/20-ComplexNumbers/index.html#introduction",
    "title": "\n Complex Numbers",
    "section": "",
    "text": "One of the most basic kinds of numbers we will need are, of course, Complex Numbers. But what are they?",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"tabler:math-xy\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Complex Numbers"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/20-ComplexNumbers/index.html#complex-planes-what-am-i",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/20-ComplexNumbers/index.html#complex-planes-what-am-i",
    "title": "\n Complex Numbers",
    "section": "Complex Planes: What am \\(i\\)?",
    "text": "Complex Planes: What am \\(i\\)?\nDude, what’s the square root of -1?? 🙀 🙀 🙀!! And, what can you do repeatedly to arrive at \\([-1, 0]\\)?",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"tabler:math-xy\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Complex Numbers"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/20-ComplexNumbers/index.html#wait-but-why",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/20-ComplexNumbers/index.html#wait-but-why",
    "title": "\n Complex Numbers",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nComplex Numbers are very useful in handling “2D data” in a compact fashion\nComplex Numbers help us to intuitively visualize, and implement, ideas such as rotation, scaling, and shadows, and projections.\nThe duality between rotating vectors and complex numbers is a very important concept.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"tabler:math-xy\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Complex Numbers"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/20-ComplexNumbers/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/20-ComplexNumbers/index.html#references",
    "title": "\n Complex Numbers",
    "section": "\n References",
    "text": "References\n\nWorking with Shadows. https://www.wikiwand.com/en/Map_projection\n\nWorking with Fourier Series and Epicyles http://www.jezzamon.com/fourier/index.html and https://alex.miller.im/posts/fourier-series-spinning-circles-visualization/\n\nhttps://twitter.com/i/status/962449509782495232 https://codegolf.stackexchange.com/questions/36374/redraw-an-image-with-just-one-closed-curve",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"tabler:math-xy\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Complex Numbers"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/listing.html",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/listing.html",
    "title": "Geometry",
    "section": "",
    "text": "Title\n\n\n\nReading Time\n\n\n\n\n\n\n\n\n  Circles\n\n\n25 min\n\n\n\n\n\n\n Complex Numbers\n\n\n6 min\n\n\n\n\n\n\nFractals\n\n\n10 min\n\n\n\n\n\n\n  Affine Transformation Fractals\n\n\n16 min\n\n\n\n\n\n\nL-Systems\n\n\n17 min\n\n\n\n\n\n\nKolams and Lusona\n\n\n4 min\n\n\n\n\n\n\nNo matching items\n Back to top",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/50-LSystems/index.html",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/50-LSystems/index.html",
    "title": "L-Systems",
    "section": "",
    "text": "Trees\n“I think that I shall never see\nA poem lovely as a tree.\nA tree whose hungry mouth is prest\nAgainst the earth’s sweet flowing breast;\nA tree that looks at God all day,\nAnd lifts her leafy arms to pray;\nA tree that may in Summer wear\nA nest of robins in her hair;\nUpon whose bosom snow has lain;\nWho intimately lives with rain.\nPoems are made by fools like me,\nBut only God can make a tree.”\n— Joyce Kilmer, 1915",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "L-Systems"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/50-LSystems/index.html#inspiration",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/50-LSystems/index.html#inspiration",
    "title": "L-Systems",
    "section": "\n Inspiration",
    "text": "Inspiration\n\n\nJapanese Daisugi",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "L-Systems"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/50-LSystems/index.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/50-LSystems/index.html#introduction",
    "title": "L-Systems",
    "section": "Introduction",
    "text": "Introduction\nTrees are fractal in nature, meaning that patterns created by the large structures, such as the main branches, repeat themselves in smaller structures, such as smaller branches…. a universal growth pattern first observed by Leonardo da Vinci 500 years ago: a simple yet startling relationship that always holds between the size of a tree’s trunk and sizes of its branches.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "L-Systems"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/50-LSystems/index.html#an-introduction-to-l-systems",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/50-LSystems/index.html#an-introduction-to-l-systems",
    "title": "L-Systems",
    "section": "An Introduction to L-Systems",
    "text": "An Introduction to L-Systems\nFrom Job Talle’s Website:\nLindenmayer systems were originally conceived by Hungarian biologist Aristid Lindenmayer while studying algae growth. He developed L-systems as a way to describe the growth process of algae and simple plants. The result was a type of language in which the recursive and self similar properties of organism growth can be expressed. Indeed, L-systems can be used to generate natural patterns, but well known mathematical patterns can also be written as an L-system. In this article, I will explain different flavors of L-systems, and I will demonstrate them by rendering 2D Lindenmayer systems and 3D Lindenmayer systems using turtle graphics.\nThe language is very simple. It consists of symbols (the alphabet) and production rules. The first state of the sentence is called the axiom. The production rules can be applied repeatedly on this axiom to evolve or grow the system. A simple example would be a system with the axiom \\(AA\\), and the rule \\(A→ABA\\).\nYou can see how a self expanding sentence can be analogous to cell division in plants and other biological processes.\n\\[\naxiom: AA\n\\] \\[\nProduction ~ Rule: A --&gt; ABA\n\\]\n\\[\nIterations:\\\\\\\n1. A\\\\\\\n\\]\n\\[\n2. ~\\color{magenta}{A}~\\color{Black}B~\\color{magenta}{A}\\\\\\\n\\] \\[\n3.~\\color{magenta}{ABA}~\\color{Black}B~\\color{magenta}{ABA}\\\\\\\n\\] \\[\n4.~\\color{magenta}{ABA}~\\color{Black}B~\\color{magenta}{ABA}~~\\color{Black}B~~\\color{magenta}{ABA}~\\color{Black}B~\\color{magenta}{ABA}\n\\]\nL-system Structures thus develop through a process of string rewriting. A string of letters is transformed into a new string of letters using simple rules called productions. The process is repeated indefinitely, each time using the string that was just produced as the source for the next string. Because the strings tend to grow with each rewrite, an L-system can become arbitrarily complex, but always guided by a simple process dictated by a fixed set of simple rules. In this respect, L-systems are a manifestation of Complexity Phenomena.\nAll right, but how does this become a tree??\nTwo things need to be done:\n\nEach symbol in the language needs to be mapped to a left branch or a right branch. (with turn angle)\nAt each application of the production rules, branch size is scaled down by a number.\n\n\n\n\n\n\nFigure 1\n\n\nImage Courtesy Christophe Eloy | University of Provence.\nIn the Figure 1, the RHS shows a typical figure generated by the L-system language. This figure shows both the LHS and RHS turns, and the fairly rapid reduction in the size of the branches.\nLet us see how we can create “Algorithmic Trees”.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "L-Systems"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/50-LSystems/index.html#creating-trees-using-l-systems",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/50-LSystems/index.html#creating-trees-using-l-systems",
    "title": "L-Systems",
    "section": "Creating Trees using L-systems",
    "text": "Creating Trees using L-systems\n\n\nUsing p5.js\nUsing R\nFractal Grower\n\n\n\n\n\n\n\n\n\nWe will use the package LindenmayerR to create our tree.\n\nShow the Codelibrary(tidyverse)\n###\nlibrary(LindenmayeR)\nlibrary(LearnGeom)\nlibrary(TurtleGraphics)\noptions(max.print = 20)\n\n\n\n# Lindemayer tree\n\n## Dictionary of Symbols\ndictionary &lt;- data.frame(\n  symbol = c(\"F\", \"f\", \"+\", \"-\", \"[\", \"]\"), # symbol column\n  action = c(\"F\", \"f\", \"+\", \"-\", \"[\", \"]\"), # action column\n  stringsAsFactors = FALSE\n)\n\n## Axioms to start and morph\ntree_morph_rules &lt;- data.frame(\n  inp = c(\"F\"), # starting axiom\n  out = c(\"F[+F][-F]\"), # Morphing Rule\n  stringsAsFactors = FALSE\n)\n\n## Build the Tree with commands\nLtree &lt;- Lsys(\n  init = \"F\",\n  rules = tree_morph_rules,\n  n = 2,\n  verbose = 0, # No progress messages please...\n  retAll = FALSE # One Vector output at the end only\n)\n\n[[1]]\n     start end\n[1,]     1   1\n\n[[1]]\n  start end    insert\n1     1   1 F[+F][-F]\n\n[[1]]\n     start end\n[1,]     1   1\n[2,]     4   4\n[3,]     8   8\n\n[[1]]\n  start end    insert\n1     1   1 F[+F][-F]\n2     4   4 F[+F][-F]\n3     8   8 F[+F][-F]\n\n## Now draw the tree\ndrawLsys(\n  string = Ltree,\n  drules = dictionary,\n  stepSize = 10, shrinkFactor = 1.2, # integers shrink!\n  ang = 12,\n  st = c(50, 10, 90) # Root Position x, y, angle from bottom-left\n)\n\n\n\n\n\n\n\nA more complex, and more botanical-looking, tree or seaweed:\n\n## Define Axiom and Mutation Rules\nfractal_tree_rules &lt;- data.frame(\n  inp = c(\"X\", \"F\"),\n  out = c(\"F-[[X]+X]+F[+FX]-X\", \"FF\"),\n  stringsAsFactors = FALSE\n)\n\n## Create the Algorithmic Tree\nfractal_tree &lt;- Lsys(\n  init = \"X\", # Axiom\n  rules = fractal_tree_rules,\n  n = 4,\n  verbose = 0,\n  retAll = FALSE\n)\n\n## Now draw the tree\ndrawLsys(\n  string = fractal_tree,\n  drules = dictionary,\n  stepSize = 2, # Shrink by half each time\n  ang = runif(n = 1, min = 3, max = 30),\n  st = c(50, 5, 90), # Origin of tree\n  gp = gpar(col = \"chocolate4\", fill = \"honeydew\")\n)\n\ngrid.text(\"Fractal Seaweed (n = 4)\", 0.25, 0.25)\n\n\n\n\n\n\n\n[[1]]\n     start end\n[1,]     1   1\n\n[[1]]\n  start end             insert\n1     1   1 F-[[X]+X]+F[+FX]-X\n\n[[1]]\n     start end\n[1,]     5   5\n[2,]     8   8\n[3,]    15  15\n[4,]    18  18\n\n[[2]]\n     start end\n[1,]     1   1\n[2,]    11  11\n[3,]    14  14\n\n[[1]]\n  start end             insert\n1     5   5 F-[[X]+X]+F[+FX]-X\n2     8   8 F-[[X]+X]+F[+FX]-X\n3    15  15 F-[[X]+X]+F[+FX]-X\n4    18  18 F-[[X]+X]+F[+FX]-X\n\n[[2]]\n  start end insert\n1     1   1     FF\n2    11  11     FF\n3    14  14     FF\n\n[[1]]\n      start end\n [1,]    10  10\n [2,]    13  13\n [3,]    20  20\n [4,]    23  23\n [5,]    30  30\n [6,]    33  33\n [7,]    40  40\n [8,]    43  43\n [9,]    56  56\n[10,]    59  59\n [ reached 'max' / getOption(\"max.print\") -- omitted 6 rows ]\n\n[[2]]\n      start end\n [1,]     1   1\n [2,]     2   2\n [3,]     6   6\n [4,]    16  16\n [5,]    19  19\n [6,]    26  26\n [7,]    36  36\n [8,]    39  39\n [9,]    46  46\n[10,]    47  47\n [ reached 'max' / getOption(\"max.print\") -- omitted 8 rows ]\n\n[[1]]\n  start end             insert\n1    10  10 F-[[X]+X]+F[+FX]-X\n2    13  13 F-[[X]+X]+F[+FX]-X\n3    20  20 F-[[X]+X]+F[+FX]-X\n4    23  23 F-[[X]+X]+F[+FX]-X\n5    30  30 F-[[X]+X]+F[+FX]-X\n6    33  33 F-[[X]+X]+F[+FX]-X\n [ reached 'max' / getOption(\"max.print\") -- omitted 10 rows ]\n\n[[2]]\n  start end insert\n1     1   1     FF\n2     2   2     FF\n3     6   6     FF\n4    16  16     FF\n5    19  19     FF\n6    26  26     FF\n [ reached 'max' / getOption(\"max.print\") -- omitted 12 rows ]\n\n[[1]]\n      start end\n [1,]    17  17\n [2,]    20  20\n [3,]    27  27\n [4,]    30  30\n [5,]    37  37\n [6,]    40  40\n [7,]    47  47\n [8,]    50  50\n [9,]    63  63\n[10,]    66  66\n [ reached 'max' / getOption(\"max.print\") -- omitted 54 rows ]\n\n[[2]]\n      start end\n [1,]     1   1\n [2,]     2   2\n [3,]     3   3\n [4,]     4   4\n [5,]     8   8\n [6,]     9   9\n [7,]    13  13\n [8,]    23  23\n [9,]    26  26\n[10,]    33  33\n [ reached 'max' / getOption(\"max.print\") -- omitted 74 rows ]\n\n[[1]]\n  start end             insert\n1    17  17 F-[[X]+X]+F[+FX]-X\n2    20  20 F-[[X]+X]+F[+FX]-X\n3    27  27 F-[[X]+X]+F[+FX]-X\n4    30  30 F-[[X]+X]+F[+FX]-X\n5    37  37 F-[[X]+X]+F[+FX]-X\n6    40  40 F-[[X]+X]+F[+FX]-X\n [ reached 'max' / getOption(\"max.print\") -- omitted 58 rows ]\n\n[[2]]\n  start end insert\n1     1   1     FF\n2     2   2     FF\n3     3   3     FF\n4     4   4     FF\n5     8   8     FF\n6     9   9     FF\n [ reached 'max' / getOption(\"max.print\") -- omitted 78 rows ]\n\n\n\n\nHead off to https://www.cs.unm.edu/~joel/PaperFoldingFractal/paper.html. Download the .jar file and save it say in your Documents folder. Open and play. Instructions are on the website. Make note of how the scaling factor works here.\nSome suggestions!! Note the alphabet!!! \n\n\n\n\n\n\n\nCol1\nCol2\n\n\n\n\n\nPythagorean Tree:\n\n\nAxiom: [++!++!++!]xy\n\nProduction Rules:\n\nx =\n\n[-[!++!++!++!]!xy\n\ny =\n\n\n\n[+![!++!++!++!]!xy\n-   startAngle: 0\n-   turnAngle: 45.0\n-   growth : 1.408\n\n\n |\n\n\n\nB. Krishna’s Anklets:\n\n\nAxiom: ++af-h-f+h\n-   f\n\n\n## h\nf++h++f-h-f++h++f-h-f++h\n\n\n\n\nProduction Rules:\n\nf = f - h - f ++ h ++ f - h - f\nstartAngle: 0\nturnAngle: 45.0\ngrowth:1.0",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "L-Systems"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/50-LSystems/index.html#design-principles-for-l-systems",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/50-LSystems/index.html#design-principles-for-l-systems",
    "title": "L-Systems",
    "section": "Design Principles for L-Systems",
    "text": "Design Principles for L-Systems\n\nPick a set of symbols. i.e. our alphabet (say two or three letters of the alphabet)\nMap these to specific movements in the growth of the tree\nDecide on an axiom. It can include one or more of the symbols.\nDecide on a (set of) production rules. These should generate all the symbols in our alphabet. (Why?)\nDecide on a scaling factor\n\nApply the production rules multiple times starting with the axiom, develop an extensive string using this recursion\nPlot the resulting string, scaling the individual recursions by the scaling factor.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "L-Systems"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/50-LSystems/index.html#wait-but-why",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/50-LSystems/index.html#wait-but-why",
    "title": "L-Systems",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nBy making use of “just sufficient randomness” in a few parameters, it is possible to generate very organic-looking trees\nThese tree-like layouts can show up in a surprising number of places, such as transport networks, residential layouts.\nThe multiple iterations generated from a few simple rules embody all the complexity of a language.\nTrees become a great metaphor for a diverse set of things and ideas.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "L-Systems"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/50-LSystems/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/50-LSystems/index.html#references",
    "title": "L-Systems",
    "section": "\n References",
    "text": "References\n\nJob Talle. Lindenmayer Systems https://jobtalle.com/lindenmayer_systems.html\n\nC. J Jennings. L-systems. https://cgjennings.ca/articles/l-systems/\n\nJoel Castellanos. Fractal Grower: Java Software for Growing Lindenmayer Substitution Fractals (L-systems). https://www.cs.unm.edu/~joel/PaperFoldingFractal/paper.html\n\nPaul Bourke. L systems User Notes. https://paulbourke.net/fractals/lsys/\n\nPrzemysław Prusinkiewicz and Aristid Lindenmayer. The Algorithmic Beauty of Plants. Springer-Verlag, 1990.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "L-Systems"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/10-Physics/01-Movement/index.html",
    "href": "content/courses/MathModelsDesign/Modules/10-Physics/01-Movement/index.html",
    "title": "  Dancing With Newton",
    "section": "",
    "text": "What do we recall from our school-time encounter with Newtonian Physics?\nWe have several aspects to movement, e.g.:\n\nposition\nvelocity\nacceleration\nrotation\noscillation\nfriction\n\nAnd here are some more terms from the spacecraft/rocket science domain:\n\nroll\npitch\nyaw\njerk\n\nAnything else?\nWe need to think of a way in which these aspects can be modelled and coded to suit our creative needs. Let us define these terms once again, so that we know what we are talking about!\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhere an object is, say in {x,y} coordinates.\n\n\nIn p5.js we can create the coordinates of an object separately, as x and y, OR create a p5.Vector object that can manipulate both coordinates together.\nWhy would we manipulate coordinates? Because we want things to move! Movement is manipulation of position.\n\n\n\n\n\n\n\n\n\nNoteVelocity\n\n\n\nRate of change of distance travelled. So \\(velocity \\sim distance/time\\). We can have movement in the x or y direction independently, or both movements can be simultaneous ( but not necessarily equal!) using the p5.Vector objects.\n\n\nHow do we model this in code? p5.js has a FrameRate (60 frames/second. Check this!!); this is the rate at which the code computes and refreshes the display. We can take individual x and y positions and add something to them each frame, causing the object to move. If the object is a p5.Vector object, we manipulate these coordinates using Vector arithmetic, which we will peep into shortly."
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/10-Physics/01-Movement/index.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/10-Physics/01-Movement/index.html#introduction",
    "title": "  Dancing With Newton",
    "section": "",
    "text": "What do we recall from our school-time encounter with Newtonian Physics?\nWe have several aspects to movement, e.g.:\n\nposition\nvelocity\nacceleration\nrotation\noscillation\nfriction\n\nAnd here are some more terms from the spacecraft/rocket science domain:\n\nroll\npitch\nyaw\njerk\n\nAnything else?\nWe need to think of a way in which these aspects can be modelled and coded to suit our creative needs. Let us define these terms once again, so that we know what we are talking about!\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhere an object is, say in {x,y} coordinates.\n\n\nIn p5.js we can create the coordinates of an object separately, as x and y, OR create a p5.Vector object that can manipulate both coordinates together.\nWhy would we manipulate coordinates? Because we want things to move! Movement is manipulation of position.\n\n\n\n\n\n\n\n\n\nNoteVelocity\n\n\n\nRate of change of distance travelled. So \\(velocity \\sim distance/time\\). We can have movement in the x or y direction independently, or both movements can be simultaneous ( but not necessarily equal!) using the p5.Vector objects.\n\n\nHow do we model this in code? p5.js has a FrameRate (60 frames/second. Check this!!); this is the rate at which the code computes and refreshes the display. We can take individual x and y positions and add something to them each frame, causing the object to move. If the object is a p5.Vector object, we manipulate these coordinates using Vector arithmetic, which we will peep into shortly."
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/05-Maths/20-Vectors/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/05-Maths/20-Vectors/index.html#references",
    "title": "\n Vectors",
    "section": "References",
    "text": "References\n\nhttps://www.math.hkust.edu.hk/~machas/vector-calculus-for-engineers.pdf",
    "crumbs": [
      "Math Models for Creative Coders",
      "Maths Basics",
      "<iconify-icon icon=\"ph:circles-three-fill\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Vectors"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/05-Maths/50-Matrices/index.html",
    "href": "content/courses/MathModelsDesign/Modules/05-Maths/50-Matrices/index.html",
    "title": "Matrix Algebra Whirlwind Tour",
    "section": "",
    "text": "In this hopefully short and crisp tour, we will look at elementary operations on matrices: transpose, addition/subtraction and multiplication, and finally the inverse.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Maths Basics",
      "Matrix Algebra Whirlwind Tour"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/05-Maths/50-Matrices/index.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/05-Maths/50-Matrices/index.html#introduction",
    "title": "Matrix Algebra Whirlwind Tour",
    "section": "",
    "text": "In this hopefully short and crisp tour, we will look at elementary operations on matrices: transpose, addition/subtraction and multiplication, and finally the inverse.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Maths Basics",
      "Matrix Algebra Whirlwind Tour"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/05-Maths/50-Matrices/index.html#what-is-a-matrix",
    "href": "content/courses/MathModelsDesign/Modules/05-Maths/50-Matrices/index.html#what-is-a-matrix",
    "title": "Matrix Algebra Whirlwind Tour",
    "section": "What is a Matrix?",
    "text": "What is a Matrix?\nAn rectangular array of numbers, arranged with rows and columns. A few examples are:\n\n\nEquation 1 is a 3 x 3 matrix (3 rows and 3 columns).\n\n\\[\n\\begin{bmatrix}\n2,3,3\\\\\n3,1,2\\\\\n1,2,4\\\\\n\\end{bmatrix}\n\\tag{1}\\]\n\nEquation 2 is a 3 x 4 matrix (3 rows and 4 columns).\n\n\\[\n\\begin{bmatrix}\n2,3,3,4\\\\\n3,1,2,3\\\\\n1,2,4,1\\\\\n\\end{bmatrix}\n\\tag{2}\\]\n\nEquation 3 is a 1 x 4 matrix (1 rows and 4 columns).\n\n\\[\n\\begin{bmatrix}\n2 ~ 3 ~ 4 ~ 2\\\\\n\\end{bmatrix}\n\\tag{3}\\]\n\nEquation 2 is a 3 x 1 matrix (3 rows and 1 columns).\n\n\\[\n\\begin{bmatrix}\n2\\\\\n3\\\\\n1\\\\\n\\end{bmatrix}\n\\tag{4}\\]",
    "crumbs": [
      "Math Models for Creative Coders",
      "Maths Basics",
      "Matrix Algebra Whirlwind Tour"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/05-Maths/50-Matrices/index.html#transpose",
    "href": "content/courses/MathModelsDesign/Modules/05-Maths/50-Matrices/index.html#transpose",
    "title": "Matrix Algebra Whirlwind Tour",
    "section": "Transpose",
    "text": "Transpose\nA matrix transpose is accomplished by writing rows as columns and vice versa. Hence an \\(r~ x ~ c\\) matrix becomes transposed into a \\(c~ x ~ r\\) matrix, as shown below:\n\n\n\n\nc: ⎡a  c  1⎤\n   ⎢       ⎥\n   ⎣b  d  2⎦\n\n\n\n\n\nc: ⎡a  b⎤\n   ⎢    ⎥\n   ⎢c  d⎥\n   ⎢    ⎥\n   ⎣1  2⎦",
    "crumbs": [
      "Math Models for Creative Coders",
      "Maths Basics",
      "Matrix Algebra Whirlwind Tour"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/05-Maths/50-Matrices/index.html#addition-and-subtraction",
    "href": "content/courses/MathModelsDesign/Modules/05-Maths/50-Matrices/index.html#addition-and-subtraction",
    "title": "Matrix Algebra Whirlwind Tour",
    "section": "Addition and Subtraction",
    "text": "Addition and Subtraction\nMatrices can be added or subtracted when they are of the same dimensions, rows and columns. Matrix elements are added or subtracted in element-wise fashion.\nAddition\n\n\n\n\nc: ⎡a  0⎤\n   ⎢    ⎥\n   ⎣b  1⎦\n\n\n\n\n\nc: ⎡a  c⎤\n   ⎢    ⎥\n   ⎣b  1⎦\n\n\n\n\n\nc: ⎡2⋅a  c⎤\n   ⎢      ⎥\n   ⎣2⋅b  2⎦\n\n\n\n\nSubtraction\n\n\n\n\nc: ⎡a  0⎤\n   ⎢    ⎥\n   ⎣b  1⎦\n\n\n\n\n\nc: ⎡a  c⎤\n   ⎢    ⎥\n   ⎣b  1⎦\n\n\n\n\n\nc: ⎡0  -c⎤\n   ⎢     ⎥\n   ⎣0  0 ⎦",
    "crumbs": [
      "Math Models for Creative Coders",
      "Maths Basics",
      "Matrix Algebra Whirlwind Tour"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/05-Maths/50-Matrices/index.html#multiplication",
    "href": "content/courses/MathModelsDesign/Modules/05-Maths/50-Matrices/index.html#multiplication",
    "title": "Matrix Algebra Whirlwind Tour",
    "section": "Multiplication",
    "text": "Multiplication\nMatrix multiplication can be visualized in many ways!! According to Gilbert Strang’s famous book on Linear Algebra, there are no less than 4 very useful ways. Let us understand perhaps the easiest one, Linear combination of Columns.\nSuppose there are three students who bought two kinds of food and drink:\n\nStudent A: 2 samosas and 1 chai\nStudent B: 3 samosas and 0 chai\nStudent C: 0 samosa and 2 chai-s (Just before Arvind’s R class.)\n\nSamosas cost 20 and chai costs 10. How much do they pay?\n\n\n\\[\n\\left[\\begin{matrix}\\color{red}{2} & \\color{blue}{1}\\\\\\\\\\color{red}{3} & \\color{blue}{0}\\\\\\\\\\color{red}{0} & \\color{blue}{2}\\end{matrix}\\right]\n\\]\n\n\\[\n\\left[\\begin{matrix}\\color{red}{20}\\\\\\\\\\color{blue}{10}\\end{matrix}\\right]\n\\]\n\n\\[\n\\left[\\begin{matrix}50\\\\\\\\60\\\\\\\\20\\end{matrix}\\right]\n\\]\n\n\n\n\n\\[Orders\\]\n\n\\[Prices\\]\n\n\\[Bills\\]\n\n\nWe see that the first number \\(20\\) in B multiplies the entire first column in A, and the second number \\(10\\) multiplies the entire second column in A. These two multiplied columns are added to get the matrix C.\nSuppose they had two options for shops? And the prices were different?\n\nSamosas cost 20 and chai costs 10 at Shop#1. (as before)\nSamosas cost 15 and chai costs 15 at Shop#2.\n\n\n\n\\[\n\\left[\\begin{matrix}\\color{red}{2} & \\color{blue}{1}\\\\\\\\\\color{red}{3} & \\color{blue}{0}\\\\\\\\\\color{red}{0} & \\color{blue}{2}\\end{matrix}\\right]\n\\]\n\n\\[\n\\left[\\begin{matrix}20 & 15\\\\\\\\10 & 15\\end{matrix}\\right]\n\\]\n\n\\[\n\\left[\\begin{matrix}50 & 45\\\\\\\\60 & 45\\\\\\\\20 & 30\\end{matrix}\\right]\n\\]\n\n\n\n\n\\[Orders\\]\n\n\\[Menus\\]\n\n\\[Amounts\\]\n\n\nShop#2 is cheaper. Drag peasant#3 and go there, peasants#1 and #2.\nHow did this multiplication happen? Same story as with one shop/prices, repeated to handle the second shop. Matrix B2 now has two columns for prices, for Shop#1 and Shop#2. Multiplication takes each column in B2 and does the same weighted-column-addition with A2. Hence two columns of answers in C2.\n\n\n\n\n\n\nImportantMatrix Multiplication in 4 ways\n\n\n\nAs shown in Gilbert Strang’s book, there are 4 ways of thinking about matrix multiplication:\n\nOur method of weighting columns in A using columns B and adding\nWeighting rows in B using rows in A and adding\nMultiplying individual values from rows in A and columns in B and adding up, for each location in C. ( This is the common textbook method)\nMultiplying rows in A and columns in B. ( A vectorized version of the previous method).\n\nFor now, this one method should suffice.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Maths Basics",
      "Matrix Algebra Whirlwind Tour"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/05-Maths/50-Matrices/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/05-Maths/50-Matrices/index.html#references",
    "title": "Matrix Algebra Whirlwind Tour",
    "section": "References",
    "text": "References\n\nGilbert Strang. Linear Algebra and its Applications. Thomson/Brooks-Cole.\nGlenn Henshaw.(Sep 28, 2019) Three Ways to Understand Matrix Multiplication. https://ghenshaw-work.medium.com/3-ways-to-understand-matrix-multiplication-fe8a007d7b26",
    "crumbs": [
      "Math Models for Creative Coders",
      "Maths Basics",
      "Matrix Algebra Whirlwind Tour"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/05-Maths/listing.html",
    "href": "content/courses/MathModelsDesign/Modules/05-Maths/listing.html",
    "title": "Math Concepts",
    "section": "",
    "text": "No matching items\n Back to top",
    "crumbs": [
      "Math Models for Creative Coders",
      "Maths Basics"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/55-Connections/10-GraphNetworks/index.html",
    "href": "content/courses/MathModelsDesign/Modules/55-Connections/10-GraphNetworks/index.html",
    "title": "Working with Networks",
    "section": "",
    "text": "Do you think your friends have more friends than you have? Do you think that you are outside the herd, and that what you think or do is from your own mind?\nAnyways…"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/55-Connections/10-GraphNetworks/index.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/55-Connections/10-GraphNetworks/index.html#introduction",
    "title": "Working with Networks",
    "section": "",
    "text": "Do you think your friends have more friends than you have? Do you think that you are outside the herd, and that what you think or do is from your own mind?\nAnyways…"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/55-Connections/10-GraphNetworks/index.html#activities",
    "href": "content/courses/MathModelsDesign/Modules/55-Connections/10-GraphNetworks/index.html#activities",
    "title": "Working with Networks",
    "section": "Activities",
    "text": "Activities\nActivity-1: Secret Santa Game\nLet us play this in the vanilla way: Paper chits with names in a bin and drawing them in turn. What can go wrong with this?\nShould we use this instead? https://www.drawnames.com.sg/secret-santa-generator\nDiscussion: Nodes, Links, Link Directionality, Connected and Disconnected Networks\nActivity-2: Barabasi Cocktail Party Game\nThis is a game “invented” by Alberto-Laszlo Barabasi, a Network Science pioneer and expert, who has written a wonderful, and wonderfully acessible, book on Network Science, available online http://networksciencebook.com/\nDiscussion: Network Mechanisms, Information Flow, Giant Component,Emergence\nActivity-3: Indian Surnames Game\n\nHow many common India surnames do we know? Let us write them on the board.\nEach of us will now look at each surname and recollect how many people they know with that surname.\nWrite down the score for each surname.\nLet’s plot this on (yet another ) network!!\nTry also to create a map using this website: The Memory Underground https://memoryunderground.com\n\n\nDiscussion: Node Degree, Giant Component?  Small Worlds?  Multi-Link network, Link Values or Costs\nActivity-4: Way-Spotting Game\nNow that we have an idea of nodes, links and costs, let us get an experience of some more network science ideas:\n\nMake groups of 3.\nHead over to http://www.wayspotting.com/index.html\n\nPlay!! Make a note of your route each time ( your “traversal” of the network)\nNote if you can see the following:\n\n\n\nFrequently Used Nodes\nFrequently used Links\n\n\n\nSee here for more info: https://medium.com/@ran_katzir/teaching-network-science-using-board-games-f78489a3b3bd\n\n\nDiscussion: Network Traversal, Node Degree, Centrality, Betweenness, Link Values or Costs\nActivity-5: Hi, I am Kevin Bacon, SMI Foundation 2022 Batch\nLet us find a Keven Bacon in SMI Foundation Studies Programme!!\n\nCollect friends Data from across college/class, import and plot, analyze and comment\nUse this online tool at DataBasic.io https://databasic.io to Connect the Dots, OR\nEven more fun at at GraphCommons https://graphcommons.com/graphs/new\n\n\nDiscussion: Node Degree, Centrality, Betweenness, Link Values or Costs\nActivity-6: Can you Introduce me to Chandler, again?\n\nTake your favourite Literary Work / TV Serial / Movie and create a Network Database for it.\nVisualize it either with or without tech tools From Teach Engineering, this Activity Sheet https://www.teachengineering.org/activities/view/uno_graphtheory_lesson01_activity2\n\nCan also use Graph Comicshttps://aviz.fr/~bbach/graphcomics/\n\n\nDiscussion: Networks are everywhere, Cannot \"unsee\" them, You are a node and you are a link..are you?\nActivity-7: Why are all Metro Maps at 45 degrees?\n\nhttps://artsandculture.google.com/asset/the-tate-gallery-by-tube-david-booth/PAG-Gx_SV2jNiw?hl=en\nhttps://search.r-project.org/CRAN/refmans/ggraph/html/layout_tbl_graph_metro.html\nHenry Beck’s London Underground Map. https://artsandculture.google.com/asset/underground-map-henry-c-beck-london-transport-and-waterlow-sons-ltd/fAHJweSexswKxw?hl=en and\n\nhttps://g.co/arts/UNfBm6QL5kukYm4r9"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/55-Connections/10-GraphNetworks/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/55-Connections/10-GraphNetworks/index.html#references",
    "title": "Working with Networks",
    "section": "References",
    "text": "References\n\nDmitry Zinoniev, Network Science Intro Slides, https://www.slideshare.net/DmitryZinoviev/workshop-20212296\nMark Newman, The Physics of Networks,Read the PDF\nA Network oriented short story. Frigyes Karinthy, “Chains”. Read PDF\nWho told you about Srishti? Where? Read Mark Granovetter, The Strength of Weak Ties. https://www.cs.cmu.edu/~jure/pub/papers/granovetter73ties.pdf\nPlamen Ch. Ivanov.(30 June 2021) The New Field of Network Physiology: Building the Human Physiolome. https://doi.org/10.3389/fnetp.2021.711778"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/20-Systems/listing.html",
    "href": "content/courses/MathModelsDesign/Modules/20-Systems/listing.html",
    "title": "Systems",
    "section": "",
    "text": "No matching items\n Back to top"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/30-MLP/index.html",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/30-MLP/index.html",
    "title": "The Multilayer Perceptron",
    "section": "",
    "text": "This was our bare bones Perceptron, or neuron as we will refer to it henceforth:\n\\[\ny_k = sign~(~\\sum_{k=1}^n W_k*x_k + b~)\n\\]\n\n\n\nFor the multi-layer perceptron, two changes were made:\n\nChanging the hard-threshold activation into a more soft sigmoid activation\naddition of (one or more ) hidden layers.\n\nLet us discuss these changes in detail.\n\n\nWe said earlier that the weighting and adding is a linear operation.\nWhile this is great, simple linear translations of data are not capable of generating what we might call learning or generalization ability.\nThe outout of the perceptron is a “learning decision” that is made by deciding if the combined output is greater or smaller than a threshold.\nWe need to have some non-linear block to allow the data to create nonlinear transformations of the data space, such as curving it, or folding it, or creating bumps, depressions, twists, and so on.\n\n\n\nActivation\n\n\nThis nonlinear function needs to be chosen with care so that it is both differentiable and keeps the math analysis tractable. (More later)\nSuch a nonlinear mathematical function is implemented in the Activation Block.\nSee this example: red and blue areas, which we wish to separate and classify these with our DLNN, are not separable unless we fold and curve our 2D data space.\nThe separation is achieved using a linear operation, i.e. a LINE!!\n\n\n\n\n\n\nFigure 1: From Colah Blog, used sadly without permission\n\n\n\nFor instance in Figure 2, no amount of stretching or compressing of the surface can separate the two sets ( blue and red ) using a line or plane, unless the surface can be warped into another dimension by folding.\n\nThe hard-threshold used in the Perceptron allowed us to make certain decisions based on linear combinations of the input data. But what is the dataset possesses classes that are not separable in a linear way? What if different categories of points are intertwined with a curved boundary between classes?\nWe need to have some non-linear block to allow the data to create nonlinear transformations of the data space, such as curving it, or folding it, or creating bumps, depressions, twists, and so on.\n\n\nActivation\n\n\nThis nonlinear function needs to be chosen with care so that it is both differentiable and keeps the math analysis tractable. (More later)\nSuch a nonlinear mathematical function is implemented in the Activation Block.\nSee this example: red and blue areas, which we wish to separate and classify these with our DLNN, are not separable unless we fold and curve our 2D data space.\nThe separation is achieved using a linear operation, i.e. a LINE!!\n\n\n\n\n\n\nFigure 2: From Colah Blog, used sadly without permission\n\n\n\nFor instance in Figure 2, no amount of stretching or compressing of the surface can separate the two sets ( blue and red ) using a line or plane, unless the surface can be warped into another dimension by folding.\n\nSo how do we implement this nonlinear Activation Block?\n\nOne of the popular functions used in the Activation Block is a function based on the exponential function \\(e^x\\).\nWhy? Because this function retains is identity when differentiated! This is a very convenient property!\n\n\n\nSigmoid Activation\n\n\n\n\n\n\n\nNoteRemembering Logistic Regression\n\n\n\nRecall your study of Logistic Regression. There, the Sigmoid function was used to model the odds of the (Qualitative) target variable against the (Quantitative) predictor.\n\n\n\n\n\n\n\n\nNoteBut Why Sigmoid?\n\n\n\nBecause the Sigmoid function is differentiable. And linear in the mid ranges. Oh, and remember the Chain Rule?\n\\[\n\\begin{align}\n\\frac{df(x)}{dx}\n&= \\frac{d}{dx} * \\frac{1}{1 + e^{-x}} \\\\\\\n&= -(1 + e^{-x})^{-2} * \\frac{d}{dx}(1 + e^{-x})~~\\text{(Using Chain Rule)}\\\\\n&= -(1 + e^{-x})^{-2} * (-e^{-x})\\\\\n&=  \\frac{e^{-x}}{(1 + e^{-x})^{2}}\\\\\n&= \\frac{(1 + e^{-x}) -1}{(1 + e^{-x})^{2}}\\\\\n&= \\frac{1}{1 + e^{-x}} * \\Bigg({\\frac{1 + e^{-x}}{1 + e^{-x}}} - \\frac{1}{1 + e^{-x}}\\Bigg)\\\\\\\n&\\text{ and therefore}\\\\\\\n\\Large{\\frac{df(x)}{dx}} &= \\Large{f(x) * (1 - f(x))}\\\\\n\\end{align}\n\\]\n\n\n\nThe MLP adds several layers of perceptrons, in layers, as shown below:\n\n\n\n\n\nHere, i1, i2, and i3 are input neurons: they are simply inputs and are drawn as circles in the literature.\nThe h1, h2, h3 are neuron in the so-called hidden layer; hidden because they are not inputs!\nThe neurons o1, o2, and o3 are output neurons.\nThe signals/information flows from left to right in the diagram. And we have shown every neuron connected to everyone in the next layer downstream.\n\nHow do we mathematically, and concisely, express the operation of the MLP? Let us setup a notation for the MLP weights.\n\n\n\\(l\\) : layer index;\n\n\\(j\\), \\(k\\) : neuron index in two adjacent layers\n\n\\(W^l_{jk}\\) (i.e. \\(W^{layer}_{{source}~{destn}}\\)) : weight from \\(j\\)th neuron / \\((l−1)\\)th layer to \\(k\\)th neuron / \\(l\\)th layer;\n\n\\(b^l_k\\) : bias of the \\(k\\)th neuron in the \\(l\\)th layer.\n\n\\(a^l_k\\) : activation (output) of \\(k\\)th neuron / \\(l\\)th layer.\n\n\n\n\n We can write the outputs of the layer-2 as:\n\\[\n\\begin{align}\n(k = 1): ~ a_{12} = sigmoid~(~\\color{red}{W^2_{11}*a_{11}} + \\color{skyblue}{W^2_{21}*a_{21}} + \\color{forestgreen}{W^2_{31}*a_{31}} ~ + b_{12})\\\\\n(k = 2): ~ a_{22} = sigmoid~(~W^2_{12}*a_{11} + W^2_{22}*a_{21} + W^2_{32}*a_{31}~ + b_{22} )\\\\\n(k = 3): ~ a_{32} = sigmoid~(~W^2_{13}*a_{11} + W^2_{23}*a_{21} + W^2_{33}*a_{31}~ + b_{32})\\\\\n\\end{align}\n\\]\nIn (dreaded?) matrix notation :\n\\[\n\\begin{bmatrix}\na_{12}\\\\\na_{22}\\\\\na_{32}\\\\\n\\end{bmatrix} =\nsigmoid~\\Bigg(\n\\begin{bmatrix}\n\\color{red}{W^2_{11}} & \\color{skyblue}{W^2_{21}} & \\color{forestgreen}{W^2_{31}}\\\\\nW^2_{12} & W^2_{22} & W^2_{32}\\\\\nW^2_{13} & W^2_{23} & W^2_{33}\\\\\n\\end{bmatrix} *\n\\begin{bmatrix}\n\\color{red}{a_{11}}\\\\\n\\color{skyblue}{a_{21}}\\\\\n\\color{forestgreen}{a_{31}}\\\\\n\\end{bmatrix} +\n\\begin{bmatrix}\nb_{12}\\\\\nb_{22}\\\\\nb_{32}\\\\\n\\end{bmatrix}\n\\Bigg)\n\\]\nIn compact notation we write, in general:\n\\[\nA^l = \\sigma\\Bigg(W^lA^{l-1} + B^l\\Bigg)\n\\]\n\\[\na^l_j=σ(\\sum_kW^l_{jk} * a^{l−1}_k+b^l_j)\n\\tag{1}\\]",
    "crumbs": [
      "Math Models for Creative Coders",
      "AI",
      "The Multilayer Perceptron"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/30-MLP/index.html#what-is-a-multilayer-perceptron",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/30-MLP/index.html#what-is-a-multilayer-perceptron",
    "title": "The Multilayer Perceptron",
    "section": "",
    "text": "This was our bare bones Perceptron, or neuron as we will refer to it henceforth:\n\\[\ny_k = sign~(~\\sum_{k=1}^n W_k*x_k + b~)\n\\]\n\n\n\nFor the multi-layer perceptron, two changes were made:\n\nChanging the hard-threshold activation into a more soft sigmoid activation\naddition of (one or more ) hidden layers.\n\nLet us discuss these changes in detail.\n\n\nWe said earlier that the weighting and adding is a linear operation.\nWhile this is great, simple linear translations of data are not capable of generating what we might call learning or generalization ability.\nThe outout of the perceptron is a “learning decision” that is made by deciding if the combined output is greater or smaller than a threshold.\nWe need to have some non-linear block to allow the data to create nonlinear transformations of the data space, such as curving it, or folding it, or creating bumps, depressions, twists, and so on.\n\n\n\nActivation\n\n\nThis nonlinear function needs to be chosen with care so that it is both differentiable and keeps the math analysis tractable. (More later)\nSuch a nonlinear mathematical function is implemented in the Activation Block.\nSee this example: red and blue areas, which we wish to separate and classify these with our DLNN, are not separable unless we fold and curve our 2D data space.\nThe separation is achieved using a linear operation, i.e. a LINE!!\n\n\n\n\n\n\nFigure 1: From Colah Blog, used sadly without permission\n\n\n\nFor instance in Figure 2, no amount of stretching or compressing of the surface can separate the two sets ( blue and red ) using a line or plane, unless the surface can be warped into another dimension by folding.\n\nThe hard-threshold used in the Perceptron allowed us to make certain decisions based on linear combinations of the input data. But what is the dataset possesses classes that are not separable in a linear way? What if different categories of points are intertwined with a curved boundary between classes?\nWe need to have some non-linear block to allow the data to create nonlinear transformations of the data space, such as curving it, or folding it, or creating bumps, depressions, twists, and so on.\n\n\nActivation\n\n\nThis nonlinear function needs to be chosen with care so that it is both differentiable and keeps the math analysis tractable. (More later)\nSuch a nonlinear mathematical function is implemented in the Activation Block.\nSee this example: red and blue areas, which we wish to separate and classify these with our DLNN, are not separable unless we fold and curve our 2D data space.\nThe separation is achieved using a linear operation, i.e. a LINE!!\n\n\n\n\n\n\nFigure 2: From Colah Blog, used sadly without permission\n\n\n\nFor instance in Figure 2, no amount of stretching or compressing of the surface can separate the two sets ( blue and red ) using a line or plane, unless the surface can be warped into another dimension by folding.\n\nSo how do we implement this nonlinear Activation Block?\n\nOne of the popular functions used in the Activation Block is a function based on the exponential function \\(e^x\\).\nWhy? Because this function retains is identity when differentiated! This is a very convenient property!\n\n\n\nSigmoid Activation\n\n\n\n\n\n\n\nNoteRemembering Logistic Regression\n\n\n\nRecall your study of Logistic Regression. There, the Sigmoid function was used to model the odds of the (Qualitative) target variable against the (Quantitative) predictor.\n\n\n\n\n\n\n\n\nNoteBut Why Sigmoid?\n\n\n\nBecause the Sigmoid function is differentiable. And linear in the mid ranges. Oh, and remember the Chain Rule?\n\\[\n\\begin{align}\n\\frac{df(x)}{dx}\n&= \\frac{d}{dx} * \\frac{1}{1 + e^{-x}} \\\\\\\n&= -(1 + e^{-x})^{-2} * \\frac{d}{dx}(1 + e^{-x})~~\\text{(Using Chain Rule)}\\\\\n&= -(1 + e^{-x})^{-2} * (-e^{-x})\\\\\n&=  \\frac{e^{-x}}{(1 + e^{-x})^{2}}\\\\\n&= \\frac{(1 + e^{-x}) -1}{(1 + e^{-x})^{2}}\\\\\n&= \\frac{1}{1 + e^{-x}} * \\Bigg({\\frac{1 + e^{-x}}{1 + e^{-x}}} - \\frac{1}{1 + e^{-x}}\\Bigg)\\\\\\\n&\\text{ and therefore}\\\\\\\n\\Large{\\frac{df(x)}{dx}} &= \\Large{f(x) * (1 - f(x))}\\\\\n\\end{align}\n\\]\n\n\n\nThe MLP adds several layers of perceptrons, in layers, as shown below:\n\n\n\n\n\nHere, i1, i2, and i3 are input neurons: they are simply inputs and are drawn as circles in the literature.\nThe h1, h2, h3 are neuron in the so-called hidden layer; hidden because they are not inputs!\nThe neurons o1, o2, and o3 are output neurons.\nThe signals/information flows from left to right in the diagram. And we have shown every neuron connected to everyone in the next layer downstream.\n\nHow do we mathematically, and concisely, express the operation of the MLP? Let us setup a notation for the MLP weights.\n\n\n\\(l\\) : layer index;\n\n\\(j\\), \\(k\\) : neuron index in two adjacent layers\n\n\\(W^l_{jk}\\) (i.e. \\(W^{layer}_{{source}~{destn}}\\)) : weight from \\(j\\)th neuron / \\((l−1)\\)th layer to \\(k\\)th neuron / \\(l\\)th layer;\n\n\\(b^l_k\\) : bias of the \\(k\\)th neuron in the \\(l\\)th layer.\n\n\\(a^l_k\\) : activation (output) of \\(k\\)th neuron / \\(l\\)th layer.\n\n\n\n\n We can write the outputs of the layer-2 as:\n\\[\n\\begin{align}\n(k = 1): ~ a_{12} = sigmoid~(~\\color{red}{W^2_{11}*a_{11}} + \\color{skyblue}{W^2_{21}*a_{21}} + \\color{forestgreen}{W^2_{31}*a_{31}} ~ + b_{12})\\\\\n(k = 2): ~ a_{22} = sigmoid~(~W^2_{12}*a_{11} + W^2_{22}*a_{21} + W^2_{32}*a_{31}~ + b_{22} )\\\\\n(k = 3): ~ a_{32} = sigmoid~(~W^2_{13}*a_{11} + W^2_{23}*a_{21} + W^2_{33}*a_{31}~ + b_{32})\\\\\n\\end{align}\n\\]\nIn (dreaded?) matrix notation :\n\\[\n\\begin{bmatrix}\na_{12}\\\\\na_{22}\\\\\na_{32}\\\\\n\\end{bmatrix} =\nsigmoid~\\Bigg(\n\\begin{bmatrix}\n\\color{red}{W^2_{11}} & \\color{skyblue}{W^2_{21}} & \\color{forestgreen}{W^2_{31}}\\\\\nW^2_{12} & W^2_{22} & W^2_{32}\\\\\nW^2_{13} & W^2_{23} & W^2_{33}\\\\\n\\end{bmatrix} *\n\\begin{bmatrix}\n\\color{red}{a_{11}}\\\\\n\\color{skyblue}{a_{21}}\\\\\n\\color{forestgreen}{a_{31}}\\\\\n\\end{bmatrix} +\n\\begin{bmatrix}\nb_{12}\\\\\nb_{22}\\\\\nb_{32}\\\\\n\\end{bmatrix}\n\\Bigg)\n\\]\nIn compact notation we write, in general:\n\\[\nA^l = \\sigma\\Bigg(W^lA^{l-1} + B^l\\Bigg)\n\\]\n\\[\na^l_j=σ(\\sum_kW^l_{jk} * a^{l−1}_k+b^l_j)\n\\tag{1}\\]",
    "crumbs": [
      "Math Models for Creative Coders",
      "AI",
      "The Multilayer Perceptron"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/30-MLP/index.html#wait-but-why",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/30-MLP/index.html#wait-but-why",
    "title": "The Multilayer Perceptron",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nThe “vanilla” perceptron was big advance in AI and learning. However, it was realized that this can only make classification decisions with data that are linearly separable.\nIncluding a differentiable non-linearity in the activation block allows us to deform the coordinate space in which the data points are mapped.\nThis deformation may permit unique views of the data wherein the categories of data are separable by an n-dimensional plane.\nThis idea is also used in a machine learning algorithm called Support Vector Machines.",
    "crumbs": [
      "Math Models for Creative Coders",
      "AI",
      "The Multilayer Perceptron"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/30-MLP/index.html#mlps-in-code",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/30-MLP/index.html#mlps-in-code",
    "title": "The Multilayer Perceptron",
    "section": "MLPs in Code",
    "text": "MLPs in Code\n\n\nUsing p5.js\nUsing R\n\n\n\n\n\n\nUsing torch.",
    "crumbs": [
      "Math Models for Creative Coders",
      "AI",
      "The Multilayer Perceptron"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/30-MLP/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/30-MLP/index.html#references",
    "title": "The Multilayer Perceptron",
    "section": "References",
    "text": "References\n\nTariq Rashid. Make your own Neural Network. PDF Online\n\nMathoverflow. Intuitive Crutches for Higher Dimensional Thinking. https://mathoverflow.net/questions/25983/intuitive-crutches-for-higher-dimensional-thinking\n\n3D MatMul Visualizerhttps://bhosmer.github.io/mm/ref.html",
    "crumbs": [
      "Math Models for Creative Coders",
      "AI",
      "The Multilayer Perceptron"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/40-BackProp/index.html",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/40-BackProp/index.html",
    "title": "MLPs and Backpropagation",
    "section": "",
    "text": "We saw how each layer works:\n\\[\n\\begin{bmatrix}\na_{12}\\\\\na_{22}\\\\\na_{32}\\\\\n\\end{bmatrix} =\nsigmoid~\\Bigg(\n\\begin{bmatrix}\n\\color{red}{W^2_{11}} & \\color{skyblue}{W^2_{21}} & \\color{forestgreen}{W^2_{31}}\\\\\nW^2_{12} & W^2_{22} & W^2_{32}\\\\\nW^2_{13} & W^2_{23} & W^2_{33}\\\\\n\\end{bmatrix} *\n\\begin{bmatrix}\n\\color{red}{a_{11}}\\\\\n\\color{skyblue}{a_{21}}\\\\\n\\color{forestgreen}{a_{31}}\\\\\n\\end{bmatrix} +\n\\begin{bmatrix}\nb_{12}\\\\\nb_{22}\\\\\nb_{32}\\\\\n\\end{bmatrix}\n\\Bigg)\n\\tag{1}\\]\nand:\n\\[\nA^l = \\sigma\\Bigg(W^lA^{l-1} + B^l\\Bigg)\n\\tag{2}\\]\nSee how the connections between neurons are marked by weights: these multiply the signal from the previous neuron. The multiplied/weighted products are added up in the neuron, and the sum is given to the activation block therein.\nSo learning?\nThe only controllable variables in a neural network are these weights! So learning involves adapting these weights so that they can perform a useful function.",
    "crumbs": [
      "Math Models for Creative Coders",
      "AI",
      "MLPs and Backpropagation"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/40-BackProp/index.html#what-is-the-learning-process",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/40-BackProp/index.html#what-is-the-learning-process",
    "title": "MLPs and Backpropagation",
    "section": "What is the Learning Process?",
    "text": "What is the Learning Process?\nThe process of adapting the weights of a neural network can be described in the following steps:\n\n\nTraining Set: Training is over several known input-output pairs (“training data”)\n\nTraining Epoch: For each input, the signals propagate forward until we have an output\n\nError Calculation: Output is compared with desired output, to calculate error\n\n\nBackpropagation: Each neuron (and its weights) need to be told what is their share of the error! Errors therefore need to be sent backward from the output to input, unravelling them from layer \\(l\\) to layer \\(l-1\\). (like apportioning blame !!).\n\nError-to-Cost: How does error at any given neuron relate to the idea of an overall Cost function? Is the Cost function also apportioned in the same way?\n\nDifferentiate: Evaluate the effect of each weight/bias on the (apportioned) error overall Cost. (Slope!!)\n\nGradient Descent: Adapt the weights/biases with a small step in the opposite direction to the slope.\n\nThere.\nWhat is the Output Error?\nIf \\(d(k)\\) are the desired outputs of the NN (over an entire training set), and \\(y(k)\\) are the outputs of the output layer, then we calculate the error at the outputs of the NN as:\n\\[\ne(k) = a(k) - d(k)\n\\tag{3}\\]\nThis error is calculated at each output for each training epoch/sample/batch. (More about the batch-mode in a bit.)\nWhat is the Cost Function?\nWe define the cost or objective function as the squared error averaged over all neurons:\n\\[\n\\begin{align}\nC(W, b) &= \\frac{1}{2n}\\sum^{n ~ neurons}_{i=1}e^2(i)\\\\\n\\\\\n&= \\frac{1}{2n}\\sum^{n~neurons}_{k=1}(a_i - d_i)^2\n\\end{align}\n\\tag{4}\\]\nThe \\(a_i\\)s are the outputs of \\(n\\) neurons and \\(d_i\\) are the desired outputs for each of the training samples.\nThe Cost Function is of course dependent upon the Weights and the biases, and is to be minimized by adapting these. Using the sum of squared errors, along with the linear operations in the NN guarantees that the Cost Function (usually) has one global, minimum.\nWhat is Backpropagation of Error?\nAs we stated earlier, error is calculated at the output. In order to adapt all weights, we need to send error proportionately back along the network, towards the input. This proportional error will give us a basis to adapt the individual weights anywhere in the network.\nWhat does “proportional” mean here? Consider the diagram below:\n\n\n\n\n\n\\[\n\\begin{align}\ne_{11} ~~\\pmb\\sim~~ ~ e_{12} * \\frac{W_{11}}{Sum~of~Weights~to~{\\color{magenta}{\\pmb{h_1}}}}\\\\\ne_{21} ~~\\pmb\\sim~~ ~ e_{12} * \\frac{W_{21}}{Sum~of~Weights~to~{\\color{magenta}{\\pmb{h_1}}}} \\\\\ne_{31}~~\\pmb\\sim~~ ~ e_{12} * \\frac{W_{31}}{Sum~of~Weights~to~\\color{magenta}{\\pmb{h_1}}} \\\\\n\\end{align}\n\\]\n\n\n\n\\[\n\\begin{align}\ne_{11} ~~\\pmb\\sim~~ ~ e_{12} * \\frac{W_{11}}{\\pmb{\\color{magenta}{W_{11} + W_{21} + W_{31}}}} \\\\\ne_{21} ~~\\pmb\\sim~~ ~ e_{12} *\\frac{W_{21}}{\\pmb{\\color{magenta}{W_{11} + W_{21} + W_{31}}}} \\\\\ne_{31} ~~\\pmb\\sim~~ ~ e_{12} *\\frac{W_{31}}{\\pmb{\\color{magenta}{W_{11} + W_{21} + W_{31}}}}  \\\\\n\\end{align}\n\\]\n\n\nThese are the contributions of the error \\(e_{12}\\) to each of the previous neurons.\nAnother way of looking at this:\n\n\n\n\n\n\\[\n\\begin{align}\ne_{11} =~ e_{12} * \\frac{W_{11}}{Sum~of~weights~to~{\\color{orange}{\\pmb {h_1}}}}\\\\\n+ ~ e_{22} * \\frac{W_{21}}{Sum~of~Weights~to~\\color{pink}{\\pmb{h_2}}} \\\\\n+ ~ e_{32} * \\frac{W_{31}}{Sum~of~Weights~to~\\color{teal}{\\pmb{h_3}}}  \\\\\n\\end{align}\n\\]\n\n\n\n\\[\n\\begin{align}\ne_{11} = ~ e_{12} * \\frac{W_{11}}{\\pmb{\\color{orange}{W_{11} + W_{21} + W_{31}}}}\\\\\n+ ~e_{22} * \\frac{W_{12}}{\\pmb{\\color{pink}{W_{12} + W_{22} + W_{32}}}} \\\\\n+ ~e_{32} * \\frac{W_{13}}{\\pmb{\\color{teal}{W_{13} + W_{23} + W_{33}}}}  \\\\\n\\end{align}\n\\]\n\\[\n\\begin{align}\ne_{21} = similar~expression!!\\\\\n\\\ne_{31} = similar~expression!!\\\\\n\\end{align}\n\\]\n\n\nEquation corrected by Gayatri Jadhav, April 2025\nThis is the total error at \\(e_{11}\\) from all the three output errors. So:\n\nWe have taken each output error, \\(e_{*2}\\) and parcelled it back to the preceding neurons in proportion to the connecting Weight. This makes intuitive sense; we are making those neurons put their money where their mouth is. As Nassim Nicholas Taleb says, people (and neurons!) need to pay for their opinions, especially when things go wrong!\nThe accumulated error at each neuron in layer \\(l-1\\) is the weighted sum of back-propagated error contributions from all layer \\(l\\) neurons to which we are connected.\nSo we can compactly write the relationships above as:\n\n\\[\n\\begin{bmatrix}\ne_{11}\\\\\ne_{21}\\\\\ne_{31}\\\\\n\\end{bmatrix} =\n\\Bigg(\n\\begin{bmatrix}\n\\frac{W_{11}}{D_{11}} & \\frac{W_{12}}{D_{12}} & \\frac{W_{13}}{D_{13}}\\\\\n\\frac{W_{21}}{D_{21}} & \\frac{W_{22}}{D_{22}} & \\frac{W_{23}}{D_{23}}\\\\\n\\frac{W_{31}}{D_{31}} & \\frac{W_{32}}{D_{32}} & \\frac{W_{33}}{D_{33}}\\\\\n\\end{bmatrix} *\n\\begin{bmatrix}\n{e_{12}}\\\\\n{e_{22}}\\\\\n{e_{32}}\\\\\n\\end{bmatrix}\n\\Bigg)\n\\]\nThe denominators make things look complicated! But if we are able to simply ignore them for a moment, then we see a very interesting thing:\n\\[\n\\begin{bmatrix}\ne_{11}\\\\\ne_{21}\\\\\ne_{31}\\\\\n\\end{bmatrix} \\pmb{\\sim}\n\\begin{bmatrix}\nW_{11} & W_{12} & W_{13} \\\\\nW_{21} & W_{22}  & W_{23} \\\\\nW_{31} & W_{32} & W_{33} \\\\\n\\end{bmatrix} *\n\\begin{bmatrix}\n{e_{12}}\\\\\n{e_{22}}\\\\\n{e_{32}}\\\\\n\\end{bmatrix}\n\\]\nThis new approximate matrix is the tranpose of our original Weight matrix from Equation 1! The rows there have become columns here!! That makes intuitive sense: in the forward information direction, we were accounting for information from the point of view of the destinations; in the reverse error backpropagation direction, we are accounting for information from the point of view of the sources.\nWriting this equation in a compact way:\n\\[\n\\Large{e^{l-1} ~ \\pmb{\\sim} ~ {W^l}^{\\pmb{\\color{red}{T}}}* e^{l}}\n\\tag{5}\\]\nThis is our equation for backpropagation of error.\nWhy is ignoring all those individual denominators justified? Let us park that question until we have understood the one last step in NN training, the Gradient Descent.",
    "crumbs": [
      "Math Models for Creative Coders",
      "AI",
      "MLPs and Backpropagation"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/40-BackProp/index.html#here-comes-the-rain-maths-again",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/40-BackProp/index.html#here-comes-the-rain-maths-again",
    "title": "MLPs and Backpropagation",
    "section": "Here Comes the Rain Maths Again!",
    "text": "Here Comes the Rain Maths Again!\nNow, we are ready (maybe?) to watch these two very beautifully made videos on Backpropagation. One is of course from Dan Shiffman, and the other from Grant Sanderson a.ka. 3Blue1Brown.",
    "crumbs": [
      "Math Models for Creative Coders",
      "AI",
      "MLPs and Backpropagation"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/40-BackProp/index.html#backpropagation-in-code",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/40-BackProp/index.html#backpropagation-in-code",
    "title": "MLPs and Backpropagation",
    "section": "Backpropagation in Code",
    "text": "Backpropagation in Code\n\n\nUsing p5.js\nUsing R\n\n\n\n\n\n\nUsing torch.",
    "crumbs": [
      "Math Models for Creative Coders",
      "AI",
      "MLPs and Backpropagation"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/40-BackProp/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/40-BackProp/index.html#references",
    "title": "MLPs and Backpropagation",
    "section": "References",
    "text": "References\n\nTariq Rashid. Make your own Neural Network. PDF Online\n\nMathoverflow. Intuitive Crutches for Higher Dimensional Thinking. https://mathoverflow.net/questions/25983/intuitive-crutches-for-higher-dimensional-thinking\n\nInteractive Backpropagation Explainer https://xnought.github.io/backprop-explainer/",
    "crumbs": [
      "Math Models for Creative Coders",
      "AI",
      "MLPs and Backpropagation"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/20-Perceptron/index.html#what-is-a-perceptron",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/20-Perceptron/index.html#what-is-a-perceptron",
    "title": "The Perceptron",
    "section": "What is a Perceptron?",
    "text": "What is a Perceptron?\nThe perceptron was invented by Frank Rosenblatt is considered one of the foundational pieces of neural network structures. The output is viewed as a decision from the neuron and is usually propagated as an input to other neurons inside the neural network.\n\n\nPerceptron\n\nMath Intuition\n\nWe can imagine this as a set of inputs that averaged in weighted fashion.\n\n\\[\ny_k = sign~(~\\sum_{k=1}^n W_k*x_k + b~)\n\\tag{1}\\]\n\nSince the inputs are added with linear weighting, this effectively acts like a linear transformation of the input data.\n\nA linear equation of this sort is the general equation of an n-dimensional plane.\nIf we imagine the input as representing the n-coordinates in a plane, then the multiplications scale/stretch/compress the plane, like a rubber sheet. (But do not fold it.)\nIf there were only 2 inputs, we could mentally picture this with a handkerchief.\n\n\nMore metaphorically, it seems like the neuron is consulting each of the inputs, asking for their opinion, and then making a decision by attaching different amounts of significance to each opinion.\nThe Structure should remind you of Linear Regression !\n\nSo how does it work? Consider the interactive diagram below:\n\n\n\nThe coordinate axes are as shown X, Y, and Z\nThe grey and yellow points are the data we wish to classify into two categories, unsurprisingly “yellow” and grey”.\nThe Weight vector line is a vector of all the weights in the Perceptron.\nNow, as per the point-normal form of an n-dimensional plane, the multiplication of the input data with the weight vector is like taking a vector dot product ( aka inner product) ! And: every point on the plane has a dot product of ZERO. See the purple vector which is normal to the Weight vector: its dot product with the Weight vector is zero.\nData points that are off this “normal plane” in either direction (above and below) will have dot-products which will be positive or negative depending upon the direction!\nHence we can use the dot-product POLARITY to decide if a point is above or below the plane defined by the Weight vector. Which is what is done in the threshold-based activation!\nThe bias \\(b\\) defines the POSITION of the plane; and the Weights define the direction. Together, they classify the points based on the Equation 1.\nTry to move the slider to get an intuition of how the plane moves with the bias. Clearly, the bias is very influential in deciding the POLARITY of the dot-products. When it aligns with the purple vector (\\(dot product = 0\\)), it works best.\nWhy “Linear”?\nWhy are (almost) all operations linear operations in a NN?\n\nWe said that the weighted sums are a linear operation, but why is this so?\nWe wish to be able to set-up analytic functions for performance of the NN, and be able to differentiate them to be able to optimize them.\nNon-linear blocks, such as threshold blocks/signum-function based slicers are not differentiable and we are unable to set up such analysis.\nNote the title of this reference.\nWhy is there a Bias input?\n\nWe want the weighted sum of the inputs to mean something significant, before we accept it.\nThe bias is subtracted from the weighted sum of inputs, and the bias input could also (notionally) have a weight.\nThe bias is like a threshold which the weighted sum has to exceed; if it does, the neuron is said to fire.\n\nSo with all that vocabulary, we might want to watch this longish video by the great Dan Shiffman:",
    "crumbs": [
      "Math Models for Creative Coders",
      "AI",
      "The Perceptron"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/20-Perceptron/index.html#perceptrons-in-code",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/20-Perceptron/index.html#perceptrons-in-code",
    "title": "The Perceptron",
    "section": "Perceptrons in Code",
    "text": "Perceptrons in Code\n\n\n R\n p5.js\n\n\n\nLet us try a simple single layer NN in R. We will use the R package neuralnet.\n\nShow the Code# Load the package\n# library(neuralnet)\n\n# Use iris\n# Create Training and Testing Datasets\ndf_train &lt;- iris %&gt;% slice_sample(n = 100)\ndf_test &lt;- iris %&gt;% anti_join(df_train)\nhead(iris)\n\n\n  \n\n\nShow the Code# Create a simle Neural Net\nnn &lt;- neuralnet(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width,\n  data = df_train,\n  hidden = 0,\n  # act.fct = \"logistic\", # Sigmoid\n  linear.output = TRUE\n) # TRUE to ignore activation function\n\n# str(nn)\n\n# Plot\nplot(nn)\n\n# Predictions\n# Predict &lt;- compute(nn, df_test)\n# Predict\n# cat(\"Predicted values:\\n\")\n# print(Predict$net.result)\n#\n# probability &lt;- Predict$net.result\n# pred &lt;- ifelse(probability &gt; 0.5, 1, 0)\n# cat(\"Result in binary values:\\n\")\n# pred %&gt;% as_tibble()\n\n\n\n\n\nTo Be Written Up.",
    "crumbs": [
      "Math Models for Creative Coders",
      "AI",
      "The Perceptron"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/20-Perceptron/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/20-Perceptron/index.html#references",
    "title": "The Perceptron",
    "section": "References",
    "text": "References\n\nThe Neural Network Zoo - The Asimov Institute. http://www.asimovinstitute.org/neural-network-zoo/\n\nIt’s just a linear model: neural networks edition. https://lucy.shinyapps.io/neural-net-linear/\n\nNeural Network Playground. https://playground.tensorflow.org/\n\nRohit Patel (20 Oct 2024). Understanding LLMs from Scratch Using Middle School Math: A self-contained, full explanation to inner workings of an LLM. https://towardsdatascience.com/understanding-llms-from-scratch-using-middle-school-math-e602d27ec876\n\nMachine Learning Tokyo: Interactive Tools for ML/DL, and Math. https://github.com/Machine-Learning-Tokyo/Interactive_Tool\n\n\nAnyone Can Learn AI Using This Blog. https://colab.research.google.com/drive/1g5fj7W6QMER4-03jtou7k1t7zMVE9TVt#scrollTo=V8Vq_6Q3zivl\n\nNeural Networks Visual with vcubingx\n\nPart 1. https://youtu.be/UOvPeC8WOt8\n\nPart 2. https://www.youtube.com/watch?v=-at7SLoVK_I\n\n\n\nPractical Deep Learning for Coders: An Online Free Course.https://course.fast.ai\n\n\nText Books\n\nMichael Nielsen. Neural Networks and Deep Learning, a free online book. http://neuralnetworksanddeeplearning.com/index.html\n\nSimone Scardapane. (2024) Alice’s Adventures in a differentiable Wonderland.https://www.sscardapane.it/alice-book/\n\nUsing R for DL\n\nDavid Selby (9 January 2018). Tea and Stats Blog. Building a neural network from scratch in R. https://selbydavid.com/2018/01/09/neural-network/\n\n\ntorch for R: An open source machine learning framework based on PyTorch. https://torch.mlverse.org\n\nAkshaj Verma. (2020-07-24). Building A Neural Net from Scratch Using R - Part 1 and Part 2. https://rviews.rstudio.com/2020/07/20/shallow-neural-net-from-scratch-using-r-part-1/ and https://rviews.rstudio.com/2020/07/24/building-a-neural-net-from-scratch-using-r-part-2/\n\nMaths\n\nParr and Howard (2018). The Matrix Calculus You Need for Deep Learning.https://arxiv.org/abs/1802.01528\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\nneuralnet\n1.44.2\n@neuralnet",
    "crumbs": [
      "Math Models for Creative Coders",
      "AI",
      "The Perceptron"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/200-LLMs/index.html",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/200-LLMs/index.html",
    "title": "Large Language Models",
    "section": "",
    "text": "Frank Rosenblatt’s Perceptron\nDeep Learning Networks\n\n\nInput Layers\nOutput Layers\nHidden Layers\nActivation\n\n\nAdaptation and Training\n\n\nBackpropagation\nError Functions and Surfaces\n\n\nWorking\n\n\n“Repeated Weighted Averaging with Thresholding”\nHow does that end up “learning”? Is there an intuitive explanation?"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/200-LLMs/index.html#inspiration",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/200-LLMs/index.html#inspiration",
    "title": "Large Language Models",
    "section": "",
    "text": "Frank Rosenblatt’s Perceptron\nDeep Learning Networks\n\n\nInput Layers\nOutput Layers\nHidden Layers\nActivation\n\n\nAdaptation and Training\n\n\nBackpropagation\nError Functions and Surfaces\n\n\nWorking\n\n\n“Repeated Weighted Averaging with Thresholding”\nHow does that end up “learning”? Is there an intuitive explanation?"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/200-LLMs/index.html#neural-nets-in-code",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/200-LLMs/index.html#neural-nets-in-code",
    "title": "Large Language Models",
    "section": "Neural Nets in Code",
    "text": "Neural Nets in Code\n\n\nUsing p5.js\nUsing R\n\n\n\n\n\n\nUsing torch."
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/200-LLMs/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/200-LLMs/index.html#references",
    "title": "Large Language Models",
    "section": "References",
    "text": "References\n\nThe Neural Network Zoo - The Asimov Institute. http://www.asimovinstitute.org/neural-network-zoo/\nIt’s just a linear model: neural networks edition. https://lucy.shinyapps.io/neural-net-linear/\nNeural Network Playground. https://playground.tensorflow.org/\nRohit Patel (20 Oct 2024). Understanding LLMs from Scratch Using Middle School Math: A self-contained, full explanation to inner workings of an LLM. https://towardsdatascience.com/understanding-llms-from-scratch-using-middle-school-math-e602d27ec876\nMachine Learning Tokyo: Interactive Tools for ML/DL, and Math. https://github.com/Machine-Learning-Tokyo/Interactive_Tool\nAnyone Can Learn AI Using This Blog. https://colab.research.google.com/drive/1g5fj7W6QMER4-03jtou7k1t7zMVE9TVt#scrollTo=V8Vq_6Q3zivl\nNeural Networks Visual with vcubingx\n\n\nPart 1. https://youtu.be/UOvPeC8WOt8\nPart 2. https://www.youtube.com/watch?v=-at7SLoVK_I\n\n\nPractical Deep Learning for Coders: An Online Free Course.https://course.fast.ai\n\n\nText Books\n\nMichael Nielsen. Neural Networks and Deep Learning, a free online book. http://neuralnetworksanddeeplearning.com/index.html\nSimone Scardapane. (2024) Alice’s Adventures in a differentiable Wonderland. https://www.sscardapane.it/alice-book/\nUsing R for DL\n\nDavid Selby (9 January 2018). Tea and Stats Blog. Building a neural network from scratch in R. https://selbydavid.com/2018/01/09/neural-network/\ntorch for R: An open source machine learning framework based on PyTorch. https://torch.mlverse.org\nAkshaj Verma. (2020-07-24). Building A Neural Net from Scratch Using R - Part 1 and Part 2. https://rviews.rstudio.com/2020/07/20/shallow-neural-net-from-scratch-using-r-part-1/ and https://rviews.rstudio.com/2020/07/24/building-a-neural-net-from-scratch-using-r-part-2/\nMaths\n\nParr and Howard (2018). The Matrix Calculus You Need for Deep Learning.https://arxiv.org/abs/1802.01528"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/45-Uncertainty/30-Agents/index.html",
    "href": "content/courses/MathModelsDesign/Modules/45-Uncertainty/30-Agents/index.html",
    "title": "Working with Agents",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/45-Uncertainty/10-Chance/index.html",
    "href": "content/courses/MathModelsDesign/Modules/45-Uncertainty/10-Chance/index.html",
    "title": "Working with Chance",
    "section": "",
    "text": "Topics that may be covered here:\n\n\nMonte Carlo Simulations\n\nBayesian Thinking"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/45-Uncertainty/10-Chance/index.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/45-Uncertainty/10-Chance/index.html#introduction",
    "title": "Working with Chance",
    "section": "",
    "text": "Topics that may be covered here:\n\n\nMonte Carlo Simulations\n\nBayesian Thinking"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/45-Uncertainty/10-Chance/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/45-Uncertainty/10-Chance/index.html#references",
    "title": "Working with Chance",
    "section": "References",
    "text": "References\n\nhttps://californiaglobe.com/fr/the-story-of-arnold-schwarzeneggers-infamous-hidden-expletive-veto/\n\nPhilip B. Stark (2010). Null and Vetoed: “Chance Coincidence”?, Chance, November 2010, v23(4), 43–46.). https://www.stat.berkeley.edu/~stark/Preprints/acrosticVeto09.htm\nLo Bello, A. (1991). Ask Marilyn: The Mathematical Controversy in Parade Magazine. The Mathematical Gazette, 75(473), 275–277. https://doi.org/10.2307/3619484.PDF."
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/45-Uncertainty/40-Time/index.html",
    "href": "content/courses/MathModelsDesign/Modules/45-Uncertainty/40-Time/index.html",
    "title": "Working with Time",
    "section": "",
    "text": "Topics that may be covered here: - Seasons and Trends - Forecasting - Queues and Queuing Theory ( R package “simmer”)"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/45-Uncertainty/40-Time/index.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/45-Uncertainty/40-Time/index.html#introduction",
    "title": "Working with Time",
    "section": "",
    "text": "Topics that may be covered here: - Seasons and Trends - Forecasting - Queues and Queuing Theory ( R package “simmer”)"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/20-MakingNoise/index.html",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/20-MakingNoise/index.html",
    "title": "\n Making Noise Predictably",
    "section": "",
    "text": "TRON the movie",
    "crumbs": [
      "Math Models for Creative Coders",
      "Media",
      "<iconify-icon icon=\"arcticons:noise-reducer\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Making Noise Predictably"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/20-MakingNoise/index.html#what-noise-based-outcomes-will-we-see-today",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/20-MakingNoise/index.html#what-noise-based-outcomes-will-we-see-today",
    "title": "\n Making Noise Predictably",
    "section": "\n What noise based outcomes will we see today?",
    "text": "What noise based outcomes will we see today?\nWe will understand the basics of procedural noise generation: generating random noise-like numbers that allow us to model and create very realistic-looking textures, such as wood, fire, marble, terrain, mountains, and clouds.\nLet us quickly see this intro to Perlin Noise:",
    "crumbs": [
      "Math Models for Creative Coders",
      "Media",
      "<iconify-icon icon=\"arcticons:noise-reducer\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Making Noise Predictably"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/20-MakingNoise/index.html#inspiration",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/20-MakingNoise/index.html#inspiration",
    "title": "\n Making Noise Predictably",
    "section": "\n Inspiration",
    "text": "Inspiration\n\n\n\n\n\n\nArch\n\n\n\n\n\nCat Fur Up Close\n\n\n\n\n\nFigure 1: Perlin Noise based Textures",
    "crumbs": [
      "Math Models for Creative Coders",
      "Media",
      "<iconify-icon icon=\"arcticons:noise-reducer\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Making Noise Predictably"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/20-MakingNoise/index.html#what-is-perlin-noise",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/20-MakingNoise/index.html#what-is-perlin-noise",
    "title": "\n Making Noise Predictably",
    "section": "\n What is Perlin Noise?",
    "text": "What is Perlin Noise?\nOk, this is going to be a long explanation!!!\nA. Inner Product Computation\nLet us start by dividing up 2D space ( for now!!) into square-shaped cells. At each vertex we randomly place a unit gradient vector labelled \\(r_{i}\\) that points in a random direction. See the figure below:\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nA perhaps more evocative picture may be this representation of an event from the Mahabharata: \n\n\nWe wish to calculate the Perlin Noise amount at any point of interest inside the cell.\n\nWe draw difference vectors to the point from each of the 4 vertices.\nWe compute the vector dot product with each of the \\(r_{i}\\) and the above difference vectors. ( 4 dot products )\nThese are shown in the text print at the side of the figure.\n\n\n\n\n\n\n\nNoteDot Products are Scalars with Polarity\n\n\n\nNote how the 4 dot products change as you move the mouse/touchpad. This changes the 4 gradient vectors and hence the scalar dot products change in amplitude and polarity.\nIn a typical Perlin Noise implementation, the gradient vectors are fixed after an initial setup. So each gradient vector generates a range of dot-product values as the point of interest moves within the cell.\n\n\nB. Interpolation of Dot Product values\nWith the 4 scalar dot products, we are now ready to compute the Perlin Noise value at the point of interest. There are several ways of doing this:\n\nSimply take the average\nTake a weighted average, with fixed weights.\nUse a weighting/interpolating function: The closer a point of interest is to one or other of the cell vertices, the higher is the contribution of the corresponding dot-product.\n\nThe third approach is the one embedded within (all?) Perlin Noise implementations. The interpolating function is:\n\\[\nf(t) = 6t^5-15t^4+10t^3\n\\tag{1}\\]\nand looks like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteInterpolation Function \\(f(t)\\) has smooth ends\n\n\n\nBoth \\(\\frac{df(t)}{dt}\\) and \\(\\frac{d^2f(t)}{dt^2}\\) are continuous at the ends of the range of the function (t = 0 and t = 1).\n\\[\n\\begin{array}{lcl}f'(t) & = & \\ \\frac{d}{dt}[6*t^5 - 15*t^4 + 10*t^3]\\\\\n& = & 30 * (t^4 - 2 * t^3 + t^2)\\\\\n& = & 0  \\ \\text{@ t = 0 and t = 1}\n\\end{array}\n\\]\n\\[\n\\begin{array}{lcl}f''(t) & = & \\ \\frac{d^2}{dt^2}[6*t^5 - 15*t^4 + 10*t^3]\\\\\n& = &60 * (2 * t^3 - 3 * t^2 + t)\\\\\n& = & 0  \\ \\text{@ t = 0 and t = 1}\n\\end{array}\n\\] This ensures that there are not sudden changes in the noise function near about the vertices.\n\n\nD. Fractal Overlay and Combining\nNow that we have one grid full of a layer of noise generated by weighted dot-products, we can appreciate one more thing: we can overlay the space with several layers of such noise values. Why would this be a good idea?\nThis multiple layer overlay creates a very natural-looking fractal-ness in the resulting noise function. Most natural looking shapes like landscapes, mountains, vegetables, flames…all have this self-similar structure where when one zooms in, the magnified function looks pretty much like the un-zoomed version!!\nSo how we create and merge overlays? We create several more-closely-spaced grids overlaid on the first one, and generate noise in the same way. These layers of noise-s are scaled by a factor (Usually \\(\\Large{\\frac{1}{2^n}}\\)), where \\(n\\) is the “order” of the layer. Each new finely-spaced layer generates similar-looking noise functions, which are combined with smaller and smaller weights to achieve that final polished fractal look of Perlin Noise.\nWe will explore this fractality with code. For now, here is Ken Perlin’s own explanation from 1999:\n\n\n“The outline of my algorithm to create noise is very simple. Given an input point P, look at each of the surrounding grid points. In two dimensions there will be four surrounding grid points; in three dimensions there will be eight. In n dimensions, a point will have 2n surrounding grid points.For each surrounding grid point Q, choose a pseudo-random gradient vector G. It is very important that for any particular grid point you always choose the same gradient vector. Compute the inner product G . (P-Q). This will give the value at P of the linear function with gradient G which is zero at grid point Q. Now you have 2n of these values. Interpolate between them down to your point, using an S-shaped cross-fade curve (eg: 3t2-2t3) to weight the interpolant in each dimension. This step will require computing n S-curves, followed by 2n-1 linear interpolations.”\n— Ken Perlin",
    "crumbs": [
      "Math Models for Creative Coders",
      "Media",
      "<iconify-icon icon=\"arcticons:noise-reducer\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Making Noise Predictably"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/20-MakingNoise/index.html#creating-textures-and-waveforms-with-perlin-noise",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/20-MakingNoise/index.html#creating-textures-and-waveforms-with-perlin-noise",
    "title": "\n Making Noise Predictably",
    "section": "\n Creating Textures and Waveforms with Perlin Noise",
    "text": "Creating Textures and Waveforms with Perlin Noise\n\n\nUsing p5.js\nUsing R\n\n\n\nHere is a landscape generated using Perlin Noise:\n\n\n\n\n\n\nThe ambient package allows us to create a variety of noise patterns, including Perlin Noise. The commands are: gen_perlin() and noise_perlin(), whose arguments are:\n\n\ndim: The dimensions (height, width, (and depth)) of the noise to be generated. The length determines the dimensionality of the noise.\n\nfrequency: Determines the granularity of the features in the noise.\n\ninterpolator:How should values between sampled points be calculated? Either ‘linear’, ‘hermite’, or ‘quintic’ (default), ranging from lowest to highest quality.\n\nfractal: The fractal type to use. Either ‘none’, ‘fbm’ (default), ‘billow’, or ‘rigid-multi’. It is suggested that you experiment with the different types to get a feel for how they behaves.\n\noctaves: The number of noise layers used to create the fractal noise. Ignored if fractal = ‘none’. Defaults to 3.\n\nlacunarity: The frequency multiplier between successive noise layers when building fractal noise. Ignored if fractal = ‘none’. Defaults to 2.\n\ngain: The relative strength between successive noise layers when building fractal noise. Ignored if fractal = ‘none’. Defaults to 0.5.\n\npertubation: The perturbation to use. Either ‘none’ (default), ‘normal’, or ‘fractal’. Defines the displacement (warping) of the noise, with ‘normal’ giving a smooth warping and ‘fractal’ giving a more erratic warping.\n\npertubation_amplitude: The maximal perturbation distance from the origin. Ignored if pertubation = ‘none’. Defaults to 1.\n\nx, y, z: Coordinates to get noise value from\n\nseed: The seed to use for the noise. If NULL a random seed will be used:\n\nnoise2 &lt;- noise_perlin(\n  dim = c(400, 400), # height/width\n  frequency = 0.01, # Lower = less granular, more organic\n  interpolator = \"quintic\", #' linear', 'hermite', or 'quintic'\n  fractal = \"fbm\", # Try \"billow\" , \"rigid-multi\"\n  octaves = 5,\n  lacunarity = 2,\n  gain = 0.8, # Default = 0.5 giving 1/2^n scaling\n  pertubation = \"none\", # Note the incorrect spelling\n  pertubation_amplitude = 1 # Note the incorrect spelling\n)\n## generates a matrix\nnoise2 %&gt;% as_tibble()\n# Plot the matrix\nplot(as.raster(normalise(noise2)))\n\n\n\n\n  \n\n\n\n\n\n\n\n# Using the generator\ngrid &lt;- long_grid(seq(1, 10, length.out = 1000), seq(1, 10, length.out = 1000))\ngrid$noise &lt;- gen_perlin(grid$x, grid$y,\n  octaves = 5,\n  frequency = 1.2\n)\nplot(grid, noise)\n##\ngrid %&gt;%\n  gf_point(y ~ x,\n    colour = ~noise,\n    size = 0.01, show.legend = F\n  ) %&gt;%\n  gf_refine(\n    scale_color_gradient(\n      low = \"orangered\",\n      high = \"black\"\n    ),\n    coord_fixed()\n  ) %&gt;%\n  gf_theme(theme_void())\n\n\n\n\n\n\n\n\n\n\nIt seems the ambient package cannot generate 1D-Perlin noise, so we cannot generate say time waveforms based on this idea. p5.js of course can do 1D.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Media",
      "<iconify-icon icon=\"arcticons:noise-reducer\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Making Noise Predictably"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/20-MakingNoise/index.html#videos",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/20-MakingNoise/index.html#videos",
    "title": "\n Making Noise Predictably",
    "section": "Videos",
    "text": "Videos\nOf course there are videos by Dan Shiffman on the topic of Perlin Noise:\n\nPerlin Noise in p5.js. https://www.youtube.com/watch?v=Qf4dIN99e2w&list=PLRqwX-V7Uu6ZV4yEcW3uDwOgGXKUUsPOM&index=3&pp=iAQB\n\nPerlin Noise in 2D. https://www.youtube.com/watch?v=ikwNrFvnL3g&list=PLRqwX-V7Uu6ZV4yEcW3uDwOgGXKUUsPOM&index=6&pp=iAQB\n\nPerlin Noise in Detail. https://www.youtube.com/watch?v=D1BBj2VaBl4&list=PLRqwX-V7Uu6ZV4yEcW3uDwOgGXKUUsPOM&index=7&pp=iAQB\n\nGraphing Perlin Noise. https://www.youtube.com/watch?v=y7sgcFhk6ZM&list=PLRqwX-V7Uu6ZV4yEcW3uDwOgGXKUUsPOM&index=5&pp=iAQB",
    "crumbs": [
      "Math Models for Creative Coders",
      "Media",
      "<iconify-icon icon=\"arcticons:noise-reducer\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Making Noise Predictably"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/20-MakingNoise/index.html#wait-but-why",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/20-MakingNoise/index.html#wait-but-why",
    "title": "\n Making Noise Predictably",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nGenerating random waveforms and textures is an important part of Creative Computational projects.\nThese ideas are used in landscape, texture and sound generation.\n“Normal” random noise is too jagged to have the natural look that we would like\nPerlin Noise creates both a smoothness and a fine-grain random structure in an arbitrary number of dimensions.\nThis ends up looking more “organic” and “natural”. Think cats.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Media",
      "<iconify-icon icon=\"arcticons:noise-reducer\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Making Noise Predictably"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/20-MakingNoise/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/20-MakingNoise/index.html#references",
    "title": "\n Making Noise Predictably",
    "section": "\n References",
    "text": "References\n\nKen Perlin.(1999). Making Noise. https://web.archive.org/web/20151221035155/http://www.noisemachine.com/talk1/index.html. Based on a talk presented at GDCHardCore on Dec 9, 1999.\nhttps://www.scratchapixel.com/lessons/procedural-generation-virtual-worlds/perlin-noise-part-2/perlin-noise.html\nhttps://www.khanacademy.org/computing/computer-programming/programming-natural-simulations/programming-noise/a/perlin-noise\nhttps://adrianb.io/2014/08/09/perlinnoise.html\nhttps://www.arendpeter.com/Perlin_Noise.html\nhttps://mzucker.github.io/html/perlin-noise-math-faq.html\nhttps://betterexplained.com/articles/vector-calculus-understanding-the-dot-product/\nGorilla Sun Blog. Perlin Noise. https://www.gorillasun.de/tag/perlin-noise/\n\nThe ambient package in R: https://ambient.data-imaginist.com/\n\n\nTextbooks\n\nPatricio Gonzalez Vivo & Jen Lowe. The Book of Shaders. https://thebookofshaders.com/11/\n\n\n\n\n R Package Citations\nResources\n\nhttps://opengameart.org/content/700-noise-textures\nhttps://github.com/sighack/perlin-noise-fields\n\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nambient\n1.0.2\nPedersen and Peck (2022)\n\n\nmosaicCalc\n0.6.4\nKaplan, Pruim, and Horton (2024)\n\n\nplot3D\n1.4.1\nSoetaert (2024)\n\n\n\n\n\n\nKaplan, Daniel T., Randall Pruim, and Nicholas J. Horton. 2024. mosaicCalc: R-Language Based Calculus Operations for Teaching. https://doi.org/10.32614/CRAN.package.mosaicCalc.\n\n\nPedersen, Thomas Lin, and Jordan Peck. 2022. ambient: A Generator of Multidimensional Noise. https://doi.org/10.32614/CRAN.package.ambient.\n\n\nSoetaert, Karline. 2024. plot3D: Plotting Multi-Dimensional Data. https://doi.org/10.32614/CRAN.package.plot3D.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Media",
      "<iconify-icon icon=\"arcticons:noise-reducer\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Making Noise Predictably"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/12-AdditiveSynth/index.html",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/12-AdditiveSynth/index.html",
    "title": "Additive Sound Synthesis",
    "section": "",
    "text": "So we understand the Fourier Transform: we can express any waveform as a sum of sinusoids that are appropriately weighted and are at discrete multiples of a chosen “fundamental frequency”.\nHow do we use these ideas to synthesize sound?",
    "crumbs": [
      "Math Models for Creative Coders",
      "Media",
      "Additive Sound Synthesis"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/12-AdditiveSynth/index.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/12-AdditiveSynth/index.html#introduction",
    "title": "Additive Sound Synthesis",
    "section": "",
    "text": "So we understand the Fourier Transform: we can express any waveform as a sum of sinusoids that are appropriately weighted and are at discrete multiples of a chosen “fundamental frequency”.\nHow do we use these ideas to synthesize sound?",
    "crumbs": [
      "Math Models for Creative Coders",
      "Media",
      "Additive Sound Synthesis"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/12-AdditiveSynth/index.html#inspiration",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/12-AdditiveSynth/index.html#inspiration",
    "title": "Additive Sound Synthesis",
    "section": "\n Inspiration",
    "text": "Inspiration\nTO BE ADDED (sic!)",
    "crumbs": [
      "Math Models for Creative Coders",
      "Media",
      "Additive Sound Synthesis"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/12-AdditiveSynth/index.html#what-is-additive-synthesis",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/12-AdditiveSynth/index.html#what-is-additive-synthesis",
    "title": "Additive Sound Synthesis",
    "section": "What is Additive Synthesis?",
    "text": "What is Additive Synthesis?\nFirst we need to get used to the idea of an oscillator.\nAn oscillator is a source: it generates waveforms that we perceive as sound. Let us play with a few oscillator types here:\nhttps://musiclab.chromeexperiments.com/Oscillators/\nEach of these waveforms, by the Fourier series, is the sum of an ( infinite) number of sine wave outputs.\nIn Fourier series, we normally use just sine wave oscillators, and use many of them to add up to obtain the wave form we need. Now let us hear from Mr Shiffman again:",
    "crumbs": [
      "Math Models for Creative Coders",
      "Media",
      "Additive Sound Synthesis"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/12-AdditiveSynth/index.html#the-math-of-waveform-addition",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/12-AdditiveSynth/index.html#the-math-of-waveform-addition",
    "title": "Additive Sound Synthesis",
    "section": "The Math of Waveform Addition",
    "text": "The Math of Waveform Addition\nIn general, we can write a sum of sine/cos waves as:\n\\[\nf(\\theta) = \\frac{1}{a_0} + \\sum_{k=0}^{\\infty} a_k*sin(k\\theta) + b_k*cos(k\\theta)\n\\tag{1}\\]\n\\(f\\) is the desired time-waveform, the \\(\\theta = 2\\pi\\times fundamental~frequency \\times t\\), and the \\(a_k\\) and \\(b_k\\) are weights for the individual components that are to be designed.\nHow does this look like? Mr. Shiffman again:\n\nNow let us see how we can design something using the Additive Method.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Media",
      "Additive Sound Synthesis"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/12-AdditiveSynth/index.html#design-principles-for-additive-synthesis",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/12-AdditiveSynth/index.html#design-principles-for-additive-synthesis",
    "title": "Additive Sound Synthesis",
    "section": "Design Principles for Additive Synthesis",
    "text": "Design Principles for Additive Synthesis\nHow do we do this with intent? We will follow the development in Farnell and Risset and Mathews, (Risset and Mathews 1969) and Moorer.\n\nThe idea is to take an original sound, analyze that using the Fourier Series, and then use those coefficients to synthesize the sound with code.\nThe coefficients, or parameters, need to be manipulated and transformed with time, in order for the synthesized sound to have a “live” feel.\nThe number of such parameters and their control over time could pose a formidable data management challenge. This leads to the idea of data reduction in order to have a manageable number of these, and generate the sound in its essentials.\nOne essential part of this is to use envelopes around the amplitudes of several sine waves, what is called the ADSR method. This could also lead to several oscillators being turned on or off based on need.\nSo one needs to break down the sound into “principal components” that are harmonically related ( as with the Fourier series) and then fill in inharmonic tones using additional oscillators.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Media",
      "Additive Sound Synthesis"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/12-AdditiveSynth/index.html#what-is-adsr",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/12-AdditiveSynth/index.html#what-is-adsr",
    "title": "Additive Sound Synthesis",
    "section": "What is ADSR?",
    "text": "What is ADSR?\nADSR stands for “Attack Decay Sustain Release”. These related to the way a note of music varies over time in a typical piece of music.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Media",
      "Additive Sound Synthesis"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/12-AdditiveSynth/index.html#additive-synthesis-with-code",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/12-AdditiveSynth/index.html#additive-synthesis-with-code",
    "title": "Additive Sound Synthesis",
    "section": "Additive Synthesis with Code",
    "text": "Additive Synthesis with Code\n\n\nUsing p5.js\nUsing R\n\n\n\n\n\n\n\nShow the Codemusic &lt;-\n  Music() +\n  Meter(4, 4) +\n  Line(c(\"C5\", \"D5\", \"E5\", \"F5\"))\n\nshow(music)",
    "crumbs": [
      "Math Models for Creative Coders",
      "Media",
      "Additive Sound Synthesis"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/12-AdditiveSynth/index.html#wait-but-why",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/12-AdditiveSynth/index.html#wait-but-why",
    "title": "Additive Sound Synthesis",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\nTo be Written Up.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Media",
      "Additive Sound Synthesis"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/12-AdditiveSynth/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/12-AdditiveSynth/index.html#references",
    "title": "Additive Sound Synthesis",
    "section": "\n References",
    "text": "References\n\nJames Moorer. (Nov 1976) The Synthesis of Complex Audio Spectra by Means of Discrete Summation Formulae. Journal of the Audio Society. PDF\n\nJean-Claude Risset, Max V. Matthews. (Feb 1969). Analysis of Musical Instrument Tones. Physics Today. https://sci-hub.se/https://doi.org/10.1063/1.3035399\n\np5.Sound Tutorial.https://pdm.lsupathways.org/6_resources/7_soundandmusic/p5.sound/\n\nSound in p5.js Playlist. https://www.youtube.com/playlist?list=PLRqwX-V7Uu6aFcVjlDAkkGIixw70s7jpW\n\n\nSounds with Tone.js. https://pdm.lsupathways.org/3_audio/\n\nMister Bomb. p5.Sound project tutorials https://www.youtube.com/playlist?list=PLIsdHp2z9wFl7A1wWb2VmQUUojEGsKELE\n\nhttps://www.cs.cmu.edu/~tcortina/15104-f20/lectures/24-MoreSound.pdf\n\nR package gm: the grammar of Music. https://cran.r-project.org/web/packages/gm/vignettes/gm.html\n\nPhil Burk,Larry Polansky, Douglas Repetto, Mary Roberts Dan Rockmore. Music and Computers: A Theoretical and Historical Approach https://musicandcomputersbook.com\n\nJulius O. Smith. PHYSICAL AUDIO SIGNAL PROCESSING FOR VIRTUAL MUSICAL INSTRUMENTS AND AUDIO EFFECTS https://ccrma.stanford.edu/~jos/pasp/pasp.html\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\ngm\n2.0.0\nMao (2024)\n\n\n\n\n\nMao, Renfei. 2024. gm: Create Music with Ease. https://doi.org/10.32614/CRAN.package.gm.\n\n\nRisset, Jean-Claude, and Max V. Mathews. 1969. “Analysis of Musical-Instrument Tones.” Physics Today 22 (2): 23–30. https://doi.org/10.1063/1.3035399.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Media",
      "Additive Sound Synthesis"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/99-KarplusStrong/index.html",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/99-KarplusStrong/index.html",
    "title": "The Karplus-Strong Guitar Algorithm",
    "section": "",
    "text": "Here is the GUI for a guitar created in JavaScript (not p5.js).",
    "crumbs": [
      "Math Models for Creative Coders",
      "Media",
      "The Karplus-Strong Guitar Algorithm"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/99-KarplusStrong/index.html#inspiration",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/99-KarplusStrong/index.html#inspiration",
    "title": "The Karplus-Strong Guitar Algorithm",
    "section": "",
    "text": "Here is the GUI for a guitar created in JavaScript (not p5.js).",
    "crumbs": [
      "Math Models for Creative Coders",
      "Media",
      "The Karplus-Strong Guitar Algorithm"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/99-KarplusStrong/index.html#what-is-the-karplus-strong-algorithm",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/99-KarplusStrong/index.html#what-is-the-karplus-strong-algorithm",
    "title": "The Karplus-Strong Guitar Algorithm",
    "section": "What is the Karplus-Strong Algorithm?",
    "text": "What is the Karplus-Strong Algorithm?",
    "crumbs": [
      "Math Models for Creative Coders",
      "Media",
      "The Karplus-Strong Guitar Algorithm"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/99-KarplusStrong/index.html#what-is-a-delay-line",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/99-KarplusStrong/index.html#what-is-a-delay-line",
    "title": "The Karplus-Strong Guitar Algorithm",
    "section": "What is a Delay Line?",
    "text": "What is a Delay Line?",
    "crumbs": [
      "Math Models for Creative Coders",
      "Media",
      "The Karplus-Strong Guitar Algorithm"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/99-KarplusStrong/index.html#what-is-feedback-oscillation",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/99-KarplusStrong/index.html#what-is-feedback-oscillation",
    "title": "The Karplus-Strong Guitar Algorithm",
    "section": "What is Feedback Oscillation?",
    "text": "What is Feedback Oscillation?",
    "crumbs": [
      "Math Models for Creative Coders",
      "Media",
      "The Karplus-Strong Guitar Algorithm"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/99-KarplusStrong/index.html#karplus-strong-guitar-in-code",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/99-KarplusStrong/index.html#karplus-strong-guitar-in-code",
    "title": "The Karplus-Strong Guitar Algorithm",
    "section": "Karplus-Strong Guitar in Code",
    "text": "Karplus-Strong Guitar in Code\nLet us code up our guitar and see how it works!\n\n\n p5.js\n R",
    "crumbs": [
      "Math Models for Creative Coders",
      "Media",
      "The Karplus-Strong Guitar Algorithm"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/99-KarplusStrong/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/99-KarplusStrong/index.html#references",
    "title": "The Karplus-Strong Guitar Algorithm",
    "section": "References",
    "text": "References\n\nKarplus, K., & Strong, A. (1983). Digital Synthesis of Plucked-String and Drum Timbres. Computer Music Journal, 7(2), 43. doi:10.2307/3680062. https://sci-hub.se/https://doi.org/10.2307/3680062\n\nJulius O. Smith. PHYSICAL AUDIO SIGNAL PROCESSING FOR VIRTUAL MUSICAL INSTRUMENTS AND AUDIO EFFECTS. https://ccrma.stanford.edu/~jos/pasp/pasp.html\n\n\nEuphonics: The Science of Musical Instruments.https://euphonics.org/about/\n\nKarplus-Strong Guitar in PureData (pD). https://youtu.be/xEpbMWpz65E?si=Mkz-eQ4UTMrYR3Fu\n\nAmid Fish.(May 2017). Karplus Strong String Synthesis. http://amid.fish/karplus-strong\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nambient\n1.0.2\nPedersen and Peck (2022)\n\n\nmosaicCalc\n0.6.4\nKaplan, Pruim, and Horton (2024)\n\n\nplot3D\n1.4.1\nSoetaert (2024)\n\n\n\n\n\n\nKaplan, Daniel T., Randall Pruim, and Nicholas J. Horton. 2024. mosaicCalc: R-Language Based Calculus Operations for Teaching. https://doi.org/10.32614/CRAN.package.mosaicCalc.\n\n\nPedersen, Thomas Lin, and Jordan Peck. 2022. ambient: A Generator of Multidimensional Noise. https://doi.org/10.32614/CRAN.package.ambient.\n\n\nSoetaert, Karline. 2024. plot3D: Plotting Multi-Dimensional Data. https://doi.org/10.32614/CRAN.package.plot3D.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Media",
      "The Karplus-Strong Guitar Algorithm"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/listing.html",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/listing.html",
    "title": "Media",
    "section": "",
    "text": "Title\n\n\n\nReading Time\n\n\n\n\n\n\n\n\nFourier Series\n\n\n11 min\n\n\n\n\n\n\nAdditive Sound Synthesis\n\n\n9 min\n\n\n\n\n\n\n Making Noise Predictably\n\n\n20 min\n\n\n\n\n\n\nThe Karplus-Strong Guitar Algorithm\n\n\n6 min\n\n\n\n\n\n\nNo matching items\n Back to top",
    "crumbs": [
      "Math Models for Creative Coders",
      "Media"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/listing.html#introduction",
    "href": "content/courses/MathModelsDesign/listing.html#introduction",
    "title": "Using matters of Life and Death Math and Code to Create Novel Experiences",
    "section": "Introduction",
    "text": "Introduction\nWe will study several Mathematical Models and apply them to Art and Design. The algorithms will be examined and then coded in p5.js / q5.js / p5play.js; however other open source tools (FOSS) and Javascript libraries may also be introduced as and when needed and as and when I learn and get excited about them.\nThis course stands on “three legs”: Maths, Code + Tech, and Artifacts, as shown below. As this course grows and hopefully becomes more un-popular ;-D, more and more of these modules below will become reality!\n\nMath Experiments with Code\n\n\nMaths\nCode + Tech\nArtifacts\n\n\n\n\nIterated FunctionsComplex NumbersRandom NumbersVector AlgebraPerlin Noise\np5.js / q5.js / p5play.jsShapesStack, Push and Pull, TranslateVectorsRecursionConstructors and OOP\nFractalsKolamsAngolan Sona PatternsL-Systems SymmetriesTextures\n\n\nLinear Systems  Impulse ResponsesConvolutionElectrical System Theory (poles; zeroes; resonance…)?DSP basics?AM/FM/PM Modulation?Fourier SeriesBessel Functions? Waves; Echoes ?Non-linearities; Saturation; Hysteresis ?\nAdding External Physics and Sound Librariesp5.soundCamera, Sound and MicHandphone based InteractionsOSP Protocol based interactions with external hardware over WiFi / BT\nSoundVideoTextCrowd-Sourced acts (jam session; flash-techno mob…)?Mouse - Touchpad Orchestra?\n\n\nNeural Net Basics Matrix AlgebraTrainingBackpropagationGradient DescentClassification, Regression, Clustering\nml5.js libraries\nMulti-Layer PerceptronsConvolutional Neural NetworksPose, Gesture, and Face Detection\n\n\nComplexity and Emergent Phenomena?\nInterfacing p5.js with Makey-Makey / Arduino / Raspberry Pi?Sensors?Working with other materials like sand, water, paper, wood, cloth, balloons ?“Action at a Distance” using say quantum entanglement IFTTT?\nFriendship Networks? Games?Public Space InstallationsSimulationsRemote Education with Gesture tracking and live Coding?“Literary” Events with Tech?\n\n\nI’ll\nBe\nBack",
    "crumbs": [
      "Math Models for Creative Coders"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/listing.html#references",
    "href": "content/courses/MathModelsDesign/listing.html#references",
    "title": "Using matters of Life and Death Math and Code to Create Novel Experiences",
    "section": "References",
    "text": "References\n\nCourse Abstract written by Arnold Schwarzenegger.\n\n\nGeneral\n\nBret Victor. Learnable Programming. https://worrydream.com/LearnableProgramming/\nMichael Nielsen.(February 2016). Toward an exploratory medium for mathematics. https://cognitivemedium.com/emm/emm.html\nRune Madsen. Programming Design Systems. https://programmingdesignsystems.com. A free digital book that teaches a practical introduction to the new foundations of graphic design.\nColah’s ( not Kolha’s 🦊 ) Blog. https://colah.github.io/https://colah.github.io/\n\n\n\np5.js, p5play.js, q5.js, and Processing\n\nGetting Started with p5.js https://p5js.org/tutorials/setting-up-your-environment/\nCoding Rainbow: Basic-est set of videos on p5.js &lt;https://youtube.com/playlist?list=PLglp04UYZK_PrN6xWo_nJ-8kzyXDyFUwi&si=BWSDVX9-Bt85KXdO&gt;\nCoding Train with Dan Shiffman:\n\nWebsite: https://thecodingtrain.com/\nGithub: https://github.com/CodingTrain/website-archive\n\np5.js Wiki. https://github.com/processing/p5.js/wiki\np5.js at CodeAcademy https://www.codecademy.com/learn/learn-p5js\np5.js at HappyCoding https://happycoding.io/tutorials/p5js/\nOpenProcessing https://openprocessing.org\nhttps://www.codecademy.com/content-items/5e5c0d2a7b20535fbe8aed05e739e027\np5play Game Engine https://p5play.org/ p5play is for creating interactive art and games with the Box2D physics engine.\nq5.js Home https://q5js.org/home/\n\n\n\nR-language-related Resources\n\nThomas Lin Pedersen:\n\nWebsite: https://www.data-imaginist.com/art\nWebsite: https://ambient.data-imaginist.com/index.html (R package ambient)\nGithub:\n\nAntonio Sánchez Chinchón:\n\nWebsite: https://fronkonstin.com\nGithub: https://github.com/aschinchon/abstractions\n\nDanielle Navarro’s Generative Art:\n\nWebsite: https://art-from-code.netlify.app\nGithub: https://github.com/arvindvenkatadri/art-from-code (forked by me)\nWebsite: https://art-from-code.netlify.app\n\nClaus Wilke:\n\nWebsite: https://clauswilke.com/art/\n\nGenerative Art by Katharina Brunner:\n\nGithub:https://github.com/cutterkom/generativeart\nWebpage:https://katharinabrunner.de/generativeart/\n\nWilliam Chase:\n\nWebsite: https://www.williamrchase.com/\nGithub: https://github.com/will-r-chase\nBlog Posts: https://www.williamrchase.com/writing/\n\nhttps://buttondown.email/willchase/archive/the-generative-art-dataviz-spectrum/\nhttps://www.williamrchase.com/post/strange-attractors-12-months-of-art-february/\nhttps://www.williamrchase.com/writing/2019-09-30-flow-fields-12-months-of-art-september/\nhttps://www.williamrchase.com/writing/2019-08-30-12-months-of-art-august/\nPoisson Disc sampling https://www.williamrchase.com/writing/2019-07-29-textues-and-geometric-shapes-12-months-of-art-july/ and the poissoned R package by @coolbutuseless\n\n\nMarcus Volz:\n\nWebsite: https://marcusvolz.com\nGithub: https://github.com/marcusvolz\n\nhttps://generative.substack.com/p/generative-art-and-r\nGenerative Art. https://paulvanderlaken.com/2020/05/02/generative-art-computer-design-painting/\nR-tistry with ggplot: https://www.bigbookofr.com/art.html#thinking-outside-the-grid---a-bare-bones-intro-to-rtistry-concepts-in-r-using-ggplot\nhttps://www.rforscience.com/scientific-computing.html\n\n\n\nDeep Learning and AI\n\nPractical Deep Learning for Coders. https://course.fast.ai/\nMichael Nielsen. http://neuralnetworksanddeeplearning.com/index.html\nIan Goodfellow and Yoshua Bengio and Aaron Courville. Deep Learning Book. https://www.deeplearningbook.org/\nDive into Deep Learning. Interactive deep learning book with code, math, and discussions. Implemented with PyTorch, NumPy/MXNet, JAX, and TensorFlowUsing Other Tools. https://www.d2l.ai/index.html\nSimon Scardapane. Alice’s Adventures in Differentiable Wonderland. https://www.sscardapane.it/assets/alice/Alice_book_volume_1.pdf\nFrançois Fleuret. The Little Book of Deep Learning. https://fleuret.org/public/lbdl.pdf\nFrank Rosenblatt. Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms. https://gwern.net/doc/ai/nn/1962-rosenblatt-principlesofneurodynamics.pdf\nWarren S. McCulloch, Walter Pitts (1943). A Logical Calculus of the Ideas of Immanent in Nervous Activity. BULLETIN OF MATHEMATICAL BIOPHYSICS VOLUME 5, 1943. &lt;https://www.aemea.org/math/McCulloch_Pitts_1943.pdfhttps://www.aemea.org/math/McCulloch_Pitts_1943.pdf\n\n\n\nOther Tools\n\nhttps://generatecoll.medium.com/how-i-used-excel-to-create-abstract-album-artwork-fee740d4414f\nRandom Digital Beauty. https://anaselk.com/p/generative-r/\nMaking Explanations (tools): https://explorabl.es/tools/\nUsing p5 in R. Yeah. https://alistaire.rbind.io/blog/p5-in-r/\ncreateCanvas Podcast. https://soundcloud.com/processingfoundation\nhttps://processingfoundation.org/education\nhttps://nannou.cc\nhttps://openframeworks.cc/\nhttps://libcinder.org\nSophia Crespo: (makes speculative biological creatures using neural networks)\n\nWebsite: https://sofiacrespo.com/\nWebsite: https://entangledothers.studio/\nGithub:\n\nR for Scientific Visualization. https://www.rforscience.com/visualisation.html\nR for Scientific Computing. https://www.rforscience.com/scientific-computing.html",
    "crumbs": [
      "Math Models for Creative Coders"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/listing.html#other-interesting-websites-and-works",
    "href": "content/courses/MathModelsDesign/listing.html#other-interesting-websites-and-works",
    "title": "Using matters of Life and Death Math and Code to Create Novel Experiences",
    "section": "Other Interesting Websites and Works",
    "text": "Other Interesting Websites and Works\n\nThe Book of Shaders by Patricio Gonzalez Vivo and Jen Lowe. https://thebookofshaders.com/\nScott Murray’s D3 Art page: https://scottmurray.org/\nInigo Quilez, Digital Artist: https://iquilezles.org/ (ShaderToy, GraphToy, and MadeThisThing)\nReddit Generative Art Forum. https://www.reddit.com/r/generative/\nAI tools for Journalists. https://journaliststoolbox.ai/ A vast list of tools for every purpose you can think of.",
    "crumbs": [
      "Math Models for Creative Coders"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/listing.html#creative-coding-courses-elsewhere",
    "href": "content/courses/MathModelsDesign/listing.html#creative-coding-courses-elsewhere",
    "title": "Using matters of Life and Death Math and Code to Create Novel Experiences",
    "section": "Creative Coding Courses Elsewhere",
    "text": "Creative Coding Courses Elsewhere\n\nAllison Parrish. https://creative-coding.decontextualize.com\nMatthew Bardin. https://pdm.lsupathways.org\nTim Cortina at Carnegie Mellon Univ2. https://www.cs.cmu.edu/~tcortina/15104-f20/lectures/",
    "crumbs": [
      "Math Models for Creative Coders"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/listing.html#learning-modules",
    "href": "content/courses/MathModelsDesign/listing.html#learning-modules",
    "title": "Using matters of Life and Death Math and Code to Create Novel Experiences",
    "section": "Learning Modules",
    "text": "Learning Modules",
    "crumbs": [
      "Math Models for Creative Coders"
    ]
  },
  {
    "objectID": "content/courses/listing.html",
    "href": "content/courses/listing.html",
    "title": "Teaching",
    "section": "",
    "text": "Here are the courses that I teach:\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nTitle\n\n\n\nSubtitle\n\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\nUsing matters of Life and Death Math and Code to Create Novel Experiences\n\n\n \n\n\nDec 31, 2022\n\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html",
    "title": "Fourier Series",
    "section": "",
    "text": "Can Circles do more for us than draw these lovely patterns? Can they give us an alphabet, a universal way of generating and representing many forms of interest? Can we treat them like a bunch of kitchen ingredients, that we throw into a recipe to conjure up new dishes that look different?",
    "crumbs": [
      "Math Models for Creative Coders",
      "Media",
      "Fourier Series"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#introduction",
    "title": "Fourier Series",
    "section": "",
    "text": "Can Circles do more for us than draw these lovely patterns? Can they give us an alphabet, a universal way of generating and representing many forms of interest? Can we treat them like a bunch of kitchen ingredients, that we throw into a recipe to conjure up new dishes that look different?",
    "crumbs": [
      "Math Models for Creative Coders",
      "Media",
      "Fourier Series"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#inspiration",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#inspiration",
    "title": "Fourier Series",
    "section": "\n Inspiration",
    "text": "Inspiration\nTake a look at these paintings:\n  \nAlso see: https://x.com/jagarikin/status/962449509782495232",
    "crumbs": [
      "Math Models for Creative Coders",
      "Media",
      "Fourier Series"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#what-is-the-fourier-series",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#what-is-the-fourier-series",
    "title": "Fourier Series",
    "section": "What is the Fourier Series?",
    "text": "What is the Fourier Series?\n\n\n\n\n\n\nImportant\n\n\n\nA Fourier Series is a way of composing/decomposing a complex waveform into a set of harmonically related sine Oscillations, which are summed up to create the original waveform.\n\n\nIn Circles, we saw how we could make symmetric patterns from rotating circles. We did not have a pattern in mind, except for the symmetry order. So, when we chose number of circles \\(M\\) and their complex amplitudes \\(a_j\\), \\(j={1..M}\\) relying on our (hopefully growing) intuition, we could systematically generate symmetric patterns based on the idea of rolling circles. By trial and error, we can design both the value of \\(M\\) and the values for \\(a_j\\), \\(j={1..M}\\). So far, so good.\nBut how about the other way around? What if we had a pattern in mind, and wanted to compute the circles, their number and amplitudes, that would generate that pattern? This is where the Fourier Series comes in.\nThe best way to form this intuition is to play some of the Wave Game that is available on the University of Colorado PHET Simulations website:",
    "crumbs": [
      "Math Models for Creative Coders",
      "Media",
      "Fourier Series"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#rolling-circles-and-the-fourier-series",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#rolling-circles-and-the-fourier-series",
    "title": "Fourier Series",
    "section": "Rolling Circles and the Fourier Series",
    "text": "Rolling Circles and the Fourier Series\nBy sliding the amplitudes of various sine Oscillators (whose number you could choose), you were hopefully able to visually create a waveform that looked very close the one on the screen. This was a way of doing waveform synthesis. How did you know, visually speaking, how to set the amplitude?\nA. Correlation of Time waveforms: We adjusted the slider on each sine wave when the selected sine Oscillation that you were manipulating had the best possible correlation with the target waveform!! But how does this correlation work here, with waveforms, instead of data variables?\nWe all know what Pearson Correlations are: we take the product of two (scaled and centered) quantitative variables, value by value, and take the average of these products. With waveforms, we can intuitively do the same thing to determine the coefficient of each component of the Fourier Series:\n\\[\ncoeff~for~sin(\\omega_c*t) = Average~Product \\Big(sin(\\omega_c*t) * target.waveform\\Big)\n\\]\n\\[\n= \\frac{1}{Waveform~Period} * \\displaystyle{\\int}_{0}^{Waveform~Period} sin(\\omega_c*t)*target.waveform * dt\n\\tag{1}\\]\nOK, but how does one make use of these time-waveform correlations?\nB. Orthogonal Waveforms: We need one more concept here: that of “orthogonal waveforms”: these are waveforms whose correlations, as defined above, are zero! But which are these? Our good old sine and cosine waves!!\nWhen we take sine/cosine waves whose frequencies are integer multiples of some base frequency, then all such waveforms are orthogonal.\n\\[\n\\frac{1}{Waveform~Period} * \\int sin(m*\\omega_c*t)*sin(n*\\omega_c*t)* dt = 0\\\\\n\\] \\[\n\\text{where m and n are multiples of some base frequency}\n\\tag{2}\\]\n\n\n\n\n\n\nNote\n\n\n\nFor more on Orthogonality, see here: https://qr.ae/pATe4W\n\n\nC: “Base Frequency”: So what is this base frequency we have been assuming? It is determined by the target waveform:\n\\[\nBase~ Frequency = \\frac{1}{Period~of~Target~Waveform}\n\\tag{3}\\]",
    "crumbs": [
      "Math Models for Creative Coders",
      "Media",
      "Fourier Series"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#how-does-the-fourier-series-compute",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#how-does-the-fourier-series-compute",
    "title": "Fourier Series",
    "section": "How does the Fourier Series Compute?",
    "text": "How does the Fourier Series Compute?\nSo now we are ready to define the steps in computing the Fourier Series:\n\nCompute the base-time-period \\(T\\) of the target waveform, and calculate the base frequency \\(f_c = \\frac{1}{T}\\) using Equation 3.\nTake say \\(M\\) integer multiples of this base frequency (\\(n = 1.....M\\)) and create sine waves with these. These are called harmonics.\nCompute the correlations of each harmonic with the target waveform, as indicated in Equation 1. These are the coefficients (i.e. amplitudes) for each of these harmonics.\nWrite the Fourier Series for the target waveform as:\n\n\\[\ntarget~waveform \\sim \\sum_{i=1}^{M} corr(i)*sin/cos(2\\pi*i*f_c*t)\n\\]",
    "crumbs": [
      "Math Models for Creative Coders",
      "Media",
      "Fourier Series"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#videos",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#videos",
    "title": "Fourier Series",
    "section": "Videos",
    "text": "Videos\nLet us now hear from Dan Schiffman, and also from 3Blue1Brown!",
    "crumbs": [
      "Math Models for Creative Coders",
      "Media",
      "Fourier Series"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#fourier-series-in-code",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#fourier-series-in-code",
    "title": "Fourier Series",
    "section": "Fourier Series in Code",
    "text": "Fourier Series in Code\nHow if we just enter a series of numbers, representing our waveform, or pick up sounds off the micrphone, and then make up a Fourier Series for that? We will use pretty much the techique used in creating the rolling circles for the drawing that we saw at first.\n\n\n p5.js\n R",
    "crumbs": [
      "Math Models for Creative Coders",
      "Media",
      "Fourier Series"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#wait-but-why",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#wait-but-why",
    "title": "Fourier Series",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nThink of the Fourier Series as a set of sinewaves that are derived by decomposing an original waveform\nHow are these components related? As integer multiples of a fundamental frequency.\nHow are their amplitudes calculated? By taking a correlation between the original waveform and the given sinewave component (unit amplitude)\nHow is this accurate? By minimizing a “least square error” between the original waveform and the sum of sinusoids.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Media",
      "Fourier Series"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#a-sound-vocabulary",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#a-sound-vocabulary",
    "title": "Fourier Series",
    "section": "\n A Sound Vocabulary",
    "text": "A Sound Vocabulary\nSome terms will show up repeatedly in our work and we should be clear what they mean:\n\n\nOscillation: Any periodic change in amplitude. https://natureofcode.com/oscillation/\n\n\nSinusoid: A Sine Wave Oscillation, created typically with p5.Oscillator\n\n\nWaveform: A graph of amplitude vs time\n\nFrequency: The rate of the oscillation, in cycles per second. Look for a repeating pattern, and measure its time period. \\(1/time.period\\) will give you frequency in Hertz(Hz)\n\nAmplitude: The height, or scaling factor of the oscillation. Easiest to decipher for a simple repeating pattern like sine, square, or triangle.\n\nPhase: The instantaneous angle-position of a rotating vector which generates the wave: Remember the Euler’s Formula. Also the instantaneous angle-value of a repeating wave at a certain amplitude.\n\nHarmonic: A (usually) Sine Oscillation that is at some integer multiple frequency of a reference Sine Oscillation. 2X = octave; 10X = decade.\n\nIn-harmonic: TBW\n\nPartials: TBW\n\nTransient: TBW\n\nAlias: TBW",
    "crumbs": [
      "Math Models for Creative Coders",
      "Media",
      "Fourier Series"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#references",
    "title": "Fourier Series",
    "section": "\n References",
    "text": "References\n\nJez Swanson. An Interactive Introduction to Fourier Transforms https://www.jezzamon.com/fourier/index.html\n\nAlex Miller. (2018). Fourier Series and Spinning Circles. https://alex.miller.im/posts/fourier-series-spinning-circles-visualization/\n\nBetter Explained. An Interactive Guide to the Fourier Transform. http://betterexplained.com/articles/an-interactive-guide-to-the-fourier-transform/\n\nAatish Bhatia (November 6, 2013). The Math Trick Behind MP3s, JPEGs, and Homer Simpson’s Face. https://nautil.us/the-math-trick-behind-mp3s-jpegs-and-homer-simpsons-face-234629/",
    "crumbs": [
      "Math Models for Creative Coders",
      "Media",
      "Fourier Series"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#resources",
    "href": "content/courses/MathModelsDesign/Modules/35-Media/10-FourierSeries/index.html#resources",
    "title": "Fourier Series",
    "section": "Resources",
    "text": "Resources\n\nhttps://mathlets.org/mathlets/fourier-coefficients/\nWorking with Audio in p5.js. https://pdm.lsupathways.org/3_audio/\n\nViolet Whitney. (Sep 28, 2023) Sounds: Working with sounds and speech in P5.js. https://medium.spatialpixel.com/sounds-bd05429aba38\n\nMister Bomb. p5.Sound project tutorials. https://www.youtube.com/playlist?list=PLIsdHp2z9wFl7A1wWb2VmQUUojEGsKELE\n\nhttps://musiclab.chromeexperiments.com/oscillators\nhttps://www.electronicbeats.net/the-feed/excel-drum-machine/\nhttps://junshern.github.io/algorithmic-music-tutorial/\nhttps://blackwhiskercult.com/visual-music-in-p5-js-i/\nJason Sigal.Visualizing Music with p5.js https://therewasaguy.github.io/p5-music-viz/\n\nDoga Kurkcuoglu. https://bilimneguzellan.net/en/?s=Fourier\n\n\nOther tools to explore\n\nStrudel REPL https://strudel.cc\n\nIntroducing Jukebox, a neural net that generates music, including rudimentary singing, as raw audio in a variety of genres and artist styles. We’re releasing a tool for everyone to explore the generated samples, as well as the model and code: https://openai.com/index/jukebox/ (OpenAI, April 30, 2020,via Twitter https://twitter.com/OpenAI)\nhttps://algorithmicpattern.org/2023/05/15/strudel-live-coding-patterns-on-the-web/\nhttps://betterexplained.com/articles/vector-calculus-understanding-the-dot-product/\nFreesound: Find Any Sound you Like. https://freesound.org\n\nWebSpeech API. https://developer.chrome.com/blog/voice-driven-web-apps-introduction-to-the-web-speech-api/\n\nhttps://dogbotic.com\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nambient\n1.0.2\nPedersen and Peck (2022)\n\n\nmosaicCalc\n0.6.4\nKaplan, Pruim, and Horton (2024)\n\n\nplot3D\n1.4.1\nSoetaert (2024)\n\n\n\n\n\n\nKaplan, Daniel T., Randall Pruim, and Nicholas J. Horton. 2024. mosaicCalc: R-Language Based Calculus Operations for Teaching. https://doi.org/10.32614/CRAN.package.mosaicCalc.\n\n\nPedersen, Thomas Lin, and Jordan Peck. 2022. ambient: A Generator of Multidimensional Noise. https://doi.org/10.32614/CRAN.package.ambient.\n\n\nSoetaert, Karline. 2024. plot3D: Plotting Multi-Dimensional Data. https://doi.org/10.32614/CRAN.package.plot3D.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Media",
      "Fourier Series"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/45-Uncertainty/50-Thoughts/index.html",
    "href": "content/courses/MathModelsDesign/Modules/45-Uncertainty/50-Thoughts/index.html",
    "title": "Working With Thoughts",
    "section": "",
    "text": "We will play several short games followed by discussions. These games may bring to light some of our Cognitive Biases and see how they affect us, and especially as we try to function as Artists/Designers/Creators.\nThere will be short readings that follow after each game.\nBut first, let us see how frail/fragile/fallible… we all are:\n\n Right! On to our first little fallibility!\n\n\n Test: PPT\n\nShort Reading: PDF\n\nTool: PDF\n\nReading: Here is a short reading on Exaggerated Emotional Coherence, also known as the Halo Effect** Download PDF **\n\nYou have to Stick the lighted candle to the Wall in such a way that the melting wax does not drop on to the floor.\n\nLook at the graph below: does it remind you of something you know very well?\n\n\n\n\n\n\n\n\nWhat does this graph represent?\nLet us pretend we are part of this graph and see where our Problem Formulating Skills take us!"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/45-Uncertainty/50-Thoughts/index.html#can-you-see-straight",
    "href": "content/courses/MathModelsDesign/Modules/45-Uncertainty/50-Thoughts/index.html#can-you-see-straight",
    "title": "Working With Thoughts",
    "section": "",
    "text": "We will play several short games followed by discussions. These games may bring to light some of our Cognitive Biases and see how they affect us, and especially as we try to function as Artists/Designers/Creators.\nThere will be short readings that follow after each game.\nBut first, let us see how frail/fragile/fallible… we all are:\n\n Right! On to our first little fallibility!\n\n\n Test: PPT\n\nShort Reading: PDF\n\nTool: PDF\n\nReading: Here is a short reading on Exaggerated Emotional Coherence, also known as the Halo Effect** Download PDF **\n\nYou have to Stick the lighted candle to the Wall in such a way that the melting wax does not drop on to the floor.\n\nLook at the graph below: does it remind you of something you know very well?\n\n\n\n\n\n\n\n\nWhat does this graph represent?\nLet us pretend we are part of this graph and see where our Problem Formulating Skills take us!"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/45-Uncertainty/50-Thoughts/index.html#discussion",
    "href": "content/courses/MathModelsDesign/Modules/45-Uncertainty/50-Thoughts/index.html#discussion",
    "title": "Working With Thoughts",
    "section": "Discussion",
    "text": "Discussion\n\nProblems and Contradictions\nAll Available Resources\n\nAssumptions and Functional Fixedness\n\n\n\nA comparable switch of attention occurs in an old joke about a worker in a high security factory, in which the employees were carefully watched when they left at the end of their work day. On a particular day, this worker was stopped at the factory gate as he walked out with a wheelbarrow full of styrofoam packing peanuts. He explained that he had salvaged these from the trash, and was planning to use them in shipping gifts to his grandchildren. Searching through this packing material, the guards found nothing, and so they let the man go home. The following week the same thing happened, and the worker was again stopped. But he offered the very same story, and when the guards searched through the packing peanuts and found nothing, he was allowed to leave. But this continued, week after week, until the guards could no longer believe that one person would want or could make use of so much packing material. Finally, the man was held for interrogation, at which time he admitted that he had absolutely no use for packing peanuts - and that, all these weeks, he had been stealing wheelbarrows.\n\n\nHearing this joke, I am reminded of the phrase “part and parcel”, which is a rough equivalent of “figure and ground”, the Gestalt Principles. Throughout most of it, the packing peanuts occupy center stage as figure (part), while the wheelbarrows (which function merely as containers) are completely ignored as innocuous ground (parcel). At the end of the joke, there is an unexpected twist, a switch of emphasis, a recentering, when we learn that the parcel is really the part.\n\nThis should also remind us of the Guilford Alternative Uses Exercise that we did, where we forced ourselves to leave the “regular use” of an object behind and think of it as serving quite another function.\nBias on TV\nLet’s find some of these ideas in our favourite Episode of one Season of your favourite show and tell everybody with a poster!"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/45-Uncertainty/50-Thoughts/index.html#bayesian-estimation",
    "href": "content/courses/MathModelsDesign/Modules/45-Uncertainty/50-Thoughts/index.html#bayesian-estimation",
    "title": "Working With Thoughts",
    "section": "Bayesian Estimation",
    "text": "Bayesian Estimation\nTaxicab Accident problem\nDisease Problem\nBaseball score prediction in R ( David Robinson)"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/45-Uncertainty/50-Thoughts/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/45-Uncertainty/50-Thoughts/index.html#references",
    "title": "Working With Thoughts",
    "section": "References",
    "text": "References\n\nThe Halo Effect, https://explorable.com/halo-effect\nNisbett, R. E., & Wilson, T. D. (1977). The halo effect: Evidence for unconscious alteration of judgments. Journal of Personality and Social Psychology, 35(4), 250–256. https://doi.org/10.1037/0022-3514.35.4.250 Download PDF\nBayesian Thinking Tutorial https://arbital.com/p/bayes_frequency_diagram/?l=55z&pathId=86923\nhttp://ndl.ethernet.edu.et/bitstream/123456789/37455/1/Max_Marchi.pdf"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/45-Uncertainty/listing.html",
    "href": "content/courses/MathModelsDesign/Modules/45-Uncertainty/listing.html",
    "title": "Uncertainty",
    "section": "",
    "text": "Title\n\n\n\nReading Time\n\n\n\n\n\n\n\n\n\n\n\nWorking with Chance\n\n\n1 min\n\n\n\n\n\n\n\n\n\nWorking With Thoughts\n\n\n6 min\n\n\n\n\n\n\n\n\n\nWorking With Chaos\n\n\n2 min\n\n\n\n\n\n\n\n\n\nWorking with Agents\n\n\n1 min\n\n\n\n\n\n\n\n\n\nWorking with Time\n\n\n1 min\n\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/45-Uncertainty/20-Chaos/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/45-Uncertainty/20-Chaos/index.html#references",
    "title": "Working With Chaos",
    "section": "References",
    "text": "References\nWebsites\n\nhttps://openprocessing.org\nhttps://timrodenbroeker.de/processing-or-p5/\nCode References\n\nAnimations, coding, interactives in this video by Jonny Hyman 🙌 Try the code yourself: https://github.com/jonnyhyman/Chaos\n\nPapers\n\nMay, R. Simple mathematical models with very complicated dynamics. Nature 261, 459–467 (1976). https://doi.org/10.1038/261459a0\nRobert Shaw, The Dripping Faucet as a Model Chaotic System https://archive.org/details/ShawRober…\nCrevier DW, Meister M. Synchronous period-doubling in flicker vision of salamander and man. J Neurophysiol. 1998 Apr;79(4):1869-78.\nBing Jia, Huaguang Gu, Li Li, Xiaoyan Zhao. Dynamics of period-doubling bifurcation to chaos in the spontaneous neural firing patterns Cogn Neurodyn (2012) 6:89–106 DOI 10.1007/s11571-011-9184-7\nA Garfinkel, ML Spano, WL Ditto, JN Weiss. Controlling cardiac chaos Science 28 Aug 1992: Vol. 257, Issue 5074, pp. 1230-1235 DOI: 10.1126/science.1519060\nR. M. May, D. M. G. Wishart, J. Bray and R. L. Smith Chaos and the Dynamics of Biological Populations Source: Proceedings of the Royal Society of London. Series A, Mathematical and Physical Sciences, Vol. 413, No. 1844, Dynamical Chaos (Sep. 8, 1987), pp. 27-44\nChialvo, D., Gilmour Jr, R. & Jalife, J. Low dimensional chaos in cardiac tissue. Nature 343, 653–657 (1990). https://doi.org/10.1038/343653a0\nXujun Ye, Kenshi Sakai. A new modified resource budget model for nonlinear dynamics in citrus production. Chaos, Solitons and Fractals 87 (2016) 51–60\nLibchaber, A. & Laroche, C. & Fauve, Stephan. (1982). Period doubling cascade in mercury, a quantitative measurement. http://dx.doi.org/10.1051/jphyslet:01…. 43. 10.1051/jphyslet:01982004307021100."
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/50-GradientDescent/index.html",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/50-GradientDescent/index.html",
    "title": "Gradient Descent",
    "section": "",
    "text": "We obtained the backpropagated error for each layer:\n\\[\n\\begin{bmatrix}\ne_{11}\\\\\ne_{21}\\\\\ne_{31}\\\\\n\\end{bmatrix} \\pmb{\\sim}\n\\begin{bmatrix}\nW_{11} & W_{12} & W_{13} \\\\\nW_{21} & W_{22}  & W_{23} \\\\\nW_{31} & W_{32} & W_{33} \\\\\n\\end{bmatrix} *\n\\begin{bmatrix}\n{e_{12}}\\\\\n{e_{22}}\\\\\n{e_{32}}\\\\\n\\end{bmatrix}\n\\]\nAnd the matrix form:\n\\[\ne^{l-1} ~ \\pmb{\\sim} ~ {W^l}^{\\pmb{\\color{red}{T}}}* e^{l}\n\\tag{1}\\]\nNow what? How do we use all these errors, from the output right up to those backpropagated backwards up to the first (\\(l=1\\)) layer? To adapt the weights of the NN using these backpropagated errors, here are the steps:\n\n\nPer-Weight Cost Gradient: We are looking for something like \\(\\large{\\pmb{\\color{red}{\\frac{dC}{W_{jk}}}}}\\) for all possible combos of \\(jk\\).\n\nLearn: Adapt the Weights in the opposite direction to its Cost-Gradient. (Why?)\n\nAre you ready? ;-D Let us do this !\n\n\nThe cost function was the squared error averaged over all \\(n\\) neurons:\n\n\\[\n\\begin{align}\nC(W, b) &= \\frac{1}{2n}\\sum^{n ~ neurons}_{i=1}e^2(i)\\\\\n\\end{align}\n\\tag{2}\\]\n\n\nSerious Magic: We want to differentiate this sum for each Weight. Before we calculate \\(\\frac{dC}{dW^l_{jk}}\\), we realize that any weight \\(W^l_{jk}\\) connects only as input to one neuron \\(k\\), which outputs \\(a_k\\). No other neuron-terms in the above summation depend upon this specific Weight, so the summation becomes just one term, pertaining to activation-output, say \\(a_k\\)!\n\n\\[\n\\begin{align}\n\\frac{d~C}{d~\\color{orange}{\\pmb{W^l_{jk}}}} &= \\large\\frac{d}{d~\\color{orange}{\\pmb{W^l_{jk}}}}\\Bigg({\\frac{1}{2n}\\sum^{all~n~neurons}_{i=1}(e_i)^2}~\\Bigg)\\\\\n\\\\\n&= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * \\frac{d}{d~\\color{orange}{\\pmb{W^l_{jk}}}} ~ \\Bigg(\\pmb{\\color{red}{\\Large{{e^{l}_k}}}} ~ \\Bigg) ~~only~~k^{th}~neuron~l^{th}~layer\\\\\n\\\\\n&= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * ~ {\\frac{d}{d~\\color{orange}{\\pmb{W^l_{jk}}}}\\bigg(\\Large{\\color{red}{a^{l}_k - d^l_k}}}\\bigg)\n\\end{align}\n\\]\n\nNow, the relationship between \\(a^{l}_k\\) and \\(W^l_{jk}\\) involves the sigmoid function. (And \\(d_k\\) is not dependent upon anything!)\n\n\\[\n\\begin{align}\n\\color{red}{\\pmb{a^l_k}} ~ &= \\sigma~\\bigg(\\sum^{neurons~in~l-1}_{j=1} \\pmb{\\color{orange}{W^l_{jk}}} ~ * ~{a^{l-1}_j + b^l_j}\\bigg)\\\\\n&= \\color{red}{\\sigma(everything)}\\\\\n\\end{align}\n\\]\n\nWe also know \\[\n\\large{\\frac{d\\sigma(x)}{dx}} = \\sigma(x) * \\big(1 - \\sigma(x)\\big)\n\\]\nFinal Leap: Using the great chain rule for differentiation, we obtain:\n\n\n\\[\n\\begin{align}\n\\frac{d~C}{d~\\color{orange}{\\pmb{W^l_{jk}}}} &= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * ~ {\\frac{d}{d~\\color{orange}{\\pmb{W^l_{jk}}}}\\bigg(\\Large{\\color{red}{a^{l}_k - d^l_k}}}\\bigg)\\\\\n&= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * ~\\frac{d~\\color{red}{\\pmb{a^l_k}}}{d~\\color{orange}{\\pmb{W^l_{jk}}}}\\\\\n&= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ *\\frac{d~ \\color{red}{\\sigma(everything)}}{d~\\color{orange}{\\pmb{W^l_{jk}}}}\\\\\n\\\\\n&= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * \\sigma(everything) * (1 -\\sigma(everything)) * \\frac{d(everything)}{d~\\color{orange}{\\pmb{W^l_{jk}}}}~~ \\text{Applying Chain Rule!}\\\\\n&= \\huge{\\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * \\color{red}{~a^{l-1}_j} * ~\\\\\n\\large{\\sigma~\\bigg(\\sum^{neurons~in~l-1}_{j=1} \\pmb{\\color{orange}{W^l_{jk}}} ~ * ~ {a^{l-1}_j + b^l_j}\\bigg) * \\\\\n\\bigg(1 - \\sigma~\\bigg(\\sum^{neurons~in~l-1}_{j=1} \\pmb{\\color{orange}{W^l_{jk}}} ~ * ~ {a^{l-1}_j + b^l_j}\\bigg)\\bigg)}}\\\\\n&= \\huge{\\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * \\color{red}{~a^{l-1}_j} * a^{l}_k * [1- a^{l}_k}]\n\\end{align}\n\\tag{3}\\]\n\nEquation corrected by Adit Joshi and Ananya Krishnan, April 2025\nHow to understand this monster equation intuitively? Let us first draw a diagram to visualize the components:\n\n\n\nLet us take the Weight \\(Wjk\\). It connects neuron \\(j^{l-1}\\) with neuron \\(k^l\\), using the activation \\(a^{l-1}_j\\). The relevant output error ( that contributes to the Cost function) is \\(e^l_{k}\\).\n\nThe product \\(\\large{\\color{red}{a^{l-1}_j} ~ * ~ \\color{lightblue}{e^l_k}}\\) is like a correlation product of the two quanties at the input and output of the neuron \\(k\\). This product contributes to a sense of slope: the larger either of these, larger is the Cost-slope going from neuron \\(j\\) to \\(k\\).\nHow do we account for the magnitude of the Weight \\(Wjk\\) itself? Surely that matters! Yes, but note that \\(Wjk\\) is entwined with the remaining inputs and weights via the \\(\\sigma\\) function term! We must differentiate that and put that differential into the product! That gives is the two other product terms in the formula above which involve the sigmoid function.\n\nSo, monster as it is, the formula is quite intuitive and even beautiful!\n\nThis gradient is calculated (in vector fashion) for all weights.\n\nSo now that we have the gradient of Cost vs \\(W^l_{jk}\\), we can adapt \\(W^l_{jk}\\) by moving a small tuning step in the opposite direction:\n\\[\nW^l_{jk}~|~new = W^l_{jk}~|~old - \\alpha * gradient\n\\tag{4}\\]\nand we adapt all weights in opposition to their individual cost gradient. The parameter \\(\\alpha\\) is called the learning rate.\nYes, but not all neurons have a desired output; so what do we use for error?? Only the output neurons have a desired output!!\nThe backpropagated error, peasants! Each neuron has already “received” its share of error, which is converted to Cost, whose gradient wrt all input weights of the specific neuron is calculated using Equation 3, and each weight thusly adapted using Equation 4.",
    "crumbs": [
      "Math Models for Creative Coders",
      "AI",
      "Gradient Descent"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/50-GradientDescent/index.html#learning-adapting-the-weights",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/50-GradientDescent/index.html#learning-adapting-the-weights",
    "title": "Gradient Descent",
    "section": "",
    "text": "We obtained the backpropagated error for each layer:\n\\[\n\\begin{bmatrix}\ne_{11}\\\\\ne_{21}\\\\\ne_{31}\\\\\n\\end{bmatrix} \\pmb{\\sim}\n\\begin{bmatrix}\nW_{11} & W_{12} & W_{13} \\\\\nW_{21} & W_{22}  & W_{23} \\\\\nW_{31} & W_{32} & W_{33} \\\\\n\\end{bmatrix} *\n\\begin{bmatrix}\n{e_{12}}\\\\\n{e_{22}}\\\\\n{e_{32}}\\\\\n\\end{bmatrix}\n\\]\nAnd the matrix form:\n\\[\ne^{l-1} ~ \\pmb{\\sim} ~ {W^l}^{\\pmb{\\color{red}{T}}}* e^{l}\n\\tag{1}\\]\nNow what? How do we use all these errors, from the output right up to those backpropagated backwards up to the first (\\(l=1\\)) layer? To adapt the weights of the NN using these backpropagated errors, here are the steps:\n\n\nPer-Weight Cost Gradient: We are looking for something like \\(\\large{\\pmb{\\color{red}{\\frac{dC}{W_{jk}}}}}\\) for all possible combos of \\(jk\\).\n\nLearn: Adapt the Weights in the opposite direction to its Cost-Gradient. (Why?)\n\nAre you ready? ;-D Let us do this !\n\n\nThe cost function was the squared error averaged over all \\(n\\) neurons:\n\n\\[\n\\begin{align}\nC(W, b) &= \\frac{1}{2n}\\sum^{n ~ neurons}_{i=1}e^2(i)\\\\\n\\end{align}\n\\tag{2}\\]\n\n\nSerious Magic: We want to differentiate this sum for each Weight. Before we calculate \\(\\frac{dC}{dW^l_{jk}}\\), we realize that any weight \\(W^l_{jk}\\) connects only as input to one neuron \\(k\\), which outputs \\(a_k\\). No other neuron-terms in the above summation depend upon this specific Weight, so the summation becomes just one term, pertaining to activation-output, say \\(a_k\\)!\n\n\\[\n\\begin{align}\n\\frac{d~C}{d~\\color{orange}{\\pmb{W^l_{jk}}}} &= \\large\\frac{d}{d~\\color{orange}{\\pmb{W^l_{jk}}}}\\Bigg({\\frac{1}{2n}\\sum^{all~n~neurons}_{i=1}(e_i)^2}~\\Bigg)\\\\\n\\\\\n&= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * \\frac{d}{d~\\color{orange}{\\pmb{W^l_{jk}}}} ~ \\Bigg(\\pmb{\\color{red}{\\Large{{e^{l}_k}}}} ~ \\Bigg) ~~only~~k^{th}~neuron~l^{th}~layer\\\\\n\\\\\n&= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * ~ {\\frac{d}{d~\\color{orange}{\\pmb{W^l_{jk}}}}\\bigg(\\Large{\\color{red}{a^{l}_k - d^l_k}}}\\bigg)\n\\end{align}\n\\]\n\nNow, the relationship between \\(a^{l}_k\\) and \\(W^l_{jk}\\) involves the sigmoid function. (And \\(d_k\\) is not dependent upon anything!)\n\n\\[\n\\begin{align}\n\\color{red}{\\pmb{a^l_k}} ~ &= \\sigma~\\bigg(\\sum^{neurons~in~l-1}_{j=1} \\pmb{\\color{orange}{W^l_{jk}}} ~ * ~{a^{l-1}_j + b^l_j}\\bigg)\\\\\n&= \\color{red}{\\sigma(everything)}\\\\\n\\end{align}\n\\]\n\nWe also know \\[\n\\large{\\frac{d\\sigma(x)}{dx}} = \\sigma(x) * \\big(1 - \\sigma(x)\\big)\n\\]\nFinal Leap: Using the great chain rule for differentiation, we obtain:\n\n\n\\[\n\\begin{align}\n\\frac{d~C}{d~\\color{orange}{\\pmb{W^l_{jk}}}} &= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * ~ {\\frac{d}{d~\\color{orange}{\\pmb{W^l_{jk}}}}\\bigg(\\Large{\\color{red}{a^{l}_k - d^l_k}}}\\bigg)\\\\\n&= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * ~\\frac{d~\\color{red}{\\pmb{a^l_k}}}{d~\\color{orange}{\\pmb{W^l_{jk}}}}\\\\\n&= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ *\\frac{d~ \\color{red}{\\sigma(everything)}}{d~\\color{orange}{\\pmb{W^l_{jk}}}}\\\\\n\\\\\n&= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * \\sigma(everything) * (1 -\\sigma(everything)) * \\frac{d(everything)}{d~\\color{orange}{\\pmb{W^l_{jk}}}}~~ \\text{Applying Chain Rule!}\\\\\n&= \\huge{\\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * \\color{red}{~a^{l-1}_j} * ~\\\\\n\\large{\\sigma~\\bigg(\\sum^{neurons~in~l-1}_{j=1} \\pmb{\\color{orange}{W^l_{jk}}} ~ * ~ {a^{l-1}_j + b^l_j}\\bigg) * \\\\\n\\bigg(1 - \\sigma~\\bigg(\\sum^{neurons~in~l-1}_{j=1} \\pmb{\\color{orange}{W^l_{jk}}} ~ * ~ {a^{l-1}_j + b^l_j}\\bigg)\\bigg)}}\\\\\n&= \\huge{\\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * \\color{red}{~a^{l-1}_j} * a^{l}_k * [1- a^{l}_k}]\n\\end{align}\n\\tag{3}\\]\n\nEquation corrected by Adit Joshi and Ananya Krishnan, April 2025\nHow to understand this monster equation intuitively? Let us first draw a diagram to visualize the components:\n\n\n\nLet us take the Weight \\(Wjk\\). It connects neuron \\(j^{l-1}\\) with neuron \\(k^l\\), using the activation \\(a^{l-1}_j\\). The relevant output error ( that contributes to the Cost function) is \\(e^l_{k}\\).\n\nThe product \\(\\large{\\color{red}{a^{l-1}_j} ~ * ~ \\color{lightblue}{e^l_k}}\\) is like a correlation product of the two quanties at the input and output of the neuron \\(k\\). This product contributes to a sense of slope: the larger either of these, larger is the Cost-slope going from neuron \\(j\\) to \\(k\\).\nHow do we account for the magnitude of the Weight \\(Wjk\\) itself? Surely that matters! Yes, but note that \\(Wjk\\) is entwined with the remaining inputs and weights via the \\(\\sigma\\) function term! We must differentiate that and put that differential into the product! That gives is the two other product terms in the formula above which involve the sigmoid function.\n\nSo, monster as it is, the formula is quite intuitive and even beautiful!\n\nThis gradient is calculated (in vector fashion) for all weights.\n\nSo now that we have the gradient of Cost vs \\(W^l_{jk}\\), we can adapt \\(W^l_{jk}\\) by moving a small tuning step in the opposite direction:\n\\[\nW^l_{jk}~|~new = W^l_{jk}~|~old - \\alpha * gradient\n\\tag{4}\\]\nand we adapt all weights in opposition to their individual cost gradient. The parameter \\(\\alpha\\) is called the learning rate.\nYes, but not all neurons have a desired output; so what do we use for error?? Only the output neurons have a desired output!!\nThe backpropagated error, peasants! Each neuron has already “received” its share of error, which is converted to Cost, whose gradient wrt all input weights of the specific neuron is calculated using Equation 3, and each weight thusly adapted using Equation 4.",
    "crumbs": [
      "Math Models for Creative Coders",
      "AI",
      "Gradient Descent"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/50-GradientDescent/index.html#here-comes-the-rain-maths-again",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/50-GradientDescent/index.html#here-comes-the-rain-maths-again",
    "title": "Gradient Descent",
    "section": "Here Comes the Rain Maths Again!",
    "text": "Here Comes the Rain Maths Again!\nNow, we are ready (maybe?) to watch these two very beautifully made videos on Backpropagation. One is of course from Dan Shiffman, and the other from Grant Sanderson a.k.a. 3Blue1Brown.",
    "crumbs": [
      "Math Models for Creative Coders",
      "AI",
      "Gradient Descent"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/50-GradientDescent/index.html#gradient-descent-in-code",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/50-GradientDescent/index.html#gradient-descent-in-code",
    "title": "Gradient Descent",
    "section": "Gradient Descent in Code",
    "text": "Gradient Descent in Code\n\n\nUsing p5.js\nUsing R\n\n\n\n\n\n\nUsing torch.",
    "crumbs": [
      "Math Models for Creative Coders",
      "AI",
      "Gradient Descent"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/50-GradientDescent/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/50-GradientDescent/index.html#references",
    "title": "Gradient Descent",
    "section": "References",
    "text": "References\n\nTariq Rashid. Make your own Neural Network. PDF Online\n\nMathoverflow. Intuitive Crutches for Higher Dimensional Thinking. https://mathoverflow.net/questions/25983/intuitive-crutches-for-higher-dimensional-thinking\n\nInteractive Backpropagation Explainer https://xnought.github.io/backprop-explainer/",
    "crumbs": [
      "Math Models for Creative Coders",
      "AI",
      "Gradient Descent"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/10-NeuralNets/index.html",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/10-NeuralNets/index.html",
    "title": "\n Working with Neural Nets",
    "section": "",
    "text": "(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 1: Bharat Natyam Poses",
    "crumbs": [
      "Math Models for Creative Coders",
      "AI",
      "<iconify-icon icon=\"lucide:person-standing\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"mdi:human-female-dance\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Working with Neural Nets"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/10-NeuralNets/index.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/10-NeuralNets/index.html#introduction",
    "title": "\n Working with Neural Nets",
    "section": "Introduction",
    "text": "Introduction\nOne of our aims with Creative Coding is to of course make things interactive. Here we will apply the ml5.js library in p5.js to use an ML/DL algorithm called Classification to detect human poses in front of the camera. The code can then create unique experiences based on pose-detection with ML, and the subsequent code that responds to the user.\nWe will be following the ideas from here:\nAdavu Detection\nhttps://docs.ml5js.org/#/reference/bodypose\nMudra Detection\nhttps://docs.ml5js.org/#/reference/handpose\nBhava Detection\nhttps://docs.ml5js.org/#/reference/facemesh",
    "crumbs": [
      "Math Models for Creative Coders",
      "AI",
      "<iconify-icon icon=\"lucide:person-standing\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"mdi:human-female-dance\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Working with Neural Nets"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/10-NeuralNets/index.html#but-wait-how-does-classification-work",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/10-NeuralNets/index.html#but-wait-how-does-classification-work",
    "title": "\n Working with Neural Nets",
    "section": "But Wait! How does Classification Work?",
    "text": "But Wait! How does Classification Work?\nAh, peasants. Isn’t it enough that you can dance?\nSo, we can perform Classification based on Machine Learning (ML) structured and algorithms such as:\n\nRandom Forests. Also see Google Decision Forests. We will try to get an intuition into bootstrapping of variables in data, creating decision trees, and making random selections of variables from a dataset to create random forests.\n\nAnd there are Deep Learning (DL) structured and algorithms that allow us to do the same things, perhaps in a more “black-box” manner. We will peep into:\n\nThe Perceptron\nThe Multilayer Perceptron\nBackpropagation\nGradient Descent\n\nConvolutional Neural Nets (in a later course)\n\n\n\nHere, we will also try to build an intuitive sense of some of the technical terminology involved: convolution, regression, activation, weighting…and such terms that generally elude peasants.",
    "crumbs": [
      "Math Models for Creative Coders",
      "AI",
      "<iconify-icon icon=\"lucide:person-standing\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"mdi:human-female-dance\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Working with Neural Nets"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/10-NeuralNets/index.html#wait-but-why",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/10-NeuralNets/index.html#wait-but-why",
    "title": "\n Working with Neural Nets",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\nUnderstanding the underlying math inside of Neural Nets can help us appreciate better how to apply them design with them, and even keep them as simple as needed.",
    "crumbs": [
      "Math Models for Creative Coders",
      "AI",
      "<iconify-icon icon=\"lucide:person-standing\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"mdi:human-female-dance\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Working with Neural Nets"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/10-NeuralNets/index.html#how-to-train-your-dragon-neural-network",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/10-NeuralNets/index.html#how-to-train-your-dragon-neural-network",
    "title": "\n Working with Neural Nets",
    "section": "How to Train your Dragon Neural Network",
    "text": "How to Train your Dragon Neural Network",
    "crumbs": [
      "Math Models for Creative Coders",
      "AI",
      "<iconify-icon icon=\"lucide:person-standing\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"mdi:human-female-dance\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Working with Neural Nets"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/10-NeuralNets/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/10-NeuralNets/index.html#references",
    "title": "\n Working with Neural Nets",
    "section": "\n References",
    "text": "References\n\nColah’s Blog.(Apr 6, 2014). Neural Networks, Manifolds, and Topology. https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/. Very simple and readable article.\nMachine Learning Tokyo: Interactive Tools for ML/DL, and Math. https://github.com/Machine-Learning-Tokyo/Interactive_Tool\n\nhttps://developers.google.com/machine-learning.https://developers.google.com/machine-learning\n\nThe Neural Network Zoo - The Asimov Institute. http://www.asimovinstitute.org/neural-network-zoo/\n\n\nIt’s just a linear model: neural networks edition. https://lucy.shinyapps.io/neural-net-linear/\n\n\nConvolutional Neural Networks\n\n\nDigit Recognition with CNNs. Interactive! https://transcranial.github.io/keras-js/#/mnist-cnn\n\n\nCNN Convoluter. https://pwwang.github.io/cnn-convoluter/\n\n\nCNN Explainer: Learn Convolutional Neural Network (CNN) in your browser!. https://poloclub.github.io/cnn-explainer/\n\nDeep Lizard. Understanding Convolution Operations in Neural Networks. https://deeplizard.com/resource/pavq7noze2\n\nAndrej Karpathy. ConvNetJS: Deep Learning in your browser.https://cs.stanford.edu/people/karpathy/convnetjs/\n\nAdit Deshpande. A Beginner’s Guide to CNNs. https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/\n\n\nAnyone Can Learn AI Using This Blog. https://colab.research.google.com/drive/1g5fj7W6QMER4-03jtou7k1t7zMVE9TVt#scrollTo=V8Vq_6Q3zivl\n\nPractical Deep Learning for Coders: An Online Free Course.https://course.fast.ai\n\nNeural Networks Visual with vcubingx\n\nPart 1. https://youtu.be/UOvPeC8WOt8\n\nPart 2. https://www.youtube.com/watch?v=-at7SLoVK_I\n\n\n\n\nLLMs\n\nBrendan Bycroft.Visualizing LLMs. https://bbycroft.net/llm\n\nRohit Patel (20 Oct 2024). Understanding LLMs from Scratch Using Middle School Math: A self-contained, full explanation to inner workings of an LLM. https://towardsdatascience.com/understanding-llms-from-scratch-using-middle-school-math-e602d27ec876\n\nAI-powered reporting and annotation for radiology. https://www.md.ai\n\nUsing R for DL\n\n\ntorch for R: An open source machine learning framework based on PyTorch. https://torch.mlverse.org\n\nTorch Interactive Tutorial. https://mlverse.shinyapps.io/torch-tour\n\nGeeks for Geeks. Convolutional Neural Nets in R. https://www.geeksforgeeks.org/convolutional-neural-networks-cnns-in-r/\n\nDavid Selby (9 January 2018). Tea and Stats Blog. Building a neural network from scratch in R. https://selbydavid.com/2018/01/09/neural-network/\n\nAkshaj Verma. (2020-07-24). Building A Neural Net from Scratch Using R - Part 1 and Part 2. https://rviews.rstudio.com/2020/07/20/shallow-neural-net-from-scratch-using-r-part-1/ and https://rviews.rstudio.com/2020/07/24/building-a-neural-net-from-scratch-using-r-part-2/\n\nAnder Fernandez Jauregui. https://anderfernandez.com/en/blog/how-to-create-neural-networks-with-torch-in-r/\n\nhttps://f0nzie.github.io/rtorch-minimal-book/",
    "crumbs": [
      "Math Models for Creative Coders",
      "AI",
      "<iconify-icon icon=\"lucide:person-standing\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"mdi:human-female-dance\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Working with Neural Nets"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/10-NeuralNets/index.html#textbooks",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/10-NeuralNets/index.html#textbooks",
    "title": "\n Working with Neural Nets",
    "section": "Textbooks",
    "text": "Textbooks\n\nMichael Nielsen. Neural Networks and Deep Learning. Available Online\n\nThe Little Book of Deep Learning. Available Online\n\nSimone Scardapane. Alice’s Adventures in Diffferentiable WonderLand: A Primer on Designing Neural Networks. https://www.sscardapane.it/alice-book/\n\nParr and Howard (2018). The Matrix Calculus You Need for Deep Learning.https://arxiv.org/abs/1802.01528\n\nZhang, Lipton, Li, Smola. Dive into Deep Learning. https://www.d2l.ai/\n\nSigrid Keydana. Deep Learning and Scientific Computing with R torch https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nkeras\n2.15.0\nAllaire and Chollet (2024)\n\n\nsafetensors\n0.1.2\nFalbel (2023)\n\n\ntensorflow\n2.16.0.9000\nAllaire and Tang (2025)\n\n\ntorch\n0.15.1\nFalbel and Luraschi (2025)\n\n\n\n\n\n\nAllaire, JJ, and François Chollet. 2024. keras: R Interface to “Keras”. https://doi.org/10.32614/CRAN.package.keras.\n\n\nAllaire, JJ, and Yuan Tang. 2025. tensorflow: R Interface to “TensorFlow”. https://github.com/rstudio/tensorflow.\n\n\nFalbel, Daniel. 2023. safetensors: Safetensors File Format. https://doi.org/10.32614/CRAN.package.safetensors.\n\n\nFalbel, Daniel, and Javier Luraschi. 2025. torch: Tensors and Neural Networks with “GPU” Acceleration. https://doi.org/10.32614/CRAN.package.torch.",
    "crumbs": [
      "Math Models for Creative Coders",
      "AI",
      "<iconify-icon icon=\"lucide:person-standing\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"mdi:human-female-dance\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Working with Neural Nets"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/listing.html",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/listing.html",
    "title": "AI",
    "section": "",
    "text": "Title\n\n\n\nReading Time\n\n\n\n\n\n\n\n\n  Working with Neural Nets\n\n\n9 min\n\n\n\n\n\n\nThe Perceptron\n\n\n12 min\n\n\n\n\n\n\nThe Multilayer Perceptron\n\n\n17 min\n\n\n\n\n\n\nMLPs and Backpropagation\n\n\n16 min\n\n\n\n\n\n\nGradient Descent\n\n\n12 min\n\n\n\n\n\n\nAI by Hand\n\n\n12 min\n\n\n\n\n\n\nConvolutional Neural Nets\n\n\n9 min\n\n\n\n\n\n\nLarge Language Models\n\n\n7 min\n\n\n\n\n\n\nNo matching items\n Back to top",
    "crumbs": [
      "Math Models for Creative Coders",
      "AI"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/55-HandCalculation/index.html",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/55-HandCalculation/index.html",
    "title": "AI by Hand",
    "section": "",
    "text": "Let us head off to this website and build a small Neural network by hand.\nhttps://student.desmos.com/join/6x8gme\nEvery time we give the network a new “sketch” to train with, potentially all network connections are updated, using backpropagation.\n\n\nThe cost function was the squared error averaged over all \\(n\\) neurons:\n\n\\[\n\\begin{align}\nC(W, b) &= \\frac{1}{2n}\\sum^{n ~ neurons}_{i=1}e^2(i)\\\\\n\\end{align}\n\\tag{1}\\]\n\n\nSerious Magic: We want to differentiate this sum for each Weight. Before we calculate \\(\\frac{dC}{dW^l_{jk}}\\), we realize that any weight \\(W^l_{jk}\\) connects only as input to one neuron \\(k\\), which outputs \\(a_k\\). No other neuron-terms in the above summation depend upon this specific Weight, so the summation becomes just one term, pertaining to activation-output, say \\(a_k\\)!\n\n\\[\n\\begin{align}\n\\frac{d~C}{d~\\color{orange}{\\pmb{W^l_{jk}}}} &= \\large\\frac{d}{d~\\color{orange}{\\pmb{W^l_{jk}}}}\\Bigg({\\frac{1}{2n}\\sum^{all~n~neurons}_{i=1}(e_i)^2}~\\Bigg)\\\\\n\\\\\n&= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * \\frac{d}{d~\\color{orange}{\\pmb{W^l_{jk}}}} ~ \\Bigg(\\pmb{\\color{red}{\\Large{{e^{l}_k}}}} ~ \\Bigg) ~~only~~k^{th}~neuron~l^{th}~layer\\\\\n\\\\\n&= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * ~ {\\frac{d}{d~\\color{orange}{\\pmb{W^l_{jk}}}}\\bigg(\\Large{\\color{red}{a^{l}_k - d^l_k}}}\\bigg)\n\\end{align}\n\\]\n\nNow, the relationship between \\(a^{l}_k\\) and \\(W^l_{jk}\\) involves the sigmoid function. (And \\(d_k\\) is not dependent upon anything!)\n\n\\[\n\\begin{align}\n\\color{red}{\\pmb{a^l_k}} ~ &= \\sigma~\\bigg(\\sum^{neurons~in~l-1}_{j=1} \\pmb{\\color{orange}{W^l_{jk}}} ~ * ~{a^{l-1}_j + b^l_j}\\bigg)\\\\\n&= \\color{red}{\\sigma(everything)}\\\\\n\\end{align}\n\\]\n\nWe also know \\[\n\\large{\\frac{d\\sigma(x)}{dx}} = \\sigma(x) * \\big(1 - \\sigma(x)\\big)\n\\]\nFinal Leap: Using the great chain rule for differentiation, we obtain:\n\n\n\\[\n\\begin{align}\n\\frac{d~C}{d~\\color{orange}{\\pmb{W^l_{jk}}}} &= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * ~ {\\frac{d}{d~\\color{orange}{\\pmb{W^l_{jk}}}}\\bigg(\\Large{\\color{red}{a^{l}_k - d^l_k}}}\\bigg)\\\\\n&= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * ~\\frac{d~\\color{red}{\\pmb{a^l_k}}}{d~\\color{orange}{\\pmb{W^l_{jk}}}}\\\\\n&= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ *\\frac{d~ \\color{red}{\\sigma(everything)}}{d~\\color{orange}{\\pmb{W^l_{jk}}}}\\\\\n\\\\\n&= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * \\sigma(everything) * (1 -\\sigma(everything)) * \\frac{d(everything)}{d~\\color{orange}{\\pmb{W^l_{jk}}}}~~ \\text{Applying Chain Rule!}\\\\\n&= \\huge{\\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * \\color{red}{~a^{l-1}_k} * ~\\\\\n\\large{\\sigma~\\bigg(\\sum^{neurons~in~l-1}_{j=1} \\pmb{\\color{orange}{W^l_{jk}}} ~ * ~ {a^{l-1}_j + b^l_j}\\bigg) * \\\\\n\\bigg(1 - \\sigma~\\bigg(\\sum^{neurons~in~l-1}_{j=1} \\pmb{\\color{orange}{W^l_{jk}}} ~ * ~ {a^{l-1}_j + b^l_j}\\bigg)\\bigg)}}\n\\end{align}\n\\tag{2}\\]\n\nHow to understand this monster equation intuitively? Let us first draw a diagram to visualize the components:\n\n\n\nLet us take the Weight \\(Wjk\\). It connects neuron \\(j^{l-1}\\) with neuron \\(k^l\\), using the activation \\(a^{l-1}_j\\). The relevant output error ( that contributes to the Cost function) is \\(e^l_{k}\\).\n\nThe product \\(\\large{\\color{red}{a^{l-1}_j} ~ * ~ \\color{lightblue}{e^l_k}}\\) is like a correlation product of the two quanties at the input and output of the neuron \\(k\\). This product contributes to a sense of slope: the larger either of these, larger is the Cost-slope going from neuron \\(j\\) to \\(k\\).\nHow do we account for the magnitude of the Weight \\(Wjk\\) itself? Surely that matters! Yes, but note that \\(Wjk\\) is entwined with the remaining inputs and weights via the \\(\\sigma\\) function term! We must differentiate that and put that differential into the product! That gives is the two other product terms in the formula above which involve the sigmoid function.\n\nSo, monster as it is, the formula is quite intuitive and even beautiful!\n\nThis gradient is calculated (in vector fashion) for all weights.\n\nSo now that we have the gradient of Cost vs \\(W^l_{jk}\\), we can adapt \\(W^l_{jk}\\) by moving a small tuning step in the opposite direction:\n\\[\nW^l_{jk}~|~new = W^l_{jk}~|~old - \\alpha * gradient\n\\tag{3}\\]\nand we adapt all weights in opposition to their individual cost gradient. The parameter \\(\\alpha\\) is called the learning rate.\nYes, but not all neurons have a desired output; so what do we use for error?? Only the output neurons have a desired output!!\nThe backpropagated error, peasants! Each neuron has already “received” its share of error, which is converted to Cost, whose gradient wrt all input weights of the specific neuron is calculated using Equation 2, and each weight thusly adapted using Equation 3."
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/55-HandCalculation/index.html#training-the-neural-network",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/55-HandCalculation/index.html#training-the-neural-network",
    "title": "AI by Hand",
    "section": "",
    "text": "Let us head off to this website and build a small Neural network by hand.\nhttps://student.desmos.com/join/6x8gme\nEvery time we give the network a new “sketch” to train with, potentially all network connections are updated, using backpropagation.\n\n\nThe cost function was the squared error averaged over all \\(n\\) neurons:\n\n\\[\n\\begin{align}\nC(W, b) &= \\frac{1}{2n}\\sum^{n ~ neurons}_{i=1}e^2(i)\\\\\n\\end{align}\n\\tag{1}\\]\n\n\nSerious Magic: We want to differentiate this sum for each Weight. Before we calculate \\(\\frac{dC}{dW^l_{jk}}\\), we realize that any weight \\(W^l_{jk}\\) connects only as input to one neuron \\(k\\), which outputs \\(a_k\\). No other neuron-terms in the above summation depend upon this specific Weight, so the summation becomes just one term, pertaining to activation-output, say \\(a_k\\)!\n\n\\[\n\\begin{align}\n\\frac{d~C}{d~\\color{orange}{\\pmb{W^l_{jk}}}} &= \\large\\frac{d}{d~\\color{orange}{\\pmb{W^l_{jk}}}}\\Bigg({\\frac{1}{2n}\\sum^{all~n~neurons}_{i=1}(e_i)^2}~\\Bigg)\\\\\n\\\\\n&= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * \\frac{d}{d~\\color{orange}{\\pmb{W^l_{jk}}}} ~ \\Bigg(\\pmb{\\color{red}{\\Large{{e^{l}_k}}}} ~ \\Bigg) ~~only~~k^{th}~neuron~l^{th}~layer\\\\\n\\\\\n&= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * ~ {\\frac{d}{d~\\color{orange}{\\pmb{W^l_{jk}}}}\\bigg(\\Large{\\color{red}{a^{l}_k - d^l_k}}}\\bigg)\n\\end{align}\n\\]\n\nNow, the relationship between \\(a^{l}_k\\) and \\(W^l_{jk}\\) involves the sigmoid function. (And \\(d_k\\) is not dependent upon anything!)\n\n\\[\n\\begin{align}\n\\color{red}{\\pmb{a^l_k}} ~ &= \\sigma~\\bigg(\\sum^{neurons~in~l-1}_{j=1} \\pmb{\\color{orange}{W^l_{jk}}} ~ * ~{a^{l-1}_j + b^l_j}\\bigg)\\\\\n&= \\color{red}{\\sigma(everything)}\\\\\n\\end{align}\n\\]\n\nWe also know \\[\n\\large{\\frac{d\\sigma(x)}{dx}} = \\sigma(x) * \\big(1 - \\sigma(x)\\big)\n\\]\nFinal Leap: Using the great chain rule for differentiation, we obtain:\n\n\n\\[\n\\begin{align}\n\\frac{d~C}{d~\\color{orange}{\\pmb{W^l_{jk}}}} &= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * ~ {\\frac{d}{d~\\color{orange}{\\pmb{W^l_{jk}}}}\\bigg(\\Large{\\color{red}{a^{l}_k - d^l_k}}}\\bigg)\\\\\n&= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * ~\\frac{d~\\color{red}{\\pmb{a^l_k}}}{d~\\color{orange}{\\pmb{W^l_{jk}}}}\\\\\n&= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ *\\frac{d~ \\color{red}{\\sigma(everything)}}{d~\\color{orange}{\\pmb{W^l_{jk}}}}\\\\\n\\\\\n&= \\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * \\sigma(everything) * (1 -\\sigma(everything)) * \\frac{d(everything)}{d~\\color{orange}{\\pmb{W^l_{jk}}}}~~ \\text{Applying Chain Rule!}\\\\\n&= \\huge{\\frac{\\color{skyblue}{\\large{e^l_k}} }{n} ~ * \\color{red}{~a^{l-1}_k} * ~\\\\\n\\large{\\sigma~\\bigg(\\sum^{neurons~in~l-1}_{j=1} \\pmb{\\color{orange}{W^l_{jk}}} ~ * ~ {a^{l-1}_j + b^l_j}\\bigg) * \\\\\n\\bigg(1 - \\sigma~\\bigg(\\sum^{neurons~in~l-1}_{j=1} \\pmb{\\color{orange}{W^l_{jk}}} ~ * ~ {a^{l-1}_j + b^l_j}\\bigg)\\bigg)}}\n\\end{align}\n\\tag{2}\\]\n\nHow to understand this monster equation intuitively? Let us first draw a diagram to visualize the components:\n\n\n\nLet us take the Weight \\(Wjk\\). It connects neuron \\(j^{l-1}\\) with neuron \\(k^l\\), using the activation \\(a^{l-1}_j\\). The relevant output error ( that contributes to the Cost function) is \\(e^l_{k}\\).\n\nThe product \\(\\large{\\color{red}{a^{l-1}_j} ~ * ~ \\color{lightblue}{e^l_k}}\\) is like a correlation product of the two quanties at the input and output of the neuron \\(k\\). This product contributes to a sense of slope: the larger either of these, larger is the Cost-slope going from neuron \\(j\\) to \\(k\\).\nHow do we account for the magnitude of the Weight \\(Wjk\\) itself? Surely that matters! Yes, but note that \\(Wjk\\) is entwined with the remaining inputs and weights via the \\(\\sigma\\) function term! We must differentiate that and put that differential into the product! That gives is the two other product terms in the formula above which involve the sigmoid function.\n\nSo, monster as it is, the formula is quite intuitive and even beautiful!\n\nThis gradient is calculated (in vector fashion) for all weights.\n\nSo now that we have the gradient of Cost vs \\(W^l_{jk}\\), we can adapt \\(W^l_{jk}\\) by moving a small tuning step in the opposite direction:\n\\[\nW^l_{jk}~|~new = W^l_{jk}~|~old - \\alpha * gradient\n\\tag{3}\\]\nand we adapt all weights in opposition to their individual cost gradient. The parameter \\(\\alpha\\) is called the learning rate.\nYes, but not all neurons have a desired output; so what do we use for error?? Only the output neurons have a desired output!!\nThe backpropagated error, peasants! Each neuron has already “received” its share of error, which is converted to Cost, whose gradient wrt all input weights of the specific neuron is calculated using Equation 2, and each weight thusly adapted using Equation 3."
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/55-HandCalculation/index.html#here-comes-the-rain-maths-again",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/55-HandCalculation/index.html#here-comes-the-rain-maths-again",
    "title": "AI by Hand",
    "section": "Here Comes the Rain Maths Again!",
    "text": "Here Comes the Rain Maths Again!\nNow, we are ready (maybe?) to watch these two very beautifully made videos on Backpropagation. One is of course from Dan Shiffman, and the other from Grant Sanderson a.k.a. 3Blue1Brown."
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/55-HandCalculation/index.html#gradient-descent-in-code",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/55-HandCalculation/index.html#gradient-descent-in-code",
    "title": "AI by Hand",
    "section": "Gradient Descent in Code",
    "text": "Gradient Descent in Code\n\n\nUsing p5.js\nUsing R\n\n\n\n\n\n\nUsing torch."
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/55-HandCalculation/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/55-HandCalculation/index.html#references",
    "title": "AI by Hand",
    "section": "References",
    "text": "References\n\nTariq Rashid. Make your own Neural Network. PDF Online\n\nMathoverflow. Intuitive Crutches for Higher Dimensional Thinking. https://mathoverflow.net/questions/25983/intuitive-crutches-for-higher-dimensional-thinking\n\nInteractive Backpropagation Explainer https://xnought.github.io/backprop-explainer/"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/60-Convnet/index.html#convolutional-neural-networks",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/60-Convnet/index.html#convolutional-neural-networks",
    "title": "Convolutional Neural Nets",
    "section": "Convolutional Neural Networks",
    "text": "Convolutional Neural Networks\nWhat is Convolution?\nConsider that you life in a high rise apartment complex. Have you heard an ambulance go by? How does the sound of the siren change as the ambulance approaches towards your dwelling and then goes past it to get lost amidst the surrounding buildings again?\nThe siren’s emitted sound is always the same. It is the local surroundings and the geometry of the echoes that brings the same sound to your ears again and again, but in altered form. The sound from the ambulance goes all around, hits on or other of the buildings, reflects, and comes back to your ears after a delay and weighted by the strength of the echo geometry.\nYou might consider that a CNN has several such echo mechanisms operating. Each pixel value (if you are dealing with image input) goes through a series of such delayed weightings which multiply the pixel input. The output of each pixel contributes in such fashion to the activation of that unit/layer of the CNN.\n\nHow does a CNN Structure use “Convolution”?\nLet us contemplate the structure of a CNN."
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/60-Convnet/index.html#videos",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/60-Convnet/index.html#videos",
    "title": "Convolutional Neural Nets",
    "section": "Videos",
    "text": "Videos"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/60-Convnet/index.html#convolutional-neural-nets-in-code",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/60-Convnet/index.html#convolutional-neural-nets-in-code",
    "title": "Convolutional Neural Nets",
    "section": "Convolutional Neural Nets in Code",
    "text": "Convolutional Neural Nets in Code\n\n\nUsing p5.js\nUsing R\n\n\n\n\n\n\nUsing torch."
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/60-Convnet/index.html#wait-but-why",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/60-Convnet/index.html#wait-but-why",
    "title": "Convolutional Neural Nets",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-AI/60-Convnet/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/100-AI/60-Convnet/index.html#references",
    "title": "Convolutional Neural Nets",
    "section": "\n References",
    "text": "References\n\n\nDigit Recognition with CNNs. Interactive! https://transcranial.github.io/keras-js/#/mnist-cnn\n\n\nCNN Convoluter. https://pwwang.github.io/cnn-convoluter/\n\n\nCNN Explainer: Learn Convolutional Neural Network (CNN) in your browser!. https://poloclub.github.io/cnn-explainer/\n\nDeep Lizard. Understanding Convolution Operations in Neural Networks. https://deeplizard.com/resource/pavq7noze2\n\nColah’s Blog.(Apr 6, 2014). Neural Networks, Manifolds, and Topology. https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/. Very simple and readable article.\nAndrej Karpathy. ConvNetJS:Deep Learning in your browser.https://cs.stanford.edu/people/karpathy/convnetjs/\n\nhttps://developers.google.com/machine-learning.https://developers.google.com/machine-learning\n\nAdit Deshpande. A Beginner’s Guide to CNNs. https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/\n\nThe Neural Network Zoo - The Asimov Institute. http://www.asimovinstitute.org/neural-network-zoo/\n\n\nIt’s just a linear model: neural networks edition. https://lucy.shinyapps.io/neural-net-linear/\n\nMachine Learning Tokyo: Interactive Tools for ML/DL, and Math. https://github.com/Machine-Learning-Tokyo/Interactive_Tool\n\n\nAnyone Can Learn AI Using This Blog. https://colab.research.google.com/drive/1g5fj7W6QMER4-03jtou7k1t7zMVE9TVt#scrollTo=V8Vq_6Q3zivl\n\nPractical Deep Learning for Coders: An Online Free Course.https://course.fast.ai\n\nNeural Networks Visual with vcubingx\n\n\n\nPart 1. https://youtu.be/UOvPeC8WOt8\n\nPart 2. https://www.youtube.com/watch?v=-at7SLoVK_I\n\n\n\nThe Neural Network Zoo - The Asimov Institute. http://www.asimovinstitute.org/neural-network-zoo/\n\nIt’s just a linear model: neural networks edition. https://lucy.shinyapps.io/neural-net-linear/\n\nNeural Network Playground. https://playground.tensorflow.org/\n\nRohit Patel (20 Oct 2024). Understanding LLMs from Scratch Using Middle School Math: A self-contained, full explanation to inner workings of an LLM. https://towardsdatascience.com/understanding-llms-from-scratch-using-middle-school-math-e602d27ec876\n\nMachine Learning Tokyo: Interactive Tools for ML/DL, and Math. https://github.com/Machine-Learning-Tokyo/Interactive_Tool\n\n\nAnyone Can Learn AI Using This Blog. https://colab.research.google.com/drive/1g5fj7W6QMER4-03jtou7k1t7zMVE9TVt#scrollTo=V8Vq_6Q3zivl\n\nPractical Deep Learning for Coders: An Online Free Course.https://course.fast.ai\n\n\nText Books\n\nMichael Nielsen. Neural Networks and Deep Learning, a free online book. https://neuralnetworksanddeeplearning.com/index.html\n\nSimone Scardapane. (2024) Alice’s Adventures in a differentiable Wonderland. https://www.sscardapane.it/alice-book/\nParr and Howard (2018). The Matrix Calculus You Need for Deep Learning.https://arxiv.org/abs/1802.01528\n\nThe Little Book of Deep Learning. Available Online\n\nParr and Howard (2018). The Matrix Calculus You Need for Deep Learning.https://arxiv.org/abs/1802.01528\n\nZhang, Lipton, Li, Smola. Dive into Deep Learning. https://www.d2l.ai/\n\nLLMs\n\nBrendan Bycroft.Visualizing LLMs. https://bbycroft.net/llm\n\nRohit Patel (20 Oct 2024). Understanding LLMs from Scratch Using Middle School Math: A self-contained, full explanation to inner workings of an LLM. https://towardsdatascience.com/understanding-llms-from-scratch-using-middle-school-math-e602d27ec876\n\nAI-powered reporting and annotation for radiology. https://www.md.ai\n\nCarl T. Bergstrom and Jevin D. West. Modern-Day Oracles or Bullshit Machines? https://thebullshitmachines.com/index.html\n\nUsing R for DL\n\nGeeks for Geeks. Convolutional Neural Nets in R. https://www.geeksforgeeks.org/convolutional-neural-networks-cnns-in-r/\n\nDavid Selby (9 January 2018). Tea and Stats Blog. Building a neural network from scratch in R. https://selbydavid.com/2018/01/09/neural-network/\n\n\ntorch for R: An open source machine learning framework based on PyTorch. https://torch.mlverse.org\n\nAkshaj Verma. (2020-07-24). Building A Neural Net from Scratch Using R - Part 1 and Part 2. https://rviews.rstudio.com/2020/07/20/shallow-neural-net-from-scratch-using-r-part-1/ and https://rviews.rstudio.com/2020/07/24/building-a-neural-net-from-scratch-using-r-part-2/\n\nAnder Fernandez Jauregui. https://anderfernandez.com/en/blog/how-to-create-neural-networks-with-torch-in-r/\n\nhttps://f0nzie.github.io/rtorch-minimal-book/\n\ntorch for R: An open source machine learning framework based on PyTorch. https://torch.mlverse.org/\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nkeras\n2.15.0\n@keras\n\n\nsafetensors\n0.1.2\n@safetensors\n\n\ntensorflow\n2.16.0.9000\n@tensorflow"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/20-Systems/80-Convolution/index.html",
    "href": "content/courses/MathModelsDesign/Modules/20-Systems/80-Convolution/index.html",
    "title": "What is Convolution?",
    "section": "",
    "text": "Consider that you live in a high rise apartment complex. Have you heard an ambulance go by? How does the sound of the siren change as the ambulance approaches towards your dwelling and then goes past it to get lost amidst the surrounding buildings again?\nThe siren’s emitted sound is always the same. It is the local surroundings and the geometry of the echoes that brings the same sound to your ears again and again, but in altered/weighted form. The sound from the ambulance goes all around, hits on or other of the buildings, reflects, and comes back to your ears after a delay and weighted by the strength of the echo geometry.\nWhat you hear is the overlapping of multiple, weighted copies of the sound emitted by the ambulance. As long as you have direct, i.e. non-reflected path from the ambulance to your ears, the echoes are relatively subdued. Once the vehicle gets right into your building complex and you lose the direct line-of-sight path, the echoes take over and the sound becomes a very confused mass that is barely recognizable.\n\nAll right, what does this have to do with convolution? Let us make some definitions first:\n\n\n\n\n\n\nImportantChannel\n\n\n\nThe free-space medium plus the buildings and other things that reflect sound in our environment, are called the “Channel”. The channel ascts as a conduit between a source (transmitter) and a receiver.\n\n\n\n\n\n\n\n\nImportantImpulse Response of the Channel\n\n\n\nThe geometry of the echoes that connect transmitter to receiver, including the bounces of the walls, the resulting path-delays, and weighting are together denoted as the impulse response of the channel. This is what the channel would put out at the receiver if the source transmitter were to emit a very-short-duration signal, like the squeak of a mouse.\n\n\nNow, most signals emitted by a source are usually not “squeak-like”: the ambulance has a siren that continuously emits the wellknown sound. Such a continuous signal is capable of mathematically decomposed into a series of “squeak-like” signals, which we call impulses.\nSo finally:\n\n\n\n\n\n\nImportantWhat is Convolution?\n\n\n\nEach impulse undergoes the same geometry path-delays and path-weightings posed by the channel impulse response. This is diagrammatically shown below:\n\n\n\n\n\nFigure 1: Convolution\n\n\nWe see that impulses in the input waveform that arrive later undergo wieghting by the earliest of path-delays and path-weightings. This should give you an intuition, that mathematically, this is like taking a weighted average but with the sequence of weights inverted in time!!!\nIf \\(in(t)\\) is the emitted sound waveform, and \\(f(t)\\) is the channel impulse response, we write the output of the channel as:\n\\[\n\\Large{out(t) = \\int_{-\\infty}^{\\infty} in(t) * f(t-\\tau) *d\\tau}\n\\tag{1}\\]\nNote that we are integrating wrt delay \\(\\tau\\); and \\(f\\) uses negative \\(\\tau\\) as its variable. Hence it is hence inverted in time, as shown in the bottom left of the Figure 1."
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/20-Systems/80-Convolution/index.html#inspiration",
    "href": "content/courses/MathModelsDesign/Modules/20-Systems/80-Convolution/index.html#inspiration",
    "title": "What is Convolution?",
    "section": "",
    "text": "Consider that you live in a high rise apartment complex. Have you heard an ambulance go by? How does the sound of the siren change as the ambulance approaches towards your dwelling and then goes past it to get lost amidst the surrounding buildings again?\nThe siren’s emitted sound is always the same. It is the local surroundings and the geometry of the echoes that brings the same sound to your ears again and again, but in altered/weighted form. The sound from the ambulance goes all around, hits on or other of the buildings, reflects, and comes back to your ears after a delay and weighted by the strength of the echo geometry.\nWhat you hear is the overlapping of multiple, weighted copies of the sound emitted by the ambulance. As long as you have direct, i.e. non-reflected path from the ambulance to your ears, the echoes are relatively subdued. Once the vehicle gets right into your building complex and you lose the direct line-of-sight path, the echoes take over and the sound becomes a very confused mass that is barely recognizable.\n\nAll right, what does this have to do with convolution? Let us make some definitions first:\n\n\n\n\n\n\nImportantChannel\n\n\n\nThe free-space medium plus the buildings and other things that reflect sound in our environment, are called the “Channel”. The channel ascts as a conduit between a source (transmitter) and a receiver.\n\n\n\n\n\n\n\n\nImportantImpulse Response of the Channel\n\n\n\nThe geometry of the echoes that connect transmitter to receiver, including the bounces of the walls, the resulting path-delays, and weighting are together denoted as the impulse response of the channel. This is what the channel would put out at the receiver if the source transmitter were to emit a very-short-duration signal, like the squeak of a mouse.\n\n\nNow, most signals emitted by a source are usually not “squeak-like”: the ambulance has a siren that continuously emits the wellknown sound. Such a continuous signal is capable of mathematically decomposed into a series of “squeak-like” signals, which we call impulses.\nSo finally:\n\n\n\n\n\n\nImportantWhat is Convolution?\n\n\n\nEach impulse undergoes the same geometry path-delays and path-weightings posed by the channel impulse response. This is diagrammatically shown below:\n\n\n\n\n\nFigure 1: Convolution\n\n\nWe see that impulses in the input waveform that arrive later undergo wieghting by the earliest of path-delays and path-weightings. This should give you an intuition, that mathematically, this is like taking a weighted average but with the sequence of weights inverted in time!!!\nIf \\(in(t)\\) is the emitted sound waveform, and \\(f(t)\\) is the channel impulse response, we write the output of the channel as:\n\\[\n\\Large{out(t) = \\int_{-\\infty}^{\\infty} in(t) * f(t-\\tau) *d\\tau}\n\\tag{1}\\]\nNote that we are integrating wrt delay \\(\\tau\\); and \\(f\\) uses negative \\(\\tau\\) as its variable. Hence it is hence inverted in time, as shown in the bottom left of the Figure 1."
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/20-Systems/80-Convolution/index.html#convolution-in-code",
    "href": "content/courses/MathModelsDesign/Modules/20-Systems/80-Convolution/index.html#convolution-in-code",
    "title": "What is Convolution?",
    "section": "Convolution in Code",
    "text": "Convolution in Code\n\n\nUsing p5.js\nUsing R\n\n\n\n\n\n\nWe’ll see"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/20-Systems/80-Convolution/index.html#wait-but-why",
    "href": "content/courses/MathModelsDesign/Modules/20-Systems/80-Convolution/index.html#wait-but-why",
    "title": "What is Convolution?",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\n\nPerceptrons are a standard convolution operation.\nConvolution is an operation that is crucial to the operation of Convolutional Neural Networks. The early (spatial) filter layers in a CNN implement a convolution with impulse responses that learn to look for edges, curves and similar canonical pieces in an input image.\nWhen we generate guitar-like sounds using the Karplus-Strong Guitar Algorithm, we are using a set of filters (with low-pass/band-pass impulse responses) in the feedback loop of a delay-line primed with random noise.\nConvolution can be seen as a series of Vector Dot Products between two vectors sliding past each other."
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/20-Systems/80-Convolution/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/20-Systems/80-Convolution/index.html#references",
    "title": "What is Convolution?",
    "section": "\n References",
    "text": "References\nTo be Written Up.\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nkeras\n2.15.0\n@keras\n\n\nsafetensors\n0.1.2\n@safetensors\n\n\ntensorflow\n2.16.0.9000\n@tensorflow"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/55-Connections/listing.html",
    "href": "content/courses/MathModelsDesign/Modules/55-Connections/listing.html",
    "title": "Connections",
    "section": "",
    "text": "Title\n\n\n\nReading Time\n\n\n\n\n\n\n\n\n\n\n\nWorking with Networks\n\n\n3 min\n\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/05-Maths/60-Orthogonality/index.html",
    "href": "content/courses/MathModelsDesign/Modules/05-Maths/60-Orthogonality/index.html",
    "title": "\n Things at Right Angles",
    "section": "",
    "text": "Human beings since old seem to have had an affinity for things at right angles. The Pythagoras theorem is of course testament to that. And Eratosthenes’ method of measuring the radius/circumference of the earth. But why does this angle seem “right” to us? And where does it show up in our lives?\nIn this brief module we will examine the idea of “right-ness” in several different natural phenomena, and application areas, to develop an intuition for how this an essential property that is supremely useful.\nSo where do we see this idea popping up?\n\n\nIn free space electromagnetic wave propagation, we have electric and magnetic fields making up the two components of the radiated wave. These are at right angles.\nAn object casts a near-zero shadow when the light source illuminating it is right above it, at 90 degrees from the horizontal (i.e perpendicular).\n\n\nRight-angled triangles and the Pythagoras theorem\nWhen we draw a graph in Cartesian coordinates, we wish to represent quantities \\((x,y)\\) on a set of axes, which are usually drawn perpendicular to each other, to ensure that the variations along each axis does not cast a shadow on the other axis, and are therefore independent.\n\n\nWhen vectors are right angles, their dot-product / inner product is zero.\nWhen we compute the cross product / outer product of two vectors, the resultant vector is perpendicular to the plane containing the two vectors.\nIf we have a family of vectors in 2 or more dimensions such that they are all mutually perpendicular to any other vector in the family, these are called an orthogonal basis set of vectors. Such vectors can be used to create a coordinate space of their own, and any other vector can be generated as a weighted sum of these basis vectors.\n\n\nWhen two (or more!) waveforms are multiplied together, and the product averaged over time, we obtain a time correlation of the two. If this happens to be zero, we classify the waveforms as being orthogonal. This is, sort of, the calculation of the “shadow” each waveform casts on the other.\nWhen we have a family of time functions which are each mutually orthogonal to any other waveform from the family, we have what is called an orthogonal basis set of waveforms. Such waveforms can be used to create a waveform space of their own, and any other waveform can be generated as a weighted sum of these basis waveforms.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Maths Basics",
      "<iconify-icon icon=\"mdi:angle-right\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Things at Right Angles"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/05-Maths/60-Orthogonality/index.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/05-Maths/60-Orthogonality/index.html#introduction",
    "title": "\n Things at Right Angles",
    "section": "",
    "text": "Human beings since old seem to have had an affinity for things at right angles. The Pythagoras theorem is of course testament to that. And Eratosthenes’ method of measuring the radius/circumference of the earth. But why does this angle seem “right” to us? And where does it show up in our lives?\nIn this brief module we will examine the idea of “right-ness” in several different natural phenomena, and application areas, to develop an intuition for how this an essential property that is supremely useful.\nSo where do we see this idea popping up?\n\n\nIn free space electromagnetic wave propagation, we have electric and magnetic fields making up the two components of the radiated wave. These are at right angles.\nAn object casts a near-zero shadow when the light source illuminating it is right above it, at 90 degrees from the horizontal (i.e perpendicular).\n\n\nRight-angled triangles and the Pythagoras theorem\nWhen we draw a graph in Cartesian coordinates, we wish to represent quantities \\((x,y)\\) on a set of axes, which are usually drawn perpendicular to each other, to ensure that the variations along each axis does not cast a shadow on the other axis, and are therefore independent.\n\n\nWhen vectors are right angles, their dot-product / inner product is zero.\nWhen we compute the cross product / outer product of two vectors, the resultant vector is perpendicular to the plane containing the two vectors.\nIf we have a family of vectors in 2 or more dimensions such that they are all mutually perpendicular to any other vector in the family, these are called an orthogonal basis set of vectors. Such vectors can be used to create a coordinate space of their own, and any other vector can be generated as a weighted sum of these basis vectors.\n\n\nWhen two (or more!) waveforms are multiplied together, and the product averaged over time, we obtain a time correlation of the two. If this happens to be zero, we classify the waveforms as being orthogonal. This is, sort of, the calculation of the “shadow” each waveform casts on the other.\nWhen we have a family of time functions which are each mutually orthogonal to any other waveform from the family, we have what is called an orthogonal basis set of waveforms. Such waveforms can be used to create a waveform space of their own, and any other waveform can be generated as a weighted sum of these basis waveforms.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Maths Basics",
      "<iconify-icon icon=\"mdi:angle-right\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Things at Right Angles"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/05-Maths/60-Orthogonality/index.html#uses-of-orthogonality",
    "href": "content/courses/MathModelsDesign/Modules/05-Maths/60-Orthogonality/index.html#uses-of-orthogonality",
    "title": "\n Things at Right Angles",
    "section": "Uses of Orthogonality",
    "text": "Uses of Orthogonality\nFourier Series and Laplace Transforms\nData Visualization\nMachine Learning and Deep Learning\n\nIn a Perceptron, we have a vector dot product between the input and the weight vectors.\nIn an ML-Classification problem, we consider a dataset of points containing say two classes. Each row in the data is a vector, with \\(length = n(columns)\\). We take a vector of weights of the same length, and take the vector dot-product of this fixed weight vector with each of the rwo-observations. The result of this operation is one dot-product number per observation in the data.\n\nNow consider the values of these dot products. Is the dot product negative, or positive in value? Can we use that polarity to decide on which observation belongs to which class? This is at the heart of an ML algorithm called Support Vector Machines.\nTechnology\n\nGPS / CDMA: See here for a quick intro to GPS. The codes used in GPS are a family of digital sequences called Gold codes. These are also an orthogonal set with near-zero cross-correlation between any pair of sequences from the set.\nBluetooth\nHR\n\nBrainstorming: If all members of a team sit together to brainstorm, GroupThink ensues very quickly, and people will be aligning* themselves with each other, by way of opinion or ideas.\nIn a team, we need to have people who are orthogonal to each other, in the sense that they have different skill sets, and different ways of looking at the same problem. This is what makes a team effective.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Maths Basics",
      "<iconify-icon icon=\"mdi:angle-right\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Things at Right Angles"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/05-Maths/60-Orthogonality/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/05-Maths/60-Orthogonality/index.html#references",
    "title": "\n Things at Right Angles",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Math Models for Creative Coders",
      "Maths Basics",
      "<iconify-icon icon=\"mdi:angle-right\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Things at Right Angles"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/05-Maths/60-Orthogonality/index.html#readings",
    "href": "content/courses/MathModelsDesign/Modules/05-Maths/60-Orthogonality/index.html#readings",
    "title": "\n Things at Right Angles",
    "section": "Readings",
    "text": "Readings\n\nArvind V on Quora. (2016). What is orthogonality of a signal? https://qr.ae/pATe4W",
    "crumbs": [
      "Math Models for Creative Coders",
      "Maths Basics",
      "<iconify-icon icon=\"mdi:angle-right\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Things at Right Angles"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/10-Physics/listing.html",
    "href": "content/courses/MathModelsDesign/Modules/10-Physics/listing.html",
    "title": "Physics",
    "section": "",
    "text": "No matching items\n Back to top"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html",
    "href": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Here are projects that we will contemplate creating at the end of the course. All the techniques, software, packages, theory, hardware…will be pieces aimed the realization of these projects. So we begin this course with the end in mind.\nThe projects below have been grouped into two groups, one for the junior students and the other senior students, who are both paradoxically in the same class. But there is an opinion that prerequisites are irrelevant.\n\n\nDo not start with fundamentals. This is an awful approach to learning. Start with so-called \"advanced\" topics and ask questions until every term/concept is understood. This is the correct, rigorous, scientific way to learn, because the advanced topics are embedded in larger,…\n\n— Sean McClure (@sean_a_mcclure) February 27, 2025",
    "crumbs": [
      "Math Models for Creative Coders",
      "Projects"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html#introduction",
    "title": "Projects",
    "section": "",
    "text": "Here are projects that we will contemplate creating at the end of the course. All the techniques, software, packages, theory, hardware…will be pieces aimed the realization of these projects. So we begin this course with the end in mind.\nThe projects below have been grouped into two groups, one for the junior students and the other senior students, who are both paradoxically in the same class. But there is an opinion that prerequisites are irrelevant.\n\n\nDo not start with fundamentals. This is an awful approach to learning. Start with so-called \"advanced\" topics and ask questions until every term/concept is understood. This is the correct, rigorous, scientific way to learn, because the advanced topics are embedded in larger,…\n\n— Sean McClure (@sean_a_mcclure) February 27, 2025",
    "crumbs": [
      "Math Models for Creative Coders",
      "Projects"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html#project-j0-make-an-ad-for-a-wellknown-logo",
    "href": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html#project-j0-make-an-ad-for-a-wellknown-logo",
    "title": "Projects",
    "section": "Project #J0: Make an ad for a wellknown logo",
    "text": "Project #J0: Make an ad for a wellknown logo",
    "crumbs": [
      "Math Models for Creative Coders",
      "Projects"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html#project-j1",
    "href": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html#project-j1",
    "title": "Projects",
    "section": "Project #J1:",
    "text": "Project #J1:",
    "crumbs": [
      "Math Models for Creative Coders",
      "Projects"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html#project-1-kingqueen-of-the-mountain",
    "href": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html#project-1-kingqueen-of-the-mountain",
    "title": "Projects",
    "section": "Project #1: King/Queen of the Mountain",
    "text": "Project #1: King/Queen of the Mountain\n\nDescription\n\nEach participant in the exhibit chooses whether they want to be on a mountain or in a valley, or in a grassland/meadow. More choices if you are up to it…volcano, anyone?\nTheir position within the space will be sensed. And they can move.\nA terrain map will be projected, either on the ceiling, or the screen, or on the participants themselves, with each individual being located on a specific terrain feature type that they chose at the start.\n\n\n\nPieces\n\nProject #1\n\n\n\n\n\n\n\nTheory\nTool\nWait, but why?\n\n\n\n\nTerrain Generation\n\np5.js/ Perlin Noise\n\n\nTerrain!\n\n\n\nLocation Sensing\n\nOSC protocol?\nArduino sensors?\nLink p5.js to Arduino, need libraries for interfacing and data transfer\n\n\nGive seed locations to Perlin Noise generator\nDefines preferences of terrain feature based in identity\n\n\n\nProjection\n\nProject terrain map\nDrive Projector using p5.js\nDo we need projection mapping libraries?\n\n\nWatch participants move and terrain follows them",
    "crumbs": [
      "Math Models for Creative Coders",
      "Projects"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html#project-2-sound-relationships",
    "href": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html#project-2-sound-relationships",
    "title": "Projects",
    "section": "Project #2: Sound Relationships",
    "text": "Project #2: Sound Relationships\n\nDescription\n\nThe floor is divided into footprint-sized areas. Each area is mapped to a particular note in a musical scale, that can be selected. Multi-octave.\nParticipants are told of the mapping and given a smartphone. And they can play hopscotch or dance or move, in sync with other participants to create a jam session.\nCan they make music / rhythms?\nThey are given headphones?\n\n\n\nPieces\n\nProject #2\n\n\n\n\n\n\n\nTheory\nTool\nWait, but why?\n\n\n\n\nSound Generation\n\np5.js with Sound/Music Libraries\nDo any of these SuperCollider? PureData? Strudel? JukeBox AI? have a library for p5.js?\n\n\nInstruments\nVocals?\n\n\n\nLocation Sensing\n\nOSC protocol?\nArduino sensors?\nLink p5.js to Arduino, need libraries for interfacing and data transfer\n\n\nJust location sensing\n\n\n\nProjection\n\nProject Network and musical Note?\nDrive Projector using p5.js\nDo we need projection mapping libraries?\n\n\nNetwork links must decay when participants are too far apart, or are silent…pauses in music",
    "crumbs": [
      "Math Models for Creative Coders",
      "Projects"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html#project-3a-and-3b-the-100-acre-wood-with-winnie-the-pooh",
    "href": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html#project-3a-and-3b-the-100-acre-wood-with-winnie-the-pooh",
    "title": "Projects",
    "section": "Project #3A and #3B: The 100-Acre Wood with Winnie the Pooh",
    "text": "Project #3A and #3B: The 100-Acre Wood with Winnie the Pooh\n\nDescription\n\nCreate natural shapes / images / illustrations / characters based on kolams/ lu-sona / fractals / L-Systems and graphs.\nA: Use grids, smoke, and mirrors to create laser diagrams in a table-top maze.\n\nPointer laser? Smoke? Agarbatti?\nMovable mirrors ( 0/90 degrees only so black paper clips should work.)\nSingle Point of entry for laser.\nLook from above through the smoke.\n\nB: Viewers can draw their illustration on a large paper canvas to create a Hundred Acre Wood\n\nPaper and Pencil / Colour / etc\nCheck with Code and replicate by hand.\nCatalogue of Shapes is essential\nGrid, Kolam Choice for shape, changes, innovation are possible\n\n\n\nPieces\n\n\n\n\n\n\n\n\nTheory\nTool\nWait, but why?\n\n\n\n\nShape Generation\n\np5.js\nGrids + Mirror locations\nL Systems with random user-chosen seeds - Catalogue of shapes to choose from? In software? Poster also.\nNot too much code\n\n\nViewers can match their personality/choice and what is on the garden-wall already to decide on their Creature\nLaser + Smoke to check the shape if it is a Kolam / Lu-Sona\n\n\n\nHardware\n\nSmoke and Mirrors\nPointer Laser",
    "crumbs": [
      "Math Models for Creative Coders",
      "Projects"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html#project-4-kagazi-hai-pairhan",
    "href": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html#project-4-kagazi-hai-pairhan",
    "title": "Projects",
    "section": "Project #4: Kagazi hai Pairhan",
    "text": "Project #4: Kagazi hai Pairhan\n\n\n“Naqsh fariyādī hai kis kī shoḳhi-e-tahrīr kā Kāġhzī hai pairahan har paikar-e-tasvīr kā”\n— Ghalib\n\n\n\nDescription\n\nDress/Clothing design with paper\nParticipants will turn and pirouette/turn about in front of a camera\nMeasurements will be taken and modelled using non-Euclidean geometry\nPaper cutout diagrams which can be fabricated to “dress up” the viewer in paper.\nPaper can have their favourite theorem or piece of text printed upon it!\n\n\nPieces\n\n\n\n\n\n\n\n\nTheory\nTool\nWait, but why?\n\n\n\n\nMeasurements of Fixed Points on/from Camera Image\n\nml5.js Pose Detection?\nMust get hold of Pose Detection Data and convert into “printed dress” pieces\nChoice of Clothing??\n\n\nClothing Design\n\n\n\nMath Model of Connecting Points with Hyperbolic Geometry Functions\n\nOther ways than ml5.js??\nImage to Geometry libraries?\n\n\nAllow for fit and drape\nShould not be too tight\n“Extra” Cloth to stitch with\nAll paper must be A4?\nEntire costume must be possible to carry in a file\nCan be “Assembled any time with tape/ pins/clips",
    "crumbs": [
      "Math Models for Creative Coders",
      "Projects"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html#project-4-my-face-is-a-fourier-series",
    "href": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html#project-4-my-face-is-a-fourier-series",
    "title": "Projects",
    "section": "Project #4: My Face is a Fourier Series",
    "text": "Project #4: My Face is a Fourier Series\nSee this:\nhttps://gofigure.impara.ai/drawing/5711774133256192\n\nPeople stand in front of camera\nDetailed picture is taken. Profile preferable!\nThe code will generate a Fourier Series of their profile and present an animated version on the screen.\nReset and Play should be possible\n\n\nPieces\n\n\n\n\n\n\n\n\nTheory\nTool\nWait, but why?\n\n\n\n\nMeasurements of Fixed Points on/from Camera Image\n\nml5.js?\nProfile Detection and conversion to waveform\n\n\nCapturing Profile “waveform”\n\n\n\nGenerating Fourier Series with Waveform + Animation\n\nMath Library\nShould be easily available\n\n\nAnimation",
    "crumbs": [
      "Math Models for Creative Coders",
      "Projects"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html#project-5-music-by-hand-waving",
    "href": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html#project-5-music-by-hand-waving",
    "title": "Projects",
    "section": "Project #5: Music by Hand-Waving",
    "text": "Project #5: Music by Hand-Waving",
    "crumbs": [
      "Math Models for Creative Coders",
      "Projects"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html#other-project-ideas",
    "href": "content/courses/MathModelsDesign/Modules/1000-Projects/index.html#other-project-ideas",
    "title": "Projects",
    "section": "Other Project Ideas",
    "text": "Other Project Ideas\n\nLink IFTTT app to p5.js and make things happen over WiFi, from other classes or buildings",
    "crumbs": [
      "Math Models for Creative Coders",
      "Projects"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/60-Kolams/index.html",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/60-Kolams/index.html",
    "title": "Kolams and Lusona",
    "section": "",
    "text": "Look at this fabric map of Africa:\n\n\n\n\n\nFigure 1: Africa Fabric Map: What’s your next T-Shirt?\n\n\nAnd look at this map of Nobel prize winners!\n\n\n\n\n\nFigure 2: Network of Nobel Prize Winners\n\n\nWould there be anything in common between these two 😮 ??!! How???!!!",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "Kolams and Lusona"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/60-Kolams/index.html#inspiration",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/60-Kolams/index.html#inspiration",
    "title": "Kolams and Lusona",
    "section": "",
    "text": "Look at this fabric map of Africa:\n\n\n\n\n\nFigure 1: Africa Fabric Map: What’s your next T-Shirt?\n\n\nAnd look at this map of Nobel prize winners!\n\n\n\n\n\nFigure 2: Network of Nobel Prize Winners\n\n\nWould there be anything in common between these two 😮 ??!! How???!!!",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "Kolams and Lusona"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/60-Kolams/index.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/60-Kolams/index.html#introduction",
    "title": "Kolams and Lusona",
    "section": "Introduction",
    "text": "Introduction\nThe South Indian tradition of Kolam, and the Angolan tradition of Lusona art have quite a few things in common. Both are also strongly linked to finite grammars and networks.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "Kolams and Lusona"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/60-Kolams/index.html#inspiration-1",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/60-Kolams/index.html#inspiration-1",
    "title": "Kolams and Lusona",
    "section": "\n Inspiration",
    "text": "Inspiration\n\n\nClick to generate a fresh Kolam!",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "Kolams and Lusona"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/60-Kolams/index.html#creating-kolams",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/60-Kolams/index.html#creating-kolams",
    "title": "Kolams and Lusona",
    "section": "Creating Kolams",
    "text": "Creating Kolams\nHow do we create these Kolam Patterns? Let us do this two ways: first pretending we are a South-Indian Woman adorning her doorstep in the morning. And then with two other methods, that lend themselves to computation / iteration. So, first by hand!",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "Kolams and Lusona"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/60-Kolams/index.html#by-hand",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/60-Kolams/index.html#by-hand",
    "title": "Kolams and Lusona",
    "section": "By Hand!",
    "text": "By Hand!\nSee if you can manually mimic some of the moves here! As an exercise, try to anchor your elbow and forearm to the table, and draw the pattern by rotating the paper! What are your observations?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMethod #1: Using Canonical Pieces\n\nWhat does canonical piece mean? These are pieces like the alphabet: pieces that can be repeatedly used to create a vast variety of patterns? Sounds familiar again?\nCheck the Polypad: https://polypad.amplify.com/p#patterns\nHere we use “pieces of Kolam” that are standard: by repeated usage of combinations of these pieces, ( I believe ) any kolam can be produced. Here is a video showing kolams with a few canonical pieces:\n\nWhich are the canonical pieces here?\nThis is also the idea embedded in this toy called Kolam Tiles. See this YT Playlist on Kolam Tiles.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMethod #2: Using Mirrors and Light\nThis follows the development of Paul Gerdes.\nFirst let us get a printable grid to make our Kolams manually, since making grids can become tedious when you are making a lot of Kolams. We can use the grid to place “pulli-s” on the grid to make our Kolam. Head off to: https://editor.p5js.org/arvindv/sketches/UuHApkvqd and open it in your p5.js web-editor. Print out a few samples of the .svg grid file that is generated.\nNow consider that each of your Kolam “lines” or “trajectories” is made of light. And place some single horizontal or vertical mirrors, at some locations midway between adjacent pulli-s. See the figure below:\n\n\n\n\n\nFigure 3: Kolam with Mirrors\n\n\nThe black lines here are to be imagined as “made of light”. Whenever they hit a mirror, a “curved reflection” occurs. Note how the arrangement of mirrors is symmetric here. Can we take computational liberties here and make asymmetric mirror arrangements? Can the grid also be non-square? Try?\nFor more inspiration, see here. This is a multipage article with many different grid+mirror arrangements! There is also an intriguing technique shown therein of colouring the squares in the grid alternatively white and black, to generate very symmetric shaded patterns!",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "Kolams and Lusona"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/60-Kolams/index.html#kolams-with-code",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/60-Kolams/index.html#kolams-with-code",
    "title": "Kolams and Lusona",
    "section": "Kolams with Code",
    "text": "Kolams with Code\nWork in (slow….) Progress!!!\n\n\nUsing p5.js\nUsing R\n\n\n\n\n&lt;iframe width=“780px” height=“600px”",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "Kolams and Lusona"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/60-Kolams/index.html#wait-but-why",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/60-Kolams/index.html#wait-but-why",
    "title": "Kolams and Lusona",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\nKolams and Sona are powerful metaphors for graphs and networks. These ideas show up in a variety of situations, such as tranportation networks, supply-chain, friendship networks, tracing literary and artistic influence, and so on.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "Kolams and Lusona"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/60-Kolams/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/60-Kolams/index.html#references",
    "title": "Kolams and Lusona",
    "section": "\n References",
    "text": "References\n\nSome kolam like patterns for inspiration. https://www.pinterest.com/gbenainous/p5js-teaching-ideas/\n\nDr. Gift Siromoney’s webpage. https://www.cmi.ac.in/gift/Kolam.htm\n\nMirror Designs and Mirror Curves: Comparing Kolam and Tchokwe Art. https://www.mi.sanu.ac.rs/vismath/paulus/pg1.htm\n\nYANAGISAWA, Kiwamu & Nagata, Shojiro. (2007). Fundamental Study on Design System of Kolam Pattern.https://www.researchgate.net/publication/237442288_Fundamental_Study_on_Design_System_of_Kolam_Pattern)\nPaulus Gerdes. Lunda Geometry: Mirror Curves, Designs, Knots, Polyominoes, Patterns, Symmetries. https://www.sahistory.org.za/sites/default/files/archive-files3/paulus_gerdes_lunda_geometry_mirror_curves_desbook4you.pdf\n\nVisual Mathematics. Mathematical Institute of the Serbian Academy of Sciences and Arts. Editor: Ljiljana Radovic. ISSN: 1821-1437. https://www.mi.sanu.ac.rs/vismath/\n\nImaginary.Org. Frozen Light App. https://www.imaginary.org/program/frozenlight\n\nhttps://kolamtiles.com\nAnu Reddy and Alex McLean.(March 2024). Drawing Kolam Patterns in Stitches and Code. https://alpaca.pubpub.org/pub/eljjyi80/release/6\n\nhttps://algorithmicpattern.org\nAscher, M. (2002). The Kolam Tradition: A tradition of figure-drawing in southern India expresses mathematical ideas and has attracted the attention of computer science. American Scientist, 90(1), 56+. https://link.gale.com/apps/doc/A81528419/AONE?u=anon\\~274c1208&sid=googleScholar&xid=4105e718",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "Kolams and Lusona"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html",
    "title": "\n Circles",
    "section": "",
    "text": "Let us start with an investigation into rolling circles! Circles have been with us since our childhood toys and to our older (and more silly!) aspirations for wheels (ahem!). Let us understand their mathematics and see what we can make with them.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"ph:circles-three-fill\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Circles"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#introduction",
    "title": "\n Circles",
    "section": "",
    "text": "Let us start with an investigation into rolling circles! Circles have been with us since our childhood toys and to our older (and more silly!) aspirations for wheels (ahem!). Let us understand their mathematics and see what we can make with them.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"ph:circles-three-fill\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Circles"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#inspiration",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#inspiration",
    "title": "\n Circles",
    "section": "\n Inspiration",
    "text": "Inspiration\n\nShow the Codet &lt;- seq(0, 2 * pi, by = 0.001)\nx &lt;- t\ndata &lt;- tibble::tibble(t,\n  x = cos(t) + cos(6 * t) / 2 + sin(14 * t) / 3,\n  y = sin(t) + sin(6 * t) / 2 + cos(14 * t) / 3\n)\n\ndata %&gt;%\n  gf_point(y ~ x) %&gt;%\n  gf_refine(coord_equal()) %&gt;%\n  gf_theme(theme_void())\n\n\n\n\n\n\nFigure 1: Mystery Curve",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"ph:circles-three-fill\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Circles"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#what-is-a-parametric-equation",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#what-is-a-parametric-equation",
    "title": "\n Circles",
    "section": "What is a Parametric Equation?",
    "text": "What is a Parametric Equation?\nHow was this curve created? The equation for the curve is given by a pair of parametric equations, one for \\(x\\) and one for \\(y\\):\n\\[\n\\begin{eqnarray}\nx &= cos(t) + cos(6t)/2 + sin(14t)/3\\\\\ny &= sin(t) + sin(6t)/2 + cos(14t)/3\n\\end{eqnarray}\n\\tag{1}\\]\nThis form is especially suited for a computational depiction of the curve, since we can have the parameter \\(t\\) go from \\(0~ -&gt;\\infty\\) and let the \\(x\\) and \\(y\\) be computed and plotted. All right, whatever…but what does this have to do with circles?? For that we need to turn to the famous Euler formula relating complex vectors and circles.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"ph:circles-three-fill\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Circles"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#how-about-the-euler-formula",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#how-about-the-euler-formula",
    "title": "\n Circles",
    "section": "How about the Euler Formula?",
    "text": "How about the Euler Formula?\nWhat is the equation of a circle? Most likely you will say:\n\\[\n\\begin{eqnarray}\nx^2 + y^2 &= 1\\\\\nor ~ perhaps\\\\\n(𝑥 − ℎ)^2 + (𝑦 − 𝑘)^2 &= 𝑅^2\\\\\n\\end{eqnarray}\n\\tag{2}\\]\nfor a circle with center \\(C(h,k)\\) and radius R.\nAs Frank Farris says, this is fine, but it represents a static view of a circle, which is not the simplest way to direct the drawing of one. The simplest way to instruct a machine to draw a circle uses a parametric form discussed above, also known as a vector-valued function:\n\\[\n\\gamma(t) = (cos(t), sin(t))\n\\] for the unit circle and\n\\[\n𝛾(𝑡) = (h + 𝑅 cos(𝑡), k + 𝑅 sin(𝑡))\n\\] for a more general one.\nNow, if we were to use complex numbers as our notation, then the function for our circle becomes:\n\\[\n\\begin{eqnarray}\n\\gamma(t) &=& (cos(t), sin(t))\\\\\n&=& e^{it}\n\\end{eqnarray}\n\\tag{3}\\]\nwhere of course, \\(i = \\sqrt{-1}\\).\nThis is the famous Euler Formula that connects complex numbers with trigonometry.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"ph:circles-three-fill\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Circles"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#the-mystery-curve",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#the-mystery-curve",
    "title": "\n Circles",
    "section": "The Mystery Curve",
    "text": "The Mystery Curve\nUsing this formula, our parametric function \\(\\mu(t)\\) for our mystery curve becomes a family of three circles, of different sizes and rotating at different speeds:\n\\[\n\\begin{eqnarray}\nx &= cos(t) + cos(6t)/2 + sin(14t)/3\\\\\ny &= sin(t) + sin(6t)/2 + cos(14t)/3\\\\\n\\end{eqnarray}\n\\]\nand\n\\[\n\\mu(t) = {\\large{\\color{hotpink}{1}} * {e^{\\color{Blue}{\\Large\\pmb{1it}}}}} +\n{\\large{\\color{hotpink}{\\frac{1}{2}}} * {e^{\\color{Blue}{\\Large{\\pmb{6it}}}}}} +\n{\\large{\\color{hotpink}{\\frac{i}{3}}} * {e^{\\color{Blue}{\\Large{\\pmb{-14it}}}}}}\n\\tag{4}\\]\nwhich gives us three rotating vectors with amplitudes given by {1, 1/2, 1/3} and with rotation speeds in the ratio {1 : 6: -14}. The first two rotate counter-clockwise; the third vector rotates in the clockwise direction since we have a negative coefficient for \\(t\\)). The tips of these rotating vectors course trace out the individual circles. We can picture the pattern as the vector sum of the vectors, or as three circles where each subsequent circle rotates and rolls on the circumference of the earlier one, like meshed gears.\n\n\nFrom The Math Less Travelled Blog\n\nHow do we go from the parametric form in Equation 1 to the complex exponential form in Equation 4? The first two terms are direct combinings of the respective cos and sin terms into exponentials; the third term may need a bit of understanding.\nHere the \\(sin\\) and \\(cos\\) terms are “interchanged” between x and y, so we need multiply by \\(i\\) (rotate by \\(\\pi/2\\)) to swap them, which means that the third circle starts from a 90 degree angle compared to the other two. Multiplying by \\(i\\) however makes the \\(sin\\) term negative, so we need to negate t as well, since \\(-sin(-t) = sin(t)\\). This means that the third exponential rotates in the opposite direction compared to the first two. See the expansion / explanation in the margin. We discuss this more in the following.\n\n\n\\[\n\\begin{eqnarray}\n\\frac{i}{3}*e^{-i14t} &=& \\frac{i}{3} \\Big\\{cos(-14t) + i(sin(-14t) \\Big\\}\\\\\n&=& \\frac{i}{3} \\Big\\{cos(14t) - i*sin(14t)\\Big\\}\\\\\n&=& \\frac{1}{3} \\Big\\{i*cos(14t) + sin(14t)\\Big\\}\\\\\n&=& \\frac{1}{3} \\Big\\{sin(14t) + i*cos(14t)\\Big\\}\\\\\n\\end{eqnarray}\n\\]\nwhich are respectively the desired x and y parametric functions for the third term.\n\n\n\n\n\n\nNoteReverse Rotating Vectors and Complex Amplitudes!!\n\n\n\nSigh.\n\n\n\n\n\n\n\n\n\n\n\nAmplitude\nRotation\nExample\nOperation in Parametric Equation\nWhat does it mean, really?\n\n\n\nReal\nPositive/CCW\n\\(2*e^{3it}\\)\n\n\\(x = 2*cos(3t)\\); \\(y=2*sin(3t)\\)\n\nVector starts from x-axis, goes CCW\n\n\nReal\nNegative/CW\n\\(2*e^{-3it}\\)\n\n\\(x = 2*cos(3t)\\); \\(y=-2*sin(3t)\\)\n\nVector starts from x-axis, goes CW\n\n\nComplex\nPositive/CCW\n\\(2*i*e^{3it}\\)\n\n\\(x = -2*sin(3t)\\); \\(y=2*cos(3t)\\)\n\nVector starts at \\(\\pi/2\\), goes CCW\n\n\nComplex\nNegative/CW\n\\(2*i*e^{-3it}\\)\n\n\\(x = 2*sin(3t)\\); \\(y=2*cos(3t)\\)\n\nVector starts at \\(\\pi/2\\), goes CW\n\n\n\n\n\nTable 1: Rotating Complex Exponentials and their Complex Amplitudes\n\n\n\n\nDo think of what might happen when the amplitude has an overall negative sign, like \\(-2*e^{-3it}\\) or \\(-2i*e^{-3it}\\)! (Gasp!! Swoon…). Just flip the vector on its head and rotate the same way as stated.\nPlotting with Complex Exponentials in R\nWe can use the rules in the above table to directly plot using complex vectors in R:\n\nShow the Codef_mystery1 &lt;- function(x) {\n  (exp((0 + 1i) * x) +\n    0.5 * exp((0 + 6i) * x) +\n    1 / 3 * 1i * exp((0 - 14i) * x)) # Note the \"1i\"!\n}\n\ndata_mystery_1 &lt;- tibble(t, pattern = f_mystery1(t))\n\ndata_mystery_1 %&gt;%\n  gf_point(Im(pattern) ~ Re(pattern)) %&gt;%\n  gf_refine(coord_equal()) %&gt;%\n  gf_theme(theme_void())",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"ph:circles-three-fill\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Circles"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#rotational-symmetry",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#rotational-symmetry",
    "title": "\n Circles",
    "section": "Rotational Symmetry",
    "text": "Rotational Symmetry\nWe notice a pattern in Figure 1, our inspiration example: the shape has a five-fold symmetry: If we rotate the entire figure by \\(\\frac{2\\pi}{5}\\), it will overlap exactly with the original. Further, we suspect that the curve has 5 “pieces”, that repeat every \\(\\frac{2\\pi}{5}\\). If we chop up the parametric variable \\(t\\) into 5 sections, we might obtain each individual piece, rotated by that angle. What properties does the generating function Equation 4 have that causes this symmetry?\nFollowing the development in Frank Farris’ book, let us record our ideas/suspicions of symmetry as:\n\\[\n\\begin{eqnarray}\n\\mu(t) &= \\mu(t + \\color{Blue}{\\Large\\pmb{2\\pi/5}})\\\\\n&= e^{\\color{Blue}{\\huge\\pmb{2\\pi *i/5}}} * \\mu(t)\n\\end{eqnarray}\n\\tag{5}\\]\nDoes this work out? Let’s see:\n\\[\n\\begin{eqnarray}\n\\mu(t + 2\\pi/5) &=& \\Big\\{ e^{i(t + 2\\pi/5)} + \\frac{1}{2}*e^{i6(t + 2\\pi/5)} + \\frac{1}{3}*i*e^{-i14(t + 2\\pi/5)}\\Big\\}\\\\\n&=& \\Big\\{e^{2\\pi i/5} *e^{it} + \\frac{1}{2}*e^{12\\pi i/5} *e^{i6t} + \\frac{1}{3}*e^{-28\\pi i/5} *e^{-i14t}\\Big\\}\\\\\n&=& \\Big\\{e^{2\\pi i/5}*e^{it} + \\frac{1}{2}*e^{(10+2)\\pi i/5}*e^{i6t} +  \\frac{1}{3}*e^{(-30 +2)\\pi i/5} *e^{-i14t}\\Big\\}\\\\\n&=& e^{2\\pi i/5}* \\Big\\{ e^{it} + \\frac{1}{2}*e^{i6t} + \\frac{1}{3}*i*e^{-i14t}\\Big\\}\\\\\n\\end{eqnarray}\n\\tag{6}\\]\nSo if we shift time by \\(t = 2\\pi/5\\), we get the same pattern rotated by \\(2\\pi/5\\) radians. Because the frequencies 1, 6, and −14 are all congruent to 1 modulo 5, shifting time by \\(2\\pi/5\\) causes the equation to add on a complex rotation term of \\(e^{2\\pi*i/5}\\).\nTime shifts are Angle Shifts. And our mystery curve hence meets the symmetry condition in Equation 5.\nThe Symmetry Condition Theorem\nSuppose that \\(m\\) and \\(k\\) are integers and that all the frequency numbers \\(n_j\\), \\(j={1..M}\\) in the finite sum:\n\\[\nf(t) = \\sum_{i=1}^M(a_1*e^{in_1t} + a_2*e^{in_2t}...+ a_M*e^{in_Mt})\n\\] have \\(n_j = k(mod~𝑚)\\).\nThen, for any choice of the complex coefficients \\(a_j\\), \\(f(t)\\) satisfies the symmetry condition:\n\\[\nf(t + \\frac{2\\pi}{m}) = e{\\frac{2k*\\pi*i}{m}} * f(t)\\\\\nfor~ all~ t\n\\]\nWhat a mouthful! What does that mean?\nIf we take a set of \\(M\\) integer frequencies, such that they have the same remainder \\(k\\) when divided by another integer \\(m\\), then these frequencies when attached to rotating circles will give us \\(m\\)-fold symmetry. E.g: M = 5, m = 7, k = 1 implies the frequencies are -14+1, -7+1, 1, 7+1, 14+1.\n\n\n\n\n\n\nNoteComplex Coefficients?\n\n\n\nQuestion: How do we handle \\(n_j\\), \\(j={1..M}\\) being complex, at least some of them?\nLook back at the table Table 1.\nConsider a \\(term = i/3 * e^{i6t}\\). We can expand this as:\n\\[\n\\begin{eqnarray}\nterm &=& i/3 * e^{i6t}\\\\\n&=& i/3 * \\big[cos(6t) + i*sin(6t)\\big]\\\\\n&=& -1/3*sin(6t) + i/3*cos(t)\\\\\n\\end{eqnarray}\n\\]\nWe can view this as a rotation by \\(\\pi/2\\) in the counter-clockwise direction. Other angles will contribute to rotations of the coefficients in the same way. Complex Coefficients will alter the nature of the pattern of course, but not the symmetry!\n\n\n\n\n\n\n\n\nNoteMutually Prime?\n\n\n\nQuestion: What happens when \\(k\\) is a factor of \\(m\\)? E.g: \\(k=3\\) and \\(m=9\\): what happens? Find out!",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"ph:circles-three-fill\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Circles"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#design-principles-for-rotational-symmetry",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#design-principles-for-rotational-symmetry",
    "title": "\n Circles",
    "section": "Design Principles for Rotational Symmetry",
    "text": "Design Principles for Rotational Symmetry\nHow do we capture all of the above in a set of design principles for symmetric rotation-based patterns? The design parameters for us are:\n\nNumber of frequencies / rolling circles: M\nThe Frequency values for each rolling circle: \\(n_j\\), \\(j={1..M}\\)\n\nThe (complex) Amplitudes \\(a_j\\), \\(j={1..M}\\)\n\n\nLarger values of \\(M\\) give a more fine grain structure to the pattern, especially when combined with diminishing amplitudes of \\(a_j\\), an idea that we will encounter again in Making Noise.\nLet us randomly create an equation, using the following parameters:\n\nM = 5 (Number of rotating circles)\nm = 7 (Prime Modulus) i.e. Order of Symmetry\nk = 2 (The remainder of \\(n*m~mod~k\\)) i.e. Type of Symmetry\n\nHere is the plot of the frequency components:\n\nShow the Code# Set graph theme\ntheme_set(new = theme_custom())\n#\nset.seed(42)\nM &lt;- 5 # Number of circles\nm &lt;- 7 # Prime Modulus\nk &lt;- 2 # Type of Symmetry\ntibble(\n  index = seq(-floor((M - 1) / 2), floor((M - 1) / 2), 1),\n  prime_multiple = m * index,\n  remainder = rep(k, length(index)),\n  frequency = prime_multiple + remainder\n) %&gt;%\n  mutate(amplitude = if_else(frequency == k, 1, k / frequency)) %&gt;%\n  # scaling amplitudes\n  # mutate(y0 = rep(0, length(index)),\n  #        z0 = rep(0, length(index))) %&gt;%\n  dplyr::select(prime_multiple, frequency, amplitude) -&gt; circles\ncircles\n\n\n  \n\n\nShow the Code##\ncircles %&gt;%\n  gf_hline(yintercept = 0, colour = \"grey\") %&gt;%\n  gf_segment(rep(0, length(prime_multiple)) + rep(0, length(prime_multiple)) ~ prime_multiple + frequency,\n    arrow = arrow(\n      angle = 20,\n      length = unit(0.15, \"inches\"),\n      ends = \"last\", type = \"open\"\n    )\n  ) %&gt;%\n  gf_segment(\n    rep(0, length(prime_multiple)) + amplitude ~\n      frequency + frequency,\n    data = circles, linewidth = 2,\n    arrow = arrow(\n      angle = 30,\n      length = unit(0.1, \"inches\"),\n      ends = \"last\", type = \"open\"\n    )\n  ) %&gt;%\n  gf_point(rep(0, length(prime_multiple)) ~ prime_multiple,\n    colour = \"red\", size = 3\n  ) %&gt;%\n  gf_point(rep(0, length(prime_multiple)) ~ frequency,\n    xlab = \"Frequency Component\",\n    ylab = \"Amplitude\", data = circles\n  ) %&gt;%\n  gf_labs(\n    title = \"Rotating Vectors Frquencies and Amplitudes\",\n    subtitle = \"Negative Frequency components rotate counterclockwise\", caption = \"Red Dots: Prime Modulus Multiples\"\n  ) %&gt;%\n  gf_refine(annotate(\n    x = circles$frequency + 1.75,\n    y = circles$amplitude,\n    geom = \"text\",\n    label = as.character(round(circles$amplitude, 4), nsmall = 3), size = 3.5\n  )) %&gt;%\n  gf_refine(scale_x_continuous(breaks = c(-28, -21, -14, -7, -5, 0, 2, 7, 9, 14, 16))) %&gt;%\n  gf_theme(theme_custom())\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\n\n\n\n\n\n\nThe function for this curve would be:\n\\[\n\\begin{multline}\n\\mu(t) = {\\large{\\color{hotpink}{1}} * {e^{\\color{Blue}{\\large{\\pmb{2it}}}}}}+\n\\\\\n+{\\large{\\color{hotpink}{0.2222}} * {e^{\\color{Blue}{\\large{\\pmb{9it}}}}}} +\n{\\large{\\color{hotpink}{0.125}} *\n{e^{\\color{Blue}{\\large{\\pmb{16it}}}}}}\n\\\\\\ -\n{\\large{\\color{hotpink}{0.4}} * {e^{\\color{Blue}{\\large{\\pmb{-5it}}}}}}\n-{\\large{\\color{hotpink}{0.1667}} * {e^{\\color{Blue}{\\large{\\pmb{-12it}}}}}}\n\\\\\\\n\\end{multline}\n\\tag{7}\\]\nLet us now plot this:\nShow the Code# remainder = 2 from 7\n# frequencies: 2, 7+2, 14+2, -7+2, -14+2\nf_mystery2 &lt;- function(x) {\n  1.0 * (exp((0 + 2i) * x) +\n    0.2222 * exp((0 + 9i) * x) +\n    0.125 * exp((0 + 16i) * x) -\n    0.4 * exp((0 - 5i) * x) -\n    0.1667 * exp((0 - 12i) * x))\n}\ndata_mystery_2 &lt;- tibble(t, pattern = f_mystery2(t))\ndata_mystery_2 %&gt;%\n  gf_point(Im(pattern) ~ Re(pattern), title = \"Complex Exponential Rendering\n           \") %&gt;%\n  gf_refine(coord_equal()) %&gt;%\n  gf_theme(theme_void())\n###\ndata2 &lt;- tibble::tibble(\n  t = seq(0, 2 * pi, 0.001),\n  x = cos(2 * t) + 0.2222 * cos(9 * t) + 0.125 * cos(16 * t) - 0.4 * cos(5 * t) - 0.1667 * cos(12 * t),\n  y = sin(2 * t) + 0.2222 * sin(9 * t) + 0.125 * sin(16 * t) + 0.4 * sin(5 * t) + 0.1667 * sin(12 * t)\n)\ndata2 %&gt;%\n  gf_point(y ~ x, title = \"Parametric Equation Rendering\") %&gt;%\n  gf_refine(coord_equal()) %&gt;%\n  gf_theme(theme_void())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere, we have designed a pattern with seven-fold rotational symmetry. Can you make this in p5.js? Can you try for other orders and types of symmetry?",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"ph:circles-three-fill\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Circles"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#mirror-symmetry",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#mirror-symmetry",
    "title": "\n Circles",
    "section": "\n Mirror Symmetry",
    "text": "Mirror Symmetry\nThe coordinate system defines a positive increase in angle as the counterclockwise direction. So an increase in the parameter \\(t\\), increases the angle for each frequency component in that direction, if their coefficient is positive, and the other way of their coefficient is negative. So far so good.\nConsider a small modification to our original Figure 1:\nShow the Code## Original Mystery Curve\n# ## remainder = +1 from 5\n# ## frequencies 1, 5+1, -15+1\ndata1 &lt;- tibble::tibble(\n  t = seq(0, 2 * pi, 0.001),\n  x = cos(t) + cos(6 * t) / 2 + sin(14 * t) / 3,\n  y = sin(t) + sin(6 * t) / 2 + cos(14 * t) / 3\n)\n\n# Mystery Curve\ndata1 %&gt;%\n  gf_point(y ~ x) %&gt;%\n  gf_refine(coord_equal()) %&gt;%\n  gf_theme(theme_void())\n# Derived from Mystery\n# Remainder = 2 from 5\n# Frequencies: 2, 5+2, -15+2\n# Coeffs: 1,1,1\ndata3 &lt;- tibble::tibble(\n  t = seq(0, 2 * pi, 0.001),\n  x = cos(t) + cos(6 * t) + cos(14 * t),\n  y = sin(t) + sin(6 * t) + sin(14 * t)\n)\ndata3 %&gt;%\n  gf_point(y ~ x) %&gt;%\n  gf_refine(coord_equal()) %&gt;%\n  gf_hline(yintercept = 0) %&gt;%\n  gf_theme(theme_void())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt should be immediately clear that the second pattern above is the same above and below the horizontal line; it exhibits horizontal mirror symmetry, \\(f(-t) = f(t)\\).\nUnder what conditions would a pattern be symmetric about an arbitrarily-tilted mirror, a mirror at angle \\(\\alpha\\) say?\nFrom Farris:\n\n\n\n\n\n\nNoteMirror at Angle \\(\\alpha\\)\n\n\n\nWhen every coefficient is a real multiple of \\(e^{i\\alpha}\\), the curve satisfies \\(f(-t) = e^{2\\alpha*i}f(t)\\).\nThe right-hand side is the correct expression for reflection across the line through the origin inclined at angle \\(\\alpha\\)(Check!!). If one wants curves with slanted mirrors, simply finds a curve symmetric about the x-axis satisfying \\(f(-t) = f(t)\\), and tilts it by \\(\\alpha\\).",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"ph:circles-three-fill\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Circles"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#fun-extras-to-try",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#fun-extras-to-try",
    "title": "\n Circles",
    "section": "Fun Extras to Try",
    "text": "Fun Extras to Try\nIt would be cool to simply develop the equations for any pattern in complex notation as in Equation 4 and throw that into code, without the tedious conversions into sines and cosines. Can we try that?\nHere is an example in R:\nShow the Codet &lt;- seq(0, 2 * pi, by = 0.001)\nx &lt;- t\n## NOTE: need the minus sign here inside the exponential!!\n## AND Absolutely need the \"1\" here before the solitary \"i\"!!\n## Need to figure these out\nf1 &lt;- function(x) {\n  (exp(-(0 + 1i) * x) +\n    0.25 * exp(-(0 + 6i) * x) +\n    0.2 * exp(-(0 + 11i) * x))\n}\nplot(f1(x), asp = 1)\n##\nf2 &lt;- function(x) {\n  (exp(-(0 + 2i) * x) +\n    0.2222 * exp(-(0 + 9i) * x) +\n    0.125 * exp(-(0 + 16i) * x) -\n    0.4 * exp(-(0 - 5i) * x) -\n    0.1667 * exp(-(0 - 12i) * x))\n}\nplot(f2(x), asp = 1)\n## Plotting with Exponential Functions\nf3 &lt;- function(x) {\n  (exp(-(0 + 1i) * x) +\n    0.5 * exp(-(0 + 6i) * x) +\n    1 / 3 * exp(-(0 - 14i) * x)\n  )\n}\nplot(f3(x), asp = 1)\n##\nf4 &lt;- function(x) {\n  (exp(-(0 + 1i) * x) +\n    0.5 * exp(-(0 + 6i) * x) +\n    1 / 3 * exp(-(0 + 14i) * x)\n  )\n}\nplot(f4(x), asp = 1)\n##\n# ## remainder = +2 from 5\n# ## frequencies 0+2, 5+2, -15+2\n# ## Coefficients 1, 1, 1\nf5 &lt;- function(x) {\n  (exp(-(0 + 2i) * x) +\n    1.0 * exp(-(0 + 7i) * x) +\n    1.0 * exp(-(0 + 13i) * x)\n  )\n}\nplot(f5(x), asp = 1)\n##\n# ## remainder = +2 from 5\n# ## frequencies 0+2, 5+2, -15+2\n# ## Coefficients 1, -1/2, -i/3 ( Note!!!)\nf6 &lt;- function(x) {\n  (exp(-(0 + 2i) * x) +\n    -0.5 * exp(-(0 + 7i) * x) +\n    -1 / 3 * exp(pi / 2 * 1i) * exp(-(0 + 13i) * x)\n  )\n}\nplot(f6(x), asp = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the Code# t &lt;- seq(0, 2 * pi, by = 0.001) # Already computed\ndata1 &lt;- tibble(t, pattern = f1(t))\ndata2 &lt;- tibble(t, pattern = f2(t))\ndata3 &lt;- tibble(t, pattern = f3(t))\ndata4 &lt;- tibble(t, pattern = f4(t))\ndata5 &lt;- tibble(t, pattern = f5(t))\ndata6 &lt;- tibble(t, pattern = f6(t))\n\ndata1 %&gt;%\n  gf_point(Im(pattern) ~ Re(pattern)) %&gt;%\n  gf_refine(coord_equal()) %&gt;%\n  gf_theme(theme_void())\ndata2 %&gt;%\n  gf_point(Im(pattern) ~ Re(pattern)) %&gt;%\n  gf_refine(coord_equal()) %&gt;%\n  gf_theme(theme_void())\ndata3 %&gt;%\n  gf_point(Im(pattern) ~ Re(pattern)) %&gt;%\n  gf_refine(coord_equal()) %&gt;%\n  gf_theme(theme_void())\ndata4 %&gt;%\n  gf_point(Im(pattern) ~ Re(pattern)) %&gt;%\n  gf_refine(coord_equal()) %&gt;%\n  gf_theme(theme_void())",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"ph:circles-three-fill\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Circles"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#comparing-exponential-and-trigonometric-functions",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#comparing-exponential-and-trigonometric-functions",
    "title": "\n Circles",
    "section": "Comparing Exponential and Trigonometric Functions",
    "text": "Comparing Exponential and Trigonometric Functions\nJust for practice, let us once more be clear between the complex exponential notation, and the parametric trigonometric functions.\nShow the Code# ## remainder = +2 from 5\n# ## frequencies 0+2, 5+2, -15+2\n# ## Coefficients 1, 1, 1\nf7 &lt;- function(x) {\n  (exp(-(0 + 2i) * x) + exp(-(0 + 7i) * x) + exp(-(0 + 13i) * x))\n}\n\n### Parametric Coordinates Tibble\ndata7a &lt;- tibble::tibble(\n  t = seq(0, 2 * pi, 0.001),\n  x = cos((2) * t) + cos(7 * t) + cos(13 * t),\n  y = sin(2 * t) + sin(7 * t) + sin(13 * t)\n)\n### Complex Exponential Tibble\ndata7b &lt;- tibble(t, pattern = f7(t))\n### Plots\nplot(f7(x),\n  asp = 1, cex = 0.2,\n  main = \"Base R: Exponential Function Plot\"\n)\n###\ndata7a %&gt;%\n  gf_point(y ~ x,\n    title = \"ggFormula: Trigonometric Function Plot\"\n  ) %&gt;%\n  gf_refine(coord_equal()) %&gt;%\n  gf_theme(theme_void())\n###\ndata7b %&gt;%\n  gf_point(Im(pattern) ~ Re(pattern),\n    title = \"ggFormula: Exponential Function Plot\"\n  ) %&gt;%\n  gf_refine(coord_equal()) %&gt;%\n  gf_theme(theme_void())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the Code# ## remainder = +2 from 5\n# ## frequencies 0+2, 5+2, -15+2\n# ## Coefficients 1, -1/2, -i/3 ( Note!!!)\nf8 &lt;- function(x) {\n  (exp(-(0 + 2i) * x) - 0.5 * exp(-(0 + 7i) * x) +\n    1i / 3 * exp((0 + 13i) * x))\n}\n\ndata8a &lt;- tibble::tibble(\n  t = seq(0, 2 * pi, 0.001),\n  x = cos(2 * t) - 0.5 * cos(7 * t) +\n    0.3 * cos(-13 * t + pi / 2),\n  y = sin(2 * t) - 0.5 * sin(7 * t) +\n    0.3 * sin(-13 * t + pi / 2)\n)\ndata8b &lt;- tibble(t, pattern = f8(t))\n###\nplot(f8(x),\n  asp = 1, cex = 0.2,\n  main = \"Base R: Exponential Function Plot\"\n)\n###\ndata8a %&gt;%\n  gf_point(y ~ x, title = \"ggFormula: Trigonometric Function Plot\") %&gt;%\n  gf_refine(coord_equal()) %&gt;%\n  gf_theme(theme_void())\n###\ndata8b %&gt;%\n  gf_point(Im(pattern) ~ Re(pattern),\n    title = \"ggFormula: Exponential Function Plot\"\n  ) %&gt;%\n  gf_refine(coord_equal()) %&gt;%\n  gf_theme(theme_void())",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"ph:circles-three-fill\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Circles"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#wait-but-why",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#wait-but-why",
    "title": "\n Circles",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\nSums of Complex exponentials are very common in mathematics and show up in many places: here, with symmetry, with Fourier Series, with Sound synthesis and Analysis.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"ph:circles-three-fill\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Circles"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#conclusion",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#conclusion",
    "title": "\n Circles",
    "section": "\n Conclusion",
    "text": "Conclusion\nWe have seen the close relationship between complex rotating exponentials and their trigonometric decompositions, embodied in the Euler Formula.\nWe also saw how multiple such exponentials can be used to combine using complex weighting to create symmetric patterns.\nAnd how symmetry depends upon the frequencies of the exponentials having a very specific relationship using modulo arithmetic.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"ph:circles-three-fill\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Circles"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#your-turn",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#your-turn",
    "title": "\n Circles",
    "section": "Your Turn",
    "text": "Your Turn\nCan you reverse engineer these curves, in R or in p5.js?\n\n\nFrom The Math Less Travelled Blog",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"ph:circles-three-fill\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Circles"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/10-Circles/index.html#references",
    "title": "\n Circles",
    "section": "\n References",
    "text": "References\n\nFrank Farris. Creating Symmetry: The Artful Mathematics of Wallpaper Patterns. Princeton University Press (2 June 2015).\nDoga Kurkcuoglu. https://bilimneguzellan.net/en/follow-up-to-fourier-series-2/. Look at some very cool animations here!\nGorilla Sun Blog. https://www.gorillasun.de/blog/parametric-functions-and-particles/\n\nCrateCode: Complex Generative Art with p5.js. https://cratecode.com/info/p5js-generative-art-complex-functions\n\nGorilla Sun Blog. https://www.gorillasun.de/blog/parametric-functions-and-particles/\n\nBrent Yorgey.(2015). The Math Less Travelled Blog. Random Cylic Curves. https://mathlesstraveled.wordpress.com/2015/06/04/random-cyclic-curves-5/\n\nUniversity of New South wales. Exponential Sums Page. https://www.unsw.edu.au/science/our-schools/maths/our-school/spotlight-on-our-people/history-school/glimpses-mathematics-and-statistics/exponential-sums\n\nJohn Myles White. Complex Numbers in R. https://www.johnmyleswhite.com/notebook/2009/12/18/using-complex-numbers-in-r/\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\nggformula\n0.12.0\nKaplan and Pruim (2023)\n\n\n\n\n\nKaplan, Daniel, and Randall Pruim. 2023. ggformula: Formula Interface to the Grammar of Graphics. https://doi.org/10.32614/CRAN.package.ggformula.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"ph:circles-three-fill\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Circles"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/42-AffineFractals/index.html",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/42-AffineFractals/index.html",
    "title": "\n Affine Transformation Fractals",
    "section": "",
    "text": "Warning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\n\n\n\n\n\n\nThis is a mathematically created fern! It uses, (gasp!) repeated matrix multiplication and addition!\nWe’ll see.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"mdi:reiterate\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Affine Transformation Fractals"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/42-AffineFractals/index.html#inspiration",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/42-AffineFractals/index.html#inspiration",
    "title": "\n Affine Transformation Fractals",
    "section": "",
    "text": "Warning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): no font could\nbe found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\n\n\n\n\n\n\nThis is a mathematically created fern! It uses, (gasp!) repeated matrix multiplication and addition!\nWe’ll see.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"mdi:reiterate\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Affine Transformation Fractals"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/42-AffineFractals/index.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/42-AffineFractals/index.html#introduction",
    "title": "\n Affine Transformation Fractals",
    "section": "Introduction",
    "text": "Introduction\nThe self-similarity of fractals suggests that we could create new fractals from a basic shape using the following procedure:\n\nStart with a basic shape, e.g. a rectangle\nDefine a set of transformations: scaling / mirroring / translation / combination (say n scaled+rotated replicates)\nRun these transformations on the basic shape\nFeed the output back to the input ( Classic IFR )\nWait for the pattern to emerge.\n\nSee the figure below to get an idea of this process.\n\n\n\n\n\nFigure 1: Emerging Fractal\n\n\nWell, this works, provided the transformations include significant amounts of scaling (i.e. reduction in size). You can imagine that if the basic shape does not shrink fast enough, the pattern converges very slowly and would remain chunky even after a large number of iterations.\nSecondly, the number of operations quickly becomes exponentially high, as each stage creates n-fold computation increase. Indeed, if we run \\(d\\) iterations, then the computations scale as \\(n^d\\), which can very quickly become out of hand!!\nSo what to do? Just like with the DeepSeek-R1 algorithm that simplified a great deal of AI algorithms, we have recourse to what is called the Barnsley Algorithm. NOTE: especially note the terrific pictures on this stackexchange page!\nFirst let us understand what are Affine Transformations and then build our fractals.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"mdi:reiterate\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Affine Transformation Fractals"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/42-AffineFractals/index.html#what-is-an-affine-transformation",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/42-AffineFractals/index.html#what-is-an-affine-transformation",
    "title": "\n Affine Transformation Fractals",
    "section": "What is an Affine Transformation?",
    "text": "What is an Affine Transformation?\nAffine Transformations are defined as a transformations of a space that are:\n\nlinear (no nonlinear functions of an x-coordinate, say \\(e^x\\))\nreversible.\n\nAffine transformations can be represented by matrices which multiply the coordinates of a shape in space. Multiple transformations can be understood a series of matrix multiplications, and can indeed be collapsed into a SINGLE matrix multiplication of the coordinates of the shape.\nSee this webpage at Mathigon to get an idea of rigid transformations of shape.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"mdi:reiterate\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Affine Transformation Fractals"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/42-AffineFractals/index.html#some-examples-of-affine-transformations",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/42-AffineFractals/index.html#some-examples-of-affine-transformations",
    "title": "\n Affine Transformation Fractals",
    "section": "Some Examples of Affine Transformations",
    "text": "Some Examples of Affine Transformations\n\nHere are some short videos of affine transformations:\n\n\n\n\n\n\n\n\nFigure 2: Scaling Along X\n\n\n\n\n\n\n\n\n\nFigure 3: Scaling Along Y\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Shearing Along X\n\n\n\n\n\n\n\n\n\nFigure 5: Shearing Along Y\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Translation Along X\n\n\n\n\n\n\n\n\n\nFigure 7: Translation Along Y",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"mdi:reiterate\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Affine Transformation Fractals"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/42-AffineFractals/index.html#designing-with-affine-transformations",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/42-AffineFractals/index.html#designing-with-affine-transformations",
    "title": "\n Affine Transformation Fractals",
    "section": "Designing with Affine Transformations",
    "text": "Designing with Affine Transformations\nSo how do we use these Affine Transformations? Let us paraphrase what Gary William Flake says in his book The Computational Beauty of Nature:\nIf \\(p\\) is a point in space, and its affine transformation(s) is \\(L(p\\)), then:\n\nIf \\(p\\) is on the final fractal, then so is \\(L(p)\\);\nIf \\(p\\) is not part of the final fractal, then \\(L(p)\\) will be atleast closer to the final fractal than \\(p\\) itself.\n\nThese ideas give us our final algorithm for designing a fractal with affine transformations.\n\nStart with any point \\(p\\)\n\nPick a (set of) Affine transformations \\(L_i(p)\\) that allow us to imagine the final shape\nTake the affine transformation \\(L_i(p)\\) of point \\(p\\). Choose \\(i\\) at random!\nUse an IFR: pipe the result back into the input\nMake a large number of iterations\nPlot all intermediate points that come out of the IFR\n\nWith this approach, the points rapidly land up on the fractal which builds up over multiple iterations. We can start anywhere in space and it will still converge.\nThe additional feature of the Barnsley algorithm is the randomness: since most fractals use not one but several affine transformations to create a multiplicity of forms, at each iteration we can randomly choose between them!\nThe block diagram of the Barnsley Algorithm looks like this:\n\n\n\n\n\nUsing p5.js\nUsing R\n\n\n\n\n\nHow to understand this sketch? Here is Dan Shiffman again!\n\n\n\nIn the code below, the Affine transformations \\(Af_i\\) are of the form\n\\[\nAF_i = A_i * X + B_i, ~ i = 1...4\n\\tag{1}\\]\nwith four options each for matrix \\(A\\) and matrix \\(B\\), and \\(X = (x,y)\\), the current point coordinates (seed input, then output feedback for recursion). There are 50000 iterations performed and at each interation, a random A and a random B are picked to provide the Affine Transformation for that iteration.\nThe starting “seed point” is simply \\(X = (0,0)\\).\nThe probabilities with which each affine transformation is chosen are not all equal; these can be tweaked to see the effect on the fractal.\nThe four options for the \\(A_i\\) matrices are:\nShow the CodeA &lt;- vector(mode = \"list\", length = 4)\n# Four Affine translation Matrices\nA[[1]] &lt;- matrix(c(0, 0, 0, 0.18), nrow = 2)\nA[[2]] &lt;- matrix(c(0.85, -0.04, 0.04, 0.85), nrow = 2)\nA[[3]] &lt;- matrix(c(0.2, 0.23, -0.26, 0.22), nrow = 2)\nA[[4]] &lt;- matrix(c(-0.15, 0.36, 0.28, 0.24), nrow = 2)\nas_sym(A[[1]])\nas_sym(A[[2]])\nas_sym(A[[3]])\nas_sym(A[[4]])\n\n\n\n\nc: ⎡0   0  ⎤\n   ⎢       ⎥\n   ⎣0  0.18⎦\nc: ⎡0.85   0.04⎤\n   ⎢           ⎥\n   ⎣-0.04  0.85⎦\nc: ⎡0.2   -0.26⎤\n   ⎢           ⎥\n   ⎣0.23  0.22 ⎦\nc: ⎡-0.15  0.28⎤\n   ⎢           ⎥\n   ⎣0.36   0.24⎦\n\n\n\n\n\n$$\n\\[\\begin{bmatrix}\n0.00 & 0.00 \\\\\n0.00 & 0.18 \\\\\n\\end{bmatrix}\\]\n$$ {#eq-A1}\n$$\n\\[\\begin{bmatrix}\n0.85 &  0.04 \\\\\n-0.04 &  0.85 \\\\\n\\end{bmatrix}\\]\n$$\n$$\n\\[\\begin{bmatrix}\n0.20 & -0.26 \\\\\n0.23 &  0.22 \\\\\n\\end{bmatrix}\\]\n$$\n$$\n\\[\\begin{bmatrix}\n-0.15 &  0.28 \\\\\n0.36 &  0.24 \\\\\n\\end{bmatrix}\\]\n$$\n\n\n\\[\n\\mathbf{X} = \\mathbf{U} \\mathbf{\\Lambda} \\mathbf{V}\n\\tag{2}\\]\n\n\n$$\n\\[\\begin{bmatrix}\n0.00 & 0.00 \\\\\n0.00 & 0.18 \\\\\n\\end{bmatrix}\\]\n$$ {#eq-A1}\n\nAnd the four options for the \\(B_i\\) matrices are:\n\nShow the Code# Four Simple translation Matrices\nb &lt;- vector(mode = \"list\", length = 4)\nb[[1]] &lt;- matrix(c(0, 0))\nb[[2]] &lt;- matrix(c(0, 1.6))\nb[[3]] &lt;- matrix(c(0, 1.6))\nb[[4]] &lt;- matrix(c(0, 0.54))\nas_sym(b[[1]])\nas_sym(b[[2]])\nas_sym(b[[3]])\nas_sym(b[[4]])\n\nc: [0  0]ᵀ\nc: [0  1.6]ᵀ\nc: [0  1.6]ᵀ\nc: [0  0.54]ᵀ\n\n\nBy randomly choosing any of the \\(16\\) resulting transformations, with different but fixed probablilities, we compute and render the Barnsley fern:\n\nShow the Code# Iteratively build the fern\ntheme_set(theme_custom())\n#\nn &lt;- 50000\nx &lt;- numeric(n)\ny &lt;- numeric(n)\nx[1] &lt;- 0\ny[1] &lt;- 0 # Starting point (0,0). Can be anything!\n\nfor (i in 1:(n - 1)) {\n  # Randomly sample the 4 + 4 translations based on a probability\n  # Change these to try different kinds of ferns\n  trans &lt;- sample(1:4, prob = c(.02, .9, .09, .08), size = 1)\n\n  # Translate **current** xy based on the selected translation\n  # Apply one of 16 possible affine transformations\n  xy &lt;- A[[trans]] %*% c(x[i], y[i]) + b[[trans]]\n  x[i + 1] &lt;- xy[1] # Save x component\n  y[i + 1] &lt;- xy[2] # Save y component\n}\n# Plot this baby\n# plot(y,x,col= \"pink\",cex=0.1)\ngf_point(y ~ x,\n  colour = \"lightgreen\", size = 0.02,\n  title = \"Barnsley Fern\"\n)\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, :\nUnable to calculate text width/height (using zero)\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : no\nfont could be found for family \"Roboto Condensed\"\n\n\n\n\n\n\n\n\n\nShow the CodeX &lt;- matrix(c(5, 5), nrow = 2)\nas_sym(A[[1]])\nas_sym(X)\nas_sym(A[[1]]) %*% as_sym(X)\n\nc: ⎡0   0  ⎤\n   ⎢       ⎥\n   ⎣0  0.18⎦\nc: [5  5]ᵀ\nc: [0  0.9]ᵀ\n\nVertical movement with Shrinkage\n\n\nShow the CodeX &lt;- matrix(c(5, 5), nrow = 2)\nas_sym(A[[2]])\nas_sym(X)\nas_sym(A[[2]]) %*% as_sym(X)\n\nc: ⎡0.85   0.04⎤\n   ⎢           ⎥\n   ⎣-0.04  0.85⎦\nc: [5  5]ᵀ\nc: [4.45  4.05]ᵀ\n\nModest Shrinkage of Both X and Y, X more than Y\n\n\nShow the CodeX &lt;- matrix(c(5, 5), nrow = 2)\nas_sym(A[[3]])\nas_sym(X)\nas_sym(A[[3]]) %*% as_sym(X)\n\nc: ⎡0.2   -0.26⎤\n   ⎢           ⎥\n   ⎣0.23  0.22 ⎦\nc: [5  5]ᵀ\nc: [-0.3  2.25]ᵀ\n\nLarge Shrinkage of Both X and Y, Y more than X\n\n\nShow the CodeX &lt;- matrix(c(5, 5), nrow = 2)\nas_sym(A[[4]])\nas_sym(X)\nas_sym(A[[4]]) %*% as_sym(X)\n\nc: ⎡-0.15  0.28⎤\n   ⎢           ⎥\n   ⎣0.36   0.24⎦\nc: [5  5]ᵀ\nc: [0.65  3.0]ᵀ\n\nShrinkage of Both X and Y, X more than Y",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"mdi:reiterate\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Affine Transformation Fractals"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/42-AffineFractals/index.html#wait-but-why",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/42-AffineFractals/index.html#wait-but-why",
    "title": "\n Affine Transformation Fractals",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\nOK, so why did this become a fern??\nIf we look at the list of affine transformations, we see that there are essentially 4 movements possible: https://en.wikipedia.org/wiki/Barnsley_fern\n\na simple vertical y-axis movement, with shrinkage\na gentle rotation with very little shrinkage\na rotation to the right with shrinkage\na rotation to the left with shrinkage\n\nThe second transformation is the one most commonly used!! The others are relatively rarely used! So the points slowly slope to the right and do now get squashed up close to the start: they retain sufficient size in (x,y) coordinates for the fern to slowly spread to the right.\nSo we can design the affine transformations based on an intuition of how we might draw the fractal by hand, say larger strokes to the right, smaller to the left etc, and and decide on the frequency of strokes based on how often these strokes might be used in drawing.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"mdi:reiterate\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Affine Transformation Fractals"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/25-Geometry/42-AffineFractals/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/25-Geometry/42-AffineFractals/index.html#references",
    "title": "\n Affine Transformation Fractals",
    "section": "\n References",
    "text": "References\n\nRyan Bradley-Evans. (Oct 7, 2020). Barnsley’s Fern Fractal in R. https://astro-ryan.medium.com/barnsleys-fern-fractal-in-r-e52a357e23db\n\nAffine Transformations @ The Algorithm Archive. https://www.algorithm-archive.org/contents/affine_transformations/affine_transformations.html\n\nIterated Function systems @ The Algorithm Archivehttps://www.algorithm-archive.org/contents/IFS/IFS.html\n\np5.js Tutorial: Coordinates and Transformations. https://p5js.org/tutorials/coordinates-and-transformations/\n\nThe Coding Train: Algorithmic Botany. https://thecodingtrain.com/tracks/algorithmic-botany\n\nBarnsley Fern @ Wikipedia https://en.wikipedia.org/wiki/Barnsley_fern\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\ncaracas\n2.1.1\nAndersen and Højsgaard (2023)\n\n\nmatlib\n1.0.0\nFriendly, Fox, and Chalmers (2024)\n\n\n\n\n\n\nAndersen, Mikkel Meyer, and Søren Højsgaard. 2023. caracas: Computer Algebra. https://doi.org/10.32614/CRAN.package.caracas.\n\n\nFriendly, Michael, John Fox, and Phil Chalmers. 2024. matlib: Matrix Functions for Teaching and Learning Linear Algebra and Multivariate Statistics. https://doi.org/10.32614/CRAN.package.matlib.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Geometry",
      "<iconify-icon icon=\"mdi:reiterate\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> <iconify-icon icon=\"gravity-ui:function\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Affine Transformation Fractals"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/500-Tech/listing.html",
    "href": "content/courses/MathModelsDesign/Modules/500-Tech/listing.html",
    "title": "Tools and Tech",
    "section": "",
    "text": "Title\n\n\n\nReading Time\n\n\n\n\n\n\n\n\nTools and Installation\n\n\n2 min\n\n\n\n\n\n\nThe Open Sound Protocol\n\n\n1 min\n\n\n\n\n\n\nAdding Libraries to p5.js\n\n\n1 min\n\n\n\n\n\n\nUsing Constructor Objects in p5.js\n\n\n1 min\n\n\n\n\n\n\nNo matching items\n Back to top",
    "crumbs": [
      "Math Models for Creative Coders",
      "Tech"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/500-Tech/05-Tools/index.html",
    "href": "content/courses/MathModelsDesign/Modules/500-Tech/05-Tools/index.html",
    "title": "Tools and Installation",
    "section": "",
    "text": "We will predomnantly use p5.js in this course, since most of use have had some exposure to it and it is of course easy to acquire skill in p5.js rapidly. (Some R code may also be demonstrated.)",
    "crumbs": [
      "Math Models for Creative Coders",
      "Tech",
      "Tools and Installation"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/500-Tech/05-Tools/index.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/500-Tech/05-Tools/index.html#introduction",
    "title": "Tools and Installation",
    "section": "",
    "text": "We will predomnantly use p5.js in this course, since most of use have had some exposure to it and it is of course easy to acquire skill in p5.js rapidly. (Some R code may also be demonstrated.)",
    "crumbs": [
      "Math Models for Creative Coders",
      "Tech",
      "Tools and Installation"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/500-Tech/05-Tools/index.html#the-p5.js-web-editor",
    "href": "content/courses/MathModelsDesign/Modules/500-Tech/05-Tools/index.html#the-p5.js-web-editor",
    "title": "Tools and Installation",
    "section": "The p5.js web editor",
    "text": "The p5.js web editor\nThe best way to use p5.js is to use its web editor:\np5js web editor",
    "crumbs": [
      "Math Models for Creative Coders",
      "Tech",
      "Tools and Installation"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/500-Tech/05-Tools/index.html#installing-p5.js-if-you-have-to",
    "href": "content/courses/MathModelsDesign/Modules/500-Tech/05-Tools/index.html#installing-p5.js-if-you-have-to",
    "title": "Tools and Installation",
    "section": "Installing p5.js (if you have to)",
    "text": "Installing p5.js (if you have to)\nOne may also install p5.js, for situations when one is not online, e.g. for stand-alone projects and public installations that do not have internet access, like a Cat that has swallowed a 8Raspberry Pi that makes cat calls at passersby at the metro, if you want to go that far. We will then also have to install a web-server so that we can see our code outputs locally.\n\nHead off to https://p5js.org/download/ and download the Complete Library. This is a .zip file that you should extract to a folder named p5 within your ~/Documents folder.\nYour p5 folder will look like this:\n\n\n\n\n\n\nFigure 1: p5 folder",
    "crumbs": [
      "Math Models for Creative Coders",
      "Tech",
      "Tools and Installation"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/500-Tech/05-Tools/index.html#installing-visual-studio-code",
    "href": "content/courses/MathModelsDesign/Modules/500-Tech/05-Tools/index.html#installing-visual-studio-code",
    "title": "Tools and Installation",
    "section": "Installing Visual Studio Code",
    "text": "Installing Visual Studio Code\n\nHead off to https://code.visualstudio.com/download and choose the appropriate file for your OS.\n\n\n\n\n\n\nNoteMac people\n\n\n\nCheck whether you have Apple silicon ( M1/M2/M3…) and choose appropriately. The universal version of the software also seems worth trying.\n\n\nOpen VS Code and click on View -&gt; Explorer. Navigate to your p5 folder and open it. This will allow you to edit all files there and keep track of other files within this “project” folder.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Tech",
      "Tools and Installation"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/500-Tech/05-Tools/index.html#installing-a-local-webserver",
    "href": "content/courses/MathModelsDesign/Modules/500-Tech/05-Tools/index.html#installing-a-local-webserver",
    "title": "Tools and Installation",
    "section": "Installing a local WebServer",
    "text": "Installing a local WebServer\nWhen we edit p5.js code within VS Code, we will want to see the resultant HTML rendering, since p5.js puts out an HTML file everytime. We need to bind this output to the browser using a VSCode Extension called, Live Server.\n\nOpen the VS Code extension manager (CTRL-SHIFT-X / CMD-SHIFT-X)\nSearch for and install the Live Server extension.\n\nAdd a p5.js project folder to your VS Code Workspace. You have already done this.\nWith your project’s index.html or sketch.js file open, start the Live Server using the “Go Live” button in the status bar, or by using ALT-L ALT-O.\nYour sketch should now open in your default browser at location: 127.0.0.1:5500. Browser window usually pops up.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Tech",
      "Tools and Installation"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/500-Tech/05-Tools/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/500-Tech/05-Tools/index.html#references",
    "title": "Tools and Installation",
    "section": "References",
    "text": "References\n\nThe Coding Train set of video tutorials https://www.youtube.com/@TheCodingTrain\nDan Shiffman. The Nature of Code book. https://natureofcode.com\nCodeAcademy. p5.js short cheatsheet. https://www.codecademy.com/learn/learn-p5js/modules/p5js-introduction-to-creative-coding/cheatsheet",
    "crumbs": [
      "Math Models for Creative Coders",
      "Tech",
      "Tools and Installation"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/500-Tech/30-Constructors/index.html",
    "href": "content/courses/MathModelsDesign/Modules/500-Tech/30-Constructors/index.html",
    "title": "Using Constructor Objects in p5.js",
    "section": "",
    "text": "p5.js allows us to use Javascript features such as Objects. An Object in p5.js is a form of data + a set of things you can do with that data, what are called methods. Think of an Object as a spanner in your toolkit, which can tell you the gauge/size of mechanical components and also do specific things with these.\nWhen would we need to use these?\nSome libraries in p5.js, such as p5.sound create Oscillator objects, which can generate sounds with different frequencies. And these have methods associated with them, such as turning them on/off, modifying their amplitudes and frequencies (“modulations”).\nWe can also create our own Objects which can include some methods of their own.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Tech",
      "Using Constructor Objects in p5.js"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/500-Tech/30-Constructors/index.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/500-Tech/30-Constructors/index.html#introduction",
    "title": "Using Constructor Objects in p5.js",
    "section": "",
    "text": "p5.js allows us to use Javascript features such as Objects. An Object in p5.js is a form of data + a set of things you can do with that data, what are called methods. Think of an Object as a spanner in your toolkit, which can tell you the gauge/size of mechanical components and also do specific things with these.\nWhen would we need to use these?\nSome libraries in p5.js, such as p5.sound create Oscillator objects, which can generate sounds with different frequencies. And these have methods associated with them, such as turning them on/off, modifying their amplitudes and frequencies (“modulations”).\nWe can also create our own Objects which can include some methods of their own.",
    "crumbs": [
      "Math Models for Creative Coders",
      "Tech",
      "Using Constructor Objects in p5.js"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/500-Tech/30-Constructors/index.html#examples-of-objects",
    "href": "content/courses/MathModelsDesign/Modules/500-Tech/30-Constructors/index.html#examples-of-objects",
    "title": "Using Constructor Objects in p5.js",
    "section": "Examples of Objects",
    "text": "Examples of Objects\nLet us see an example of a built-in Object type offered by p5.js. Then we will look at the way in which we can create Objects of our own.\n\nObjects in the p5.sound library\n\n\nCreating our own Obects",
    "crumbs": [
      "Math Models for Creative Coders",
      "Tech",
      "Using Constructor Objects in p5.js"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/500-Tech/30-Constructors/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/500-Tech/30-Constructors/index.html#references",
    "title": "Using Constructor Objects in p5.js",
    "section": "References",
    "text": "References\n\nDan Shiffman, of course:\n\n\n\nhttps://www.cs.cmu.edu/~tcortina/15104-f20/lectures/24-MoreSound.pdf",
    "crumbs": [
      "Math Models for Creative Coders",
      "Tech",
      "Using Constructor Objects in p5.js"
    ]
  },
  {
    "objectID": "readme.html",
    "href": "readme.html",
    "title": "Applied Math for Creative Coders",
    "section": "",
    "text": "Preparatory Work to moving my full website to Quarto!"
  },
  {
    "objectID": "readme.html#get-started-with-quarto",
    "href": "readme.html#get-started-with-quarto",
    "title": "Applied Math for Creative Coders",
    "section": "",
    "text": "Preparatory Work to moving my full website to Quarto!"
  }
]