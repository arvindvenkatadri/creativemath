[
  {
    "objectID": "readme.html",
    "href": "readme.html",
    "title": "The Foundation Series",
    "section": "",
    "text": "Preparatory Work to moving my full website to Quarto!"
  },
  {
    "objectID": "readme.html#get-started-with-quarto",
    "href": "readme.html#get-started-with-quarto",
    "title": "The Foundation Series",
    "section": "",
    "text": "Preparatory Work to moving my full website to Quarto!"
  },
  {
    "objectID": "content/work-related/fsp-portfolio-2022/index.html",
    "href": "content/work-related/fsp-portfolio-2022/index.html",
    "title": "The Foundation Series",
    "section": "",
    "text": "This is a short Portfolio of Teaching Initiatives and Student Outcomes during this post(?)-pandemic year, 2021-2022, from Arvind Venkatadri.\n\n\n\n Back to top"
  },
  {
    "objectID": "content/work-related/fsp-manifesto/index.html",
    "href": "content/work-related/fsp-manifesto/index.html",
    "title": "My Teaching Manifesto",
    "section": "",
    "text": "This is a short Statement of Values, Beliefs, and Content in my Teaching.\n\nArvind Venkatadri.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#instructions",
    "href": "content/work-related/DSU/A1.html#instructions",
    "title": "A1",
    "section": "Instructions",
    "text": "Instructions\n\nEach Question in this Assignment is a chart.\nEach Chart is accompanied by a set of short questions.\nYour responses to these can be R-code, or text.\nPlease number your answers as 1.a, 1.b, 1.c…..2.a, 2.b…on your Answer Sheet.\nAll aboard? Let’s go!"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-1",
    "href": "content/work-related/DSU/A1.html#question-1",
    "title": "A1",
    "section": "Question 1",
    "text": "Question 1\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-1-1",
    "href": "content/work-related/DSU/A1.html#question-1-1",
    "title": "A1",
    "section": "Question 1",
    "text": "Question 1\n\n\n\nWhat is the ggplot geometry used in this graph?\nWhat do the colours mean?"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-2",
    "href": "content/work-related/DSU/A1.html#question-2",
    "title": "A1",
    "section": "Question 2",
    "text": "Question 2\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-3",
    "href": "content/work-related/DSU/A1.html#question-3",
    "title": "A1",
    "section": "Question 3",
    "text": "Question 3\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-4",
    "href": "content/work-related/DSU/A1.html#question-4",
    "title": "A1",
    "section": "Question 4",
    "text": "Question 4\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-5",
    "href": "content/work-related/DSU/A1.html#question-5",
    "title": "A1",
    "section": "Question 5",
    "text": "Question 5\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-6",
    "href": "content/work-related/DSU/A1.html#question-6",
    "title": "A1",
    "section": "Question 6",
    "text": "Question 6\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-7",
    "href": "content/work-related/DSU/A1.html#question-7",
    "title": "A1",
    "section": "Question 7",
    "text": "Question 7\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-8",
    "href": "content/work-related/DSU/A1.html#question-8",
    "title": "A1",
    "section": "Question 8",
    "text": "Question 8\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-9",
    "href": "content/work-related/DSU/A1.html#question-9",
    "title": "A1",
    "section": "Question 9",
    "text": "Question 9\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-10",
    "href": "content/work-related/DSU/A1.html#question-10",
    "title": "A1",
    "section": "Question 10",
    "text": "Question 10\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-11",
    "href": "content/work-related/DSU/A1.html#question-11",
    "title": "A1",
    "section": "Question 11",
    "text": "Question 11\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-12",
    "href": "content/work-related/DSU/A1.html#question-12",
    "title": "A1",
    "section": "Question 12",
    "text": "Question 12\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-13",
    "href": "content/work-related/DSU/A1.html#question-13",
    "title": "A1",
    "section": "Question 13",
    "text": "Question 13\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-14",
    "href": "content/work-related/DSU/A1.html#question-14",
    "title": "A1",
    "section": "Question 14",
    "text": "Question 14\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-15",
    "href": "content/work-related/DSU/A1.html#question-15",
    "title": "A1",
    "section": "Question 15",
    "text": "Question 15\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment#1-IA-EMBA-T4-2023, Arvind Venkatadri"
  },
  {
    "objectID": "content/slides/r-slides/spatial/PlayingwithLeaflet.html",
    "href": "content/slides/r-slides/spatial/PlayingwithLeaflet.html",
    "title": "Playing with Leaflet",
    "section": "",
    "text": "This Tutorial works through the ideas at Leaflet\n\nLeaflet is a JavaScript library for creating dynamic maps that support panning and zooming along with various annotations like markers, polygons, and popups.\n\nIn this tutorial we will work only with vector data. In a second part, we will work with raster data in leaflet.\n\nlibrary(tidyverse)\nlibrary(leaflet)\nlibrary(maps)\nlibrary(sp)\nlibrary(sf)\n\n# Data\nlibrary(osmdata) # Import OSM Vector Data into R\nlibrary(osmplotr) # Creating maps with OSM data in R\n# library(OpenStreetMap) # Raster Data"
  },
  {
    "objectID": "content/slides/r-slides/spatial/PlayingwithLeaflet.html#add-shapes-to-a-map",
    "href": "content/slides/r-slides/spatial/PlayingwithLeaflet.html#add-shapes-to-a-map",
    "title": "Playing with Leaflet",
    "section": "Add Shapes to a Map",
    "text": "Add Shapes to a Map\nleaflet offers several commands to add points, markers, icons, lines, polylines and polygons to a map. Let us examine a few of these.\nAdd Markers with popups\n\nm %&gt;% addMarkers(lng = 77.580643, lat = 12.972442, \n                 popup = \"The birthplace of Rvind\")\n\n\n\n\n# Click on the Marker for the popup to appear\n\nThis uses the default pin shape as the Marker.\nAdding Popups to a Map\nPopups are small boxes containing arbitrary HTML, that point to a specific point on the map. Use the addPopups() function to add standalone popup to the map.\n\nm %&gt;%\n  addPopups(\n    lng = 77.580643,\n    lat = 12.972442,\n    popup = paste(\n      \"The birthplace of Rvind\",\n      \"&lt;br&gt;\",\n      \"Website: https://the-foundation-series.netlify.app\",\n      \"&lt;br&gt;\"\n    ),\n    \n    # Ensuring we cannot close the popup, else we will not be able to find where it is, since there is no Marker\n    options = popupOptions(closeButton = FALSE)\n  )\n\n\n\n\n\nPopups are usually added to icons, Markers and other shapes can show up when these are clicked.\nAdding Labels to a Map\nLabels are messages attached to all shapes, using the argument label wherever it is available.\nLabels are static, and Popups are usually visible on mouse click. Hence a Marker can have both a label and a popup. For example, the function addPopup() offers only a popup argument, whereas the function addMarkers() offers both a popup and a label argument.\nIt is also possible to create labels standalone using addLabelOnlyMarkers() where we can show only text and no Markers.\n\nm %&gt;%\n  addMarkers(\n    lng = 77.580643,\n    lat = 12.972442,\n    \n    # Here is the Label defn.\n    label = \"The birthplace of Rvind\",\n    labelOptions = labelOptions(noHide = TRUE, # Label always visible\n                                textOnly = F, \n                                textsize = 20),\n    \n    # And here is the popup defn.\n    popup = \"This is the Popup Text\"\n  )\n\n\n\n\n\nAdding Circles and CircleMarkers on a Map\nWe can add shapes on to a map to depict areas or locations of interest. NOTE: the radius argument works differently in addCircles() and addCircleMarkers().\n\n# Some Cities in the US and their location\nmd_cities &lt;- tibble(\n  name = c(\"Baltimore\",\"Frederick\",\"Rockville\",\"Gaithersburg\",\"Bowie\",\"Hagerstown\",\"Annapolis\",\"College Park\",\"Salisbury\",\"Laurel\"),\n  pop = c(619493,66169,62334,61045,55232,39890,38880,30587,30484,25346),\n  lat = c(39.2920592,39.4143921,39.0840,39.1434,39.0068,39.6418,38.9784,38.9897,38.3607,39.0993), \n  lng = c(-76.6077852,-77.4204875,-77.1528,-77.2014,-76.7791,-77.7200,-76.4922,-76.9378,-75.5994,-76.8483)\n)\n\n\nmd_cities %&gt;%\n  leaflet() %&gt;%\n  addTiles() %&gt;%\n  \n  # CircleMarkers, in blue\n  # radius scales the Marker. Units are in Pixels!!\n  # Here, radius is made proportional to `pop` number\n  addCircleMarkers(radius = ~ pop/1000, # Pixels!!\n                   color = \"blue\",\n                   stroke = FALSE, # no border for the Markers\n                   opacity = 0.8) %&gt;% \n  \n  \n  # Circles, in red\n  addCircles(\n    radius = 5000, # Meters !!!\n    stroke = TRUE,\n    color = \"yellow\", # Stroke Colour\n    weight = 3, # Stroke Weight\n    fill = TRUE,\n    fillColor = \"red\",\n\n  )\n\n\n\n\n\nThe shapes need not be of fixed size or colour; their attributes can be made to correspond to other attribute variables in the geospatial data, as we did with radius in the addCircleMarkers() function above.\nAdding Rectangles to a Map\n\n## Adding Rectangles\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  setView(lng = 77.580643, lat = 12.972442, zoom = 6) %&gt;% \n  addRectangles(lat1 = 10.3858, lng1 = 75.0595, \n                lat2 = 12.8890, lng2 = 77.9625)\n\n\n\n\n\nAdd Polygons to a Map\n\n## Adding Polygons\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  setView(lng = 77.580643, lat = 12.972442, zoom = 6) %&gt;% \n  \n  # arbitrary vector data for lat and lng\n   addPolygons(lng = c(73.5, 75.9, 76.1, 77.23, 79.8),\n               lat =c(10.12, 11.04, 11.87, 12.04, 10.7))\n\n\n\n\n\nAdd PolyLines to a Map\nThis can be useful say for manually marking a route on a map, with waypoints.\n\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  setView(lng = 77.580643, lat = 12.972442, zoom = 6) %&gt;% \n  \n  # arbitrary vector data for lat and lng\n  # If start and end points are the same, it looks like Polygon\n  # Without the fill\n   addPolylines(lng = c(73.5, 75.9, 76.1, 77.23, 79.8),\n               lat =c(10.12, 11.04, 11.87, 12.04, 10.7)) %&gt;% \n  \n  # Add Waypoint Icons\n  addMarkers(lng = c(73.5, 75.9, 76.1, 77.23, 79.8),\n               lat =c(10.12, 11.04, 11.87, 12.04, 10.7))\n\n\n\n\n\nAs seen, we have created Markers, Labels, Polygons, and PolyLines using fixed.i.e. literal text and numbers. In the following we will also see how external geospatial data columns can be used instead of these literals.\nNOTE: The mapedit package https://r-spatial.org//r/2017/01/30/mapedit_intro.html can also be used to interactively add shapes onto a map and save as an geo-spatial object."
  },
  {
    "objectID": "content/slides/r-slides/spatial/PlayingwithLeaflet.html#point-data-sources-for-leaflet",
    "href": "content/slides/r-slides/spatial/PlayingwithLeaflet.html#point-data-sources-for-leaflet",
    "title": "Playing with Leaflet",
    "section": "Point Data Sources for leaflet\n",
    "text": "Point Data Sources for leaflet\n\nPoint data for markers can come from a variety of sources:\n\n\nSpatialPoints or SpatialPointsDataFrame objects (from the sp package)\n\n\nPOINT, sfc_POINT, and sf objects (from the sf package); only X and Y dimensions will be considered\n\nTwo-column numeric matrices (first column is longitude, second is latitude)\n\n\nData frame/tibble with latitude and longitude columns. You can explicitly tell the marker function which columns contain the coordinate data (e.g. addMarkers(lng = ~Longitude, lat = ~Latitude)), or let the function look for columns named lat/latitude and lon/lng/long/longitude (case insensitive).\n\nSimply provide numeric vectors as lng and lat arguments, which we have covered already in the preceding sections.\n\nNote that MULTIPOINT objects from sf are not supported at this time.\nWe will not consider the use of sp related data structures for plotting POINTs in leaflet since sp is being phased out in favour of the more modern package sf.\nPoints using simple Data Frames\nLet us read in the data set from data.world that gives us POINT locations of all airports in India in a data frame / tibble. The dataset is available at https://query.data.world/s/ahtyvnm2ybylf65syp4rsb5tulxe6a. You can either download it, save a copy, and read it in as usual, or use the URL itself to read it in from the web. In the latter case, you will need the package data.world and also need to register your credentials for that page with RStudio. The (simple!) instructions are available here at data.world.\n\n#library(devtools)\n#devtools::install_github(\"datadotworld/data.world-r\", build_vignettes = TRUE)\n\nlibrary(data.world)\n\nindia_airports &lt;-\n  read_csv(\"https://query.data.world/s/ahtyvnm2ybylf65syp4rsb5tulxe6a\") %&gt;% \n  slice(-1) %&gt;% # Drop the first row which contains labels\n  dplyr::mutate(\n    id = as.integer(id),\n    latitude_deg = as.numeric(latitude_deg),\n    longitude_deg = as.numeric(longitude_deg),\n    elevation_ft = as.integer(elevation_ft)\n  ) %&gt;% \n  rename(\"lon\" = longitude_deg, \"lat\" = latitude_deg) %&gt;% \n  # Remove four locations which seem to be in the African Atlantic\n  filter(!id %in% c(330834, 330867, 325010, 331083))\n\nindia_airports %&gt;% head()\n\n\n\n  \n\n\n\nLet us plot this in leaflet, using an ESRI National Geographic style map instead of the OSM Base Map. We will also place small circle markers for each airport.\n\nleaflet(data = india_airports) %&gt;% \n  setView(lat = 18, lng = 77, zoom = 4) %&gt;% \n  \n  # Add NatGeo style base map\n  addProviderTiles(providers$Esri.NatGeoWorldMap) %&gt;% # ESRI Basemap\n  \n  # Add Markers for each airport\n  addCircleMarkers(lng = ~lon, lat = ~lat,\n                   # Optional, variables stated for clarity\n                   # leaflet can automatically detect lon-lat columns\n                   # if they are appropriately named in the data\n                   # longitude/lon/lng\n                   # latitude/lat\n                   radius = 2, # Pixels\n                   color = \"red\",\n                   opacity = 1)\n\n\n\n\n\nWe can also change the icon for each airport. Let us try one of theseveral icon families that we can use with leaflet : glyphicons, ionicons, and fontawesome icons.\n\n# Define popup message for each airport\n# Based on data in india_airports\npopup &lt;- paste(\n  \"&lt;strong&gt;\",\n  india_airports$name,\n  \"&lt;/strong&gt;&lt;br&gt;\",\n  india_airports$iata_code,\n  \"&lt;br&gt;\",\n  india_airports$municipality,\n  \"&lt;br&gt;\",\n  \"Elevation(feet)\",\n  india_airports$elevation_ft,\n  \"&lt;br&gt;\",\n  india_airports$wikipedia_link,\n  \"&lt;br&gt;\"\n)\n\niata_icon &lt;- makeIcon(\n  \"iata-logo-transp.png\", # Downloaded from www.iata.org\n  iconWidth = 24,\n  iconHeight = 24,\n  iconAnchorX = 0,\n  iconAnchorY = 0\n)\n\n# Create the Leaflet map\nleaflet(data = india_airports) %&gt;%\n  setView(lat = 18, lng = 77, zoom = 4) %&gt;%\n  addProviderTiles(providers$Esri.NatGeoWorldMap) %&gt;%\n  addMarkers(\n    icon = iata_icon,\n    popup = popup\n  )\n\n\n\n\n\nThere are other icons we can use to mark the POINTs. leaflet allows the use of ionicons, glyphicons, and FontAwesomeIcons\nIt is possible to create a list of icons, so that different Markers can have different icons. Let us try to map the MNCs in the ITPL area of Bangalore: we use the ideas in Using Leaflet Markers @JLA-Data.net\n\n# Make a dataframe of addresses of Companies we wan to plot in ITPL\ncompanies_itpl &lt;-\n  data.frame(\n    ticker = c(\n      \"MBRDI\",\n      \"DTICI\",\n      \"IBM\",\n      \"Exxon\",\n      \"Mindtree\",\n      \"FIS Global\",\n      \"Sasken\",\n      \"LTI\"),\n    lat = c(\n      12.986178620989264,\n      12.984160906190121,\n      12.983659088566357,\n      12.985112265986636,\n      12.983794997606187,\n      12.980658616215155,\n      12.982080447350246,\n      12.981338168875348),\n    lon = c(\n      77.7270652183105,\n      77.72808445774321,\n      77.73103488768001,\n      77.72935046040699,\n      77.7227844126931,\n      77.72685064158782,\n      77.72545589289041,\n      77.72287024338216)\n  ) %&gt;% sf::st_as_sf(coords = c(\"lon\", \"lat\"), crs = 4326)\n \n# Vanilla leaflet map\nleaflet(companies_itpl) %&gt;% \n  addTiles() %&gt;% \n  addMarkers()\n\n\n\n\n\nLet us make a list of logos of the Companies and use them as markers!\n\n# a named list of rescaled icons with links to images\nfavicons &lt;- iconList(\n  \"MBRDI\" = makeIcon(\n    iconUrl = \"https://www.mercedes-benz.com/etc/designs/brandhub/frontend/static-assets/header/logo.svg\", \n    iconWidth = 25,\n    iconHeight = 25\n  ),\n  \"DTICI\" = makeIcon(\n    iconUrl = \"https://media-exp1.licdn.com/dms/image/C4D0BAQGzOep26lC03w/company-logo_200_200/0/1638298367374?e=2147483647&v=beta&t=mPyF4gvNhNFvd-tedbqNzJofq4q9qcw6A9z9jQeLAwc\",\n    iconWidth = 45,\n    iconHeight = 45\n  ),\n  \"IBM\" = makeIcon(\n    iconUrl = \"https://www.ibm.com/favicon.ico\",\n    iconWidth = 25,\n    iconHeight = 25\n  ),\n  \"Exxon\" = makeIcon(\n    iconUrl = \"https://corporate.exxonmobil.com/-/media/Global/Icons/logos/ExxonMobilLogoColor2x.png\",\n    iconWidth = 45,\n    iconHeight = 25\n  ),\n  \"Mindtree\" = makeIcon(\n    iconUrl = \"https://www.mindtree.com/themes/custom/mindtree_theme/mindtree-lnt-logo-png.png\",\n    iconWidth = 75,\n    iconHeight = 25\n  ),\n  \"FIS Global\" = makeIcon(\n    iconUrl = \"https://1000logos.net/wp-content/uploads/2021/09/FIS-Logo-768x432.png\",\n    iconWidth = 25,\n    iconHeight = 25\n  ),\n  \"Sasken\" = makeIcon(\n    iconUrl = \"https://www.sasken.com/sites/all/themes/sasken_website/logo.png\",\n    iconWidth = 35,\n    iconHeight = 35,\n  ),\n  \"LTI\" = makeIcon(\n    iconUrl = \"https://www.lntinfotech.com/wp-content/uploads/2021/09/LTI-logo.svg\",\n    iconWidth = 25,\n    iconHeight = 25\n  )\n)\n\n\n# Create the Leaflet map\n\nleaflet(companies_itpl) %&gt;% \n  addMarkers(icon = ~ favicons[ticker], # lookup based on ticker\n             label = ~ companies_itpl$ticker,\n             labelOptions = labelOptions(noHide = F,offset = c(15,-25))) %&gt;%\n  addProviderTiles(\"CartoDB.Positron\")\n\n\n\n\n\nPoints using sf objects\nWe will use data from an sf data object. This differs from the earlier situation where we had a simple data frame with lon and lat columns. In sf, the lon and lat info is embedded in the geometry column of the sf data frame.\nThe tmap package has a data set of all World metro cities, titled metro. We will plot these on the map and also scale the markers in proportion to one of the feature attributes, pop2030. The popup will be the name of the metro city. We will also use the CartoDB.Positron base map.\nNote that the metro data set has a POINT geometry, as needed!\n\ndata(metro, package = \"tmap\")\nmetro\n\n\n\n  \n\n\nleaflet(data = metro) %&gt;% \n  setView(lat = 18, lng = 77, zoom = 4) %&gt;% \n  \n  # Add CartoDB.Positron\n  addProviderTiles(providers$CartoDB.Positron) %&gt;% # CartoDB Basemap\n  \n  # Add Markers for each airport\n  addCircleMarkers(radius = ~ sqrt(pop2030)/350,\n                   color = \"red\",\n                   popup = paste(\"Name: \", metro$name, \"&lt;br&gt;\",\n                                  \"Population 2030: \", metro$pop2030))\n\n\n\n\n\nWe can also try downloading an sf data frame with POINT geometry from say OSM data&lt;https://osm. Let us get hold of restaurants data in Malleswaram, Bangalore from OSM data:\n\nbbox&lt;- osmdata::getbb(\"Malleswaram, Bengaluru\")\nbbox\n\n       min      max\nx 77.55033 77.59033\ny 12.98274 13.02274\n\nlocations &lt;- osmplotr::extract_osm_objects(\n  bbox = bbox,\n  key = \"amenity\",\n  value = \"restaurant\",\n  return_type = \"point\") \n\nlocations &lt;- locations %&gt;% \n  dplyr::filter(cuisine == \"indian\")\nlocations %&gt;% head()\n\n\n\n  \n\n\n# Fontawesome icons seem to work in `leaflet` only up to FontAwesome V4.7.0.\n# The Fontawesome V4.7.0 Cheatsheet is here: &lt;https://fontawesome.com/v4/cheatsheet/&gt;\n\n\nleaflet(data = locations, options = leafletOptions(minZoom = 12)) %&gt;% \n  \n  addProviderTiles(providers$CartoDB.Voyager) %&gt;% \n  \n  # Regular `leaflet` code\n  addAwesomeMarkers(icon = awesomeIcons(icon = \"fa-coffee\", \n                                        library = \"fa\",\n                                        markerColor = \"blue\",\n                                        iconColor = \"black\",\n                                        iconRotate = TRUE),\n                     popup = paste(\"Name: \", locations$name,\"&lt;br&gt;\",\n                           \"Food: \", locations$cuisine)) \n\n\n\n\n\nFontawesome Workaround\nFor more later versions of Fontawesome, here below is a workaround from https://github.com/rstudio/leaflet/issues/691. Despite this some fontawesome icons simply do not seem to show up. ;-()\n\nlibrary(fontawesome)\ncoffee &lt;- makeAwesomeIcon(\n  text = fa(\"mug-hot\"), # mug-hot was introduced in fa version 5\n  iconColor = \"black\",\n  markerColor = \"blue\",\n  library = \"fa\"\n)\n\n\nleaflet(data = locations) %&gt;% \n  addProviderTiles(providers$CartoDB.Voyager) %&gt;% \n  \n  # Workaround code\n\n  addAwesomeMarkers(icon = coffee,\n             popup = paste(\"Name: \", locations$name,\"&lt;br&gt;\",\n                           \"Food: \", locations$cuisine, \"&lt;br&gt;\"))\n\n\n\n\n\nNote that leaflet automatically detects the lon/lat columns from within the POINT geometry column of the sf data frame.\nPoints using Two-Column Matrices\nWe can now quickly try providing lon and lat info in a two column matrix.This can be useful to plot a bunch of points recorded on a mobile phone app.\n\nmysore5 &lt;- matrix(c(runif(5, 76.652985-0.01, 76.652985+0.01),\n                 runif(5, 12.311827-0.01, 12.311827+0.01)),\n                 nrow = 5)\nmysore5\n\n         [,1]     [,2]\n[1,] 76.66267 12.32061\n[2,] 76.65384 12.32126\n[3,] 76.65767 12.30852\n[4,] 76.64444 12.31647\n[5,] 76.66274 12.32073\n\nleaflet(data = mysore5) %&gt;% \n  addProviderTiles(providers$OpenStreetMap) %&gt;% \n  \n# Pick an icon from &lt;https://www.w3schools.com/bootstrap/bootstrap_ref_comp_glyphs.asp&gt;\n  addAwesomeMarkers(icon = awesomeIcons(\n  icon = 'music',\n  iconColor = 'black',\n  library = 'glyphicon'),\n  popup = \"Carnatic Music !!\")"
  },
  {
    "objectID": "content/slides/r-slides/spatial/PlayingwithLeaflet.html#polygons-lines-and-polylines-data-sources-for-leaflet",
    "href": "content/slides/r-slides/spatial/PlayingwithLeaflet.html#polygons-lines-and-polylines-data-sources-for-leaflet",
    "title": "Playing with Leaflet",
    "section": "Polygons, Lines, and Polylines Data Sources for leaflet\n",
    "text": "Polygons, Lines, and Polylines Data Sources for leaflet\n\nWe have seen how to get POINT data into leaflet.\nLine and polygon data can come from a variety of sources:\n\n\nSpatialPolygons, SpatialPolygonsDataFrame, Polygons, and Polygon objects (from the sp package)\n\n\nSpatialLines, SpatialLinesDataFrame, Lines, and Line objects (from the sp package)\n\n\nMULTIPOLYGON, POLYGON, MULTILINESTRING, and LINESTRING objects (from the sf package)\n\n\nmap objects (from the maps package’s map() function); use map(fill = TRUE) for polygons, FALSE for polylines\n\nTwo-column numeric matrix; the first column is longitude and the second is latitude. Polygons are separated by rows of (NA, NA). It is not possible to represent multi-polygons nor polygons with holes using this method; use SpatialPolygons instead.\n\nWe will concentrate on using sf data into leaflet. We may explore maps() objects at a later date.\nPolygons/MultiPolygons and LineString/MultiLineString using sf data frames\nLet us download College buildings, parks, and the cycling lanes in Amsterdam, Netherlands, and plot these in leaflet.\n\nbbox &lt;- osmdata::getbb(\"Amsterdam, Netherlands\")\nbbox\n\n        min       max\nx  4.728756  5.079162\ny 52.278174 52.431064\n\ncolleges &lt;- osmplotr::extract_osm_objects(bbox = bbox,\n                                           key = \"amenity\",\n                                           value = \"college\",\n                                           return_type = \"polygon\" )\nparks &lt;- osmplotr::extract_osm_objects(bbox = bbox,\n                                           key = \"park\",\n                                           return_type = \"polygon\" )\nroads &lt;- osmplotr::extract_osm_objects(bbox = bbox,\n                                       key = \"highway\",\n                                       value = \"primary\",\n                                       return_type = \"line\")\ncyclelanes &lt;-\n  osmplotr::extract_osm_objects(bbox,\n                                key = \"cycleway\",\n                                value =  \"lane\",\n                                return_type = \"line\")\n\nWe have 23 colleges in our data and 378 parks in our data.\n\nleaflet() %&gt;% \n  addTiles() %&gt;% \n  addPolygons(data = colleges, popup = ~colleges$name) %&gt;% \n  addPolygons(data = parks, color = \"green\", popup = parks$name) %&gt;% \n  addPolylines(data = roads, color = \"red\") %&gt;% \n  addPolylines(data = cyclelanes, color = \"purple\")"
  },
  {
    "objectID": "content/slides/r-slides/spatial/PlayingwithLeaflet.html#chapter-3-using-raster-data-in-leaflet",
    "href": "content/slides/r-slides/spatial/PlayingwithLeaflet.html#chapter-3-using-raster-data-in-leaflet",
    "title": "Playing with Leaflet",
    "section": "Chapter 3: Using Raster Data in leaflet\n",
    "text": "Chapter 3: Using Raster Data in leaflet\n\nSo far all the geospatial data we have plotted in leaflet has been vector data. We will now explore how to plot raster data using leaflet. Raster data are used to depict continuous variables across space, such as vegitation, salinity, forest cover etc. Satellite imagery is frequently available as raster data.\nImporting Raster Data [Work in Progress!]\nRaster data can be imported into R in many ways:\n\nusing the maptiles package\n\nusing the OpenStreetMap package\n\n\nlibrary(terra)\nlibrary(maptiles)\n#library(OpenStreetMap) # causes RStudio to crash..."
  },
  {
    "objectID": "content/slides/r-slides/spatial/PlayingwithLeaflet.html#adding-legendswork-in-progress",
    "href": "content/slides/r-slides/spatial/PlayingwithLeaflet.html#adding-legendswork-in-progress",
    "title": "Playing with Leaflet",
    "section": "Adding Legends[Work in Progress!]",
    "text": "Adding Legends[Work in Progress!]\n\n## Generate some random lat lon data around Bangalore\ndf &lt;- data.frame(lat = runif(20, min = 11.97, max = 13.07),\n                 lng = runif(20, min = 77.48, max = 77.68),\n                 col = sample(c(\"red\", \"blue\", \"green\"), 20, \n                              replace = TRUE),\n                 stringsAsFactors = FALSE)\n\ndf %&gt;%\n  leaflet() %&gt;%\n  addTiles() %&gt;%\n  addCircleMarkers(color = df$col) %&gt;%\n  addLegend(values = df$col, labels = LETTERS[1:3], colors = c(\"blue\", \"red\", \"green\"))"
  },
  {
    "objectID": "content/slides/r-slides/networks/LICENSE.html",
    "href": "content/slides/r-slides/networks/LICENSE.html",
    "title": "MIT License",
    "section": "",
    "text": "MIT License\nCopyright (c) 2022 quarto-letterbox authors\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#what-makes-human-experience",
    "href": "content/slides/r-slides/nature/new.html#what-makes-human-experience",
    "title": "The Nature of Data",
    "section": "What makes Human Experience?",
    "text": "What makes Human Experience?\n()\nHow would we begin to describe this experience?\n\n\n\nWhere / When?\nWho?\nHow?\nHow Big? How small? How frequent? How sudden?\n\n\n\n\n\nAnd….How Surprising ! How Shocking! How sad…How Wonderful !!!\nSo: Our Questions, and our Surprise lead us to creating Human Experiences."
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#does-this-surprise-you",
    "href": "content/slides/r-slides/nature/new.html#does-this-surprise-you",
    "title": "The Nature of Data",
    "section": "Does this Surprise you?",
    "text": "Does this Surprise you?"
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#the-element-of-surprise",
    "href": "content/slides/r-slides/nature/new.html#the-element-of-surprise",
    "title": "The Nature of Data",
    "section": "The Element of Surprise?",
    "text": "The Element of Surprise?\n\n\n\n\n\nJane Austen knew a lot about human information processing as these snippets from Pride and Prejudice (published in 1813 – over 200 years ago) show :\n\nShe was a woman of mean understanding, little information , and uncertain temper.\nCatherine and Lydia had information for them of a different sort.\nWhen this information was given, and they had all taken their seats, Mr. Collins was at leisure to look around him and admire,…\nYou could not have met with a person more capable of giving you certain information on that head than myself, for I have been connected with his family in a particular manner from my infancy.\nThis information made Elizabeth smile, as she thought of poor Miss Bingley.\nThis information, however, startled Mrs. Bennet …\nhttps://www.cs.bham.ac.uk/research/projects/cogaff/misc/austen-info.html"
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#claude-shannon-and-information",
    "href": "content/slides/r-slides/nature/new.html#claude-shannon-and-information",
    "title": "The Nature of Data",
    "section": "Claude Shannon and Information",
    "text": "Claude Shannon and Information\n\nhttps://plus.maths.org/content/information-surprise"
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#human-experience-is.data",
    "href": "content/slides/r-slides/nature/new.html#human-experience-is.data",
    "title": "The Nature of Data",
    "section": "Human Experience is….Data??",
    "text": "Human Experience is….Data??"
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#experiments-and-hypotheses-a-kitchen-experiment",
    "href": "content/slides/r-slides/nature/new.html#experiments-and-hypotheses-a-kitchen-experiment",
    "title": "The Nature of Data",
    "section": "Experiments and Hypotheses: A Kitchen Experiment",
    "text": "Experiments and Hypotheses: A Kitchen Experiment\n\n\nInputs are: Ingredients, Recipes, Processes\nOutputs are: Taste, Texture, Colour, Quantity!!"
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#what-is-the-result-of-an-experiment",
    "href": "content/slides/r-slides/nature/new.html#what-is-the-result-of-an-experiment",
    "title": "The Nature of Data",
    "section": "What is the Result of an Experiment?",
    "text": "What is the Result of an Experiment?\n\n\n\nAll experiments give us data about phenomena\nWe obtain data about the things that happen: Outputs\nWhat makes things happen?: Inputs\nHow?: Process\nWhen? Factors\nHow much “output” is caused by how much “input”? Effect Size\n\n\nAll Experiments stem from - Human Curiosity, - a Hypothesis, and - a Desire to Find out and Talk about Something"
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#a-famous-lady-and-her-famous-experiment",
    "href": "content/slides/r-slides/nature/new.html#a-famous-lady-and-her-famous-experiment",
    "title": "The Nature of Data",
    "section": "A Famous Lady and her Famous Experiment",
    "text": "A Famous Lady and her Famous Experiment\n\n\n\n\n\n\n\n\n\n\nIn 1853, Turkey declared war on Russia. After the Russian Navy destroyed a Turkish squadron in the Black Sea, Great Britain and France joined with Turkey. In September of the following year, the British landed on the Crimean Peninsula and set out, with the French and Turks, to take the Russian naval base at Sevastopol.\nWhat followed was a tragicomedy of errors – failure of supply, failed communications, international rivalries. Conditions in the armies were terrible, and disease ate through their ranks. They finally did take Sevastopol a year later, after a ghastly assault. It was ugly business all around. Well over half a million soldiers lost their lives during the Crimean War."
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#florence-nightingales-data",
    "href": "content/slides/r-slides/nature/new.html#florence-nightingales-data",
    "title": "The Nature of Data",
    "section": "Florence Nightingale’s Data",
    "text": "Florence Nightingale’s Data"
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#how-does-data-look-like-then",
    "href": "content/slides/r-slides/nature/new.html#how-does-data-look-like-then",
    "title": "The Nature of Data",
    "section": "How Does Data look Like, then?",
    "text": "How Does Data look Like, then?\n\n\n\n\nTypes of Variables - Using Interrogative Pronouns\n\nNominal: What? Who? Where? (Factors, Dimensions)\nOrdinal: Which Types? What Sizes? How Big? (Factors, Dimensions)\nInterval: How Often? (Numbers, Facts)\nRatio: How many? How much? How heavy? (Numbers, Facts)"
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#types-of-variables-in-nightingale-data",
    "href": "content/slides/r-slides/nature/new.html#types-of-variables-in-nightingale-data",
    "title": "The Nature of Data",
    "section": "Types of Variables in Nightingale Data",
    "text": "Types of Variables in Nightingale Data\n\n\nUsing Interrogative Pronouns\n\nNominal: None\nOrdinal: (Factors, Dimensions)\n\nHOW? War, Disease, Other\n\nInterval: (Numbers, Facts)\n\nWHEN? Year, Month\n\nRatio: (Numbers, Facts)\n\nHOW MANY? Rate of Deaths (War, Disease, Other)\n\n\n\n\n\n\ns\n\n\nhttps://arvindvenkatadri.com"
  },
  {
    "objectID": "content/slides/r-slides/graphics/LICENSE.html",
    "href": "content/slides/r-slides/graphics/LICENSE.html",
    "title": "MIT License",
    "section": "",
    "text": "MIT License\nCopyright (c) 2022 quarto-letterbox authors\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/LICENSE.html",
    "href": "content/slides/projects-slides/portfolio/LICENSE.html",
    "title": "MIT License",
    "section": "",
    "text": "MIT License\nCopyright (c) 2022 quarto-letterbox authors\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/projects/Resources-For-Order-and-Chaos/index.html",
    "href": "content/projects/Resources-For-Order-and-Chaos/index.html",
    "title": "Resources for Order and Chaos",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "content/projects/fsp-doe/index.html",
    "href": "content/projects/fsp-doe/index.html",
    "title": "A Design of Experiments Class",
    "section": "",
    "text": "Code Lab"
  },
  {
    "objectID": "content/projects/fsp-doe/index.html#slides-and-code-links",
    "href": "content/projects/fsp-doe/index.html#slides-and-code-links",
    "title": "A Design of Experiments Class",
    "section": "",
    "text": "Code Lab"
  },
  {
    "objectID": "content/projects/fsp-doe/index.html#fa-envelope-introduction",
    "href": "content/projects/fsp-doe/index.html#fa-envelope-introduction",
    "title": "A Design of Experiments Class",
    "section": " Introduction",
    "text": "Introduction\nThis is a brief description and analysis of a Design of Experiments module conducted as a part of the Order and Chaos course, in the Foundation Studies Program (FSP 2021-2022) at SMI, MAHE, Bangalore."
  },
  {
    "objectID": "content/projects/fsp-doe/index.html#context",
    "href": "content/projects/fsp-doe/index.html#context",
    "title": "A Design of Experiments Class",
    "section": "Context",
    "text": "Context\nA Short Term Memory(STM) Test was the investigative tool used to verify several Hypotheses that were documented on the subject of STM.\nThis article describes the statistical analysis that was done with the readings. In particular, Permutations Tests were used to verify the effect size for each of three parameters that were hypothesized.\nFor more information, please click on the icon above to look at the Lab document."
  },
  {
    "objectID": "content/projects/fsp-doe/index.html#references",
    "href": "content/projects/fsp-doe/index.html#references",
    "title": "A Design of Experiments Class",
    "section": "References",
    "text": "References\n\nLawrance, A. J. 1996. “A Design of Experiments Workshop as an Introduction to Statistics.” American Statistician 50 (2): 156–58. doi:10.1080/00031305.1996.10474364.\nErnst, Michael D. 2004. “Permutation Methods: A Basis for Exact Inference.” Statistical Science 19 (4): 676–85. doi:10.1214/088342304000000396.\nPruim R, Kaplan DT, Horton NJ (2017). “The mosaic Package: Helping Students to ‘Think with Data’ Using R.” The R Journal, 9(1), 77–102. https://journal.r-project.org/archive/2017/RJ-2017-024/index.html."
  },
  {
    "objectID": "content/projects/2023-07-02-Wildlife-Crossing/index.html",
    "href": "content/projects/2023-07-02-Wildlife-Crossing/index.html",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the Banff Wildlife Crossings",
    "section": "",
    "text": "Here we go with another of my TRIZ Chronicles ! The earlier editions are here: Lawrence of Arabia, Spotify, Great Bubble Barrier, and the OWind Turbine.\nThis is another piece stems from my teaching a course on Creative Thinking and Problem Solving based on TRIZ, titled Play and Invent, over the past 8 years or more at the Srishti Manipal Institute of Art, Design, and Technology and now at DSU School of Commerce & Management Studies, both in Bangalore, INDIA."
  },
  {
    "objectID": "content/projects/2023-07-02-Wildlife-Crossing/index.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "href": "content/projects/2023-07-02-Wildlife-Crossing/index.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the Banff Wildlife Crossings",
    "section": "",
    "text": "Here we go with another of my TRIZ Chronicles ! The earlier editions are here: Lawrence of Arabia, Spotify, Great Bubble Barrier, and the OWind Turbine.\nThis is another piece stems from my teaching a course on Creative Thinking and Problem Solving based on TRIZ, titled Play and Invent, over the past 8 years or more at the Srishti Manipal Institute of Art, Design, and Technology and now at DSU School of Commerce & Management Studies, both in Bangalore, INDIA."
  },
  {
    "objectID": "content/projects/2023-07-02-Wildlife-Crossing/index.html#iconify-maki-road-accident-where-did-the-bear-cross-the-road",
    "href": "content/projects/2023-07-02-Wildlife-Crossing/index.html#iconify-maki-road-accident-where-did-the-bear-cross-the-road",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the Banff Wildlife Crossings",
    "section": "\n Where did the Bear cross the Road?",
    "text": "Where did the Bear cross the Road?\n\nWildlife roadkill was a serious problem in Banff National Park in Canada, for both wildlife and motorists. The problem was tackled beautifully by Parks Canada using a system of tunnels and stunning natural-looking overpasses.\nWithout further ado, let us do a TRIZ Analysis of this remarkable set of inventions."
  },
  {
    "objectID": "content/projects/2023-07-02-Wildlife-Crossing/index.html#iconify-pajamas-issue-type-test-case-a-triz-analysis-of-the-banff-wildlife-crossings",
    "href": "content/projects/2023-07-02-Wildlife-Crossing/index.html#iconify-pajamas-issue-type-test-case-a-triz-analysis-of-the-banff-wildlife-crossings",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the Banff Wildlife Crossings",
    "section": "\n A TRIZ Analysis of the Banff Wildlife Crossings",
    "text": "A TRIZ Analysis of the Banff Wildlife Crossings\nFor a TRIZ workflow, we proceed as before:\n\nFirst, using the method described in Open Source TRIZ, we identify knobs or parameters within the situation\nWe see how turning these could lead to identifying a Statement / Cause for a Problem in the form of a Contradiction.\nRe-word the plain English Contradiction into TRIZ Parameters and look it up in the Contradiction Matrix. Obtain the Inventive Principles.\nApply these Inventive Principles into your Problem and solve it.\n\nHere below is a quick Ishikawa Diagram to help us identify the Parameters of this Problem:\n\n\n\n\n\nLooking at this Diagram, with the aspects identified, we could pair them off and see how they affect one another. In doing so, we could make up several problem statements. Let us state some of our Problems\n\n\nSignage would help drivers slow down, but slowing down may make journey less enjoyable.\n\nSlowing Down may improve animal movement but may endanger humans.\n\nClearing the Vegetation may make animals more visible, but may also make vehicles visible to animals and affect their movement\n\nAs you can see, many different problems and contradictions await our attention. Let us cut to the chase and state our Administrative Contradiction(AC) in plain English:\n\n\n\n\n\n\n Administrative Contradiction\n\n\n\nAC: We wish to drive at high speeds, but not kill migrating wild animals nor endanger our vehicles.\n\n\nWhat would an IFR be in this situation? How “unreasonable” can we be? Let us try:\n\n\n\n\n\n\n Ideal Final Result\n\n\n\nThe Animals and Humans should both use the Road whenever they want without being mutually affected!\n\n\nLet us take our AC and convert it into a Technical Contradiction(TC), keeping this IFR in mind. We will look at the 48 TRIZ Parameters in the TRIZ Contradiction Matrix and see which Parameter we want to improve, while not worsening another. Here is what we can obtain. We will analyze the Contradictions both ways1:\n\n\n\n\n\n\n Technical Contradictions\n\n\n\n\n\nTC1: Improve Loss of Time(26) and not worsen Duration of Action by Moving Object(12)\n\n\n\n\nSince our IFR is all about time, we have chosen the TRIZ Parameters that are time-oriented. We could have also tried the following:\n\n\nTC2: Improve Volume of Moving Object(7) and not worsen Loss of Time(26)\n\n\nTC3: Improve Other harmful factors Acting on the System(40) and not worsen Duration of Action by Stationary Object(13)2\n\nThese include Volume and External Factors which are not quite there in out IFR. Is there a Physical Contradiction(PC)3 possible here?\n\n\n\n\n\n\n\n Physical Contradiction\n\n\n\nIn fact our IFR is nearly worded as a PC: The Vehicles and the Animals must use the Road at the Same Time."
  },
  {
    "objectID": "content/projects/2023-07-02-Wildlife-Crossing/index.html#iconify-game-icons-multiple-targets-solving-the-technical-contradictions",
    "href": "content/projects/2023-07-02-Wildlife-Crossing/index.html#iconify-game-icons-multiple-targets-solving-the-technical-contradictions",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the Banff Wildlife Crossings",
    "section": "\n Solving the Technical Contradictions",
    "text": "Solving the Technical Contradictions\nLet us take the set of TC-s into the Contradiction Matrix and arrive at the list of TRIZ Inventive Principles. Here is what the Matrix suggests:\nFor TC1:\n\n3(Local Quality)\n17(Another Dimension)\n28(Mechanics Substitution)\n8(Anti-Weight); and\n19(Periodic Action),\n10(Preliminary Action)\n\nHmm…based on the PC, we may have expected a Separation in Space solution, suggested by Another Dimension and Local Quality. Viewing these Inventive Principles as we Generalized Solutions, we try to map these back into the Problem at hand. In keeping with the metaphoric/analogic way of thinking that TRIZ embodies, I deliberately use many visual hints here from math, physics, geography, and biology.\n\n3(Local Quality): So something that is local…local where? Well, along the highway, of course. So something that is located a specific points along the highway. Nice but not really clear enough to be actionable, yet.\n17(Another Dimension): Well, well. The Road is a linear thing and has length and breadth. What would we use for another dimension? Height, of course! So, we need to go either above the road or below! And that leads us to a …bridge and a tunnel !!!\n\nThe other Inventive Principles are, to me, not evocative enough in this instance. But we already have a decent idea: we could just imagine a set of Local bridges and tunnels that occur at Periodic Intervals along the highway. And that is exactly what Parks Canada have done.\n\nHere is the solution in action:"
  },
  {
    "objectID": "content/projects/2023-07-02-Wildlife-Crossing/index.html#iconify-fluent-mdl2-parameter-using-triz-separation-principles",
    "href": "content/projects/2023-07-02-Wildlife-Crossing/index.html#iconify-fluent-mdl2-parameter-using-triz-separation-principles",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the Banff Wildlife Crossings",
    "section": "\n Using TRIZ Separation Principles",
    "text": "Using TRIZ Separation Principles\nAs Hipple explains, there is frequently an underlying physical parameter, such as length, breadth, weight, or energy, or speed for example that lies at the root of our Technical Contradiction. Our IFR states that we want the humans and animals to use the road at the same TIME, and hence Separation in Space becomes a nice way to think of a solution.\nThat’s a wrap! In the next episode of the #TRIZ Chronicles, I wish to step even further out of my area of expertise and dabble in HR! I think looking at some of the institution-building ideas in Ricardo Semler’s book, Maverick would be a good idea!"
  },
  {
    "objectID": "content/projects/2023-07-02-Wildlife-Crossing/index.html#iconify-ooui-references-ltr-readings",
    "href": "content/projects/2023-07-02-Wildlife-Crossing/index.html#iconify-ooui-references-ltr-readings",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the Banff Wildlife Crossings",
    "section": "\n Readings",
    "text": "Readings\n\nhttps://discoverapega.ca/stories/wildlife-crossings-key-to-highway-safety-in-banff/\nJack Hipple, The Ideal Result and How to Achieve It. Springer; 2012th edition (June 26, 2012)\nValery Souchkov, Defining Contradictions. http://www.xtriz.com/Training/TRIZ_DefineContradiction_Tutorial.pdf\n\nOpen Source TRIZ: Making Contradictions. https://www.youtube.com/watch?v=cah0OhCH55k\n\nScrucca (2004)\nCano, Moguerza, and Redchuk (2012)"
  },
  {
    "objectID": "content/projects/2023-07-02-Wildlife-Crossing/index.html#footnotes",
    "href": "content/projects/2023-07-02-Wildlife-Crossing/index.html#footnotes",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the Banff Wildlife Crossings",
    "section": "Footnotes",
    "text": "Footnotes\n\nThe Contradiction Matrix is not quite symmetric, so stating the Contradiction both ways allows us to access a slightly larger set of Inventive Principles from two cells of the Matrix.↩︎\nAnimals are nearly stationary compared to the vehicles.↩︎\nArriving at Physical Contradictions is not always easy! If we can, then there are a very crisp set of TRIZ Separation Principles that we can apply to solve the Problem.↩︎"
  },
  {
    "objectID": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html",
    "href": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html",
    "title": "A TRIZ Analysis of Lawrence of Arabia",
    "section": "",
    "text": "The movie Lawrence of Arabia is justly famous for its terrific story, great cast of characters and actors, and some truly legendary scenes. Here I take one of the iconic scenes from the movie, the attack on the port town of Aqaba and interpret the entire event and the build up to it from a TRIZ viewpoint.\nI will give a short description of the movie plot first.\nI then discuss the source of the problem, how it occurred to Lawrence, and how he went about solving it in this dramatic fashion. I give an analysis of the Problem from a (classical) TRIZ perspective, including the formulation of the Contradiction, Identification of Causes, the statement of the Ideal Final Result, and finally using the TRIZ Contradiction Matrix to find Inventive Principles that inform Lawrence’s solution. I also dwell in passing upon aspects of how this story can be interpreted as a manifestation of Mihaly Csikszentmihalyi’s Creativity Systems Model.\nThis piece stems from my teaching a course on Creative Thinking and Problem Solving based on TRIZ, titled Play and Invent, over the past 8 years or more at the Srishti Manipal Institute of Art, Design, and Technology, Bangalore, India. (https://srishtimanipalinstitute.in)."
  },
  {
    "objectID": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html#introduction",
    "href": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html#introduction",
    "title": "A TRIZ Analysis of Lawrence of Arabia",
    "section": "",
    "text": "The movie Lawrence of Arabia is justly famous for its terrific story, great cast of characters and actors, and some truly legendary scenes. Here I take one of the iconic scenes from the movie, the attack on the port town of Aqaba and interpret the entire event and the build up to it from a TRIZ viewpoint.\nI will give a short description of the movie plot first.\nI then discuss the source of the problem, how it occurred to Lawrence, and how he went about solving it in this dramatic fashion. I give an analysis of the Problem from a (classical) TRIZ perspective, including the formulation of the Contradiction, Identification of Causes, the statement of the Ideal Final Result, and finally using the TRIZ Contradiction Matrix to find Inventive Principles that inform Lawrence’s solution. I also dwell in passing upon aspects of how this story can be interpreted as a manifestation of Mihaly Csikszentmihalyi’s Creativity Systems Model.\nThis piece stems from my teaching a course on Creative Thinking and Problem Solving based on TRIZ, titled Play and Invent, over the past 8 years or more at the Srishti Manipal Institute of Art, Design, and Technology, Bangalore, India. (https://srishtimanipalinstitute.in)."
  },
  {
    "objectID": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html#lawrence-of-arabia-a-summary",
    "href": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html#lawrence-of-arabia-a-summary",
    "title": "A TRIZ Analysis of Lawrence of Arabia",
    "section": "Lawrence of Arabia: a Summary",
    "text": "Lawrence of Arabia: a Summary\nThe movie is the story of T.E. Lawrence, the English officer who successfully united and led the diverse, often warring, Arab tribes during World War I in order to fight the Turks. The stellar cast includes Peter O’Toole as Lawrence, Omar Sharif as Ali, Alec Guinness as Prince Feisal, Anthony Quinn as Auda Abu Tayi, Claude Raines as Dryden, and Anthony Quayle as Col. Brighton. The director was David Lean. The editing of the film by Anne Coates is also much admired. (https://womenfilmeditors.princeton.edu/tag/lawrence-of-arabia/)\nLawrence is a complex, talented, and yet simple man, who is extremely well read (Greek philosophy and the Koran, for example) and is also an expert in Arab affairs and has considerable skill at map-making. Due to his being interpreted as insolent and insubordinate , he is given a lowly job at the HQ in Cairo. Dryden manages to convince the General that Lawrence should be allowed to go into Arabia and to find out what kind of long-term plans Prince Feisal is making for Arabia.\nHere is the map of the events that are unfolding in the movie at this time.1\n\n\n\n\n\nLawrence encounters Ali in dramatic fashion at the Masturah Well, on the way to meet Feisal, and his Arab guide is shot by Ali, a direct experience for Lawrence of inter-tribe rivalry in Arabia. (Ali is a Harith, and Tafas the guide was a Hashemi). Lawrence peremptorily rejects an offer of help from Ali, and finds his way alone to Wadi Safra, where Feisal is camped. He is met by Col. Brighton as he nears the camp. Both enter camp just in time to witness another bombing raid by Turkish airplanes.\nLater in the meeting with Feisal, Brighton tries to convince Feisal to retreat to Yenbo (Yanbu) and be out of range for the Turks, and where the British Army would supply them, train them to fight against the Turks. Feisal reluctantly accepts this plan, though he would rather the British navy take the port city of Aqaba and supply his army from there. Brighton simply scoffs at that idea, because the Turkish have 12 inch guns at Aqaba and the British have other things to do.\nLawrence has already intrigued Feisal by completing a verse from the Koran as it was being read by Selim, the cleric. At the end of the meeting, Feisal confronts Lawrence alone, as to his intentions in Arabia and finds out, to his astonishment, that Lawrence has his own interpretation of what his tasks and loyalties were, and these did not necessarily coincide with those of Brighton. In fact, Lawrence is not in favour of the Arab Army’s retreat to Yenbo, as it would become one small part of the British Army. As a parting remark, Feisal says to Lawrence that the Arabs need what no man can provide, a miracle.\nHere is the video of that terrific tent meeting scene."
  },
  {
    "objectID": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html#lawrences-problem",
    "href": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html#lawrences-problem",
    "title": "A TRIZ Analysis of Lawrence of Arabia",
    "section": "Lawrence’s Problem",
    "text": "Lawrence’s Problem\nLawrence does not sleep that night. Provoked by Feisal’s parting remark, he sits up all night on a sand dune close to the camp, thinking about how Aqaba could be taken, since he wants the Arabs to continue fighting from where they were, and even advance if possible with British help. His detailed understanding of the Arabian geography, his knowledge of the Aqaba port and its fortifications, all come to the fore here. Aqaba is a port at the head end of a narrow gulf to the east of the Sinai Peninsula.\nIn the early morning, seemingly in a eureka moment, he decides that attacking Aqaba from the landward side would be a good solution, since the guns there could not be turned around.\nHere is Lawrence trying to convince Ali about this plan:\n\n\n\n\nLawrence does not inform Brighton of his plans, nor even Feisal. It is Ali who informs Feisal of this enterprise. Clearly, Lawrence does not consider Brighton as a member of his Field (as defined by Csikszentmihalyi in his Creativity Systems Model), but Feisal is a Field Member to Ali.\nApropos, the act of sitting up all night can be seen as the Incubation and Elaboration stages of the 5 Stages of Creativity from Csikszentmihalyi (Preparation, Incubation, Insight, Elaboration, Execution)."
  },
  {
    "objectID": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html#a-triz-analysis-of-the-plan-to-take-aqaba",
    "href": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html#a-triz-analysis-of-the-plan-to-take-aqaba",
    "title": "A TRIZ Analysis of Lawrence of Arabia",
    "section": "A TRIZ Analysis of the Plan to Take Aqaba",
    "text": "A TRIZ Analysis of the Plan to Take Aqaba\nFor a TRIZ workflow, we proceed as follows:\nFirst, using the method described in Open Source TRIZ, (https://www.youtube.com/watch?v=cah0OhCH55k), we identify knobs or parameters within the situation and see how turning these could lead to identifying a Cause for a Problem in the form of a Contradiction.\nHere below is a quick Ishikawa Diagram for this purpose:\n\n\n\n\n\nTurning the knobs/parameters in the Ishikawa Diagram, it seems that if the British allies attack Aqaba, they may win, BUT they may lose a few warships. If the Arabs want to attack, they are too small in number and have no warships, and hence their chances of success are very slim. Herein lies the Contradiction, which we can now specify as an Administrative Contradiction(AC) in plain English:\n\n\nAC: The Arabs need the British to supply them via Aqaba port. Aqaba has huge guns and they will sink the British ships in that narrow gulf if they try a naval attack. So the Arabs need to take Aqaba without losing British ships.\n\n\nNext, based on this Contradiction and the inspection of the Ishikawa Diagram above, we are now ready to define the Ideal Final Result:\n\n\nIFR: The Arabs need to attack and take Aqaba port, and the big guns there should have no effect.\n\n\nNote how the tone of this IFR is like a “eat my cake and have it too”. Very typical for IFRs, this impossible-sounding tone!\nLet us take the AC and convert it into a Technical Contradiction(TC). We will look at the 48 TRIZ Parameters in the TRIZ Contradiction Matrix (PDF) and see which Parameter we want to improve, while not worsening another. Here is what we can attempt, stating the Contradiction both ways2:\n\n- TC 1: Increase Duration of Action of a Moving Object (12) and not worsen Stability of Objects Composition (21)- TC 2: Increase Stability of Objects Composition (21) and not worsen Duration of Action of a Moving Object (12)\n\nHere we choose these Parameters based on our IFR that the guns at Aqaba should not affect the Arab attack at all. The Parameters chosen from the TRIZ Matrix can be thought of as metaphors for the knobs that lie within our AC. Going from the AC to the TC is an act of making metaphors. We could easily have chosen the Parameter Power(18) or Illumination Intensity(23) to “metaphorize” the effectiveness of the attack, if our imaginations run in that direction. There is here a considerable flexibility and possibility for imaginative interpretations of the AC, but using the language of TRIZ.\nWe could even stretch to making a Physical Contradiction(PC)3 happen:\n\n\nPC: The Ships must be near the guns but not be near enough to be shot at. (They must be near and not near at the same time)"
  },
  {
    "objectID": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html#solving-the-technical-contradiction",
    "href": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html#solving-the-technical-contradiction",
    "title": "A TRIZ Analysis of Lawrence of Arabia",
    "section": "Solving the Technical Contradiction",
    "text": "Solving the Technical Contradiction\nLet us take the both the TC-s into the Contradiction Matrix and arrive at the list of TRIZ Inventive Principles. Here is the Matrix solution for TC-1 in the figure below:\n\n\n\n\n\nThe Inventive Principles are:(TC1)\n\nIP 13 (The Other Way Around)\n\nIP 35 (Parameter Change)\n\nIP 24 (Intermediary)\n\nIP 40 (Composite Materials)\n\nand (TC2)\n\nIP 10 (Preliminary Action)\n\nIP 5 (Merging)\n\nIP 35 (Parameter Change)\n\nIP 13 (The Other Way Around)\n\nHow are we to apply these Inventive Principles? Here again is an imaginative exercise as we map these Generalized Solutions back into the Problem at hand:\n\nIP 13: The Other Way Around. How? Not attack by sea? Wait…ATTACK BY LAND!! Change the DIRECTION of Attack! So attack from the other side, the land side!! (We could retrospectively add this parameter to the Ishikawa Diagram too). Will this work? Yes, the guns can’t turn around !!\nIP 35: Parameter Change. But “ships” on land?? Note, the desert is an ocean into which no oar is dipped. Sand and Water are both Resources in the problem, as we have duly noted in the Ishikawa Diagram. So a different kind of ocean and therefore a different kind of ship? At a stretch, we can say the warships of the British Navy are being substituted with the use of ….Camels!! And, metaphorically speaking, it is still an attack using ships….The Ships of the Desert!! Parameter Change.\nIP 10: Prior Action. How? Lawrence and Ali are far from Aqaba and cannot do anything “in advance”. What could this be?\nIP 5: Merging. However, on the way to Aqaba, Lawrence and Ali must recruit the Howeitat tribe “in advance” of their attack !! As Lawrence tells Ali, If 50 men came out of the Nefud Desert, they might be 50 men other men would join. This is in accordance with what IP 10 is suggesting, to get other tribes to join in, in advance of the attack.\nIP 40: Composite Materials. What object within the situation can we reconstitute with smaller pieces of different types? The British Army…so an army made up of pieces? Yes! The Tribes need to unite into one composite army.\nAnd, instead of large warships, the Arabs switch to a composite force with camels…\n\nSo IP 13 works nicely now, along with IP 35 and IP 40, to give us a camel-borne attack from the landward side. IP 10 also teams up with IP 40 and IP 5 to give the idea of tribe unification.\nAnd so Lawrence and Ali, with the help of Auda Abu Tayi, attack Aqaba port from the landward side by crossing the Nefud desert on camels, and take it! And we have justified their decision using TRIZ !!\nHere is the final solution in action !!\n\n\n\n\n I hope that was as much fun to read as it was for me to write it up !!"
  },
  {
    "objectID": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html#points-to-ponder",
    "href": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html#points-to-ponder",
    "title": "A TRIZ Analysis of Lawrence of Arabia",
    "section": "Points to Ponder",
    "text": "Points to Ponder\n\nDo we each of us need a Dryden to vouch for us and help us get access to the Field?\nDoes TRIZ work in both mundane and industrial contexts? (Yes of course!)\nCan we just take the 40 Inventive Principles directly and throw them at every Problem, without necessarily going through the process of creating Contradictions and IFR? Hipple’s book has a remark in this direction."
  },
  {
    "objectID": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html#references",
    "href": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html#references",
    "title": "A TRIZ Analysis of Lawrence of Arabia",
    "section": "References",
    "text": "References\n\nLawrence of Arabia at the Internet Movie Data Base https://www.imdb.com/title/tt0056172/\n\nMihaly Csikszentmihalyi, Creativity, Flow, and the Psychology of Discovery and Invention. Harper Perennial; Reprint edition (August 6, 2013)\nJack Hipple, The Ideal Result and How to Achieve It. Springer; 2012th edition (June 26, 2012)\nValery Souchkov, Defining Contradictions. http://www.xtriz.com/Training/TRIZ_DefineContradiction_Tutorial.pdf"
  },
  {
    "objectID": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html#footnotes",
    "href": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html#footnotes",
    "title": "A TRIZ Analysis of Lawrence of Arabia",
    "section": "Footnotes",
    "text": "Footnotes\n\nI was not able to ascertain who is the author of this map. I would be happy to write to obtain permission and use it with acknowledgement.↩︎\nThe Contradiction Matrix is not quite symmetric, so stating the Contradiction both ways allows us to access a slightly larger set of Inventive Principles from two cells of the Matrix.↩︎\nArriving at Physical Contradictions is not always easy! If we can, then there are a very crisp set of TRIZ Separation Principles that we can apply to solve the Problem.↩︎"
  },
  {
    "objectID": "content/posts/project-2/index.html",
    "href": "content/posts/project-2/index.html",
    "title": "Project 2",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\nThis is inline code plus a small code chunk.\n\nCodelibrary(tidyverse)\n\nggplot(mpg) +\n  geom_jitter(aes(cty, hwy), size = 4, alpha = 0.5) \n\n\n\n\n\n\n\n\n\n\nTransforming OLS estimates\nMaximizing likelihood\n\n\n\n\nCodepreds_lm %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  theme_minimal(base_size = 12) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\nCodeglm.mod &lt;- glm(sex ~ body_mass_g + bill_length_mm + species, family = binomial, data = dat)\n\npreds &lt;- dat %&gt;% \n  mutate(\n    prob.fit = glm.mod$fitted.values,\n    prediction = if_else(prob.fit &gt; 0.5, 'male', 'female'),\n    correct = if_else(sex == prediction, 'correct', 'incorrect')\n  )\n\n\npreds %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  theme_minimal(base_size = 10) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\\[\n\\int_0^1 f(x) \\ dx\n\\]\n\n\n\n\nCodegeom_density(\n  mapping = NULL,\n  data = NULL,\n  stat = \"density\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  outline.type = \"upper\"\n)\n\n\n\nCodestat_density(\n  mapping = NULL,\n  data = NULL,\n  geom = \"area\",\n  position = \"stack\",\n  ...,\n  bw = \"nrd0\",\n  adjust = 1,\n  kernel = \"gaussian\",\n  n = 512,\n  trim = FALSE,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\n\n\n\n\n\nCodeggplot(data = gapminder::gapminder, mapping = aes(x = lifeExp, fill = continent)) +\n  stat_density(position = \"identity\", alpha = 0.5)\n\n\n\n\nBla bla bla. This is a caption in the margin. Super cool isn’t it?"
  },
  {
    "objectID": "content/posts/project-2/index.html#merriweather",
    "href": "content/posts/project-2/index.html#merriweather",
    "title": "Project 2",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\nThis is inline code plus a small code chunk.\n\nCodelibrary(tidyverse)\n\nggplot(mpg) +\n  geom_jitter(aes(cty, hwy), size = 4, alpha = 0.5) \n\n\n\n\n\n\n\n\n\n\nTransforming OLS estimates\nMaximizing likelihood\n\n\n\n\nCodepreds_lm %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  theme_minimal(base_size = 12) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\nCodeglm.mod &lt;- glm(sex ~ body_mass_g + bill_length_mm + species, family = binomial, data = dat)\n\npreds &lt;- dat %&gt;% \n  mutate(\n    prob.fit = glm.mod$fitted.values,\n    prediction = if_else(prob.fit &gt; 0.5, 'male', 'female'),\n    correct = if_else(sex == prediction, 'correct', 'incorrect')\n  )\n\n\npreds %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  theme_minimal(base_size = 10) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\\[\n\\int_0^1 f(x) \\ dx\n\\]"
  },
  {
    "objectID": "content/posts/project-2/index.html#columns",
    "href": "content/posts/project-2/index.html#columns",
    "title": "Project 2",
    "section": "",
    "text": "Codegeom_density(\n  mapping = NULL,\n  data = NULL,\n  stat = \"density\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  outline.type = \"upper\"\n)\n\n\n\nCodestat_density(\n  mapping = NULL,\n  data = NULL,\n  geom = \"area\",\n  position = \"stack\",\n  ...,\n  bw = \"nrd0\",\n  adjust = 1,\n  kernel = \"gaussian\",\n  n = 512,\n  trim = FALSE,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)"
  },
  {
    "objectID": "content/posts/project-2/index.html#margin-captions-and-figures",
    "href": "content/posts/project-2/index.html#margin-captions-and-figures",
    "title": "Project 2",
    "section": "",
    "text": "Codeggplot(data = gapminder::gapminder, mapping = aes(x = lifeExp, fill = continent)) +\n  stat_density(position = \"identity\", alpha = 0.5)\n\n\n\n\nBla bla bla. This is a caption in the margin. Super cool isn’t it?"
  },
  {
    "objectID": "content/posts/listing.html",
    "href": "content/posts/listing.html",
    "title": "The Foundation Series",
    "section": "",
    "text": "This is a dummy blog posts\n\n\nThis is a test post. In this post, I try out different functionalities\n\n\n\nArvind Venkatadri\n\n\nJun 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject 2\n\n\nThis is a test post. In this post, I try out different functionalities\n\n\n\nArvind Venkatadri\n\n\nJun 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nA Tufte Handout Example\n\n\n\n\n\n\nJJ Allaire and Yihui Xie\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\nBootswatch Themed QMD\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\nBoston Terrier\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\nCallout Boxes\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\nCallouts in PDF\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\nExample Lightbox Document\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\nFont Awesome Quarto Extension\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\nHousing Prices\n\n\n\n\n\n\nArvind Venkatadri\n\n\nJan 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nInline Code\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\nNutshell: Expandable Explanations\n\n\n\n\n\n\nDavid Schoch\n\n\n\n\n\n\n\n\n\n\n\n\nOld Markdown - reactable example\n\n\n\n\n\n\nTom Mock\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\nOverview of using gtsummary\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\nPenguin Report Presentation\n\n\n\n\n\n\nTom Mock\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\nPenguins Distilled\n\n\nA great new article on Penguins\n\n\n\nTom Mock\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\nPenguins Parametric Reports\n\n\n\n\n\n\n\n\n\nJul 13, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nPlot Layout\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\nPlots\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\nPresentation with Columns?\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\nSample Blog Post Template\n\n\nDescription: This is a test post. In this post, I try out different functionalities\n\n\n\nArvind Venkatadri\n\n\nJun 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Lordicons, Fontawesome Icons,Academicons, and Iconify Icons\n\n\n\n\n\n\nArvind Venkatadri\n\n\nJan 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nUsing sketch\n\n\n\n\n\n\nArvind Venkatadri\n\n\nJan 9, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nVisual Editor for Quarto\n\n\n\n\n\n\nThomas Mock\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\ngtsummary + R Markdown\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n Back to top"
  },
  {
    "objectID": "content/posts/13-rapp-sample-document/index.html",
    "href": "content/posts/13-rapp-sample-document/index.html",
    "title": "Sample Blog Post Template",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\nThis is inline code plus a small code chunk.\n\nlibrary(tidyverse)\n\nggplot(mpg) +\n  geom_jitter(aes(cty, hwy), size = 4, alpha = 0.5) \n\n\n\n\n\n\n\nTransforming OLS estimates\nMaximizing likelihood\n\n\n\n\nCodepreds_lm %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  theme_minimal(base_size = 12) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\nCodeglm.mod &lt;- glm(sex ~ body_mass_g + bill_length_mm + species, family = binomial, data = dat)\n\npreds &lt;- dat %&gt;% \n  mutate(\n    prob.fit = glm.mod$fitted.values,\n    prediction = if_else(prob.fit &gt; 0.5, 'male', 'female'),\n    correct = if_else(sex == prediction, 'correct', 'incorrect')\n  )\n\n\npreds %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  theme_minimal(base_size = 10) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\\[\n\\int_0^1 f(x) \\ dx\n\\]\n\n\n\n\ngeom_density(\n  mapping = NULL,\n  data = NULL,\n  stat = \"density\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  outline.type = \"upper\"\n)\n\n\nstat_density(\n  mapping = NULL,\n  data = NULL,\n  geom = \"area\",\n  position = \"stack\",\n  ...,\n  bw = \"nrd0\",\n  adjust = 1,\n  kernel = \"gaussian\",\n  n = 512,\n  trim = FALSE,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\n\n\n\n\nggplot(data = gapminder::gapminder, mapping = aes(x = lifeExp, fill = continent)) +\n  stat_density(position = \"identity\", alpha = 0.5)\n\n\n\n\nBla bla bla. This is a caption in the margin. Super cool isn’t it?"
  },
  {
    "objectID": "content/posts/13-rapp-sample-document/index.html#merriweather",
    "href": "content/posts/13-rapp-sample-document/index.html#merriweather",
    "title": "Sample Blog Post Template",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\nThis is inline code plus a small code chunk.\n\nlibrary(tidyverse)\n\nggplot(mpg) +\n  geom_jitter(aes(cty, hwy), size = 4, alpha = 0.5) \n\n\n\n\n\n\n\nTransforming OLS estimates\nMaximizing likelihood\n\n\n\n\nCodepreds_lm %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  theme_minimal(base_size = 12) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\nCodeglm.mod &lt;- glm(sex ~ body_mass_g + bill_length_mm + species, family = binomial, data = dat)\n\npreds &lt;- dat %&gt;% \n  mutate(\n    prob.fit = glm.mod$fitted.values,\n    prediction = if_else(prob.fit &gt; 0.5, 'male', 'female'),\n    correct = if_else(sex == prediction, 'correct', 'incorrect')\n  )\n\n\npreds %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  theme_minimal(base_size = 10) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\\[\n\\int_0^1 f(x) \\ dx\n\\]"
  },
  {
    "objectID": "content/posts/13-rapp-sample-document/index.html#columns",
    "href": "content/posts/13-rapp-sample-document/index.html#columns",
    "title": "Sample Blog Post Template",
    "section": "",
    "text": "geom_density(\n  mapping = NULL,\n  data = NULL,\n  stat = \"density\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  outline.type = \"upper\"\n)\n\n\nstat_density(\n  mapping = NULL,\n  data = NULL,\n  geom = \"area\",\n  position = \"stack\",\n  ...,\n  bw = \"nrd0\",\n  adjust = 1,\n  kernel = \"gaussian\",\n  n = 512,\n  trim = FALSE,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)"
  },
  {
    "objectID": "content/posts/13-rapp-sample-document/index.html#margin-figure-and-captions",
    "href": "content/posts/13-rapp-sample-document/index.html#margin-figure-and-captions",
    "title": "Sample Blog Post Template",
    "section": "",
    "text": "ggplot(data = gapminder::gapminder, mapping = aes(x = lifeExp, fill = continent)) +\n  stat_density(position = \"identity\", alpha = 0.5)\n\n\n\n\nBla bla bla. This is a caption in the margin. Super cool isn’t it?"
  },
  {
    "objectID": "content/posts/10-using-nutshell/nutshell.html",
    "href": "content/posts/10-using-nutshell/nutshell.html",
    "title": "Nutshell: Expandable Explanations",
    "section": "",
    "text": "Nutshell is a tool to make “expandable explanations”, like this! This lets your readers learn only the details they need, just-in-time, always-in-context.\nUnlike links, Nutshell lets you include only the snippet you need, not the whole page. Plus, instead of being lost in a jungle of new tabs, your reader stays on one page, keeping their flow of reading. Even if you interrupt a sentence, Nutshell recaps the sentence afterwards, so your reader never loses context.\nYou can find more information on the nutshell webpage and here is a live demo !"
  },
  {
    "objectID": "content/posts/10-using-nutshell/nutshell.html#what-is-nutshell",
    "href": "content/posts/10-using-nutshell/nutshell.html#what-is-nutshell",
    "title": "Nutshell: Expandable Explanations",
    "section": "",
    "text": "Nutshell is a tool to make “expandable explanations”, like this! This lets your readers learn only the details they need, just-in-time, always-in-context.\nUnlike links, Nutshell lets you include only the snippet you need, not the whole page. Plus, instead of being lost in a jungle of new tabs, your reader stays on one page, keeping their flow of reading. Even if you interrupt a sentence, Nutshell recaps the sentence afterwards, so your reader never loses context.\nYou can find more information on the nutshell webpage and here is a live demo !"
  },
  {
    "objectID": "content/posts/10-using-nutshell/nutshell.html#test",
    "href": "content/posts/10-using-nutshell/nutshell.html#test",
    "title": "Nutshell: Expandable Explanations",
    "section": "Test",
    "text": "Test\nThis is a senseless paragraph"
  },
  {
    "objectID": "content/posts/10-using-nutshell/nutshell.html#testing-links",
    "href": "content/posts/10-using-nutshell/nutshell.html#testing-links",
    "title": "Nutshell: Expandable Explanations",
    "section": "Testing Links",
    "text": "Testing Links\n\n:link to senseless paragraph\n:link to wikipedia article\n:link to invisible sections"
  },
  {
    "objectID": "content/posts/10-using-nutshell/nutshell.html#x-invisible",
    "href": "content/posts/10-using-nutshell/nutshell.html#x-invisible",
    "title": "Nutshell: Expandable Explanations",
    "section": ":x invisible",
    "text": ":x invisible\nUse ## :x header to include an invisible section that can be linked to via nutshell"
  },
  {
    "objectID": "content/posts/08-knitr/penguin-params.html",
    "href": "content/posts/08-knitr/penguin-params.html",
    "title": "Penguins Parametric Reports",
    "section": "",
    "text": "We have data about 344 penguins. Only 193 are classified asAdelie. The distribution of the Adelie penguins are shown below:\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/posts/08-knitr/fa-example.html",
    "href": "content/posts/08-knitr/fa-example.html",
    "title": "Font Awesome Quarto Extension",
    "section": "",
    "text": "This extension allows you to use font-awesome icons in your Quarto HTML and PDF documents. It provides an {{&lt; fa &gt;}} shortcode:\n{{&lt; fa icon-name &gt;}}\nFor example:\n\n\n\nShortcode\nIcon\n\n\n\n\n{{&lt; fa thumbs-up &gt;}}\n\n\n\n{{&lt; fa folder &gt;}}\n\n\n\n{{&lt; fa chess-pawn &gt;}}\n\n\n\n{{&lt; fa brands bluetooth &gt;}}\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/posts/07-visuals/plots.html",
    "href": "content/posts/07-visuals/plots.html",
    "title": "Plots",
    "section": "",
    "text": "library(palmerpenguins)\nlibrary(ggplot2)\nggplot2::theme_set(ggplot2::theme_minimal())\npenguins &lt;- na.omit(penguins)"
  },
  {
    "objectID": "content/posts/07-visuals/plots.html#load-libraries",
    "href": "content/posts/07-visuals/plots.html#load-libraries",
    "title": "Plots",
    "section": "",
    "text": "library(palmerpenguins)\nlibrary(ggplot2)\nggplot2::theme_set(ggplot2::theme_minimal())\npenguins &lt;- na.omit(penguins)"
  },
  {
    "objectID": "content/posts/07-visuals/plots.html#ggplot2",
    "href": "content/posts/07-visuals/plots.html#ggplot2",
    "title": "Plots",
    "section": "ggplot2",
    "text": "ggplot2\nCredit to Alison Hill + Allison Horst\n\n```{r}\nmass_flipper &lt;- ggplot(data = penguins, \n                       aes(x = flipper_length_mm,\n                           y = body_mass_g)) +\n  geom_point(aes(color = species, \n                 shape = species),\n             size = 3,\n             alpha = 0.8) +\n  scale_color_manual(values = c(\"darkorange\",\"purple\",\"cyan4\")) +\n  labs(title = \"Penguin size, Palmer Station LTER\",\n       subtitle = \"Flipper length and body mass for Adelie, Chinstrap and Gentoo Penguins\",\n       x = \"Flipper length (mm)\",\n       y = \"Body mass (g)\",\n       color = \"Penguin species\",\n       shape = \"Penguin species\") +\n  theme(legend.position = c(0.2, 0.7),\n        plot.title.position = \"plot\",\n        plot.caption = element_text(hjust = 0, face= \"italic\"),\n        plot.caption.position = \"plot\",\n        plot.background = element_rect(color = \"black\"))\n```"
  },
  {
    "objectID": "content/posts/07-visuals/plots.html#basic-plot",
    "href": "content/posts/07-visuals/plots.html#basic-plot",
    "title": "Plots",
    "section": "Basic plot",
    "text": "Basic plot\n\n```{r}\nmass_flipper\n```"
  },
  {
    "objectID": "content/posts/07-visuals/plots.html#basic-plot-1",
    "href": "content/posts/07-visuals/plots.html#basic-plot-1",
    "title": "Plots",
    "section": "Basic plot",
    "text": "Basic plot\nIncreasing the width/DPI only affects the scaling of the image, it will not overflow.\n\n```{r}\n#| fig-width: 10\n#| fig-height: 4\n#| fig-dpi: 600\nmass_flipper\n```"
  },
  {
    "objectID": "content/posts/07-visuals/plots.html#aside",
    "href": "content/posts/07-visuals/plots.html#aside",
    "title": "Plots",
    "section": "Aside",
    "text": "Aside\n\n\n\n\n\n\nThe palmerpenguins R package contains two datasets that we believe are a viable alternative to Anderson’s Iris data (see datasets::iris). In this introductory vignette, we’ll highlight some of the properties of these datasets that make them useful for statistics and data science education, as well as software documentation and testing."
  },
  {
    "objectID": "content/posts/07-visuals/plots.html#overflow-content",
    "href": "content/posts/07-visuals/plots.html#overflow-content",
    "title": "Plots",
    "section": "Overflow Content",
    "text": "Overflow Content\nThere are many options for overflow, either left/right\n\n```{r}\n#| column: body-outset-right\n#| fig-width: 10\nmass_flipper\n```\n\n\n\n\n\n```{r}\n#| column: screen-inset-right\n#| fig-width: 10\nmass_flipper\n```\n\n\n\n\n\n```{r}\n#| column: page-inset-left\n#| fig-width: 10\nmass_flipper\n```\n\n\n\n\n\n```{r}\n#| column: screen-left\n#| fig-width: 10\nmass_flipper\n```"
  },
  {
    "objectID": "content/posts/07-visuals/gt-summary.html#introduction",
    "href": "content/posts/07-visuals/gt-summary.html#introduction",
    "title": "Overview of using gtsummary\n",
    "section": "Introduction",
    "text": "Introduction\nReproducible reports are an important part of good practices. We often need to report the results from a table in the text of an Quarto report. Inline reporting has been made simple with inline_text(). The inline_text() function reports statistics from {gtsummary} tables inline in an Quarto report."
  },
  {
    "objectID": "content/posts/07-visuals/gt-summary.html#setup",
    "href": "content/posts/07-visuals/gt-summary.html#setup",
    "title": "Overview of using gtsummary\n",
    "section": "Setup",
    "text": "Setup\nBefore going through the tutorial, install and load {gtsummary}.\n\n# install.packages(\"gtsummary\")\nlibrary(gtsummary)"
  },
  {
    "objectID": "content/posts/07-visuals/gt-summary.html#example-data-set",
    "href": "content/posts/07-visuals/gt-summary.html#example-data-set",
    "title": "Overview of using gtsummary\n",
    "section": "Example data set",
    "text": "Example data set\nWe’ll be using the trial data set throughout this example.\n\nThis set contains data from 200 patients who received one of two types of chemotherapy (Drug A or Drug B). The outcomes are tumor response and death.\n\nFor brevity in the tutorial, let’s keep a subset of the variables from the trial data set.\n\ntrial2 &lt;-\n  trial %&gt;%\n  select(trt, marker, stage)"
  },
  {
    "objectID": "content/posts/07-visuals/gt-summary.html#inline_text_tbl_summary",
    "href": "content/posts/07-visuals/gt-summary.html#inline_text_tbl_summary",
    "title": "Overview of using gtsummary\n",
    "section": "Inline results from tbl_summary()",
    "text": "Inline results from tbl_summary()\nFirst create a basic summary table using tbl_summary() (review tbl_summary() vignette for detailed overview of this function if needed).\n\ntab1 &lt;- tbl_summary(trial2, by = trt)\ntab1\n\n\n\n\n\n\nCharacteristic\n\nDrug A, N = 981\n\n\nDrug B, N = 1021\n\n\n\n\nMarker Level (ng/mL)\n0.84 (0.24, 1.57)\n0.52 (0.19, 1.20)\n\n\n    Unknown\n6\n4\n\n\nT Stage\n\n\n\n\n    T1\n28 (29%)\n25 (25%)\n\n\n    T2\n25 (26%)\n29 (28%)\n\n\n    T3\n22 (22%)\n21 (21%)\n\n\n    T4\n23 (23%)\n27 (26%)\n\n\n\n\n1 Median (IQR); n (%)\n\n\n\n\n\nTo report the median (IQR) of the marker levels in each group, use the following commands inline.\n\nThe median (IQR) marker level in the Drug A and Drug B groups are 0.84 (0.24, 1.57) and 0.52 (0.19, 1.20), respectively. Here’s how the line will appear in your report.\n\n\nThe median (IQR) marker level in the Drug A and Drug B groups are 0.84 (0.24, 1.57) and 0.52 (0.19, 1.20), respectively. If you display a statistic from a categorical variable, include the level argument.\n\n\n25 (25%) resolves to “25 (25%)”"
  },
  {
    "objectID": "content/posts/07-visuals/gt-summary.html#inline_text_tbl_regression",
    "href": "content/posts/07-visuals/gt-summary.html#inline_text_tbl_regression",
    "title": "Overview of using gtsummary\n",
    "section": "Inline results from tbl_regression()",
    "text": "Inline results from tbl_regression()\nSimilar syntax is used to report results from tbl_regression() and tbl_uvregression() tables. Refer to the tbl_regression() vignette if you need detailed guidance on using these functions.\nLet’s first create a regression model.\n\n# build logistic regression model\nm1 &lt;- glm(response ~ age + stage, trial, family = binomial(link = \"logit\"))\n\nNow summarize the results with tbl_regression(); exponentiate to get the odds ratios.\n\ntbl_m1 &lt;- tbl_regression(m1, exponentiate = TRUE)\ntbl_m1\n\n\n\n\n\n\nCharacteristic\n\nOR1\n\n\n95% CI1\n\np-value\n\n\n\nAge\n1.02\n1.00, 1.04\n0.091\n\n\nT Stage\n\n\n\n\n\n    T1\n—\n—\n\n\n\n    T2\n0.58\n0.24, 1.37\n0.2\n\n\n    T3\n0.94\n0.39, 2.28\n0.9\n\n\n    T4\n0.79\n0.33, 1.90\n0.6\n\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\nTo report the result for age, use the following commands inline.\n\n1.02 (95% CI 1.00, 1.04; p=0.091) Here’s how the line will appear in your report.\n\n\n1.02 (95% CI 1.00, 1.04; p=0.091) It is reasonable that you’ll need to modify the text. To do this, use the pattern argument. The pattern argument syntax follows glue::glue() format with referenced R objects being inserted between curly brackets. The default is pattern = \"{estimate} ({conf.level*100}% CI {conf.low}, {conf.high}; {p.value})\". You have access the to following fields within the pattern argument.\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\n{estimate}\n\nprimary estimate (e.g. model coefficient, odds ratio)\n\n\n\n{conf.low}\n\nlower limit of confidence interval\n\n\n\n{conf.high}\n\nupper limit of confidence interval\n\n\n\n{p.value}\n\np-value\n\n\n\n{conf.level}\n\nconfidence level of interval\n\n\n\n{N}\n\nnumber of observations\n\n\n\n\n\n\n\nAge was not significantly associated with tumor response (OR 1.02; 95% CI 1.00, 1.04; p=0.091). Age was not significantly associated with tumor response (OR 1.02; 95% CI 1.00, 1.04; p=0.091). If you’re printing results from a categorical variable, include the level argument, e.g. inline_text(tbl_m1, variable = stage, level = \"T3\") resolves to “0.94 (95% CI 0.39, 2.28; p=0.9)”.\n\nThe inline_text function has arguments for rounding the p-value (pvalue_fun) and the coefficients and confidence interval (estimate_fun). These default to the same rounding performed in the table, but can be modified when reporting inline.\nFor more details about inline code, review to the RStudio documentation page."
  },
  {
    "objectID": "content/posts/05-presentations/fragments.html#make-these-columns-appear-in-order",
    "href": "content/posts/05-presentations/fragments.html#make-these-columns-appear-in-order",
    "title": "Presentation with Columns?",
    "section": "Make these columns appear in order",
    "text": "Make these columns appear in order\n\n\nThese appear first\n\nMake\nYour\nList\n\n\nThen this\n\n```{r}\nhead(mtcars)\n```\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1"
  },
  {
    "objectID": "content/posts/04-static/old-rmarkdown.html",
    "href": "content/posts/04-static/old-rmarkdown.html",
    "title": "Old Markdown - reactable example",
    "section": "",
    "text": "This is an example from reactable documentation - showing the Twitter followers of some politicians.\nExample adapted from {reactable} documentation\n\nlibrary(reactable)\nlibrary(htmltools)\nlibrary(dplyr)\n\ndata &lt;- read.csv(\"https://glin.github.io/reactable/articles/twitter-followers/twitter_followers.csv\",\n                 stringsAsFactors = FALSE)\n\n# Render a bar chart with a label on the left\nbar_chart &lt;- function(label, width = \"100%\", height = \"14px\", fill = \"#00bfc4\", background = NULL) {\n  bar &lt;- div(style = list(background = fill, width = width, height = height))\n  chart &lt;- div(style = list(flexGrow = 1, marginLeft = \"6px\", background = background), bar)\n  div(style = list(display = \"flex\", alignItems = \"center\"), label, chart)\n}\n\ntbl &lt;- reactable(\n  data,\n  pagination = FALSE,\n  defaultSorted = \"exclusive_followers_pct\",\n  defaultColDef = colDef(headerClass = \"header\", align = \"left\"),\n  columns = list(\n    account = colDef(\n      cell = function(value) {\n        url &lt;- paste0(\"https://twitter.com/\", value)\n        tags$a(href = url, target = \"_blank\", paste0(\"@\", value))\n      },\n      width = 150\n    ),\n    followers = colDef(\n      defaultSortOrder = \"desc\",\n      cell = function(value) {\n        width &lt;- paste0(value * 100 / max(data$followers), \"%\")\n        value &lt;- format(value, big.mark = \",\")\n        value &lt;- format(value, width = 9, justify = \"right\")\n        bar &lt;- div(\n          class = \"bar-chart\",\n          style = list(marginRight = \"6px\"),\n          div(class = \"bar\", style = list(width = width, backgroundColor = \"#3fc1c9\"))\n        )\n        div(class = \"bar-cell\", span(class = \"number\", value), bar)\n      }\n    ),\n    exclusive_followers_pct = colDef(\n      name = \"Exclusive Followers\",\n      defaultSortOrder = \"desc\",\n      cell = JS(\"function(cellInfo) {\n        // Format as percentage\n        const pct = (cellInfo.value * 100).toFixed(1) + '%'\n        // Pad single-digit numbers\n        let value = pct.padStart(5)\n        // Show % on first row only\n        if (cellInfo.viewIndex &gt; 0) {\n          value = value.replace('%', ' ')\n        }\n        // Render bar chart\n        return (\n          '&lt;div class=\\\"bar-cell\\\"&gt;' +\n            '&lt;span class=\\\"number\\\"&gt;' + value + '&lt;/span&gt;' +\n            '&lt;div class=\\\"bar-chart\\\" style=\\\"background-color: #e1e1e1\\\"&gt;' +\n              '&lt;div class=\\\"bar\\\" style=\\\"width: ' + pct + '; background-color: #fc5185\\\"&gt;&lt;/div&gt;' +\n            '&lt;/div&gt;' +\n          '&lt;/div&gt;'\n        )\n      }\"),\n      html = TRUE\n    )\n  ),\n  compact = TRUE,\n  class = \"followers-tbl\"\n)\n\ntbl\n\n\n\n\n\n\n\n# Add Google Fonts to the page\ntags$link(href = \"https://fonts.googleapis.com/css?family=Karla:400,700|Fira+Mono&display=fallback\",\n          rel = \"stylesheet\")\n\n\n\n\n\n/* CSS for the R Markdown document, inserted through a ```{css} code chunk */\n\n/* Styles for the table container, title, and subtitle */\n.twitter-followers {\n  /* Center the table */\n  margin: 0 auto;\n  /* Reduce the table width */\n  width: 575px;\n  font-family: Karla, \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n}\n\n.followers-header {\n  margin: 8px 0;\n  font-size: 16px;\n}\n\n.followers-title {\n  font-size: 20px;\n  font-weight: 600;\n}\n\n/* Styles for the table */\n.followers-tbl {\n  font-size: 14px;\n  line-height: 18px;\n}\n\n.followers-tbl a {\n  color: inherit;\n}\n\n/* Styles for the column headers */\n.header {\n  border-bottom: 2px solid #555;\n  font-size: 13px;\n  font-weight: 400;\n  text-transform: uppercase;\n}\n\n.header:hover {\n  background-color: #eee;\n}\n\n/* Styles for the bar charts */\n.bar-cell {\n  display: flex;\n  align-items: center;\n}\n\n.number {\n  font-family: \"Fira Mono\", Consolas, Monaco, monospace;\n  font-size: 13.5px;\n  white-space: pre;\n}\n\n.bar-chart {\n  flex-grow: 1;\n  margin-left: 6px;\n  height: 14px;\n}\n\n.bar {\n  height: 100%;\n}"
  },
  {
    "objectID": "content/posts/04-static/old-rmarkdown.html#twitter-followers",
    "href": "content/posts/04-static/old-rmarkdown.html#twitter-followers",
    "title": "Old Markdown - reactable example",
    "section": "",
    "text": "This is an example from reactable documentation - showing the Twitter followers of some politicians.\nExample adapted from {reactable} documentation\n\nlibrary(reactable)\nlibrary(htmltools)\nlibrary(dplyr)\n\ndata &lt;- read.csv(\"https://glin.github.io/reactable/articles/twitter-followers/twitter_followers.csv\",\n                 stringsAsFactors = FALSE)\n\n# Render a bar chart with a label on the left\nbar_chart &lt;- function(label, width = \"100%\", height = \"14px\", fill = \"#00bfc4\", background = NULL) {\n  bar &lt;- div(style = list(background = fill, width = width, height = height))\n  chart &lt;- div(style = list(flexGrow = 1, marginLeft = \"6px\", background = background), bar)\n  div(style = list(display = \"flex\", alignItems = \"center\"), label, chart)\n}\n\ntbl &lt;- reactable(\n  data,\n  pagination = FALSE,\n  defaultSorted = \"exclusive_followers_pct\",\n  defaultColDef = colDef(headerClass = \"header\", align = \"left\"),\n  columns = list(\n    account = colDef(\n      cell = function(value) {\n        url &lt;- paste0(\"https://twitter.com/\", value)\n        tags$a(href = url, target = \"_blank\", paste0(\"@\", value))\n      },\n      width = 150\n    ),\n    followers = colDef(\n      defaultSortOrder = \"desc\",\n      cell = function(value) {\n        width &lt;- paste0(value * 100 / max(data$followers), \"%\")\n        value &lt;- format(value, big.mark = \",\")\n        value &lt;- format(value, width = 9, justify = \"right\")\n        bar &lt;- div(\n          class = \"bar-chart\",\n          style = list(marginRight = \"6px\"),\n          div(class = \"bar\", style = list(width = width, backgroundColor = \"#3fc1c9\"))\n        )\n        div(class = \"bar-cell\", span(class = \"number\", value), bar)\n      }\n    ),\n    exclusive_followers_pct = colDef(\n      name = \"Exclusive Followers\",\n      defaultSortOrder = \"desc\",\n      cell = JS(\"function(cellInfo) {\n        // Format as percentage\n        const pct = (cellInfo.value * 100).toFixed(1) + '%'\n        // Pad single-digit numbers\n        let value = pct.padStart(5)\n        // Show % on first row only\n        if (cellInfo.viewIndex &gt; 0) {\n          value = value.replace('%', ' ')\n        }\n        // Render bar chart\n        return (\n          '&lt;div class=\\\"bar-cell\\\"&gt;' +\n            '&lt;span class=\\\"number\\\"&gt;' + value + '&lt;/span&gt;' +\n            '&lt;div class=\\\"bar-chart\\\" style=\\\"background-color: #e1e1e1\\\"&gt;' +\n              '&lt;div class=\\\"bar\\\" style=\\\"width: ' + pct + '; background-color: #fc5185\\\"&gt;&lt;/div&gt;' +\n            '&lt;/div&gt;' +\n          '&lt;/div&gt;'\n        )\n      }\"),\n      html = TRUE\n    )\n  ),\n  compact = TRUE,\n  class = \"followers-tbl\"\n)\n\ntbl\n\n\n\n\n\n\n\n# Add Google Fonts to the page\ntags$link(href = \"https://fonts.googleapis.com/css?family=Karla:400,700|Fira+Mono&display=fallback\",\n          rel = \"stylesheet\")\n\n\n\n\n\n/* CSS for the R Markdown document, inserted through a ```{css} code chunk */\n\n/* Styles for the table container, title, and subtitle */\n.twitter-followers {\n  /* Center the table */\n  margin: 0 auto;\n  /* Reduce the table width */\n  width: 575px;\n  font-family: Karla, \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n}\n\n.followers-header {\n  margin: 8px 0;\n  font-size: 16px;\n}\n\n.followers-title {\n  font-size: 20px;\n  font-weight: 600;\n}\n\n/* Styles for the table */\n.followers-tbl {\n  font-size: 14px;\n  line-height: 18px;\n}\n\n.followers-tbl a {\n  color: inherit;\n}\n\n/* Styles for the column headers */\n.header {\n  border-bottom: 2px solid #555;\n  font-size: 13px;\n  font-weight: 400;\n  text-transform: uppercase;\n}\n\n.header:hover {\n  background-color: #eee;\n}\n\n/* Styles for the bar charts */\n.bar-cell {\n  display: flex;\n  align-items: center;\n}\n\n.number {\n  font-family: \"Fira Mono\", Consolas, Monaco, monospace;\n  font-size: 13.5px;\n  white-space: pre;\n}\n\n.bar-chart {\n  flex-grow: 1;\n  margin-left: 6px;\n  height: 14px;\n}\n\n.bar {\n  height: 100%;\n}"
  },
  {
    "objectID": "content/posts/03-computation/visual-editor.html",
    "href": "content/posts/03-computation/visual-editor.html",
    "title": "Visual Editor for Quarto",
    "section": "",
    "text": "RStudio 2022.07.1 comes with support for the Visual Mode of editing Quarto and other markdown-based files!\nThis is a WYSIWYM editor, meaning:\n\nWYSIWYM is an acronym that stands for What you see is what you mean. This was positioned to not be confused with WYSIWYG (what you see is what you get). The idea behind WYSIWYG is to display text on screen in much the exact same way as they will appear when printed on paper.\n\nWYSIWYM means that it can be translated differently, (where) the same content can lead to different output formats .\n\nThe Visual Markdown mode in RStudio allows for editing in plain text or visual mode, along with a visual representation of what it will actually look like while maintaining the ability to output to HTML or PDF.\nFull guide guide from the RStudio dev team that covers all the major topics and sub topics of the new features.\n\n\nOS\nDownload\nSize\nSHA-256\n\n\n\nWindows 10/11\n\nRStudio-2022.07.1-554.exe(opens in a new tab)\n\n190.14 MB\n5ab6215b\n\n\nmacOS 10.15+\n\nRStudio-2022.07.1-554.dmg(opens in a new tab)\n\n221.04 MB\n7b1a2285\n\n\nUbuntu 18+/Debian 10+\n\nrstudio-2022.07.1-554-amd64.deb(opens in a new tab)\n\n132.91 MB\n74b9e751\n\n\nUbuntu 22\n\nrstudio-2022.07.1-554-amd64.deb(opens in a new tab)\n\n145.33 MB\n92f2ab75\n\n\nFedora 19/Red Hat 7\n\nrstudio-2022.07.1-554-x86_64.rpm(opens in a new tab)\n\n103.29 MB\n0fc15d16\n\n\nFedora 34/Red Hat 8\n\nrstudio-2022.07.1-554-x86_64.rpm(opens in a new tab)\n\n149.77 MB\n0c4ef334\n\n\nOpenSUSE 15\n\nrstudio-2022.07.1-554-x86_64.rpm(opens in a new tab)\n\n133.76 MB\n45f277d0\n\n\n\n\n\nMarkdown documents can be edited in either source or visual button at the top-right of the document toolbar (or alternatively the ⌘ + ⇧ + F4 keyboard shortcut):\n\n\nPlease see long section of the guide.\n\nIf you have a workflow that involves editing in both visual and source mode, you may want to ensure that the same markdown is written no matter which mode edits originate from. You can accomplish this using the canonical option. For example:\n---\ntitle: \"My Document\"\neditor_options:\n  markdown:\n    wrap: 72\n    references: \n      location: block\n    canonical: true\n---\n\nThe editor toolbar includes buttons for the most commonly used formatting commands:\n\nAdditional commands are available on the Format, Insert, and Table menus:\n\n\n\n\n\n\n\n\n\n\n\n\nCommand\nKeyboard Shortcut\nMarkdown Shortcut\n\n\n\nBold\n⌘ B\n**bold**\n\n\nItalic\n⌘ I\n*italic*\n\n\nCode\n⌘ D\n`code`\n\n\nLink\n⌘ K\n&lt;href&gt;\n\n\nHeading 1\n⌥⌘ 1\n#\n\n\nHeading 2\n⌥⌘ 2\n##\n\n\nHeading 3\n⌥⌘ 3\n###\n\n\nR Code Chunk\n⌥⌘ I\n```{r}\n\n\n\nYou can also use the catch-all ⌘/ shortcut to insert just about anything. Just execute the shortcut then type what you want to insert. For example:\nUse the bullet\n\nOutput\n\nOr numbered\n\nNumber list!\n\nHere’s a link - how to turn into an image?\n\nEXAMPLE LINK: https://rstudio.github.io/visual-markdown-editing/images/visual-editing-omni-list.png"
  },
  {
    "objectID": "content/posts/03-computation/visual-editor.html#equations",
    "href": "content/posts/03-computation/visual-editor.html#equations",
    "title": "Visual Editor for Quarto",
    "section": "Equations",
    "text": "Equations\nLaTeX equations are authored using standard Pandoc markdown syntax (the editor will automatically recognize the syntax and treat the equation as math). When you aren’t directly editing an equation it will appear as rendered math:\n\\[\nP(E) = {n \\choose k} p^k (2-p)^{n-k}\n\\]\n\nFootnotes\n\nYou can include footnotes using the Insert -&gt; Footnote command (or the ⇧ ⌘ F7 keyboard shortcut). Footnote editing occurs in a pane immediately below the main document:1 &lt;- NOTE THE FOOTMARK"
  },
  {
    "objectID": "content/posts/03-computation/visual-editor.html#inserting-citations",
    "href": "content/posts/03-computation/visual-editor.html#inserting-citations",
    "title": "Visual Editor for Quarto",
    "section": "Inserting Citations",
    "text": "Inserting Citations\nYou insert citations by either using the Insert -&gt; Citation command or by using markdown syntax directly (e.g. [@cite]).\nCitations go inside square brackets and are separated by semicolons. Each citation must have a key, composed of ‘@’ + the citation identifier from the database, and may optionally have a prefix, a locator, and a suffix. The citation key must begin with a letter, digit, or _, and may contain alphanumerics, _, and internal punctuation characters (:.#$%&-+?&lt;&gt;~/). Here are some examples:\n\n(Rottman-Sagebiel et al. 2018)\nDEMO OF CITATION WITH INSERT"
  },
  {
    "objectID": "content/posts/03-computation/visual-editor.html#embedded-code",
    "href": "content/posts/03-computation/visual-editor.html#embedded-code",
    "title": "Visual Editor for Quarto",
    "section": "Embedded Code",
    "text": "Embedded Code\nSource code which you include in an Quarto document can either by for display only or can be executed by knitr as part of rendering. Code can furthermore be either inline or block (e.g. an Rmd code chunk).\nDisplaying Code\nTo display but not execute code, either use the Insert -&gt; Code Block menu item, or start a new line and type either:\n\n``` (for a plain code block); or\n```&lt;lang&gt; (where &lt;lang&gt; is a language) for a code block with syntax highlighting.\n\nThen press the Enter key. To display code inline, simply surround text with backticks (`code`), or use the Format -&gt; Code menu item.\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\npenguins %&gt;% \n  ggplot(aes(x = body_weight_g, y = flipper_length_mm, color = species)) +\n  geom_point()\nCode Chunks\nTo insert an executable code chunk, use the Insert -&gt; Code Chunk menu item, or start a new line and type:\n```{r}\nThen press the Enter key. Note that r could be another language supported by knitr (e.g. python or sql) and you can also include a chunk label and other chunk options.\nTo include inline R code, you just create normal inline code (e.g. by using backticks or the ⌘ D shortcut) but preface it with r. For example, this inline code will be executed by knitr: 2023-01-05. Note that when the code displays in visual mode it won’t have the backticks (but they will still appear in source mode).\n\npenguin_plot &lt;- penguins %&gt;% \n  na.omit() %&gt;% \n  ggplot(aes(x = body_mass_g, y = flipper_length_mm)) +\n  geom_point(aes(color = species)) +\n  labs(title = \"Important Penguins\") +\n  geom_smooth(method = \"lm\", color = \"black\")\n\npenguin_plot\n\n\n\n\nR generated Tables\n\nlibrary(gt)\n\npenguins %&gt;% \n  na.omit() %&gt;% \n  select(species, bill_length_mm, body_mass_g) %&gt;% \n  head() %&gt;% \n  gt()\n\n\n\n\n\n\nspecies\nbill_length_mm\nbody_mass_g\n\n\n\nAdelie\n39.1\n3750\n\n\nAdelie\n39.5\n3800\n\n\nAdelie\n40.3\n3250\n\n\nAdelie\n36.7\n3450\n\n\nAdelie\n39.3\n3650\n\n\nAdelie\n38.9\n3625\n\n\n\n\n\n\n\nlibrary(reactable)\npenguins %&gt;% \n  filter(species == \"Adelie\") %&gt;% \n  na.omit() %&gt;% \n  select(species, bill_length_mm, body_mass_g) %&gt;% \n  reactable(defaultPageSize = 5)"
  },
  {
    "objectID": "content/posts/03-computation/visual-editor.html#footnotes",
    "href": "content/posts/03-computation/visual-editor.html#footnotes",
    "title": "Visual Editor for Quarto",
    "section": "Footnotes",
    "text": "Footnotes\n\nVery fancy footnote to this portion↩︎"
  },
  {
    "objectID": "content/posts/02-authoring/authoring.html",
    "href": "content/posts/02-authoring/authoring.html",
    "title": "Housing Prices",
    "section": "",
    "text": "In this analysis, we build a model predicting sale prices of houses based on data on houses that were sold in the Duke Forest neighborhood of Durham, NC around November 2020. Let’s start by loading the packages we’ll use for the analysis.\n\nPackageslibrary(openintro)  # for data\nlibrary(tidyverse)  # for data wrangling and visualization\nlibrary(knitr)      # for tables\nlibrary(broom)      # for model summary\n\n\nWe present the results of exploratory data analysis in Section 2 and the regression model in Section 3."
  },
  {
    "objectID": "content/posts/02-authoring/authoring.html#introduction",
    "href": "content/posts/02-authoring/authoring.html#introduction",
    "title": "Housing Prices",
    "section": "",
    "text": "In this analysis, we build a model predicting sale prices of houses based on data on houses that were sold in the Duke Forest neighborhood of Durham, NC around November 2020. Let’s start by loading the packages we’ll use for the analysis.\n\nPackageslibrary(openintro)  # for data\nlibrary(tidyverse)  # for data wrangling and visualization\nlibrary(knitr)      # for tables\nlibrary(broom)      # for model summary\n\n\nWe present the results of exploratory data analysis in Section 2 and the regression model in Section 3."
  },
  {
    "objectID": "content/posts/02-authoring/authoring.html#sec-eda",
    "href": "content/posts/02-authoring/authoring.html#sec-eda",
    "title": "Housing Prices",
    "section": "\n2 Exploratory data analysis",
    "text": "2 Exploratory data analysis\nThe data contains 98 houses. As part of the exploratory analysis let’s visualize and summarize the relationship between areas and prices of these houses.\n\n2.1 Data visualization\nFigure 1 shows two histograms displaying the distributions of price and area individually.\n\nCodeggplot(duke_forest, aes(x = price)) +\n  geom_histogram(binwidth = 50000) +\n  labs(title = \"Histogram of prices\")\n\nggplot(duke_forest, aes(x = area)) +\n  geom_histogram(binwidth = 250) +\n  labs(title = \"Histogram of areas\")\n\n\n\n\n\n(a) Histogram of prices\n\n\n\n\n\n(b) Histogram of areas\n\n\n\nFigure 1: Histograms of individual variables\n\n\n\nFigure 2 displays the relationship between these two variables in a scatterplot.\n\nCodeggplot(duke_forest, aes(x = area, y = price)) +\n  geom_point() +\n  labs(title = \"Price and area of houses in Duke Forest\")\n\n\n\nFigure 2: Scatterplot of price vs. area of houses in Duke Forest\n\n\n\n\n2.2 Summary statistics\nTable 1 displays basic summary statistics for these two variables.\n\nCodeduke_forest %&gt;%\n  summarise(\n    `Median price` = median(price),\n    `IQR price` = IQR(price),\n    `Median area` = median(area),\n    `IQR area` = IQR(area),\n    `Correlation, r` = cor(price, area)\n    ) %&gt;%\n  kable(digits = c(0, 0, 0, 0, 2))\n\n\n\nTable 1: Summary statistics for price and area of houses in Duke Forest\n\nMedian price\nIQR price\nMedian area\nIQR area\nCorrelation, r\n\n\n540000\n193125\n2623\n1121\n0.67"
  },
  {
    "objectID": "content/posts/02-authoring/authoring.html#sec-model",
    "href": "content/posts/02-authoring/authoring.html#sec-model",
    "title": "Housing Prices",
    "section": "\n3 Modeling",
    "text": "3 Modeling\nWe can fit a simple linear regression model of the form shown in Equation 1.\n\nprice = \\hat{\\beta_0} + \\hat{\\beta_1} \\times area + \\epsilon\n\\tag{1}\nTable 2 shows the regression output for this model.\n\nCodeprice_fit &lt;- lm(price ~ area, data = duke_forest)\n  \nprice_fit %&gt;%\n  tidy() %&gt;%\n  kable(digits = c(0, 0, 2, 2, 2))\n\n\n\nTable 2: Linear regression model for predicting price from area\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n(Intercept)\n116652\n53302.46\n2.19\n0.03\n\n\narea\n159\n18.17\n8.78\n0.00\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis is a pretty incomplete analysis, but hopefully the document provides a good overview of some of the authoring features of Quarto !"
  },
  {
    "objectID": "content/posts/02-authoring/authoring.html#crossreferences",
    "href": "content/posts/02-authoring/authoring.html#crossreferences",
    "title": "Housing Prices",
    "section": "\n4 Crossreferences",
    "text": "4 Crossreferences\nWe present the results of exploratory data analysis in Section 2 and the regression model in Section 3 .\nFigure 2 displays the relationship between these two variables in a scatterplot.\nTable 1 displays basic summary statistics for these two variables.\nWe can fit a simple linear regression model of the form shown in Equation 1."
  },
  {
    "objectID": "content/labs/r-labs/time/index.html",
    "href": "content/labs/r-labs/time/index.html",
    "title": "Lab 01 - Introduce Yourself",
    "section": "",
    "text": "At the end of this Lab, we will:\n\nhave installed R and RStudio on our machines\nunderstood how to add additional R-packages for specific features and graphic capability\nrun code within RStudio and interpret the results\nhave learnt to look for help within R and RStudio"
  },
  {
    "objectID": "content/labs/r-labs/time/index.html#Check-In-R",
    "href": "content/labs/r-labs/time/index.html#Check-In-R",
    "title": "Lab 01 - Introduce Yourself",
    "section": "\n3.1 Check in",
    "text": "3.1 Check in\nLaunch R by clicking this logo. You should see one console with a command line interpreter. Try typing 2 + 2 and check !\nClose R."
  },
  {
    "objectID": "content/labs/r-labs/time/index.html#Check-In-RStudio",
    "href": "content/labs/r-labs/time/index.html#Check-In-RStudio",
    "title": "Lab 01 - Introduce Yourself",
    "section": "\n4.1 Check in",
    "text": "4.1 Check in\nLaunch RStudio. You should get a window similar to the screenshot you see here, but yours will be empty. Look at the bottom left pane: this is the same console window you saw when you opened R in step @Check-In-R\n\nPlace your cursor where you see &gt; and type x &lt;- 2 + 2 again hit enter or return, then type x, and hit enter/return again.\nIf [1] 4 prints to the screen, you have successfully installed R and RStudio, and you can move onto installing packages."
  },
  {
    "objectID": "content/labs/r-labs/time/index.html#save-and-share",
    "href": "content/labs/r-labs/time/index.html#save-and-share",
    "title": "Lab 01 - Introduce Yourself",
    "section": "\n7.1 Save and share",
    "text": "7.1 Save and share\nSave your work so you can share your favorite plot with us. You will not like the looks of your plot if you mouse over to Export and save it. Instead, use ggplot2’s command for saving a plot with sensible defaults:\n\nhelp(ggsave)\n\n\nggsave(\"file_name_here.pdf\", plot) # please make the filename unique!\n\nUpload this exported plot to Teams -&gt; Assignments.\nBefore you do that, check that you can submit stuff/assignments on the MLS by uploading a cat picture 🐈 first, especially if you are an ailurophile like me. (Acceptable Breeds: Scottish Folds or Maine Coons)"
  },
  {
    "objectID": "content/labs/r-labs/tidy/dplyr.html",
    "href": "content/labs/r-labs/tidy/dplyr.html",
    "title": "Introduction to the dplyr package",
    "section": "",
    "text": "One of the dominant paradigms of working with data in R is to render it into “tidy” form. A huge benefit of the tidy way of working is that it influences your thinking with data and helps plan out your operations, in going from purpose to actual code in a swift and intuitive manner. This tidy form allows for a huge variety of data manipulation, summarizing, and plotting tasks, that can be performed using the packages of the tidyverse, and other packages that leverage the power of the tidyverse."
  },
  {
    "objectID": "content/labs/r-labs/tidy/dplyr.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "href": "content/labs/r-labs/tidy/dplyr.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "title": "Introduction to the dplyr package",
    "section": "",
    "text": "One of the dominant paradigms of working with data in R is to render it into “tidy” form. A huge benefit of the tidy way of working is that it influences your thinking with data and helps plan out your operations, in going from purpose to actual code in a swift and intuitive manner. This tidy form allows for a huge variety of data manipulation, summarizing, and plotting tasks, that can be performed using the packages of the tidyverse, and other packages that leverage the power of the tidyverse."
  },
  {
    "objectID": "content/labs/r-labs/tidy/dplyr.html#setting-up-the-packages",
    "href": "content/labs/r-labs/tidy/dplyr.html#setting-up-the-packages",
    "title": "Introduction to the dplyr package",
    "section": "\n2 Setting up the Packages",
    "text": "2 Setting up the Packages\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.0\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "content/labs/r-labs/tidy/dplyr.html#tidy-data",
    "href": "content/labs/r-labs/tidy/dplyr.html#tidy-data",
    "title": "Introduction to the dplyr package",
    "section": "\n3 Tidy Data",
    "text": "3 Tidy Data\n\ndata(starwars)\ndim(starwars)\n\n[1] 87 14\n\nstarwars\n\n\n\n  \n\n\n\n“Tidy Data” is an important way of thinking about what data typically look like in R. Let’s fetch a figure from the web to show the (preferred) structure of data in R.\n\n\nTidy Data\n\nThe three features described in the figure above define the nature of tidy data:\n\nVariables in Columns\n\nObservations in Rows and\n\nMeasurements in Cells.\n\nData are imagined to be resulting from an experiment. Each variable represents a parameter/aspect in the experiment. Each row represents an additional datum of measurement. A cell is a single measurement on a single parameter(column) in a single observation(row).\nWhen working with data you must:\n\nFigure out what you want to do. (Purpose)\nDescribe those tasks in the form of a computer program. (Plain English to R Code)\nExecute the program.\n\nThe dplyr package makes these steps fast and easy:\n\nBy constraining your options, it helps you think about your data manipulation challenges.\nIt provides simple “verbs”, functions that correspond to the most common data manipulation tasks, to help you translate your thoughts into code.\nIt uses efficient backends, so you spend less time waiting for the computer.\n\n\nNe’er you mind about backends ;-) See Shakespeare’s Hamlet.\n\nThis document introduces you to dplyr’s basic set of tools, and shows you how to apply them to data frames. dplyr also supports databases via the dbplyr package, once you’ve installed, read vignette(\"dbplyr\") to learn more."
  },
  {
    "objectID": "content/labs/r-labs/tidy/dplyr.html#data-starwars",
    "href": "content/labs/r-labs/tidy/dplyr.html#data-starwars",
    "title": "Introduction to the dplyr package",
    "section": "\n4 Data: starwars",
    "text": "4 Data: starwars\nTo explore the basic data manipulation verbs of dplyr, we’ll use the dataset starwars. This dataset contains 87 characters and comes from the Star Wars API, and is documented in ?starwars\n\nThis means: type ?starwars in the Console. Try.\n\nNote that starwars is a tibble, a modern re-imagining of the data frame. It’s particularly useful for large datasets because it only prints the first few rows. You can learn more about tibbles at https://tibble.tidyverse.org; in particular you can convert data frames to tibbles with as_tibble().\n\nCheck your Environment Tab to inspect starwars in a separate tab."
  },
  {
    "objectID": "content/labs/r-labs/tidy/dplyr.html#single-table-verbs",
    "href": "content/labs/r-labs/tidy/dplyr.html#single-table-verbs",
    "title": "Introduction to the dplyr package",
    "section": "\n5 Single table verbs",
    "text": "5 Single table verbs\ndplyr aims to provide a function for each basic verb of data manipulation. These verbs can be organised into three categories based on the component of the dataset that they work with:\n\nRows:\n\n\nfilter() chooses rows based on column values.\n\nslice() chooses rows based on location.\n\narrange() changes the order of the rows.\n\n\nColumns:\n\n\nselect() changes whether or not a column is included.\n\nrename() changes the name of columns.\n\nmutate() changes the values of columns and creates new columns.\n\nrelocate() changes the order of the columns.\n\n\nGroups of rows:\n\n\nsummarise() collapses a group into a single row.\n\n\n\n\nThink of the parallels from Microsoft Excel.\n\n\n5.1 The pipe\nAll of the dplyr functions take a data frame (or tibble) as the first argument. Rather than forcing the user to either save intermediate objects or nest functions, dplyr provides the %&gt;% operator from magrittr. x %&gt;% f(y) turns into f(x, y) so the result from one step is then “piped” into the next step. You can use the pipe to rewrite multiple operations that you can read left-to-right, top-to-bottom (reading the pipe operator as “then”).\n\n5.2 Filter rows with filter()\n\nfilter() allows you to select a subset of rows in a data frame. Like all single verbs, the first argument is the tibble (or data frame). The second and subsequent arguments refer to variables within that data frame, selecting rows where the expression is TRUE.\nFor example, we can select all character with light skin color and brown eyes with:\n\nNote the double equal to sign (==) below! Equivalent to MS Excel Data -&gt; Filter\n\n\nstarwars %&gt;% filter(skin_color == \"light\", eye_color == \"brown\")\n\n\n\n  \n\n\n\n\n5.3 Arrange rows with arrange()\n\narrange() works similarly to filter() except that instead of filtering or selecting rows, it reorders them. It takes a data frame, and a set of column names (or more complicated expressions) to order by. If you provide more than one column name, each additional column will be used to break ties in the values of preceding columns:\n\nstarwars %&gt;% arrange(height, mass)\n\n\n\n  \n\n\n\nUse desc() to order a column in descending order:\n\nstarwars %&gt;% arrange(desc(height))\n\n\n\n  \n\n\n\n\n5.4 Choose rows using their position with slice()\n\nslice() lets you index rows by their (integer) locations. It allows you to select, remove, and duplicate rows.\n\nThis is an important step in Prediction, Modelling and Machine Learning.\n\nWe can get characters from row numbers 5 through 10.\n\nstarwars %&gt;% slice(5:10)\n\n\n\n  \n\n\n\nIt is accompanied by a number of helpers for common use cases:\n\n\nslice_head() and slice_tail() select the first or last rows.\n\n\nstarwars %&gt;% slice_head(n = 3)\n\n\n\n  \n\n\n\n\n\nslice_sample() randomly selects rows. Use the option prop to choose a certain proportion of the cases.\n\n\nstarwars %&gt;% slice_sample(n = 5)\n\n\n\n  \n\n\nstarwars %&gt;% slice_sample(prop = 0.1)\n\n\n\n  \n\n\n\nUse replace = TRUE to perform a bootstrap sample. If needed, you can weight the sample with the weight argument.\n\nBootstrap samples are a special statistical sampling method. Counterintuitive perhaps, since you sample with replacement. Should remind you of your high school Permutation and Combination class, with all those urn models and so on. If you remember.\n\n\n\nslice_min() and slice_max() select rows with highest or lowest values of a variable. Note that we first must choose only the values which are not NA.\n\n\nstarwars %&gt;%\n  filter(!is.na(height)) %&gt;%\n  slice_min(height, n = 3)\n\n\n\n  \n\n\n\n\n5.5 Select columns with select()\n\nOften you work with large datasets with many columns but only a few are actually of interest to you. select() allows you to rapidly zoom in on a useful subset using operations that usually only work on numeric variable positions:\n\n# Select columns by name\nstarwars %&gt;% select(hair_color, skin_color, eye_color)\n\n\n\n  \n\n\n# Select all columns between hair_color and eye_color (inclusive)\nstarwars %&gt;% select(hair_color:eye_color)\n\n\n\n  \n\n\n# Select all columns except those from hair_color to eye_color (inclusive)\nstarwars %&gt;% select(!(hair_color:eye_color))\n\n\n\n  \n\n\n# Select all columns ending with color\nstarwars %&gt;% select(ends_with(\"color\"))\n\n\n\n  \n\n\n\nThere are a number of helper functions you can use within select(), like starts_with(), ends_with(), matches() and contains(). These let you quickly match larger blocks of variables that meet some criterion. See ?select for more details.\nYou can rename variables with select() by using named arguments:\n\nstarwars %&gt;% select(home_world = homeworld)\n\n\n\n  \n\n\n\nBut because select() drops all the variables not explicitly mentioned, it’s not that useful. Instead, use rename():\n\nstarwars %&gt;% rename(home_world = homeworld)\n\n\n\n  \n\n\n\n\n5.6 Add new columns with mutate()\n\nBesides selecting sets of existing columns, it’s often useful to add new columns that are functions of existing columns. This is the job of mutate():\n\nstarwars %&gt;% mutate(height_m = height / 100)\n\n\n\n  \n\n\n\nWe can’t see the height in meters we just calculated, but we can fix that using a select command.\n\nstarwars %&gt;%\n  mutate(height_m = height / 100) %&gt;%\n  select(height_m, height, everything())\n\n\n\n  \n\n\n\ndplyr::mutate() is similar to the base transform(), but allows you to refer to columns that you’ve just created:\n\nstarwars %&gt;%\n  mutate(\n    height_m = height / 100,\n    BMI = mass / (height_m^2)\n  ) %&gt;%\n  select(BMI, everything())\n\n\n\n  \n\n\n\nIf you only want to keep the new variables, use transmute():\n\nstarwars %&gt;%\n  transmute(\n    height_m = height / 100,\n    BMI = mass / (height_m^2)\n  )\n\n\n\n  \n\n\n\n\n5.7 Change column order with relocate()\n\nUse a similar syntax as select() to move blocks of columns at once\n\nstarwars %&gt;% relocate(sex:homeworld, .before = height)\n\n\n\n  \n\n\n\n\n5.8 Summarise values with summarise()\n\nThe last verb is summarise(). It collapses a data frame to a single row.\n\nstarwars %&gt;% summarise(mean_height = mean(height, na.rm = TRUE))\n\n\n\n  \n\n\n\nIt’s not that useful until we learn the group_by() verb below.\n\n5.9 Commonalities\nYou may have noticed that the syntax and function of all these verbs are very similar:\n\nThe first argument is a data frame.\nThe subsequent arguments describe what to do with the data frame. You can refer to columns in the data frame directly without using $.\nThe result is a new data frame\n\nTogether these properties make it easy to chain together multiple simple steps to achieve a complex result.\nThese five functions provide the basis of a language of data manipulation. At the most basic level, you can only alter a tidy data frame in five useful ways: you can reorder the rows (arrange()), pick observations and variables of interest (filter() and select()), add new variables that are functions of existing variables (mutate()), or collapse many values to a summary (summarise())."
  },
  {
    "objectID": "content/labs/r-labs/tidy/dplyr.html#combining-functions-with",
    "href": "content/labs/r-labs/tidy/dplyr.html#combining-functions-with",
    "title": "Introduction to the dplyr package",
    "section": "\n6 Combining functions with %>%\n",
    "text": "6 Combining functions with %&gt;%\n\nThe dplyr API is functional in the sense that function calls don’t have side-effects. You must always save their results. This doesn’t lead to particularly elegant code, especially if you want to do many operations at once. You either have to do it step-by-step:\n\na1 &lt;- group_by(starwars, species, sex)\na2 &lt;- select(a1, height, mass)\na3 &lt;- summarise(a2,\n  height = mean(height, na.rm = TRUE),\n  mass = mean(mass, na.rm = TRUE)\n)\n\nOr if you don’t want to name the intermediate results, you need to wrap the function calls inside each other:\n\nsummarise(\n  select(\n    group_by(starwars, species, sex),\n    height, mass\n  ),\n  height = mean(height, na.rm = TRUE),\n  mass = mean(mass, na.rm = TRUE)\n)\n\nAdding missing grouping variables: `species`, `sex`\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\n\n\n  \n\n\n\nThis is difficult to read because the order of the operations is from inside to out. Thus, the arguments are a long way away from the function. To get around this problem, dplyr provides the %&gt;% operator from magrittr. x %&gt;% f(y) turns into f(x, y) so you can use it to rewrite multiple operations that you can read left-to-right, top-to-bottom (reading the pipe operator as “then”):\n\nstarwars %&gt;%\n  group_by(species, sex) %&gt;%\n  summarise(\n    mean_height = mean(height, na.rm = TRUE),\n    mean_mass = mean(mass, na.rm = TRUE)\n  )\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument."
  },
  {
    "objectID": "content/labs/r-labs/tidy/dplyr.html#patterns-of-operations",
    "href": "content/labs/r-labs/tidy/dplyr.html#patterns-of-operations",
    "title": "Introduction to the dplyr package",
    "section": "\n7 Patterns of operations",
    "text": "7 Patterns of operations\nThe dplyr verbs can be classified by the type of operations they accomplish (we sometimes speak of their semantics, i.e., their meaning). It’s helpful to have a good grasp of the difference between select and mutate operations.\n\n7.1 Selecting operations\nOne of the appealing features of dplyr is that you can refer to columns from the tibble as if they were regular variables. However, the syntactic uniformity of referring to bare column names hides semantical differences across the verbs. A column symbol supplied to select() does not have the same meaning as the same symbol supplied to mutate().\nSelecting operations expect column names and positions. Hence, when you call select() with bare variable names, they actually represent their own positions in the tibble. The following calls are completely equivalent from dplyr’s point of view:\n\n# `name` represents the integer 1\nselect(starwars, name)\n\n\n\n  \n\n\nselect(starwars, 1)\n\n\n\n  \n\n\n\nBy the same token, this means that you cannot refer to variables from the surrounding context if they have the same name as one of the columns. In the following example, height still represents 2, not 5:\n\nheight &lt;- 5\nselect(starwars, height)\n\n\n\n  \n\n\n\nOne useful subtlety is that this only applies to bare names and to selecting calls like c(height, mass) or height:mass. In all other cases, the columns of the data frame are not put in scope. This allows you to refer to contextual variables in selection helpers:\n\nname &lt;- \"color\"\nselect(starwars, ends_with(name))\n\n\n\n  \n\n\n\nThese semantics are usually intuitive. But note the subtle difference:\n\nname &lt;- 5\nselect(starwars, name, identity(name))\n\n\n\n  \n\n\n\nIn the first argument, name represents its own position 1. In the second argument, name is evaluated in the surrounding context and represents the fifth column.\n\n7.2 Mutating operations\nMutate semantics are quite different from selection semantics. Whereas select() expects column names or positions, mutate() expects column vectors. We will set up a smaller tibble to use for our examples.\n\ndf &lt;- starwars %&gt;% select(name, height, mass)\n\nWhen we use select(), the bare column names stand for their own positions in the tibble. For mutate() on the other hand, column symbols represent the actual column vectors stored in the tibble. Consider what happens if we give a string or a number to mutate():\n\nmutate(df, \"height\", 2)\n\n\n\n  \n\n\n\nmutate() gets length-1 vectors that it interprets as new columns in the data frame. These vectors are recycled so they match the number of rows. That’s why it doesn’t make sense to supply expressions like \"height\" + 10 to mutate(). This amounts to adding 10 to a string! The correct expression is:\n\nmutate(df, height + 10)\n\n\n\n  \n\n\n\nIn the same way, you can unquote values from the context if these values represent a valid column. They must be either length 1 (they then get recycled) or have the same length as the number of rows. In the following example we create a new vector that we add to the data frame:\n\nvar &lt;- seq(1, nrow(df))\nmutate(df, new = var)\n\n\n\n  \n\n\n\nA case in point is group_by(). While you might think it has select semantics, it actually has mutate semantics. This is quite handy as it allows to group by a modified column:\n\ngroup_by(starwars, sex)\n\n\n\n  \n\n\ngroup_by(starwars, sex = as.factor(sex))\n\n\n\n  \n\n\ngroup_by(starwars, height_binned = cut(height, 3))\n\n\n\n  \n\n\n\nThis is why you can’t supply a column name to group_by(). This amounts to creating a new column containing the string recycled to the number of rows:\n\ngroup_by(df, \"month\")"
  },
  {
    "objectID": "content/labs/r-labs/tidy/dplyr.html#two-table-verbs",
    "href": "content/labs/r-labs/tidy/dplyr.html#two-table-verbs",
    "title": "Introduction to the dplyr package",
    "section": "\n8 Two table verbs",
    "text": "8 Two table verbs\nSometimes our data is spread across more than one table. Often these tables are linked by some common, or common-looking, variable columns. dplyr allows us to work with such data that is spread over more than one table. More information is available here: Two Table Verbs in dplyr\nThe operations/verbs used to manipulate two-table verbs are:\n\nMutating joins, which add new variables to one table from matching rows in another.\n\ninner_join()\n\n\n\n\n\n\n\n\n\nleft_join()\n\n\n\n\n\n\n\nright_join()\n\n\n\n\n\n\n\nfull_join()\n\n\n\n\n\n\n\nFiltering joins, which filter observations from one table based on whether or not they match an observation in the other table.\n\n\nsemi_join(x, y) keeps all observations in x that have a match in y.\n\n\n\n\n\n\n\n\n\n\nanti_join(x, y) drops all observations in x that have a match in\n\n\n\n\n\n\n\n\n\n\n\n\nSet operations, which combine the observations in the data sets as if they were set elements.\n\nunion()\n\n\n\n\n\n\n\n\n\nunion_all(),\n\n\n\n\n\n\n\nintersect(),\n\n\n\n\n\n\n\nsetdiff()\n\n\n\n\n\n\n\nTidyr Operations:\npivot_longer()\npivot_wider()"
  },
  {
    "objectID": "content/labs/r-labs/pronouns/pronouns.html",
    "href": "content/labs/r-labs/pronouns/pronouns.html",
    "title": "Lab-02: Pronouns and Data",
    "section": "",
    "text": "Understand different kinds of data variables\nAppreciate how they can be identified based on the Interrogative Pronouns they answer to\nUnderstand how each kind of variable lends itself to a specific geometric aspect in the data visualization.\nUnderstand how ask Questions of Data to develop Visualizations"
  },
  {
    "objectID": "content/labs/r-labs/pronouns/pronouns.html#set-up",
    "href": "content/labs/r-labs/pronouns/pronouns.html#set-up",
    "title": "Lab-02: Pronouns and Data",
    "section": "\n2.1 Set Up",
    "text": "2.1 Set Up\nThe setup code chunk below brings into our coding session R packages that provide specific computational abilities and also datasets which we can use.\nTo reiterate: Packages and datasets are not the same thing !! Packages are (small) collections of programs. Datasets are just….information."
  },
  {
    "objectID": "content/labs/r-labs/pronouns/pronouns.html#the-penguins-dataset",
    "href": "content/labs/r-labs/pronouns/pronouns.html#the-penguins-dataset",
    "title": "Lab-02: Pronouns and Data",
    "section": "\n5.1 The penguins dataset",
    "text": "5.1 The penguins dataset\n\nnames(penguins) # Column, i.e. Variable names\n\n[1] \"species\"           \"island\"            \"bill_length_mm\"   \n[4] \"bill_depth_mm\"     \"flipper_length_mm\" \"body_mass_g\"      \n[7] \"sex\"               \"year\"             \n\nhead(penguins) # first six rows\n\n\n\n  \n\n\ntail(penguins) # Last six rows\n\n\n\n  \n\n\ndim(penguins) # Size of dataset\n\n[1] 344   8\n\n# Check for missing data\nany(is.na(penguins) == TRUE)\n\n[1] TRUE\n\n\n\n\n\n\n\n\nInspect the Data\n\n\n\n\nWhat are the variable names()?\nWhat would be the Question you might have asked to obtain each of the variables?\nWhat further questions/meta questions would you ask to “process” that variable? ( Hint: Add another word after any of the Interrogative Pronouns, e.g. How…MANY?)\nWhere might the answers take your story?\n\n\n\n\n\n\n\n\n\nYour Turn #1\n\n\n\nState a few questions after discussion with your friend and state possible variables, or what you could DO with the variables, as an answer.\nE.g. Q. How many penguins? A. We need to count…rows?"
  },
  {
    "objectID": "content/labs/r-labs/pronouns/pronouns.html#pronouns-and-variables",
    "href": "content/labs/r-labs/pronouns/pronouns.html#pronouns-and-variables",
    "title": "Lab-02: Pronouns and Data",
    "section": "\n5.2 Pronouns and Variables",
    "text": "5.2 Pronouns and Variables\nIn the Table below, we have a rough mapping of interrogative pronouns to the kinds of variables in the data:\n\n\n\n\n\nNo\nPronoun\nAnswer\nVariable/Scale\nExample\nWhat Operations?\n\n\n\n1\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities, with Scale and a Zero Value.Differences and Ratios /Products are meaningful.\nQuantitative/Ratio\nLength,Height,Temperature in Kelvin,Activity,Dose Amount,Reaction Rate,Flow Rate,Concentration,Pulse,Survival Rate\nCorrelation\n\n\n2\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities with Scale. Differences are meaningful, but not products or ratios\nQuantitative/Interval\npH,SAT score(200-800),Credit score(300-850),SAT score(200-800),Year of Starting College\nMean,Standard Deviation\n\n\n3\nHow, What Kind, What Sort\nA Manner / Method, Type or Attribute from a list, with list items in some \" order\" ( e.g. good, better, improved, best..)\nQualitative/Ordinal\nSocioeconomic status (Low income, Middle income, High income),Education level (HighSchool, BS, MS, PhD),Satisfaction rating(Very much Dislike, Dislike, Neutral, Like, Very Much Like)\nMedian,Percentile\n\n\n4\nWhat, Who, Where, Whom, Which\nName, Place, Animal, Thing\nQualitative/Nominal\nName\nCount no. of cases,Mode\n\n\n\n\n\n\nAs you go from Qualitative to Quantitative data types in the table, I hope you can detect a movement from fuzzy groups/categories to more and more crystallized numbers. Each variable/scale can be subjected to the operations of the previous group. In the words of S.S. Stevens (https://psychology.okstate.edu/faculty/jgrice/psyc3214/Stevens_FourScales_1946.pdf)\n\nthe basic operations needed to create each type of scale is cumulative: to an operation listed opposite a particular scale must be added all those operations preceding it.\n\nDo think about this as you work with data.\n\nDo take a look at these references:\n\nhttps://stats.idre.ucla.edu/other/mult-pkg/whatstat/what-is-the-difference-between-categorical-ordinal-and-interval-variables/\nhttps://www.freecodecamp.org/news/types-of-data-in-statistics-nominal-ordinal-interval-and-ratio-data-types-explained-with-examples/"
  },
  {
    "objectID": "content/labs/r-labs/pronouns/pronouns.html#the-mpg-dataset",
    "href": "content/labs/r-labs/pronouns/pronouns.html#the-mpg-dataset",
    "title": "Lab-02: Pronouns and Data",
    "section": "\n5.3 The mpg dataset",
    "text": "5.3 The mpg dataset\n\nnames(mpg) # Column, i.e. Variable names\n\n [1] \"manufacturer\" \"model\"        \"displ\"        \"year\"         \"cyl\"         \n [6] \"trans\"        \"drv\"          \"cty\"          \"hwy\"          \"fl\"          \n[11] \"class\"       \n\nhead(mpg) # first six rows\n\n\n\n  \n\n\ntail(mpg) # Last six rows\n\n\n\n  \n\n\ndim(mpg) # Size of dataset\n\n[1] 234  11\n\n# Check for missing data\nany(is.na(mpg) == TRUE)\n\n[1] FALSE\n\n\n\n5.3.1 YOUR TURN-2\nLook carefully at the variables here. How would you interpret say the cyl variable? Is it a number and therefore Quantitative, or could it be something else?"
  },
  {
    "objectID": "content/labs/r-labs/pronouns/pronouns.html#single-qualitativecategorical-nominal-variable",
    "href": "content/labs/r-labs/pronouns/pronouns.html#single-qualitativecategorical-nominal-variable",
    "title": "Lab-02: Pronouns and Data",
    "section": "\n6.1 Single Qualitative/Categorical/ Nominal Variable",
    "text": "6.1 Single Qualitative/Categorical/ Nominal Variable\n\nQuestions: Which? What Kind? How? How many of each Kind?\n\n\nIsland ( Which island ? )\nSpecies ( Which Species? )\n\n\nCalculations: No of levels / Counts for each level\n\n\n\n\ncount / tally of no. of penguins on each island or in each species\n\nsort and order by island or species\n\n\nCharts: Bar Chart / Pie Chart / Tree Map\n\n\n\ngeom_bar / geom_bar + coord_polar() / Find out!!\n\n\npenguins %&gt;% count(species)\n\n\n\n  \n\n\n\n\nggplot(penguins) + geom_bar(aes(x = island))\n\n\n\nggplot(penguins) + geom_bar(aes(x = sex))\n\n\n\n\n\n6.1.1 YOUR TURN-3"
  },
  {
    "objectID": "content/labs/r-labs/pronouns/pronouns.html#single-quantitative-variable",
    "href": "content/labs/r-labs/pronouns/pronouns.html#single-quantitative-variable",
    "title": "Lab-02: Pronouns and Data",
    "section": "\n6.2 Single Quantitative Variable",
    "text": "6.2 Single Quantitative Variable\n\nQuestions: How many? How few? How often? How much?\nCalculations: max / min / mean / mode / (units)\n\n\n\nmax(), min(), range(), mean(), mode(), summary()\n\n\n\nCharts: Bar Chart / Histogram / Density\n\n\ngeom_histogram() / geom_density()\n\n\n\n\n\nmax(penguins$bill_length_mm)\n\n[1] 59.6\n\nrange(penguins$bill_length_mm, na.rm =TRUE) \n\n[1] 32.1 59.6\n\nsummary(penguins$flipper_length_mm)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    172     190     197     201     213     231 \n\n\n\nggplot(penguins) + geom_density(aes(bill_length_mm))\n\n\n\nggplot(penguins) + geom_histogram(aes(x = bill_length_mm))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n6.2.1 YOUR TURN-4\nAre all the above Quantitative variables ratio variables? Justify."
  },
  {
    "objectID": "content/labs/r-labs/pronouns/pronouns.html#two-variables-quantitative-vs-quantitative",
    "href": "content/labs/r-labs/pronouns/pronouns.html#two-variables-quantitative-vs-quantitative",
    "title": "Lab-02: Pronouns and Data",
    "section": "\n6.3 Two Variables: Quantitative vs Quantitative",
    "text": "6.3 Two Variables: Quantitative vs Quantitative\nWe can easily extend our intuition about one quantitative variable, to a pair of them. What Questions can we ask?\n\nQuestions: How many of this vs How many of that? Does this depend upon that? How are they related? (Remember \\(y = mx + c\\) and friends?)\nCalculations: Correlation / Covariance / T-test / Chi-Square Test for Two Means etc. We won’t go into this here !\nCharts: Scatter Plot / Line Plot / Regression i.e. best fit lines\n\n\ncor(penguins$bill_length_mm, penguins$bill_depth_mm)\n\n[1] -0.2286256\n\n\n\nggplot(penguins) +\n  geom_point(aes(x = flipper_length_mm,\n                 y = body_mass_g))\n\n\n\nggplot(penguins) +\n  geom_point(aes(x = flipper_length_mm, \n                 y = bill_length_mm))\n\n\n\n\n\n6.3.1 YOUR TURN-5"
  },
  {
    "objectID": "content/labs/r-labs/pronouns/pronouns.html#two-variables-categorical-vs-categorical",
    "href": "content/labs/r-labs/pronouns/pronouns.html#two-variables-categorical-vs-categorical",
    "title": "Lab-02: Pronouns and Data",
    "section": "\n6.4 Two Variables: Categorical vs Categorical",
    "text": "6.4 Two Variables: Categorical vs Categorical\nWhat sort of question could we ask that involves two categorical variables?\n\nQuestions: How Many of this Kind( ~x) are How Many of that Kind( ~y ) ?\n\nCalculations: Counts and Tallies sliced by Category\n\n\ncounts , tally\n\n\n\n\nCharts: Stacked Bar Charts / Grouped Bar Charts / Segmented Bar Chart / Mosaic Chart\n\ngeom_bar()\nUse the second Categorical variables to modify fill, color.\nAlso try to vary the parameter position of the bars.\n\n\n\n\n\n\n\nggplot(penguins) + geom_bar(aes(x = island, \n                                fill = species),\n                            position = \"stack\")\n\n\n\n\nStoryline: तीन पेनगीन। और तुम भी तीन(Oh never mind!)\n\n6.4.1 YOUR TURN-6"
  },
  {
    "objectID": "content/labs/r-labs/pronouns/pronouns.html#two-variables-quantitative-vs-qualitative",
    "href": "content/labs/r-labs/pronouns/pronouns.html#two-variables-quantitative-vs-qualitative",
    "title": "Lab-02: Pronouns and Data",
    "section": "\n6.5 Two Variables: Quantitative vs Qualitative",
    "text": "6.5 Two Variables: Quantitative vs Qualitative\nFinally, what if we want to look at Quant variables and Qual variables together? What questions could we ask?\n\nQuestions: How much of this is Which Kind of that? How many vs Which? How many vs How?\nCalculations: Counts, Means, Ranges etc., grouped by Categorical variable.\n\n\nggplot(penguins) + \n    geom_density(aes(x = body_mass_g, \n                 color = island, \n                 fill = island), \n                 alpha = 0.3)\n\n\n\n\n\nCharts: Bar Chart using group / density plots by group / violin plots by group / box plots by group\n\n\n\ngeom_bar / geom_density / geom_violin / geom_boxplot using Categorical Variable for grouping\n\n\nggplot(penguins) + \n    geom_density(aes(x = body_mass_g, \n                 color = island, \n                 fill = island), \n                 alpha = 0.3)\n\n\n\nggplot(penguins) + \n  geom_histogram(aes(x = flipper_length_mm,\n                 fill = sex))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n6.5.1 YOUR TURN-7\n\n6.5.2 Time to Play\n\nCreate a fresh RMarkdown and similarly analyse two datasets of the following data sets\n\n\nAny dataset in your R installation. Type data() in your console to see what is available.\ndiamonds . This dataset is part of the tidyverse package so just type diamonds in your code and there it is.\ngapminder !! Yes!!You will need to install the gapminder package to access this dataset\nmosaicData package datasets. Install mosaicData\ndata.world: Find Datasets of your choice: https://docs.data.world/en/64499-64516-Quickstarts-and-tutorials.html\nkaggle: https://www.kaggle.com/datasets"
  },
  {
    "objectID": "content/labs/r-labs/maps/PlayingwithLeaflet.html",
    "href": "content/labs/r-labs/maps/PlayingwithLeaflet.html",
    "title": "Playing with Leaflet",
    "section": "",
    "text": "This Tutorial works through the ideas at Leaflet\n\nLeaflet is a JavaScript library for creating dynamic maps that support panning and zooming along with various annotations like markers, polygons, and popups.\n\nIn this tutorial we will work only with vector data. In a second part, we will work with raster data in leaflet.\n\nlibrary(tidyverse)\nlibrary(leaflet)\nlibrary(maps)\nlibrary(sp)\nlibrary(sf)\n\n# Data\nlibrary(osmdata) # Import OSM Vector Data into R\nlibrary(osmplotr) # Creating maps with OSM data in R\n# library(OpenStreetMap) # Raster Data"
  },
  {
    "objectID": "content/labs/r-labs/maps/PlayingwithLeaflet.html#add-shapes-to-a-map",
    "href": "content/labs/r-labs/maps/PlayingwithLeaflet.html#add-shapes-to-a-map",
    "title": "Playing with Leaflet",
    "section": "\n2.1 Add Shapes to a Map",
    "text": "2.1 Add Shapes to a Map\nleaflet offers several commands to add points, markers, icons, lines, polylines and polygons to a map. Let us examine a few of these.\n\n2.1.1 Add Markers with popups\n\nm %&gt;% addMarkers(lng = 77.580643, lat = 12.972442, \n                 popup = \"The birthplace of Rvind\")\n\n\n\n\n# Click on the Marker for the popup to appear\n\nThis uses the default pin shape as the Marker.\n\n2.1.2 Adding Popups to a Map\nPopups are small boxes containing arbitrary HTML, that point to a specific point on the map. Use the addPopups() function to add standalone popup to the map.\n\nm %&gt;%\n  addPopups(\n    lng = 77.580643,\n    lat = 12.972442,\n    popup = paste(\n      \"The birthplace of Rvind\",\n      \"&lt;br&gt;\",\n      \"Website: https://the-foundation-series.netlify.app\",\n      \"&lt;br&gt;\"\n    ),\n    \n    # Ensuring we cannot close the popup, else we will not be able to find where it is, since there is no Marker\n    options = popupOptions(closeButton = FALSE)\n  )\n\n\n\n\n\nPopups are usually added to icons, Markers and other shapes can show up when these are clicked.\n\n2.1.3 Adding Labels to a Map\nLabels are messages attached to all shapes, using the argument label wherever it is available.\nLabels are static, and Popups are usually visible on mouse click. Hence a Marker can have both a label and a popup. For example, the function addPopup() offers only a popup argument, whereas the function addMarkers() offers both a popup and a label argument.\nIt is also possible to create labels standalone using addLabelOnlyMarkers() where we can show only text and no Markers.\n\nm %&gt;%\n  addMarkers(\n    lng = 77.580643,\n    lat = 12.972442,\n    \n    # Here is the Label defn.\n    label = \"The birthplace of Rvind\",\n    labelOptions = labelOptions(noHide = TRUE, # Label always visible\n                                textOnly = F, \n                                textsize = 20),\n    \n    # And here is the popup defn.\n    popup = \"This is the Popup Text\"\n  )\n\n\n\n\n\n\n2.1.4 Adding Circles and CircleMarkers on a Map\nWe can add shapes on to a map to depict areas or locations of interest. NOTE: the radius argument works differently in addCircles() and addCircleMarkers().\n\n# Some Cities in the US and their location\nmd_cities &lt;- tibble(\n  name = c(\"Baltimore\",\"Frederick\",\"Rockville\",\"Gaithersburg\",\"Bowie\",\"Hagerstown\",\"Annapolis\",\"College Park\",\"Salisbury\",\"Laurel\"),\n  pop = c(619493,66169,62334,61045,55232,39890,38880,30587,30484,25346),\n  lat = c(39.2920592,39.4143921,39.0840,39.1434,39.0068,39.6418,38.9784,38.9897,38.3607,39.0993), \n  lng = c(-76.6077852,-77.4204875,-77.1528,-77.2014,-76.7791,-77.7200,-76.4922,-76.9378,-75.5994,-76.8483)\n)\n\n\nmd_cities %&gt;%\n  leaflet() %&gt;%\n  addTiles() %&gt;%\n  \n  # CircleMarkers, in blue\n  # radius scales the Marker. Units are in Pixels!!\n  # Here, radius is made proportional to `pop` number\n  addCircleMarkers(radius = ~ pop/1000, # Pixels!!\n                   color = \"blue\",\n                   stroke = FALSE, # no border for the Markers\n                   opacity = 0.8) %&gt;% \n  \n  \n  # Circles, in red\n  addCircles(\n    radius = 5000, # Meters !!!\n    stroke = TRUE,\n    color = \"yellow\", # Stroke Colour\n    weight = 3, # Stroke Weight\n    fill = TRUE,\n    fillColor = \"red\",\n\n  )\n\nAssuming \"lng\" and \"lat\" are longitude and latitude, respectively\nAssuming \"lng\" and \"lat\" are longitude and latitude, respectively\n\n\n\n\n\n\nThe shapes need not be of fixed size or colour; their attributes can be made to correspond to other attribute variables in the geospatial data, as we did with radius in the addCircleMarkers() function above.\n\n2.1.5 Adding Rectangles to a Map\n\n## Adding Rectangles\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  setView(lng = 77.580643, lat = 12.972442, zoom = 6) %&gt;% \n  addRectangles(lat1 = 10.3858, lng1 = 75.0595, \n                lat2 = 12.8890, lng2 = 77.9625)\n\n\n\n\n\n\n2.1.6 Add Polygons to a Map\n\n## Adding Polygons\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  setView(lng = 77.580643, lat = 12.972442, zoom = 6) %&gt;% \n  \n  # arbitrary vector data for lat and lng\n   addPolygons(lng = c(73.5, 75.9, 76.1, 77.23, 79.8),\n               lat =c(10.12, 11.04, 11.87, 12.04, 10.7))\n\n\n\n\n\n\n2.1.7 Add PolyLines to a Map\nThis can be useful say for manually marking a route on a map, with waypoints.\n\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  setView(lng = 77.580643, lat = 12.972442, zoom = 6) %&gt;% \n  \n  # arbitrary vector data for lat and lng\n  # If start and end points are the same, it looks like Polygon\n  # Without the fill\n   addPolylines(lng = c(73.5, 75.9, 76.1, 77.23, 79.8),\n               lat =c(10.12, 11.04, 11.87, 12.04, 10.7)) %&gt;% \n  \n  # Add Waypoint Icons\n  addMarkers(lng = c(73.5, 75.9, 76.1, 77.23, 79.8),\n               lat =c(10.12, 11.04, 11.87, 12.04, 10.7))\n\n\n\n\n\nAs seen, we have created Markers, Labels, Polygons, and PolyLines using fixed.i.e. literal text and numbers. In the following we will also see how external geospatial data columns can be used instead of these literals.\nNOTE: The mapedit package https://r-spatial.org//r/2017/01/30/mapedit_intro.html can also be used to interactively add shapes onto a map and save as an geo-spatial object."
  },
  {
    "objectID": "content/labs/r-labs/maps/PlayingwithLeaflet.html#point-data-sources-for-leaflet",
    "href": "content/labs/r-labs/maps/PlayingwithLeaflet.html#point-data-sources-for-leaflet",
    "title": "Playing with Leaflet",
    "section": "\n3.1 Point Data Sources for leaflet\n",
    "text": "3.1 Point Data Sources for leaflet\n\nPoint data for markers can come from a variety of sources:\n\n\nSpatialPoints or SpatialPointsDataFrame objects (from the sp package)\n\n\nPOINT, sfc_POINT, and sf objects (from the sf package); only X and Y dimensions will be considered\n\nTwo-column numeric matrices (first column is longitude, second is latitude)\n\n\nData frame/tibble with latitude and longitude columns. You can explicitly tell the marker function which columns contain the coordinate data (e.g. addMarkers(lng = ~Longitude, lat = ~Latitude)), or let the function look for columns named lat/latitude and lon/lng/long/longitude (case insensitive).\n\nSimply provide numeric vectors as lng and lat arguments, which we have covered already in the preceding sections.\n\nNote that MULTIPOINT objects from sf are not supported at this time.\nWe will not consider the use of sp related data structures for plotting POINTs in leaflet since sp is being phased out in favour of the more modern package sf.\n\n3.1.1 Points using simple Data Frames\nLet us read in the data set from data.world that gives us POINT locations of all airports in India in a data frame / tibble. The dataset is available at https://query.data.world/s/ahtyvnm2ybylf65syp4rsb5tulxe6a. You can either download it, save a copy, and read it in as usual, or use the URL itself to read it in from the web. In the latter case, you will need the package data.world and also need to register your credentials for that page with RStudio. The (simple!) instructions are available here at data.world.\n\n#library(devtools)\n#devtools::install_github(\"datadotworld/data.world-r\", build_vignettes = TRUE)\n\nlibrary(data.world)\n\nLoading required package: dwapi\n\n\n\nAttaching package: 'dwapi'\n\n\nThe following object is masked from 'package:dplyr':\n\n    sql\n\nindia_airports &lt;-\n  read_csv(\"https://query.data.world/s/ahtyvnm2ybylf65syp4rsb5tulxe6a\") %&gt;% \n  slice(-1) %&gt;% # Drop the first row which contains labels\n  dplyr::mutate(\n    id = as.integer(id),\n    latitude_deg = as.numeric(latitude_deg),\n    longitude_deg = as.numeric(longitude_deg),\n    elevation_ft = as.integer(elevation_ft)\n  ) %&gt;% \n  rename(\"lon\" = longitude_deg, \"lat\" = latitude_deg) %&gt;% \n  # Remove four locations which seem to be in the African Atlantic\n  filter(!id %in% c(330834, 330867, 325010, 331083))\n\nRows: 345 Columns: 20\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (20): id, ident, type, name, latitude_deg, longitude_deg, elevation_ft, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nindia_airports %&gt;% head()\n\n\n\n  \n\n\n\nLet us plot this in leaflet, using an ESRI National Geographic style map instead of the OSM Base Map. We will also place small circle markers for each airport.\n\nleaflet(data = india_airports) %&gt;% \n  setView(lat = 18, lng = 77, zoom = 4) %&gt;% \n  \n  # Add NatGeo style base map\n  addProviderTiles(providers$Esri.NatGeoWorldMap) %&gt;% # ESRI Basemap\n  \n  # Add Markers for each airport\n  addCircleMarkers(lng = ~lon, lat = ~lat,\n                   # Optional, variables stated for clarity\n                   # leaflet can automatically detect lon-lat columns\n                   # if they are appropriately named in the data\n                   # longitude/lon/lng\n                   # latitude/lat\n                   radius = 2, # Pixels\n                   color = \"red\",\n                   opacity = 1)\n\n\n\n\n\nWe can also change the icon for each airport. Let us try one of theseveral icon families that we can use with leaflet : glyphicons, ionicons, and fontawesome icons.\n\n# Define popup message for each airport\n# Based on data in india_airports\npopup &lt;- paste(\n  \"&lt;strong&gt;\",\n  india_airports$name,\n  \"&lt;/strong&gt;&lt;br&gt;\",\n  india_airports$iata_code,\n  \"&lt;br&gt;\",\n  india_airports$municipality,\n  \"&lt;br&gt;\",\n  \"Elevation(feet)\",\n  india_airports$elevation_ft,\n  \"&lt;br&gt;\",\n  india_airports$wikipedia_link,\n  \"&lt;br&gt;\"\n)\n\niata_icon &lt;- makeIcon(\n  \"iata-logo-transp.png\", # Downloaded from www.iata.org\n  iconWidth = 24,\n  iconHeight = 24,\n  iconAnchorX = 0,\n  iconAnchorY = 0\n)\n\n# Create the Leaflet map\nleaflet(data = india_airports) %&gt;%\n  setView(lat = 18, lng = 77, zoom = 4) %&gt;%\n  addProviderTiles(providers$Esri.NatGeoWorldMap) %&gt;%\n  addMarkers(\n    icon = iata_icon,\n    popup = popup\n  )\n\nAssuming \"lon\" and \"lat\" are longitude and latitude, respectively\n\n\n\n\n\n\nThere are other icons we can use to mark the POINTs. leaflet allows the use of ionicons, glyphicons, and FontAwesomeIcons\nIt is possible to create a list of icons, so that different Markers can have different icons. Let us try to map the MNCs in the ITPL area of Bangalore: we use the ideas in Using Leaflet Markers @JLA-Data.net\n\n# Make a dataframe of addresses of Companies we wan to plot in ITPL\ncompanies_itpl &lt;-\n  data.frame(\n    ticker = c(\n      \"MBRDI\",\n      \"DTICI\",\n      \"IBM\",\n      \"Exxon\",\n      \"Mindtree\",\n      \"FIS Global\",\n      \"Sasken\",\n      \"LTI\"),\n    lat = c(\n      12.986178620989264,\n      12.984160906190121,\n      12.983659088566357,\n      12.985112265986636,\n      12.983794997606187,\n      12.980658616215155,\n      12.982080447350246,\n      12.981338168875348),\n    lon = c(\n      77.7270652183105,\n      77.72808445774321,\n      77.73103488768001,\n      77.72935046040699,\n      77.7227844126931,\n      77.72685064158782,\n      77.72545589289041,\n      77.72287024338216)\n  ) %&gt;% sf::st_as_sf(coords = c(\"lon\", \"lat\"), crs = 4326)\n \n# Vanilla leaflet map\nleaflet(companies_itpl) %&gt;% \n  addTiles() %&gt;% \n  addMarkers()\n\n\n\n\n\nLet us make a list of logos of the Companies and use them as markers!\n\n# a named list of rescaled icons with links to images\nfavicons &lt;- iconList(\n  \"MBRDI\" = makeIcon(\n    iconUrl = \"https://www.mercedes-benz.com/etc/designs/brandhub/frontend/static-assets/header/logo.svg\", \n    iconWidth = 25,\n    iconHeight = 25\n  ),\n  \"DTICI\" = makeIcon(\n    iconUrl = \"https://media-exp1.licdn.com/dms/image/C4D0BAQGzOep26lC03w/company-logo_200_200/0/1638298367374?e=2147483647&v=beta&t=mPyF4gvNhNFvd-tedbqNzJofq4q9qcw6A9z9jQeLAwc\",\n    iconWidth = 45,\n    iconHeight = 45\n  ),\n  \"IBM\" = makeIcon(\n    iconUrl = \"https://www.ibm.com/favicon.ico\",\n    iconWidth = 25,\n    iconHeight = 25\n  ),\n  \"Exxon\" = makeIcon(\n    iconUrl = \"https://corporate.exxonmobil.com/-/media/Global/Icons/logos/ExxonMobilLogoColor2x.png\",\n    iconWidth = 45,\n    iconHeight = 25\n  ),\n  \"Mindtree\" = makeIcon(\n    iconUrl = \"https://www.mindtree.com/themes/custom/mindtree_theme/mindtree-lnt-logo-png.png\",\n    iconWidth = 75,\n    iconHeight = 25\n  ),\n  \"FIS Global\" = makeIcon(\n    iconUrl = \"https://1000logos.net/wp-content/uploads/2021/09/FIS-Logo-768x432.png\",\n    iconWidth = 25,\n    iconHeight = 25\n  ),\n  \"Sasken\" = makeIcon(\n    iconUrl = \"https://www.sasken.com/sites/all/themes/sasken_website/logo.png\",\n    iconWidth = 35,\n    iconHeight = 35,\n  ),\n  \"LTI\" = makeIcon(\n    iconUrl = \"https://www.lntinfotech.com/wp-content/uploads/2021/09/LTI-logo.svg\",\n    iconWidth = 25,\n    iconHeight = 25\n  )\n)\n\n\n# Create the Leaflet map\n\nleaflet(companies_itpl) %&gt;% \n  addMarkers(icon = ~ favicons[ticker], # lookup based on ticker\n             label = ~ companies_itpl$ticker,\n             labelOptions = labelOptions(noHide = F,offset = c(15,-25))) %&gt;%\n  addProviderTiles(\"CartoDB.Positron\")\n\n\n\n\n\n\n3.1.2 Points using sf objects\nWe will use data from an sf data object. This differs from the earlier situation where we had a simple data frame with lon and lat columns. In sf, the lon and lat info is embedded in the geometry column of the sf data frame.\nThe tmap package has a data set of all World metro cities, titled metro. We will plot these on the map and also scale the markers in proportion to one of the feature attributes, pop2030. The popup will be the name of the metro city. We will also use the CartoDB.Positron base map.\nNote that the metro data set has a POINT geometry, as needed!\n\ndata(metro, package = \"tmap\")\nmetro\n\n\n\n  \n\n\nleaflet(data = metro) %&gt;% \n  setView(lat = 18, lng = 77, zoom = 4) %&gt;% \n  \n  # Add CartoDB.Positron\n  addProviderTiles(providers$CartoDB.Positron) %&gt;% # CartoDB Basemap\n  \n  # Add Markers for each airport\n  addCircleMarkers(radius = ~ sqrt(pop2030)/350,\n                   color = \"red\",\n                   popup = paste(\"Name: \", metro$name, \"&lt;br&gt;\",\n                                  \"Population 2030: \", metro$pop2030))\n\n\n\n\n\nWe can also try downloading an sf data frame with POINT geometry from say OSM data&lt;https://osm. Let us get hold of restaurants data in Malleswaram, Bangalore from OSM data:\n\nbbox&lt;- osmdata::getbb(\"Malleswaram, Bengaluru\")\nbbox\n\n       min      max\nx 77.55033 77.59033\ny 12.98274 13.02274\n\nlocations &lt;- osmplotr::extract_osm_objects(\n  bbox = bbox,\n  key = \"amenity\",\n  value = \"restaurant\",\n  return_type = \"point\") \n\nIssuing query to Overpass API ...\n\n\nAnnounced endpoint: z.overpass-api.de/api/\n\n\nQuery complete!\n\n\nconverting OSM data to sf format\n\nlocations &lt;- locations %&gt;% \n  dplyr::filter(cuisine == \"indian\")\nlocations %&gt;% head()\n\n\n\n  \n\n\n# Fontawesome icons seem to work in `leaflet` only up to FontAwesome V4.7.0.\n# The Fontawesome V4.7.0 Cheatsheet is here: &lt;https://fontawesome.com/v4/cheatsheet/&gt;\n\n\nleaflet(data = locations, options = leafletOptions(minZoom = 12)) %&gt;% \n  \n  addProviderTiles(providers$CartoDB.Voyager) %&gt;% \n  \n  # Regular `leaflet` code\n  addAwesomeMarkers(icon = awesomeIcons(icon = \"fa-coffee\", \n                                        library = \"fa\",\n                                        markerColor = \"blue\",\n                                        iconColor = \"black\",\n                                        iconRotate = TRUE),\n                     popup = paste(\"Name: \", locations$name,\"&lt;br&gt;\",\n                           \"Food: \", locations$cuisine)) \n\n\n\n\n\nFontawesome Workaround\nFor more later versions of Fontawesome, here below is a workaround from https://github.com/rstudio/leaflet/issues/691. Despite this some fontawesome icons simply do not seem to show up. ;-()\n\nlibrary(fontawesome)\ncoffee &lt;- makeAwesomeIcon(\n  text = fa(\"mug-hot\"), # mug-hot was introduced in fa version 5\n  iconColor = \"black\",\n  markerColor = \"blue\",\n  library = \"fa\"\n)\n\n\nleaflet(data = locations) %&gt;% \n  addProviderTiles(providers$CartoDB.Voyager) %&gt;% \n  \n  # Workaround code\n\n  addAwesomeMarkers(icon = coffee,\n             popup = paste(\"Name: \", locations$name,\"&lt;br&gt;\",\n                           \"Food: \", locations$cuisine, \"&lt;br&gt;\"))\n\n\n\n\n\nNote that leaflet automatically detects the lon/lat columns from within the POINT geometry column of the sf data frame.\n\n3.1.3 Points using Two-Column Matrices\nWe can now quickly try providing lon and lat info in a two column matrix.This can be useful to plot a bunch of points recorded on a mobile phone app.\n\nmysore5 &lt;- matrix(c(runif(5, 76.652985-0.01, 76.652985+0.01),\n                 runif(5, 12.311827-0.01, 12.311827+0.01)),\n                 nrow = 5)\nmysore5\n\n         [,1]     [,2]\n[1,] 76.65462 12.31162\n[2,] 76.64928 12.30791\n[3,] 76.64946 12.30259\n[4,] 76.64796 12.30682\n[5,] 76.65478 12.31749\n\nleaflet(data = mysore5) %&gt;% \n  addProviderTiles(providers$OpenStreetMap) %&gt;% \n  \n# Pick an icon from &lt;https://www.w3schools.com/bootstrap/bootstrap_ref_comp_glyphs.asp&gt;\n  addAwesomeMarkers(icon = awesomeIcons(\n  icon = 'music',\n  iconColor = 'black',\n  library = 'glyphicon'),\n  popup = \"Carnatic Music !!\")"
  },
  {
    "objectID": "content/labs/r-labs/maps/PlayingwithLeaflet.html#polygons-lines-and-polylines-data-sources-for-leaflet",
    "href": "content/labs/r-labs/maps/PlayingwithLeaflet.html#polygons-lines-and-polylines-data-sources-for-leaflet",
    "title": "Playing with Leaflet",
    "section": "\n3.2 Polygons, Lines, and Polylines Data Sources for leaflet\n",
    "text": "3.2 Polygons, Lines, and Polylines Data Sources for leaflet\n\nWe have seen how to get POINT data into leaflet.\nLine and polygon data can come from a variety of sources:\n\n\nSpatialPolygons, SpatialPolygonsDataFrame, Polygons, and Polygon objects (from the sp package)\n\n\nSpatialLines, SpatialLinesDataFrame, Lines, and Line objects (from the sp package)\n\n\nMULTIPOLYGON, POLYGON, MULTILINESTRING, and LINESTRING objects (from the sf package)\n\n\nmap objects (from the maps package’s map() function); use map(fill = TRUE) for polygons, FALSE for polylines\n\nTwo-column numeric matrix; the first column is longitude and the second is latitude. Polygons are separated by rows of (NA, NA). It is not possible to represent multi-polygons nor polygons with holes using this method; use SpatialPolygons instead.\n\nWe will concentrate on using sf data into leaflet. We may explore maps() objects at a later date.\n\n3.2.1 Polygons/MultiPolygons and LineString/MultiLineString using sf data frames\nLet us download College buildings, parks, and the cycling lanes in Amsterdam, Netherlands, and plot these in leaflet.\n\nbbox &lt;- osmdata::getbb(\"Amsterdam, Netherlands\")\nbbox\n\n        min       max\nx  4.728756  5.079162\ny 52.278174 52.431064\n\ncolleges &lt;- osmplotr::extract_osm_objects(bbox = bbox,\n                                           key = \"amenity\",\n                                           value = \"college\",\n                                           return_type = \"polygon\" )\n\nIssuing query to Overpass API ...\n\n\nAnnounced endpoint: z.overpass-api.de/api/\n\n\nQuery complete!\n\n\nconverting OSM data to sf format\n\nparks &lt;- osmplotr::extract_osm_objects(bbox = bbox,\n                                           key = \"park\",\n                                           return_type = \"polygon\" )\n\nIssuing query to Overpass API ...\n\n\nAnnounced endpoint: z.overpass-api.de/api/\n\n\nQuery complete!\n\n\nconverting OSM data to sf format\n\nroads &lt;- osmplotr::extract_osm_objects(bbox = bbox,\n                                       key = \"highway\",\n                                       value = \"primary\",\n                                       return_type = \"line\")\n\nIssuing query to Overpass API ...\n\n\nAnnounced endpoint: z.overpass-api.de/api/\n\n\nQuery complete!\n\n\nconverting OSM data to sf format\n\ncyclelanes &lt;-\n  osmplotr::extract_osm_objects(bbox,\n                                key = \"cycleway\",\n                                value =  \"lane\",\n                                return_type = \"line\")\n\nIssuing query to Overpass API ...\n\n\nAnnounced endpoint: z.overpass-api.de/api/\n\n\nQuery complete!\n\n\nconverting OSM data to sf format\n\n\nWe have 17 colleges in our data and 370 parks in our data.\n\nleaflet() %&gt;% \n  addTiles() %&gt;% \n  addPolygons(data = colleges, popup = ~colleges$name) %&gt;% \n  addPolygons(data = parks, color = \"green\", popup = parks$name) %&gt;% \n  addPolylines(data = roads, color = \"red\") %&gt;% \n  addPolylines(data = cyclelanes, color = \"purple\")"
  },
  {
    "objectID": "content/labs/r-labs/maps/PlayingwithLeaflet.html#chapter-3-using-raster-data-in-leaflet",
    "href": "content/labs/r-labs/maps/PlayingwithLeaflet.html#chapter-3-using-raster-data-in-leaflet",
    "title": "Playing with Leaflet",
    "section": "\n3.3 Chapter 3: Using Raster Data in leaflet\n",
    "text": "3.3 Chapter 3: Using Raster Data in leaflet\n\nSo far all the geospatial data we have plotted in leaflet has been vector data. We will now explore how to plot raster data using leaflet. Raster data are used to depict continuous variables across space, such as vegitation, salinity, forest cover etc. Satellite imagery is frequently available as raster data.\n\n3.3.1 Importing Raster Data [Work in Progress!]\nRaster data can be imported into R in many ways:\n\nusing the maptiles package\n\nusing the OpenStreetMap package\n\n\nlibrary(terra)\n\nterra 1.7.18\n\n\n\nAttaching package: 'terra'\n\n\nThe following object is masked from 'package:data.world':\n\n    query\n\n\nThe following objects are masked from 'package:igraph':\n\n    blocks, compare\n\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\nlibrary(maptiles)\n#library(OpenStreetMap) # causes RStudio to crash..."
  },
  {
    "objectID": "content/labs/r-labs/maps/PlayingwithLeaflet.html#adding-legendswork-in-progress",
    "href": "content/labs/r-labs/maps/PlayingwithLeaflet.html#adding-legendswork-in-progress",
    "title": "Playing with Leaflet",
    "section": "\n4.1 Adding Legends[Work in Progress!]",
    "text": "4.1 Adding Legends[Work in Progress!]\n\n## Generate some random lat lon data around Bangalore\ndf &lt;- data.frame(lat = runif(20, min = 11.97, max = 13.07),\n                 lng = runif(20, min = 77.48, max = 77.68),\n                 col = sample(c(\"red\", \"blue\", \"green\"), 20, \n                              replace = TRUE),\n                 stringsAsFactors = FALSE)\n\ndf %&gt;%\n  leaflet() %&gt;%\n  addTiles() %&gt;%\n  addCircleMarkers(color = df$col) %&gt;%\n  addLegend(values = df$col, labels = LETTERS[1:3], colors = c(\"blue\", \"red\", \"green\"))\n\nAssuming \"lng\" and \"lat\" are longitude and latitude, respectively"
  },
  {
    "objectID": "content/labs/r-labs/maps/index.html",
    "href": "content/labs/r-labs/maps/index.html",
    "title": "Lab 01 - Introduce Yourself",
    "section": "",
    "text": "At the end of this Lab, we will:\n\nhave installed R and RStudio on our machines\nunderstood how to add additional R-packages for specific features and graphic capability\nrun code within RStudio and interpret the results\nhave learnt to look for help within R and RStudio"
  },
  {
    "objectID": "content/labs/r-labs/maps/index.html#Check-In-R",
    "href": "content/labs/r-labs/maps/index.html#Check-In-R",
    "title": "Lab 01 - Introduce Yourself",
    "section": "\n3.1 Check in",
    "text": "3.1 Check in\nLaunch R by clicking this logo. You should see one console with a command line interpreter. Try typing 2 + 2 and check !\nClose R."
  },
  {
    "objectID": "content/labs/r-labs/maps/index.html#Check-In-RStudio",
    "href": "content/labs/r-labs/maps/index.html#Check-In-RStudio",
    "title": "Lab 01 - Introduce Yourself",
    "section": "\n4.1 Check in",
    "text": "4.1 Check in\nLaunch RStudio. You should get a window similar to the screenshot you see here, but yours will be empty. Look at the bottom left pane: this is the same console window you saw when you opened R in step @Check-In-R\n\nPlace your cursor where you see &gt; and type x &lt;- 2 + 2 again hit enter or return, then type x, and hit enter/return again.\nIf [1] 4 prints to the screen, you have successfully installed R and RStudio, and you can move onto installing packages."
  },
  {
    "objectID": "content/labs/r-labs/maps/index.html#save-and-share",
    "href": "content/labs/r-labs/maps/index.html#save-and-share",
    "title": "Lab 01 - Introduce Yourself",
    "section": "\n7.1 Save and share",
    "text": "7.1 Save and share\nSave your work so you can share your favorite plot with us. You will not like the looks of your plot if you mouse over to Export and save it. Instead, use ggplot2’s command for saving a plot with sensible defaults:\n\nCodehelp(ggsave)\n\n\n\nCodeggsave(\"file_name_here.pdf\", plot) # please make the filename unique!\n\n\nUpload this exported plot to Teams -&gt; Assignments.\nBefore you do that, check that you can submit stuff/assignments on the MLS by uploading a cat picture 🐈 first, especially if you are an ailurophile like me. (Acceptable Breeds: Scottish Folds or Maine Coons)"
  },
  {
    "objectID": "content/labs/r-labs/installation/installation.html",
    "href": "content/labs/r-labs/installation/installation.html",
    "title": "Lab 01 - Introduce Yourself",
    "section": "",
    "text": "At the end of this Lab, we will:\n\nhave installed R and RStudio on our machines\nunderstood how to add additional R-packages for specific features and graphic capability\nrun code within RStudio and interpret the results\nhave learnt to look for help within R and RStudio"
  },
  {
    "objectID": "content/labs/r-labs/installation/installation.html#Check-In-R",
    "href": "content/labs/r-labs/installation/installation.html#Check-In-R",
    "title": "Lab 01 - Introduce Yourself",
    "section": "\n3.1 Check in",
    "text": "3.1 Check in\nLaunch R by clicking this logo. You should see one console with a command line interpreter. Try typing 2 + 2 and check !\nClose R."
  },
  {
    "objectID": "content/labs/r-labs/installation/installation.html#Check-In-RStudio",
    "href": "content/labs/r-labs/installation/installation.html#Check-In-RStudio",
    "title": "Lab 01 - Introduce Yourself",
    "section": "\n4.1 Check in",
    "text": "4.1 Check in\nLaunch RStudio. You should get a window similar to the screenshot you see here, but yours will be empty. Look at the bottom left pane: this is the same console window you saw when you opened R in step @Check-In-R\n\nPlace your cursor where you see &gt; and type x &lt;- 2 + 2 again hit enter or return, then type x, and hit enter/return again.\nIf [1] 4 prints to the screen, you have successfully installed R and RStudio, and you can move onto installing packages."
  },
  {
    "objectID": "content/labs/r-labs/installation/installation.html#save-and-share",
    "href": "content/labs/r-labs/installation/installation.html#save-and-share",
    "title": "Lab 01 - Introduce Yourself",
    "section": "\n7.1 Save and share",
    "text": "7.1 Save and share\nSave your work so you can share your favorite plot with us. You will not like the looks of your plot if you mouse over to Export and save it. Instead, use ggplot2’s command for saving a plot with sensible defaults:\n\nhelp(ggsave)\n\n\nggsave(\"file_name_here.pdf\", plot) # please make the filename unique!"
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html",
    "href": "content/labs/r-labs/graphics/index.html",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "",
    "text": "This RMarkdown document is part of my course on R for Artists and Designers. The material is based on A Layered Grammar of Graphics by Hadley Wickham. The intent of this Course is to build Skill in coding in R, and also appreciate R as a way to metaphorically visualize information of various kinds, using predominantly geometric figures and structures.\nAll RMarkdown files combine code, text, web-images, and figures developed using code. Everything is text; code chunks are enclosed in fences (```)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#introduction",
    "href": "content/labs/r-labs/graphics/index.html#introduction",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "",
    "text": "This RMarkdown document is part of my course on R for Artists and Designers. The material is based on A Layered Grammar of Graphics by Hadley Wickham. The intent of this Course is to build Skill in coding in R, and also appreciate R as a way to metaphorically visualize information of various kinds, using predominantly geometric figures and structures.\nAll RMarkdown files combine code, text, web-images, and figures developed using code. Everything is text; code chunks are enclosed in fences (```)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#goals",
    "href": "content/labs/r-labs/graphics/index.html#goals",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "\n2 Goals",
    "text": "2 Goals\nAt the end of this Lab session, we should: - know the types and structures of tidy data and be able to work with them - be able to create data visualizations using ggplot - Understand aesthetics and scales in `ggplot"
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#pedagogical-note",
    "href": "content/labs/r-labs/graphics/index.html#pedagogical-note",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "\n3 Pedagogical Note",
    "text": "3 Pedagogical Note\nThe method followed will be based on PRIMM:\n\n\nPREDICT Inspect the code and guess at what the code might do, write predictions\n\n\nRUN the code provided and check what happens\n\nINFER what the parameters of the code do and write comments to explain. What bells and whistles can you see?\n\nMODIFY the parameters code provided to understand the options available. Write comments to show what you have aimed for and achieved.\n\nMAKE : take an idea/concept of your own, and graph it."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#set-up",
    "href": "content/labs/r-labs/graphics/index.html#set-up",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "\n4 Set Up",
    "text": "4 Set Up\nThe setup code chunk below brings into our coding session R packages that provide specific computational abilities and also datasets which we can use.\nTo reiterate: Packages and datasets are not the same thing !! Packages are (small) collections of programs. Datasets are just….information."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#a-teaser-from-john-snow",
    "href": "content/labs/r-labs/graphics/index.html#a-teaser-from-john-snow",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "\n5 A Teaser from John Snow",
    "text": "5 A Teaser from John Snow"
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#review-of-tidy-data",
    "href": "content/labs/r-labs/graphics/index.html#review-of-tidy-data",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "\n6 Review of Tidy Data",
    "text": "6 Review of Tidy Data\n“Tidy Data” is an important way of thinking about what data typically look like in R. Let’s fetch a figure from the web to show the (preferred) structure of data in R. (The syntax to bring in a web-figure is ![caption](url))\n The three features described in the figure above define the nature of tidy data:\n\n\nVariables in Columns\n\n\nObservations in Rows and\n\n\nMeasurements in Cells.\n\nData are imagined to be resulting from an experiment. Each variable represents a parameter/aspect in the experiment. Each row represents an additional datum of measurement. A cell is a single measurement on a single parameter(column) in a single observation(row)."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#kinds-of-variables",
    "href": "content/labs/r-labs/graphics/index.html#kinds-of-variables",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "\n7 Kinds of Variables",
    "text": "7 Kinds of Variables\nKinds of Variable are defined by the kind of questions they answer to:\n\nWhat/Who/Where? -&gt; Some kind of Name. Categorical variable\nWhat Kind? How? -&gt; Some kind of “Type”. Factor variable\nHow Many? How large? -&gt; Some kind of Quantity. Numerical variable. Most Figures in R are computed with variables, and therefore, with columns."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#interrogations-and-graphs",
    "href": "content/labs/r-labs/graphics/index.html#interrogations-and-graphs",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "\n8 Interrogations and Graphs",
    "text": "8 Interrogations and Graphs\nCreating graphs from data is an act of asking questions and viewing answers in a geometric way. Let us write some simple English descriptions of measures and visuals and see what commands they use in R.\n\n8.1 Components of the layered grammar of graphics\nLayers are used to create the objects on a plot. They are defined by five basic parts:\n\nData (What dataset/spreadsheet am I using?)\nMapping (What does each column do in my graph?)\nStatistical transformation (stat) (Do I have count something first?)\nGeometric object (geom) (What shape, colour, size…do I want?)\nPosition adjustment (position) (Where do I want it on the graph?)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#data",
    "href": "content/labs/r-labs/graphics/index.html#data",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "\n9 Data",
    "text": "9 Data\nWe will use “real world” data. Let’s use the penguins dataset in the palmerpenguins package. Run ?penguins in the console to get more information about this dataset.\n\n9.1 Head\n\n\n\n\n  \n\n\n\n\n9.2 Tail\n\n\n\n\n  \n\n\n\n\n9.3 Dim\n\n\n[1] 344   8\n\n\nSo we know what our data looks like. We pass this data to ggplot use to plot as follows: in R this creates an empty graph sheet!! Because we have not (yet) declared the geometric shapes we want to use to plot our information."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#mapping",
    "href": "content/labs/r-labs/graphics/index.html#mapping",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "\n10 Mapping",
    "text": "10 Mapping\nNow that we have told R what data to use, we need to state what variables to plot and how.\nAesthetic Mapping defines how the variables are applied to the plot, i.e. we take a variable from the data and “metaphorize” it into a geometric feature. We can map variables metaphorically to a variety of geometric things: coordinate, length, height, size, shape, colour, alpha(how dark?)….\nThe syntax uses: aes(some_geometric_thing = some_variable)\nRemember variable = column.\nSo if we were graphing information from penguins, we might map a penguin’s flipper_length_mm column to the \\(x\\) position, and the body_mass_g column to the \\(y\\) position.\n\n10.1 Mapping Example-1\nWe can try another example of aesthetic mapping with the same dataset:\n\n10.1.1 Plot-1a\n\n\n\n\n\n\n10.1.2 Plot-1b\n\n10.1.3 Plot-1c\n\n10.2 Mapping Example-2\nWe can try another example of aesthetic mapping with the same dataset:\n\n10.2.1 Plot-2a\n\n\n\n\n\n\n10.2.2 Plot-2b\n\n10.2.3 Plot-2c\n\n10.3 Mapping Example-3\nWe can try another example of aesthetic mapping with the same dataset:\n\n10.3.1 Plot-3a\n\n\n\n\n\n\n10.3.2 Plot-3b\n\n10.3.3 Plot-3c\n\n10.4 Mapping Example-4\nWe can try another example of aesthetic mapping with the same dataset:\n\n10.4.1 Plot-4a\n\n\n\n\n\n\n10.4.2 Plot-4b\n\n10.4.3 Plot-4c"
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#geometric-objects",
    "href": "content/labs/r-labs/graphics/index.html#geometric-objects",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "\n11 Geometric objects",
    "text": "11 Geometric objects\nGeometric objects (geoms) control the type of plot you create. Geoms are classified by their dimensionality:\n\n0 dimensions - point, text\n1 dimension - path, line\n2 dimensions - polygon, interval\n\nEach geom can only display certain aesthetics or visual attributes of the geom. For example, a point geom has position, color, shape, and size aesthetics.\nWe can also stack up geoms on top of one another to add layers to the graph.\n\n11.1 Plot1\n\n\nWarning: Removed 2 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\n11.2 Plot2\n\n\nWarning: Removed 2 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\n11.3 Plot3\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPosition determines the starting location (origin) of each bar\nHeight determines how tall to draw the bar. Here the height is based on the number of observations in the dataset for each possible number of cylinders."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#position-adjustment",
    "href": "content/labs/r-labs/graphics/index.html#position-adjustment",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "\n12 Position adjustment",
    "text": "12 Position adjustment\nSometimes with dense data we need to adjust the position of elements on the plot, otherwise data points might obscure one another. Bar plots frequently stack or dodge the bars to avoid overlap:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSometimes scatterplots with few unique \\(x\\) and \\(y\\) values are jittered (random noise is added) to reduce overplotting.\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#statistical-transformation",
    "href": "content/labs/r-labs/graphics/index.html#statistical-transformation",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "\n13 Statistical transformation",
    "text": "13 Statistical transformation\nA statistical transformation (stat) pre-transforms the data, before plotting. For instance, in a bar graph you might summarize the data by counting the total number of observations within a set of categories, and then plotting the count.\n\n13.1 Count\n\n\n\n\n  \n\n\n\n\n13.2 Count and Bar Graph\n\n\n\n\n\n\n13.3 Tidy Count and Bar Graph\n\n\n\n\n\n\n13.4 Count inside the Plot\n\n\n\n\n\nSometimes you don’t need to make a statistical transformation. For example, in a scatterplot you use the raw values for the \\(x\\) and \\(y\\) variables to map onto the graph. In these situations, the statistical transformation is an identity transformation - the stat simply passes in the original dataset and exports the exact same dataset."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#scale",
    "href": "content/labs/r-labs/graphics/index.html#scale",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "\n14 Scale",
    "text": "14 Scale\nA scale controls how data is mapped to aesthetic attributes, so we need one scale for every aesthetic property employed in a layer. For example, this graph defines a scale for color:\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nThe scale can be changed to use a different color palette:\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nNow we are using a different palette, but the scale is still consistent: all Adelie penguins utilize the same color, whereas Chinstrap use a new color but each Adelie still uses the same, consistent color."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#coordinate-system",
    "href": "content/labs/r-labs/graphics/index.html#coordinate-system",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "\n15 Coordinate system",
    "text": "15 Coordinate system\nA coordinate system (coord) maps the position of objects onto the plane of the plot, and controls how the axes and grid lines are drawn. Plots typically use two coordinates (\\(x, y\\)), but could use any number of coordinates. Most plots are drawn using the Cartesian coordinate system:\n\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nThis system requires a fixed and equal spacing between values on the axes. That is, the graph draws the same distance between 1 and 2 as it does between 5 and 6. The graph could be drawn using a semi-log coordinate system which logarithmically compresses the distance on an axis:\n\n\n\n\n\nOr could even be drawn using polar coordinates:"
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#faceting",
    "href": "content/labs/r-labs/graphics/index.html#faceting",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "\n16 Faceting",
    "text": "16 Faceting\nFaceting can be used to split the data up into subsets of the entire dataset. This is a powerful tool when investigating whether patterns are the same or different across conditions, and allows the subsets to be visualized on the same plot (known as conditioned or trellis plots). The faceting specification describes which variables should be used to split up the data, and how they should be arranged.\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#defaults",
    "href": "content/labs/r-labs/graphics/index.html#defaults",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "\n17 Defaults",
    "text": "17 Defaults\nRather than explicitly declaring each component of a layered graphic (which will use more code and introduces opportunities for errors), we can establish intelligent defaults for specific geoms and scales. For instance, whenever we want to use a bar geom, we can default to using a stat that counts the number of observations in each group of our variable in the \\(x\\) position.\nConsider the following scenario: you wish to generate a scatterplot visualizing the relationship between penguins’ bill_length and their body_mass. With no defaults, the code to generate this graph is:\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nThe above code:\n\nCreates a new plot object (ggplot)\n\nAdds a layer (layer)\n\nSpecifies the data (penguins)\nMaps engine bill length to the \\(x\\) position and body mass to the \\(y\\) position (mapping)\nUses the point geometric transformation (geom = \"point\")\nImplements an identity transformation and position (stat = \"identity\" and position = \"identity\")\n\n\nEstablishes two continuous position scales (scale_x_continuous and scale_y_continuous)\nDeclares a cartesian coordinate system (coord_cartesian)\n\nHow can we simplify this using intelligent defaults?\n\nWe only need to specify one geom and stat, since each geom has a default stat.\nCartesian coordinate systems are most commonly used, so it should be the default.\n\nDefault scales can be added based on the aesthetic and type of variables.\n\nContinuous values are transformed with a linear scaling.\nDiscrete values are mapped to integers.\nScales for aesthetics such as color, fill, and size can also be intelligently defaulted.\n\n\n\nUsing these defaults, we can rewrite the above code as:\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nThis generates the exact same plot, but uses fewer lines of code. Because multiple layers can use the same components (data, mapping, etc.), we can also specify that information in the ggplot() function rather than in the layer() function:\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nAnd as we will learn, function arguments in R use specific ordering, so we can omit the explicit call to data and mapping:\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "content/labs/r-labs/diagrams/index.html",
    "href": "content/labs/r-labs/diagrams/index.html",
    "title": "Lab 01 - Introduce Yourself",
    "section": "",
    "text": "At the end of this Lab, we will:\n\nhave installed R and RStudio on our machines\nunderstood how to add additional R-packages for specific features and graphic capability\nrun code within RStudio and interpret the results\nhave learnt to look for help within R and RStudio"
  },
  {
    "objectID": "content/labs/r-labs/diagrams/index.html#Check-In-R",
    "href": "content/labs/r-labs/diagrams/index.html#Check-In-R",
    "title": "Lab 01 - Introduce Yourself",
    "section": "\n3.1 Check in",
    "text": "3.1 Check in\nLaunch R by clicking this logo. You should see one console with a command line interpreter. Try typing 2 + 2 and check !\nClose R."
  },
  {
    "objectID": "content/labs/r-labs/diagrams/index.html#Check-In-RStudio",
    "href": "content/labs/r-labs/diagrams/index.html#Check-In-RStudio",
    "title": "Lab 01 - Introduce Yourself",
    "section": "\n4.1 Check in",
    "text": "4.1 Check in\nLaunch RStudio. You should get a window similar to the screenshot you see here, but yours will be empty. Look at the bottom left pane: this is the same console window you saw when you opened R in step @Check-In-R\n\nPlace your cursor where you see &gt; and type x &lt;- 2 + 2 again hit enter or return, then type x, and hit enter/return again.\nIf [1] 4 prints to the screen, you have successfully installed R and RStudio, and you can move onto installing packages."
  },
  {
    "objectID": "content/labs/r-labs/diagrams/index.html#save-and-share",
    "href": "content/labs/r-labs/diagrams/index.html#save-and-share",
    "title": "Lab 01 - Introduce Yourself",
    "section": "\n7.1 Save and share",
    "text": "7.1 Save and share\nSave your work so you can share your favorite plot with us. You will not like the looks of your plot if you mouse over to Export and save it. Instead, use ggplot2’s command for saving a plot with sensible defaults:\n\nCodehelp(ggsave)\n\n\n\nCodeggsave(\"file_name_here.pdf\", plot) # please make the filename unique!\n\n\nUpload this exported plot to Teams -&gt; Assignments.\nBefore you do that, check that you can submit stuff/assignments on the MLS by uploading a cat picture 🐈 first, especially if you are an ailurophile like me. (Acceptable Breeds: Scottish Folds or Maine Coons)"
  },
  {
    "objectID": "content/labs/r-labs/dashboard/example.html",
    "href": "content/labs/r-labs/dashboard/example.html",
    "title": "Critical Result Callback Monitor",
    "section": "",
    "text": "mean_tat &lt;- round(mean(cb_data$call_tat), 0)\nvalueBox(value = mean_tat, icon = \"fa-stopwatch\", \n         caption = \"Mean callback time\", color = \"#708090\")\n\n15\n\n\n\n\ncalls &lt;- cb_data %&gt;% nrow()\nvalueBox(value = calls, icon = \"fa-hashtag\", \n         caption = \"Total calls\", color = \"orange\")\n\n2167\n\n\n\n\nontime_n &lt;- cb_data %&gt;% filter(call_tat &lt;= 30) %&gt;% nrow()\nlate_n &lt;- cb_data %&gt;% filter(call_tat &gt;30) %&gt;% nrow()\npct_ontime &lt;- round((ontime_n/calls)*100,0)\n\n\nif(pct_ontime &gt;= 95){\n  valueBox(value = pct_ontime, icon = \"fa-percent\", \n           caption = \"Percent on time\", color = \"mediumseagreen\") #3CB371\n} else{\nvalueBox(value = pct_ontime, icon = \"fa-percent\", \n         caption = \"Percent on time\", color = \"indianred\")} #CD5C5C\n\n89\n\n\n\n\n\np1 &lt;- ggplot(filter(cb_data, \n              call_wday %in% c(\"Mon\", \"Tue\", \"Wed\", \n                               \"Thu\", \"Fri\"))) +\n  geom_bar(aes(x = call_hour, fill = tech_location)) + \n  facet_grid(call_wday~call_week) +\n  labs(x = \"Hour\", y = \"Count\", fill = \"Type\") +\n  scale_fill_manual(values=c(\"steelblue3\", \"gray60\")) +\n  theme_bw() +\n  theme(legend.position = \"top\")\n\nggplotly(p1) %&gt;% layout(legend = list(orientation = \"h\", \n                                      x = 0.35, y = 1.2)) \n\n\n\n\n# have to manually position the legend\n\n\n\ndaily_vol &lt;- cb_data %&gt;% \n              group_by(call_year, call_month, call_date) %&gt;%\n              summarize(n = n()) %&gt;%\n              ungroup() %&gt;%\n              unite(dttm, call_year, call_month, call_date, \n                    sep = \"-\", remove = TRUE) %&gt;%\n              mutate(dttm = as.Date(dttm))\n\n`summarise()` has grouped output by 'call_year', 'call_month'. You can override\nusing the `.groups` argument.\n\n#dygraph requires ts object\ndaily_vol_ts &lt;- xts(daily_vol$n, order.by = daily_vol$dttm) \n\np2 &lt;- dygraph(daily_vol_ts) %&gt;% \n  dyOptions(colors = \"#4F94CD\", strokeWidth = 2) \np2"
  },
  {
    "objectID": "content/labs/r-labs/dashboard/example.html#row",
    "href": "content/labs/r-labs/dashboard/example.html#row",
    "title": "Critical Result Callback Monitor",
    "section": "",
    "text": "mean_tat &lt;- round(mean(cb_data$call_tat), 0)\nvalueBox(value = mean_tat, icon = \"fa-stopwatch\", \n         caption = \"Mean callback time\", color = \"#708090\")\n\n15\n\n\n\n\ncalls &lt;- cb_data %&gt;% nrow()\nvalueBox(value = calls, icon = \"fa-hashtag\", \n         caption = \"Total calls\", color = \"orange\")\n\n2167\n\n\n\n\nontime_n &lt;- cb_data %&gt;% filter(call_tat &lt;= 30) %&gt;% nrow()\nlate_n &lt;- cb_data %&gt;% filter(call_tat &gt;30) %&gt;% nrow()\npct_ontime &lt;- round((ontime_n/calls)*100,0)\n\n\nif(pct_ontime &gt;= 95){\n  valueBox(value = pct_ontime, icon = \"fa-percent\", \n           caption = \"Percent on time\", color = \"mediumseagreen\") #3CB371\n} else{\nvalueBox(value = pct_ontime, icon = \"fa-percent\", \n         caption = \"Percent on time\", color = \"indianred\")} #CD5C5C\n\n89"
  },
  {
    "objectID": "content/labs/r-labs/dashboard/example.html#row-1",
    "href": "content/labs/r-labs/dashboard/example.html#row-1",
    "title": "Critical Result Callback Monitor",
    "section": "",
    "text": "p1 &lt;- ggplot(filter(cb_data, \n              call_wday %in% c(\"Mon\", \"Tue\", \"Wed\", \n                               \"Thu\", \"Fri\"))) +\n  geom_bar(aes(x = call_hour, fill = tech_location)) + \n  facet_grid(call_wday~call_week) +\n  labs(x = \"Hour\", y = \"Count\", fill = \"Type\") +\n  scale_fill_manual(values=c(\"steelblue3\", \"gray60\")) +\n  theme_bw() +\n  theme(legend.position = \"top\")\n\nggplotly(p1) %&gt;% layout(legend = list(orientation = \"h\", \n                                      x = 0.35, y = 1.2)) \n\n\n\n\n# have to manually position the legend\n\n\n\ndaily_vol &lt;- cb_data %&gt;% \n              group_by(call_year, call_month, call_date) %&gt;%\n              summarize(n = n()) %&gt;%\n              ungroup() %&gt;%\n              unite(dttm, call_year, call_month, call_date, \n                    sep = \"-\", remove = TRUE) %&gt;%\n              mutate(dttm = as.Date(dttm))\n\n`summarise()` has grouped output by 'call_year', 'call_month'. You can override\nusing the `.groups` argument.\n\n#dygraph requires ts object\ndaily_vol_ts &lt;- xts(daily_vol$n, order.by = daily_vol$dttm) \n\np2 &lt;- dygraph(daily_vol_ts) %&gt;% \n  dyOptions(colors = \"#4F94CD\", strokeWidth = 2) \np2"
  },
  {
    "objectID": "content/labs/r-labs/dashboard/example.html#row-2",
    "href": "content/labs/r-labs/dashboard/example.html#row-2",
    "title": "Critical Result Callback Monitor",
    "section": "\n2.1 Row",
    "text": "2.1 Row\n\n2.1.1 Mean callback time\n\ncc_calls &lt;- cb_data %&gt;% filter(tech_location == \"CallCenter\")\nmean_tat_cc &lt;- round(mean(cc_calls$call_tat),0)\n \nvalueBox(value = mean_tat_cc, icon = \"fa-stopwatch\", \n         caption = \"Mean callback time\", color = \"#708090\")\n\n10\n\n\n\n2.1.2 Total calls\n\ncc_calls_n &lt;- cb_data %&gt;% filter(tech_location == \"CallCenter\") %&gt;% nrow()\nvalueBox(value = cc_calls_n, icon = \"fa-hashtag\", \n         caption = \"Total calls\", color = \"orange\")\n\n864\n\n\n\n2.1.3 Percent on Time\n\ncc_ontime_n &lt;- cb_data %&gt;% \n  filter(tech_location == \"CallCenter\", call_tat &lt;= 30) %&gt;% nrow()\ncc_late_n &lt;- cb_data %&gt;% \n  filter(tech_location == \"CallCenter\", call_tat &gt;30) %&gt;% nrow()\ncc_pct_ontime &lt;- round((cc_ontime_n/cc_calls_n)*100,0)\n\n\nif(cc_pct_ontime &gt;= 95){\n  valueBox(value = cc_pct_ontime, icon = \"fa-percent\", \n           caption = \"Percent on time\", color = \"mediumseagreen\") #3CB371\n} else{\nvalueBox(value = cc_pct_ontime, icon = \"fa-percent\", \n         caption = \"Percent on time\", color = \"indianred\")} #CD5C5C\n\n99"
  },
  {
    "objectID": "content/labs/r-labs/dashboard/example.html#row-3",
    "href": "content/labs/r-labs/dashboard/example.html#row-3",
    "title": "Critical Result Callback Monitor",
    "section": "\n2.2 Row",
    "text": "2.2 Row\n\n2.2.1 Overdue Call Details for 2020-07-01 through 2020-07-31\n\ncc_late &lt;- cb_data %&gt;% \n  filter(tech_location == \"CallCenter\", call_tat &gt;30) %&gt;%\n  select(call_tat, accession, pt_type, pt_loc_code, test_code,\n         result_datetime, phoned_title, tech)\n\ndatatable(cc_late, options = list(pageLength = 20, autoWidth = TRUE))\n\n\n\n\n\n\n\n2.2.2 Tech Summary for 2020-07-01 through 2020-07-31\n\np3 &lt;- hcboxplot(\n  outliers = FALSE,\n  x = cc_calls$call_tat,\n  var = cc_calls$tech,\n  name = \"TAT\", \n  color = \"#4F94CD\",\n  lineWidth = 2) %&gt;%\n  hc_title(text = \"\") %&gt;%\n  hc_xAxis(title = list(text = \"Tech Code\")) %&gt;%\n  hc_yAxis(title = list(text = \"Call Time in Min\")) %&gt;%\n  hc_chart(type = \"column\")\n\nWarning: 'hcboxplot' is deprecated.\nUse 'data_to_boxplot' instead.\nSee help(\"Deprecated\")\n\np3"
  },
  {
    "objectID": "content/labs/r-labs/dashboard/example.html#row-4",
    "href": "content/labs/r-labs/dashboard/example.html#row-4",
    "title": "Critical Result Callback Monitor",
    "section": "\n3.1 Row",
    "text": "3.1 Row\n\n3.1.1 Mean callback time\n\nnon_calls &lt;- cb_data %&gt;% filter(tech_location == \"Non-CC\")\nmean_tat_non &lt;- round(mean(non_calls$call_tat),0)\n \nvalueBox(value = mean_tat_non, icon = \"fa-stopwatch\",\n         caption = \"Mean callback time\", color = \"#708090\")\n\n18\n\n\n\n3.1.2 Total calls\n\nnon_calls_n &lt;- cb_data %&gt;% filter(tech_location == \"Non-CC\") %&gt;% nrow()\nvalueBox(value = non_calls_n, icon = \"fa-hashtag\",\n         caption = \"Total calls\", color = \"orange\")\n\n1303\n\n\n\n3.1.3 Percent on Time\n\nnon_ontime_n &lt;- cb_data %&gt;% \n  filter(tech_location == \"Non-CC\", call_tat &lt;= 30) %&gt;% nrow()\nnon_late_n &lt;- cb_data %&gt;% \n  filter(tech_location == \"Non-CC\", call_tat &gt;30) %&gt;% nrow()\nnon_pct_ontime &lt;- round((non_ontime_n/non_calls_n)*100,0)\n\n\nif(non_pct_ontime &gt;= 95){\n  valueBox(value = non_pct_ontime, icon = \"fa-percent\", \n           caption = \"Percent on time\", color = \"mediumseagreen\") #3CB371\n} else{\nvalueBox(value = non_pct_ontime, icon = \"fa-percent\", \n         caption = \"Percent on time\", color = \"indianred\")} #CD5C5C\n\n82"
  },
  {
    "objectID": "content/labs/r-labs/dashboard/example.html#row-5",
    "href": "content/labs/r-labs/dashboard/example.html#row-5",
    "title": "Critical Result Callback Monitor",
    "section": "\n3.2 Row",
    "text": "3.2 Row\n\n3.2.1 Overdue Call Details for 2020-07-01 through 2020-07-31\n\nnon_late &lt;- cb_data %&gt;% \n  filter(tech_location == \"Non-CC\", call_tat &gt;30) %&gt;%\n  select(call_tat, accession, pt_type, pt_loc_code, test_code,\n         result_datetime, phoned_title, tech)\n\ndatatable(non_late, options = list(pageLength = 20, autoWidth = TRUE))\n\n\n\n\n\n\n\n3.2.2 Tech Summary for 2020-07-01 through 2020-07-31\n\np4 &lt;- hcboxplot(\n  outliers = FALSE,\n  x = non_calls$call_tat,\n  var = non_calls$tech,\n  name = \"TAT\", \n  color = \"#4F94CD\",\n  lineWidth = 2) %&gt;%\n  hc_title(text = \"\") %&gt;%\n  hc_xAxis(title = list(text = \"Tech Code\")) %&gt;%\n  hc_yAxis(title = list(text = \"Call Time in Min\")) %&gt;%\n  hc_chart(type = \"column\")\n\nWarning: 'hcboxplot' is deprecated.\nUse 'data_to_boxplot' instead.\nSee help(\"Deprecated\")\n\np4"
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/listing.html",
    "href": "content/courses/TRIZ4ProbSolving/listing.html",
    "title": "TRIZ for Innovation",
    "section": "",
    "text": "Title\n\n\nReading Time\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/80-htmlwidgets/htmlwidgets.html#iconify-noto-v1-package-setting-up-r-packages",
    "href": "content/courses/R4Artists/Modules/80-htmlwidgets/htmlwidgets.html#iconify-noto-v1-package-setting-up-r-packages",
    "title": "Lab-8: Did you ever see such a thing as a drawing of a muchness?",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\n# Data manipulation and data sources\noptions(htmltools.preserve.raw = FALSE, echo = TRUE)\nlibrary(tidyverse)\nlibrary(tidygraph)\nlibrary(igraph)\nlibrary(palmerpenguins)\nlibrary(igraphdata)\n\n\n# To render htmlwidgets as iframe widgets\n# https://communicate-data-with-r.netlify.app/docs/communicate/htmlwidgets-in-documents/\nlibrary(widgetframe)\n\n\n# htmlwidget related libraries\nlibrary(htmlwidgets)\nlibrary(crosstalk)\n\n# Widget Libraries\nlibrary(leaflet)\nlibrary(plotly)\nlibrary(DT)\nlibrary(echarts4r)\nlibrary(echarts4r.assets)\nlibrary(canvasXpress)\nlibrary(rgl)\nlibrary(networkD3)\nlibrary(threejs)\nlibrary(slickR)\n# Linkable widgets in crosstalk - github repo only\n#devtools::install_github(\"kent37/summarywidget\")\nlibrary(summarywidget)"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/80-htmlwidgets/htmlwidgets.html#introduction",
    "href": "content/courses/R4Artists/Modules/80-htmlwidgets/htmlwidgets.html#introduction",
    "title": "Lab-8: Did you ever see such a thing as a drawing of a muchness?",
    "section": "Introduction",
    "text": "Introduction\nThere are very many great JavaScript libraries for creating eye-popping and even interactive charts! And these are now available in R, and can be invoked using R code! So we can “use JavaScript” in R, as it were, without knowing JavaScript! And create something like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnlike the Dormouse, no complaints!\nWe will explore a few them, as an alternative to ggplot !!\nThis may be too much of a good thing, or a much of muchness but then, we can always use more then one way of telling our stories!!"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/80-htmlwidgets/htmlwidgets.html#htmlwidgets-usage",
    "href": "content/courses/R4Artists/Modules/80-htmlwidgets/htmlwidgets.html#htmlwidgets-usage",
    "title": "Lab-8: Did you ever see such a thing as a drawing of a muchness?",
    "section": "\nhtmlwidgets usage",
    "text": "htmlwidgets usage\nhtmlwidgets creates, well, widgets, that can visualize data in many ways. HTML widgets work just like R plots except they produce interactive web visualizations. These can be used in RMarkdown, in flexdashboards, and in shiny apps.\nAll the possible widgets ( 127 of them on CRAN ) are listed here: https://gallery.htmlwidgets.org/\nSome packages that offer widgets for use in htmlwidgets:\n\n\nnetworkD3:\n\n\nForce directed networks with simpleNetwork and forceNetwork\n\nSankey diagrams with sankeyNetwork\n\nRadial networks with radialNetwork\n\nDendro networks with dendroNetwork\n\n\nUsing networkD3\n\n\nlibrary(networkD3)\ndata(\"karate\")\n\n# Make separate data frames for edges and nodes\n# networkD3 needs indices starting from 1\nkarate_edges &lt;- karate %&gt;% \n  as_tbl_graph() %&gt;% \n  tidygraph::activate(edges) %&gt;% \n  as.data.frame() %&gt;% \n  dplyr::mutate(source = from - 1, target = to - 1) %&gt;% \n  rename(value = weight) %&gt;% \n  select(source, target, value)\nkarate_edges\n\n\n\n  \n\n\nkarate_nodes &lt;- karate %&gt;% \n  as_tbl_graph() %&gt;% \n  tidygraph::activate(nodes) %&gt;% \n  as.data.frame() %&gt;% \n  dplyr::mutate(group = as.character(Faction)) %&gt;% \n  select(name, label, group, color)\nkarate_nodes\n\n\n\n  \n\n\nsimpleNetwork(karate_edges, \n              charge = -50, \n              fontSize = 12, zoom = FALSE, \n              fontFamily = \"serif\")\n\n\n\n\n\n\n\nforceNetwork(\n  Links = karate_edges,\n  Nodes = karate_nodes,\n  Value = \"value\",\n  # width of edges, dbl\n  NodeID = \"name\",\n  # chr\n  Group = \"group\",\n  # Node group, chr\n  # Nodesize = \"label\" # chr !!!\n  # linkColour = \"value\"\n)\n\n\n\n\n\n\nCreating a Sankey Diagram.\n\nUCB_graph &lt;-\n  UCBAdmissions %&gt;% \n  as.data.frame() %&gt;% \n  select(Gender, Admit, Dept, Freq) %&gt;% \n  as_tbl_graph()\nUCB_graph\n\n# A tbl_graph: 4 nodes and 24 edges\n#\n# A directed acyclic multigraph with 1 component\n#\n# A tibble: 4 × 1\n  name    \n  &lt;chr&gt;   \n1 Male    \n2 Female  \n3 Admitted\n4 Rejected\n#\n# A tibble: 24 × 4\n   from    to Dept   Freq\n  &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1     3 A       512\n2     1     4 A       313\n3     2     3 A        89\n# ℹ 21 more rows\n\nUCB_nodes &lt;- UCB_graph %&gt;% activate(nodes) %&gt;% as_tibble()\nUCB_nodes\n\n\n\n  \n\n\nUCB_links &lt;- UCB_graph %&gt;% \n  activate(edges) %&gt;% \n  as_tibble() %&gt;%\n  dplyr::mutate(from = from - 1, to = to - 1)\nUCB_links\n\n\n\n  \n\n\nsankeyNetwork(\n  Links = UCB_links,\n  Nodes = UCB_nodes,\n  Source = \"from\",\n  Target = \"to\",\n  Value = \"Freq\",\n  LinkGroup = \"Dept\",\n  fontSize = 20,\n  fontFamily = \"Arial\"\n)\n\n\n\n\n\n\nsimpleNetwork, forceNetwork and sankeyNetwork use a similar node and link data structure, organized as two data frames (not tibbles)\nchordNetwork uses an association matrix type of matrix or a data frame organized in the same way, where entry (n,m) represents the strength of the link from group n to group m. Matrix needs to be square !! “Column names” and “Row names” need to be the same if the data is a data.frame.\n\ndata &lt;- matrix(rpois(n = 16, lambda = 50), \n               nrow = 4, ncol = 4)\nchordNetwork(data,\n             labels = c(\"A\", \"B\", \"C\", \"D\"))"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/80-htmlwidgets/htmlwidgets.html#using-threejs",
    "href": "content/courses/R4Artists/Modules/80-htmlwidgets/htmlwidgets.html#using-threejs",
    "title": "Lab-8: Did you ever see such a thing as a drawing of a muchness?",
    "section": "Using threejs",
    "text": "Using threejs\n\nhttps://bwlewis.github.io/rthreejs/\nhttps://bwlewis.github.io/rthreejs/crosstalk.html\n\n\ngraphjs usage\n\ndata(\"LeMis\") # igraph object\nLeMis\n\nIGRAPH b7852d9 U--- 77 254 -- \n+ attr: label (v/c), color (v/c), size (v/n)\n+ edges from b7852d9:\n [1]  1-- 2  1-- 3  1-- 4  3-- 4  1-- 5  1-- 6  1-- 7  1-- 8  1-- 9  1--10\n[11] 11--12  4--12  3--12  1--12 12--13 12--14 12--15 12--16 17--18 17--19\n[21] 18--19 17--20 18--20 19--20 17--21 18--21 19--21 20--21 17--22 18--22\n[31] 19--22 20--22 21--22 17--23 18--23 19--23 20--23 21--23 22--23 17--24\n[41] 18--24 19--24 20--24 21--24 22--24 23--24 13--24 12--24 24--25 12--25\n[51] 25--26 24--26 12--26 25--27 12--27 17--27 26--27 12--28 24--28 26--28\n[61] 25--28 27--28 12--29 28--29 24--30 28--30 12--30 24--31 31--32 12--32\n[71] 24--32 28--32 12--33 12--34 28--34 12--35 30--35 12--36 35--36 30--36\n+ ... omitted several edges\n\n#V(LeMis)$label\n#V(LeMis)$color\ngraphjs(LeMis,\n    layout=list( # animates between a list of layouts\n      # layouts need to be 3D layouts\n      # Or each can be a 3 column matrix with n(rows) = n(vertices)\n    layout_randomly(LeMis, dim=3),\n    layout_on_sphere(LeMis),\n    layout_with_drl(LeMis, dim=3),  # note! somewhat slow...\n    layout_with_fr(LeMis, dim=3, niter=30)\n    ),\n  main=list(\"random layout\", \"sphere layout\", \"drl layout\", \"fr layout\"),\n  fpl=300)"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/80-htmlwidgets/htmlwidgets.html#globejs-usage",
    "href": "content/courses/R4Artists/Modules/80-htmlwidgets/htmlwidgets.html#globejs-usage",
    "title": "Lab-8: Did you ever see such a thing as a drawing of a muchness?",
    "section": "\nglobejs usage",
    "text": "globejs usage\nPlot points, arcs and images on a globe in 3D using three.js. The globe can be rotated and and zoomed. Great Circles and historical routes are a good idea for this perhaps.\nRefer to this page for more ideas http://bwlewis.github.io/rthreejs/globejs.html\n\n# Random Lats and Longs\nlat &lt;- rpois(10, 60) + rnorm(10, 80)\nlong &lt;- rpois(10, 60) + rnorm(10, 10)\n\n# Random \"Spike\" heights for each location. Population? Tourists? GDP?\nvalue &lt;- rpois(10, lambda = 80)\n \nglobejs(lat = lat, long = long)\n\n\n\n\n\nglobejs(\n  lat = lat,\n  long = long,\n  \n  # random heights of the Spikes (!!) at lat-long combo\n  value = value,\n  color = \"red\",\n  # Zoom factor, default is 35\n  fov = 50\n)\n\n\n\n\n\nglobejs(\n  lat = lat,\n  long = long,\n  value = value,\n  color = \"red\",\n  pointsize = 4, # width of the columns\n  # Zoom position\n  fov = 35,\n  # initial position of the globe\n  rotationlat = 0.6, #  in RADIANS !!! Good Heavens!!\n  rotationlong = 0.2 #  in RADIANS !!! Good Heavens!!\n)\n\n\n\n\n\nglobejs(\n  lat = lat,\n  long = long,\n  value = value,\n  color = \"red\",\n  pointsize = 4,\n  fov = 35,\n  rotationlat = 0.6,\n  rotationlong = 0.2,\n  lightcolor = \"#aaeeff\",\n  emissive = \"#0000ee\",\n  bodycolor = \"#ffffff\",\n  bg = \"grey\"\n)\n\n\n\n\n\n\n\n# Plotting arcs on the globe\n# Requires 4 column data frame for start and end lat and long\n\narcs &lt;- data.frame(start_lat = runif(10, min = -20, max = 20), \n                   start_lon = runif(10, min = -20, max = 20), \n                   end_lat = runif(10, min = -20, max = 20),\n                   end_lon = runif(10, min = -20, max = 20) + 60)\narcs\n\n\n\n  \n\n\nglobejs(arcs = arcs,\n        arcsColor = \"gold\",\n        arcsLwd = 4,\n        arcsHeight = 0.6,\n        arcsOpacity = 1,\n        rotationlat = 0,\n        rotationlong = -2.2,\n        bg = \"lightblue\",atmosphere = FALSE,\n        pointsize = 2)\n\n\n\n\n\n\nthreejs contains a dataset called flights obtained from Callum Prentice’s FlightStream page: http://callumprentice.github.io/apps/flight_stream/index.html\n\ndata(\"flights\",package = \"threejs\")\nflights %&gt;% head()\n\n\n\n  \n\n\nfrequent_destinations &lt;-\n  flights %&gt;% group_by(dest_lat, dest_long) %&gt;% \n  count(sort = TRUE) %&gt;% \n  ungroup() %&gt;% \n  slice_max(n = 10, order_by = n)\nfrequent_destinations\n\n\n\n  \n\n\nfrequent_flights &lt;- flights %&gt;% \n  semi_join(frequent_destinations,\n            by = c(\"dest_lat\" = \"dest_lat\", \"dest_long\" = \"dest_long\")) %&gt;% unique() \n\nfrequent_flights %&gt;% \n  kableExtra::kbl() %&gt;%\n  kableExtra::kable_paper(full_width = TRUE) %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"condensed\", \"responsive\")) %&gt;%\n  kableExtra::scroll_box(width = \"700px\", height = \"500px\")\n\n\n\n\n\n\norigin_lat\norigin_long\ndest_lat\ndest_long\n\n\n\n1\n40.657633\n17.947033\n47.464722\n8.549167\n\n\n2\n44.828335\n-0.715556\n47.464722\n8.549167\n\n\n3\n51.382669\n-2.719089\n47.464722\n8.549167\n\n\n4\n34.875117\n33.624850\n47.464722\n8.549167\n\n\n5\n27.931886\n-15.386586\n47.464722\n8.549167\n\n\n6\n25.557111\n34.583711\n47.464722\n8.549167\n\n\n7\n40.783200\n-91.125500\n41.978603\n-87.904842\n\n\n8\n39.834564\n-88.865689\n41.978603\n-87.904842\n\n\n9\n13.681108\n100.747283\n1.350189\n103.994433\n\n\n10\n-12.408333\n130.872660\n1.350189\n103.994433\n\n\n11\n19.934856\n110.458961\n1.350189\n103.994433\n\n\n12\n30.229503\n120.434453\n1.350189\n103.994433\n\n\n13\n22.308919\n113.914603\n1.350189\n103.994433\n\n\n14\n8.113200\n98.316872\n1.350189\n103.994433\n\n\n15\n34.434722\n135.244167\n25.077731\n121.232822\n\n\n16\n2.745578\n101.709917\n1.350189\n103.994433\n\n\n17\n14.508647\n121.019581\n1.350189\n103.994433\n\n\n18\n5.297139\n100.276864\n1.350189\n103.994433\n\n\n19\n-31.940278\n115.966944\n1.350189\n103.994433\n\n\n20\n11.546556\n104.844139\n1.350189\n103.994433\n\n\n21\n13.410666\n103.812840\n1.350189\n103.994433\n\n\n22\n16.907305\n96.133222\n1.350189\n103.994433\n\n\n23\n10.818797\n106.651856\n1.350189\n103.994433\n\n\n24\n1.350189\n103.994433\n25.077731\n121.232822\n\n\n25\n51.132767\n13.767161\n47.464722\n8.549167\n\n\n26\n42.760277\n10.239445\n47.464722\n8.549167\n\n\n27\n46.991067\n15.439628\n47.464722\n8.549167\n\n\n28\n47.793304\n13.004333\n47.464722\n8.549167\n\n\n29\n21.539400\n109.294000\n34.447119\n108.751592\n\n\n30\n29.719217\n106.641678\n40.080111\n116.584556\n\n\n31\n29.719217\n106.641678\n31.143378\n121.805214\n\n\n32\n29.719217\n106.641678\n34.447119\n108.751592\n\n\n33\n30.578528\n103.947086\n40.080111\n116.584556\n\n\n34\n30.578528\n103.947086\n31.143378\n121.805214\n\n\n35\n30.578528\n103.947086\n34.447119\n108.751592\n\n\n36\n25.935064\n119.663272\n34.447119\n108.751592\n\n\n37\n34.490900\n102.371900\n34.447119\n108.751592\n\n\n38\n45.623403\n126.250328\n34.447119\n108.751592\n\n\n39\n24.992364\n102.743536\n25.077731\n121.232822\n\n\n40\n24.992364\n102.743536\n34.447119\n108.751592\n\n\n41\n29.297778\n90.911944\n34.447119\n108.751592\n\n\n42\n35.046100\n118.412000\n34.447119\n108.751592\n\n\n43\n36.857214\n117.215992\n34.447119\n108.751592\n\n\n44\n43.907106\n87.474244\n34.447119\n108.751592\n\n\n45\n38.269200\n109.731000\n34.447119\n108.751592\n\n\n46\n36.898731\n30.800461\n47.464722\n8.549167\n\n\n47\n27.178317\n33.799436\n47.464722\n8.549167\n\n\n48\n42.572778\n21.035833\n47.464722\n8.549167\n\n\n50\n41.961622\n21.621381\n47.464722\n8.549167\n\n\n51\n50.865917\n7.142744\n51.477500\n-0.461389\n\n\n52\n50.865917\n7.142744\n48.110278\n16.569722\n\n\n53\n50.865917\n7.142744\n47.464722\n8.549167\n\n\n54\n52.461056\n9.685078\n48.110278\n16.569722\n\n\n55\n53.630389\n9.988228\n51.477500\n-0.461389\n\n\n56\n53.630389\n9.988228\n48.110278\n16.569722\n\n\n57\n53.630389\n9.988228\n47.464722\n8.549167\n\n\n58\n48.689878\n9.221964\n48.110278\n16.569722\n\n\n59\n52.559686\n13.287711\n48.110278\n16.569722\n\n\n60\n10.307542\n123.979439\n1.350189\n103.994433\n\n\n61\n15.185833\n120.560278\n1.350189\n103.994433\n\n\n62\n10.713044\n122.545297\n1.350189\n103.994433\n\n\n63\n14.508647\n121.019581\n40.080111\n116.584556\n\n\n64\n14.508647\n121.019581\n31.143378\n121.805214\n\n\n66\n14.508647\n121.019581\n25.077731\n121.232822\n\n\n67\n12.994414\n80.180517\n1.350189\n103.994433\n\n\n68\n40.560000\n109.997000\n34.447119\n108.751592\n\n\n69\n31.780019\n117.298436\n31.143378\n121.805214\n\n\n70\n24.992364\n102.743536\n40.080111\n116.584556\n\n\n71\n24.992364\n102.743536\n31.143378\n121.805214\n\n\n73\n26.538522\n106.800703\n1.350189\n103.994433\n\n\n74\n26.883333\n100.233330\n34.447119\n108.751592\n\n\n75\n24.401100\n98.531700\n40.080111\n116.584556\n\n\n77\n5.937208\n116.051181\n31.143378\n121.805214\n\n\n78\n13.681108\n100.747283\n31.143378\n121.805214\n\n\n79\n43.541200\n125.120100\n31.143378\n121.805214\n\n\n80\n33.511306\n126.493028\n31.143378\n121.805214\n\n\n82\n18.766847\n98.962644\n31.143378\n121.805214\n\n\n83\n38.965667\n121.538600\n31.143378\n121.805214\n\n\n84\n29.102800\n110.443000\n31.143378\n121.805214\n\n\n85\n22.308919\n113.914603\n31.143378\n121.805214\n\n\n86\n8.113200\n98.316872\n31.143378\n121.805214\n\n\n87\n45.623403\n126.250328\n31.143378\n121.805214\n\n\n88\n33.149700\n130.302000\n31.143378\n121.805214\n\n\n89\n36.181083\n140.415444\n31.143378\n121.805214\n\n\n90\n22.577094\n120.350006\n31.143378\n121.805214\n\n\n91\n34.434722\n135.244167\n31.143378\n121.805214\n\n\n93\n25.218106\n110.039197\n31.143378\n121.805214\n\n\n94\n36.117000\n103.617000\n31.143378\n121.805214\n\n\n95\n22.149556\n113.591558\n31.143378\n121.805214\n\n\n96\n31.143378\n121.805214\n1.350189\n103.994433\n\n\n97\n31.143378\n121.805214\n25.077731\n121.232822\n\n\n98\n31.197875\n121.336319\n34.447119\n108.751592\n\n\n99\n41.382400\n123.290100\n34.447119\n108.751592\n\n\n100\n38.280686\n114.697300\n25.077731\n121.232822\n\n\n102\n22.006400\n113.376000\n31.143378\n121.805214\n\n\n103\n19.088686\n72.867919\n51.477500\n-0.461389\n\n\n104\n19.088686\n72.867919\n1.350189\n103.994433\n\n\n105\n50.901389\n4.484444\n43.677223\n-79.630556\n\n\n106\n28.566500\n77.103088\n51.477500\n-0.461389\n\n\n107\n28.566500\n77.103088\n1.350189\n103.994433\n\n\n109\n37.936358\n23.944467\n51.477500\n-0.461389\n\n\n110\n37.936358\n23.944467\n48.110278\n16.569722\n\n\n111\n35.339719\n25.180297\n48.110278\n16.569722\n\n\n112\n41.669167\n44.954722\n48.110278\n16.569722\n\n\n113\n35.040222\n-106.609194\n41.978603\n-87.904842\n\n\n114\n57.201944\n-2.197778\n51.477500\n-0.461389\n\n\n115\n5.605186\n-0.166786\n51.477500\n-0.461389\n\n\n116\n36.674900\n-4.499106\n51.477500\n-0.461389\n\n\n117\n42.557100\n-92.400300\n41.978603\n-87.904842\n\n\n118\n31.722556\n35.993214\n41.978603\n-87.904842\n\n\n119\n52.308613\n4.763889\n51.477500\n-0.461389\n\n\n120\n59.651944\n17.918611\n51.477500\n-0.461389\n\n\n121\n43.991922\n-76.021739\n41.978603\n-87.904842\n\n\n123\n33.636719\n-84.428067\n51.477500\n-0.461389\n\n\n124\n33.636719\n-84.428067\n41.978603\n-87.904842\n\n\n125\n24.432972\n54.651138\n51.477500\n-0.461389\n\n\n126\n24.432972\n54.651138\n41.978603\n-87.904842\n\n\n127\n30.194528\n-97.669889\n51.477500\n-0.461389\n\n\n128\n30.194528\n-97.669889\n41.978603\n-87.904842\n\n\n129\n42.234875\n-85.552058\n41.978603\n-87.904842\n\n\n130\n26.270834\n50.633610\n51.477500\n-0.461389\n\n\n131\n41.297078\n2.078464\n51.477500\n-0.461389\n\n\n132\n41.938889\n-72.683222\n41.978603\n-87.904842\n\n\n133\n60.293386\n5.218142\n51.477500\n-0.461389\n\n\n134\n54.618056\n-5.872500\n51.477500\n-0.461389\n\n\n135\n44.535444\n11.288667\n51.477500\n-0.461389\n\n\n136\n12.949986\n77.668206\n51.477500\n-0.461389\n\n\n137\n40.477111\n-88.915917\n41.978603\n-87.904842\n\n\n138\n36.124472\n-86.678194\n41.978603\n-87.904842\n\n\n140\n42.364347\n-71.005181\n51.477500\n-0.461389\n\n\n141\n42.364347\n-71.005181\n41.978603\n-87.904842\n\n\n142\n50.901389\n4.484444\n51.477500\n-0.461389\n\n\n143\n47.590000\n7.529167\n51.477500\n-0.461389\n\n\n144\n47.436933\n19.255592\n51.477500\n-0.461389\n\n\n145\n42.940525\n-78.732167\n41.978603\n-87.904842\n\n\n146\n39.175361\n-76.668333\n51.477500\n-0.461389\n\n\n147\n39.175361\n-76.668333\n41.978603\n-87.904842\n\n\n148\n49.012779\n2.550000\n51.477500\n-0.461389\n\n\n149\n49.012779\n2.550000\n41.978603\n-87.904842\n\n\n150\n35.035278\n-85.203808\n41.978603\n-87.904842\n\n\n151\n38.138639\n-78.452861\n41.978603\n-87.904842\n\n\n152\n41.884694\n-91.710806\n41.978603\n-87.904842\n\n\n153\n41.411689\n-81.849794\n41.978603\n-87.904842\n\n\n154\n35.214000\n-80.943139\n51.477500\n-0.461389\n\n\n155\n35.214000\n-80.943139\n41.978603\n-87.904842\n\n\n156\n35.214000\n-80.943139\n43.677223\n-79.630556\n\n\n157\n39.997972\n-82.891889\n41.978603\n-87.904842\n\n\n158\n40.039250\n-88.278056\n41.978603\n-87.904842\n\n\n159\n38.818094\n-92.219631\n41.978603\n-87.904842\n\n\n160\n55.617917\n12.655972\n51.477500\n-0.461389\n\n\n161\n-33.964806\n18.601667\n51.477500\n-0.461389\n\n\n162\n21.036528\n-86.877083\n41.978603\n-87.904842\n\n\n163\n39.048836\n-84.667822\n41.978603\n-87.904842\n\n\n164\n44.772726\n-89.646635\n41.978603\n-87.904842\n\n\n165\n39.902375\n-84.219375\n41.978603\n-87.904842\n\n\n166\n42.402000\n-90.709472\n41.978603\n-87.904842\n\n\n167\n38.852083\n-77.037722\n41.978603\n-87.904842\n\n\n168\n38.852083\n-77.037722\n43.677223\n-79.630556\n\n\n170\n39.861656\n-104.673178\n51.477500\n-0.461389\n\n\n171\n39.861656\n-104.673178\n41.978603\n-87.904842\n\n\n172\n32.896828\n-97.037997\n51.477500\n-0.461389\n\n\n173\n32.896828\n-97.037997\n41.978603\n-87.904842\n\n\n174\n32.896828\n-97.037997\n43.677223\n-79.630556\n\n\n175\n25.261125\n51.565056\n41.978603\n-87.904842\n\n\n176\n41.533972\n-93.663083\n41.978603\n-87.904842\n\n\n177\n42.212444\n-83.353389\n41.978603\n-87.904842\n\n\n178\n53.421333\n-6.270075\n51.477500\n-0.461389\n\n\n179\n53.421333\n-6.270075\n41.978603\n-87.904842\n\n\n180\n51.289453\n6.766775\n51.477500\n-0.461389\n\n\n181\n51.289453\n6.766775\n41.978603\n-87.904842\n\n\n182\n25.252778\n55.364444\n51.477500\n-0.461389\n\n\n183\n0.042386\n32.443503\n51.477500\n-0.461389\n\n\n184\n55.950000\n-3.372500\n51.477500\n-0.461389\n\n\n185\n31.807250\n-106.377583\n41.978603\n-87.904842\n\n\n186\n38.036997\n-87.532364\n41.978603\n-87.904842\n\n\n187\n40.692500\n-74.168667\n51.477500\n-0.461389\n\n\n188\n40.692500\n-74.168667\n41.978603\n-87.904842\n\n\n189\n46.920650\n-96.815764\n41.978603\n-87.904842\n\n\n190\n41.804475\n12.250797\n51.477500\n-0.461389\n\n\n191\n41.804475\n12.250797\n41.978603\n-87.904842\n\n\n192\n26.072583\n-80.152750\n41.978603\n-87.904842\n\n\n193\n42.965424\n-83.743629\n41.978603\n-87.904842\n\n\n194\n50.026421\n8.543125\n51.477500\n-0.461389\n\n\n195\n43.582014\n-96.741914\n41.978603\n-87.904842\n\n\n196\n40.978472\n-85.195139\n41.978603\n-87.904842\n\n\n197\n36.151219\n-5.349664\n51.477500\n-0.461389\n\n\n198\n55.871944\n-4.433056\n51.477500\n-0.461389\n\n\n199\n57.662836\n12.279819\n51.477500\n-0.461389\n\n\n200\n44.485072\n-88.129589\n41.978603\n-87.904842\n\n\n201\n42.880833\n-85.522806\n41.978603\n-87.904842\n\n\n202\n46.238064\n6.108950\n51.477500\n-0.461389\n\n\n203\n52.461056\n9.685078\n51.477500\n-0.461389\n\n\n205\n60.317222\n24.963333\n51.477500\n-0.461389\n\n\n206\n22.308919\n113.914603\n41.978603\n-87.904842\n\n\n207\n41.066959\n-73.707575\n41.978603\n-87.904842\n\n\n208\n34.637194\n-86.775056\n41.978603\n-87.904842\n\n\n209\n17.453117\n78.467586\n51.477500\n-0.461389\n\n\n210\n38.944533\n-77.455811\n51.477500\n-0.461389\n\n\n211\n29.984433\n-95.341442\n51.477500\n-0.461389\n\n\n212\n29.984433\n-95.341442\n41.978603\n-87.904842\n\n\n213\n37.649944\n-97.433056\n41.978603\n-87.904842\n\n\n214\n39.717331\n-86.294383\n41.978603\n-87.904842\n\n\n215\n40.976922\n28.814606\n51.477500\n-0.461389\n\n\n216\n30.494056\n-81.687861\n41.978603\n-87.904842\n\n\n217\n40.639751\n-73.778925\n51.477500\n-0.461389\n\n\n218\n40.639751\n-73.778925\n41.978603\n-87.904842\n\n\n219\n40.639751\n-73.778925\n43.677223\n-79.630556\n\n\n220\n40.639751\n-73.778925\n47.464722\n8.549167\n\n\n221\n-26.139166\n28.246000\n51.477500\n-0.461389\n\n\n222\n2.745578\n101.709917\n51.477500\n-0.461389\n\n\n224\n29.226567\n47.968928\n51.477500\n-0.461389\n\n\n225\n36.080056\n-115.152250\n51.477500\n-0.461389\n\n\n226\n36.080056\n-115.152250\n41.978603\n-87.904842\n\n\n227\n36.080056\n-115.152250\n43.677223\n-79.630556\n\n\n228\n33.942536\n-118.408075\n51.477500\n-0.461389\n\n\n229\n33.942536\n-118.408075\n41.978603\n-87.904842\n\n\n230\n33.942536\n-118.408075\n31.143378\n121.805214\n\n\n231\n33.942536\n-118.408075\n43.677223\n-79.630556\n\n\n232\n53.865897\n-1.660569\n51.477500\n-0.461389\n\n\n233\n34.875117\n33.624850\n51.477500\n-0.461389\n\n\n234\n38.036500\n-84.605889\n41.978603\n-87.904842\n\n\n235\n40.777245\n-73.872608\n41.978603\n-87.904842\n\n\n236\n40.777245\n-73.872608\n43.677223\n-79.630556\n\n\n237\n51.477500\n-0.461389\n41.978603\n-87.904842\n\n\n238\n51.477500\n-0.461389\n48.110278\n16.569722\n\n\n239\n51.477500\n-0.461389\n43.677223\n-79.630556\n\n\n240\n51.477500\n-0.461389\n47.464722\n8.549167\n\n\n241\n34.729444\n-92.224306\n41.978603\n-87.904842\n\n\n242\n43.878986\n-91.256711\n41.978603\n-87.904842\n\n\n243\n40.493556\n-3.566764\n41.978603\n-87.904842\n\n\n244\n53.353744\n-2.274950\n41.978603\n-87.904842\n\n\n245\n18.503717\n-77.913358\n41.978603\n-87.904842\n\n\n246\n39.297606\n-94.713905\n41.978603\n-87.904842\n\n\n247\n28.429394\n-81.308994\n41.978603\n-87.904842\n\n\n248\n28.429394\n-81.308994\n43.677223\n-79.630556\n\n\n249\n40.193494\n-76.763403\n41.978603\n-87.904842\n\n\n250\n35.042417\n-89.976667\n41.978603\n-87.904842\n\n\n251\n19.436303\n-99.072097\n41.978603\n-87.904842\n\n\n252\n39.140972\n-96.670833\n41.978603\n-87.904842\n\n\n253\n25.793250\n-80.290556\n41.978603\n-87.904842\n\n\n254\n25.793250\n-80.290556\n43.677223\n-79.630556\n\n\n255\n42.947222\n-87.896583\n41.978603\n-87.904842\n\n\n256\n41.448528\n-90.507539\n41.978603\n-87.904842\n\n\n257\n46.353611\n-87.395278\n41.978603\n-87.904842\n\n\n258\n43.139858\n-89.337514\n41.978603\n-87.904842\n\n\n259\n44.881956\n-93.221767\n41.978603\n-87.904842\n\n\n260\n29.993389\n-90.258028\n41.978603\n-87.904842\n\n\n261\n35.764722\n140.386389\n41.978603\n-87.904842\n\n\n262\n35.764722\n140.386389\n1.350189\n103.994433\n\n\n263\n35.764722\n140.386389\n25.077731\n121.232822\n\n\n264\n35.393089\n-97.600733\n41.978603\n-87.904842\n\n\n265\n41.303167\n-95.894069\n41.978603\n-87.904842\n\n\n266\n41.978603\n-87.904842\n40.080111\n116.584556\n\n\n267\n41.978603\n-87.904842\n31.143378\n121.805214\n\n\n268\n41.978603\n-87.904842\n43.677223\n-79.630556\n\n\n269\n39.871944\n-75.241139\n43.677223\n-79.630556\n\n\n270\n39.871944\n-75.241139\n47.464722\n8.549167\n\n\n271\n28.945464\n-13.605225\n47.464722\n8.549167\n\n\n272\n36.674900\n-4.499106\n48.110278\n16.569722\n\n\n273\n38.282169\n-0.558156\n47.464722\n8.549167\n\n\n274\n31.722556\n35.993214\n48.110278\n16.569722\n\n\n275\n59.651944\n17.918611\n48.110278\n16.569722\n\n\n276\n24.432972\n54.651138\n40.080111\n116.584556\n\n\n277\n24.432972\n54.651138\n1.350189\n103.994433\n\n\n278\n41.297078\n2.078464\n48.110278\n16.569722\n\n\n280\n-27.384167\n153.117500\n1.350189\n103.994433\n\n\n282\n49.012779\n2.550000\n48.110278\n16.569722\n\n\n283\n39.601944\n19.911667\n48.110278\n16.569722\n\n\n284\n35.531747\n24.149678\n48.110278\n16.569722\n\n\n285\n55.617917\n12.655972\n48.110278\n16.569722\n\n\n286\n37.466781\n15.066400\n47.464722\n8.549167\n\n\n288\n51.289453\n6.766775\n48.110278\n16.569722\n\n\n289\n51.289453\n6.766775\n47.464722\n8.549167\n\n\n290\n41.804475\n12.250797\n48.110278\n16.569722\n\n\n291\n43.809953\n11.205100\n48.110278\n16.569722\n\n\n292\n32.697889\n-16.774453\n48.110278\n16.569722\n\n\n293\n32.697889\n-16.774453\n47.464722\n8.549167\n\n\n295\n50.026421\n8.543125\n48.110278\n16.569722\n\n\n296\n28.452717\n-13.863761\n48.110278\n16.569722\n\n\n297\n28.452717\n-13.863761\n47.464722\n8.549167\n\n\n298\n54.913250\n8.340472\n47.464722\n8.549167\n\n\n303\n60.317222\n24.963333\n48.110278\n16.569722\n\n\n304\n60.317222\n24.963333\n47.464722\n8.549167\n\n\n306\n35.339719\n25.180297\n47.464722\n8.549167\n\n\n307\n38.872858\n1.373117\n47.464722\n8.549167\n\n\n308\n37.435128\n25.348103\n48.110278\n16.569722\n\n\n309\n36.399169\n25.479333\n48.110278\n16.569722\n\n\n310\n37.068319\n22.025525\n48.110278\n16.569722\n\n\n311\n34.875117\n33.624850\n48.110278\n16.569722\n\n\n312\n51.505278\n0.055278\n47.464722\n8.549167\n\n\n315\n27.931886\n-15.386586\n48.110278\n16.569722\n\n\n317\n40.493556\n-3.566764\n48.110278\n16.569722\n\n\n318\n40.493556\n-3.566764\n47.464722\n8.549167\n\n\n319\n35.857497\n14.477500\n48.110278\n16.569722\n\n\n320\n48.353783\n11.786086\n48.110278\n16.569722\n\n\n321\n45.630606\n8.728111\n48.110278\n16.569722\n\n\n322\n40.886033\n14.290781\n47.464722\n8.549167\n\n\n323\n43.658411\n7.215872\n48.110278\n16.569722\n\n\n324\n49.498700\n11.066897\n48.110278\n16.569722\n\n\n325\n40.898661\n9.517628\n48.110278\n16.569722\n\n\n326\n40.898661\n9.517628\n47.464722\n8.549167\n\n\n327\n39.553610\n2.727778\n48.110278\n16.569722\n\n\n328\n39.553610\n2.727778\n47.464722\n8.549167\n\n\n329\n38.925467\n20.765311\n48.110278\n16.569722\n\n\n330\n36.405419\n28.086192\n48.110278\n16.569722\n\n\n331\n40.519725\n22.970950\n48.110278\n16.569722\n\n\n332\n38.905394\n16.242269\n47.464722\n8.549167\n\n\n333\n47.793304\n13.004333\n48.110278\n16.569722\n\n\n334\n28.044475\n-16.572489\n48.110278\n16.569722\n\n\n335\n28.044475\n-16.572489\n47.464722\n8.549167\n\n\n337\n52.559686\n13.287711\n47.464722\n8.549167\n\n\n338\n48.110278\n16.569722\n47.464722\n8.549167\n\n\n339\n17.136749\n-61.792667\n43.677223\n-79.630556\n\n\n340\n33.636719\n-84.428067\n43.677223\n-79.630556\n\n\n341\n12.501389\n-70.015221\n43.677223\n-79.630556\n\n\n342\n24.432972\n54.651138\n43.677223\n-79.630556\n\n\n343\n19.267000\n-69.742000\n43.677223\n-79.630556\n\n\n344\n41.297078\n2.078464\n43.677223\n-79.630556\n\n\n345\n32.364042\n-64.678703\n43.677223\n-79.630556\n\n\n346\n41.938889\n-72.683222\n43.677223\n-79.630556\n\n\n347\n13.074603\n-59.492456\n43.677223\n-79.630556\n\n\n348\n36.124472\n-86.678194\n43.677223\n-79.630556\n\n\n349\n4.701594\n-74.146947\n43.677223\n-79.630556\n\n\n350\n42.364347\n-71.005181\n43.677223\n-79.630556\n\n\n352\n39.175361\n-76.668333\n43.677223\n-79.630556\n\n\n353\n22.513200\n-78.511000\n43.677223\n-79.630556\n\n\n354\n49.012779\n2.550000\n43.677223\n-79.630556\n\n\n355\n41.411689\n-81.849794\n43.677223\n-79.630556\n\n\n357\n39.997972\n-82.891889\n43.677223\n-79.630556\n\n\n358\n55.617917\n12.655972\n43.677223\n-79.630556\n\n\n359\n21.036528\n-86.877083\n43.677223\n-79.630556\n\n\n360\n39.048836\n-84.667822\n43.677223\n-79.630556\n\n\n362\n39.861656\n-104.673178\n43.677223\n-79.630556\n\n\n364\n42.212444\n-83.353389\n43.677223\n-79.630556\n\n\n365\n53.421333\n-6.270075\n43.677223\n-79.630556\n\n\n366\n40.692500\n-74.168667\n43.677223\n-79.630556\n\n\n367\n41.804475\n12.250797\n43.677223\n-79.630556\n\n\n368\n26.072583\n-80.152750\n43.677223\n-79.630556\n\n\n369\n50.026421\n8.543125\n43.677223\n-79.630556\n\n\n370\n19.292778\n-81.357750\n43.677223\n-79.630556\n\n\n371\n23.562631\n-75.877958\n43.677223\n-79.630556\n\n\n372\n12.004247\n-61.786192\n43.677223\n-79.630556\n\n\n373\n-23.432075\n-46.469511\n43.677223\n-79.630556\n\n\n374\n22.989153\n-82.409086\n43.677223\n-79.630556\n\n\n375\n22.308919\n113.914603\n43.677223\n-79.630556\n\n\n376\n20.785589\n-76.315108\n43.677223\n-79.630556\n\n\n377\n38.944533\n-77.455811\n43.677223\n-79.630556\n\n\n378\n29.984433\n-95.341442\n43.677223\n-79.630556\n\n\n379\n39.717331\n-86.294383\n43.677223\n-79.630556\n\n\n380\n40.976922\n28.814606\n43.677223\n-79.630556\n\n\n382\n17.935667\n-76.787500\n43.677223\n-79.630556\n\n\n387\n-12.021889\n-77.114319\n43.677223\n-79.630556\n\n\n388\n10.593289\n-85.544408\n43.677223\n-79.630556\n\n\n389\n18.503717\n-77.913358\n43.677223\n-79.630556\n\n\n390\n39.297606\n-94.713905\n43.677223\n-79.630556\n\n\n392\n40.193494\n-76.763403\n43.677223\n-79.630556\n\n\n393\n19.436303\n-99.072097\n43.677223\n-79.630556\n\n\n395\n42.947222\n-87.896583\n43.677223\n-79.630556\n\n\n396\n44.881956\n-93.221767\n43.677223\n-79.630556\n\n\n398\n29.993389\n-90.258028\n43.677223\n-79.630556\n\n\n399\n48.353783\n11.786086\n43.677223\n-79.630556\n\n\n400\n25.038958\n-77.466231\n43.677223\n-79.630556\n\n\n401\n35.764722\n140.386389\n43.677223\n-79.630556\n\n\n403\n40.080111\n116.584556\n43.677223\n-79.630556\n\n\n405\n33.434278\n-112.011583\n43.677223\n-79.630556\n\n\n406\n40.491467\n-80.232872\n43.677223\n-79.630556\n\n\n407\n21.773625\n-72.265886\n43.677223\n-79.630556\n\n\n408\n19.757900\n-70.570033\n43.677223\n-79.630556\n\n\n409\n18.567367\n-68.363431\n43.677223\n-79.630556\n\n\n410\n31.143378\n121.805214\n43.677223\n-79.630556\n\n\n411\n35.877639\n-78.787472\n43.677223\n-79.630556\n\n\n412\n43.118866\n-77.672389\n43.677223\n-79.630556\n\n\n413\n26.536167\n-81.755167\n43.677223\n-79.630556\n\n\n414\n32.733556\n-117.189667\n43.677223\n-79.630556\n\n\n415\n-33.392975\n-70.785803\n43.677223\n-79.630556\n\n\n416\n47.449000\n-122.309306\n43.677223\n-79.630556\n\n\n417\n37.618972\n-122.374889\n43.677223\n-79.630556\n\n\n418\n9.993861\n-84.208806\n43.677223\n-79.630556\n\n\n419\n22.492192\n-79.943611\n43.677223\n-79.630556\n\n\n420\n38.748697\n-90.370028\n43.677223\n-79.630556\n\n\n421\n43.111187\n-76.106311\n43.677223\n-79.630556\n\n\n422\n32.011389\n34.886667\n43.677223\n-79.630556\n\n\n423\n27.975472\n-82.533250\n43.677223\n-79.630556\n\n\n424\n13.733194\n-60.952597\n43.677223\n-79.630556\n\n\n425\n48.110278\n16.569722\n43.677223\n-79.630556\n\n\n426\n23.034445\n-81.435278\n43.677223\n-79.630556\n\n\n427\n52.165750\n20.967122\n43.677223\n-79.630556\n\n\n428\n46.485001\n-84.509445\n43.677223\n-79.630556\n\n\n429\n49.210833\n-57.391388\n43.677223\n-79.630556\n\n\n430\n53.309723\n-113.579722\n43.677223\n-79.630556\n\n\n431\n45.868889\n-66.537222\n43.677223\n-79.630556\n\n\n432\n44.225277\n-76.596944\n43.677223\n-79.630556\n\n\n433\n44.880833\n-63.508610\n43.677223\n-79.630556\n\n\n434\n56.653333\n-111.221944\n43.677223\n-79.630556\n\n\n435\n45.322500\n-75.669167\n43.677223\n-79.630556\n\n\n436\n46.791111\n-71.393333\n43.677223\n-79.630556\n\n\n437\n42.275556\n-82.955556\n43.677223\n-79.630556\n\n\n438\n46.112221\n-64.678611\n43.677223\n-79.630556\n\n\n439\n50.431944\n-104.665833\n43.677223\n-79.630556\n\n\n440\n48.371944\n-89.323889\n43.677223\n-79.630556\n\n\n441\n46.161388\n-60.047779\n43.677223\n-79.630556\n\n\n442\n46.625000\n-80.798889\n43.677223\n-79.630556\n\n\n443\n45.316111\n-65.890278\n43.677223\n-79.630556\n\n\n444\n48.569721\n-81.376667\n43.677223\n-79.630556\n\n\n445\n45.470556\n-73.740833\n43.677223\n-79.630556\n\n\n446\n45.470556\n-73.740833\n47.464722\n8.549167\n\n\n447\n49.193889\n-123.184444\n43.677223\n-79.630556\n\n\n448\n49.910036\n-97.239886\n43.677223\n-79.630556\n\n\n449\n52.170834\n-106.699722\n43.677223\n-79.630556\n\n\n450\n43.033056\n-81.151111\n43.677223\n-79.630556\n\n\n451\n46.363611\n-79.422778\n43.677223\n-79.630556\n\n\n452\n51.113888\n-114.020278\n43.677223\n-79.630556\n\n\n453\n46.290001\n-63.121111\n43.677223\n-79.630556\n\n\n454\n48.646944\n-123.425833\n43.677223\n-79.630556\n\n\n455\n47.618610\n-52.751945\n43.677223\n-79.630556\n\n\n456\n43.677223\n-79.630556\n47.464722\n8.549167\n\n\n457\n34.519672\n113.840889\n25.077731\n121.232822\n\n\n458\n28.189158\n113.219633\n25.077731\n121.232822\n\n\n459\n26.883333\n100.233330\n25.077731\n121.232822\n\n\n460\n29.826683\n121.461906\n25.077731\n121.232822\n\n\n461\n31.742042\n118.862025\n25.077731\n121.232822\n\n\n462\n41.382400\n123.290100\n25.077731\n121.232822\n\n\n463\n52.308613\n4.763889\n25.077731\n121.232822\n\n\n468\n23.392436\n113.298786\n1.350189\n103.994433\n\n\n471\n49.012779\n2.550000\n40.080111\n116.584556\n\n\n472\n49.012779\n2.550000\n31.143378\n121.805214\n\n\n473\n49.012779\n2.550000\n1.350189\n103.994433\n\n\n476\n49.012779\n2.550000\n47.464722\n8.549167\n\n\n477\n42.212444\n-83.353389\n51.477500\n-0.461389\n\n\n479\n45.726387\n5.090833\n48.110278\n16.569722\n\n\n481\n36.691014\n3.215408\n51.477500\n-0.461389\n\n\n482\n36.691014\n3.215408\n40.080111\n116.584556\n\n\n483\n36.691014\n3.215408\n48.110278\n16.569722\n\n\n486\n19.088686\n72.867919\n47.464722\n8.549167\n\n\n487\n22.654739\n88.446722\n1.350189\n103.994433\n\n\n489\n28.566500\n77.103088\n41.978603\n-87.904842\n\n\n490\n28.566500\n77.103088\n31.143378\n121.805214\n\n\n492\n28.566500\n77.103088\n48.110278\n16.569722\n\n\n493\n28.566500\n77.103088\n47.464722\n8.549167\n\n\n495\n37.469075\n126.450517\n1.350189\n103.994433\n\n\n500\n5.937208\n116.051181\n1.350189\n103.994433\n\n\n501\n5.937208\n116.051181\n25.077731\n121.232822\n\n\n502\n6.166850\n102.293014\n1.350189\n103.994433\n\n\n503\n1.484697\n110.346933\n1.350189\n103.994433\n\n\n505\n6.329728\n99.728667\n1.350189\n103.994433\n\n\n506\n4.322014\n113.986806\n1.350189\n103.994433\n\n\n508\n20.521800\n-103.311167\n41.978603\n-87.904842\n\n\n510\n37.466781\n15.066400\n48.110278\n16.569722\n\n\n511\n61.174361\n-149.996361\n41.978603\n-87.904842\n\n\n514\n33.367467\n-7.589967\n51.477500\n-0.461389\n\n\n515\n33.367467\n-7.589967\n47.464722\n8.549167\n\n\n518\n13.440947\n-89.055728\n43.677223\n-79.630556\n\n\n535\n60.317222\n24.963333\n40.080111\n116.584556\n\n\n536\n60.317222\n24.963333\n31.143378\n121.805214\n\n\n537\n60.317222\n24.963333\n1.350189\n103.994433\n\n\n539\n60.317222\n24.963333\n34.447119\n108.751592\n\n\n551\n41.248055\n-8.681389\n47.464722\n8.549167\n\n\n557\n28.566500\n77.103088\n25.077731\n121.232822\n\n\n560\n41.804475\n12.250797\n31.143378\n121.805214\n\n\n563\n41.804475\n12.250797\n47.464722\n8.549167\n\n\n565\n45.445103\n9.276739\n48.110278\n16.569722\n\n\n566\n53.882469\n28.030731\n48.110278\n16.569722\n\n\n569\n29.719217\n106.641678\n25.077731\n121.232822\n\n\n570\n38.965667\n121.538600\n25.077731\n121.232822\n\n\n571\n25.935064\n119.663272\n25.077731\n121.232822\n\n\n575\n22.639258\n113.810664\n25.077731\n121.232822\n\n\n576\n36.266108\n120.374436\n25.077731\n121.232822\n\n\n577\n25.077731\n121.232822\n34.447119\n108.751592\n\n\n578\n9.006792\n7.263172\n51.477500\n-0.461389\n\n\n582\n43.352072\n77.040508\n51.477500\n-0.461389\n\n\n583\n31.722556\n35.993214\n51.477500\n-0.461389\n\n\n593\n33.820931\n35.488389\n51.477500\n-0.461389\n\n\n596\n13.681108\n100.747283\n51.477500\n-0.461389\n\n\n597\n55.740322\n9.151778\n47.464722\n8.549167\n\n\n606\n30.121944\n31.405556\n51.477500\n-0.461389\n\n\n611\n30.578528\n103.947086\n51.477500\n-0.461389\n\n\n615\n55.408611\n37.906111\n51.477500\n-0.461389\n\n\n624\n-34.822222\n-58.535833\n51.477500\n-0.461389\n\n\n625\n37.014425\n-7.965911\n51.477500\n-0.461389\n\n\n628\n8.616444\n-13.195489\n51.477500\n-0.461389\n\n\n631\n-22.808903\n-43.243647\n51.477500\n-0.461389\n\n\n634\n-23.432075\n-46.469511\n51.477500\n-0.461389\n\n\n636\n40.467500\n50.046667\n51.477500\n-0.461389\n\n\n640\n22.308919\n113.914603\n51.477500\n-0.461389\n\n\n641\n35.552258\n139.779694\n51.477500\n-0.461389\n\n\n645\n38.872858\n1.373117\n51.477500\n-0.461389\n\n\n646\n37.469075\n126.450517\n51.477500\n-0.461389\n\n\n648\n21.679564\n39.156536\n51.477500\n-0.461389\n\n\n650\n37.435128\n25.348103\n51.477500\n-0.461389\n\n\n652\n36.399169\n25.479333\n51.477500\n-0.461389\n\n\n653\n50.345000\n30.894722\n51.477500\n-0.461389\n\n\n655\n-8.858375\n13.231178\n51.477500\n-0.461389\n\n\n661\n59.800292\n30.262503\n51.477500\n-0.461389\n\n\n663\n51.477500\n-0.461389\n40.080111\n116.584556\n\n\n664\n51.477500\n-0.461389\n31.143378\n121.805214\n\n\n665\n51.477500\n-0.461389\n1.350189\n103.994433\n\n\n672\n23.843333\n90.397781\n51.477500\n-0.461389\n\n\n673\n23.843333\n90.397781\n1.350189\n103.994433\n\n\n674\n4.944200\n114.928353\n31.143378\n121.805214\n\n\n675\n4.944200\n114.928353\n1.350189\n103.994433\n\n\n676\n24.550560\n55.103174\n51.477500\n-0.461389\n\n\n677\n28.189158\n113.219633\n34.447119\n108.751592\n\n\n678\n22.608267\n108.172442\n34.447119\n108.751592\n\n\n679\n39.124353\n117.346183\n34.447119\n108.751592\n\n\n681\n43.670833\n142.447500\n25.077731\n121.232822\n\n\n683\n13.681108\n100.747283\n25.077731\n121.232822\n\n\n684\n13.681108\n100.747283\n48.110278\n16.569722\n\n\n685\n-27.384167\n153.117500\n25.077731\n121.232822\n\n\n686\n23.392436\n113.298786\n25.077731\n121.232822\n\n\n687\n49.012779\n2.550000\n25.077731\n121.232822\n\n\n688\n-6.125567\n106.655897\n25.077731\n121.232822\n\n\n690\n42.775200\n141.692283\n25.077731\n121.232822\n\n\n691\n30.578528\n103.947086\n25.077731\n121.232822\n\n\n692\n-8.748169\n115.167172\n25.077731\n121.232822\n\n\n693\n33.585942\n130.450686\n25.077731\n121.232822\n\n\n694\n13.483450\n144.795983\n25.077731\n121.232822\n\n\n695\n21.221192\n105.807178\n25.077731\n121.232822\n\n\n696\n40.851422\n111.824103\n25.077731\n121.232822\n\n\n697\n30.229503\n120.434453\n25.077731\n121.232822\n\n\n698\n41.770000\n140.821944\n25.077731\n121.232822\n\n\n699\n22.308919\n113.914603\n25.077731\n121.232822\n\n\n700\n45.623403\n126.250328\n25.077731\n121.232822\n\n\n701\n37.469075\n126.450517\n25.077731\n121.232822\n\n\n702\n40.639751\n-73.778925\n25.077731\n121.232822\n\n\n705\n36.394611\n136.406544\n25.077731\n121.232822\n\n\n706\n2.745578\n101.709917\n25.077731\n121.232822\n\n\n707\n25.218106\n110.039197\n25.077731\n121.232822\n\n\n708\n33.942536\n-118.408075\n25.077731\n121.232822\n\n\n709\n22.149556\n113.591558\n25.077731\n121.232822\n\n\n712\n34.756944\n133.855278\n25.077731\n121.232822\n\n\n713\n40.080111\n116.584556\n25.077731\n121.232822\n\n\n714\n11.546556\n104.844139\n25.077731\n121.232822\n\n\n716\n38.139722\n140.916944\n25.077731\n121.232822\n\n\n717\n47.449000\n-122.309306\n25.077731\n121.232822\n\n\n718\n37.618972\n-122.374889\n25.077731\n121.232822\n\n\n719\n10.818797\n106.651856\n25.077731\n121.232822\n\n\n721\n-7.379831\n112.786858\n25.077731\n121.232822\n\n\n722\n36.857214\n117.215992\n25.077731\n121.232822\n\n\n723\n25.077731\n121.232822\n43.677223\n-79.630556\n\n\n724\n56.923611\n23.971111\n48.110278\n16.569722\n\n\n725\n56.923611\n23.971111\n47.464722\n8.549167\n\n\n726\n6.498553\n-58.254119\n43.677223\n-79.630556\n\n\n729\n10.595369\n-61.337242\n43.677223\n-79.630556\n\n\n730\n35.179528\n128.938222\n25.077731\n121.232822\n\n\n731\n35.179528\n128.938222\n34.447119\n108.751592\n\n\n732\n8.977889\n38.799319\n40.080111\n116.584556\n\n\n733\n-37.008056\n174.791667\n31.143378\n121.805214\n\n\n734\n59.651944\n17.918611\n40.080111\n116.584556\n\n\n735\n40.560000\n109.997000\n40.080111\n116.584556\n\n\n737\n21.539400\n109.294000\n40.080111\n116.584556\n\n\n738\n13.681108\n100.747283\n40.080111\n116.584556\n\n\n739\n30.121944\n31.405556\n40.080111\n116.584556\n\n\n740\n23.392436\n113.298786\n40.080111\n116.584556\n\n\n741\n23.392436\n113.298786\n31.143378\n121.805214\n\n\n742\n23.392436\n113.298786\n34.447119\n108.751592\n\n\n745\n34.519672\n113.840889\n40.080111\n116.584556\n\n\n746\n43.541200\n125.120100\n40.080111\n116.584556\n\n\n747\n41.538100\n120.435000\n40.080111\n116.584556\n\n\n748\n42.235000\n118.908000\n40.080111\n116.584556\n\n\n749\n36.716600\n127.499119\n40.080111\n116.584556\n\n\n754\n18.766847\n98.962644\n40.080111\n116.584556\n\n\n755\n55.617917\n12.655972\n40.080111\n116.584556\n\n\n756\n28.189158\n113.219633\n40.080111\n116.584556\n\n\n757\n42.775200\n141.692283\n40.080111\n116.584556\n\n\n760\n30.578528\n103.947086\n1.350189\n103.994433\n\n\n763\n31.941667\n119.711667\n40.080111\n116.584556\n\n\n764\n40.060300\n113.482000\n40.080111\n116.584556\n\n\n765\n31.300000\n107.500000\n40.080111\n116.584556\n\n\n766\n40.025500\n124.286600\n40.080111\n116.584556\n\n\n767\n28.566500\n77.103088\n40.080111\n116.584556\n\n\n768\n38.965667\n121.538600\n40.080111\n116.584556\n\n\n770\n38.965667\n121.538600\n34.447119\n108.751592\n\n\n771\n39.850000\n110.033000\n40.080111\n116.584556\n\n\n772\n51.289453\n6.766775\n40.080111\n116.584556\n\n\n773\n25.252778\n55.364444\n40.080111\n116.584556\n\n\n774\n29.102800\n110.443000\n40.080111\n116.584556\n\n\n775\n40.692500\n-74.168667\n40.080111\n116.584556\n\n\n776\n40.692500\n-74.168667\n31.143378\n121.805214\n\n\n777\n41.804475\n12.250797\n40.080111\n116.584556\n\n\n778\n39.224061\n125.670150\n40.080111\n116.584556\n\n\n779\n25.935064\n119.663272\n40.080111\n116.584556\n\n\n780\n50.026421\n8.543125\n40.080111\n116.584556\n\n\n781\n50.026421\n8.543125\n31.143378\n121.805214\n\n\n782\n32.900000\n115.816667\n40.080111\n116.584556\n\n\n783\n33.585942\n130.450686\n31.143378\n121.805214\n\n\n784\n37.558311\n126.790586\n40.080111\n116.584556\n\n\n785\n46.238064\n6.108950\n40.080111\n116.584556\n\n\n786\n32.391100\n105.702000\n40.080111\n116.584556\n\n\n787\n19.934856\n110.458961\n40.080111\n116.584556\n\n\n790\n40.851422\n111.824103\n40.080111\n116.584556\n\n\n791\n40.851422\n111.824103\n31.143378\n121.805214\n\n\n793\n31.780019\n117.298436\n40.080111\n116.584556\n\n\n794\n30.229503\n120.434453\n40.080111\n116.584556\n\n\n796\n30.229503\n120.434453\n34.447119\n108.751592\n\n\n797\n22.308919\n113.914603\n40.080111\n116.584556\n\n\n798\n8.113200\n98.316872\n40.080111\n116.584556\n\n\n799\n49.204997\n119.825000\n40.080111\n116.584556\n\n\n800\n46.083000\n122.017000\n40.080111\n116.584556\n\n\n801\n42.841400\n93.669200\n40.080111\n116.584556\n\n\n802\n35.552258\n139.779694\n40.080111\n116.584556\n\n\n803\n21.318681\n-157.922428\n40.080111\n116.584556\n\n\n804\n45.623403\n126.250328\n40.080111\n116.584556\n\n\n806\n28.562200\n121.429000\n40.080111\n116.584556\n\n\n807\n38.944533\n-77.455811\n40.080111\n116.584556\n\n\n808\n29.984433\n-95.341442\n40.080111\n116.584556\n\n\n809\n37.469075\n126.450517\n40.080111\n116.584556\n\n\n810\n38.481944\n106.009167\n40.080111\n116.584556\n\n\n811\n38.481944\n106.009167\n31.143378\n121.805214\n\n\n812\n38.481944\n106.009167\n34.447119\n108.751592\n\n\n813\n40.976922\n28.814606\n40.080111\n116.584556\n\n\n814\n40.976922\n28.814606\n31.143378\n121.805214\n\n\n815\n29.338600\n117.176000\n40.080111\n116.584556\n\n\n816\n29.338600\n117.176000\n34.447119\n108.751592\n\n\n817\n40.639751\n-73.778925\n40.080111\n116.584556\n\n\n818\n26.899700\n114.737500\n40.080111\n116.584556\n\n\n819\n29.733000\n115.983000\n40.080111\n116.584556\n\n\n820\n24.796400\n118.590000\n31.143378\n121.805214\n\n\n821\n46.843394\n130.465389\n40.080111\n116.584556\n\n\n822\n32.857000\n103.683000\n31.143378\n121.805214\n\n\n823\n28.865000\n115.900000\n40.080111\n116.584556\n\n\n824\n28.865000\n115.900000\n31.143378\n121.805214\n\n\n825\n34.434722\n135.244167\n40.080111\n116.584556\n\n\n829\n25.825800\n114.912000\n40.080111\n116.584556\n\n\n830\n26.538522\n106.800703\n40.080111\n116.584556\n\n\n831\n26.538522\n106.800703\n31.143378\n121.805214\n\n\n832\n26.538522\n106.800703\n34.447119\n108.751592\n\n\n833\n25.218106\n110.039197\n40.080111\n116.584556\n\n\n835\n25.218106\n110.039197\n34.447119\n108.751592\n\n\n836\n33.942536\n-118.408075\n40.080111\n116.584556\n\n\n838\n51.148056\n-0.190278\n40.080111\n116.584556\n\n\n841\n36.117000\n103.617000\n40.080111\n116.584556\n\n\n842\n36.117000\n103.617000\n34.447119\n108.751592\n\n\n843\n26.883333\n100.233330\n40.080111\n116.584556\n\n\n844\n29.297778\n90.911944\n40.080111\n116.584556\n\n\n845\n24.207500\n109.391000\n40.080111\n116.584556\n\n\n846\n40.493556\n-3.566764\n40.080111\n116.584556\n\n\n847\n44.523889\n129.568889\n40.080111\n116.584556\n\n\n848\n-37.673333\n144.843333\n31.143378\n121.805214\n\n\n849\n22.149556\n113.591558\n40.080111\n116.584556\n\n\n851\n31.428100\n104.741000\n40.080111\n116.584556\n\n\n853\n48.353783\n11.786086\n40.080111\n116.584556\n\n\n854\n48.353783\n11.786086\n31.143378\n121.805214\n\n\n855\n34.991389\n126.382778\n40.080111\n116.584556\n\n\n856\n45.630606\n8.728111\n40.080111\n116.584556\n\n\n857\n45.630606\n8.728111\n31.143378\n121.805214\n\n\n858\n47.239628\n123.918131\n40.080111\n116.584556\n\n\n859\n29.826683\n121.461906\n40.080111\n116.584556\n\n\n860\n34.858414\n136.805408\n31.143378\n121.805214\n\n\n861\n31.742042\n118.862025\n40.080111\n116.584556\n\n\n862\n31.742042\n118.862025\n34.447119\n108.751592\n\n\n863\n22.608267\n108.172442\n40.080111\n116.584556\n\n\n865\n35.764722\n140.386389\n40.080111\n116.584556\n\n\n866\n35.764722\n140.386389\n31.143378\n121.805214\n\n\n867\n32.070800\n120.976000\n40.080111\n116.584556\n\n\n868\n26.195814\n127.645869\n40.080111\n116.584556\n\n\n871\n40.080111\n116.584556\n31.143378\n121.805214\n\n\n872\n40.080111\n116.584556\n1.350189\n103.994433\n\n\n874\n40.080111\n116.584556\n48.110278\n16.569722\n\n\n875\n40.080111\n116.584556\n34.447119\n108.751592\n\n\n878\n31.143378\n121.805214\n34.447119\n108.751592\n\n\n881\n22.639258\n113.810664\n34.447119\n108.751592\n\n\n882\n36.266108\n120.374436\n34.447119\n108.751592\n\n\n886\n27.912200\n120.852000\n34.447119\n108.751592\n\n\n887\n31.494400\n120.429000\n34.447119\n108.751592\n\n\n898\n50.026421\n8.543125\n25.077731\n121.232822\n\n\n899\n34.796111\n138.189444\n25.077731\n121.232822\n\n\n902\n19.934856\n110.458961\n25.077731\n121.232822\n\n\n904\n34.436111\n132.919444\n25.077731\n121.232822\n\n\n906\n21.318681\n-157.922428\n25.077731\n121.232822\n\n\n908\n24.344525\n124.186983\n25.077731\n121.232822\n\n\n909\n22.577094\n120.350006\n40.080111\n116.584556\n\n\n911\n22.577094\n120.350006\n1.350189\n103.994433\n\n\n912\n28.865000\n115.900000\n25.077731\n121.232822\n\n\n914\n31.877222\n131.448611\n25.077731\n121.232822\n\n\n915\n31.803397\n130.719408\n25.077731\n121.232822\n\n\n919\n34.858414\n136.805408\n25.077731\n121.232822\n\n\n921\n26.195814\n127.645869\n25.077731\n121.232822\n\n\n923\n5.297139\n100.276864\n25.077731\n121.232822\n\n\n927\n16.907305\n96.133222\n25.077731\n121.232822\n\n\n928\n7.367303\n134.544278\n25.077731\n121.232822\n\n\n932\n-33.946111\n151.177222\n25.077731\n121.232822\n\n\n933\n18.302897\n109.412272\n25.077731\n121.232822\n\n\n935\n34.214167\n134.015556\n25.077731\n121.232822\n\n\n937\n36.648333\n137.187500\n25.077731\n121.232822\n\n\n938\n25.077731\n121.232822\n48.110278\n16.569722\n\n\n940\n9.071364\n-79.383453\n43.677223\n-79.630556\n\n\n946\n21.420428\n-77.847433\n43.677223\n-79.630556\n\n\n950\n7.180756\n79.884117\n1.350189\n103.994433\n\n\n958\n22.308919\n113.914603\n34.447119\n108.751592\n\n\n966\n52.308613\n4.763889\n40.080111\n116.584556\n\n\n967\n52.308613\n4.763889\n31.143378\n121.805214\n\n\n968\n23.392436\n113.298786\n51.477500\n-0.461389\n\n\n974\n28.918900\n111.640000\n40.080111\n116.584556\n\n\n976\n34.519672\n113.840889\n31.143378\n121.805214\n\n\n980\n43.541200\n125.120100\n25.077731\n121.232822\n\n\n981\n36.247500\n113.126000\n40.080111\n116.584556\n\n\n985\n28.189158\n113.219633\n31.143378\n121.805214\n\n\n990\n40.025500\n124.286600\n31.143378\n121.805214\n\n\n995\n40.094000\n94.481800\n34.447119\n108.751592\n\n\n996\n42.212444\n-83.353389\n40.080111\n116.584556\n\n\n997\n42.212444\n-83.353389\n31.143378\n121.805214\n\n\n999\n29.102800\n110.443000\n25.077731\n121.232822\n\n\n1001\n25.935064\n119.663272\n1.350189\n103.994433\n\n\n1005\n19.934856\n110.458961\n31.143378\n121.805214\n\n\n1006\n21.221192\n105.807178\n40.080111\n116.584556\n\n\n1007\n31.780019\n117.298436\n34.447119\n108.751592\n\n\n1015\n29.934200\n122.362000\n40.080111\n116.584556\n\n\n1017\n37.469075\n126.450517\n31.143378\n121.805214\n\n\n1020\n33.616653\n73.099233\n40.080111\n116.584556\n\n\n1021\n24.796400\n118.590000\n40.080111\n116.584556\n\n\n1022\n46.843394\n130.465389\n31.143378\n121.805214\n\n\n1023\n28.865000\n115.900000\n34.447119\n108.751592\n\n\n1027\n2.745578\n101.709917\n31.143378\n121.805214\n\n\n1030\n26.538522\n106.800703\n25.077731\n121.232822\n\n\n1037\n44.523889\n129.568889\n31.143378\n121.805214\n\n\n1038\n30.754000\n106.062000\n40.080111\n116.584556\n\n\n1039\n42.088056\n127.548889\n40.080111\n116.584556\n\n\n1040\n47.239628\n123.918131\n31.143378\n121.805214\n\n\n1045\n22.608267\n108.172442\n25.077731\n121.232822\n\n\n1054\n18.302897\n109.412272\n34.447119\n108.751592\n\n\n1060\n37.746897\n112.628428\n34.447119\n108.751592\n\n\n1064\n30.783758\n114.208100\n34.447119\n108.751592\n\n\n1065\n30.836100\n108.406000\n34.447119\n108.751592\n\n\n1067\n2.745578\n101.709917\n40.080111\n116.584556\n\n\n1071\n11.679431\n122.376294\n1.350189\n103.994433\n\n\n1072\n52.308613\n4.763889\n41.978603\n-87.904842\n\n\n1073\n52.308613\n4.763889\n43.677223\n-79.630556\n\n\n1077\n33.636719\n-84.428067\n47.464722\n8.549167\n\n\n1094\n37.469075\n126.450517\n41.978603\n-87.904842\n\n\n1097\n40.639751\n-73.778925\n31.143378\n121.805214\n\n\n1115\n-8.858375\n13.231178\n40.080111\n116.584556\n\n\n1116\n60.193917\n11.100361\n48.110278\n16.569722\n\n\n1121\n53.421333\n-6.270075\n48.110278\n16.569722\n\n\n1123\n53.421333\n-6.270075\n47.464722\n8.549167\n\n\n1128\n25.252778\n55.364444\n31.143378\n121.805214\n\n\n1129\n25.252778\n55.364444\n1.350189\n103.994433\n\n\n1130\n25.252778\n55.364444\n25.077731\n121.232822\n\n\n1131\n25.252778\n55.364444\n48.110278\n16.569722\n\n\n1132\n25.252778\n55.364444\n43.677223\n-79.630556\n\n\n1133\n25.252778\n55.364444\n47.464722\n8.549167\n\n\n1134\n-37.673333\n144.843333\n1.350189\n103.994433\n\n\n1136\n8.977889\n38.799319\n51.477500\n-0.461389\n\n\n1138\n8.977889\n38.799319\n31.143378\n121.805214\n\n\n1139\n-34.945000\n138.530556\n1.350189\n103.994433\n\n\n1140\n-37.008056\n174.791667\n1.350189\n103.994433\n\n\n1143\n-43.489358\n172.532225\n1.350189\n103.994433\n\n\n1146\n50.026421\n8.543125\n47.464722\n8.549167\n\n\n1157\n24.432972\n54.651138\n31.143378\n121.805214\n\n\n1160\n44.818444\n20.309139\n51.477500\n-0.461389\n\n\n1165\n43.809953\n11.205100\n47.464722\n8.549167\n\n\n1166\n46.238064\n6.108950\n47.464722\n8.549167\n\n\n1167\n51.432447\n12.241633\n47.464722\n8.549167\n\n\n1168\n45.200761\n7.649631\n47.464722\n8.549167\n\n\n1170\n42.695194\n23.406167\n48.110278\n16.569722\n\n\n1171\n42.695194\n23.406167\n47.464722\n8.549167\n\n\n1172\n13.912583\n100.606750\n1.350189\n103.994433\n\n\n1173\n13.912583\n100.606750\n34.447119\n108.751592\n\n\n1175\n8.095969\n98.988764\n1.350189\n103.994433\n\n\n1176\n31.780019\n117.298436\n25.077731\n121.232822\n\n\n1177\n38.481944\n106.009167\n25.077731\n121.232822\n\n\n1180\n63.985000\n-22.605556\n51.477500\n-0.461389\n\n\n1181\n63.985000\n-22.605556\n43.677223\n-79.630556\n\n\n1182\n63.985000\n-22.605556\n47.464722\n8.549167\n\n\n1189\n39.850000\n110.033000\n34.447119\n108.751592\n\n\n1198\n41.101400\n121.062000\n31.143378\n121.805214\n\n\n1203\n31.428100\n104.741000\n31.143378\n121.805214\n\n\n1204\n35.179528\n128.938222\n31.143378\n121.805214\n\n\n1208\n4.567972\n101.092194\n1.350189\n103.994433\n\n\n1209\n3.775389\n103.209056\n1.350189\n103.994433\n\n\n1210\n35.417000\n116.533000\n34.447119\n108.751592\n\n\n1211\n-6.900625\n107.576294\n1.350189\n103.994433\n\n\n1213\n-1.268272\n116.894478\n1.350189\n103.994433\n\n\n1214\n-6.125567\n106.655897\n40.080111\n116.584556\n\n\n1215\n-6.125567\n106.655897\n31.143378\n121.805214\n\n\n1216\n-6.125567\n106.655897\n1.350189\n103.994433\n\n\n1219\n-8.748169\n115.167172\n1.350189\n103.994433\n\n\n1221\n40.976922\n28.814606\n1.350189\n103.994433\n\n\n1222\n-7.788181\n110.431758\n1.350189\n103.994433\n\n\n1223\n-8.757322\n116.276675\n1.350189\n103.994433\n\n\n1225\n1.549447\n124.925878\n1.350189\n103.994433\n\n\n1226\n0.460786\n101.444539\n1.350189\n103.994433\n\n\n1227\n-2.898250\n104.699903\n1.350189\n103.994433\n\n\n1231\n33.511306\n126.493028\n25.077731\n121.232822\n\n\n1232\n18.766847\n98.962644\n25.077731\n121.232822\n\n\n1239\n43.041000\n144.193000\n25.077731\n121.232822\n\n\n1244\n13.410666\n103.812840\n25.077731\n121.232822\n\n\n1247\n30.582200\n117.050200\n40.080111\n116.584556\n\n\n1249\n28.918900\n111.640000\n34.447119\n108.751592\n\n\n1252\n40.060300\n113.482000\n34.447119\n108.751592\n\n\n1254\n32.900000\n115.816667\n34.447119\n108.751592\n\n\n1255\n40.851422\n111.824103\n34.447119\n108.751592\n\n\n1257\n33.777200\n119.147800\n34.447119\n108.751592\n\n\n1258\n42.841400\n93.669200\n34.447119\n108.751592\n\n\n1259\n35.799700\n107.603000\n34.447119\n108.751592\n\n\n1264\n28.852200\n105.393000\n34.447119\n108.751592\n\n\n1265\n30.754000\n106.062000\n34.447119\n108.751592\n\n\n1267\n40.926389\n107.738889\n34.447119\n108.751592\n\n\n1274\n36.898731\n30.800461\n48.110278\n16.569722\n\n\n1280\n55.408611\n37.906111\n48.110278\n16.569722\n\n\n1287\n27.178317\n33.799436\n48.110278\n16.569722\n\n\n1302\n25.557111\n34.583711\n48.110278\n16.569722\n\n\n1306\n32.011389\n34.886667\n48.110278\n16.569722\n\n\n1308\n40.560000\n109.997000\n31.143378\n121.805214\n\n\n1309\n21.539400\n109.294000\n31.143378\n121.805214\n\n\n1318\n37.271600\n118.281900\n31.143378\n121.805214\n\n\n1319\n25.935064\n119.663272\n31.143378\n121.805214\n\n\n1329\n26.195814\n127.645869\n31.143378\n121.805214\n\n\n1334\n43.352072\n77.040508\n40.080111\n116.584556\n\n\n1339\n50.901389\n4.484444\n40.080111\n116.584556\n\n\n1354\n37.271600\n118.281900\n40.080111\n116.584556\n\n\n1355\n36.636900\n109.554000\n40.080111\n116.584556\n\n\n1374\n48.528044\n135.188361\n40.080111\n116.584556\n\n\n1375\n56.180000\n92.475000\n40.080111\n116.584556\n\n\n1381\n59.800292\n30.262503\n40.080111\n116.584556\n\n\n1384\n36.117000\n103.617000\n25.077731\n121.232822\n\n\n1387\n4.191833\n73.529128\n40.080111\n116.584556\n\n\n1392\n49.566667\n117.329444\n40.080111\n116.584556\n\n\n1395\n55.012622\n82.650656\n40.080111\n116.584556\n\n\n1408\n39.794444\n106.799444\n34.447119\n108.751592\n\n\n1410\n51.956944\n4.437222\n48.110278\n16.569722\n\n\n1424\n41.297078\n2.078464\n47.464722\n8.549167\n\n\n1426\n43.301097\n-2.910608\n51.477500\n-0.461389\n\n\n1450\n43.302061\n-8.377256\n51.477500\n-0.461389\n\n\n1467\n35.416111\n51.152222\n51.477500\n-0.461389\n\n\n1468\n35.416111\n51.152222\n40.080111\n116.584556\n\n\n1469\n35.416111\n51.152222\n48.110278\n16.569722\n\n\n1471\n40.467500\n50.046667\n40.080111\n116.584556\n\n\n1472\n40.467500\n50.046667\n48.110278\n16.569722\n\n\n1473\n5.765280\n103.007000\n1.350189\n103.994433\n\n\n1474\n44.941444\n17.297501\n47.464722\n8.549167\n\n\n1476\n29.102800\n110.443000\n34.447119\n108.751592\n\n\n1478\n19.934856\n110.458961\n34.447119\n108.751592\n\n\n1484\n45.306110\n130.996670\n40.080111\n116.584556\n\n\n1490\n38.280686\n114.697300\n34.447119\n108.751592\n\n\n1495\n29.733300\n118.256000\n34.447119\n108.751592\n\n\n1500\n42.775200\n141.692283\n31.143378\n121.805214\n\n\n1501\n34.796111\n138.189444\n31.143378\n121.805214\n\n\n1504\n34.436111\n132.919444\n31.143378\n121.805214\n\n\n1508\n35.552258\n139.779694\n1.350189\n103.994433\n\n\n1509\n37.571100\n139.064600\n31.143378\n121.805214\n\n\n1513\n36.394611\n136.406544\n31.143378\n121.805214\n\n\n1514\n31.803397\n130.719408\n31.143378\n121.805214\n\n\n1515\n33.827222\n132.699722\n31.143378\n121.805214\n\n\n1518\n32.916944\n129.913611\n31.143378\n121.805214\n\n\n1525\n34.756944\n133.855278\n31.143378\n121.805214\n\n\n1527\n46.223686\n14.457611\n48.110278\n16.569722\n\n\n1528\n46.223686\n14.457611\n47.464722\n8.549167\n\n\n1536\n44.818444\n20.309139\n48.110278\n16.569722\n\n\n1537\n44.818444\n20.309139\n47.464722\n8.549167\n\n\n1539\n54.377569\n18.466222\n47.464722\n8.549167\n\n\n1540\n50.077731\n19.784836\n47.464722\n8.549167\n\n\n1541\n51.102683\n16.885836\n47.464722\n8.549167\n\n\n1542\n11.546556\n104.844139\n31.143378\n121.805214\n\n\n1550\n51.022222\n71.466944\n48.110278\n16.569722\n\n\n1551\n33.511306\n126.493028\n40.080111\n116.584556\n\n\n1559\n37.469075\n126.450517\n48.110278\n16.569722\n\n\n1560\n37.469075\n126.450517\n34.447119\n108.751592\n\n\n1561\n37.469075\n126.450517\n43.677223\n-79.630556\n\n\n1568\n52.308613\n4.763889\n1.350189\n103.994433\n\n\n1570\n52.308613\n4.763889\n48.110278\n16.569722\n\n\n1572\n52.308613\n4.763889\n47.464722\n8.549167\n\n\n1588\n35.857497\n14.477500\n47.464722\n8.549167\n\n\n1593\n19.292778\n-81.357750\n41.978603\n-87.904842\n\n\n1597\n49.626575\n6.211517\n48.110278\n16.569722\n\n\n1612\n50.026421\n8.543125\n41.978603\n-87.904842\n\n\n1615\n50.026421\n8.543125\n1.350189\n103.994433\n\n\n1619\n52.461056\n9.685078\n47.464722\n8.549167\n\n\n1625\n51.432447\n12.241633\n48.110278\n16.569722\n\n\n1628\n48.353783\n11.786086\n41.978603\n-87.904842\n\n\n1633\n48.353783\n11.786086\n47.464722\n8.549167\n\n\n1634\n-1.319167\n36.927500\n47.464722\n8.549167\n\n\n1635\n49.498700\n11.066897\n47.464722\n8.549167\n\n\n1637\n48.689878\n9.221964\n47.464722\n8.549167\n\n\n1643\n50.077731\n19.784836\n48.110278\n16.569722\n\n\n1645\n52.165750\n20.967122\n47.464722\n8.549167\n\n\n1646\n53.353744\n-2.274950\n48.110278\n16.569722\n\n\n1648\n4.191833\n73.529128\n31.143378\n121.805214\n\n\n1650\n36.674900\n-4.499106\n47.464722\n8.549167\n\n\n1652\n59.651944\n17.918611\n47.464722\n8.549167\n\n\n1653\n37.936358\n23.944467\n47.464722\n8.549167\n\n\n1657\n52.453856\n-1.748028\n47.464722\n8.549167\n\n\n1658\n13.681108\n100.747283\n47.464722\n8.549167\n\n\n1660\n42.364347\n-71.005181\n47.464722\n8.549167\n\n\n1661\n50.901389\n4.484444\n47.464722\n8.549167\n\n\n1662\n47.590000\n7.529167\n48.110278\n16.569722\n\n\n1663\n47.436933\n19.255592\n47.464722\n8.549167\n\n\n1664\n30.121944\n31.405556\n47.464722\n8.549167\n\n\n1667\n55.617917\n12.655972\n47.464722\n8.549167\n\n\n1669\n42.561353\n18.268244\n47.464722\n8.549167\n\n\n1671\n55.408611\n37.906111\n47.464722\n8.549167\n\n\n1675\n40.692500\n-74.168667\n47.464722\n8.549167\n\n\n1681\n-23.432075\n-46.469511\n47.464722\n8.549167\n\n\n1683\n46.238064\n6.108950\n48.110278\n16.569722\n\n\n1687\n22.989153\n-82.409086\n47.464722\n8.549167\n\n\n1689\n22.308919\n113.914603\n47.464722\n8.549167\n\n\n1691\n38.944533\n-77.455811\n47.464722\n8.549167\n\n\n1692\n40.976922\n28.814606\n47.464722\n8.549167\n\n\n1694\n-26.139166\n28.246000\n47.464722\n8.549167\n\n\n1695\n50.345000\n30.894722\n47.464722\n8.549167\n\n\n1696\n36.080056\n-115.152250\n47.464722\n8.549167\n\n\n1697\n33.942536\n-118.408075\n47.464722\n8.549167\n\n\n1700\n59.800292\n30.262503\n47.464722\n8.549167\n\n\n1704\n46.004275\n8.910578\n47.464722\n8.549167\n\n\n1705\n49.626575\n6.211517\n47.464722\n8.549167\n\n\n1706\n45.726387\n5.090833\n47.464722\n8.549167\n\n\n1708\n53.353744\n-2.274950\n47.464722\n8.549167\n\n\n1709\n25.793250\n-80.290556\n47.464722\n8.549167\n\n\n1712\n45.630606\n8.728111\n47.464722\n8.549167\n\n\n1714\n43.658411\n7.215872\n47.464722\n8.549167\n\n\n1715\n35.764722\n140.386389\n47.464722\n8.549167\n\n\n1718\n41.978603\n-87.904842\n47.464722\n8.549167\n\n\n1719\n60.193917\n11.100361\n47.464722\n8.549167\n\n\n1720\n44.572161\n26.102178\n47.464722\n8.549167\n\n\n1721\n40.080111\n116.584556\n47.464722\n8.549167\n\n\n1723\n50.100833\n14.260000\n47.464722\n8.549167\n\n\n1725\n18.567367\n-68.363431\n47.464722\n8.549167\n\n\n1726\n31.143378\n121.805214\n47.464722\n8.549167\n\n\n1727\n31.606886\n-8.036300\n47.464722\n8.549167\n\n\n1729\n37.618972\n-122.374889\n47.464722\n8.549167\n\n\n1730\n1.350189\n103.994433\n47.464722\n8.549167\n\n\n1732\n43.538944\n16.297964\n47.464722\n8.549167\n\n\n1735\n32.011389\n34.886667\n47.464722\n8.549167\n\n\n1736\n27.975472\n-82.533250\n47.464722\n8.549167\n\n\n1738\n45.505278\n12.351944\n47.464722\n8.549167\n\n\n1740\n39.489314\n-0.481625\n47.464722\n8.549167\n\n\n1745\n45.742931\n16.068778\n47.464722\n8.549167\n\n\n1766\n24.796400\n118.590000\n25.077731\n121.232822\n\n\n1794\n12.949986\n77.668206\n1.350189\n103.994433\n\n\n1798\n11.030031\n77.043383\n1.350189\n103.994433\n\n\n1799\n29.719217\n106.641678\n1.350189\n103.994433\n\n\n1800\n18.766847\n98.962644\n1.350189\n103.994433\n\n\n1801\n10.155556\n76.391389\n1.350189\n103.994433\n\n\n1802\n28.189158\n113.219633\n1.350189\n103.994433\n\n\n1804\n16.043917\n108.199370\n1.350189\n103.994433\n\n\n1805\n-8.546553\n125.524719\n1.350189\n103.994433\n\n\n1807\n7.125522\n125.645778\n1.350189\n103.994433\n\n\n1808\n21.221192\n105.807178\n1.350189\n103.994433\n\n\n1810\n17.453117\n78.467586\n1.350189\n103.994433\n\n\n1813\n24.992364\n102.743536\n1.350189\n103.994433\n\n\n1814\n27.696583\n85.359100\n1.350189\n103.994433\n\n\n1828\n-20.430235\n57.683600\n40.080111\n116.584556\n\n\n1829\n-20.430235\n57.683600\n31.143378\n121.805214\n\n\n1834\n30.121944\n31.405556\n48.110278\n16.569722\n\n\n1835\n30.121944\n31.405556\n43.677223\n-79.630556\n\n\n1838\n25.088200\n104.958700\n31.143378\n121.805214\n\n\n1852\n43.541200\n125.120100\n34.447119\n108.751592\n\n\n1864\n31.941667\n119.711667\n34.447119\n108.751592\n\n\n1865\n40.060300\n113.482000\n31.143378\n121.805214\n\n\n1866\n31.300000\n107.500000\n31.143378\n121.805214\n\n\n1872\n-8.748169\n115.167172\n31.143378\n121.805214\n\n\n1884\n34.633000\n98.867000\n34.447119\n108.751592\n\n\n1886\n21.221192\n105.807178\n31.143378\n121.805214\n\n\n1887\n36.524000\n114.430000\n31.143378\n121.805214\n\n\n1895\n33.777200\n119.147800\n40.080111\n116.584556\n\n\n1900\n42.841400\n93.669200\n31.143378\n121.805214\n\n\n1902\n21.318681\n-157.922428\n31.143378\n121.805214\n\n\n1906\n29.934200\n122.362000\n31.143378\n121.805214\n\n\n1914\n39.856900\n98.341400\n34.447119\n108.751592\n\n\n1915\n26.899700\n114.737500\n34.447119\n108.751592\n\n\n1916\n29.515000\n108.830000\n40.080111\n116.584556\n\n\n1917\n35.417000\n116.533000\n40.080111\n116.584556\n\n\n1921\n32.857000\n103.683000\n34.447119\n108.751592\n\n\n1922\n39.542922\n76.019956\n34.447119\n108.751592\n\n\n1948\n34.410000\n112.280000\n40.080111\n116.584556\n\n\n1949\n34.550000\n119.250000\n40.080111\n116.584556\n\n\n1950\n35.046100\n118.412000\n40.080111\n116.584556\n\n\n1951\n35.046100\n118.412000\n31.143378\n121.805214\n\n\n1952\n24.207500\n109.391000\n31.143378\n121.805214\n\n\n1953\n28.852200\n105.393000\n40.080111\n116.584556\n\n\n1954\n28.852200\n105.393000\n31.143378\n121.805214\n\n\n1960\n34.991389\n126.382778\n31.143378\n121.805214\n\n\n1962\n30.754000\n106.062000\n31.143378\n121.805214\n\n\n1965\n29.826683\n121.461906\n34.447119\n108.751592\n\n\n1969\n31.742042\n118.862025\n31.143378\n121.805214\n\n\n1972\n22.608267\n108.172442\n1.350189\n103.994433\n\n\n2010\n33.585942\n130.450686\n1.350189\n103.994433\n\n\n2018\n34.434722\n135.244167\n1.350189\n103.994433\n\n\n2023\n34.858414\n136.805408\n1.350189\n103.994433\n\n\n2029\n35.764722\n140.386389\n48.110278\n16.569722\n\n\n2036\n39.457583\n-74.577167\n41.978603\n-87.904842\n\n\n2047\n33.679750\n-78.928333\n41.978603\n-87.904842\n\n\n2048\n37.721278\n-122.220722\n41.978603\n-87.904842\n\n\n2050\n31.428100\n104.741000\n34.447119\n108.751592\n\n\n2074\n44.535444\n11.288667\n48.110278\n16.569722\n\n\n2075\n50.901389\n4.484444\n48.110278\n16.569722\n\n\n2077\n47.436933\n19.255592\n48.110278\n16.569722\n\n\n2082\n46.785167\n23.686167\n48.110278\n16.569722\n\n\n2085\n42.561353\n18.268244\n48.110278\n16.569722\n\n\n2087\n36.713056\n28.792500\n48.110278\n16.569722\n\n\n2089\n48.357222\n35.100556\n48.110278\n16.569722\n\n\n2092\n36.237611\n43.963158\n48.110278\n16.569722\n\n\n2093\n40.147275\n44.395881\n48.110278\n16.569722\n\n\n2098\n46.991067\n15.439628\n48.110278\n16.569722\n\n\n2105\n49.924786\n36.289986\n48.110278\n16.569722\n\n\n2106\n38.944533\n-77.455811\n48.110278\n16.569722\n\n\n2107\n47.178492\n27.620631\n48.110278\n16.569722\n\n\n2109\n47.260219\n11.343964\n48.110278\n16.569722\n\n\n2110\n40.976922\n28.814606\n48.110278\n16.569722\n\n\n2111\n40.639751\n-73.778925\n48.110278\n16.569722\n\n\n2113\n50.345000\n30.894722\n48.110278\n16.569722\n\n\n2114\n46.927744\n28.930978\n48.110278\n16.569722\n\n\n2115\n46.642514\n14.337739\n48.110278\n16.569722\n\n\n2117\n45.034689\n39.170539\n48.110278\n16.569722\n\n\n2118\n48.663055\n21.241112\n48.110278\n16.569722\n\n\n2120\n59.800292\n30.262503\n48.110278\n16.569722\n\n\n2123\n38.781311\n-9.135919\n48.110278\n16.569722\n\n\n2125\n48.233219\n14.187511\n48.110278\n16.569722\n\n\n2128\n49.812500\n23.956111\n48.110278\n16.569722\n\n\n2134\n40.886033\n14.290781\n48.110278\n16.569722\n\n\n2137\n46.426767\n30.676464\n48.110278\n16.569722\n\n\n2139\n41.978603\n-87.904842\n48.110278\n16.569722\n\n\n2141\n44.572161\n26.102178\n48.110278\n16.569722\n\n\n2144\n38.175958\n13.091019\n48.110278\n16.569722\n\n\n2145\n50.100833\n14.260000\n48.110278\n16.569722\n\n\n2146\n42.572778\n21.035833\n48.110278\n16.569722\n\n\n2149\n47.258208\n39.818089\n48.110278\n16.569722\n\n\n2150\n45.785597\n24.091342\n48.110278\n16.569722\n\n\n2151\n43.824583\n18.331467\n48.110278\n16.569722\n\n\n2153\n41.961622\n21.621381\n48.110278\n16.569722\n\n\n2155\n43.538944\n16.297964\n48.110278\n16.569722\n\n\n2157\n38.905394\n16.242269\n48.110278\n16.569722\n\n\n2161\n42.359392\n19.251894\n48.110278\n16.569722\n\n\n2162\n41.414742\n19.720561\n48.110278\n16.569722\n\n\n2166\n43.232072\n27.825106\n48.110278\n16.569722\n\n\n2167\n45.505278\n12.351944\n48.110278\n16.569722\n\n\n2187\n-26.139166\n28.246000\n1.350189\n103.994433\n\n\n2191\n40.128082\n32.995083\n48.110278\n16.569722\n\n\n2192\n40.898553\n29.309219\n48.110278\n16.569722\n\n\n2193\n40.898553\n29.309219\n47.464722\n8.549167\n\n\n2194\n47.485033\n9.560775\n48.110278\n16.569722\n\n\n2197\n33.616653\n73.099233\n51.477500\n-0.461389\n\n\n2199\n33.616653\n73.099233\n43.677223\n-79.630556\n\n\n2200\n24.906547\n67.160797\n51.477500\n-0.461389\n\n\n2201\n24.906547\n67.160797\n43.677223\n-79.630556\n\n\n2202\n31.521564\n74.403594\n51.477500\n-0.461389\n\n\n2203\n31.521564\n74.403594\n43.677223\n-79.630556\n\n\n2206\n11.679431\n122.376294\n25.077731\n121.232822\n\n\n2217\n-9.443383\n147.220050\n1.350189\n103.994433\n\n\n2232\n25.261125\n51.565056\n51.477500\n-0.461389\n\n\n2234\n25.261125\n51.565056\n40.080111\n116.584556\n\n\n2235\n25.261125\n51.565056\n31.143378\n121.805214\n\n\n2236\n25.261125\n51.565056\n1.350189\n103.994433\n\n\n2237\n25.261125\n51.565056\n48.110278\n16.569722\n\n\n2238\n25.261125\n51.565056\n47.464722\n8.549167\n\n\n2251\n31.722556\n35.993214\n47.464722\n8.549167\n\n\n2262\n38.781311\n-9.135919\n43.677223\n-79.630556\n\n\n2263\n41.248055\n-8.681389\n43.677223\n-79.630556\n\n\n2264\n37.741184\n-25.697870\n43.677223\n-79.630556\n\n\n2266\n52.268028\n104.388975\n40.080111\n116.584556\n\n\n2283\n27.701900\n118.001000\n34.447119\n108.751592\n\n\n2284\n49.207947\n-2.195508\n47.464722\n8.549167\n\n\n2286\n59.651944\n17.918611\n41.978603\n-87.904842\n\n\n2291\n55.617917\n12.655972\n41.978603\n-87.904842\n\n\n2293\n55.617917\n12.655972\n31.143378\n121.805214\n\n\n2294\n55.617917\n12.655972\n1.350189\n103.994433\n\n\n2300\n50.901389\n4.484444\n41.978603\n-87.904842\n\n\n2306\n23.077242\n72.634650\n1.350189\n103.994433\n\n\n2308\n41.297078\n2.078464\n1.350189\n103.994433\n\n\n2334\n55.408611\n37.906111\n1.350189\n103.994433\n\n\n2339\n41.804475\n12.250797\n1.350189\n103.994433\n\n\n2363\n4.191833\n73.529128\n1.350189\n103.994433\n\n\n2365\n48.353783\n11.786086\n1.350189\n103.994433\n\n\n2366\n45.630606\n8.728111\n1.350189\n103.994433\n\n\n2378\n24.957640\n46.698776\n1.350189\n103.994433\n\n\n2382\n53.047500\n8.786667\n48.110278\n16.569722\n\n\n2385\n55.972642\n37.414589\n48.110278\n16.569722\n\n\n2386\n55.972642\n37.414589\n43.677223\n-79.630556\n\n\n2387\n55.972642\n37.414589\n47.464722\n8.549167\n\n\n2389\n21.679564\n39.156536\n1.350189\n103.994433\n\n\n2390\n21.679564\n39.156536\n43.677223\n-79.630556\n\n\n2392\n46.914100\n7.497153\n48.110278\n16.569722\n\n\n2393\n37.986814\n58.360967\n51.477500\n-0.461389\n\n\n2408\n40.976922\n28.814606\n41.978603\n-87.904842\n\n\n2436\n6.933206\n100.392975\n1.350189\n103.994433\n\n\n2445\n26.883333\n100.233330\n1.350189\n103.994433\n\n\n2447\n22.149556\n113.591558\n1.350189\n103.994433\n\n\n2450\n29.826683\n121.461906\n1.350189\n103.994433\n\n\n2458\n37.936358\n23.944467\n43.677223\n-79.630556\n\n\n2465\n55.871944\n-4.433056\n43.677223\n-79.630556\n\n\n2467\n51.148056\n-0.190278\n43.677223\n-79.630556\n\n\n2469\n53.353744\n-2.274950\n43.677223\n-79.630556\n\n\n2474\n45.505278\n12.351944\n43.677223\n-79.630556\n\n\n2477\n33.875031\n10.775461\n47.464722\n8.549167\n\n\n2478\n36.075833\n10.438611\n47.464722\n8.549167\n\n\n2479\n36.851033\n10.227217\n48.110278\n16.569722\n\n\n2480\n36.851033\n10.227217\n47.464722\n8.549167\n\n\n2485\n31.742042\n118.862025\n1.350189\n103.994433\n\n\n2487\n-28.164444\n153.504722\n1.350189\n103.994433\n\n\n2490\n51.148056\n-0.190278\n48.110278\n16.569722\n\n\n2491\n51.148056\n-0.190278\n47.464722\n8.549167\n\n\n2492\n51.874722\n-0.368333\n47.464722\n8.549167\n\n\n2493\n40.652083\n-75.440806\n41.978603\n-87.904842\n\n\n2496\n42.748267\n-73.801692\n41.978603\n-87.904842\n\n\n2501\n44.257526\n-88.507576\n41.978603\n-87.904842\n\n\n2503\n35.436194\n-82.541806\n41.978603\n-87.904842\n\n\n2504\n41.338478\n-75.723403\n41.978603\n-87.904842\n\n\n2507\n33.562942\n-86.753550\n41.978603\n-87.904842\n\n\n2509\n43.564361\n-116.222861\n41.978603\n-87.904842\n\n\n2514\n44.471861\n-73.153278\n41.978603\n-87.904842\n\n\n2517\n45.777643\n-111.160151\n41.978603\n-87.904842\n\n\n2518\n33.938833\n-81.119528\n41.978603\n-87.904842\n\n\n2519\n40.916083\n-81.442194\n41.978603\n-87.904842\n\n\n2521\n32.898647\n-80.040528\n41.978603\n-87.904842\n\n\n2528\n47.168400\n-88.489100\n41.978603\n-87.904842\n\n\n2529\n38.805805\n-104.700778\n41.978603\n-87.904842\n\n\n2539\n46.842091\n-92.193649\n41.978603\n-87.904842\n\n\n2546\n44.865800\n-91.484300\n41.978603\n-87.904842\n\n\n2547\n42.159889\n-76.891611\n41.978603\n-87.904842\n\n\n2557\n38.950944\n-95.663611\n41.978603\n-87.904842\n\n\n2563\n-23.432075\n-46.469511\n41.978603\n-87.904842\n\n\n2564\n36.097750\n-79.937306\n41.978603\n-87.904842\n\n\n2565\n34.895556\n-82.218889\n41.978603\n-87.904842\n\n\n2568\n21.318681\n-157.922428\n41.978603\n-87.904842\n\n\n2573\n38.944533\n-77.455811\n41.978603\n-87.904842\n\n\n2584\n32.311167\n-90.075889\n41.978603\n-87.904842\n\n\n2589\n42.778700\n-84.587357\n41.978603\n-87.904842\n\n\n2603\n40.850971\n-96.759250\n41.978603\n-87.904842\n\n\n2604\n43.532913\n-84.079647\n41.978603\n-87.904842\n\n\n2610\n42.932556\n-71.435667\n41.978603\n-87.904842\n\n\n2614\n43.169500\n-86.238200\n41.978603\n-87.904842\n\n\n2616\n30.691231\n-88.242814\n41.978603\n-87.904842\n\n\n2622\n25.778489\n-100.106878\n41.978603\n-87.904842\n\n\n2625\n25.038958\n-77.466231\n41.978603\n-87.904842\n\n\n2629\n20.898650\n-156.430458\n41.978603\n-87.904842\n\n\n2644\n7.180756\n79.884117\n51.477500\n-0.461389\n\n\n2645\n7.180756\n79.884117\n40.080111\n116.584556\n\n\n2646\n7.180756\n79.884117\n31.143378\n121.805214\n\n\n2648\n6.284467\n81.124128\n40.080111\n116.584556\n\n\n2649\n6.284467\n81.124128\n31.143378\n121.805214\n\n\n2650\n23.593278\n58.284444\n47.464722\n8.549167\n\n\n2659\n55.591531\n37.261486\n43.677223\n-79.630556\n\n\n2783\n16.043917\n108.199370\n31.143378\n121.805214\n\n\n2817\n53.047500\n8.786667\n47.464722\n8.549167\n\n\n2818\n52.134642\n7.684831\n47.464722\n8.549167\n\n\n2819\n35.416111\n51.152222\n31.143378\n121.805214\n\n\n2843\n33.679750\n-78.928333\n43.677223\n-79.630556\n\n\n2851\n18.439417\n-66.001833\n43.677223\n-79.630556\n\n\n2853\n18.040953\n-63.108900\n43.677223\n-79.630556\n\n\n2860\n49.956112\n-119.377778\n43.677223\n-79.630556\n\n\n2877\n38.292392\n27.156953\n48.110278\n16.569722\n\n\n2878\n38.292392\n27.156953\n47.464722\n8.549167\n\n\n2885\n42.359392\n19.251894\n47.464722\n8.549167\n\n\n\n\n\n\n\n# Get a nice image of the globe from NASA\nearth &lt;- \"http://eoimages.gsfc.nasa.gov/images/imagerecords/73000/73909/world.topo.bathy.200412.3x5400x2700.jpg\"\n\nglobejs(\n  img = earth,\n  lat = frequent_flights$dest_lat,\n  long = frequent_flights$dest_long,\n  arcs = frequent_flights,\n  #value = frequent_destinations$n,\n  color = \"red\",\n  #bodycolor = \"#aaaaff\",\n  arcsHeight = 0.3,\n  arcsLwd = 2,\n  arcsColor = \"#ffff00\",\n  arcsOpacity = 0.35,\n  atmosphere = FALSE,\n  #color=\"#00aaff\",\n  pointsize = 2,\n  rotationlat = 0.6,\n  rotationlong = 0.2,\n  # lightcolor = \"#aaeeff\",\n  # emissive = \"#0000ee\",\n  # bodycolor = \"#ffffff\",\n  bg = \"grey\"\n        )"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/80-htmlwidgets/htmlwidgets.html#using-scatterplot3js-and-friends",
    "href": "content/courses/R4Artists/Modules/80-htmlwidgets/htmlwidgets.html#using-scatterplot3js-and-friends",
    "title": "Lab-8: Did you ever see such a thing as a drawing of a muchness?",
    "section": "Using scatterplot3js and friends",
    "text": "Using scatterplot3js and friends\n3D scatter plots with points and lines can be achieved using scatterplot3js, points3D, and lines3D.\n\nlibrary(palmerpenguins)\npenguins &lt;- penguins %&gt;% drop_na()\nscatterplot3js(x = penguins$bill_length_mm, \n               y = penguins$flipper_length_mm, \n               z = penguins$body_mass_g)\n\n\n\n\n\n\n\nlibrary(palmerpenguins)\npenguins &lt;- penguins %&gt;% drop_na()\nscatterplot3js(x = penguins$bill_length_mm, \n               y = penguins$flipper_length_mm, \n               z = penguins$body_mass_g,\n               cex.symbols = 0.2) # Smaller Points\n\n\n\n\n\n\n\nlibrary(palmerpenguins)\npenguins &lt;- penguins %&gt;% drop_na()\n\nscatterplot3js(x = penguins$bill_length_mm, \n               y = penguins$flipper_length_mm, \n               z = penguins$body_mass_g,\n               cex.symbols = 0.2) # Smaller Points"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/80-htmlwidgets/htmlwidgets.html#references",
    "href": "content/courses/R4Artists/Modules/80-htmlwidgets/htmlwidgets.html#references",
    "title": "Lab-8: Did you ever see such a thing as a drawing of a muchness?",
    "section": "References",
    "text": "References\n\nBring the best of JavaScript data visualization to R, https://www.htmlwidgets.org/\nUsing htmlwidgets in Rmarkdown, https://communicate-data-with-r.netlify.app/docs/communicate/htmlwidgets-in-documents/&gt;\nKarambelkar et al, htmlwidgets and knitr , https://cran.r-project.org/web/packages/widgetframe/vignettes/widgetframe_and_knitr.html\nhttps://patchwork.data-imaginist.com/\nThe threejs package: three.js widgets for R https://bwlewis.github.io/rthreejs/"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "",
    "text": "This Quarto document is part of my Course, R for Artists and Managers. The material is based on A Layered Grammar of Graphics by Hadley Wickham.\nThe intent of this Tutorial is to build Skill in coding in R, and also appreciate R as a way to metaphorically visualize information of various kinds, using predominantly geometric figures and structures.\nAll RMarkdown files combine code, text, web-images, and figures developed using code. Everything is text; code chunks are enclosed in fences (```)"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#iconify-noto-v1-package-setting-up-r-packages",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#iconify-noto-v1-package-setting-up-r-packages",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": " Setting up R Packages",
    "text": "Setting up R Packages\n\n## packages\nlibrary(tidyverse)   ## data science package collection (incl. the ggplot2 package)\nlibrary(systemfonts) ## use custom fonts (need to be installed on your OS)  \nlibrary(paletteer)   ## scico  and many other colour palettes palettes(http://www.fabiocrameri.ch/colourmaps.php) in R \nlibrary(ggtext)      ## add improved text rendering to ggplot2\nlibrary(ggforce)     ## add missing functionality to ggplot2\nlibrary(ggdist)      ## add uncertainty visualizations to ggplot2\nlibrary(ggformula)   ## Formula interface to ggplot\nlibrary(magick)      ## load images into R\nlibrary(patchwork)   ## combine outputs from ggplot2\nlibrary(palmerpenguins)\n\nlibrary(showtext)   ## add google fonts to plots\n\nknitr::opts_chunk$set(\n  error = TRUE,\n  comment = NA,\n  warning = FALSE,\n  errors = FALSE,\n  message = FALSE,\n  tidy = FALSE,\n  cache = FALSE,\n  echo = TRUE,\n  warning = FALSE,\n# from the vignette for the showtext package\n  fig.showtext = TRUE,\n  fig.retina = 1,\n\n  fig.width = 9,\n  fig.height = 8,\n  fig.path = \"06a-figs/\"\n)"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#using-google-fonts",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#using-google-fonts",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "Using Google Fonts",
    "text": "Using Google Fonts\nWe will want to add a few new fonts to our graphs. The best way (currently) is to use the showtext package ( which we loaded above) to bring into our work fonts from Google. To view and select the fonts you might want to work with, spend some time looking over:\n\nGoogle Webfonts Helper App\nGoogle Fonts\n\n\nsysfonts::font_add_google(\"Gochi Hand\", \"gochi\")\nfont_add_google(\"Schoolbell\", \"bell\")\nfont_add_google(\"Galada\", \"galada\")\nfont_add_google(\"Schoolbell\", \"bell\")\nfont_add_google(\"Roboto\", \"roboto\")\nfont_add_google(\"Noto Sans\", \"noto\")\nfont_add_google(\"Uchen\", \"uchen\")\nfont_add_google(\"Ibarra Real Nova\", \"ibarra\")\nfont_add_google(\"Open Sans\", \"open\")\nfont_add_google(\"Anton\", \"anton\")\nfont_add_google(\"Tangerine\", \"tangerine\")\n\n # set the google fonts as default\nshowtext::showtext_auto()\n\nWe will work with a familiar dataset, so that we can concentrate on the chart aesthetics, without having to spend time getting used to the data: the penguins dataset again, from the palmerpenguins package.\n\n\n\n\n\n\nggformula and ggplot worlds\n\n\n\nIt seems we can mix `ggformula` code with `ggtext` code, using the `+` sign!! What joy !!! Need to find out if this works for other `ggplot` extensions as well !!!"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#data",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#data",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "Data",
    "text": "Data\nAlways start your work with a table of the data:\n\npenguins &lt;- penguins %&gt;% drop_na() # remove data containing missing data\npenguins"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#basic-plot",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#basic-plot",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "Basic Plot",
    "text": "Basic Plot\nA basic scatter plot, which we will progressively dress up.\n\nUsing ggformulaUsing ggplot\n\n\n\n## simple plot: data + mappings + geometry\n## no colour or fill yet\ngf &lt;-  gf_point(bill_depth_mm ~ bill_length_mm, \n                 data = penguins, \n                 alpha = 0.6, size = 3.5)\ngf\n\n\n\n\n\n\n\n## simple plot: data + mappings + geometry\n## no colour or fill yet\ngg &lt;- ggplot(penguins, aes(x = bill_length_mm,\n                     y = bill_depth_mm)) +\n  geom_point(alpha = .6, size = 3.5)\ngg"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#customized-plot",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#customized-plot",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "Customized Plot",
    "text": "Customized Plot\nLet us set some ggplot theme aspects now!! Here is a handy picture showing (most of) the theme-able aspects of a ggplot plot.\n\n\n\nRosana Ferrero (@RosanaFerrero) on Twitter Sept 11, 2022\n\n\nFor more info, type ?theme in your console.\n\n## change global theme settings (for all following plots)\ntheme_set(theme_minimal(base_size = 12, base_family = \"open\"))\n\n## modify plot elements globally (for all following plots)\ntheme_update(\n  axis.ticks = element_line(color = \"grey92\"),\n  axis.ticks.length = unit(.5, \"lines\"),\n  panel.grid.minor = element_blank(),\n  legend.title = element_text(size = 12),\n  legend.text = element_text(color = \"grey30\"),\n  plot.title = element_text(size = 18, face = \"bold\"),\n  plot.subtitle = element_text(size = 12, color = \"grey30\"),\n  plot.caption = element_text(size = 9, margin = margin(t = 15))\n)\n\nSince we know what the basic plot looks like, let’s add titles, labels and colours. We will also set limits and scales.\n\nUsing ggformulaUsing ggplot\n\n\n\ngf1 &lt;- penguins %&gt;% \n  gf_point(bill_depth_mm ~ bill_length_mm, \n           \n           # colour by continuous variable\n           color =  ~ body_mass_g, \n           alpha = .6, size = 3.5) %&gt;% \n\n  \n  ## custom axes scaling\n  gf_refine(\n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)),\n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5), \n                     limits = c(12.5, 22.5)), \n  \n  ## custom colors from the scico package\n  ## using the paletteer super package\n  paletteer:: scale_color_paletteer_c(`\"scico::bamako\"`, direction = -1),\n  \n  ## custom labels\n  labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (Pygoscelis)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) PLoS ONE',\n    x = 'Bill Length (mm)', \n    y = 'Bill Depth (mm)',\n    color = 'Body mass (g)' #&lt;1&gt;\n  ))\n\ngf1\n\n\n\n\n\nNote this neat way of naming a scale and the legend!\n\n\n\n\ngg1 &lt;- penguins %&gt;% \n  ggplot(aes(y = bill_depth_mm, x = bill_length_mm), alpha = .6, \n         size = 3.5) +\n  geom_point(aes(colour = body_mass_g)) + \n\n  \n  ## custom axes scaling\n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) + \n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5), \n                     limits = c(12.5, 22.5)) + \n  \n  ## custom colors from the scico package\n  paletteer:: scale_color_paletteer_c(`\"scico::bamako\"`, \n                                      direction = -1) + \n  \n  ## custom labels\n  labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (Pygoscelis)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) PLoS ONE',\n    x = 'Bill Length (mm)', \n    y = 'Bill Depth (mm)',\n    color = 'Body mass (g)' \n  )\ngg1"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#using-element_markdown",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#using-element_markdown",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "Using element_markdown()",
    "text": "Using element_markdown()\n\nUsing ggformulaUsing ggplot\n\n\nWe can use our familiar markdown syntax right inside the titles and captions of the plot. element_markdown() is a theme-ing command made available by the ggtext package.\nelement_markdown() → formatted text elements, e.g. titles, caption, axis text, striptext\n\ngf2 &lt;- penguins %&gt;% gf_point(bill_depth_mm ~ bill_length_mm, \n                            color = ~ body_mass_g, \n                            alpha = 0.6, size = 3.5) %&gt;% \n gf_refine(\n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)),\n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5), \n                     limits = c(12.5, 22.5)), \n  \n  ## custom colors from the scico package\n  paletteer:: scale_color_paletteer_c(\"scico::bamako\", \n                                      direction = -1),\n  \n  ## custom labels using element_markdown()\n   labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) *PLoS ONE*',\n    x = '**Bill Length** (mm)', \n    y = '**Bill Depth** (mm)',\n    color = 'Body mass (g)')\n  ) %&gt;% \n  \n  # New code from here\n  # Enables markdown titles, captions and labels\n  gf_theme(theme(\n    plot.title = ggtext::element_markdown(),\n    plot.caption = ggtext::element_markdown(),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown()\n  ))\n\n gf2\n\n\n\n\n\n\n\ngg2 &lt;- ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5) + \n  \n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5), \n                     limits = c(12.5, 22.5)) +\n   paletteer:: scale_color_paletteer_c(`\"scico::bamako\"`, direction = -1) +\n   \n  ## New code starts here: Two Step Procedure with ggtext\n  ## 1. Markdown formatting of labels and title, using asterisks\n  labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) *PLoS ONE*',\n    x = '**Bill Length** (mm)', \n    y = '**Bill Depth** (mm)',\n    color = 'Body mass (g)'\n  ) +\n   \n  ## 2. Add theme related commands from ggtext\n  ## render respective text elements\n  theme(\n    plot.title = ggtext::element_markdown(),\n    plot.caption = ggtext::element_markdown(),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown()\n  )\ngg2"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#element_markdown-in-combination-with-html",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#element_markdown-in-combination-with-html",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "element_markdown() in combination with HTML",
    "text": "element_markdown() in combination with HTML\nThis allows us to change fonts in titles, labels, and captions.\n\nUsing ggformulaUsing ggplot\n\n\n\n## use HTML syntax to change text color\n## \ngf2 %&gt;% \n  \n  # html in labels\n  gf_labs(title = 'Bill Dimensions of Brush-Tailed Penguins\n          &lt;i style = \"color:#28A87D;\"&gt;Pygoscelis &lt;/i&gt;'\n            ) \n\n\n\n## use HTML syntax to change font and text size\ngf2 %&gt;% \n  gf_labs(title = 'Bill Dimensions of Brush-Tailed Penguins &lt;b style=\"font-size:32pt;font-family:tangerine;\"&gt;Pygoscelis&lt;/b&gt;') \n\n\n\n\n\n\n\n## use HTML syntax to change text color\ngg2 +\n  labs(title = 'Bill Dimensions of Brush-Tailed Penguins &lt;i style=\"color:#28A87D;\"&gt;Pygoscelis&lt;/i&gt;') +\n  theme(plot.margin = margin(t = 25))\n\n\n\n## use HTML syntax to change font and text size\ngg2 +\n  labs(title = 'Bill Dimensions of Brush-Tailed Penguins &lt;b style=\"font-size:32pt;font-family:tangerine;\"&gt;Pygoscelis&lt;/b&gt;')"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#adding-images-to-ggplot",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#adding-images-to-ggplot",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "Adding images to ggplot",
    "text": "Adding images to ggplot\nSave an image from the web in the same folder as your RMarkdown. Use html tags to include it, say as part of your plot title, as shown below.\n\nUsing ggformulaUsing ggplot\n\n\n\n## use HTML syntax to add images to text elements\ngf2 %&gt;% \n  gf_labs(title = 'Bill Dimensions of Brush-Tailed Penguins &nbsp;&nbsp;&nbsp; &lt;img src=\"../images/culmen_depth.png\"‚ width=\"480\"/&gt;') \n\n\n\n\n\n\n\n## use HTML syntax to add images to text elements\ngg2 + \n  labs(title = 'Bill Dimensions of Brush-Tailed Penguins &nbsp;&nbsp;&nbsp; &lt;img src=\"../images/culmen_depth.png\"‚ width=\"480\"/&gt;')"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#annotations-with-geom_richtext-and-geom_textbox",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#annotations-with-geom_richtext-and-geom_textbox",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "Annotations with geom_richtext() and geom_textbox()",
    "text": "Annotations with geom_richtext() and geom_textbox()\nFurther ggplot annotations can be achieved using geom_richtext() and geom_textbox(). geom_richtext() also allows formatted text labels with 360° rotation. One needs to pass a dataframe to geom_richtext() giving the location, colour, rotation etc of the label annotation.\n\nUsing ggformulaUsing ggplot\n\n\n\n# Create a label tibble\n# Three rich text labels, \n# so three sets of locations x and y, and angle of rotation\nlabels &lt;- tibble(\n      x = c(34, 56, 54), \n      y = c(20, 18.5, 14.5),\n      angle = c(12, 20, 335),\n      species = c(\"Adelie\", \"Chinstrap\", \"Gentoo\"),\n      \n      lab = c(\"&lt;b style='font-family:anton;font-size:24pt;'&gt;Adélie&lt;/b&gt;&lt;br&gt;&lt;i style='color:darkgrey;'&gt;P. adéliae&lt;/i&gt;\", \n              \"&lt;b style='font-family:anton;font-size:24pt;'&gt;Chinstrap&lt;/b&gt;&lt;br&gt;&lt;i style='color:darkgrey;'&gt;P. antarctica&lt;/i&gt;\", \n              \"&lt;b style='font-family:anton;font-size:24pt;'&gt;Gentoo&lt;/b&gt;&lt;br&gt;&lt;i style='color:darkgrey;'&gt;P. papua&lt;/i&gt;\"))\nlabels\n\ngf_rich &lt;- penguins %&gt;% \n  gf_point(bill_depth_mm ~ bill_length_mm, \n                      color = ~ species, \n                      alpha = 0.6, size = 3.5, data = penguins) + # &lt;1&gt;\n  \n  \n    ## add text annotations for each species\n    ggtext::geom_richtext(\n    data = labels,\n    # Now pass the data variables as aesthetics\n    aes(x, y, label = lab, color = species, angle = angle), \n    size = 4, fill = NA, label.color = NA,\n    lineheight = .3) +\n    # show.legend = FALSE else we get some unusual legends!\n    # fill = NA makes the labels' fill transparent\n  \n  \n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5), \n                     limits = c(12.5, 22.5)) +\n  scale_colour_paletteer_d(palette = `\"rcartocolor::Bold\"`, guide = \"none\") +\n  \n  labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) *PLoS ONE*',\n    x = '**Bill Length** (mm)', \n    y = '**Bill Depth** (mm)',\n    color = 'Body mass (g)'\n  ) +\n  \n  # Use theme and element_markdown() to format axes and titles as usual\n  theme(\n    plot.title = ggtext::element_markdown(),\n    plot.caption = ggtext::element_markdown(),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown(),\n    plot.margin = margin(25, 6, 15, 6)\n  )\n\n\ngf_rich\n\n\n\n\n\n\n  \n\n\n\n\nNote the plus sign usage here!!We are combining the ggformula and ggplot syntax, and it works!\n\n\n\n\ngg_rich &lt;- ggplot(penguins, aes(x = bill_length_mm, \n                                y = bill_depth_mm)) +\n  geom_point(aes(color = species), alpha = .6, size = 3.5) + \n  \n  ## add text annotations for each species\n  ggtext::geom_richtext(\n    data = labels,\n    # Now pass the data variables as aesthetics\n    aes(x, y, label = lab, color = species, angle = angle), \n    size = 4, fill = NA, label.color = NA,\n    lineheight = .3) +\n  \n  \n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5), \n                     limits = c(12.5, 22.5)) +\n  scale_colour_paletteer_d(`\"rcartocolor::Bold\"`, guide = \"none\")+\n  \n  labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) *PLoS ONE*',\n    x = '**Bill Length** (mm)', \n    y = '**Bill Depth** (mm)',\n    color = 'Body mass (g)'\n  ) +\n  \n  # Use theme and element_markdown() to format axes and titles as usual\n  theme(\n    plot.title = ggtext::element_markdown(),\n    plot.caption = ggtext::element_markdown(),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown(),\n    plot.margin = margin(25, 6, 15, 6)\n  )\n  \ngg_rich"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#formatted-text-boxes-on-plots",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#formatted-text-boxes-on-plots",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "Formatted Text boxes on plots",
    "text": "Formatted Text boxes on plots\nelement_textbox() and element_textbox_simple() → formatted text boxes with word wrapping.\n\nUsing ggformulaUsing ggplot\n\n\n\ngf_box &lt;- penguins %&gt;% \n  gf_point(bill_depth_mm ~ bill_length_mm, \n                      color = ~ species, \n                      alpha = 0.6, size = 3.5, data = penguins) + # &lt;1&gt;\n  \n  \n    ## add text annotations for each species\n    ggtext::geom_richtext(\n    data = labels,\n    # Now pass the data variables as aesthetics\n    aes(x, y, label = lab, color = species, angle = angle), \n    size = 4, fill = NA, label.color = NA,\n    lineheight = .3) +\n    # show.legend = FALSE else we get some unusual legends!\n    # fill = NA makes the labels' fill transparent\n  \n  \n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5), \n                     limits = c(12.5, 22.5)) +\n  scale_colour_paletteer_d(palette = `\"rcartocolor::Bold\"`, guide = \"none\") +\n  \n  \n# Now for the Plot Titles and Labels, as before  \n  labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) *PLoS ONE*',\n    x = '**Bill Length** (mm)',\n    y = '**Bill Depth** (mm)',\n    color = 'Body mass (g)'\n  ) +\n  \n# Add the ggtext theme related commands\n  theme(\n    ## turn title into filled textbox\n    plot.title = ggtext::element_textbox_simple(\n      color = \"white\",\n      fill = \"#28A78D\",\n      size = 32,\n      padding = margin(8, 4, 8, 4),\n      margin = margin(b = 5),\n      lineheight = .9\n    ),\n    plot.subtitle = ggtext::element_textbox_simple(\n      size = 10,\n      padding = margin(5.5, 5.5, 5.5, 5.5),\n      margin = margin(0, 0, 5.5, 0),\n      fill = \"orange1\"\n    ),\n    \n    ## add round outline to caption\n    plot.caption = ggtext::element_textbox_simple(\n      width = NULL,\n      linetype = 1,\n      fill = \"grey\",\n      padding = margin(4, 8, 4, 8),\n      margin = margin(t = 15),\n      r = grid::unit(8, \"pt\")\n    ),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown(),\n    plot.margin = margin(25, 6, 15, 6)\n  )\n\ngf_box\n\n\n\n\n\nNote again the use of the plus sign with ggformula. This works!\n\n\n\n\ngg_box &lt;- ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(aes(color = species), alpha = .6, size = 3.5) +\n  \n     ## add text annotations for each species\n    ggtext::geom_richtext(\n    data = labels,\n    # Now pass the data variables as aesthetics\n    aes(x, y, label = lab, color = species, angle = angle), \n    size = 4, fill = NA, label.color = NA,\n    lineheight = .3) +\n    # show.legend = FALSE else we get some unusual legends!\n    # fill = NA makes the labels' fill transparent\n  \n  \n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5), \n                     limits = c(12.5, 22.5)) +\n  scale_colour_paletteer_d(palette = `\"rcartocolor::Bold\"`, guide = \"none\") +\n  \n  \n# Now for the Plot Titles and Labels, as before  \n  labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) *PLoS ONE*',\n    x = '**Bill Length** (mm)',\n    y = '**Bill Depth** (mm)',\n    color = 'Body mass (g)'\n  ) +\n  \n# Add the ggtext theme related commands\n  theme(\n    ## turn title into filled textbox\n    plot.title = ggtext::element_textbox_simple(\n      color = \"white\",\n      fill = \"#28A78D\",\n      size = 32,\n      padding = margin(8, 4, 8, 4),\n      margin = margin(b = 5),\n      lineheight = .9\n    ),\n    plot.subtitle = ggtext::element_textbox_simple(\n      size = 10,\n      padding = margin(5.5, 5.5, 5.5, 5.5),\n      margin = margin(0, 0, 5.5, 0),\n      fill = \"orange1\"\n    ),\n    \n    ## add round outline to caption\n    plot.caption = ggtext::element_textbox_simple(\n      width = NULL,\n      linetype = 1,\n      fill = \"grey\",\n      padding = margin(4, 8, 4, 8),\n      margin = margin(t = 15),\n      r = grid::unit(8, \"pt\")\n    ),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown(),\n    plot.margin = margin(25, 6, 15, 6)\n  )\n\ngg_box"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#using-geom_texbox-for-formatted-text-boxes-with-word-wrapping",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#using-geom_texbox-for-formatted-text-boxes-with-word-wrapping",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "Using geom_texbox() for formatted text boxes with word wrapping",
    "text": "Using geom_texbox() for formatted text boxes with word wrapping\n\nUsing ggformulaUsing ggplot\n\n\n\ntext_box &lt;- tibble(x = 34, y = 13.7, label = \"&lt;span style='font-size:12pt;font-family:anton;'&gt;Lorem Ipsum Dolor Sit Amet&lt;/span&gt;&lt;br&gt;&lt;br&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\")\n\n\ngf_box +\n  ## add textbox with long paragraphs\n  ggtext::geom_textbox(\n    data = text_box,\n    aes(x, y, \n        label = label),\n    size = 2.2, family = \"sans\",\n    fill = \"cornsilk\", \n    \n    colour = \"black\",\n# This is ESSENTIAL !!!\n# It appears that the original colour aesthetic mapping in `gf_box` and a possible colour aesthetic with `geom_textbox` have a clash, *only* with ggformula. No such issues below with the ggplot.\n# So declaring a colour here is essential\n\n    box.color = \"cornsilk3\",\n    #box.padding = c(2,2,2,2),\n    width = unit(11, \"lines\")\n  ) +\n  coord_cartesian(clip = \"off\") # ensure no clipping of labels near the edge\n\n\n\n\n\n\n\ngg_box +\n  ## add textbox with long paragraphs\n  ggtext::geom_textbox(\n    data = text_box,\n    aes(x, y, label = label),\n    size = 2.2, family = \"sans\",\n    fill = \"cornsilk\", box.color = \"cornsilk3\",\n    width = unit(11, \"lines\")\n  ) +\n  coord_cartesian(clip = \"off\") # ensure no clipping of labels near the edge"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#using-ggforce",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#using-ggforce",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "Using {ggforce}",
    "text": "Using {ggforce}\nFrom Thomas Lin Pedersen’s website → www.ggforce.data-imaginist.com\n\nggforce is a package aimed at providing missing functionality to ggplot2 through the extension system introduced with ggplot2 v2.0.0. Broadly speaking ggplot2 has been aimed primarily at explorative data visualization in order to investigate the data at hand, and less at providing utilities for composing custom plots a la D3.js. ggforce is mainly an attempt to address these “shortcoming” (design choices might be a better description). The goal is to provide a repository of geoms, stats, etc. that are as well documented and implemented as the official ones found in ggplot2.\n\nWe will start with the basic plot, with the ggtext related work done up to now:\n\n## use ggtext rendering for the following plots\ntheme_update(\n  plot.title = ggtext::element_markdown(),\n  plot.caption = ggtext::element_markdown(),\n  axis.title.x = ggtext::element_markdown(),\n  axis.title.y = ggtext::element_markdown()\n)\n\n\nUsing ggformula and ggforceUsing ggplot and ggforce\n\n\n\n## plot that we will annotate with ggforce afterwards\ngf3 &lt;- penguins %&gt;% \n  gf_point(bill_depth_mm ~ bill_length_mm,\n           color = ~ body_mass_g,\n           alpha = .6, \n           size = 3.5) + \n  \n  coord_cartesian(xlim = c(25, 65), ylim = c(10, 25)) +\n  \n  # Add Colour scales\n  scale_color_paletteer_c(`\"grDevices::Lajolla\"`, direction = -1) +\n  \n  # Add labels\n  labs(\n    title = \"Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)\",\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE*\",\n    x = \"**Bill Length** (mm)\",\n    y = \"**Bill Depth** (mm)\",\n    color = \"Body mass (g)\",\n    fill = \"Species\"\n  )\n\n\n\n## ellipsoids for all groups\ngf3 +\n  ggforce::geom_mark_ellipse(\n    aes(fill = species, \n        label = species), \n    \n    color = \"black\", \n    # This is good to include\n    # Else ellipses get coloured too\n    \n    alpha = .15, \n    show.legend = FALSE\n  ) \n\n\n\n\n\n\n\n## plot that we will annotate with ggforce afterwards\ngg3 &lt;- ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(aes(color = body_mass_g), \n             alpha = .6, \n             size = 3.5) + \n\n  coord_cartesian(xlim = c(25, 65), ylim = c(10, 25)) +\n  \n  # Add Colour scales\n  scale_color_paletteer_c(`\"grDevices::Lajolla\"`, direction = -1) +\n  # rcartocolor::scale_fill_carto_d(palette = \"Bold\") +\n  \n  # Add labels\n  labs(\n    title = \"Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)\",\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE*\",\n    x = \"**Bill Length** (mm)\",\n    y = \"**Bill Depth** (mm)\",\n    color = \"Body mass (g)\",\n    fill = \"Species\"\n  )\n\n\n## ellipsoids for all groups\ngg3 +\n  ggforce::geom_mark_ellipse(\n    aes(fill = species, \n        label = species), \n    alpha = .15, \n    show.legend = FALSE\n  ) \n\n\n\n\n\n## ellipsoids for specific subset\ngg3 +\n  ggforce::geom_mark_ellipse(\n    aes(fill = species, label = species, filter = species == 'Gentoo'), \n    alpha = 0, show.legend = FALSE\n  ) +\n geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5)\n\n\n\n\n\n## circles\ngg3 +\n  ggforce::geom_mark_circle(\n    aes(fill = species, label = species, filter = species == 'Gentoo'), \n    alpha = 0, show.legend = FALSE\n  ) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5)\n\n\n\n\n\n## rectangles\ngg3 +\n  ggforce::geom_mark_rect(\n    aes(fill = species, label = species, filter = species == 'Gentoo'), \n    alpha = 0, show.legend = FALSE\n  ) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5) \n\n\n\n\n\nlibrary(concaveman)\n## hull\ngg3 +\n  ggforce::geom_mark_hull(\n    aes(fill = species, label = species, filter = species == 'Gentoo'), \n    alpha = 0, show.legend = FALSE\n  ) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5)"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#ggplot-tricks",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#ggplot-tricks",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "ggplot tricks",
    "text": "ggplot tricks\n\ngg0 &lt;- \n  ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n    ggforce::geom_mark_ellipse(\n    aes(fill = species, \n        label = species), \n    alpha = .15, \n    show.legend = FALSE\n  ) +\n    geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5) + \n    scale_x_continuous(breaks = seq(25, 65, by = 5), limits = c(25, 65)) +\n    scale_y_continuous(breaks = seq(12, 24, by = 2), limits = c(12, 24)) +\n    scico::scale_color_scico(palette = \"bamako\", direction = -1) +\n    labs(\n      title = \"Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)\",\n      subtitle = 'A scatter plot of bill depth versus bill length.',\n      caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE*\",\n      x = \"Bill Length (mm)\", \n      y = \"Bill Depth (mm)\",\n      color = \"Body mass (g)\"\n    )\ngg0\n\n\n\n\n\nLeft-Aligned Title\n\n(gg1 &lt;- gg0 + theme(plot.title.position = \"plot\"))\n\n\n\n\n\n\nRight-Aligned Caption\n\ngg1b &lt;- gg1 +  theme(plot.caption.position = \"panel\")\ngg1b\n\n\n\n\n\n\nLegend Design\n\ngg1b + theme(legend.position = \"top\")\n\n\n\n#ggsave(\"06a_legend_position.pdf\", width = 9, height = 8, device = cairo_pdf)\n\ngg1b + \n  theme(legend.position = \"top\") +\n  guides(color = guide_colorbar(title.position = \"top\", \n                                title.hjust = .5, \n                                barwidth = unit(20, \"lines\"), \n                                barheight = unit(.5, \"lines\")))\n\n\n\n\n\n\nAdd Images\n\n## read PNG file from web\npng &lt;- magick::image_read(\"../images/culmen_depth.png\")\n## turn image into `rasterGrob`\nimg &lt;- grid::rasterGrob(png, interpolate = TRUE)\n\ngg5 &lt;- gg2 + \n  annotation_custom(img, ymin = 18, ymax = 28, xmin = 58, xmax = 65) +\n    labs(caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE* &bull; Illustration: Allison Horst\")\ngg5"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#using-patchwork",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#using-patchwork",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "Using {patchwork}",
    "text": "Using {patchwork}\n\nThe goal of patchwork is to make it ridiculously simple to combine separate ggplots into the same graphic. As such it tries to solve the same problem as gridExtra::grid.arrange() and cowplot::plot_grid but using an API that incites exploration and iteration, and scales to arbitrily complex layouts.\n\n→ https://patchwork.data-imaginist.com/\nLet us make two plots and combine them into a single patchwork plot.\n\n## calculate bill ratio\npenguins_stats &lt;- penguins %&gt;% \n  mutate(bill_ratio = bill_length_mm / bill_depth_mm) %&gt;% \n  filter(!is.na(bill_ratio))\n\n## create a second chart\ngg6 &lt;- ggplot(penguins_stats, aes(y = bill_ratio, x = species, fill = species, color = species)) + geom_violin() + \n  labs(\n    y = \"Bill ratio\",\n    x = \"Species\",\n    subtitle = \"\",\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE* &bull; Illustration: Allison Horst\"\n  ) +\n  theme(\n    panel.grid.major.x = element_line(size = .35),\n    panel.grid.major.y = element_blank(),\n    axis.text.y = element_text(size = 13),\n    axis.ticks.length = unit(0, \"lines\"),\n    plot.title.position = 'plot',\n    plot.subtitle = element_text(margin = margin(t = 5, b = 10)),\n    plot.margin = margin(10, 25, 10, 25)\n  )\n\nNow to combine both plots into one using simple operators:\n\nFor the special case of putting plots besides each other or on top of each other patchwork provides 2 shortcut operators. | will place plots next to each other while / will place them on top of each other.\n\nFirst we stack up the graphs side by side:\n\n## combine both plots\ngg5 | (gg6 + labs(title = \"Bill Ratios of Brush-Tailed Penguins\",\n                  subtitle = \"Violin Plots of Bill Ration versus species\"))\n\n\n\n\nWe can place them in one column:\n\ngg5 / (gg6 + labs(title = \"Bill Ratios of Brush-Tailed Penguins\",\n                  subtitle = \"Violin Plots of Bill Ration versus species\")) +\n  plot_layout(heights = c(0.4, 0.4))"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/50-working-in-tidyverse/index.html",
    "href": "content/courses/R4Artists/Modules/50-working-in-tidyverse/index.html",
    "title": "Lab-5: Twas brillig, and the slithy toves…",
    "section": "",
    "text": "R Tutorial\n  Slides\n dplyr Tutorial"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/50-working-in-tidyverse/index.html#fa-folder-open-slides-and-tutorials",
    "href": "content/courses/R4Artists/Modules/50-working-in-tidyverse/index.html#fa-folder-open-slides-and-tutorials",
    "title": "Lab-5: Twas brillig, and the slithy toves…",
    "section": "",
    "text": "R Tutorial\n  Slides\n dplyr Tutorial"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/50-working-in-tidyverse/index.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "href": "content/courses/R4Artists/Modules/50-working-in-tidyverse/index.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "title": "Lab-5: Twas brillig, and the slithy toves…",
    "section": " Introduction",
    "text": "Introduction\nWe meet the most important idea in R: tidy data. Once data is tidy, there is a great deal of insight to be obtained from it, by way of tables, graphs and explorations!\nWe will get hands on with dplyr, the R-package that belongs in the tidyverse and is a terrific toolbox to clean, transform, reorder, and summarize your data. And we will be ready to ask Questions of our data and embark on analyzing it."
  },
  {
    "objectID": "content/courses/R4Artists/Modules/50-working-in-tidyverse/index.html#readings",
    "href": "content/courses/R4Artists/Modules/50-working-in-tidyverse/index.html#readings",
    "title": "Lab-5: Twas brillig, and the slithy toves…",
    "section": "Readings",
    "text": "Readings\n\nR4DS dplyr chapter\nModernDive dplyr chapter\nRStudio dplyr Cheatsheet"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/300-website/index.html#introduction",
    "href": "content/courses/R4Artists/Modules/300-website/index.html#introduction",
    "title": "Lab-14: You’re are Nothing but a Pack of Cards!!",
    "section": "Introduction",
    "text": "Introduction\nLet’s make a website in RStudio to show off our data viz portfolio, and to share with friends, parents, prospective employers…\nWe will encounter a new package called blogdown and use workflows with github and a free web hosting service called Netlify to create a website where all our RMarkdowns become individual blog posts, complete with Titles, Sections, Text, Diagrams and Links!\nNOTE: Need to update this to Quarto!!!"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/300-website/index.html#references",
    "href": "content/courses/R4Artists/Modules/300-website/index.html#references",
    "title": "Lab-14: You’re are Nothing but a Pack of Cards!!",
    "section": "References",
    "text": "References\n\nAllison Hill\nSharon Macliss\nYihui Xie"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/200-wrap/index.html#introduction",
    "href": "content/courses/R4Artists/Modules/200-wrap/index.html#introduction",
    "title": "Lab-13: Old Tortoise Taught Us",
    "section": "Introduction",
    "text": "Introduction\nWe will spend a little time wrapping up everything that we have learnt so far, in R.\nWe will take one large dataset that has numerical, spatial and network type data, and see how we can make multiple visual depictions of it. We will use most of the packages that we have encountered so far and try to make a polished info visualization with all the graphs, text, and descriptions put together!"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/200-wrap/index.html#references",
    "href": "content/courses/R4Artists/Modules/200-wrap/index.html#references",
    "title": "Lab-13: Old Tortoise Taught Us",
    "section": "References",
    "text": "References\n\nAlex Cookson, Great Data Sets github repo, https://github.com/tacookson/data"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/130-purrr/purrr.html",
    "href": "content/courses/R4Artists/Modules/130-purrr/purrr.html",
    "title": "Iteration: Learning to purrr",
    "section": "",
    "text": "knitr::opts_chunk$set(message = FALSE)\nlibrary(tidyverse)\nlibrary(gapminder)\nlibrary(ggformula)\n\n\n\nLearning to purrr"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/130-purrr/purrr.html#iconify-noto-v1-package-setting-up-r-packages",
    "href": "content/courses/R4Artists/Modules/130-purrr/purrr.html#iconify-noto-v1-package-setting-up-r-packages",
    "title": "Iteration: Learning to purrr",
    "section": "",
    "text": "knitr::opts_chunk$set(message = FALSE)\nlibrary(tidyverse)\nlibrary(gapminder)\nlibrary(ggformula)\n\n\n\nLearning to purrr"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/130-purrr/purrr.html#iconify-fxemoji-japanesesymbolforbeginner-introduction",
    "href": "content/courses/R4Artists/Modules/130-purrr/purrr.html#iconify-fxemoji-japanesesymbolforbeginner-introduction",
    "title": "Iteration: Learning to purrr",
    "section": "\n Introduction",
    "text": "Introduction\nOften we want to perform the same operation on several different sets of data. Rather than repeat the operation for each instance of data, it is faster, more intuitive, and less error-prone if we create a data structure that holds all the data, and use the map-* series functions from the purrr package to perform all the repeated operations in one shot.\nThis requires getting used to. We need to understand:\n\nthe data structure\nthe iteration mechanism using map functions\nthe form of the results"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/130-purrr/purrr.html#iconify-pajamas-issue-type-test-case-case-study-1-multiple-models-for-life-expectancy-with-gapminder",
    "href": "content/courses/R4Artists/Modules/130-purrr/purrr.html#iconify-pajamas-issue-type-test-case-case-study-1-multiple-models-for-life-expectancy-with-gapminder",
    "title": "Iteration: Learning to purrr",
    "section": "\n Case Study #1: Multiple Models for Life Expectancy with gapminder\n",
    "text": "Case Study #1: Multiple Models for Life Expectancy with gapminder\n\nWe will start with a complete case study and then work backwards to understand the various pieces of code that make it up.\nLet us look at the gapminder dataset:\n\nskimr::skim(gapminder)\n\n\nData summary\n\n\nName\ngapminder\n\n\nNumber of rows\n1704\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\ncountry\n0\n1\nFALSE\n142\nAfg: 12, Alb: 12, Alg: 12, Ang: 12\n\n\ncontinent\n0\n1\nFALSE\n5\nAfr: 624, Asi: 396, Eur: 360, Ame: 300\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nyear\n0\n1\n1979.50\n17.27\n1952.00\n1965.75\n1979.50\n1993.25\n2007.0\n▇▅▅▅▇\n\n\nlifeExp\n0\n1\n59.47\n12.92\n23.60\n48.20\n60.71\n70.85\n82.6\n▁▆▇▇▇\n\n\npop\n0\n1\n29601212.32\n106157896.74\n60011.00\n2793664.00\n7023595.50\n19585221.75\n1318683096.0\n▇▁▁▁▁\n\n\ngdpPercap\n0\n1\n7215.33\n9857.45\n241.17\n1202.06\n3531.85\n9325.46\n113523.1\n▇▁▁▁▁\n\n\n\n\n\nWe have lifeExp, gdpPerCap, and pop as Quant variables over time (year) for each country in the world. Suppose now that we wish to create Linear Regression Models predicting lifeExp using year, for each country. ( We will leave out gdpPercap and pop for now) The straightforward by laborious and naive way would be to use the lm command after filtering the dataset for each country, creating 140+ Linear Models manually! This would be horribly tedious!\nThere is a better way with purrr, and also more recently, with dplyr itself. Let us see both methods, the established purrr method first, and the new dplyr based method thereafter.\n\n EDA Plots\nWe can first plot lifeExp over year, grouped by country:\n\nggplot(gapminder,aes(x = year, y = lifeExp, colour = country)) + \n  geom_line(show.legend = FALSE) + \n  theme_classic()\nggplot(gapminder,aes(x = year, y = lifeExp, colour = country)) + \n  geom_line(show.legend = FALSE) + \n  facet_wrap(~ continent) + \n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nBy and large we see positive slopes, but some countries do show non-linear behaviour.\nConstructing a Linear Model\nLet us take \\(1950\\) as a baseline year for all countries. Then we model lifeExp using year1950 across all countries together:\n\ngapminder &lt;- gapminder %&gt;% mutate(year1950 = year - 1950) # baseline year\nmodel &lt;- lm(lifeExp ~ year1950, data = gapminder)\nsummary(model)\n\n\nCall:\nlm(formula = lifeExp ~ year1950, data = gapminder)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-39.949  -9.651   1.697  10.335  22.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 49.86028    0.55792   89.37   &lt;2e-16 ***\nyear1950     0.32590    0.01632   19.96   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.63 on 1702 degrees of freedom\nMultiple R-squared:  0.1898,    Adjusted R-squared:  0.1893 \nF-statistic: 398.6 on 1 and 1702 DF,  p-value: &lt; 2.2e-16\n\n\n\nmodel %&gt;% broom::tidy() # Parameters of the Model\nmodel %&gt;% broom::glance() # Statistics of the Model\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n\n\n\n\nSince the slope 0.3259038 is positive, life expectancy has been increasing over the years, across all countries. (But r.squared 0.1897571 is low, so this model does not explain much).\nHow do we do this for each country? We need to use the split-apply-combine method to achieve this. The combination of group_by and summarise is a example of the split &gt; apply &gt; combine method. For example, we could (split) the data by country, calculate the linear model each group (apply), and (combine) the results in a data frame.\nHowever, this first-attempt code for a per-country linear model does not work:\n\n```{r}\n#| eval: false\ngapminder %&gt;% \n  group_by(country) %&gt;% \n  summarise(linmod = lm(lifeExp ~ year1950, data = .))\n```\n\nThis is because the linmod variable is a list variable and cannot be accommodated in a simple column, which is what summarize will try to create. So we need to be able to create “list” columns in a data frame…how do we do that? Before we contemplate that, let us understand the capabilities of the purrr package in R.\nThe purrr package\nThe purrr package contains a new class of functions, that can take vectors/tibbles/lists as input, and perform an identical function over each component of these, and generate vectors/tibbles/lists as output. These are the map_* functions that are part of the purrr package. The * in the map_* function defines what kind of output (vector/tibble/list) the function generates.\nLet us look at a few short examples.\nUsing map_* functions from purrr\n\nThe basic structure of the map_* functions is:\n\n```{r}\n#| eval: false\nmap_typeOfResult(.x = what_to_iterate_with, \n                 .f = function_to_apply)\n\nmap_typeOfResult(.x = what_to_iterate_with, \n                 .f = \\(x) function_to_apply(x, additional_parameters))\n```\n\nTwo examples:\n\n# Example 1: Input: vector, Output: vector\ndiamonds %&gt;% \n  select(where(is.numeric)) %&gt;% \n  \n  # We need dbl-type numbers in output **vector**\nmap_dbl(.x = ., \n        .f = mean)\n\n       carat        depth        table        price            x            y \n   0.7979397   61.7494049   57.4571839 3932.7997219    5.7311572    5.7345260 \n           z \n   3.5387338 \n\n# Example 2: Input: vector, Output: tibble\ndiamonds %&gt;% \n  select(where(is.numeric)) %&gt;% \n  \n  # We need dbl-type numbers in output **vector**\nmap_df(.x = ., \n       .f = mean)\n\n\n\n  \n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote map_dbl outputs a (numeric) vector, and map_df outputs a tibble.\nIn each of the above examples, each vector in the diamonds dataset was passed to the respective map_* function as the parameter.x.\n\n\nSometimes the function .f may need some additional parameters to be specified, and these may not come from the input .x:\n\n# Example 3, with additional parameters to .f\npalmerpenguins::penguins %&gt;% \n  select(where(is.numeric)) %&gt;% \n  \n  map_dbl(.x = ., \n          .f = \\(x) mean(x, na.rm = TRUE))\n\n   bill_length_mm     bill_depth_mm flipper_length_mm       body_mass_g \n         43.92193          17.15117         200.91520        4201.75439 \n             year \n       2008.02907 \n\n  # penguins has two rows of NA entries which need to be dropped\n  # Hence this additional parameter for the `mean` function\n\n\n# Example 4: if we want a tibble output\npalmerpenguins::penguins %&gt;% \n  select(where(is.numeric)) %&gt;% \n  \n  map_df(.x = ., \n         .f = \\(x) mean(x, na.rm = TRUE))\n\n\n\n  \n\n\n\nThe .f function can be anything, even a ggformula plot command; in this case the output will not be a vector or a tibble, but a list:\n\n#library(ggformula)\npalmerpenguins::penguins %&gt;% \n  select(where(is.numeric)) %&gt;% select(-year) %&gt;% drop_na() %&gt;% \n  \n  # `map` gives a list output\n  map(.x = .,\n      .f = \\(x) gf_histogram(~x, bins = 30) %&gt;% \n        gf_theme(theme_classic())\n  )\n\n\n\n$bill_length_mm\n\n\n\n\n\n\n\n\n$bill_depth_mm\n\n\n\n\n\n\n\n\n$flipper_length_mm\n\n\n\n\n\n\n\n\n$body_mass_g\n\n\n\n\n\n\n\nNote: we need to do just a bit of extra pre-work to get the variable names on the x-axis of the histograms. There is a possibility to store all the plots in a separate column\nOK, so we can get vectors/tibbles/lists as output using vectors as inputs. Why would it be desirable to provide tibble/list as an input to a map_* function?\n\n Using purrr to create multiple models\nNow that we have some handle on purrr’s map functions, we can see how to develop a linear regression model for every country in the gapminder dataset. It should be clear from the command for a linear model:\n\n```{r}\n#| eval: false\nmodel &lt;- lm(target ~ predictor(s), \n            data  = tibble_containing_target_and_predictors_columns)\n```\n\nthat we need to specify three things: target, predictors, and the data tibble for the development of a linear model. To do this for each country in gapminder, here is the process:\n\nGroup the gapminder data by country (and continent)\nCreate a column containing unique per-country data for each country. This column would hence contain a tibble in each cell. This is a list column!\nUse map which would take country and the data columns created above to create an lm object for each country (in another list column)\nUse map again with broom::tidy as the function to give us clean columns for the model per country.\nUse that multi-model tibble to plot graphs for each country.\n\nLet us do this now!\n\ngapminder_models &lt;- gapminder %&gt;% \n  group_by(continent, country) %&gt;% \n  \n  # Create a per-country tibble in a new column called \"data_list\"\n  nest(.key = \"data_list\") \ngapminder_models\n\n\n\n  \n\n\ngapminder_models &lt;- gapminder_models %&gt;%\n  # We use mutate + map to add a list column containing linear models\n  mutate(model = map(.x = data_list, \n                     \n          # One column .x to iterate over\n          # The .x list column contains data frames\n          # So we access individual columns for target and predictors \n          # within these individual data frames\n                     .f = \\(.x) lm(lifeExp ~ year1950, data = .x)\n          )) %&gt;% \n  \n  # Use mutate + map again to expose the columns of the models\n  # Use broom:: tidy, broom::glance:: and broom::augment for separate columns\n  mutate(model_params = map(.x = model, \n                      .f = \\(.x) broom::tidy(.x, \n                                             conf.int = TRUE, \n                                             conf.lvel = 0.95)),\n         model_metrics = map(.x = model,\n                      .f = \\(.x) broom::glance(.x)),\n         \n         model_augment = map(.x  = model, \n                             .f = \\(.x) broom::augment(.x))\n         ) \ngapminder_models\n\n\n\n  \n\n\n\nWe can now take this tibble with multiple models and use broom to tidy, to glance at, and to augment the models:\n\nparams &lt;- gapminder_models %&gt;% \n  select(continent, country,model_params, model_metrics) %&gt;% \n  ungroup() %&gt;% \n  # Now unpack the linear model parameters into columns\n  unnest(cols = model_params)\nparams\n\n\n\n  \n\n\nmetrics &lt;- gapminder_models %&gt;% \n  select(continent, country, model_metrics) %&gt;% \n  ungroup() %&gt;% \n  # Now unpack the linear model parameters into columns\n  unnest(cols = model_metrics)\nmetrics\n\n\n\n  \n\n\naugments &lt;- gapminder_models %&gt;% \n  select(continent, country, model_augment) %&gt;% \n  ungroup() %&gt;% \n  # Now unpack the linear model parameters into columns\n  unnest(cols = model_augment)\naugments\n\n\n\n  \n\n\n\n\n Model Visualization\nWe can now plot these models and their uncertainty (i.e Confidence Intervals). We can select a few of the countries and plot:\n\nparams_filtered &lt;- params %&gt;% \n  filter(country %in% c(\"India\", \"United States\", \"Brazil\", \"China\"), \n         term == \"year1950\") %&gt;% \n  select(country, estimate, conf.low, conf.high,p.value) %&gt;%\n  arrange(estimate)\nparams_filtered\nparams_filtered %&gt;%\n  gf_errorbar(conf.high + conf.low ~ reorder(country, estimate),\n              linewidth = ~ -log10(p.value), width = 0.3,\n              ylab = \"Effect Size\", \n              xlab = \"Country\",\n              title = \"Effect of years on Life Expectancy\",\n              caption = \"Significance = - log10(p.value)\") %&gt;% \n  \n  gf_point(estimate ~ reorder(country,estimate), \n           colour = \"black\", size = 4) %&gt;%\n  \n  gf_theme(theme_classic()) %&gt;% \n  \n  gf_refine(coord_flip(),\n            scale_linewidth_continuous(\"Significance\", \n                                       range = c(0.2, 3))) %&gt;%\n  gf_refine(guides(linewidth = guide_legend(reverse = TRUE)),\n            theme(axis.text.x = element_text(angle = 30, hjust = 1)))\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nBut we can do better: visualize all models at once. What we will do is to plot the r.squared on the x-axis and the model term year1950 on the y-axis. We will need to combine params and metrics to do this:\n\nparams_combo &lt;- params %&gt;% \n  select(continent, country, term , estimate) %&gt;% \n  filter(term == \"year1950\") %&gt;% \n  left_join(metrics %&gt;% select(continent, country, r.squared)) \nparams_combo\nparams_combo %&gt;% \n  gf_point(reorder(country, r.squared) ~ r.squared, \n           color = \"grey90\") %&gt;%\n  gf_point(reorder(country, r.squared) ~ r.squared, \n           data = params_combo %&gt;% filter(continent == \"Africa\"),\n           shape = 21,\n    fill = \"salmon\",\n    ylab = \"Country\",\n    title = \"African Countries are Hard to Model\") %&gt;% \n  \n  gf_label(60 ~ 0.25,\n           label = \"African Countries\",\n           fill = \"salmon\",\n           color = \"black\",\n           inherit = FALSE) %&gt;%\n    \n  gf_theme(theme_classic()) %&gt;% \n  gf_refine(theme(axis.text.y = element_text(size = 3, face = \"bold\")))\nparams_combo %&gt;%\n  gf_point(estimate ~ r.squared, color = \"grey90\") %&gt;% \n  gf_point(estimate ~ r.squared,\n           data = params_combo %&gt;% filter(continent == \"Africa\"),\n           shape = 21,\n    fill = \"salmon\",\n    ylab = \"Slope Estimate for Linear Model\",\n    title = \"African Countries are Hard to Model\",\n    show.legend = FALSE) %&gt;%\n  gf_label(0.3 ~ 0.25,\n           label = \"African Countries\",\n           fill = \"salmon\",\n           color = \"black\",\n           inherit = FALSE) %&gt;%\n  gf_theme(theme_minimal())\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nAs can be seen, there are many models with low values of r.squared and these are sadly all about countries in Africa. The linear model fares badly for these countries, since there are other factors (not just year) that affects lifeExp in these countries.\nWe can look at the model metrics and see for which (African) countries the model fares the worst. We will reverse sort on r.squared and choose the 5 worst models:\n\nmetrics %&gt;% slice_min(order_by = r.squared, n = 5)\n\n\n\n  \n\n\n\nThere are of course reasons for this: genocide in Rwanda, and hyper-inflation in Zimbabwe, and of course the HIV-AIDS pandemic. These reasons are not captured in the original gapminder data!\nOne last plot! We can plot the model intercept on the x-axis and the slope year term on the y-axis to see where countries were in the beginning (1950) and at what rate they have improved in lifeExp:\n\nparams %&gt;%\n  select(continent, country, term , estimate) %&gt;%\n  pivot_wider(\n    id_cols = c(continent, country),\n    names_from = term,\n    values_from = estimate\n  ) %&gt;%\n  left_join(metrics %&gt;% select(continent, country, r.squared)) %&gt;%\n  \n  gf_point(\n    year1950 ~ `(Intercept)`,\n    color = ~ continent,\n    size = ~ r.squared,\n    xlab = \"Baseline at 1950\",\n    ylab = \"Rate of Improvement\",\n    title = \"Asian Countries Show Improvement in Life Expectancy\",\n    subtitle = \"African Countries still struggling\",\n    caption = \"Data from Gapminder\"\n  ) %&gt;%\n  gf_refine(scale_size(range = c(0.1, 4)),\n            scale_color_manual(\n              values =\n                c(\n                  \"Africa\" = \"salmon\",\n                  \"Asia\" = \"limegreen\",\n                  \"Americas\" = \"grey90\",\n                  \"Europe\" = \"grey90\",\n                  \"Oceania\" = \"grey90\"\n                )\n            )) %&gt;%\n  gf_refine(guides(size = guide_legend(reverse = TRUE))) %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\nMany Asian countries were low in lifeExp in 1950 and have shown good rates of improvement; r.squared is also decent. Sadly African countries had low lifeExp in 1950 and have not shown good rates of improvement."
  },
  {
    "objectID": "content/courses/R4Artists/Modules/130-purrr/purrr.html#iconify-mdi-new-box-recent-developments-in-dplyr",
    "href": "content/courses/R4Artists/Modules/130-purrr/purrr.html#iconify-mdi-new-box-recent-developments-in-dplyr",
    "title": "Iteration: Learning to purrr",
    "section": "\n Recent developments in dplyr\n",
    "text": "Recent developments in dplyr\n\nIn recent times, the familiar dplyr package also has experimental functions that are syntactically easier and offer pretty much purrr-like capability, and without introducing the complexity of the list columns or list output.\nLook the code below and decipher how it works:\n\n# Using group_modify\ngapminder_model_dplyr &lt;- gapminder %&gt;%\n  group_by(continent, country) %&gt;%\n  \n  # Here is the new function in dplyr!\n  # No need to use `mutate`\n  dplyr::group_modify(\n    .data = .,\n    \n    # .f MUST generate a tibble here and *not* a list\n    # Hence broom::tidy is essential!\n    # glance/tidy is part of the group_map's .f variable.\n    # Applies to each model\n\n    .f = ~ lm(lifeExp ~ year, data = .) %&gt;%\n      broom::glance(conf.int = TRUE,  # try `tidy()` and `augment()`\n                    conf.lvel = 0.95)\n  ) %&gt;%\n  \n  # We already have a grouped tibble from `group_modify()`\n  # So just ungroup()\n  ungroup()\n\ngapminder_model_dplyr\n\n\n\n  \n\n\n\nThere is no nesting and un-nesting; the data is the familiar tibble throughout! This seems like a simple and elegant method.\n\n\n\n\n\n\nUsing dplyr::group_modify\n\n\n\nNote: group_modify is new experimental function in dplyr (June 2023), as are group_map, list_cbind and list_rbind. group_modify requires that the operation in .fgenerates a tibble, not a list, and we can retain the grouping variable easily too. We can remove the groups with ungroup.\ngroup_modify() looks very clear and crisp, in my opinion. And very learner-friendly!"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/130-purrr/purrr.html#iconify-fluent-mdl2-decision-solid-conclusion",
    "href": "content/courses/R4Artists/Modules/130-purrr/purrr.html#iconify-fluent-mdl2-decision-solid-conclusion",
    "title": "Iteration: Learning to purrr",
    "section": "\n Conclusion",
    "text": "Conclusion\nWe have seen how purrr simplifies the application of functions iteratively to large groups of data, in a faster, replicable, and less error-prone manner. The basic idea (see video below) is:\n- Use tidyr::nest to create a grouped data frame with a nested list column\n- Use purrr::map_* to create a model for each of these data frames in the list column. The model will also be a column(usually) containing a list\n- Use broom::tidy to convert the list model-column into a data frame for visualization"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/130-purrr/purrr.html#iconify-ooui-references-rtl-references",
    "href": "content/courses/R4Artists/Modules/130-purrr/purrr.html#iconify-ooui-references-rtl-references",
    "title": "Iteration: Learning to purrr",
    "section": "\n References",
    "text": "References\n\n\nRebecca Barter, Learn to purrr. https://www.rebeccabarter.com/blog/2019-08-19_purrr\nEmorie Beck, Introduction to purrr. https://emoriebeck.github.io/R-tutorials/purrr/#\nSander Wuyts, purrr Tutorial. https://sanderwuyts.com/en/blog/purrr-tutorial/\nJared Wilber, Using the tidyverse for Machine Learning. https://www.jwilber.me/nest/\nDan Ovando,Data Wrangling and Model Fitting using purrr\nCormac Nolan, Modelling with Nested Data frames. https://github.com/cormac85/modelling_practice/blob/master/nested_data_frames.Rmd"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/index.html#sequence-diagram",
    "href": "content/courses/R4Artists/Modules/110-GoD/index.html#sequence-diagram",
    "title": "Lab-11: The Queen of Hearts, She Made some Tarts",
    "section": "Sequence Diagram",
    "text": "Sequence Diagram\nLook at the code below: What do you think it represents?\n\nCodeDiagrammeR(\"\nsequenceDiagram\nArvind -&gt;&gt; Anamika: Why are you late today?\nAnamika -&gt;&gt; Anamika: Ulp...\nAnamika -&gt;&gt; Arvind: I am sorry... &lt;br&gt; may I come in please?\n\nArvind -&gt;&gt; Komal: And you? What kept you?\nKomal -&gt;&gt; Anamika: (Quietly) He's having a bad day, dude...\nAnamika -&gt;&gt; Komal: (Whisper) Boomer...\n\")\n\n\n\n\n\nThis is a simple Sequence Diagram! Shows a strictly imaginary exchange between a pair of students and an unknown Faculty Member.\nLet us now see how we can embellish this kind of diagram. Can we have a Garden of Forking Paths?\n\nCodeDiagrammeR(\"\n  graph LR\n    A--&gt;B\n    A--&gt;C\n    C--&gt;E\n    B--&gt;D\n    C--&gt;D\n    D--&gt;F\n    E--&gt;F\n\")\n\n\n\n\n\n\nCodeDiagrammeR(\"\n        sequenceDiagram\n        \n        alt Anamika is always punctual\n        Arvind -&gt;&gt; Anamika: Why haven't you put up your Daily Reflection?\n        Anamika -&gt;&gt; Anamika: Ulp...\n        Note right of Anamika : I have had it today..\n        Anamika -&gt;&gt; Arvind: I am sorry... \n        Arvind -&gt;&gt; Anamika: Ok write it today\n        \n        else Anamika is usually tardy\n        Arvind -&gt;&gt; Anamika: Why haven't you put up your Daily Reflection?\n        Anamika -&gt;&gt; Anamika: Ulp...\n        Anamika -&gt;&gt; Arvind: I am sorry... \n        Arvind -&gt;&gt; Anamika: This is not acceptable and will reflect in your grade\n        end\n        \n        Arvind -&gt;&gt; Komal: And you? What kept you?\n        Komal -&gt;&gt; Anamika: (Quietly) He's having a bad day, dude...\n        Anamika -&gt;&gt; Komal: (Whisper) Boomer...\n        Note over Anamika,Komal: Giggle...\n\")\n\n\n\n\n\n\nFrom here: https://cyberhelp.sesync.org/blog/visualization-with-diagrammeR.html\n\nCodegrViz(\"digraph{\n\n      graph[rankdir = LR]\n  \n      node[shape = rectangle, style = filled]\n  \n      node[fillcolor = Coral, margin = 0.2]\n      A[label = 'Figure 1: Map']\n      B[label = 'Figure 2: Metrics']\n  \n      node[fillcolor = Cyan, margin = 0.2]\n      C[label = 'Figures.Rmd']\n  \n      node[fillcolor = Violet, margin = 0.2]\n      D[label = 'Analysis_1.R']\n      E[label = 'Analysis_2.R']\n  \n      subgraph cluster_0 {\n        graph[shape = rectangle]\n        style = rounded\n        bgcolor = Gold\n    \n        label = 'Data Source 1'\n        node[shape = rectangle, fillcolor = LemonChiffon, margin = 0.25]\n        F[label = 'my_dataframe_1.csv']\n        G[label = 'my_dataframe_2.csv']\n      }\n  \n      subgraph cluster_1 {\n         graph[shape = rectangle]\n         style = rounded\n         bgcolor = Gold\n    \n         label = 'Data Source 2'\n         node[shape = rectangle, fillcolor = LemonChiffon, margin = 0.25]\n         H[label = 'my_dataframe_3.csv']\n         I[label = 'my_dataframe_4.csv']\n      }\n  \n      edge[color = black, arrowhead = vee, arrowsize = 1.25]\n      C -&gt; {A B}\n      D -&gt; C\n      E -&gt; C\n      F -&gt; D\n      G -&gt; D\n      H -&gt; E\n      I -&gt; E\n      \n      }\")\n\n\n\n\n\n\nCodemermaid(\"\n        graph BT\n        A((Salinity))\n        A--&gt;B(Barnacles)\n        B-.-&gt;|-0.10|B1{Mussels}\n        A-- 0.30 --&gt;B1\n\n        C[Air Temp]\n        C--&gt;B\n        C-.-&gt;E(Macroalgae)\n        E--&gt;B1\n        C== 0.89 ==&gt;B1\n\n        style A fill:#FFF, stroke:#333, stroke-width:4px\n        style B fill:#9AA, stroke:#9AA, stroke-width:2px\n        style B1 fill:#879, stroke:#333, stroke-width:1px\n        style C fill:#ADF, stroke:#333, stroke-width:2px\n        style E fill:#9C2, stroke:#9C2, stroke-width:2px\n\n        \")\n\n\n\n\n\n\nCodeDiagrammeR(\"\nsequenceDiagram\n  Arvind -&gt;&gt;ticket seller: ask ticket\n  ticket seller-&gt;&gt;database: seats\n  alt tickets available\n    database-&gt;&gt;ticket seller: ok\n    ticket seller-&gt;&gt;customer: confirm\n    Arvind -&gt;&gt;ticket seller: ok\n    ticket seller-&gt;&gt;database: book a seat\n    ticket seller-&gt;&gt;printer: print ticket\n  else sold out\n    database-&gt;&gt;ticket seller: none left\n    ticket seller-&gt;&gt;customer: sorry\n  end\n\")\n\n\n\n\n\n\nCodeDiagrammeR(\n\"graph TB;\nA(Rounded)--&gt;B[Squared];\nB--&gt;C{A Decision};\nC--&gt;D[Square One];\nC--&gt;E[Square Two];\n\n%% Now styling these blocks\nstyle A fill:#E5E25F;  \nstyle B fill:#87AB51; \nstyle C fill:#3C8937;\nstyle D fill:#23772C;  \nstyle E fill:#B6E6E6;\n\"\n)\n\n\n\n\n\n\nCode  grViz(\"\ndigraph boxes_and_circles {\n\n  # a 'graph' statement\n  graph [overlap = true, fontsize = 10,forcelabels = true]\n\n  # several 'node' statements\n  node [shape = box,fontname = Helvetica, color = red, style = filled]\n  A[label = 'This is \\\\n an internal \\\\n label', xlabel = 'This is \\\\nan external \\\\nlabel']; B; C; D; E; F\n\n  node [shape = circle, fixedsize = true, color = palegreen, width = 0.9] // sets as circles\n  1; 2; 3; 4; 5; 6; 7; 8\n\n  # several 'edge' statements\n  A-&gt;{1,2,3,4} B-&gt;2 B-&gt;3 B-&gt;4 C-&gt;A\n  1-&gt;D E-&gt;A 2-&gt;4 1-&gt;5 1-&gt;F\n  E-&gt;6 4-&gt;6 5-&gt;7 6-&gt;7 3-&gt;8 3-&gt;1\n}\n\")"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/index.html#sequence-diagram-2",
    "href": "content/courses/R4Artists/Modules/110-GoD/index.html#sequence-diagram-2",
    "title": "Lab-11: The Queen of Hearts, She Made some Tarts",
    "section": "Sequence Diagram-2",
    "text": "Sequence Diagram-2"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/index.html#sequence-diagram-3",
    "href": "content/courses/R4Artists/Modules/110-GoD/index.html#sequence-diagram-3",
    "title": "Lab-11: The Queen of Hearts, She Made some Tarts",
    "section": "Sequence Diagram 3",
    "text": "Sequence Diagram 3"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/index.html#mindmap",
    "href": "content/courses/R4Artists/Modules/110-GoD/index.html#mindmap",
    "title": "Lab-11: The Queen of Hearts, She Made some Tarts",
    "section": "Mindmap",
    "text": "Mindmap"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/index.html#gantt-chart",
    "href": "content/courses/R4Artists/Modules/110-GoD/index.html#gantt-chart",
    "title": "Lab-11: The Queen of Hearts, She Made some Tarts",
    "section": "Gantt Chart",
    "text": "Gantt Chart"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/index.html#flow-chart",
    "href": "content/courses/R4Artists/Modules/110-GoD/index.html#flow-chart",
    "title": "Lab-11: The Queen of Hearts, She Made some Tarts",
    "section": "Flow chart",
    "text": "Flow chart"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/index.html#some-definitions-on-the-grammar-of-shapes-in-nomnoml",
    "href": "content/courses/R4Artists/Modules/110-GoD/index.html#some-definitions-on-the-grammar-of-shapes-in-nomnoml",
    "title": "Lab-11: The Queen of Hearts, She Made some Tarts",
    "section": "Some definitions on the “grammar of shapes” in nomnoml\n",
    "text": "Some definitions on the “grammar of shapes” in nomnoml\n\n\nAssociation Types: Connectors between blocks( i.e. Classifiers)\nClassifier Types: Kinds of blocks.\nDirective Types: Directives change the nature of the diagram rendered, by affective parameters like colour, direction and margins. ( Ha! VC people!!)\n\nCSS colours https://www.w3schools.com/cssref/css_colors.asp Only these colours are permitted, so use either the names or these specific colour hash codes. Any general hash code will not render.\n\nCode//association-1\n[a] - [b] \n\n//association-2\n[b] -&gt; [c] \n\n//association_3\n[c] &lt;-&gt; [a]\n\n//dependency-1\n[a] &lt;--&gt;[d]\n\n//dependency-2\n#.ell: visual=ellipse fill=#fbfb09 bold\n#.arvind: visual=rhomb fill=#ff2234 bold\n[&lt;ell&gt;e]--&gt;[a]\n//generalization-1\n[c]-:&gt;[&lt;arvind&gt;k]\n\n//implementation --:&gt;\n[k]--:&gt;[d]\n\n\n\n\n\n\nCode//composition +-\n[a]+-[b]\n//composition +-&gt;\n[b]-+[c]\n//aggregation o-\n[c]o-&gt;[d]\n//aggregation o-&gt;\n[d]o-&gt;[a]\n//note --\n[d]--[everything happens;here]\n//hidden -/-\n[d]-/-[f]\n////////////////////////\n//weightless edge _&gt;\n//[k]_&gt;[d] //not working\n//weightless dashed__\n//[d]__[j] //not working\n\n\n\n\n\nClassifier Types\nThese are different kinds of blocks.\n\nCode[class]-&gt;[&lt;abstract&gt; abstract]\n[&lt;abstract&gt; abstract]-:&gt;[&lt;instance&gt; instance]\n[&lt;instance&gt; instance]-:&gt;[&lt;note&gt; note]\n[&lt;note&gt; note]--&gt;[&lt;reference&gt; reference]\n\n\n\n\n\n\nCode[&lt;package&gt; package|components]--&gt;[&lt;frame&gt; frame|]\n[&lt;database&gt; database]--&gt;[&lt;start&gt; start]\n[&lt;end&gt; end]-o&gt;[&lt;state&gt; state]\n\n\n\n\n\n\nCode[&lt;choice&gt; choice]---&gt;[&lt;sync&gt; sync]\n[&lt;input&gt; input]-&gt;[&lt;sender&gt; sender]\n[&lt;receiver&gt; receiver]o-[&lt;transceiver&gt; transceiver]\n\n\n\n\n\n\nCode#direction:down\n#background:lightgrey\n#fill: fuchsia; green; purple\n#fillArrows: false\n#font: Courier\n[class]-&gt;[&lt;abstract&gt; abstract]\n[&lt;abstract&gt; abstract]-:&gt;[&lt;instance&gt; instance]\n[&lt;instance&gt; instance]-:&gt;[&lt;note&gt; note]\n[&lt;note&gt; note]--&gt;[&lt;reference&gt; reference]\n\n\n\n\n\n\nCode#font: CenturySchoolbook\n#fill: lightyellow\n#stroke: green\n\n[&lt;actor&gt; actor]---[&lt;usecase&gt; usecase]\n[&lt;usecase&gt; usecase]&lt;--&gt;[&lt;label&gt; label]\n[&lt;usecase&gt; usecase]-/-[&lt;hidden&gt; hidden]\n\n\n\n\n\n\nCode[&lt;table&gt; table| a | 5 || b | 7]\n\n\n\n\n\n\n\nCode[&lt;table&gt; table| c | 9 ]"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/index.html#directives",
    "href": "content/courses/R4Artists/Modules/110-GoD/index.html#directives",
    "title": "Lab-11: The Queen of Hearts, She Made some Tarts",
    "section": "Directives",
    "text": "Directives\nDirectives change the nature of the diagram rendered, by affective parameters like colour, direction and margins."
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/index.html#custom-classifier-styles",
    "href": "content/courses/R4Artists/Modules/110-GoD/index.html#custom-classifier-styles",
    "title": "Lab-11: The Queen of Hearts, She Made some Tarts",
    "section": "Custom classifier styles",
    "text": "Custom classifier styles\nA directive that starts with “.” define a classifier’s style. The style is written as a space separated list of modifiers and key/value pairs.\n\nCode#.box: fill=#8f8 dashed\n#.blob: visual=ellipse title=bold\n#.arvind: visual=rhomb title=bold dashed fill=CornFlowerBlue\n[&lt;box&gt; GreenBox]\n[&lt;blob&gt; Blobby]\n[&lt;arvind&gt; Someone]"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/index.html#nomnoml-keyvalue-pairs",
    "href": "content/courses/R4Artists/Modules/110-GoD/index.html#nomnoml-keyvalue-pairs",
    "title": "Lab-11: The Queen of Hearts, She Made some Tarts",
    "section": "\nnomnoml Key/value pairs",
    "text": "nomnoml Key/value pairs\n\nfill=(any css color)\nstroke=(any css color)\nalign=center align=left\ndirection=right direction=down\nvisual=actor\nvisual=class\nvisual=database\nvisual=ellipse\nvisual=end\nvisual=frame\nvisual=hidden\nvisual=input\nvisual=none\nvisual=note\nvisual=package\nvisual=receiver\nvisual=rhomb\nvisual=roundrect\nvisual=sender\nvisual=start\nvisual=sync\nvisual=table\nvisual=transceiver"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/index.html#text-modifiers",
    "href": "content/courses/R4Artists/Modules/110-GoD/index.html#text-modifiers",
    "title": "Lab-11: The Queen of Hearts, She Made some Tarts",
    "section": "Text modifiers",
    "text": "Text modifiers\nbold center italic left underline\n\nCode# .box: fill=#8f8 dashed\n# .blob: visual=rhomb title=bold fill=#8f8 dashed\n\n[A]-[B]\n[B]--[&lt;usecase&gt;C]\n[C]-[&lt;box&gt; D]\n[B]--[&lt;blob&gt; Jabba;TheHut]\n\n\n\n\n\n\nCode[a] -&gt;[b]\n[b] -:&gt; [c]\n[c]o-&gt;[d]\n[d]-/-[e]\n\n\n\n\n\n\nCode#fill: lightgreen; lightblue; lightyellow; grey; white\n\n[&lt;table&gt; table | c | 9 ]\n\n[R | [&lt;table&gt; Packages |\n         Base R |\n         [ &lt;table&gt; tidyverse| ggplot | tidyr | readr |\n             [&lt;table&gt; dplyr|\n                 magrittr | Others]]]]\n\n\n\n\n\n\n\nCode#fill: lightgreen; lightblue; lightyellow; pink; white\n\n[RStudio | [R | [&lt;table&gt; Packages |\n                   Base R | [ tidyverse |\n                               ggplot | tidyr | readr |\n                               [dplyr]--[magrittr]\n                               [dplyr]--[Others]\n                             | tibble\n                             ]\n                 | lubridate | DiagrammeR | Lattice]]]\n\n\n\n\n\n\n\nCode[Linux]+-[Ubuntu]\n[Linux]+-[Mint]\n[Ubuntu]--[Mint]\n[Linux]+-[Rosa Linux]\n[Linux]+-[Mx Linux]\n[Debian]-+[Linux]\n\n\n[Fedora]-+[Linux]\n[Puppy Linux]-+[Linux]\n[Personal Pups]-+[Puppy Linux]"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/100-GoN/index.html#fa-folder-open-slides-and-tutorials",
    "href": "content/courses/R4Artists/Modules/100-GoN/index.html#fa-folder-open-slides-and-tutorials",
    "title": "Lab-10: An Invitation from the Queen…to play Croquet",
    "section": " Slides and Tutorials",
    "text": "Slides and Tutorials"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/100-GoN/index.html#fa-envelope-introduction",
    "href": "content/courses/R4Artists/Modules/100-GoN/index.html#fa-envelope-introduction",
    "title": "Lab-10: An Invitation from the Queen…to play Croquet",
    "section": " Introduction",
    "text": "Introduction\nNetwork Diagrams are important in data visualization to bring out relationships between diverse entities. They are used in ecology, biology, transportation, and even history!\nAnd hey, whom did Jon Snow marry?"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/100-GoN/index.html#fa-asterisk-references",
    "href": "content/courses/R4Artists/Modules/100-GoN/index.html#fa-asterisk-references",
    "title": "Lab-10: An Invitation from the Queen…to play Croquet",
    "section": " References",
    "text": "References\n\nMichael Gastner, Data Analysis and Visualisation with R, Chapter 23: Networks\nDavid Schoch, Network Visualizations in R using ggraph and graphlayouts\nKonrad M. Lawson, Toilers and Gangsters:Simple Network Visualization with R for Historians"
  },
  {
    "objectID": "content/courses/R4Artists/listing.html",
    "href": "content/courses/R4Artists/listing.html",
    "title": "R for Artists and Managers",
    "section": "",
    "text": "🕶 Lab-1: Science, Human Experience, Experiments, and Data\n\n\nWhy do we visualize data\n\n\n\n\n\n\nNov 1, 2021\n\n\n10 min\n\n\n\n\n\n\n  \n\n\n\n\nLab-2: Down the R-abbit Hole…\n\n\nWelcome ! Introduce Yourself to R, RStudio, and to all of Us!\n\n\n\n\n\n\nJul 9, 2021\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nLab-3: Drink Me!\n\n\nWorking with Quarto\n\n\n\n\n\n\nMar 10, 2023\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nLab-4: I say what I mean and I mean what I say\n\n\nGetting started with Data in R\n\n\n\n\n\n\nJul 12, 2022\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nLab-5: Twas brillig, and the slithy toves…\n\n\nTidy Data at the wabe MoMA\n\n\n\n\n\n\nNov 22, 2022\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nLab-6: These Roses have been Painted !!\n\n\nThe Grammar of Graphics in R\n\n\n\n\n\n\nAug 21, 2022\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nLab-7: The Lobster Quadrille\n\n\nFonts and other Wizardy in ggplot\n\n\n\n\n\n\nInvalid Date\n\n\n33 min\n\n\n\n\n\n\n  \n\n\n\n\nLab-8: Did you ever see such a thing as a drawing of a muchness?\n\n\nWorking with htmlwidgets in R!\n\n\n\n\n\n\nInvalid Date\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nLab-9: If you please sir…which way to the Secret Garden?\n\n\nThe Grammar of Maps\n\n\n\n\n\n\nJul 10, 2022\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nLab-10: An Invitation from the Queen…to play Croquet\n\n\nThe Grammar of Networks\n\n\n\n\n\n\nJun 6, 2022\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nLab-11: The Queen of Hearts, She Made some Tarts\n\n\nThe Grammar of Diagrams\n\n\n\n\n\n\nInvalid Date\n\n\n11 min\n\n\n\n\n\n\n  \n\n\n\n\nLab-12: Time is a Him!!\n\n\nTime Series in R\n\n\n\n\n\n\nFeb 14, 2022\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nIteration: Learning to purrr\n\n\nPerforming Iterations in R\n\n\n\n\n\n\nJun 14, 2023\n\n\n17 min\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "content/courses/ML4Artists/listing.html",
    "href": "content/courses/ML4Artists/listing.html",
    "title": "Machine Learning for Artists and Managers",
    "section": "",
    "text": "No matching items\n\n Back to top"
  },
  {
    "objectID": "content/courses/ML4Artists/3-Classification/Random-Forests.html",
    "href": "content/courses/ML4Artists/3-Classification/Random-Forests.html",
    "title": "Random Forests",
    "section": "",
    "text": "Machine Learning Basics - Random Forest at Shirin’s Playground"
  },
  {
    "objectID": "content/courses/ML4Artists/3-Classification/Random-Forests.html#references",
    "href": "content/courses/ML4Artists/3-Classification/Random-Forests.html#references",
    "title": "Random Forests",
    "section": "",
    "text": "Machine Learning Basics - Random Forest at Shirin’s Playground"
  },
  {
    "objectID": "content/courses/ML4Artists/3-Classification/Random-Forests.html#penguin-random-forest-model-withrandomforest",
    "href": "content/courses/ML4Artists/3-Classification/Random-Forests.html#penguin-random-forest-model-withrandomforest",
    "title": "Random Forests",
    "section": "Penguin Random Forest Model withrandomForest\n",
    "text": "Penguin Random Forest Model withrandomForest\n\nUsing the penguins dataset and Random Forest Classification.\n\npenguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_…¹ body_…² sex    year\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;      &lt;int&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7        181    3750 male   2007\n 2 Adelie  Torgersen           39.5          17.4        186    3800 fema…  2007\n 3 Adelie  Torgersen           40.3          18          195    3250 fema…  2007\n 4 Adelie  Torgersen           NA            NA           NA      NA &lt;NA&gt;   2007\n 5 Adelie  Torgersen           36.7          19.3        193    3450 fema…  2007\n 6 Adelie  Torgersen           39.3          20.6        190    3650 male   2007\n 7 Adelie  Torgersen           38.9          17.8        181    3625 fema…  2007\n 8 Adelie  Torgersen           39.2          19.6        195    4675 male   2007\n 9 Adelie  Torgersen           34.1          18.1        193    3475 &lt;NA&gt;   2007\n10 Adelie  Torgersen           42            20.2        190    4250 &lt;NA&gt;   2007\n# … with 334 more rows, and abbreviated variable names ¹​flipper_length_mm,\n#   ²​body_mass_g\n\nsummary(penguins)\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2                                 \n\npenguins %&gt;% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n344\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nspecies\n0\n1.00\nFALSE\n3\nAde: 152, Gen: 124, Chi: 68\n\n\nisland\n0\n1.00\nFALSE\n3\nBis: 168, Dre: 124, Tor: 52\n\n\nsex\n11\n0.97\nFALSE\n2\nmal: 168, fem: 165\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nbill_length_mm\n2\n0.99\n43.92\n5.46\n32.1\n39.23\n44.45\n48.5\n59.6\n▃▇▇▆▁\n\n\nbill_depth_mm\n2\n0.99\n17.15\n1.97\n13.1\n15.60\n17.30\n18.7\n21.5\n▅▅▇▇▂\n\n\nflipper_length_mm\n2\n0.99\n200.92\n14.06\n172.0\n190.00\n197.00\n213.0\n231.0\n▂▇▃▅▂\n\n\nbody_mass_g\n2\n0.99\n4201.75\n801.95\n2700.0\n3550.00\n4050.00\n4750.0\n6300.0\n▃▇▆▃▂\n\n\nyear\n0\n1.00\n2008.03\n0.82\n2007.0\n2007.00\n2008.00\n2009.0\n2009.0\n▇▁▇▁▇\n\n\n\n\npenguins &lt;- penguins %&gt;% tidyr::drop_na()\n# Spent one hour trying to find `drop-na()` ( 14 June 2020)\n\n\n# library(corrplot)\ncor &lt;- penguins %&gt;% select(is.numeric) %&gt;% cor() \n\nWarning: Use of bare predicate functions was deprecated in tidyselect 1.1.0.\nℹ Please use wrap predicates in `where()` instead.\n  # Was:\n  data %&gt;% select(is.numeric)\n\n  # Now:\n  data %&gt;% select(where(is.numeric))\n\ncor %&gt;% corrplot(., method = \"ellipse\", order = \"hclust\",tl.cex = 0.5)\n\n\n\n# try these too:\n# cor %&gt;% corrplot(., method = \"square\", order = \"hclust\",tl.cex = 0.5)\n# cor %&gt;% corrplot(., method = \"color\", order = \"hclust\",tl.cex = 0.5)\n# cor %&gt;% corrplot(., method = \"shade\", order = \"hclust\",tl.cex = 0.5)\n\nNotes: - flipper_length_mm and culmen_depth_mm are negtively correlated at approx (-0.7) - flipper_length_mm and body_mass_g are positively correlated at approx 0.8\nSo we will use steps in the recipe to remove correlated variables.\nPenguin Data Sampling and Recipe\n\n# Data Split\npenguin_split &lt;- initial_split(penguins, prop = 0.6)\npenguin_train &lt;- training(penguin_split)\npenguin_test &lt;- testing(penguin_split)\npenguin_split\n\n&lt;Training/Testing/Total&gt;\n&lt;199/134/333&gt;\n\nhead(penguin_train)\n\n# A tibble: 6 × 8\n  species   island bill_length_mm bill_depth_mm flipper_le…¹ body_…² sex    year\n  &lt;fct&gt;     &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;        &lt;int&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt;\n1 Adelie    Dream            41.1          19            182    3425 male   2007\n2 Adelie    Biscoe           41.6          18            192    3950 male   2008\n3 Gentoo    Biscoe           40.9          13.7          214    4650 fema…  2007\n4 Adelie    Dream            42.2          18.5          180    3550 fema…  2007\n5 Chinstrap Dream            43.2          16.6          187    2900 fema…  2007\n6 Adelie    Dream            34            17.1          185    3400 fema…  2008\n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n\n# Recipe\npenguin_recipe &lt;- penguins %&gt;% \n  recipe(species ~ .) %&gt;% \n  step_normalize(all_numeric()) %&gt;% # Scaling and Centering\n  step_corr(all_numeric()) %&gt;%  # Handling correlated variables\n  prep()\n\n# Baking the data\npenguin_train_baked &lt;-  penguin_train %&gt;% \n  bake(object = penguin_recipe, new_data = .)\n\npenguin_test_baked &lt;-  penguin_test %&gt;% \n  bake(object = penguin_recipe, new_data = .)\n\nhead(penguin_train_baked)\n\n# A tibble: 6 × 8\n  island bill_length_mm bill_depth_mm flipper_le…¹ body_…² sex      year species\n  &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;   &lt;dbl&gt; &lt;fct&gt;  \n1 Dream          -0.529        0.932        -1.35   -0.971 male  -1.28   Adelie \n2 Biscoe         -0.438        0.424        -0.640  -0.319 male  -0.0517 Adelie \n3 Biscoe         -0.566       -1.76          0.930   0.550 fema… -1.28   Gentoo \n4 Dream          -0.328        0.678        -1.50   -0.816 fema… -1.28   Adelie \n5 Dream          -0.145       -0.287        -0.997  -1.62  fema… -1.28   Chinst…\n6 Dream          -1.83        -0.0329       -1.14   -1.00  fema… -0.0517 Adelie \n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n\n\nPenguin Random Forest Model\n\npenguin_model &lt;- \n  rand_forest(trees = 100) %&gt;% \n  set_engine(\"randomForest\") %&gt;% \n  set_mode(\"classification\")\npenguin_model\n\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  trees = 100\n\nComputational engine: randomForest \n\npenguin_fit &lt;- \n  penguin_model %&gt;% \n  fit(species ~ .,penguin_train_baked)\npenguin_fit\n\nparsnip model object\n\n\nCall:\n randomForest(x = maybe_data_frame(x), y = y, ntree = ~100) \n               Type of random forest: classification\n                     Number of trees: 100\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 0.5%\nConfusion matrix:\n          Adelie Chinstrap Gentoo class.error\nAdelie        85         0      0  0.00000000\nChinstrap      1        40      0  0.02439024\nGentoo         0         0     73  0.00000000\n\n# iris_ranger &lt;- \n#   rand_forest(trees = 100) %&gt;% \n#   set_mode(\"classification\") %&gt;% \n#   set_engine(\"ranger\") %&gt;% \n#   fit(Species ~ ., data = iris_training_baked)\n\nMetrics for the Penguin Random Forest Model\n\n# Predictions\npredict(object = penguin_fit, new_data = penguin_test_baked) %&gt;%  \n  dplyr::bind_cols(penguin_test_baked) %&gt;% \n  glimpse()\n\nRows: 134\nColumns: 9\n$ .pred_class       &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Biscoe, …\n$ bill_length_mm    &lt;dbl&gt; -0.6752636, -0.9312674, -0.5289757, -1.7541369, -1.4…\n$ bill_depth_mm     &lt;dbl&gt; 0.42409105, 0.32252879, 0.22096653, 0.62721557, 1.03…\n$ flipper_length_mm &lt;dbl&gt; -0.42573251, -1.42460769, -1.35325946, -1.21056301, …\n$ body_mass_g       &lt;dbl&gt; -1.18857213, -0.72285846, -1.25066728, -1.09542939, …\n$ sex               &lt;fct&gt; female, female, female, female, female, male, male, …\n$ year              &lt;dbl&gt; -1.2818130, -1.2818130, -1.2818130, -1.2818130, -1.2…\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n\n# Prediction Accuracy Metrics\npredict(object = penguin_fit, new_data = penguin_test_baked) %&gt;%  \n  dplyr::bind_cols(penguin_test_baked) %&gt;% \n  yardstick::metrics(truth = species, estimate = .pred_class)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.985\n2 kap      multiclass     0.976\n\n# Prediction Probabilities\npenguin_fit_probs &lt;- \n  predict(penguin_fit, penguin_test_baked, type = \"prob\") %&gt;%\n  dplyr::bind_cols(penguin_test_baked)\nglimpse(penguin_fit_probs)\n\nRows: 134\nColumns: 11\n$ .pred_Adelie      &lt;dbl&gt; 0.99, 0.98, 0.98, 1.00, 0.98, 0.99, 0.96, 0.99, 0.99…\n$ .pred_Chinstrap   &lt;dbl&gt; 0.01, 0.02, 0.01, 0.00, 0.00, 0.00, 0.01, 0.00, 0.00…\n$ .pred_Gentoo      &lt;dbl&gt; 0.00, 0.00, 0.01, 0.00, 0.02, 0.01, 0.03, 0.01, 0.01…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Biscoe, …\n$ bill_length_mm    &lt;dbl&gt; -0.6752636, -0.9312674, -0.5289757, -1.7541369, -1.4…\n$ bill_depth_mm     &lt;dbl&gt; 0.42409105, 0.32252879, 0.22096653, 0.62721557, 1.03…\n$ flipper_length_mm &lt;dbl&gt; -0.42573251, -1.42460769, -1.35325946, -1.21056301, …\n$ body_mass_g       &lt;dbl&gt; -1.18857213, -0.72285846, -1.25066728, -1.09542939, …\n$ sex               &lt;fct&gt; female, female, female, female, female, male, male, …\n$ year              &lt;dbl&gt; -1.2818130, -1.2818130, -1.2818130, -1.2818130, -1.2…\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n\n# Confusion Matrix\npenguin_fit$fit$confusion %&gt;% tidy()\n\nWarning: 'tidy.numeric' is deprecated.\nSee help(\"Deprecated\")\n\n\n# A tibble: 3 × 1\n  x[,\"Adelie\"] [,\"Chinstrap\"] [,\"Gentoo\"] [,\"class.error\"]\n         &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;            &lt;dbl&gt;\n1           85              0           0           0     \n2            1             40           0           0.0244\n3            0              0          73           0     \n\n# Gain Curves\npenguin_fit_probs %&gt;% \n  yardstick::gain_curve(species, .pred_Adelie:.pred_Gentoo) %&gt;%\n  autoplot()\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\nℹ The deprecated feature was likely used in the yardstick package.\n  Please report the issue at &lt;https://github.com/tidymodels/yardstick/issues&gt;.\n\n\n\n\n# ROC Plot\npenguin_fit_probs%&gt;%\n  roc_curve(species, .pred_Adelie:.pred_Gentoo) %&gt;%\n  autoplot()\n\n\n\n\nUsing broom on the penguin model\n\npenguin_split\n\n&lt;Training/Testing/Total&gt;\n&lt;199/134/333&gt;\n\npenguin_split %&gt;% broom::tidy()\n\n# A tibble: 333 × 2\n     Row Data    \n   &lt;int&gt; &lt;chr&gt;   \n 1     1 Analysis\n 2     2 Analysis\n 3     4 Analysis\n 4     5 Analysis\n 5     7 Analysis\n 6     9 Analysis\n 7    10 Analysis\n 8    11 Analysis\n 9    12 Analysis\n10    13 Analysis\n# … with 323 more rows\n\npenguin_recipe %&gt;% broom::tidy()\n\n# A tibble: 2 × 6\n  number operation type      trained skip  id             \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;          \n1      1 step      normalize TRUE    FALSE normalize_zCUDv\n2      2 step      corr      TRUE    FALSE corr_WPWs7     \n\n# Following do not work for `random forest models` !! ;-()\n#penguin_model %&gt;% tidy()\n#penguin_fit %&gt;% tidy() \npenguin_model %&gt;% str()\n\nList of 7\n $ args                 :List of 3\n  ..$ mtry : language ~NULL\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_EmptyEnv&gt; \n  ..$ trees: language ~100\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_EmptyEnv&gt; \n  ..$ min_n: language ~NULL\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_EmptyEnv&gt; \n $ eng_args             : Named list()\n  ..- attr(*, \"class\")= chr [1:2] \"quosures\" \"list\"\n $ mode                 : chr \"classification\"\n $ user_specified_mode  : logi TRUE\n $ method               : NULL\n $ engine               : chr \"randomForest\"\n $ user_specified_engine: logi TRUE\n - attr(*, \"class\")= chr [1:2] \"rand_forest\" \"model_spec\"\n\npenguin_test_baked\n\n# A tibble: 134 × 8\n   island    bill_length_mm bill_depth_mm flipper_…¹ body_…² sex    year species\n   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt;  \n 1 Torgersen         -0.675        0.424      -0.426  -1.19  fema… -1.28 Adelie \n 2 Torgersen         -0.931        0.323      -1.42   -0.723 fema… -1.28 Adelie \n 3 Torgersen         -0.529        0.221      -1.35   -1.25  fema… -1.28 Adelie \n 4 Torgersen         -1.75         0.627      -1.21   -1.10  fema… -1.28 Adelie \n 5 Biscoe            -1.48         1.03       -0.854  -0.506 fema… -1.28 Adelie \n 6 Biscoe            -1.06         0.475      -1.14   -0.319 male  -1.28 Adelie \n 7 Biscoe            -0.950        0.0178     -1.50   -0.506 male  -1.28 Adelie \n 8 Biscoe            -0.620        0.729      -1.28   -0.816 male  -1.28 Adelie \n 9 Biscoe            -0.639        0.881      -1.50   -0.319 male  -1.28 Adelie \n10 Dream             -0.822       -0.236      -1.64   -1.19  fema… -1.28 Adelie \n# … with 124 more rows, and abbreviated variable names ¹​flipper_length_mm,\n#   ²​body_mass_g"
  },
  {
    "objectID": "content/courses/ML4Artists/3-Classification/Random-Forests.html#iris-random-forest-model-with-ranger",
    "href": "content/courses/ML4Artists/3-Classification/Random-Forests.html#iris-random-forest-model-with-ranger",
    "title": "Random Forests",
    "section": "Iris Random Forest Model with ranger\n",
    "text": "Iris Random Forest Model with ranger\n\nUsing the iris dataset and Random Forest Classification. This part uses rsample to split the data and the recipes to prep the data for model making.\n\n#set.seed(100)\niris_split &lt;- rsample::initial_split(iris, prop = 0.6)\niris_split\n\n&lt;Training/Testing/Total&gt;\n&lt;90/60/150&gt;\n\niris_split %&gt;% training() %&gt;% glimpse()\n\nRows: 90\nColumns: 5\n$ Sepal.Length &lt;dbl&gt; 5.2, 5.1, 6.1, 5.2, 6.8, 4.6, 5.4, 5.8, 7.7, 6.5, 5.1, 6.…\n$ Sepal.Width  &lt;dbl&gt; 4.1, 3.5, 2.6, 3.5, 2.8, 3.4, 3.9, 2.6, 3.0, 3.0, 3.3, 2.…\n$ Petal.Length &lt;dbl&gt; 1.5, 1.4, 5.6, 1.5, 4.8, 1.4, 1.7, 4.0, 6.1, 5.8, 1.7, 4.…\n$ Petal.Width  &lt;dbl&gt; 0.1, 0.2, 1.4, 0.2, 1.4, 0.3, 0.4, 1.2, 2.3, 2.2, 0.5, 1.…\n$ Species      &lt;fct&gt; setosa, setosa, virginica, setosa, versicolor, setosa, se…\n\niris_split %&gt;% testing() %&gt;% glimpse()\n\nRows: 60\nColumns: 5\n$ Sepal.Length &lt;dbl&gt; 4.7, 5.4, 4.8, 4.3, 5.8, 5.7, 5.1, 5.4, 5.1, 4.7, 5.5, 5.…\n$ Sepal.Width  &lt;dbl&gt; 3.2, 3.7, 3.0, 3.0, 4.0, 3.8, 3.8, 3.4, 3.7, 3.2, 4.2, 3.…\n$ Petal.Length &lt;dbl&gt; 1.3, 1.5, 1.4, 1.1, 1.2, 1.7, 1.5, 1.7, 1.5, 1.6, 1.4, 1.…\n$ Petal.Width  &lt;dbl&gt; 0.2, 0.2, 0.1, 0.1, 0.2, 0.3, 0.3, 0.2, 0.4, 0.2, 0.2, 0.…\n$ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n\nIris Data Pre-Processing: Creating the Recipe\nThe recipes package provides an interface that specializes in data pre-processing. Within the package, the functions that start, or execute, the data transformations are named after cooking actions. That makes the interface more user-friendly. For example:\n\nrecipe() - Starts a new set of transformations to be applied, similar to the ggplot() command. Its main argument is the model’s formula.\nprep() - Executes the transformations on top of the data that is supplied (typically, the training data). Each data transformation is a step() function. ( Recall what we did with the caret package: Centering, Scaling, Removing Correlated variables…)\n\nNote that in order to avoid data leakage (e.g: transferring information from the train set into the test set), data should be “prepped” using the train_tbl only. https://towardsdatascience.com/modelling-with-tidymodels-and-parsnip-bae2c01c131c CRAN: The idea is that the preprocessing operations will all be created using the training set and then these steps will be applied to both the training and test set.\n\n# Pre Processing the Training Data\n\niris_recipe &lt;- \n  training(iris_split) %&gt;% # Note: Using TRAINING data !!\n  recipe(Species ~ .)      # Note: Outcomes ~ Predictors !!\n\n# The data contained in the `data` argument need not be the training set; this data is only used to catalog the names of the variables and their types (e.g. numeric, etc.).\n\nQ: How does the recipe “figure” out which are the outcomes and which are the predictors? A.The recipe command defines Outcomes and Predictors using the formula interface. Not clear how this recipe “figures” out which are the outcomes and which are the predictors, when we have not yet specified them…\nQ. Why is the recipe not agnostic to data set? Is that a meaningful question? A. The use of the training set in the recipe command is just to declare the variables and specify the roles of the data, nothing else. Roles are open-ended and extensible. From https://cran.r-project.org/web/packages/recipes/vignettes/Simple_Example.html :\n\nThis document demonstrates some basic uses of recipes. First, some definitions are required: - variables are the original (raw) data columns in a data frame or tibble. For example, in a traditional formula Y ~ A + B + A:B, the variables are A, B, and Y. - roles define how variables will be used in the model. Examples are: predictor (independent variables), response, and case weight. This is meant to be open-ended and extensible. - terms are columns in a design matrix such as A, B, and A:B. These can be other derived entities that are grouped, such as a set of principal components or a set of columns, that define a basis function for a variable. These are synonymous with features in machine learning. Variables that have predictor roles would automatically be main effect terms.\n\n\n# Apply the transformation steps\niris_recipe &lt;- iris_recipe %&gt;% \n  step_corr(all_predictors()) %&gt;% \n  step_center(all_predictors(), -all_outcomes()) %&gt;% \n  step_scale(all_predictors(), -all_outcomes()) %&gt;% \n  prep()\n\nThis has created the recipe() and prepped it too. We now need to apply it to our datasets:\n\nTake training data and bake() it to prepare it for modelling.\nDo the same for the testing set.\n\n\niris_training_baked &lt;- \n  iris_split %&gt;% \n  training() %&gt;% \n  bake(iris_recipe,.)\niris_training_baked\n\n# A tibble: 90 × 4\n   Sepal.Length Sepal.Width Petal.Width Species   \n          &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;     \n 1      -0.759        2.41      -1.48   setosa    \n 2      -0.878        1.02      -1.35   setosa    \n 3       0.307       -1.08       0.193  virginica \n 4      -0.759        1.02      -1.35   setosa    \n 5       1.14        -0.616      0.193  versicolor\n 6      -1.47         0.782     -1.22   setosa    \n 7      -0.522        1.95      -1.10   setosa    \n 8      -0.0487      -1.08      -0.0644 versicolor\n 9       2.20        -0.150      1.35   virginica \n10       0.780       -0.150      1.22   virginica \n# … with 80 more rows\n\niris_testing_baked &lt;- \n  iris_split %&gt;% \n  testing() %&gt;% \n  bake(iris_recipe,.)\niris_testing_baked \n\n# A tibble: 60 × 4\n   Sepal.Length Sepal.Width Petal.Width Species\n          &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n 1      -1.35         0.316       -1.35 setosa \n 2      -0.522        1.48        -1.35 setosa \n 3      -1.23        -0.150       -1.48 setosa \n 4      -1.83        -0.150       -1.48 setosa \n 5      -0.0487       2.18        -1.35 setosa \n 6      -0.167        1.71        -1.22 setosa \n 7      -0.878        1.71        -1.22 setosa \n 8      -0.522        0.782       -1.35 setosa \n 9      -0.878        1.48        -1.10 setosa \n10      -1.35         0.316       -1.35 setosa \n# … with 50 more rows\n\n\nIris Model Training using parsnip\n\nDifferent ML packages provide different interfaces (APIs ) to do the same thing (e.g random forests). The tidymodels package provides a consistent interface to invoke a wide variety of packages supporting a wide variety of models.\nThe parsnip package is a successor to caret.\nTo model with parsnip: 1. Pick a model : 2. Set the engine 3. Set the mode (if needed): Classification or Regression\nCheck here for models available in parsnip.\n\nMode: classification and regression in parsnip, each using a variety of models. ( Which Way). This defines the form of the output.\nEngine: The engine is the R package that is invoked by parsnip to execute the model. E.g glm, glmnet,keras.( How ) parsnip provides wrappers for models from these packages.\nModel: is the specific technique used for the modelling task. E.g linear_reg(), logistic_reg(), mars, decision_tree, nearest_neighbour…(What model).\n\nand models have: - hyperparameters: that are numerical or factor variables that tune the model ( Like the alpha beta parameters for Bayesian priors)\nWe can use the random forest model to classify the iris into species. Here Species is the Outcome variable and the rest are predictor variables. The random forest model is provided by the ranger package, to which tidymodels/parsnip provides a simple and consistent interface.\n\nlibrary(ranger)\niris_ranger &lt;- \n  rand_forest(trees = 100) %&gt;% \n  set_mode(\"classification\") %&gt;% \n  set_engine(\"ranger\") %&gt;% \n  fit(Species ~ ., data = iris_training_baked)\n\nranger can generate random forest models for classification, regression, survival( time series, time to event stuff). Extreme Forests are also supported, wherein all points in the dataset are used ( instead of bootstrap samples) along with feature bagging. We can also run the same model using the randomForest package:\n\nlibrary(randomForest,quietly = TRUE)\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:ranger':\n\n    importance\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\niris_rf &lt;- \n  rand_forest(trees = 100) %&gt;% \n  set_mode(\"classification\") %&gt;% \n  set_engine(\"randomForest\") %&gt;% \n  fit(Species ~ ., data = iris_training_baked)\n\nIris Predictions\nThe predict() function run against a parsnip model returns a prediction tibble. By default, the prediction variable is called .pred_class.\n\npredict(object = iris_ranger, new_data = iris_testing_baked) %&gt;%  \n  dplyr::bind_cols(iris_testing_baked) %&gt;% \n  glimpse()\n\nRows: 60\nColumns: 5\n$ .pred_class  &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n$ Sepal.Length &lt;dbl&gt; -1.35138822, -0.52239642, -1.23296082, -1.82509781, -0.04…\n$ Sepal.Width  &lt;dbl&gt; 0.31591693, 1.48118431, -0.15019002, -0.15019002, 2.18034…\n$ Petal.Width  &lt;dbl&gt; -1.3532022, -1.3532022, -1.4820786, -1.4820786, -1.353202…\n$ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n\nIris Classification Model Validation\nWe use metrics() function from the yardstick package to evaluate how good the model is.\n\npredict(iris_ranger, iris_testing_baked) %&gt;%\n  dplyr::bind_cols(iris_testing_baked) %&gt;% \n  yardstick::metrics(truth = Species, estimate = .pred_class)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.967\n2 kap      multiclass     0.950\n\n\nWe can also check the metrics for randomForest model:\n\npredict(iris_rf, iris_testing_baked) %&gt;%\n  dplyr::bind_cols(iris_testing_baked) %&gt;% \n  yardstick::metrics(truth = Species, estimate = .pred_class)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.95 \n2 kap      multiclass     0.925\n\n\nIris Per-Classifier Metrics\nWe can use the parameter type = \"prob\" in the predict() function to obtain a probability score on each prediction. TBD: How is this prob calculated? Possible answer: the Random Forest model outputs its answer by majority voting across n trees. Each of the possible answers( i.e. predictions) for a particular test datum gets a share of the vote, that represents its probability. Hence each dataum in the test vector can show a probability for the “winning” answer. ( Quite possibly we can get the probabilities for all possible outcomes for each test datum)\n\niris_ranger_probs &lt;- \n  predict(iris_ranger, iris_testing_baked, type = \"prob\") %&gt;%\n  dplyr::bind_cols(iris_testing_baked)\nglimpse(iris_ranger_probs)\n\nRows: 60\nColumns: 7\n$ .pred_setosa     &lt;dbl&gt; 0.99147619, 0.96175794, 0.94710119, 0.94710119, 0.730…\n$ .pred_versicolor &lt;dbl&gt; 0.005666667, 0.020416667, 0.031291667, 0.031291667, 0…\n$ .pred_virginica  &lt;dbl&gt; 0.002857143, 0.017825397, 0.021607143, 0.021607143, 0…\n$ Sepal.Length     &lt;dbl&gt; -1.35138822, -0.52239642, -1.23296082, -1.82509781, -…\n$ Sepal.Width      &lt;dbl&gt; 0.31591693, 1.48118431, -0.15019002, -0.15019002, 2.1…\n$ Petal.Width      &lt;dbl&gt; -1.3532022, -1.3532022, -1.4820786, -1.4820786, -1.35…\n$ Species          &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos…\n\niris_rf_probs &lt;- \n  predict(iris_rf, iris_testing_baked, type = \"prob\") %&gt;%\n  dplyr::bind_cols(iris_testing_baked)\nglimpse(iris_rf_probs)\n\nRows: 60\nColumns: 7\n$ .pred_setosa     &lt;dbl&gt; 0.99, 0.97, 0.95, 0.95, 0.73, 0.81, 0.98, 0.98, 0.98,…\n$ .pred_versicolor &lt;dbl&gt; 0.01, 0.00, 0.02, 0.02, 0.22, 0.14, 0.00, 0.02, 0.00,…\n$ .pred_virginica  &lt;dbl&gt; 0.00, 0.03, 0.03, 0.03, 0.05, 0.05, 0.02, 0.00, 0.02,…\n$ Sepal.Length     &lt;dbl&gt; -1.35138822, -0.52239642, -1.23296082, -1.82509781, -…\n$ Sepal.Width      &lt;dbl&gt; 0.31591693, 1.48118431, -0.15019002, -0.15019002, 2.1…\n$ Petal.Width      &lt;dbl&gt; -1.3532022, -1.3532022, -1.4820786, -1.4820786, -1.35…\n$ Species          &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos…\n\n# Tabulating the probabilities\nftable(iris_rf_probs$.pred_versicolor)\n\n 0 0.01 0.02 0.03 0.05 0.06 0.08 0.12 0.13 0.14 0.15 0.17 0.22 0.24 0.26 0.28 0.29 0.3 0.34 0.5 0.62 0.64 0.68 0.72 0.74 0.75 0.76 0.77 0.78 0.79 0.81 0.84 0.88 0.9 0.94 0.95 0.96 0.97 1\n                                                                                                                                                                                          \n 8    4    8    1    2    1    1    1    1    1    1    1    1    1    1    1    1   1    1   1    1    1    1    2    1    1    1    1    1    1    2    1    1   1    1    1    1    2 1\n\nftable(iris_rf_probs$.pred_virginica)\n\n 0 0.02 0.03 0.05 0.06 0.09 0.11 0.12 0.14 0.16 0.18 0.21 0.24 0.26 0.28 0.31 0.36 0.38 0.5 0.66 0.7 0.71 0.72 0.76 0.83 0.85 0.87 0.88 0.92 0.94 0.95 0.97 0.98 0.99\n                                                                                                                                                                     \n 8    6    8    4    2    1    1    1    1    1    2    1    1    2    1    1    1    1   1    1   1    1    1    1    1    1    1    1    1    1    2    1    1    1\n\nftable(iris_rf_probs$.pred_setosa)\n\n  0 0.01 0.02 0.05 0.11 0.12 0.68 0.73 0.81 0.95 0.97 0.98 0.99  1\n                                                                  \n 28    6    1    1    1    2    1    1    1    5    1    7    3  2\n\n\n\n### Iris Classifier: Gain and ROC Curves\n\nWe can plot gain and ROC curves for each of these models\n\n::: {.cell hash='Random-Forests_cache/html/Gain and ROC Curves `ranger`_4d61a502168be25c4ca5da6162ae106e'}\n\n```{.r .cell-code}\niris_ranger_probs %&gt;% \n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %&gt;% \n  glimpse()\n\nRows: 137\nColumns: 5\n$ .level          &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"set…\n$ .n              &lt;dbl&gt; 0, 2, 4, 5, 9, 10, 11, 12, 13, 17, 18, 19, 20, 21, 22,…\n$ .n_events       &lt;dbl&gt; 0, 2, 4, 5, 9, 10, 11, 12, 13, 17, 18, 19, 20, 21, 21,…\n$ .percent_tested &lt;dbl&gt; 0.000000, 3.333333, 6.666667, 8.333333, 15.000000, 16.…\n$ .percent_found  &lt;dbl&gt; 0.00000, 9.52381, 19.04762, 23.80952, 42.85714, 47.619…\n\niris_ranger_probs %&gt;% \n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %&gt;% \n  autoplot()\n\n\n\niris_ranger_probs %&gt;% \n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %&gt;% \n  glimpse()\n\nRows: 140\nColumns: 4\n$ .level      &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\"…\n$ .threshold  &lt;dbl&gt; -Inf, 0.000000000, 0.000625000, 0.001000000, 0.003958333, …\n$ specificity &lt;dbl&gt; 0.0000000, 0.0000000, 0.3589744, 0.4102564, 0.4615385, 0.4…\n$ sensitivity &lt;dbl&gt; 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0…\n\niris_ranger_probs %&gt;% \n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %&gt;% \n  autoplot()\n\n\n\n:::\n\niris_rf_probs %&gt;% \n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %&gt;% \n  glimpse()\n\nRows: 90\nColumns: 5\n$ .level          &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"set…\n$ .n              &lt;dbl&gt; 0, 2, 5, 12, 13, 18, 19, 20, 21, 23, 24, 25, 26, 32, 6…\n$ .n_events       &lt;dbl&gt; 0, 2, 5, 12, 13, 18, 19, 20, 21, 21, 21, 21, 21, 21, 2…\n$ .percent_tested &lt;dbl&gt; 0.000000, 3.333333, 8.333333, 20.000000, 21.666667, 30…\n$ .percent_found  &lt;dbl&gt; 0.000000, 9.523810, 23.809524, 57.142857, 61.904762, 8…\n\niris_rf_probs %&gt;% \n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %&gt;% \n  autoplot()\n\n\n\niris_rf_probs %&gt;% \n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %&gt;% \n  glimpse()\n\nRows: 93\nColumns: 4\n$ .level      &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\"…\n$ .threshold  &lt;dbl&gt; -Inf, 0.00, 0.01, 0.02, 0.05, 0.11, 0.12, 0.68, 0.73, 0.81…\n$ specificity &lt;dbl&gt; 0.0000000, 0.0000000, 0.7179487, 0.8717949, 0.8974359, 0.9…\n$ sensitivity &lt;dbl&gt; 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0…\n\niris_rf_probs %&gt;% \n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %&gt;% \n  autoplot()\n\n\n\n\nIris Classifier: Metrics\n\npredict(iris_ranger, iris_testing_baked, type = \"prob\") %&gt;% \n  bind_cols(predict(iris_ranger,iris_testing_baked)) %&gt;% \n  bind_cols(select(iris_testing_baked,Species)) %&gt;% \n  glimpse()\n\nRows: 60\nColumns: 5\n$ .pred_setosa     &lt;dbl&gt; 0.99147619, 0.96175794, 0.94710119, 0.94710119, 0.730…\n$ .pred_versicolor &lt;dbl&gt; 0.005666667, 0.020416667, 0.031291667, 0.031291667, 0…\n$ .pred_virginica  &lt;dbl&gt; 0.002857143, 0.017825397, 0.021607143, 0.021607143, 0…\n$ .pred_class      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos…\n$ Species          &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos…\n\npredict(iris_ranger, iris_testing_baked, type = \"prob\") %&gt;% \n  bind_cols(predict(iris_ranger,iris_testing_baked)) %&gt;% \n  bind_cols(select(iris_testing_baked,Species)) %&gt;% \n  yardstick::metrics(data = ., truth = Species, estimate = .pred_class, ... = .pred_setosa:.pred_virginica)\n\n# A tibble: 4 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    multiclass     0.967\n2 kap         multiclass     0.950\n3 mn_log_loss multiclass     0.221\n4 roc_auc     hand_till      0.987\n\n# And for the `randomForest`method\n\npredict(iris_rf, iris_testing_baked, type = \"prob\") %&gt;% \n  bind_cols(predict(iris_ranger,iris_testing_baked)) %&gt;% \n  bind_cols(select(iris_testing_baked,Species)) %&gt;% \n  glimpse()\n\nRows: 60\nColumns: 5\n$ .pred_setosa     &lt;dbl&gt; 0.99, 0.97, 0.95, 0.95, 0.73, 0.81, 0.98, 0.98, 0.98,…\n$ .pred_versicolor &lt;dbl&gt; 0.01, 0.00, 0.02, 0.02, 0.22, 0.14, 0.00, 0.02, 0.00,…\n$ .pred_virginica  &lt;dbl&gt; 0.00, 0.03, 0.03, 0.03, 0.05, 0.05, 0.02, 0.00, 0.02,…\n$ .pred_class      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos…\n$ Species          &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos…\n\npredict(iris_rf, iris_testing_baked, type = \"prob\") %&gt;% \n  bind_cols(predict(iris_ranger,iris_testing_baked)) %&gt;% \n  bind_cols(select(iris_testing_baked,Species)) %&gt;% \n  yardstick::metrics(data = ., truth = Species, estimate = .pred_class, ... = .pred_setosa:.pred_virginica)\n\n# A tibble: 4 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    multiclass     0.967\n2 kap         multiclass     0.950\n3 mn_log_loss multiclass     0.197\n4 roc_auc     hand_till      0.988"
  },
  {
    "objectID": "content/courses/ML4Artists/2-Regression/index.html",
    "href": "content/courses/ML4Artists/2-Regression/index.html",
    "title": "Basics of Machine Learning - Regression",
    "section": "",
    "text": "Introduction: Mixing Colours\nInterpolation:\n\nbetween TWO colours, both colours inclusive using a straight line between them\n\nbetween several different colours?\n\nby mixing “equal proportions” of each\nProportions based on “distance” from each colour\nOn a “plane” with these points\n\n\nSome Examples from Drama\n\nLegally Blonde:\n\n{{% youtube \"aVRUfPRUKtU\" %}}\n\nGreek Chorus Explained:\n\n{{% youtube \"orXPMdCU-6s\" %}}\n\nSutradhar in Indian Drama\n\nhttps://www.britannica.com/art/theater-building/Developments-in-Asia#ref463835\nDiscussion\n\nInterpolation\nExtrapolation\nCalculating the optimum values for m and c, given x and y**, for \\(y = mx + c\\)\n\nPlaying with Orange: Paint My Data\n\n\n\n\n\nLet us “draw inspiration” from the picture above, and see if we can replicate it. We will fire up Orange, paint some data and see if we can fit a linear regression ML model to it.\nHere is the Orange file for you to download. Open this file in Orange.\nRegression Plane\n\n\n\n\n\nInteractive Regression Plane\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/80-BayesianThinking/bayesian.html",
    "href": "content/courses/MathModelsDesign/Modules/80-BayesianThinking/bayesian.html",
    "title": "Bayesian Thinking",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/40-IntroNetworks/intro-networks.html",
    "href": "content/courses/MathModelsDesign/Modules/40-IntroNetworks/intro-networks.html",
    "title": "I am Insta Famous",
    "section": "",
    "text": "Do you think your friends have more friends than you have? Do you think that you are outside the herd, and that what you think or do is from your own mind?\nAnyways…\n{{% vimeo \"191074419\" %}}"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/40-IntroNetworks/intro-networks.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/40-IntroNetworks/intro-networks.html#introduction",
    "title": "I am Insta Famous",
    "section": "",
    "text": "Do you think your friends have more friends than you have? Do you think that you are outside the herd, and that what you think or do is from your own mind?\nAnyways…\n{{% vimeo \"191074419\" %}}"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/40-IntroNetworks/intro-networks.html#activities",
    "href": "content/courses/MathModelsDesign/Modules/40-IntroNetworks/intro-networks.html#activities",
    "title": "I am Insta Famous",
    "section": "Activities",
    "text": "Activities\nActivity-1: Secret Santa Game\nLet us play this in the vanilla way: Paper chits with names in a bin and drawing them in turn. What can go wrong with this?\nShould we use this instead? https://www.drawnames.com.sg/secret-santa-generator\nDiscussion: Nodes, Links, Link Directionality, Connected and Disconnected Networks\nActivity-2: Barabasi Cocktail Party Game\nThis is a game “invented” by Alberto-Laszlo Barabasi, a Network Science pioneer and expert, who has written a wonderful, and wonderfully acessible, book on Network Science, available online http://networksciencebook.com/\nDiscussion: Network Mechanisms, Information Flow, Giant Component,Emergence\nActivity-3: Indian Surnames Game\n\nHow many common India surnames do we know? Let us write them on the board.\nEach of us will now look at each surname and recollect how many people they know with that surname.\nWrite down the score for each surname.\nLet’s plot this on (yet another ) network!!\n\nDiscussion: Node Degree, Giant Component?  Small Worlds?  Multi-Link network, Link Values or Costs\nActivity-4: Way-Spotting Game\nNow that we have an idea of nodes, links and costs, let us get an experience of some more network science ideas:\n\nMake groups of 3.\nHead over to http://www.wayspotting.com/index.html\n\nPlay!! Make a note of your route each time ( your “traversal” of the network)\nNote if you can see the following:\n\n\n\nFrequently Used Nodes\nFrequently used Links\n\n\n\nSee here for more info: https://medium.com/@ran_katzir/teaching-network-science-using-board-games-f78489a3b3bd\n\n\nDiscussion: Network Traversal, Node Degree, Centrality, Betweenness, Link Values or Costs\nActivity-5: Hi, I am Kevin Bacon, SMI Foundation 2022 Batch\nLet us find a Keven Bacon in SMI Foundation Studies Programme!!\n\nCollect friends Data from across college/class, import and plot, analyze and comment\nUse this online tool at DataBasic.io https://databasic.io to Connect the Dots, OR\nEven more fun at at GraphCommons https://graphcommons.com/graphs/new\n\n\nDiscussion: Node Degree, Centrality, Betweenness, Link Values or Costs\nActivity-6: Can you Introduce me to Chandler, again?\n\nTake your favourite Literary Work / TV Serial / Movie and create a Network Database for it.\nVisualize it either with or without tech tools From Teach Engineering, this Activity Sheet https://www.teachengineering.org/activities/view/uno_graphtheory_lesson01_activity2\n\nCan also use Graph Comicshttps://aviz.fr/~bbach/graphcomics/\n\n\nDiscussion: Networks are everywhere, Cannot \"unsee\" them, You are a node and you are a link..are you?"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/40-IntroNetworks/intro-networks.html#references",
    "href": "content/courses/MathModelsDesign/Modules/40-IntroNetworks/intro-networks.html#references",
    "title": "I am Insta Famous",
    "section": "References",
    "text": "References\n\nDmitry Zinoniev, Network Science Intro Slides, https://www.slideshare.net/DmitryZinoviev/workshop-20212296\nMark Newman, The Physics of Networks,Read the PDF\nA Network oriented short story. Frigyes Karinthy, “Chains”. Read PDF\nWho told you about Srishti? Where? Read Mark Granovetter, The Strength of Weak Ties, https://www.cs.cmu.edu/~jure/pub/papers/granovetter73ties.pdf"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/160-Thinking/index.html",
    "href": "content/courses/MathModelsDesign/Modules/160-Thinking/index.html",
    "title": "Thinking, Fast and Slow with Daniel Kahneman",
    "section": "",
    "text": "We will play several short games followed by discussions. These games may bring to light some of our Cognitive Biases and see how they affect us, and especially as we try to function as Artists/Designers/Creators.\nThere will be short readings that follow after each game.\nBut first, let us see how frail/fragile/fallible… we all are:\n\n\n{{% youtube \"9Il_D3Xt9W0\" %}}\n\n\n\nRight! On to our first little fallibility!\n\n\n\n{{% youtube \"IGQmdoK_ZfY\" %}}\n\n\n\nTest: PPT\n\nShort Reading: PDF\n\n\nTool: PDF\n\nReading: Here is a short reading on Exaggerated Emotional Coherence, also known as the Halo Effect** Download PDF **\n\nYou have to Stick the lighted candle to the Wall in such a way that the melting wax does not drop on to the floor.\n\nLook at the graph below: does it remind you of something you know very well?\n\n\n\n\n\nWhat does this graph represent?\nLet us pretend we are part of this graph and see where our Problem Formulating Skills take us!"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/160-Thinking/index.html#can-you-see-straight",
    "href": "content/courses/MathModelsDesign/Modules/160-Thinking/index.html#can-you-see-straight",
    "title": "Thinking, Fast and Slow with Daniel Kahneman",
    "section": "",
    "text": "We will play several short games followed by discussions. These games may bring to light some of our Cognitive Biases and see how they affect us, and especially as we try to function as Artists/Designers/Creators.\nThere will be short readings that follow after each game.\nBut first, let us see how frail/fragile/fallible… we all are:\n\n\n{{% youtube \"9Il_D3Xt9W0\" %}}\n\n\n\nRight! On to our first little fallibility!\n\n\n\n{{% youtube \"IGQmdoK_ZfY\" %}}\n\n\n\nTest: PPT\n\nShort Reading: PDF\n\n\nTool: PDF\n\nReading: Here is a short reading on Exaggerated Emotional Coherence, also known as the Halo Effect** Download PDF **\n\nYou have to Stick the lighted candle to the Wall in such a way that the melting wax does not drop on to the floor.\n\nLook at the graph below: does it remind you of something you know very well?\n\n\n\n\n\nWhat does this graph represent?\nLet us pretend we are part of this graph and see where our Problem Formulating Skills take us!"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/160-Thinking/index.html#discussion",
    "href": "content/courses/MathModelsDesign/Modules/160-Thinking/index.html#discussion",
    "title": "Thinking, Fast and Slow with Daniel Kahneman",
    "section": "Discussion",
    "text": "Discussion\n\nProblems and Contradictions\nAll Available Resources\n\nAssumptions and Functional Fixedness\n\n\n\nA comparable switch of attention occurs in an old joke about a worker in a high security factory, in which the employees were carefully watched when they left at the end of their work day. On a particular day, this worker was stopped at the factory gate as he walked out with a wheelbarrow full of styrofoam packing peanuts. He explained that he had salvaged these from the trash, and was planning to use them in shipping gifts to his grandchildren. Searching through this packing material, the guards found nothing, and so they let the man go home. The following week the same thing happened, and the worker was again stopped. But he offered the very same story, and when the guards searched through the packing peanuts and found nothing, he was allowed to leave. But this continued, week after week, until the guards could no longer believe that one person would want or could make use of so much packing material. Finally, the man was held for interrogation, at which time he admitted that he had absolutely no use for packing peanuts - and that, all these weeks, he had been stealing wheelbarrows.\n\n\nHearing this joke, I am reminded of the phrase “part and parcel”, which is a rough equivalent of “figure and ground”, the Gestalt Principles. Throughout most of it, the packing peanuts occupy center stage as figure (part), while the wheelbarrows (which function merely as containers) are completely ignored as innocuous ground (parcel). At the end of the joke, there is an unexpected twist, a switch of emphasis, a recentering, when we learn that the parcel is really the part.\n\nThis should also remind us of the Guilford Alternative Uses Exercise that we did, where we forced ourselves to leave the “regular use” of an object behind and think of it as serving quite another function.\nBias on TV\nLet’s find some of these ideas in our favourite Episode of one Season of your favourite show and tell everybody with a poster!"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/160-Thinking/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/160-Thinking/index.html#references",
    "title": "Thinking, Fast and Slow with Daniel Kahneman",
    "section": "References",
    "text": "References\n\nThe Halo Effect, https://explorable.com/halo-effect\nNisbett, R. E., & Wilson, T. D. (1977). The halo effect: Evidence for unconscious alteration of judgments. Journal of Personality and Social Psychology, 35(4), 250–256. https://doi.org/10.1037/0022-3514.35.4.250 Download PDF\nBayesian Thinking Tutorial https://arbital.com/p/bayes_frequency_diagram/?l=55z&pathId=86923"
  },
  {
    "objectID": "content/courses/MathModelsDesign/listing.html",
    "href": "content/courses/MathModelsDesign/listing.html",
    "title": "Math Models in Design",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\n\n\n\n🕹 Iterated Games!\n\n\n4 min\n\n\n\n\n\n\n\nCoordination Games and Schelling Focus Points\n\n\n2 min\n\n\n\n\n\n\n\nI am Insta Famous\n\n\n3 min\n\n\n\n\n\n\n\nEating Mangoes with Hamlet\n\n\n14 min\n\n\n\n\n\n\n\nBayesian Thinking\n\n\n1 min\n\n\n\n\n\n\n\nThinking, Fast and Slow with Daniel Kahneman\n\n\n6 min\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/10-Business-Report-using-Flextable-in-R/index.html",
    "href": "content/courses/Analytics/Workflow/Modules/10-Business-Report-using-Flextable-in-R/index.html",
    "title": "Using FlexDashboard in R",
    "section": "",
    "text": "R Tutorial"
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/10-Business-Report-using-Flextable-in-R/index.html#references",
    "href": "content/courses/Analytics/Workflow/Modules/10-Business-Report-using-Flextable-in-R/index.html#references",
    "title": "Using FlexDashboard in R",
    "section": "References",
    "text": "References\n\nFlexdashboard Basics https://rstudio.github.io/flexdashboard/articles/flexdashboard.html\nFlexdashboard Examples https://rstudio.github.io/flexdashboard/articles/examples.html\nShannon Haymond,Create laboratory business intelligence dashboards for free using R: A tutorial using the flexdashboard package, Journal of Mass Spectrometry and Advances in the Clinical Lab, Volume 23, 2022,Pages 39-43, ISSN 2667-145X, https://doi.org/10.1016/j.jmsacl.2021.12.002.\nhttps://posit.co/blog/flexdashboard-easy-interactive-dashboards-for-r/"
  },
  {
    "objectID": "content/courses/Analytics/Tools/listing.html",
    "href": "content/courses/Analytics/Tools/listing.html",
    "title": "Tools and Software",
    "section": "",
    "text": "🐉 Introduction to R and RStudio\n\n\n\n\n\n\n\n\n\n10 min\n\n\n\n\n\n\n  \n\n\n\n\n🐉 Introduction to Radiant\n\n\n\n\n\n\n\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\n🐉 Introduction to Orange\n\n\n\n\n\n\n\n\n\n2 min\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "content/courses/Analytics/Tools/20-Intro-to-Radiant/radiant.html",
    "href": "content/courses/Analytics/Tools/20-Intro-to-Radiant/radiant.html",
    "title": "🐉 Introduction to Radiant",
    "section": "",
    "text": "Radiant is an open-source platform-independent browser-based interface for business analytics in R.\nKey features\n\nExplore: Quickly and easily summarize, visualize, and analyze your data\nCross-platform: It runs in a browser on Windows, Mac, and Linux\nReproducible: Recreate results and share work with others as a state file or an Rmarkdown report\nProgramming: Integrate Radiant’s analysis functions with your own R-code\nContext: Data and examples focus on business applications\n\nRadiant can be used for a variety of tasks\n\nProbability and Stats\nData visualization\nMachine Learning\nData mining\nReport Generation.\n\nRadiant Workflows can also be exported to R/RStudio easily."
  },
  {
    "objectID": "content/courses/Analytics/Tools/20-Intro-to-Radiant/radiant.html#introduction-to-radiant",
    "href": "content/courses/Analytics/Tools/20-Intro-to-Radiant/radiant.html#introduction-to-radiant",
    "title": "🐉 Introduction to Radiant",
    "section": "",
    "text": "Radiant is an open-source platform-independent browser-based interface for business analytics in R.\nKey features\n\nExplore: Quickly and easily summarize, visualize, and analyze your data\nCross-platform: It runs in a browser on Windows, Mac, and Linux\nReproducible: Recreate results and share work with others as a state file or an Rmarkdown report\nProgramming: Integrate Radiant’s analysis functions with your own R-code\nContext: Data and examples focus on business applications\n\nRadiant can be used for a variety of tasks\n\nProbability and Stats\nData visualization\nMachine Learning\nData mining\nReport Generation.\n\nRadiant Workflows can also be exported to R/RStudio easily."
  },
  {
    "objectID": "content/courses/Analytics/Tools/20-Intro-to-Radiant/radiant.html#installing-radiant",
    "href": "content/courses/Analytics/Tools/20-Intro-to-Radiant/radiant.html#installing-radiant",
    "title": "🐉 Introduction to Radiant",
    "section": "Installing Radiant",
    "text": "Installing Radiant\nYou can download and install Radiant from here:\nhttps://radiant-rstats.github.io/docs/install.html\n\n\n\n\n\n\nImportant\n\n\n\nNOTE: This automatically installs R, RStudio, and Radiant on your machine. This is going to be convenient when we start working in R too!\nIt also installs Latex, which allows us to create crisp PDF reports of our analyses.\nThe version of R may not be the latest one, though…"
  },
  {
    "objectID": "content/courses/Analytics/Tools/20-Intro-to-Radiant/radiant.html#basic-tutorials-with-radiant",
    "href": "content/courses/Analytics/Tools/20-Intro-to-Radiant/radiant.html#basic-tutorials-with-radiant",
    "title": "🐉 Introduction to Radiant",
    "section": "Basic Tutorials with Radiant",
    "text": "Basic Tutorials with Radiant\nAll the Tutorials are available on Youtube; the links to individual videos are on the page below\nhttps://radiant-rstats.github.io/docs/radiant-tutorial-series.html"
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/30-SimplexMethodExcel/index.html",
    "href": "content/courses/Analytics/Prescriptive/Modules/30-SimplexMethodExcel/index.html",
    "title": "📅 The Simplex Method - In Excel",
    "section": "",
    "text": "Let us take the same problem as before:\n\\[\nMaximise\\ 7.75x_1 + 10x_2 \\\\\n\\] \\[\nSubject\\ to \\\\\n  \\begin{cases}\n    C1: -3x_1 + 2x_2 &&lt;= 3 \\\\\n    C2: 2x_1 + 4x_2 &&lt;= 27 \\\\\n    C3: 9x_1 + 10x_2 &&lt;= 90 \\\\\n    x_1, x_2 &gt;= 0\n  \\end{cases}\n\\]"
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/30-SimplexMethodExcel/index.html#using-the-excel-solver-add-in",
    "href": "content/courses/Analytics/Prescriptive/Modules/30-SimplexMethodExcel/index.html#using-the-excel-solver-add-in",
    "title": "📅 The Simplex Method - In Excel",
    "section": "",
    "text": "Let us take the same problem as before:\n\\[\nMaximise\\ 7.75x_1 + 10x_2 \\\\\n\\] \\[\nSubject\\ to \\\\\n  \\begin{cases}\n    C1: -3x_1 + 2x_2 &&lt;= 3 \\\\\n    C2: 2x_1 + 4x_2 &&lt;= 27 \\\\\n    C3: 9x_1 + 10x_2 &&lt;= 90 \\\\\n    x_1, x_2 &gt;= 0\n  \\end{cases}\n\\]"
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/30-SimplexMethodExcel/index.html#procedure",
    "href": "content/courses/Analytics/Prescriptive/Modules/30-SimplexMethodExcel/index.html#procedure",
    "title": "📅 The Simplex Method - In Excel",
    "section": "Procedure",
    "text": "Procedure\n\nSet up an Excel sheet as shown in the picture below. We enter in the objective function and the constraints in tabular form as shown:\n\n\n\n\n\n\n\n\n\n\nNext we invoke the Solver Add-in: (Data -&gt; Solver):\n\n\n\n\n\n\n\n\n\n\nWe set up the Solver for our problem as follows: Hit the SOLVE button.\n\n\n\n\n\n\n\n\n\n\nChoose to have all the three kinds of Reports from Solver (Answers, Sensitivity, and Limits).\n\n\n\n\n\n\n\n\n\nThis will create three new tabs which give additional information on:\n- How “centered” the solution is, or is it sensitive to variations of some parameters\n- How much slack do the individual constraints still have, at the end\nWe will discuss this in class!\nThe complete Excel file is here for your reference."
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html",
    "href": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html",
    "title": "📐 Intro to Linear Programming",
    "section": "",
    "text": "What is Linear Programming?"
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#introduction",
    "href": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#introduction",
    "title": "📐 Intro to Linear Programming",
    "section": "",
    "text": "What is Linear Programming?"
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#demonstration-of-level-curve",
    "href": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#demonstration-of-level-curve",
    "title": "📐 Intro to Linear Programming",
    "section": "Demonstration of Level Curve",
    "text": "Demonstration of Level Curve"
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#linear-programming-solver",
    "href": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#linear-programming-solver",
    "title": "📐 Intro to Linear Programming",
    "section": "Linear Programming Solver",
    "text": "Linear Programming Solver"
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#linear-programming-in-3d-view",
    "href": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#linear-programming-in-3d-view",
    "title": "📐 Intro to Linear Programming",
    "section": "Linear Programming in 3D view",
    "text": "Linear Programming in 3D view"
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#linear-programming-interactive",
    "href": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#linear-programming-interactive",
    "title": "📐 Intro to Linear Programming",
    "section": "Linear Programming Interactive",
    "text": "Linear Programming Interactive\nLet us say we have a Linear Programming problem with 3 variables: We define the model:\n\\[\nMaximise : 20x_1 + 10x_2 + 15x_3\\\\\nSubject \\ to \\\\\n\\\\\nx_1 + x_2 + x_3 &lt;= 10\\\\\n3x_1 + x_3 &lt;= 24\n\\]\nHere is the interactive LP Polytope:"
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#references",
    "href": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#references",
    "title": "📐 Intro to Linear Programming",
    "section": "References",
    "text": "References\n\nVirginia Postrel, Operations Everything, Boston Globe, Hune 27, 2004. http://archive.boston.com/news/globe/ideas/articles/2004/06/27/operation_everything?pg=full"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/50-ModelTimeSeries/index.html",
    "href": "content/courses/Analytics/Predictive/Modules/50-ModelTimeSeries/index.html",
    "title": "🕔 Modelling Time Series",
    "section": "",
    "text": "#|label:  setup\n#|include: TRUE\n\n# knitr::opts_chunk$set(echo = TRUE,message = FALSE, warning = FALSE)\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(mosaic)\n\nlibrary(tsibble)\nlibrary(feasts)\nlibrary(fable)\nlibrary(sweep) # Tidy forecast Model objects\nlibrary(timetk)\nlibrary(forecast)\nlibrary(prophet)\n\nlibrary(fontawesome)"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/50-ModelTimeSeries/index.html#setup-the-packages",
    "href": "content/courses/Analytics/Predictive/Modules/50-ModelTimeSeries/index.html#setup-the-packages",
    "title": "🕔 Modelling Time Series",
    "section": "",
    "text": "#|label:  setup\n#|include: TRUE\n\n# knitr::opts_chunk$set(echo = TRUE,message = FALSE, warning = FALSE)\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(mosaic)\n\nlibrary(tsibble)\nlibrary(feasts)\nlibrary(fable)\nlibrary(sweep) # Tidy forecast Model objects\nlibrary(timetk)\nlibrary(forecast)\nlibrary(prophet)\n\nlibrary(fontawesome)"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/50-ModelTimeSeries/index.html#introduction",
    "href": "content/courses/Analytics/Predictive/Modules/50-ModelTimeSeries/index.html#introduction",
    "title": "🕔 Modelling Time Series",
    "section": "\n Introduction",
    "text": "Introduction\nIn this module we will look at modelling of time series. We will start with the simplest of exponential models and go all the way through ARIMA and forecasting with Prophet.\nFirst, some terminology!"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/50-ModelTimeSeries/index.html#additive-and-multiplicative-time-series-models",
    "href": "content/courses/Analytics/Predictive/Modules/50-ModelTimeSeries/index.html#additive-and-multiplicative-time-series-models",
    "title": "🕔 Modelling Time Series",
    "section": "Additive and Multiplicative Time Series Models",
    "text": "Additive and Multiplicative Time Series Models\nAdditive Time Series can be represented as:\n\\[\nY_t = S_t + T_t + ϵ_t\n\\]\nMultiplicative Time Series can be described as:\n\\[\nY_t = S_t × T_t × ϵ_t\n\\]\nLet us consider a Multiplicative Time Series, pertaining to sales of souvenirs at beaches in Australia: The time series looks like this:\n\n\n\n\n\n\nNote that along with the trend, the amplitude of both seasonal and noise components are also increasing in a multiplicative here !! A multiplicative time series can be converted to additive by taking a log of the time series."
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/50-ModelTimeSeries/index.html#stationarity",
    "href": "content/courses/Analytics/Predictive/Modules/50-ModelTimeSeries/index.html#stationarity",
    "title": "🕔 Modelling Time Series",
    "section": "Stationarity",
    "text": "Stationarity\nA time series is said to be stationary if it holds the following conditions true:\n\nThe mean value of time-series is constant over time, which implies,the trend component is nullified/constant.\nThe variance does not increase over time.\nSeasonality effect is minimal.\n\nThis means it is devoid of trend or seasonal patterns, which makes it looks like a random white noise irrespective of the observed time interval.( i.e. self-similar and fractal)"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/50-ModelTimeSeries/index.html#a-bit-of-forecasting",
    "href": "content/courses/Analytics/Predictive/Modules/50-ModelTimeSeries/index.html#a-bit-of-forecasting",
    "title": "🕔 Modelling Time Series",
    "section": "A Bit of Forecasting?",
    "text": "A Bit of Forecasting?\nWe are always interested in the future. We will do this in three ways:\n\nuse Simple Exponential Smoothing\nuse a package called forecast to fit an ARIMA (Autoregressive Moving Average Integrated Model) model to the data and make predictions for weekly sales;\nAnd do the same using a package called prophet.\n\nForecasting using Exponential Smoothing\nFor example, the file contains total annual rainfall in inches for London, from 1813-1912 (original data from Hipel and McLeod, 1994).\n\nrain &lt;- scan(\"https://robjhyndman.com/tsdldata/hurst/precip1.dat\", skip = 2)\nrainseries &lt;- ts(rain, start = c(1813))\nplot(rainseries)\n\n\n\n\nThere is a nearly constant value of about 25 around which there are random fluctuations and it seems to be an additive model. How can we make forecasts with this time series?\nA deliberate detour:\nLet’s see some quick notation to aid understanding: Much of smoothing is based on the high school concept of a straight line, \\(y = m*x + c\\).\nIn the following, we choose to describe the models with:\n\n\n\\(y\\) : the actual values in the time series\n\n\\(\\hat y\\) : our predictions from whichever model we create\n\n\\(l\\) : a level or mean as forecast;\n\n\\(b\\) : a trend variable; akin to the slope in the straight line equation;\n\n\\(s\\) : seasonal component of the time series. Note that this is a set of values that stretch over one cycle of the time series.\n\nIn Exponential Smoothing and Forecasting, we make three models of increasing complexity:\n\nSimple Exponential Model: Here we deal only with the mean or level aspect of the (decomposed) time series and make predictions with that.\nHolt Model: Here we use the level and the trend from the decomposed time series for predictions\nHolt-Winters Model: Here we use the level, the trend, and the seasonal component from the decomposed time series for predictions.\n\n\n[&lt;start&gt;st]-&gt;[&lt;input&gt;input]\n[&lt;input&gt; input]-&gt;[&lt;package&gt; Time  Series|Decomposition]\n[&lt;package&gt; Time  Series|Decomposition]-&gt;[&lt;component&gt; Mean/Level]\n[&lt;package&gt; Time  Series|Decomposition]-&gt;[&lt;component&gt; Slope/Trend]\n[&lt;package&gt; Time  Series|Decomposition]-&gt;[&lt;component&gt; Seasonal]\n\n//Simple Exponential Smoothing\n[&lt;component&gt; Mean/Level]-&gt;[Delay A1]\n[Delay A1]-&gt;[Delay A2]\n[Delay A2]-&gt;[Delay A3]\n[Delay A3]...-&gt;...[Delay AN]\n[Delay A1]-&gt;[&lt;state&gt; A1]\n[Delay A2]-&gt;[&lt;state&gt; A2]\n[Delay A3]-&gt;[&lt;state&gt; A3]\n[Delay AN]-&gt;[&lt;state&gt; AN]\n[&lt;state&gt; AN]---([&lt;note&gt; $$alpha(1-alpha)^i$$]\n\n[&lt;state&gt; A1]-&gt;[&lt;state&gt; Add1]\n[&lt;state&gt; A2]-&gt;[&lt;state&gt; Add1]\n[&lt;state&gt; A3]-&gt;[&lt;state&gt; Add1]\n[&lt;state&gt; AN]-&gt;[&lt;state&gt; Add1]\n[&lt;state&gt; Add1]-&gt;[&lt;end&gt; Output]\n\n//Holt \n[&lt;component&gt; Slope/Trend]-&gt;[Delay B1]\n[Delay B1]-&gt;[Delay B2]\n[Delay B2]-&gt;[Delay B3]\n[Delay B3]...-&gt;...[Delay BN]\n[Delay B1]-&gt;[&lt;state&gt; B1]\n[Delay B2]-&gt;[&lt;state&gt; B2]\n[Delay B3]-&gt;[&lt;state&gt; B3]\n[Delay BN]-&gt;[&lt;state&gt; BN]\n[&lt;state&gt; BN]---([&lt;note&gt; $$beta(1-beta)^i$$]\n[&lt;state&gt; B1]-&gt;[&lt;state&gt; Add2]\n[&lt;state&gt; B2]-&gt;[&lt;state&gt; Add2]\n[&lt;state&gt; B3]-&gt;[&lt;state&gt; Add2]\n[&lt;state&gt; BN]-&gt;[&lt;state&gt; Add2]\n[&lt;state&gt; Add2]-&gt;[&lt;end&gt; Output]\n\n// Holt Winters\n[&lt;component&gt; Seasonal]-&gt;[Delay C1]\n[Delay C1]-&gt;[Delay C2]\n[Delay C2]-&gt;[Delay C3]\n[Delay C3]...-&gt;...[Delay CN]\n[Delay C1]-&gt;[&lt;state&gt; C1]\n[Delay C2]-&gt;[&lt;state&gt; C2]\n[Delay C3]-&gt;[&lt;state&gt; C3]\n[Delay CN]-&gt;[&lt;state&gt; CN]\n[&lt;state&gt; CN]---([&lt;note&gt; $$gamma(1-gamma)^i$$]\n[&lt;state&gt; C1]-&gt;[&lt;state&gt; Add3]\n[&lt;state&gt; C2]-&gt;[&lt;state&gt; Add3]\n[&lt;state&gt; C3]-&gt;[&lt;state&gt; Add3]\n[&lt;state&gt; CN]-&gt;[&lt;state&gt; Add3]\n[&lt;state&gt; Add3]-&gt;[&lt;end&gt; Output]\n\n// Final Output\n[&lt;end&gt; Output]-&gt;[&lt;receiver&gt; Forecast]\n\n\n\n\n\nSimple Smoothing is smoothing based forecasting using just the level ( i.e. mean) of the Time Series to make forecasts.\nDouble exponential smoothing, or Holt Smoothing Model, is just exponential smoothing applied to both level and trend.\nThe idea behind triple exponential smoothing, or the Holt-Winters Smoothing Model, is to apply exponential smoothing to the seasonal components in addition to level and trend.\nWhat does “Exponential” mean?\nAll three models use memory: at each time instant in the Time Series, a set of past values, along with the present sample is used to make a prediction of the relevant parameter ( level / slope / seasonal). These are then added together to make the forecast.\nThe memory in each case controlled by a parameter: alpha for the estimate of the level beta for the slope estimate, and gamma for the seasonal component estimate at the current time point. All these parameters are between 0 and 1. The model takes a weighted average of past values of each parameter. The weights are derived in the form of \\(\\alpha(1-\\alpha)^i\\), where \\(i\\) defines how old the sample is compared to the present one, thus forming a set of weights that decrease exponentially with delay. Values of \\(\\alpha, \\beta. \\gamma\\) that are close to 0 mean that significant weightage is placed on observations in the past.(Memory is “stronger”). To express this in mathematical notation we now need three equations: one for level, one for the trend and one to combine the level and trend to get the expected \\(\\hat y\\).\nTo make forecasts using simple exponential smoothing in R, we can use the HoltWinters() function in R, or the forecast::ets() function from forecasts. This latter function is more powerful.\n\nargs(HoltWinters)\n\nfunction (x, alpha = NULL, beta = NULL, gamma = NULL, seasonal = c(\"additive\", \n    \"multiplicative\"), start.periods = 2, l.start = NULL, b.start = NULL, \n    s.start = NULL, optim.start = c(alpha = 0.3, beta = 0.1, \n        gamma = 0.1), optim.control = list()) \nNULL\n\nargs(forecast::ets)\n\nfunction (y, model = \"ZZZ\", damped = NULL, alpha = NULL, beta = NULL, \n    gamma = NULL, phi = NULL, additive.only = FALSE, lambda = NULL, \n    biasadj = FALSE, lower = c(rep(1e-04, 3), 0.8), upper = c(rep(0.9999, \n        3), 0.98), opt.crit = c(\"lik\", \"amse\", \"mse\", \"sigma\", \n        \"mae\"), nmse = 3, bounds = c(\"both\", \"usual\", \"admissible\"), \n    ic = c(\"aicc\", \"aic\", \"bic\"), restrict = TRUE, allow.multiplicative.trend = FALSE, \n    use.initial.values = FALSE, na.action = c(\"na.contiguous\", \n        \"na.interp\", \"na.fail\"), ...) \nNULL\n\n\nTo use HoltWinters() for simple exponential smoothing, we need to set the parameters beta=FALSE and gamma=FALSE in the HoltWinters() function (the beta and gamma parameters are used for double exponential smoothing, or triple exponential smoothing.\nTo use forecast::ets, we set the model argument to “ANN”, “AAN”, and “AAA” respectively for each of the three smoothing models.\nNote: The HoltWinters() function returns a list variable, that contains several named elements.\n\nrainseriesforecasts &lt;- forecast::ets(rainseries, model = \"ANN\")\n# class(rainseriesforecasts)\n# str(rainseriesforecasts)\nplot(rainseriesforecasts)\n\n\n\nplot(forecast(rainseriesforecasts, 10))\n\n\n\n\nARIMA\nWe can also use past trends and seasonality in the data to make predictions about the future using the forecast package. Here we use an auto ARIMA model to guess at the trend in the time series. Then we use that model to forecast a few periods into the future.\nMathematically an ARIMA model can be shown as follows:\n\n\n\n\n\n\nWe will use the familiar Walmart Sales dataset, and try to predict weekly sales for one of the Departments.\n\ndata(\"walmart_sales_weekly\")\nwalmart_wide &lt;- walmart_sales_weekly %&gt;% \n  pivot_wider(., id_cols = c(Date), \n              names_from = Dept, \n              values_from = Weekly_Sales,\n              names_prefix = \"Sales_\")\n\n## forecast::auto.arima needs a SINGLE time series, so we pick one, Dept95\nsales_95_ts &lt;- walmart_wide %&gt;% \n  select(Sales_95) %&gt;% \n  ts(start = c(2010,1), end = c(2012,52),frequency = 52)\nsales_95_ts\n\nTime Series:\nStart = c(2010, 1) \nEnd = c(2012, 52) \nFrequency = 52 \n  [1] 106690.06 111390.36 107952.07 103652.58 112807.75 112048.41 117716.13\n  [8] 113117.35 111466.37 116770.82 126341.84 110204.77 107648.14 125592.28\n [15] 120247.90 120036.99 121902.19 133056.97 131995.00 134118.05 120172.47\n [22] 124821.44 126241.20 121386.73 116256.35 108781.57 131128.96 131288.83\n [29] 124601.48 117929.58 124220.10 125027.49 124372.90 114702.69 113009.41\n [36] 120764.22 123510.99 110052.15 105793.40 110332.92 110209.31 107544.02\n [43] 106015.41 100834.31 111384.36 116521.67 121695.13  93676.95 107317.32\n [50] 109955.90 103724.16  99043.34 114270.08 117548.75 112165.80 107742.95\n [57] 116225.68 120621.32 123405.41 122280.13 112905.09 126746.25 126834.30\n [64] 118632.26 111764.31 120882.84 124953.94 112581.20 119815.67 135260.49\n [71] 136364.46 135197.63 121814.84 128054.88 133213.04 127906.50 121483.11\n [78] 117284.94 138538.47 138567.10 133260.84 122721.92 130446.34 133762.77\n [85] 133939.40 116165.28 115663.78 132805.42 125954.30 116931.34 108018.21\n [92] 114793.92 115047.16 113966.34 112688.97 102798.99 119053.80 120721.07\n [99] 125041.39  93358.91 116427.93 118685.12 113021.23 102202.04 115507.25\n[106] 125038.09 119807.63 110870.94 118406.27 125840.82 132318.50 117030.73\n[113] 127706.00 137958.76 129438.22 123172.79 118589.44 130920.36 131341.85\n[120] 129031.19 127603.00 130573.37 139857.10 140806.36 124594.40 131935.56\n[127] 148798.05 129724.74 126861.49 121030.79 134832.22 137408.20 136264.68\n[134] 118845.34 124741.33 140657.40 128542.73 119121.35 115326.47 127009.22\n[141] 124559.93 123346.24 117375.38 106690.06 111390.36 107952.07 103652.58\n[148] 112807.75 112048.41 117716.13 113117.35 111466.37 116770.82 126341.84\n[155] 110204.77 107648.14\n\narima_dept_95 &lt;- forecast::auto.arima(y = sales_95_ts)\narima_dept_95\n\nSeries: sales_95_ts \nARIMA(0,1,1)(0,1,0)[52] \n\nCoefficients:\n          ma1\n      -0.8842\ns.e.   0.0530\n\nsigma^2 = 29974424:  log likelihood = -1033.02\nAIC=2070.03   AICc=2070.15   BIC=2075.3\n\nplot(arima_dept_95)\n\n\n\n# Use the model to forecast 12 weeks into the future\nsales95_forecast &lt;- forecast(arima_dept_95, h = 12)\n\n# Plot the forecast. Again, we can use autoplot.\nautoplot(sales95_forecast) +\n  theme_minimal()\n\n\n\n\nWe’re fairly limited in what we can actually tweak when using autoplot(), so instead we can convert the forecast object to a data frame and use ggplot() like normal:\n\n# Get data out of this weird sales95_forecast object\nsales95_forecast\n\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n2013.000       116571.1 109554.8 123587.5 105840.6 127301.7\n2013.019       126102.0 119038.7 133165.2 115299.7 136904.3\n2013.038       120871.5 113761.7 127981.4 109998.0 131745.1\n2013.058       111934.8 104778.7 119091.0 100990.5 122879.2\n2013.077       119470.2 112268.0 126672.3 108455.5 130484.9\n2013.096       126904.7 119656.9 134152.5 115820.1 137989.3\n2013.115       133382.4 126089.2 140675.6 122228.3 144536.5\n2013.135       118094.6 110756.3 125433.0 106871.6 129317.7\n2013.154       128769.9 121386.7 136153.1 117478.2 140061.6\n2013.173       139022.7 131594.8 146450.5 127662.8 150382.5\n2013.192       130502.1 123030.0 137974.3 119074.5 141929.8\n2013.212       124236.7 116720.5 131752.9 112741.7 135731.7\n\nsales95_forecast_tidy &lt;- sweep::sw_sweep(sales95_forecast, \n                                         fitted = TRUE, \n                                         timetk_idx = TRUE)\n\nsales95_forecast_tidy\n\n\n\n  \n\n\n# For whatever reason, the date column here is a special type of variable called\n# \"yearmon\", which ggplot doesn't know how to deal with (like, we can't zoom in\n# on the plot with coord_cartesian). We use zoo::as.Date() to convert the\n# yearmon variable into a regular date\nsales95_forecast_tidy_real_date &lt;- \n  sales95_forecast_tidy %&gt;% \n  mutate(actual_date = zoo::as.Date(index, frac = 1))\nsales95_forecast_tidy_real_date\n\n\n\n  \n\n\n# Plot this puppy!\nggplot(sales95_forecast_tidy, aes(x = index, y = value, color = key)) +\n  geom_ribbon(aes(ymin = lo.95, ymax = hi.95), \n              fill = \"#3182bd\", color = NA) +\n  geom_ribbon(aes(ymin = lo.80, ymax = hi.80, fill = key), \n              fill = \"#deebf7\", color = NA, alpha = 0.8) +\n  geom_line(size = 1) + \n  geom_point(size = 0.5) +\n  labs(x = NULL, y = \"sales95\") +\n  scale_y_continuous(labels = scales::comma) +\n  # Zoom in on 2012-2016\n  #coord_cartesian(xlim = ymd(c(\"2004-07-01\", \"2007-07-31\"))) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\nplot_time_series(.data = sales95_forecast_tidy,.date_var = index,.value = value,.color_var = key,.smooth = FALSE)\n\n\n\n\n\nA Bit of Forecasting?\nWe are always interested in the future. We will do this in three ways:\n\nuse Simple Exponential Smoothing\nuse a package called forecast to fit an ARIMA (Auto-regressive Moving Average Integrated Model) model to the data and make predictions for weekly sales;\nAnd do the same using a package called ’prophet`."
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/50-ModelTimeSeries/index.html#conclusion",
    "href": "content/courses/Analytics/Predictive/Modules/50-ModelTimeSeries/index.html#conclusion",
    "title": "🕔 Modelling Time Series",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/50-ModelTimeSeries/index.html#references",
    "href": "content/courses/Analytics/Predictive/Modules/50-ModelTimeSeries/index.html#references",
    "title": "🕔 Modelling Time Series",
    "section": "References",
    "text": "References\n1, Shampoo Dataset Brownlee: https://raw.githubusercontent.com/jbrownlee/Datasets/master/shampoo.csv"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html",
    "title": "ML - Classification",
    "section": "",
    "text": "Have you played a Childhood Game called 20 Questions? Someone has a “target” entity in mind ( a person or a thing or a literary character) and the others need to discover that entity by asking 20 questions.\n\nHow does one create questions in the game?\n\nCategories?\nNumbers? How?\nComparisons?\n\n\nWhat sort of answers can you expect for each question?"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#a-childhood-game",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#a-childhood-game",
    "title": "ML - Classification",
    "section": "",
    "text": "Have you played a Childhood Game called 20 Questions? Someone has a “target” entity in mind ( a person or a thing or a literary character) and the others need to discover that entity by asking 20 questions.\n\nHow does one create questions in the game?\n\nCategories?\nNumbers? How?\nComparisons?\n\n\nWhat sort of answers can you expect for each question?"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#twenty-questions-game-as-a-play-with-data",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#twenty-questions-game-as-a-play-with-data",
    "title": "ML - Classification",
    "section": "Twenty Questions Game as a Play with Data…",
    "text": "Twenty Questions Game as a Play with Data…\nAssuming we think of a 20Q Target as say, celebrity singer like Taylor Swift, or a cartoon character like Thomas the Tank Engine, what would an underlying “data structure” look like? We would ask Questions for instance in the following order to find the target of Taylor Swift:\n\nHuman?(Yes)\nLiving?(Yes)\nMale?(No)\nCelebrity?(Yes)\nMusic?(Yes)\nUSA?(Yes)\n\nOh…Taylor Swift!!!\nLet us try to construct the “datasets” underlying this game!\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nOccupation\nSex\nLiving\nNationality\ngenre\npet\n\n\nTaylor Swift\nSinger\nF\nTRUE\nUSA\ncountry/rock\nScottish Fold Cats\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nType\nLiving\nhuman\nNationality\ncolour\nmaterial\n\n\nThomas, the Tank Engine\nCartoon Character\nFALSE\nFALSE\nUK\nblue\nmetal\n\n\n\n\nIt should be fairly clear that the Questions we ask are based on the COLUMNs in the respective 1-row datasets! The TARGET Column in both cases is the name column."
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#what-is-a-decision-tree",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#what-is-a-decision-tree",
    "title": "ML - Classification",
    "section": "What is a Decision Tree?",
    "text": "What is a Decision Tree?\nCan you imagine how the 20 Questions Game can be shown as a tree?\n\n\n\n\n\n\nEach Question we ask, based on one of the Feature columns, begets a Yes/NO answer and we turn the left or right accordingly. When we arrive at the leaf, we should be in a position to guess the answer !"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#twenty-times-20-questions",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#twenty-times-20-questions",
    "title": "ML - Classification",
    "section": "Twenty times 20 Questions !!",
    "text": "Twenty times 20 Questions !!\nWhat if the dataset we had contained many rows, instead of just one row? How would we play the 20Q Game in this situation? Here is a sample of the famous penguins dataset:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\nChinstrap\nDream\n51.7\n20.3\n194\n3775\nmale\n2007\n\n\nAdelie\nDream\n36.0\n17.9\n190\n3450\nfemale\n2007\n\n\nAdelie\nTorgersen\n39.6\n17.2\n196\n3550\nfemale\n2008\n\n\nGentoo\nBiscoe\n46.3\n15.8\n215\n5050\nmale\n2007\n\n\nGentoo\nBiscoe\n44.5\n14.3\n216\n4100\nNA\n2007\n\n\nChinstrap\nDream\n46.5\n17.9\n192\n3500\nfemale\n2007\n\n\nGentoo\nBiscoe\n50.5\n15.9\n222\n5550\nmale\n2008\n\n\nAdelie\nDream\n36.6\n18.4\n184\n3475\nfemale\n2009\n\n\nGentoo\nBiscoe\n48.2\n15.6\n221\n5100\nmale\n2008\n\n\nChinstrap\nDream\n58.0\n17.8\n181\n3700\nfemale\n2007\n\n\nGentoo\nBiscoe\n46.5\n13.5\n210\n4550\nfemale\n2007\n\n\nGentoo\nBiscoe\n47.3\n13.8\n216\n4725\nNA\n2009\n\n\n\n\n\nAs before, we would need to look at the dataset as containing a TARGET column which we want to predict using several other FEATURE columns. Let us choose species.\nWhen we look at the FEATURE columns, We would need to formulate questions based on entire columns at a time. For instance:\n\n\n“Is the bill_length_mm* greater than 45mm?” considers the entire bill_length_mm* FEATURE column\nIs the sex female? considers the entire sex column\n\nIf the specific FEATURE column is a Numerical (N) variable, the question would use some “thresholding” as shown in the question above, to convert the Numerical Variable into a Categorical variable.\nIf a specific FEATURE column is a Categorical (C) variable, the question would be like a filter operation in Excel.\nEither way, we end up answering with a smaller and smaller subset of rows in the dataset, to which the questions are answered with a Yes. It is as if we played many 20 Questions games in parallel, since there are so many simultaneous “answers”!\nOnce we exhaust all the FEATURE columns, then what remains is a subset (i.e. rows) of the original dataset and we read off the TARGET column, which should now contain a set of identical entries, e.g. “Adelie”. Thus we can extend a single-target 20Q game to a multiple-target one using a larger dataset. ( Note how the multiple targets are all the same: “Adelie”, or “Gentoo”, or “Chinstrap”)\nThis forms the basic intuition for a Machine Learning Algorithm called a Decision Tree.\nDecision Tree in Orange\nLet us visualize this Decision Tree in Orange. Look at the now famous penguins dataset, available here:\nhttps://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\nWe see that there are three species of penguins, that live on three islands. The measurements for each penguin are flipper_length_mm, bill_length_mm, bill_depth_mm, and body_mass_g.\n\nTask 1: Create a few data visualizations for the variables, and pairs of variables from this dataset.\nTask 2: Can you inspect the visualizations and imagine how each of this dataset can be used in a 20 Questions Game, to create a Decision Tree for this dataset as shown below?\n\n\n\nPenguins Decision Tree!\n\nWhat did we learn?\n\nThe 20Q Game can be viewed as a “Decision Tree” of Questions and Answers,\nEach fork in the game is a Question.\nDepending upon whether the current answer is yes or no, we turn in one direction or the other.\nEach of our questions is based on the information available in one or other of the columns!!\nWe arrive at a final “answer” or “target” after a particular sequence of yes/no answers. This is the one of the leaf nodes in the Tree.\nThe island and the species columns are categories and are especially suited to being the targets for a 20 Questions Game.\nWe can therefore use an entire column of data as our 20Questions target, rather than just one entity, person.\n\nThis is how we will use this Game as a Model for our first ML algorithm, classification using Decision Trees."
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#how-do-we-make-predictions-using-our-decision-tree",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#how-do-we-make-predictions-using-our-decision-tree",
    "title": "ML - Classification",
    "section": "How do we Make Predictions using our Decision Tree",
    "text": "How do we Make Predictions using our Decision Tree\nOur aim is to make predictions. Predictions of what? When we are given new unseen data in the same format, we should be able to predict TARGET variable using the same FEATURE columns.\nNOTE: This that is usually a class/category (We CAN also predict a numerical value with a Decision Tree; but we will deal with that later.)\nIn order to make predictions with completely unseen data, we need to first check if the algorithm is working well with known data. The way to do this is to use a large portion of data to design the tree, and then use the tree to predict some aspect of the remaining, but similar, data. Let us split the penguins dataset into two pieces: a training set to design our tree, and a test set to check how it is working.\nDownload this penguin tree file and open it in Orange.\nHow good are the Predictions? What is the Classification Error Rate?"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#how-many-trees-do-we-need-enter-the-random-forest",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#how-many-trees-do-we-need-enter-the-random-forest",
    "title": "ML - Classification",
    "section": "How Many Trees do we Need? Enter the Random Forest!",
    "text": "How Many Trees do we Need? Enter the Random Forest!\nCheck all your individual Decision Trees: do they ask the same Questions? Do they fork in the same way? Yes, they all seem to use the same set of parameters to reach the target. So they are capable of being “biased” and make the same mistakes. So we ask: Does it help to use more than one tree, if all the questions/forks in the Trees are similar?\nNo…we need different Trees to be able to ask different questions, based on different variables or features in the data. That will make the Trees as different as possible and so…unbiased. This is what we also saw when we played 20Q: offbeat questions opened up some avenues for predicting the answer/target.\nA forest of such trees is called the Wild Wood a Random Forest !"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#an-introduction-to-random-forests",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#an-introduction-to-random-forests",
    "title": "ML - Classification",
    "section": "An Introduction to Random Forests",
    "text": "An Introduction to Random Forests\nIn the Random Forest method, we do as follows:\n\nSplit the dataset into training and test subsets (70::30 proportion is very common). Keep aside the testing dataset for final testing.\nDecide on a number of trees, say 100-500 in the forest.\nTake the training dataset and repeatedly sample some of the rows in it. Rows can be repeated too; this is called bootstrap sampling.\nGive this sampled training set to each tree. Each tree develops a question from this dataset, in a random fashion, using a randomly chosen variable. E.g. with penguins, if our target is species, then some trees will will use island, some others will use body_mass_g and some others may use bill_length_mm.\nEach tree will “grow its questions” in a unique way !! Since the questions are possibly based on a different variable at each time, the trees will grow in very different ways.\nStop when the required accuracy has been achieved (the sets contain observations/rows from only one species predominantly)\nWith the test set let each tree vote on which species it has decided upon. Take the majority vote.\n\nPhew!!\nLet’s get a visual sense of how all this works:\nhttps://waternova.github.io/random-forest-viz/"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#random-forest-classification-for-heart-patients",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#random-forest-classification-for-heart-patients",
    "title": "ML - Classification",
    "section": "Random Forest Classification for Heart Patients",
    "text": "Random Forest Classification for Heart Patients\nDo you want to develop an ML model for heart patients? We have a dataset of heart patients at the University of California, Arvind Irvine ML Dataset Repository\nHeart Patient Data. Import into Orange !!\nWhat are the variables?\n\n(age): age in years\n(sex): 1 = male; 0 = female\n(cp): chest-pain type( 4 types, 1/2/3/4)\n(trestbps): resting blood pressure (in mm Hg on admission to the hospital)\n(chol) : serum cholesterol in mg/dl\n(fbs): (fasting blood sugar &gt; 120 mg/dl) (1 = true; 0 = false)\n(restecg): resting electrocardiograph results (0 = normal; 1= ST-T wave abnormality; 3 = LV hypertrophy)\n(thalach): maximum heart rate achieved\n(exang): exercise induced angina (1 = yes; 0 = no) (remember Puneet Rajkumar)\n(oldpeak): ST depression induced by exercise relative to rest\n(slope): the slope of the peak exercise ST segment\n\nValue 1: upsloping\nValue 2: flat\nValue 3: downsloping\n\n\n(ca): number of major vessels (0-3) colored by fluoroscopy\n(thal): 3 = normal; 6 = fixed defect; 7 = reversible defect\n(num) : the target attribute, diagnosis of heart disease (angiographic disease status)\n\nValue 0: &lt; 50% diameter narrowing\nValue 1: &gt; 50% diameter narrowing\n(in any major vessel: attributes 59 through 68 are vessels)\n\n\n\nWe will create a Random Forest Model for this dataset, and compare with the Desision Tree for the same dataset."
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#how-good-is-my-random-forest",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#how-good-is-my-random-forest",
    "title": "ML - Classification",
    "section": "How good is my Random Forest?",
    "text": "How good is my Random Forest?\n\nClassification Error\n\nGini Impurity\n\nCross Entropy"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#references",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#references",
    "title": "ML - Classification",
    "section": "References",
    "text": "References\n\nhttps://towardsdatascience.com/data-science-made-easy-data-modeling-and-prediction-using-orange-f451f17061fa\nThe beauty of Random Forests: https://orangedatamining.com/blog/2016/12/22/the-beauty-of-random-forest/\nPythagorean Trees for Random Forests: https://orangedatamining.com/blog/2016/07/29/pythagorean-trees-and-forests/\ndata.tree sample applications, Christoph Glur, 2020-07-31. https://cran.r-project.org/web/packages/data.tree/vignettes/applications.html\nhttps://ryjohnson09.netlify.app/post/caret-and-tidymodels/"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/20-Regression/index.html",
    "href": "content/courses/Analytics/Predictive/Modules/20-Regression/index.html",
    "title": "ML - Regression",
    "section": "",
    "text": "Interpolation:\n\nbetween TWO colours, both colours inclusive using a straight line between them\nbetween several different colours?\n\nby mixing “equal proportions” of each\nProportions based on “distance” from each colour\nOn a “plane” with these points"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/20-Regression/index.html#workflow-in-orange",
    "href": "content/courses/Analytics/Predictive/Modules/20-Regression/index.html#workflow-in-orange",
    "title": "ML - Regression",
    "section": "Workflow in Orange",
    "text": "Workflow in Orange\nLet us “draw inspiration” from the picture above, and see if we can replicate it. We will fire up Orange, paint some data and see if we can fit a linear regression ML model to it.\nHere is the Orange file for you to download. Open this file in Orange."
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/20-Regression/index.html#workflow-in-radiant",
    "href": "content/courses/Analytics/Predictive/Modules/20-Regression/index.html#workflow-in-radiant",
    "title": "ML - Regression",
    "section": "Workflow in Radiant",
    "text": "Workflow in Radiant"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/20-Regression/index.html#workflow-in-r",
    "href": "content/courses/Analytics/Predictive/Modules/20-Regression/index.html#workflow-in-r",
    "title": "ML - Regression",
    "section": "Workflow in R",
    "text": "Workflow in R"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/20-Regression/index.html#conclusion",
    "href": "content/courses/Analytics/Predictive/Modules/20-Regression/index.html#conclusion",
    "title": "ML - Regression",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/listing.html",
    "href": "content/courses/Analytics/Predictive/listing.html",
    "title": "Predictive Analytics",
    "section": "",
    "text": "🐉 Intro to Orange\n\n\n\n\n\nUsing A Visual drag and drop tool called Orange\n\n\n\n\n\n\nOct 17, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\nML - Regression\n\n\n\n\n\n\n\nLinear Regression\n\n\nTrend Line\n\n\nFrancis Galton\n\n\n\n\nUsing Linear Regression to Predict Numerical Data\n\n\n\n\n\n\nAug 16, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\nML - Classification\n\n\n\n\n\nWe will look at the basic models for Classification of Data\n\n\n\n\n\n\nJul 22, 2022\n\n\n\n\n\n\n  \n\n\n\n\nML - Clustering\n\n\n\n\n\nWe will look at the basic models for Clustering of Data.\n\n\n\n\n\n\nJul 19, 2022\n\n\n\n\n\n\n  \n\n\n\n\n🕔 Modelling Time Series\n\n\n\n\n\n\n\nSmoothing\n\n\nExponential Models\n\n\nARIMA\n\n\nForecasting\n\n\nProphet\n\n\n\n\nWe will look at the basic models for Time Series\n\n\n\n\n\n\nNov 19, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html",
    "title": "Modelling with Linear Regression",
    "section": "",
    "text": "Multiple Regression - Forward Selection  \n\n Multiple Regression - Backward Selection  \n\n  Permutation Test for Regression"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#sec-tutorials",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#sec-tutorials",
    "title": "Modelling with Linear Regression",
    "section": "",
    "text": "Multiple Regression - Forward Selection  \n\n Multiple Regression - Backward Selection  \n\n  Permutation Test for Regression"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#iconify-noto-v1-package-setting-up-r-packages",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#iconify-noto-v1-package-setting-up-r-packages",
    "title": "Modelling with Linear Regression",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nknitr::opts_chunk$set(echo = TRUE,warning = FALSE,message = FALSE)\n#options(scipen = 1, digits = 3) #set digits to three decimal places\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(mosaic)\nlibrary(GGally)\nlibrary(corrplot)\nlibrary(corrgram)\n\nlibrary(nomnoml)\n\n\n# Let us set a plot theme for Data visualization\n\nmy_theme &lt;- function(){  # Creating a function\n  theme_classic() +  # Using pre-defined theme as base\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        axis.text.x = element_text(size = 10, face = \"bold\"),  \n        # Customizing axes text      \n        axis.text.y = element_text(size = 10, face = \"bold\"),\n        axis.title = element_text(size = 12, face = \"bold\"),  \n        # Customizing axis title\n        panel.grid = element_blank(),  # Taking off the default grid\n        plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), units = , \"cm\"),\n        legend.text = element_text(size = 8, face = \"italic\"),  \n        # Customizing legend text\n        legend.title = element_text(size = 10, face = \"bold\"),  \n        # Customizing legend title\n        legend.position = \"right\",  # Customizing legend position\n        plot.caption = element_text(size = 8))  # Customizing plot caption\n}"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "title": "Modelling with Linear Regression",
    "section": "\n Introduction",
    "text": "Introduction\nOne of the most common problems in Prediction Analytics is that of predicting a Quantitative response variable, based on one or more Quantitative predictor variables or features. This is called Linear Regression. We will use the intuitions built up during our study of ANOVA to develop our ideas about Linear Regression.\nSuppose we have data on salaries in a Company, with years of study and previous experience. Would we be able to predict the prospective salary of a new candidate, based on their years of study and experience? Or based on mileage done, could we predict the resale price of a used car? These are typical problems in Linear Regression.\nIn this tutorial, we will use the Boston housing dataset. Our research question is:\n\n\n\n\n\n\nResearch Question\n\n\n\nHow do we predict the price of a house in Boston, based on other parameters Quantitative parameters such as area, location, rooms, and crime-rate in the neighbourhood?"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#iconify-radix-icons-box-model-the-linear-regression-model",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#iconify-radix-icons-box-model-the-linear-regression-model",
    "title": "Modelling with Linear Regression",
    "section": "\n The Linear ( Regression ) Model",
    "text": "The Linear ( Regression ) Model\nThe premise here is that many common statistical tests are special cases of the linear model.\nA linear model estimates the relationship between one continuous or ordinal variable (dependent variable or “response”) and one or more other variables (explanatory variable or “predictors”). It is assumed that the relationship is linear:1\n\\[\ny = \\beta_0 + \\beta_1 *x\n\\]\n\\(\\beta_0\\) is the intercept and \\(\\beta_1\\) is the slope of the linear fit, that predicts the value of y based the value of x. Each prediction leaves a small “residual” error between the actual and predicted values. \\(\\beta_0\\) and \\(\\beta_1\\) are calculated based on minimizing the sum of squares of these residuals, and hence this method is called “ordinary least squares” (OLS) regression.\n\n\nLeast Squares\n\nThe net area of all the shaded squares is minimized in the calculation of \\(\\beta_0\\) and \\(\\beta_1\\). It is also possible that there is more than one explanatory variable: this is multiple regression.\n\\[\ny = \\beta_0 + \\beta_1*x_1 + \\beta_2*x_2 ...+ \\beta_n*x_n\n\\]\nwhere each of the \\(\\beta_i\\) are slopes defining the relationship between y and \\(x_i\\). Together, the RHS of that equation defines an n-dimensional hyperplane.\nAs per Lindoloev, many statistical tests, going from one-sample t-tests to two-way ANOVA, are special cases of this system. Also see Jeffrey Walker “A linear-model-can-be-fit-to-data-with-continuous-discrete-or-categorical-x-variables”"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#iconify-simple-icons-hypothesis-linear-models-as-hypothesis-tests",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#iconify-simple-icons-hypothesis-linear-models-as-hypothesis-tests",
    "title": "Modelling with Linear Regression",
    "section": "\n Linear Models as Hypothesis Tests",
    "text": "Linear Models as Hypothesis Tests\nUsing linear models is based on the idea of Testing of Hypotheses. The Hypothesis Testing method typically defines a NULL Hypothesis where the statements read as “there is no relationship” between the variables at hand, explanatory and responses. The Alternative Hypothesis typically states that there is a relationship between the variables.\nAccordingly, in fitting a linear model, we follow the process as follows: With \\(y = \\beta_0 + \\beta_1 *x\\)\n\nMake the following hypotheses: \\[\nNULL\\ Hypothesis\\ H_0 =&gt; x\\ and\\ y\\ are\\ unrelated.\\ (\\beta_1 = 0)\n\\] \\[\nAlternate\\ Hypothesis\\ H_1 =&gt; x\\ and\\ y\\ are\\ linearly\\ related\\ (\\beta_1 \\ne 0)\n\\]\n\nWe “assume” that \\(H_0\\) is true.\nWe calculate \\(\\beta_1\\).\nWe then find probability p that \\(\\beta_1 = Estimated\\ Value\\) when the NULL Hypothesis is assumed TRUE. This is the p-value. If that probability is p&gt;=0.05, we say we “cannot reject” \\(H_0\\) and there is unlikely to be significant linear relationship.\nHowever, if p&lt;= 0.05 can we reject the NULL hypothesis, and say that there could be a significant linear relationship, because the probability p that \\(\\beta_1 = Estimated\\ Value\\) by mere chance under \\(H_0\\) is very small."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#iconify-icon-park-outline-thinking-problem-assumptions-in-linear-models",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#iconify-icon-park-outline-thinking-problem-assumptions-in-linear-models",
    "title": "Modelling with Linear Regression",
    "section": "\n Assumptions in Linear Models",
    "text": "Assumptions in Linear Models\nWe can write the assumptions in Linear Regression Models as an acronym, LINE:\n1. L: \\(\\color{blue}{linear}\\) relationship\n2. I: Errors are independent (across observations)\n3. N: y is \\(\\color{red}{normally}\\) distributed at each “level” of x.\n4. E: equal variance at all levels of x. No heteroscedasticity.\n\n\nOLS Assumptions\n\nHence a very concise way of expressing the Linear Model is:\n\\[\ny \\sim N(x_i^T * \\beta, ~~\\sigma^2)\n\\]\n\n\n\n\n\n\nGeneral Linear Models\n\n\n\nThe target variable \\(y\\) is modelled as a normally distributed variable whose mean depends upon a linear combination of predictor variables \\(x\\), and whose variance is \\(\\sigma^2\\).\n\n\nLet us now read in the data and check for these assumptions as part of our Workflow."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#iconify-flat-color-icons-workflow-workflow-read-the-data",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#iconify-flat-color-icons-workflow-workflow-read-the-data",
    "title": "Modelling with Linear Regression",
    "section": "\n Workflow: Read the Data",
    "text": "Workflow: Read the Data\n\ndata(\"BostonHousing2\", package = \"mlbench\")\nhousing &lt;- BostonHousing2\ninspect(housing)\n\n\ncategorical variables:  \n  name  class levels   n missing                                  distribution\n1 town factor     92 506       0 Cambridge (5.9%) ...                         \n2 chas factor      2 506       0 0 (93.1%), 1 (6.9%)                          \n\nquantitative variables:  \n      name   class       min          Q1     median          Q3       max\n1    tract integer   1.00000 1303.250000 3393.50000 3739.750000 5082.0000\n2      lon numeric -71.28950  -71.093225  -71.05290  -71.019625  -70.8100\n3      lat numeric  42.03000   42.180775   42.21810   42.252250   42.3810\n4     medv numeric   5.00000   17.025000   21.20000   25.000000   50.0000\n5    cmedv numeric   5.00000   17.025000   21.20000   25.000000   50.0000\n6     crim numeric   0.00632    0.082045    0.25651    3.677083   88.9762\n7       zn numeric   0.00000    0.000000    0.00000   12.500000  100.0000\n8    indus numeric   0.46000    5.190000    9.69000   18.100000   27.7400\n9      nox numeric   0.38500    0.449000    0.53800    0.624000    0.8710\n10      rm numeric   3.56100    5.885500    6.20850    6.623500    8.7800\n11     age numeric   2.90000   45.025000   77.50000   94.075000  100.0000\n12     dis numeric   1.12960    2.100175    3.20745    5.188425   12.1265\n13     rad integer   1.00000    4.000000    5.00000   24.000000   24.0000\n14     tax integer 187.00000  279.000000  330.00000  666.000000  711.0000\n15 ptratio numeric  12.60000   17.400000   19.05000   20.200000   22.0000\n16       b numeric   0.32000  375.377500  391.44000  396.225000  396.9000\n17   lstat numeric   1.73000    6.950000   11.36000   16.955000   37.9700\n           mean           sd   n missing\n1  2700.3557312 1.380037e+03 506       0\n2   -71.0563887 7.540535e-02 506       0\n3    42.2164403 6.177718e-02 506       0\n4    22.5328063 9.197104e+00 506       0\n5    22.5288538 9.182176e+00 506       0\n6     3.6135236 8.601545e+00 506       0\n7    11.3636364 2.332245e+01 506       0\n8    11.1367787 6.860353e+00 506       0\n9     0.5546951 1.158777e-01 506       0\n10    6.2846344 7.026171e-01 506       0\n11   68.5749012 2.814886e+01 506       0\n12    3.7950427 2.105710e+00 506       0\n13    9.5494071 8.707259e+00 506       0\n14  408.2371542 1.685371e+02 506       0\n15   18.4555336 2.164946e+00 506       0\n16  356.6740316 9.129486e+01 506       0\n17   12.6530632 7.141062e+00 506       0\n\n\nThe original data are 506 observations on 14 variables, medv being the target variable:\n\n\n\n\n\n\n\ncrim\nper capita crime rate by town\n\n\nzn\nproportion of residential land zoned for lots over 25,000 sq.ft\n\n\nindus\nproportion of non-retail business acres per town\n\n\nchas\nCharles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n\n\nnox\nnitric oxides concentration (parts per 10 million)\n\n\nrm\naverage number of rooms per dwelling\n\n\nage\nproportion of owner-occupied units built prior to 1940\n\n\ndis\nweighted distances to five Boston employment centres\n\n\nrad\nindex of accessibility to radial highways\n\n\ntax\nfull-value property-tax rate per USD 10,000\n\n\nptratio\npupil-teacher ratio by town\n\n\nb\n\n\\(1000(B - 0.63)^2\\) where B is the proportion of Blacks by town\n\n\nlstat\npercentage of lower status of the population\n\n\nmedv\nmedian value of owner-occupied homes in USD 1000’s\n\n\n\nThe corrected data set has the following additional columns:\n\n\ncmedv\ncorrected median value of owner-occupied homes in USD 1000’s\n\n\ntown\nname of town\n\n\ntract\ncensus tract\n\n\nlon\nlongitude of census tract\n\n\nlat\nlatitude of census tract\n\n\nOur response variable is cmedv, the corrected median value of owner-occupied homes in USD 1000’s. Their are many Quantitative feature variables that we can use to predict cmedv. And there are two Qualitative features, chas and tax."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#iconify-flat-color-icons-workflow-workflow-eda",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#iconify-flat-color-icons-workflow-workflow-eda",
    "title": "Modelling with Linear Regression",
    "section": "\n Workflow: EDA",
    "text": "Workflow: EDA\nIn order to fit the linear model, we need to choose predictor variables that have strong correlations with the target variable. We will first do this with GGally, and then with the tidyverse itself. Both give us a very unique view into the correlations that exist within this dataset.\n\n\nCorrelations with GGally\nCorrelations using cor.test and purrr\n\n\n\nLet us select a few sets of Quantitative and Qualitative features, along with the target variable cmedv and do a pairs-plots with them:\n\ntheme_set(theme_bw())\nhousing %&gt;%\n  # Target variable cmedv\n  # Predictors Rooms / Age / Distance to City Centres / Radial Highway Access\n  select(cmedv, rm, age, dis) %&gt;%\n  GGally::ggpairs(title = \"Plot 1\",\n                  lower = list(continuous = wrap(\"smooth\", alpha = 0.2)))\nhousing %&gt;%\n  # Target variable cmedv\n  # Predictors: Access to Radial Highways, / Resid. Land Proportion / proportion of non-retail business acres / full-value property-tax rate per USD 10,000\n  select(cmedv, rad, zn, indus, tax) %&gt;%\n  GGally::ggpairs(title = \"Plot 2\", \n                  lower = list(continuous = wrap(\"smooth\", alpha = 0.2)))\nhousing %&gt;%\n  # Target variable cmedv\n  # Predictors Crime Rate / Nitrous Oxide / Black Population / Lower Status Population\n  select(cmedv, crim, nox, rad, b, lstat) %&gt;%\n  GGally::ggpairs(title = \"Plot 3\", \n                  lower = list(continuous = wrap(\"smooth\", alpha = 0.2)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClearly, rm (avg. number of rooms) is a big determining feature for median price cmedv. This we infer based on the large magnitudes of correlations of rm withcmedv. The variableage (proportion of owner-occupied units built prior to 1940) may also be a significant influence on cmedv. (See top row)\nNone of the Quant variables rad, zn, indus, tax have a overly strong correlation with cmedv. (See top row)\nThe variable lstat (proportion of lower classes in the neighbourhood) as expected, has a strong (negative) correlation with cmedv; rad(index of accessibility to radial highways), nox(nitrous oxide) and crim(crime rate) also have fairly large correlations with cmedv.\n\n\n\n\n\n\nCorrelation Scores and Uncertainty\n\n\n\nRecall that cor.test reports a correlation score and the p-value for the same. There is also a confidence interval reported for the correlation score, an interval within which we are 95% sure that the true correlation value is to be found. Note that GGally too reports the significance of the correlation scores using stars, *** or **. This indicates the p-value in the scores obtained by GGally; Presumably, there is an internal cor.test that is run for each pair of variables and the p-value and confidence levels are also computed internally.\n\n\nLet us plot (again) scatter plots of Quant Variables that have strong correlation with cmedv:\n\ngf_point(\n  data = housing,\n  cmedv ~ age,\n  title = \"Price vs Proportion of houses older than 1940\",\n  ylab = \"Median Price\",\n  xlab = \"Proportion of older-than-1940 buildings\"\n) %&gt;%\n  gf_theme(my_theme())\ngf_point(\n  data = housing,\n  cmedv ~ lstat,\n  title = \"Price vs Proportion of lower classes in the neighbourhood\",\n  ylab = \"Median Price\",\n  xlab = \"proportion of lower classes in the neighbourhood\"\n) %&gt;%\n  gf_theme(my_theme())\ngf_point(\n  data = housing,\n  cmedv ~ rm,\n  title = \"Price vs Average no. of Rooms\",\n  ylab = \"(cmedv) Median Price\",\n  xlab = \"(rm) Avg. No. of Rooms\"\n) %&gt;%\n  gf_theme(my_theme())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo, rm does have a positive effect on cmedv, and age may have a (mild?) negative effect on cmedv; lstat seems to have a pronouced negative effet on cmedv. We have now managed to get a decent idea which Quant predictor variables might be useful in modelling cmedv: rm, lstat for starters, then perhapsage.\nLet us also check the Qualitative predictor variables: Access to the Charles river (chas) does seem to affect the prices somewhat.\n\ntheme_set(theme_bw())\nhousing %&gt;%\n  # Target variable cmedv\n  # Predictor Access to Charles River\n  select(cmedv, chas) %&gt;%\n  GGally::ggpairs(title = \"Plot 4\", \n                  lower = list(continuous = wrap(\"smooth\", \n                                                 alpha = 0.2)))\n\n\n\n\nLook at the bar plot above. While not too many properties can be near the Charles River (for obvious reasons) the box plots do seem to show some dependency of cmedv on chas.\n\n\n\n\n\n\nNote\n\n\n\nQualitative predictors for a Quantitative target can be included in the model using what is called dummy variables, where each level of the Qualitative variable is given a one-hot kind of encoding. See for example https://www.statology.org/dummy-variables-regression/\n\n\n\n\nThis is somewhat advanced material: We will use the purrr package to develop all correlations with respect to our target variable in one shot and also plot these correlation test scores in an error-bar plot. This has the advantage of being able to depict all correlations in one plot. (We will use this approach again here when we trim our linear models down from the maximal one to a workable one of lesser complexity.). Let us do this.\nWe develop a list object containing all correlation test results with respect to cmedv, tidy these up using broom::tidy, and then plot these:\n\nall_corrs &lt;- housing %&gt;% \n  select(where(is.numeric)) %&gt;% \n  # leave off target variable cmedv and IDs\n  # get all the remaining ones\n  select(-cmedv, -medv) %&gt;%  \n\n  purrr::map(.x = ., # All numeric variables selected in the previous step\n             .f = \\(.x) cor.test(.x, housing$cmedv)) %&gt;% # Apply the cor.test with `cmedv`\n  \n  # Tidy up the cor.test outputs into neat columns\n  # Need \".id\" column to keep track of predictor variable name\n  map_dfr(broom::tidy, .id = \"predictor\") \n\nall_corrs\n\n\n\n  \n\n\nall_corrs %&gt;%\n  gf_hline(\n    yintercept = 0,\n    color = \"grey\",\n    linewidth = 2,\n    title = \"Correlations: Target Variable vs All Predictors\",\n    subtitle = \"Boston Housing Dataset\"\n  ) %&gt;%\n  gf_errorbar(\n    conf.high + conf.low ~ reorder(predictor, estimate),\n    colour = ~ estimate,\n    width = 0.5,\n    linewidth = ~ -log10(p.value),\n    caption = \"Significance = -log10(p.value)\"\n  ) %&gt;%\n  \n  # Plot points(smallest geom) last!\n  gf_point(estimate ~ reorder(predictor, estimate)) %&gt;%\n  gf_labs(x = \"Predictors\", y = \"Correlation with cmedv\") %&gt;%\n  \n  gf_theme(my_theme()) %&gt;%\n  \n  # tilt the x-axis labels for readability\n  gf_theme(theme(axis.text.x = element_text(angle = 45, hjust = 1))) %&gt;%\n  \n  # Colour and linewidth scales + legends\n  gf_refine(\n    scale_colour_distiller(\"Correlation\", type = \"div\", palette = \"RdBu\"),\n    scale_linewidth_continuous(\"Significance\", range = c(0.25, 3), \n                               \n  # guide_legend(reverse = TRUE): Fat Lines mean higher significance\n    )) %&gt;%\n  gf_refine(guides(linewidth = guide_legend(reverse = TRUE)))\n\n\n\n\nWe can clearly see that rm and lstat have strong correlations with cmedv and should make good choices for setting up a minimal linear regression model. (medv is the older errorred version of cmedv)\n\n\n\n\n\n\n\n\n\nSimple Regression vs Multiple Regression\n\n\n\nIf there are many predictor variables, we would typically want to use more of them to make our model and predictions. There are three ways2 to include more predictors:\n\n\nBackward Selection: We would typically start with a maximal model3 and progressively simplify the model by knocking off predictors that have the least impact on model accuracy.\n\nForward Selection: Start with no predictors and systematically add them one by one to increase the quality of the model\n\nMixed Selection: Wherein we start with no predictors and add them to gain improvement, or remove them at as their significance changes based on other predictors that have been added.\n\nWe will do the first two in the other tutorials (see Section 1 above); Mixed Selection we will leave for a more advanced course. But for now we will first use just one predictor rm(Avg. no. of Rooms) to model housing prices."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#iconify-flat-color-icons-workflow-workflow-model-building",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#iconify-flat-color-icons-workflow-workflow-model-building",
    "title": "Modelling with Linear Regression",
    "section": "\n Workflow: Model Building",
    "text": "Workflow: Model Building\nWe will first execute the lm test with code and evaluate the results. Then we will do an intuitive walk through of the process and finally, hand-calculate entire analysis for clear understanding.\n\n\n Model Code\n Linear Model Intuitive\n Linear Models Manually Demonstrated (Apologies to Spinoza)\n Using Other Packages\n\n\n\nR offers a very simple command lm to execute an Linear Model: Note the familiar formula of stating the variables: ( \\(y \\sim x\\); where \\(y\\) = target, \\(x\\) = predictor)\n\nhousing_lm &lt;- lm(cmedv ~ rm, data = housing)\nsummary(housing_lm)\n\n\nCall:\nlm(formula = cmedv ~ rm, data = housing)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-23.336  -2.425   0.093   2.918  39.434 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -34.6592     2.6421  -13.12   &lt;2e-16 ***\nrm            9.0997     0.4178   21.78   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.597 on 504 degrees of freedom\nMultiple R-squared:  0.4848,    Adjusted R-squared:  0.4838 \nF-statistic: 474.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\nThe model for \\(\\widehat{cmedv}\\) , the prediction for cmedvcan be written in the form of \\(y = mx + c\\), as:\n\\[\n\\widehat{cmedv} \\sim -34.65924 + 9.09967* rm\n\\tag{1}\\]\n\n\n\n\n\n\nImportant\n\n\n\n\nThe effect size of rm on predicting cmedv a (slope) value of \\(9.09967\\) which is significant at p-value of \\(&lt;2.2e-16\\); for every one room increase in rm, we have a $ USD 90997$ increase in median price cmedv.\nThe F-statistic for the Linear Model is given by \\(474.3\\), which is very high. (We will use the F-statistic again when we do Multiple Regression.)\nThe R-squared value is 48% which means that rm is able to explain about half of the trend in cmedv; there is substantial variation in cmedv that is left to explain, an indication that we should perhaps use a richer model, with more predictors. We will explore this in the Tutorials. Section 1\n\n\n\n\nWe can plot the scatter plot of these two variables with the model also over-plotted.\n\n# Tidy Data frame for the model using `broom`\nhousing_lm_tidy &lt;- \n  housing_lm %&gt;% \n  broom::tidy(conf.int= TRUE, \n              conf.level = 0.95)\nhousing_lm_tidy\nhousing_lm_augment &lt;- \n  housing_lm %&gt;% \n  broom::augment(se_fit = TRUE,\n                 interval = \"confidence\")\nhousing_lm_augment\nintercept &lt;- \n  housing_lm_tidy %&gt;%\n  filter(term == \"(Intercept)\") %&gt;%\n  select(estimate) %&gt;%\n  as.numeric()\n\nslope &lt;- \n  housing_lm_tidy %&gt;%\n  filter(term == \"rm\") %&gt;%\n  select(estimate) %&gt;%\n  as.numeric()\n\ngf_point(\n  data = housing,\n  cmedv ~ rm,\n  title = \"Price vs Average no. of Rooms\",\n  ylab = \"Median Price\",\n  xlab = \"Avg. No. of Rooms\",\n  alpha = 0.2\n) %&gt;%\n  \n  gf_abline(slope = slope,\n            intercept = intercept,\n            colour = \"lightcoral\", linewidth = 2)  %&gt;%\n  \n  gf_segment(\n    0 + 29 ~ 7 + 7, # manually calculated\n    linetype = \"dashed\",\n    color = \"dodgerblue\",\n    arrow = arrow(\n      angle = 30,\n      length = unit(0.25, \"inches\"),\n      ends = \"last\",\n      type = \"closed\"\n    ),\n    data = housing_lm_augment\n  ) %&gt;%\n  \n  gf_segment(\n    29 + 29 ~ 2.5 + 7, # manually calculated\n    linetype = \"dashed\",\n    arrow = arrow(\n      angle = 30,\n      length = unit(0.25, \"inches\"),\n      ends = \"first\",\n      type = \"closed\"\n    ),\n    color = \"dodgerblue\",\n    data = housing_lm_augment\n  ) %&gt;%\n  \n  gf_refine(\n    scale_x_continuous(limits = c(2.5, 10),\n                       expand = c(0, 0)),\n    # removes plot panel margins\n    scale_y_continuous(limits = c(0, 55),\n                       expand = c(0, 0))\n  )  %&gt;% \n  gf_theme(my_theme())\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nFor any new value of rm, we go up to the vertical blue line and read off the predicted median price by following the horizontal blue line. That is how the model is used (by hand).\nIn practice, we use the broom package functions (tidy, glance and augment) to obtain a clear view of the model parameters and predictions of cmedv for all existing values of rm. We see estimates for the intercept and slope (rm) for the linear model, along with the standard errors and p.values for these estimated parameters. And we see the fitted values of cmedv for the existing rm; these values will naturally lie on the straight-line depicting the model. We will examine this augment-ed data more in Section 10.\nTo predict cmedv with new values of rm, we use predict. Let us now try to make predictions with some new data:\n\nnew &lt;- tibble(rm = seq(3, 10)) # must be named \"rm\"\nnew %&gt;% mutate(predictions =\n                 stats::predict(\n                   object = housing_lm,\n                   newdata = .,\n                   se.fit = FALSE\n                 ))\n\n\n\n  \n\n\n\nNote that “negative values” for predicted cmedv would have no meaning!\n\n\nAll that is very well, but what is happening under the hood of the lm command? Consider the cmedv (target) variable and the rm feature/predictor variable. What we do is:\n\nPlot a scatter plot gf_point(cmedv ~ rm, housing)\n\nFind a line that, in some way, gives us some prediction of cmedv for any given rm\n\nCalculate the errors in prediction and use those to find the “best” line.\nUse that “best” line henceforth as a model for prediction.\n\nHow does one fit the “best” line? Consider a choice of “lines” that we can use to fit to the data. Here are 6 lines of varying slopes (and intercepts ) that we can try as candidates for the best fit line:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt should be apparent that while we cannot determine which line may be the best, the worst line seems to be the one in the final plot, which ignores the x-variable rm altogether. This corresponds to the NULL Hypothesis, that there is no relationship between the two variables. Any of the other lines could be a decent candidate, so how do we decide?\n\n\n\n\n\n\n\n\n\n\nIn Fig A, the horizontal blue line is the overall mean of cmedv, denoted as \\(\\mu_{tot}\\). The vertical green lines to the points show the departures of each point from this overall mean, called residuals. The sum of squares of these residuals in Fig A is called the Total Sum of Squares (SST).\n\\[\nSST = \\Sigma (y - \\mu_{tot})^2\n\\tag{2}\\]\nIn Fig B, the vertical red lines are the residuals of each point from the potential line of fit. The sum of the squares of these lines is called the Total Error Sum of Squares (SSE).\n\\[\nSSE = \\Sigma [(y - a - b * rm)^2]\n\\tag{3}\\]\nIt should be apparent that if there is any positive linear relationship between cmedv and rm,then \\(SSE &lt; SST\\).\nHow do we get the optimum slope + intercept? If we plot the \\(SSE\\) as a function of varying slope, we get:\n\n\n\n\n\nWe see that there is a quadratic minimum \\(SSE\\) at the optimum value of slope and at all other slopes, the \\(SSE\\) is higher. We can use this to find the optimum slope, which is what the function lm does.\n\n\nLet us hand-calculate the numbers so we know what the test is doing. Here is the SST: we pretend that there is no relationship between cmedv ans rm and compute a NULL model:\n\n# Calculate overall sum squares SST\n\nSST &lt;- deviance(lm(cmedv ~ 1, data = housing))\nSST\n\n[1] 42577.74\n\n\nAnd here is the SSE:\n\nSSE &lt;- deviance(housing_lm)\nSSE\n\n[1] 21934.39\n\n\nGiven that the model leaves unexplained variations in cmedv to the extent of \\(SSE\\), we can compute the \\(SSR\\), the Regression Sum of Squares, the amount of variation in cmedv that the linear model does explain:\n\nSSR &lt;- SST - SSE\nSSR\n\n[1] 20643.35\n\n\nWe have \\(SST = 42577.74\\), \\(SSE = 21934.39\\) and therefore \\(SSR = 20643.35\\).\nIn order to calculate the F-Statistic, we need to compute the variances, using these sum of squares. We obtain variances by dividing by their Degrees of Freedom:\n\\[\nF_{stat} = \\frac{SSR / df_{SSR}}{SSE / df_{SSE}}\n\\]\nwhere \\(df_{SSR}\\) and \\(df_{SSE}\\) are respectively the degrees of freedom in SSR and SSE.\nLet us calculate these Degrees of Freedom. If we have \\(n=\\) 506 observations of data, then:\n\n\n\\(SST\\) clearly has degree of freedom \\(n-1 = 505\\), since it uses all observations but loses one degree to calculate the global mean.\n\n\\(SSE\\) was computed using the slope and intercept, so it has \\((n-2) = 504\\) as degrees of freedom.\nAnd therefore \\(SSR\\) being their difference has just \\(1\\) degree of freedom.\n\nNow we are ready to compute the F-statistic:\n\nn &lt;- housing %&gt;% count() %&gt;% as.numeric()\ndf_SSR &lt;- 1\ndf_SSE &lt;- n -2\nF_stat &lt;- (SSR/df_SSR) / (SSE/df_SSE)\nF_stat\n\n[1] 474.3349\n\n\nThe F-stat is compared with a critical value of the F-statistic, which is computed using the formula for the f-distribution in R. As with our hypothesis tests, we set the significance level to 0.95, and quote the two relevant degrees of freedom as parameters to qf() which computes the critical F value as a quartile:\n\nF_crit &lt;-  qf(p = 0.95,     # Significance level is 5%\n              df1 = df_SSR, # Numerator degrees of freedom \n              df2 = df_SSE) # Denominator degrees of freedom\nF_crit\n\n[1] 3.859975\n\nF_stat\n\n[1] 474.3349\n\n\nThe F_crit value can also be seen in a plot4:\n\nmosaic::pdist(dist = \"f\",\n              q = F_crit, \n              df1 = df_SSR, df2 = df_SSE)\n\n\n\n\n[1] 0.95\n\n\nAny value of F more than the \\(F_{crit}\\) occurs with smaller probability than 0.05. Our F_stat is much higher than \\(F_{crit}\\), by orders of magnitude! And so we can say with confidence that rm has a significant effect on cmedv.\nThe value of R.squared is also calculated from the previously computed sums of squares:\n\\[\nR.squared = \\frac{SSR}{SST} = \\frac{SSY-SSE}{SST}\n\\tag{4}\\]\n\nr_squared &lt;- (SST - SSE)/SST\nr_squared\n\n[1] 0.484839\n\n# Also computable by\n# mosaic::rsquared(housing_lm)\n\nSo R.squared = 0.484839\nThe value of Slope and Intercept are computed using a maximum likelihood derivation and the knowledge that the means square error is a minimum at the optimum slope: for a linear model \\(y \\sim mx + c\\)\n\\[\nslope = \\frac{\\Sigma[(y - y_{mean})*(x - x_{mean})]}{\\Sigma(x - x_{mean})^2}\n\\]\n\n\n\n\n\n\nTip\n\n\n\nNote that the slope is equal to the ratio of the covariance of x and y to the variance of x.\n\n\nand\n\\[\nIntercept = y_{mean} - slope * x_{mean}\n\\]\n\nslope &lt;- mosaic::cov(cmedv ~ rm, data = housing) / mosaic::var(~ rm, data = housing)\nslope\n\n[1] 9.09967\n\nintercept &lt;- mosaic::mean(~ cmedv, data = housing) - slope * mosaic::mean(~ rm, data = housing)\nintercept\n\n[1] -34.65924\n\n\nSo, there we are! All of this is done for us by one simple formula, lm()!\n\n\nThere is a very neat package called ggstatsplot5 that allows us to plot very comprehensive statistical graphs. Let us quickly do this:\n\nlibrary(ggstatsplot)\nhousing_lm %&gt;%\n  ggstatsplot::ggcoefstats(title = \"Linear Model for Boston Housing\")\n\n\n\n\nThis chart shows the estimates for the intercept and rm along with their error bars, the t-statistic, degrees of freedom, and the p-value.\nWe can also obtain crisp-looking model tables from the new supernova package 6, which is based on the methods discussed in Judd et al.\n\nlibrary(supernova)\nsupernova::supernova(housing_lm)\n\n\n\n Analysis of Variance Table (Type III SS)\n Model: cmedv ~ rm\n\n                                SS  df        MS       F   PRE     p\n ----- --------------- | --------- --- --------- ------- ----- -----\n Model (error reduced) | 20643.347   1 20643.347 474.335 .4848 .0000\n Error (from model)    | 21934.392 504    43.521                    \n ----- --------------- | --------- --- --------- ------- ----- -----\n Total (empty model)   | 42577.739 505    84.312                    \n\n\n\n\nThis table is very neat in that it gives the Sums of Squares for both the NULL and the current model, for comparison. The PRE entry is the Proportional Reduction in Error, a measure that is identical with r.squared, which shows how much the model reduces the error compared to the NULL model(48%). The PRE idea is nicely discussed in Judd et al. Section 12"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#sec-diagnostics",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#sec-diagnostics",
    "title": "Modelling with Linear Regression",
    "section": "\n Workflow: Model Checking and Diagnostics",
    "text": "Workflow: Model Checking and Diagnostics\nWe will follow much of the treatment on Linear Model diagnostics, given here on the STHDA website.\n\nA first step of this regression diagnostic is to inspect the significance of the regression beta coefficients, as well as, the R.square that tells us how well the linear regression model fits to the data.\nFor example, the linear regression model makes the assumption that the relationship between the predictors (x) and the outcome variable is linear. This might not be true. The relationship could be polynomial or logarithmic.\nAdditionally, the data might contain some influential observations, such as outliers (or extreme values), that can affect the result of the regression.\nTherefore, the regression model must be closely diagnosed in order to detect potential problems and to check whether the assumptions made by the linear regression model are met or not. To do so, we generally examine the distribution of residuals errors, that can tell us more about our data.\n\n\n Checks for Uncertainty\nLet us first look at the uncertainties in the estimates of slope and intercept. These are most easily read off from the broom::tidy-ed model:\n\n# housing_lm_tidy &lt;-  housing_lm %&gt;% broom::tidy()\nhousing_lm_tidy\n\n\n\n  \n\n\n\nPlotting this is simple too:\n\nhousing_lm_tidy %&gt;%\n  gf_col(estimate ~ term, fill = ~ term, width = 0.25) %&gt;% \n  gf_hline(yintercept = 0) %&gt;% \n  gf_errorbar(conf.low + conf.high ~ term, \n              width = 0.1, \n              title = \"Model Estimates with Confidence Intervals\") %&gt;% \n  gf_theme(my_theme())\n\n\n\n\n\n Checks for Constant Variance/Heteroscedasticity\nLinear Modelling makes 4 fundamental assumptions:(“LINE”)\n\n\nLinear relationship between y and x\nObservations are independent.\nResiduals are normally distributed\nVariance of the y variable is equal at all values of x.\n\nWe can check these using checks and graphs: Here we plot the residuals against the independent/feature variable and see if there is a gross variation in their range\n\nhousing_lm_augment %&gt;% \n  gf_point(.resid ~ .fitted, title = \"Residuals vs Fitted\") %&gt;%\n  gf_smooth(method = \"loess\") %&gt;% \n  gf_theme(my_theme())\nhousing_lm_augment %&gt;% \n  gf_hline(yintercept = 0, colour = \"grey\", linewidth = 2) %&gt;%\n  gf_point(.resid ~ cmedv, title = \"Residuals vs Target Variable\") %&gt;% \n  gf_theme(my_theme())\nhousing_lm_augment %&gt;% \n  gf_dhistogram(~ .resid, title = \"Histogram of Residuals\") %&gt;% \n  gf_fitdistr()%&gt;% \n  gf_theme(my_theme())\nhousing_lm_augment %&gt;% \n  gf_qq(~ .resid, title = \"Q-Q Residuals\") %&gt;% \n  gf_qqline() %&gt;% \n  gf_theme(my_theme())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Q-Q plot of residuals also has significant deviations from the normal quartiles. The residuals are not quite “like the night sky”, i.e. random enough. These point to the need for a richer model, with more predictors. The “trend line” of residuals vs predictors show a U-shaped pattern, indicating significant nonlinearity: there is a curved relationship in the graph. The solution can be a nonlinear transformation of the predictor variables, such as \\(\\sqrt(X)\\), \\(log(X)\\), or even \\(X^2\\). For instance, we might try a model for cmedv using \\(rm^2\\) instead of just rm as we have done. This will still be a linear model!\n\n\n\n\n\n\nTip\n\n\n\nBase R has a crisp command to plot these diagnostic graphs. But we will continue to use ggformula.\n\nplot(housing_lm)\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nOne of the ggplot extension packages named lindia also has a crisp command to plot these diagnostic graphs.\n\nlibrary(lindia)\ngg_diagnose(housing_lm)\n\n\n\n\n\nThe r-squared for a model lm(cmedv ~ rm^2) shows some improvement:\n\n\n[1] 0.5501221"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#iconify-fluent-mdl2-decision-solid-conclusions",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#iconify-fluent-mdl2-decision-solid-conclusions",
    "title": "Modelling with Linear Regression",
    "section": "\n Conclusions",
    "text": "Conclusions\nWe have seen how starting from a basic EDA of the data, we have been able to choose a single Quantitative predictor variable to model a Quantitative target variable, using Linear Regression. As stated earlier, we may have wish to use more than one predictor variables, to build more sophisticated models with improved prediction capability. And there is more than one way of selecting these predictor variables, which we will examine in the Tutorials in Section 1.\nSecondly, sometimes it may be necessary to mathematically transform the variables in the dataset to enable the construction of better models, something that was not needed here.\nWe may also encounter cases where the predictor variables seem to work together; one predictor may influence “how well” another predictor works, something called an interaction effect or a synergy effect. We might then have to modify our formula to include interaction terms that look like \\(predictor1 \\times predictor2\\).\nSo our Linear Modelling workflow might look like this: we have not seen all stages yet, but that is for another course module or tutorial!\n\n\n\n\nflowchart TD\n    A[(A: Data)] --&gt;|mosaic  +  ggformula|B[B:EDA] \n    B --&gt; |corrplot +  corrgram  + ggformula + purrr + cor.test| C(C: Check Relationships)\n    C --&gt; D[D: Decide on Simple/Complex Model]\n    D --&gt; E{E: Is the Model Possible?}\n    E --&gt; |Yes| G[G: Build Model]\n    E --&gt;|Nope| F[F: Transform Variables]\n    E --&gt;|Nope| K[K: Try Multiple Regression &lt;br&gt; and/or Interaction Terms]\n    K --&gt; D\n    F --&gt; D\n    G --&gt; H{H: Check Model Diagnostics}\n    H --&gt; |Problems| D\n    H --&gt; |All   good| I(Interpret Your Model)\n    I --&gt; J(((Apply the Model for Predictions)))"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#sec-references",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#sec-references",
    "title": "Modelling with Linear Regression",
    "section": "\n References",
    "text": "References\n\nThe Boston Housing Dataset, corrected version. StatLib @ CMU, lib.stat.cmu.edu/datasets/boston_corrected.txt\nhttps://feliperego.github.io/blog/2015/10/23/Interpreting-Model-Output-In-R\nMichael Crawley, The R Book,second edition, 2013. Chapter 11.\nGareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, Introduction to Statistical Learning, Springer, 2021. Chapter 3. https://www.statlearning.com/\nDavid C Howell, Permutation Tests for Factorial ANOVA Designs\nMarti Anderson, Permutation tests for univariate or multivariate analysis of variance and regression\nSchloerke B, Cook D, Larmarange J, Briatte F, Marbach M, Thoen E, Elberg A, Crowley J (2022). GGally: Extension to ‘ggplot2’. https://ggobi.github.io/ggally/, https://github.com/ggobi/ggally.\nhttp://r-statistics.co/Assumptions-of-Linear-Regression.html\nJudd, Charles M., Gary H. McClelland, and Carey S. Ryan. 2017. “Introduction to Data Analysis.” In, 1–9. Routledge. https://doi.org/10.4324/9781315744131-1. Also see http://www.dataanalysisbook.com/index.html\nPatil, I. (2021). Visualizations with statistical details: The ‘ggstatsplot’ approach. Journal of Open Source Software, 6(61), 3167, https://doi:10.21105/joss.03167"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#footnotes",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#footnotes",
    "title": "Modelling with Linear Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\nThe model is linear in the parameters \\(\\beta_i\\), e.g. We can have this: \\[\ny_i \\sim \\beta_1*x_i + \\beta_0\\\\\n\\] or \\[\ny_1 \\sim exp(\\beta_1)*x_i + \\beta_0\n\\] but not: \\[\ny_i \\sim \\beta_1*exp(\\beta_2*x_i) + \\beta_0\\\\\n\\] or \\[\ny_i \\sim \\beta_1 *x^{\\beta_2} + \\beta_0\n\\]↩︎\nGareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, Introduction to Statistical Learning, Springer, 2021. Chapter 3. Linear Regression. Available Online↩︎\nMichael Crawley, The R Book, Third Edition 2023. Chapter 9. Statistical Modelling↩︎\nMichael Crawley, The R Book, Third Edition 2023. Chapter 9. Statistical Modelling↩︎\nhttps://indrajeetpatil.github.io/ggstatsplot/reference/ggcoefstats.html↩︎\nhttps://github.com/UCLATALL/supernova↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "",
    "text": "options(scipen = 1, digits = 3) #set to three decimal \nknitr::opts_chunk$set(echo = TRUE,warning = FALSE,message = FALSE) \nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(mosaic)\nlibrary(GGally)\nlibrary(corrgram)\nlibrary(corrplot)\nlibrary(broom)\n\n# datasets\nlibrary(ISLR)\n\n\n# Let us set a plot theme for Data visualisation\n\nmy_theme &lt;- function(){  # Creating a function\n  theme_classic() +  # Using pre-defined theme as base\n  theme(axis.text.x = element_text(size = 12, face = \"bold\"),  # Customizing axes text      \n        axis.text.y = element_text(size = 12, face = \"bold\"),\n        axis.title = element_text(size = 14, face = \"bold\"),  # Customizing axis title\n        panel.grid = element_blank(),  # Taking off the default grid\n        plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), units = , \"cm\"),\n        legend.text = element_text(size = 12, face = \"italic\"),  # Customizing legend text\n        legend.title = element_text(size = 12, face = \"bold\"),  # Customizing legend title\n        legend.position = \"right\",  # Customizing legend position\n        plot.caption = element_text(size = 12))  # Customizing plot caption\n}   \n\nIn this tutorial, we will use the Boston housing Hitters dataset(s) from the ISLR package. Our research question is:\n\n\n\n\n\n\nResearch Question\n\n\n\nHow do we predict the Salary of baseball players based on other Quantitative parameters such as Hits, HmRun AtBat?\nAnd how do we choose the “best” model, based on a trade-off between Model Complexity and Model Accuracy?"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#iconify-noto-v1-package-setting-up-r-packages",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#iconify-noto-v1-package-setting-up-r-packages",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "",
    "text": "options(scipen = 1, digits = 3) #set to three decimal \nknitr::opts_chunk$set(echo = TRUE,warning = FALSE,message = FALSE) \nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(mosaic)\nlibrary(GGally)\nlibrary(corrgram)\nlibrary(corrplot)\nlibrary(broom)\n\n# datasets\nlibrary(ISLR)\n\n\n# Let us set a plot theme for Data visualisation\n\nmy_theme &lt;- function(){  # Creating a function\n  theme_classic() +  # Using pre-defined theme as base\n  theme(axis.text.x = element_text(size = 12, face = \"bold\"),  # Customizing axes text      \n        axis.text.y = element_text(size = 12, face = \"bold\"),\n        axis.title = element_text(size = 14, face = \"bold\"),  # Customizing axis title\n        panel.grid = element_blank(),  # Taking off the default grid\n        plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), units = , \"cm\"),\n        legend.text = element_text(size = 12, face = \"italic\"),  # Customizing legend text\n        legend.title = element_text(size = 12, face = \"bold\"),  # Customizing legend title\n        legend.position = \"right\",  # Customizing legend position\n        plot.caption = element_text(size = 12))  # Customizing plot caption\n}   \n\nIn this tutorial, we will use the Boston housing Hitters dataset(s) from the ISLR package. Our research question is:\n\n\n\n\n\n\nResearch Question\n\n\n\nHow do we predict the Salary of baseball players based on other Quantitative parameters such as Hits, HmRun AtBat?\nAnd how do we choose the “best” model, based on a trade-off between Model Complexity and Model Accuracy?"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#iconify-flat-color-icons-workflow-workflow-plan",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#iconify-flat-color-icons-workflow-workflow-plan",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "\n Workflow Plan",
    "text": "Workflow Plan\nOur target variable is Salary.\nWe will start with an examination of correlations between Salary and other Quant predictors.\nWe will use a null model for our Linear Regression at first, keeping just an intercept term. Based on the examination of the r-square improvement offered by each predictor individually, we will add another quantitative predictor. We will follow this process through up to a point where the gains in model accuracy are good enough to justify the additional model complexity.\n\n\n\n\n\n\nNote\n\n\n\nThis approach is the exact opposite of the earlier tutorial on multiple linear regression, where we started with a maximal model and trimmed it down based on an assessment of r.squared."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#iconify-flat-color-icons-workflow-workflow-eda",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#iconify-flat-color-icons-workflow-workflow-eda",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "\n Workflow: EDA",
    "text": "Workflow: EDA\nThe Hitters dataset has the following variables:\n\ndata(\"Hitters\")\ninspect(Hitters)\n\n\ncategorical variables:  \n       name  class levels   n missing\n1    League factor      2 322       0\n2  Division factor      2 322       0\n3 NewLeague factor      2 322       0\n                                   distribution\n1 A (54.3%), N (45.7%)                         \n2 W (51.2%), E (48.8%)                         \n3 A (54.7%), N (45.3%)                         \n\nquantitative variables:  \n      name   class  min    Q1 median     Q3   max    mean      sd   n missing\n1    AtBat integer 16.0 255.2  379.5  512.0   687  380.93  153.40 322       0\n2     Hits integer  1.0  64.0   96.0  137.0   238  101.02   46.45 322       0\n3    HmRun integer  0.0   4.0    8.0   16.0    40   10.77    8.71 322       0\n4     Runs integer  0.0  30.2   48.0   69.0   130   50.91   26.02 322       0\n5      RBI integer  0.0  28.0   44.0   64.8   121   48.03   26.17 322       0\n6    Walks integer  0.0  22.0   35.0   53.0   105   38.74   21.64 322       0\n7    Years integer  1.0   4.0    6.0   11.0    24    7.44    4.93 322       0\n8   CAtBat integer 19.0 816.8 1928.0 3924.2 14053 2648.68 2324.21 322       0\n9    CHits integer  4.0 209.0  508.0 1059.2  4256  717.57  654.47 322       0\n10  CHmRun integer  0.0  14.0   37.5   90.0   548   69.49   86.27 322       0\n11   CRuns integer  1.0 100.2  247.0  526.2  2165  358.80  334.11 322       0\n12    CRBI integer  0.0  88.8  220.5  426.2  1659  330.12  333.22 322       0\n13  CWalks integer  0.0  67.2  170.5  339.2  1566  260.24  267.06 322       0\n14 PutOuts integer  0.0 109.2  212.0  325.0  1378  288.94  280.70 322       0\n15 Assists integer  0.0   7.0   39.5  166.0   492  106.91  136.85 322       0\n16  Errors integer  0.0   3.0    6.0   11.0    32    8.04    6.37 322       0\n17  Salary numeric 67.5 190.0  425.0  750.0  2460  535.93  451.12 263      59\n\n\n\n Scatter Plots and Correlations\nWe should examine scatter plots and Correlations of Salary against these variables. Let us select a few sets of Quantitative and Qualitative features, along with the target variable Salary and do a pairs-plots with them:\n\ntheme_set(theme_bw())\nHitters %&gt;% \n  select(Salary, AtBat, Hits, HmRun) %&gt;% \n  GGally::ggpairs(title = \"Plot 1\", lower = list(continuous = wrap(\"smooth\", alpha = 0.2)))\nHitters %&gt;% \n  select(Salary, Runs, RBI, Walks,Years) %&gt;% \n  GGally::ggpairs(title = \"Plot 2\", lower = list(continuous = wrap(\"smooth\", alpha = 0.2)))\nHitters %&gt;% \n  select(Salary, CRBI, CAtBat, CHits, CHmRun, CRuns, CWalks) %&gt;% \n  GGally::ggpairs(title = \"Plot 3\", lower = list(continuous = wrap(\"smooth\", alpha = 0.2)))\nHitters %&gt;% \n  select(Salary, PutOuts,Assists,Errors) %&gt;% \n  GGally::ggpairs(title = \"Plot 4\", lower = list(continuous = wrap(\"smooth\", alpha = 0.2)))\nHitters %&gt;% \n  select(Salary, League,Division,NewLeague) %&gt;% \n  GGally::ggpairs(title = \"Plot 5\", lower = list(continuous = wrap(\"smooth\", alpha = 0.2)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAtBat and Hits seem relevant predictors for Salary. So are Runs, RBI,Walks, and Years. From Plot 2, both RBI and Walks are also inter-correlated with Runs. All the C* variables are well correlated with Salary and also among one another. (Plot3). Plot 4 has no significant correlations at all. Plot 5 shows Salary nearly equally distributed across League, Division, and NewLeague.\n\n Correlation Error-Bars\nWe can also plot all correlations in one graph using cor.test and purrr:\n\nall_corrs &lt;- \n  Hitters %&gt;% \n  select(where(is.numeric)) %&gt;% \n  \n  # leave off Salary and year to get all the remaining ones\n  select(- Salary) %&gt;% \n  \n  \n  # perform a cor.test for all variables against Salary\n  purrr::map(.x = .,\n             .f = \\(x) cor.test(x, Hitters$Salary)) %&gt;%\n  \n  # tidy up the cor.test outputs into a tidy data frame\n  map_dfr(broom::tidy, .id = \"predictor\") %&gt;% \n  arrange(desc(estimate))\n\nall_corrs\nall_corrs %&gt;%\n  gf_hline(yintercept = 0,\n           linewidth = 2,\n           color = \"grey\") %&gt;%\n  gf_errorbar(\n    conf.high + conf.low ~ reorder(predictor, estimate),\n    color = ~ -log10(p.value),\n    linewidth =  ~ -log10(p.value),\n    width = 0.5,\n    caption = \"Significance = -log10(p.value)\"\n  ) %&gt;%\n  gf_point(estimate ~ reorder(predictor, estimate)) %&gt;%\n  gf_labs(x = NULL, y = \"Correlation with Salary\") %&gt;%\n  gf_theme(theme = my_theme()) %&gt;%\n  gf_refine(\n    scale_colour_distiller(\"Correlation\", type = \"div\",\n                           palette = \"RdBu\"),\n    scale_linewidth_continuous(\"Significance\", range = c(0.25, 3))\n  ) %&gt;%\n  gf_refine(guides(linewidth = guide_legend(reverse = TRUE)),\n            theme(axis.text.x = element_text(angle = 30, hjust = 1))) %&gt;%\n  gf_refine(guides(linewidth = guide_legend(reverse = TRUE)))\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nThere are a good many predictors which have statistically significant correlations with Salary, such as CRuns , CHmRun. The fatter the bar, the higher is the significance of the correlation.\nWe now start with setting up simple Linear Regressions with no predictors, only an intercept. We then fit separate Linear Models using each predictor individually. Then based on the the improvement in r.squared offered by each predictor, we progressively add it to the model, until we are “satisfied” with the quality of the model ( using rsquared and other means).\nLet us now do this."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#iconify-flat-color-icons-workflow-workflow-minimal-multiple-regression-model",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#iconify-flat-color-icons-workflow-workflow-minimal-multiple-regression-model",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "\n Workflow: Minimal Multiple Regression Model",
    "text": "Workflow: Minimal Multiple Regression Model\nNote the formula structure here: we want just and intercept.\n\nlm_min &lt;- lm(data = Hitters, Salary ~ 1)\nsummary(lm_min)\n\n\nCall:\nlm(formula = Salary ~ 1, data = Hitters)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n  -468   -346   -111    214   1924 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    535.9       27.8    19.3   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 451 on 262 degrees of freedom\n  (59 observations deleted due to missingness)\n\n\n\nlm_min %&gt;% broom::tidy()\nlm_min %&gt;% broom::glance()\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n\n\n\n\nOK, so the intercept is highly significant, the t-statistic is also high, but the intercept contributes nothing to the r.squared!!"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#iconify-flat-color-icons-workflow-workflow-predictor-addition-round1",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#iconify-flat-color-icons-workflow-workflow-predictor-addition-round1",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "\n Workflow: Predictor Addition (Round#1)",
    "text": "Workflow: Predictor Addition (Round#1)\nWe will now set up individual models for each predictor and look at the p.value and r.squared offered by each separate model:\n\nnames &lt;- names(Hitters %&gt;%\n  select(where(is.numeric), \n         -c(Salary)))\n\nn_vars &lt;- length(names)\n\nHitters_model_set &lt;- tibble(all_vars = list(names),\n                            keep_vars = seq(1, n_vars),\n                            data = list(Hitters))\n\n# Unleash purrr in a series of mutates\nHitters_model_set &lt;- Hitters_model_set %&gt;%\n  \n# Select Single Predictor for each Simple Model\n  mutate(mod_vars =\n           pmap(\n             .l = list(all_vars, keep_vars, data),\n             .f = \\(all_vars, keep_vars, data) all_vars[keep_vars]\n           )) %&gt;%\n  \n# build formulae with these for linear regression\n  mutate(formula = map(.x = mod_vars,\n                       .f = \\(mod_vars) as.formula(paste(\n                         \"Salary ~\", paste(mod_vars, collapse = \"+\")\n                       )))) %&gt;%\n  \n# use the formulae to build multiple linear models\n  mutate(models =\n           pmap(\n             .l = list(data, formula),\n             .f = \\(data, formula) lm(formula, data = data)\n           ))\n\n\n# Tidy up the models using broom to expose their metrics\nHitters_model_set &lt;- \n  Hitters_model_set %&gt;% \n  mutate(tidy_models =\n           map(\n             .x = models,\n             .f = \\(x) broom::glance(x,\n                                     conf.int = TRUE,\n                                     conf.lvel = 0.95)\n           ),\n         predictor_name = names[keep_vars]) %&gt;% \n\n  # Remove unwanted columns, keep model and predictor count\n  select(keep_vars,predictor_name, tidy_models) %&gt;%\n  unnest(tidy_models) %&gt;% \n  arrange(desc(r.squared))\n\n# Check everything after the operation\nHitters_model_set\n# Plot r.squared vs predictor count\nHitters_model_set %&gt;% \n  gf_point(r.squared ~ reorder(predictor_name, r.squared), \n           size = 3.5, \n           color = \"black\",\n           ylab = \"R.Squared\",\n           xlab = \"Params in the Linear Model\") %&gt;%\n  gf_theme(my_theme()) %&gt;% \n  gf_refine(theme(axis.text.x = element_text(angle = 30,\n                                             hjust = 1)))\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n# Which is the winning Predictor?\nwinner &lt;- Hitters_model_set %&gt;% \n  arrange(desc(r.squared)) %&gt;% \n  select(predictor_name) %&gt;% \n  head(1) %&gt;% as.character()\nwinner\n\n[1] \"CRBI\"\n\n\n\n# Here is the Round 1 Model\n# Minimal model updated to included winning predictor\nlm_round1 &lt;- update(lm_min, ~. + CRBI)\nlm_round1 %&gt;% broom::tidy()\nlm_round1 %&gt;% broom::glance()\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n\n\n\n\nSo we can add CRBI as a predictor to our model as a predictor gives us an improved r.squared of \\(0.321\\), which is the square of the correlation between Salary and CRBI, \\(.567\\).\nAnd the model itself is: \\[\nSalary \\sim 274.580 + 0.791 \\times CRBI\n\\tag{1}\\]\nLet’s press on to Round 2."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#iconify-flat-color-icons-workflow-workflow-predictor-addition-round-2",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#iconify-flat-color-icons-workflow-workflow-predictor-addition-round-2",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "\n Workflow: Predictor Addition (Round #2)",
    "text": "Workflow: Predictor Addition (Round #2)\nWe will set up a round-2 model using CRBI as the predictor, and then proceed to add each of the other predictors as an update to the model.\n\n# Preliminaries\nnames &lt;- names(Hitters %&gt;%\n  select(where(is.numeric), -c(Salary, winner)))\n# names\n\nn_vars &lt;- length(names)\n# n_vars\n# names &lt;- names %&gt;% str_remove(winner)\n# names\n# n_vars &lt;- n_vars-1\n\n\n# Round 2 Iteration\nHitters_model_set &lt;- tibble(all_vars = list(names),\n                            keep_vars = seq(1, n_vars),\n                            data = list(Hitters))\n# Hitters_model_set \n\n# Unleash purrr in a series of mutates\nHitters_model_set &lt;- Hitters_model_set %&gt;%\n  \n# list of predictor variables for each model\n  mutate(mod_vars =\n           pmap(\n             .l = list(all_vars, keep_vars, data),\n             .f = \\(all_vars, keep_vars, data) all_vars[keep_vars]\n           )) %&gt;%\n  \n# build formulae with these for linear regression\n  mutate(formula = map(.x = mod_vars,\n                       .f = \\(mod_vars) as.formula(paste(\n                         \"Salary ~ CRBI +\", paste(mod_vars, collapse = \"+\")\n                       )))) %&gt;%\n  \n# use the formulae to build multiple linear models\n  mutate(models =\n           pmap(\n             .l = list(data, formula),\n             .f = \\(data, formula) lm(formula, data = data)\n           ))\n# Check everything after the operation\n# Hitters_model_set\n\n# Tidy up the models using broom to expose their metrics\nHitters_model_set &lt;- \n  Hitters_model_set %&gt;% \n  mutate(tidy_models =\n           map(\n             .x = models,\n             .f = \\(x) broom::glance(x,\n                                     conf.int = TRUE,\n                                     conf.lvel = 0.95)\n           ),\n         predictor_name = names[keep_vars]) %&gt;% \n\n  # Remove unwanted columns, keep model and predictor count\n  select(keep_vars,predictor_name, tidy_models) %&gt;%\n  unnest(tidy_models) %&gt;% \n  arrange(desc(r.squared))\n\nHitters_model_set\n# Plot r.squared vs predictor count\nHitters_model_set %&gt;% \n  gf_point(r.squared ~ reorder(predictor_name, r.squared), \n                               size = 3.5,\n                               ylab = \"R.Squared\",\n                               xlab = \"Param in the Linear Model\") %&gt;%\n  gf_theme(my_theme()) %&gt;% \n  gf_refine(theme(axis.text.x = element_text(angle = 30, \n                                             hjust = 1)))\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n# Which is the winning Predictor?\n# \nwinner &lt;- Hitters_model_set %&gt;% \n  arrange(desc(r.squared)) %&gt;% \n  select(predictor_name) %&gt;% \n  head(1) %&gt;% as.character()\nwinner\n# Here is the Round 1 Model\nlm_round2 &lt;- update(lm_round1, ~. + Hits)\nlm_round2 %&gt;% broom::tidy()\nlm_round2 %&gt;% broom::glance()\n\n\n\n[1] \"Hits\"\n\n\n\n\n  \n\n\n\n\n\n  \n\n\n\n\n\nAnd now the model itself is: \\[\nSalary \\sim -47.96 + 0.691 \\times CRBI + 3.30 \\times Hits\n\\tag{2}\\]\nNote the change in both intercept and the slope for CRBI when the new predictor Hits is added!!"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#iconify-openmoji-chart-increasing-workflow-visualization",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#iconify-openmoji-chart-increasing-workflow-visualization",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "\n Workflow: Visualization",
    "text": "Workflow: Visualization\nLet us quickly see how this model might look. We know that with simple regression, we obtain a straight line as our model. Here, with two (or more) predictors, we should obtain a ….(hyper)plane!"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#iconify-octicon-feed-discussion-16-discussion",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#iconify-octicon-feed-discussion-16-discussion",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "\n Discussion",
    "text": "Discussion\nIt is interesting that the second variable to be added was Hits which has a lower correlation of \\(r = 0.439\\) with Salary compared to some other Quant predictors such as Chits( \\(r = 0.525\\) ). This is because CRBI is hugely correlated with all of these predictors, so CRBI effectively acts as a proxy for all of these. See Plot 3.\nWe see that adding Hits to the model gives us an improved r.squared of \\(0.425\\)."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#iconify-fluent-mdl2-decision-solid-conclusion",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#iconify-fluent-mdl2-decision-solid-conclusion",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "\n Conclusion",
    "text": "Conclusion\nWe can proceed in this way to subsequent rounds and decide to stop when the model complexity (no. of predictors ) and the resulting gain in r.squared does not seem worth it.\n\n\n\n\n\n\nIteration Method\n\n\n\nWe ought to convert the above code into an R function and run it that way for a specific number of rounds to see how things pan out. That is in the next version of this Tutorial! 😇"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#iconify-ooui-references-rtl-references",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#iconify-ooui-references-rtl-references",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "\n References",
    "text": "References\n\nhttps://ethanwicker.com/2021-01-11-multiple-linear-regression-002/"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html",
    "title": "Tutorial: Multiple Linear Regression with Backward Selection",
    "section": "",
    "text": "options(scipen = 1, digits = 3) #set to three decimal \nknitr::opts_chunk$set(echo = TRUE,warning = FALSE,message = FALSE)\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(mosaic)\nlibrary(infer)\n\n\n# Let us set a plot theme for Data visualisation\n\n# my_theme &lt;- function(){  # Creating a function\n#   theme_classic() +  # Using pre-defined theme as base\n#   theme(axis.text.x = element_text(size = 12, face = \"bold\"),  # Customizing axes text      \n#         axis.text.y = element_text(size = 12, face = \"bold\"),\n#         axis.title = element_text(size = 14, face = \"bold\"),  # Customizing axis title\n#         panel.grid = element_blank(),  # Taking off the default grid\n#         plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), units = , \"cm\"),\n#         legend.text = element_text(size = 12, face = \"italic\"),  # Customizing legend text\n#         legend.title = element_text(size = 12, face = \"bold\"),  # Customizing legend title\n#         legend.position = \"right\",  # Customizing legend position\n#         plot.caption = element_text(size = 12))  # Customizing plot caption\n# }   \n\nmy_theme &lt;- function(){  # Creating a function\n  theme_classic() +  # Using pre-defined theme as base\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        axis.text.x = element_text(size = 10, face = \"bold\"),  \n        # Customizing axes text      \n        axis.text.y = element_text(size = 10, face = \"bold\"),\n        axis.title = element_text(size = 12, face = \"bold\"),  \n        # Customizing axis title\n        panel.grid = element_blank(),  # Taking off the default grid\n        plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), units = , \"cm\"),\n        legend.text = element_text(size = 8, face = \"italic\"),  \n        # Customizing legend text\n        legend.title = element_text(size = 10, face = \"bold\"),  \n        # Customizing legend title\n        legend.position = \"right\",  # Customizing legend position\n        plot.caption = element_text(size = 8))  # Customizing plot caption\n}   \n\nIn this tutorial, we will use the Boston housing dataset. Our research question is:\n\n\n\n\n\n\nResearch Question\n\n\n\nHow do we predict the price of a house in Boston, based on other parameters Quantitative parameters such as area, location, rooms, and crime-rate in the neighbourhood?\nAnd how do we choose the “best” model, based on a tradeoff between Model Complexity and Model Accuracy?"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#iconify-noto-v1-package-setting-up-r-packages",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#iconify-noto-v1-package-setting-up-r-packages",
    "title": "Tutorial: Multiple Linear Regression with Backward Selection",
    "section": "",
    "text": "options(scipen = 1, digits = 3) #set to three decimal \nknitr::opts_chunk$set(echo = TRUE,warning = FALSE,message = FALSE)\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(mosaic)\nlibrary(infer)\n\n\n# Let us set a plot theme for Data visualisation\n\n# my_theme &lt;- function(){  # Creating a function\n#   theme_classic() +  # Using pre-defined theme as base\n#   theme(axis.text.x = element_text(size = 12, face = \"bold\"),  # Customizing axes text      \n#         axis.text.y = element_text(size = 12, face = \"bold\"),\n#         axis.title = element_text(size = 14, face = \"bold\"),  # Customizing axis title\n#         panel.grid = element_blank(),  # Taking off the default grid\n#         plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), units = , \"cm\"),\n#         legend.text = element_text(size = 12, face = \"italic\"),  # Customizing legend text\n#         legend.title = element_text(size = 12, face = \"bold\"),  # Customizing legend title\n#         legend.position = \"right\",  # Customizing legend position\n#         plot.caption = element_text(size = 12))  # Customizing plot caption\n# }   \n\nmy_theme &lt;- function(){  # Creating a function\n  theme_classic() +  # Using pre-defined theme as base\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        axis.text.x = element_text(size = 10, face = \"bold\"),  \n        # Customizing axes text      \n        axis.text.y = element_text(size = 10, face = \"bold\"),\n        axis.title = element_text(size = 12, face = \"bold\"),  \n        # Customizing axis title\n        panel.grid = element_blank(),  # Taking off the default grid\n        plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), units = , \"cm\"),\n        legend.text = element_text(size = 8, face = \"italic\"),  \n        # Customizing legend text\n        legend.title = element_text(size = 10, face = \"bold\"),  \n        # Customizing legend title\n        legend.position = \"right\",  # Customizing legend position\n        plot.caption = element_text(size = 8))  # Customizing plot caption\n}   \n\nIn this tutorial, we will use the Boston housing dataset. Our research question is:\n\n\n\n\n\n\nResearch Question\n\n\n\nHow do we predict the price of a house in Boston, based on other parameters Quantitative parameters such as area, location, rooms, and crime-rate in the neighbourhood?\nAnd how do we choose the “best” model, based on a tradeoff between Model Complexity and Model Accuracy?"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#iconify-flat-color-icons-workflow-workflow-read-the-data",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#iconify-flat-color-icons-workflow-workflow-read-the-data",
    "title": "Tutorial: Multiple Linear Regression with Backward Selection",
    "section": "\n Workflow: Read the Data",
    "text": "Workflow: Read the Data\n\ndata(\"BostonHousing2\", package = \"mlbench\")\nhousing &lt;- BostonHousing2\ninspect(housing)\n\n\ncategorical variables:  \n  name  class levels   n missing                                  distribution\n1 town factor     92 506       0 Cambridge (5.9%) ...                         \n2 chas factor      2 506       0 0 (93.1%), 1 (6.9%)                          \n\nquantitative variables:  \n      name   class       min       Q1   median       Q3      max     mean\n1    tract integer   1.00000 1303.250 3393.500 3739.750 5082.000 2700.356\n2      lon numeric -71.28950  -71.093  -71.053  -71.020  -70.810  -71.056\n3      lat numeric  42.03000   42.181   42.218   42.252   42.381   42.216\n4     medv numeric   5.00000   17.025   21.200   25.000   50.000   22.533\n5    cmedv numeric   5.00000   17.025   21.200   25.000   50.000   22.529\n6     crim numeric   0.00632    0.082    0.257    3.677   88.976    3.614\n7       zn numeric   0.00000    0.000    0.000   12.500  100.000   11.364\n8    indus numeric   0.46000    5.190    9.690   18.100   27.740   11.137\n9      nox numeric   0.38500    0.449    0.538    0.624    0.871    0.555\n10      rm numeric   3.56100    5.886    6.208    6.623    8.780    6.285\n11     age numeric   2.90000   45.025   77.500   94.075  100.000   68.575\n12     dis numeric   1.12960    2.100    3.207    5.188   12.127    3.795\n13     rad integer   1.00000    4.000    5.000   24.000   24.000    9.549\n14     tax integer 187.00000  279.000  330.000  666.000  711.000  408.237\n15 ptratio numeric  12.60000   17.400   19.050   20.200   22.000   18.456\n16       b numeric   0.32000  375.377  391.440  396.225  396.900  356.674\n17   lstat numeric   1.73000    6.950   11.360   16.955   37.970   12.653\n          sd   n missing\n1  1380.0368 506       0\n2     0.0754 506       0\n3     0.0618 506       0\n4     9.1971 506       0\n5     9.1822 506       0\n6     8.6015 506       0\n7    23.3225 506       0\n8     6.8604 506       0\n9     0.1159 506       0\n10    0.7026 506       0\n11   28.1489 506       0\n12    2.1057 506       0\n13    8.7073 506       0\n14  168.5371 506       0\n15    2.1649 506       0\n16   91.2949 506       0\n17    7.1411 506       0\n\n\nThe original data are 506 observations on 14 variables, medv being the target variable:\n\n\n\n\n\n\n\ncrim\nper capita crime rate by town\n\n\nzn\nproportion of residential land zoned for lots over 25,000 sq.ft\n\n\nindus\nproportion of non-retail business acres per town\n\n\nchas\nCharles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n\n\nnox\nnitric oxides concentration (parts per 10 million)\n\n\nrm\naverage number of rooms per dwelling\n\n\nage\nproportion of owner-occupied units built prior to 1940\n\n\ndis\nweighted distances to five Boston employment centres\n\n\nrad\nindex of accessibility to radial highways\n\n\ntax\nfull-value property-tax rate per USD 10,000\n\n\nptratio\npupil-teacher ratio by town\n\n\nb\n\n\\(1000(B - 0.63)^2\\) where B is the proportion of Blacks by town\n\n\nlstat\npercentage of lower status of the population\n\n\nmedv\nmedian value of owner-occupied homes in USD 1000’s\n\n\n\nThe corrected data set has the following additional columns:\n\n\ncmedv\ncorrected median value of owner-occupied homes in USD 1000’s\n\n\ntown\nname of town\n\n\ntract\ncensus tract\n\n\nlon\nlongitude of census tract\n\n\nlat\nlatitude of census tract\n\n\nOur response variable is cmedv, the corrected median value of owner-occupied homes in USD 1000’s. Their are many Quantitative feature variables that we can use to predict cmedv. And there are two Qualitative features, chas and tax."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#iconify-flat-color-icons-workflow-workflow-correlations",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#iconify-flat-color-icons-workflow-workflow-correlations",
    "title": "Tutorial: Multiple Linear Regression with Backward Selection",
    "section": "\n Workflow: Correlations",
    "text": "Workflow: Correlations\nWe can use purrr to evaluate all pair-wise correlations in one shot:\n\nall_corrs &lt;- housing %&gt;% \n  select(where(is.numeric)) %&gt;% \n  \n  # leave off cmedv/medv to get all the remaining ones\n  select(- cmedv, -medv) %&gt;%  \n  \n  # perform a cor.test for all variables against cmedv\n  purrr::map(.x = .,\n             .f = \\(x) cor.test(x, housing$cmedv)) %&gt;%\n  \n  # tidy up the cor.test outputs into a tidy data frame\n  map_dfr(broom::tidy, .id = \"predictor\") \n\nall_corrs\n\n\n\n  \n\n\nall_corrs %&gt;%\n  gf_hline(yintercept = 0,\n           color = \"grey\",\n           linewidth = 2) %&gt;%\n  gf_errorbar(\n    conf.high + conf.low ~ reorder(predictor, estimate),\n    colour = ~ estimate,\n    width = 0.5,\n    linewidth = ~ -log10(p.value),\n    caption = \"Significance = -log10(p.value)\"\n  ) %&gt;%\n  gf_point(estimate ~ reorder(predictor, estimate)) %&gt;%\n  gf_labs(x = \"Predictors\", y = \"Correlation with Median House Price\") %&gt;%\n  gf_theme(my_theme()) %&gt;%\n  gf_theme(theme(axis.text.x = element_text(angle = 45, hjust = 1))) %&gt;%\n  gf_refine(\n    scale_colour_distiller(\"Correlation\", type = \"div\", palette = \"RdBu\"),\n    scale_linewidth_continuous(\"Significance\", range = c(0.25, 3))\n  ) %&gt;%\n  gf_refine(guides(linewidth = guide_legend(reverse = TRUE)))\n\n\n\n\nThe variables rm, lstat seem to have high correlations with cmedv which are also statistically significant."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#iconify-flat-color-icons-workflow-workflow-maximal-multiple-regression-model",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#iconify-flat-color-icons-workflow-workflow-maximal-multiple-regression-model",
    "title": "Tutorial: Multiple Linear Regression with Backward Selection",
    "section": "\n Workflow: Maximal Multiple Regression Model",
    "text": "Workflow: Maximal Multiple Regression Model\nWe will create a regression model for cmedv using all the other numerical predictor features in the dataset.\n\nhousing_numeric &lt;- housing %&gt;% select(where(is.numeric), \n                                      \n                    # remove medv\n                    # an older version of cmedv\n                                      -c(medv))\nnames(housing_numeric) # 16 variables, one target, 15 predictors\n\n [1] \"tract\"   \"lon\"     \"lat\"     \"cmedv\"   \"crim\"    \"zn\"      \"indus\"  \n [8] \"nox\"     \"rm\"      \"age\"     \"dis\"     \"rad\"     \"tax\"     \"ptratio\"\n[15] \"b\"       \"lstat\"  \n\nhousing_maximal &lt;- lm(cmedv ~ ., data = housing_numeric)\nsummary(housing_maximal)\n\n\nCall:\nlm(formula = cmedv ~ ., data = housing_numeric)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-13.934  -2.752  -0.619   1.711  26.120 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -3.45e+02   3.23e+02   -1.07  0.28734    \ntract       -7.52e-04   4.46e-04   -1.69  0.09231 .  \nlon         -6.79e+00   3.44e+00   -1.98  0.04870 *  \nlat         -2.35e+00   5.36e+00   -0.44  0.66074    \ncrim        -1.09e-01   3.28e-02   -3.32  0.00097 ***\nzn           4.40e-02   1.39e-02    3.17  0.00164 ** \nindus        2.75e-02   6.20e-02    0.44  0.65692    \nnox         -1.55e+01   4.03e+00   -3.85  0.00014 ***\nrm           3.81e+00   4.20e-01    9.07  &lt; 2e-16 ***\nage          5.82e-03   1.34e-02    0.43  0.66416    \ndis         -1.38e+00   2.10e-01   -6.59  1.1e-10 ***\nrad          2.36e-01   8.47e-02    2.78  0.00558 ** \ntax         -1.48e-02   3.74e-03   -3.96  8.5e-05 ***\nptratio     -9.49e-01   1.41e-01   -6.73  4.7e-11 ***\nb            9.55e-03   2.67e-03    3.57  0.00039 ***\nlstat       -5.46e-01   5.06e-02  -10.80  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.73 on 490 degrees of freedom\nMultiple R-squared:  0.743, Adjusted R-squared:  0.735 \nF-statistic: 94.3 on 15 and 490 DF,  p-value: &lt;2e-16\n\n\nThe maximal model has an R.squared of \\(0.7426\\) which is much better than that we obtained for a simple model based on rm alone. How much can we simplify this maximal model, without losing out on R.squared?"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#iconify-flat-color-icons-workflow-workflow-model-reduction",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#iconify-flat-color-icons-workflow-workflow-model-reduction",
    "title": "Tutorial: Multiple Linear Regression with Backward Selection",
    "section": "\n Workflow: Model Reduction",
    "text": "Workflow: Model Reduction\nWe now proceed naively by removing one predictor after another. We will resort to what may amount to p-hacking by sorting the predictors based on their p-value1 in the maximal model and removing them in decreasing order of their p-value.\nWe will also use some powerful features from the purrr package (also part of the tidyverse), to create all these models all at once. Then we will be able to plot their R.squared values together and decide where we wish to trade off Explainability vs Complexity for our model.\n\n# No of Quant predictor variables in the dataset\nn_vars &lt;- housing %&gt;%\n  select(where(is.numeric), -c(cmedv, medv)) %&gt;%\n  length()\n\n# Maximal Model, now tidied\nhousing_maximal_tidy &lt;- \n  housing_maximal %&gt;% \n  broom::tidy() %&gt;% \n  \n# Obviously remove \"Intercept\" ;-D\n  filter(term != \"(Intercept)\") %&gt;% \n  \n# And horrors! Sort variables by p.value\n  arrange(p.value)\n\nhousing_maximal_tidy\n\n\n\n  \n\n\n\nThe last 5 variables are clearly statistically insignificant.\nAnd now we unleash the purrr package to create all the simplified models at once. We will construct a dataset containing three columns:\n\nA list of all quantitative predictor variables\nA sequence of numbers from 1 to N(predictor)\n\nA “list” column containing the housing data frame itself\n\nWe will use the iteration capability of purrr to sequentially drop one variable at a time from the maximal(15 predictor) model, build a new reduced model each time, and compute the r.squared:\n\n\n\nhousing_model_set &lt;- tibble(all_vars = \n                            list(housing_maximal_tidy$term), # p-hacked order!!\n                            keep_vars = seq(1, n_vars),\n                            data = list(housing_numeric))\nhousing_model_set\n\n\n\n  \n\n\n# Unleash purrr in a series of mutates\nhousing_model_set &lt;- housing_model_set %&gt;%\n  \n# list of predictor variables for each model\n  mutate(mod_vars =\n           pmap(\n             .l = list(all_vars, keep_vars, data),\n             .f = \\(all_vars, keep_vars, data) all_vars[1:keep_vars]\n           )) %&gt;%\n  \n# build formulae with these for linear regression\n  mutate(formula = \n           map(.x = mod_vars,\n               .f = \\(mod_vars) as.formula(paste(\n                         \"cmedv ~\", paste(mod_vars, collapse = \"+\")\n                       )))) %&gt;%\n  \n# use the formulae to build multiple linear models\n  mutate(models =\n           pmap(\n             .l = list(data, formula),\n             .f = \\(data, formula) lm(formula, data = data)\n           ))\n# Check everything after the operation\nhousing_model_set\n\n\n\n  \n\n\n# Tidy up the models using broom to expose their metrics\nhousing_models_tidy &lt;- housing_model_set %&gt;% \n  mutate(tidy_models =\n           map(\n             .x = models,\n             .f = \\(x) broom::glance(x,\n                                     conf.int = TRUE,\n                                     conf.lvel = 0.95)\n           )) %&gt;% \n\n  # Remove unwanted columns, keep model and predictor count\n  select(keep_vars, tidy_models) %&gt;%\n  unnest(tidy_models)\n\nhousing_models_tidy %&gt;%\n  gf_line(\n    r.squared ~ keep_vars,\n    ylab = \"R.Squared\",\n    xlab = \"No. params in the Linear Model\",\n    title = \"Model Explainability vs Complexity\",\n    subtitle = \"Model r.squared vs No. of Predictors\",\n    data = .\n  ) %&gt;%\n  \n  # Plot r.squared vs predictor count\n  gf_point(r.squared ~ keep_vars,\n           size = 3.5, color = \"grey\") %&gt;%\n  \n  # Show off the selected best model\n  gf_point(\n    r.squared ~ keep_vars,\n    size = 3.5,\n    color = \"red\",\n    data = housing_models_tidy %&gt;% filter(keep_vars == 4)\n  ) %&gt;%\n  \n  gf_hline(yintercept = 0.7, linetype = \"dashed\") %&gt;%\n  gf_theme(my_theme())\n\n\n\n\n\n\nAt the loss of some 5% in the r.squared, we can drop our model complexity from 15 predictors to say 4! Our final model will then be:\n\nhousing_model_final &lt;- \n  housing_model_set %&gt;% \n  \n  # filter for best model, with 4 variables\n  filter(keep_vars == 4) %&gt;% \n  \n  # tidy up the model\n  mutate(tidy_models =\n           map(\n             .x = models,\n             .f = \\(x) broom::tidy(x,\n                                     conf.int = TRUE,\n                                     conf.lvel = 0.95)\n           )) %&gt;% \n  \n  # Remove unwanted columns, keep model and predictor count\n  select(keep_vars, models, tidy_models) %&gt;%\n  unnest(tidy_models)\n\nhousing_model_final\n\n\n\n  \n\n\nhousing_model_final %&gt;%  \n  # And plot the model\n  # Remove the intercept term\n  filter(term != \"(Intercept)\") %&gt;% \n  gf_col(estimate ~ term, fill = ~ term, width = 0.25) %&gt;% \n  gf_hline(yintercept = 0) %&gt;% \n  gf_errorbar(conf.low + conf.high ~ term, \n              width = 0.1, \n              title = \"Multiple Regression\",\n              subtitle = \"Model Estimates with Confidence Intervals\") %&gt;% \n  gf_theme(my_theme())\n\n\n\n\nOur current best model can be stated as:\n\\[\n\\widehat{cmedv} \\sim 24.459 - 0.563 * dis - 0.673 * lstat - 0.957 * ptratio  + 4.199 * rm\n\\]"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#iconify-flat-color-icons-workflow-workflow-diagnostics",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#iconify-flat-color-icons-workflow-workflow-diagnostics",
    "title": "Tutorial: Multiple Linear Regression with Backward Selection",
    "section": "\n Workflow: Diagnostics",
    "text": "Workflow: Diagnostics\nLet us use broom::augment to calculate residuals and predictions to arrive at a quick set of diagnostic plots.\n\nhousing_model_final_augment &lt;- \n  housing_model_set %&gt;% \n  filter(keep_vars == 4) %&gt;% \n  \n# augment the model\n  mutate(augment_models =\n           map(\n             .x = models,\n             .f = \\(x) broom::augment(x)\n           )) %&gt;% \n  unnest(augment_models) %&gt;% \n  select(cmedv:last_col())\n\nhousing_model_final_augment\n\n\n\n  \n\n\n\n\nhousing_model_final_augment %&gt;% \n  gf_point(.resid ~ .fitted, title = \"Residuals vs Fitted\") %&gt;%\n  gf_smooth() %&gt;% \n  gf_theme(my_theme)\nhousing_model_final_augment %&gt;% \n  gf_qq(~ .std.resid, title = \"Q-Q Residuals\") %&gt;% \n  gf_qqline() %&gt;%\n  gf_theme(my_theme)\nhousing_model_final_augment %&gt;% \n  gf_point(sqrt(.std.resid) ~ .fitted, \n           title = \"Scale-Location Plot\") %&gt;%\n    gf_smooth() %&gt;% \n  gf_theme(my_theme)\nhousing_model_final_augment %&gt;% \n  gf_point(.std.resid ~ .hat, \n           title = \"Residuals vs Leverage\") %&gt;%\n    gf_smooth() %&gt;% \n  gf_theme(my_theme)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe residuals plot shows a curved trend, and certainly does not resemble the stars at night, so it is possible that we have left out some possible richness in our model-making, a “systemic inadequacy”.\nThe Q-Q plot of residuals also shows a J-shape which indicates a non-normal distribution of residuals.\nThese could indicate that more complex model ( e.g. linear model with interactions between variables ( i.e. product terms ) may be necessary."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#iconify-fluent-mdl2-decision-solid-conclusion",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#iconify-fluent-mdl2-decision-solid-conclusion",
    "title": "Tutorial: Multiple Linear Regression with Backward Selection",
    "section": "\n Conclusion",
    "text": "Conclusion\nWe have used a multiple-regression workflow that takes all predictor variables into account in a linear model, and then systematically simplified that model such that the performance was just adequate.\nThe models we chose were all linear of course, but without interaction terms : each predictor was used only for its main effect. When the diagnostic plots were examined, we did see some shortcomings in the model. This could be overcome with a more complex model. These might include selected interactions, transformations of target(\\(cmedv^2\\), or \\(sqrt(cmedv)\\)) and some selected predictors."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#iconify-ooui-references-rtl-references",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#iconify-ooui-references-rtl-references",
    "title": "Tutorial: Multiple Linear Regression with Backward Selection",
    "section": "\n References",
    "text": "References\n\nJames, Witten, Hastie, Tibshirani, An Introduction to Statistical Learning. Chapter 3. Linear Regression. https://hastie.su.domains/ISLR2/Labs/Rmarkdown_Notebooks/Ch3-linreg-lab.html"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#footnotes",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#footnotes",
    "title": "Tutorial: Multiple Linear Regression with Backward Selection",
    "section": "Footnotes",
    "text": "Footnotes\n\nJames, Witten, Hastie, Tibshirani,An Introduction to Statistical Learning. Chapter 3. Linear Regression https://hastie.su.domains/ISLR2/Labs/Rmarkdown_Notebooks/Ch3-linreg-lab.html↩︎"
  },
  {
    "objectID": "content/courses/Analytics/listing.html",
    "href": "content/courses/Analytics/listing.html",
    "title": "Data Analytics",
    "section": "",
    "text": "This Course takes Business Practitioners on a journey of Business Analytics: using data to derive insights, make predictions, and decide on plans of action that can be communicated and actualized in a Business context.\n\n“Business analytics, or simply analytics, is the use of data, information technology, statistical analysis, quantitative methods, and mathematical or computer-based models to help managers gain improved insight about their business operations and make better, fact-based decisions. Business analytics is”a process of transforming data into actions through analysis and insights in the context of organizational decision making and problem solving.”\n\nLibertore and Luo, 2010\n\n\n The Course starts with Descriptive Analytics: Datasets from various domains of Business enterprise and activity are introduced. The datasets are motivated from the point of view of the types of information they contain: students will relate the Data Variables (Qualitative and Quantitative) to various types of Data/Information Visualizations.\nStatistical Concepts such as Sampling, Hypothesis Tests, Simulation / Modelling, and Uncertainty will be introduced.\nPredictive Analytics will take us into looking at Data and training standard ML algorithms to make predictions with new Data. Regression, Clustering, and Classification will be covered.\nPrescriptive Analytics will deal with coming to terms with the uncertainty in Predictions, and using tools such as both ML, Linear/non-Linear Programming, and Decision-Making to make Business Decisions, with an assessment of the Risks involved.\nThe Course will culminate in a full Business Analytics Workflow that includes Data Gathering and Cleaning, Descriptive and Predictive Analytics, Prescriptive Analytics and Decision Making, and Communication resulting in a publication-worthy documents.(HTML / PDF/ Word)"
  },
  {
    "objectID": "content/courses/Analytics/listing.html#abstract",
    "href": "content/courses/Analytics/listing.html#abstract",
    "title": "Data Analytics",
    "section": "",
    "text": "This Course takes Business Practitioners on a journey of Business Analytics: using data to derive insights, make predictions, and decide on plans of action that can be communicated and actualized in a Business context.\n\n“Business analytics, or simply analytics, is the use of data, information technology, statistical analysis, quantitative methods, and mathematical or computer-based models to help managers gain improved insight about their business operations and make better, fact-based decisions. Business analytics is”a process of transforming data into actions through analysis and insights in the context of organizational decision making and problem solving.”\n\nLibertore and Luo, 2010\n\n\n The Course starts with Descriptive Analytics: Datasets from various domains of Business enterprise and activity are introduced. The datasets are motivated from the point of view of the types of information they contain: students will relate the Data Variables (Qualitative and Quantitative) to various types of Data/Information Visualizations.\nStatistical Concepts such as Sampling, Hypothesis Tests, Simulation / Modelling, and Uncertainty will be introduced.\nPredictive Analytics will take us into looking at Data and training standard ML algorithms to make predictions with new Data. Regression, Clustering, and Classification will be covered.\nPrescriptive Analytics will deal with coming to terms with the uncertainty in Predictions, and using tools such as both ML, Linear/non-Linear Programming, and Decision-Making to make Business Decisions, with an assessment of the Risks involved.\nThe Course will culminate in a full Business Analytics Workflow that includes Data Gathering and Cleaning, Descriptive and Predictive Analytics, Prescriptive Analytics and Decision Making, and Communication resulting in a publication-worthy documents.(HTML / PDF/ Word)"
  },
  {
    "objectID": "content/courses/Analytics/listing.html#what-you-will-learn",
    "href": "content/courses/Analytics/listing.html#what-you-will-learn",
    "title": "Data Analytics",
    "section": "What you will learn",
    "text": "What you will learn\n\nData Basics: What does data look like and why should we care?\nRapidly and intuitively creating Graphs and Data Visualizations to explore data for insights\nUse Statistical Tests, Procedures, Models, and Simulations and to answer Business Questions\nUsing ML algorithms such Regression, Classification, and Clustering to develop Business Insights\nUse Linear Programming to make Business Decisions\nCreate crisp and readable Reports that can be shared in a Business Context"
  },
  {
    "objectID": "content/courses/Analytics/listing.html#texts",
    "href": "content/courses/Analytics/listing.html#texts",
    "title": "Data Analytics",
    "section": "Texts",
    "text": "Texts\n\nJames R Evans, Business Analytics: Methods, Models, and Decisions, Pearson Education, 2021."
  },
  {
    "objectID": "content/courses/Analytics/listing.html#references",
    "href": "content/courses/Analytics/listing.html#references",
    "title": "Data Analytics",
    "section": "References",
    "text": "References\n\nThomas Maydon, The 4 Types of Data Analytics, https://www.kdnuggets.com/2017/07/4-types-data-analytics.html\nDimitris Bertsimas, Robert Freund, Data, Models, and Decisions: the Fundamentals of Management Science, Dynamic Ideas Press, 2004.\nCliff T. Ragsdale, Spreadsheet Modeling & Decision Analysis: A Practical Introduction to Management Science, South Western, Cengage Learning, Mason, OH, 2012.\nJack Dougherty and Ilya Ilyankou, Hands-On Data Visualization: Interactive Storytelling from Spreadsheets to Code, https://handsondataviz.org/. Available free Online.\nClaus O. Wilke, Fundamentals of Data Visualization, https://clauswilke.com/dataviz/. Available free Online.\nJonathan Schwabish, Better Data Visualizations: A Guide for Scholars, Researchers, and Wonks, Columbia University Press, 2021.\nAlberto Cairo, The Functional Art:An introduction to information graphics and visualization, New Riders. 2013. ISBN-9780133041361.\nCole Nussbaumer Knaflic, Storytelling With Data: A Data Visualization Guide for Business Professionals, Wiley 2015. ISBN-9781119002253."
  },
  {
    "objectID": "content/courses/Analytics/listing.html#pedagogical-note",
    "href": "content/courses/Analytics/listing.html#pedagogical-note",
    "title": "Data Analytics",
    "section": "Pedagogical Note",
    "text": "Pedagogical Note"
  },
  {
    "objectID": "content/courses/Analytics/listing.html#our-tools",
    "href": "content/courses/Analytics/listing.html#our-tools",
    "title": "Data Analytics",
    "section": "Our Tools",
    "text": "Our Tools\nFor this three-in-one course, we will gain exposure to the following free and open source tools:\n\nR https://cran.r-project.org/ and RStudio https://posit.co/\nR is a freely available language and environment for statistical computing and graphics which provides a wide variety of statistical and graphical techniques: linear and nonlinear modelling, statistical tests, time series analysis, classification, clustering, etc. RStudio is an integrated development environment (IDE) for R and Python.\nOrange Data Mining https://orangedatamining.com/\nOrange is a FOSS visual point-and-click software for Data Mining and ML, developed at the University of Slovenia, Ljubljana.\n\n\n\n\nRadiant – Business analytics using R and Shiny https://radiant-rstats.github.io/docs/index.html\n\nRadiant is a FOSS platform-independent browser-based interface for business analytics in R, developed at the University of San Diego. The application is based on the Shiny package and can be run using R, or in your browser with no installation required. The tool automatically installs a version of R and adds a Shiny-based GUI that removes the need to write R-code. Radiant can also be installed on top of an existing installation of R and invoked from within RStudio."
  },
  {
    "objectID": "content/courses/Analytics/listing.html#modules",
    "href": "content/courses/Analytics/listing.html#modules",
    "title": "Data Analytics",
    "section": "Modules",
    "text": "Modules"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/60-SimTest/files/sim-tutorial.html",
    "href": "content/courses/Analytics/Inference/Modules/60-SimTest/files/sim-tutorial.html",
    "title": "simulation",
    "section": "",
    "text": "In this module we will use simulation to solve several problems in Business Decision Making."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/50-Bootstrap/files/bootstrap.html",
    "href": "content/courses/Analytics/Inference/Modules/50-Bootstrap/files/bootstrap.html",
    "title": "Bootstrap Case Studies",
    "section": "",
    "text": "Example 5.2\n\nCodemy.sample &lt;- rgamma(16, 1, 1/2)\n\nN &lt;- 10^5\nmy.boot &lt;- numeric(N)\nfor (i in 1:N)\n {\n  x &lt;- sample(my.sample, 16, replace = TRUE)  #draw resample\n  my.boot[i] &lt;- mean(x)                     #compute mean, store in my.boot\n  }\n\nggplot() + geom_histogram(aes(my.boot), bins=15)\n\n\n\nCodemean(my.boot)  #mean\n\n[1] 2.854784\n\nCodesd(my.boot)    #bootstrap SE\n\n[1] 0.6668539\n\n\nExample 5.3\nArsenic in wells in Bangladesh\n\nCodeBangladesh &lt;- read.csv(\"../../../../../../materials/data/resampling/Bangladesh.csv\")\n\nggplot(Bangladesh, aes(Arsenic)) + geom_histogram(bins = 15)\n\n\n\nCodeggplot(Bangladesh, aes(sample = Arsenic)) + stat_qq() + stat_qq_line()\n\n\n\nCodeArsenic &lt;- pull(Bangladesh, Arsenic)\n#Alternatively\n#Arsenic &lt;- Bangladesh$Arsenic\n\nn &lt;- length(Arsenic)\nN &lt;- 10^4\n\narsenic.mean &lt;- numeric(N)\n\nfor (i in 1:N)\n{\n   x &lt;- sample(Arsenic, n, replace = TRUE)\n   arsenic.mean[i] &lt;- mean(x)\n}\n\nggplot() + geom_histogram(aes(arsenic.mean), bins = 15) + \n  labs(title=\"Bootstrap distribution of means\") + \n  geom_vline(xintercept = mean(Arsenic), colour = \"blue\")\n\n\n\nCodedf &lt;- data.frame(x = arsenic.mean)\nggplot(df, aes(sample = x)) + stat_qq() + stat_qq_line()\n\n\n\nCodemean(arsenic.mean)                 #bootstrap mean\n\n[1] 125.3559\n\nCodemean(arsenic.mean) - mean(Arsenic) #bias\n\n[1] 0.03596672\n\nCodesd(arsenic.mean)                   #bootstrap SE\n\n[1] 18.47535\n\nCodesum(arsenic.mean &gt; 161.3224)/N\n\n[1] 0.0326\n\nCodesum(arsenic.mean &lt; 89.75262)/N\n\n[1] 0.0187\n\n\nExample 5.4 Skateboard\n\nCodeSkateboard &lt;- read.csv(\"../../../../../../materials/data/resampling/Skateboard.csv\")\n\ntestF &lt;- Skateboard %&gt;% filter(Experimenter == \"Female\") %&gt;% pull(Testosterone)\ntestM &lt;- Skateboard %&gt;% filter(Experimenter == \"Male\") %&gt;% pull(Testosterone)\n\nobserved &lt;- mean(testF) - mean(testM)\n\nnf &lt;- length(testF)\nnm &lt;- length(testM)\n\nN &lt;- 10^4\n\nTestMean &lt;- numeric(N)\n\nfor (i in 1:N)\n{\n  sampleF &lt;- sample(testF, nf, replace = TRUE)\n  sampleM &lt;- sample(testM, nm, replace = TRUE)\n  TestMean[i] &lt;- mean(sampleF) - mean(sampleM)\n}\n\ndf &lt;- data.frame(TestMean)\nggplot(df) + geom_histogram(aes(TestMean), bins = 15) + \n  labs(title = \"Bootstrap distribution of difference in means\", xlab = \"means\") +\n  geom_vline(xintercept = observed, colour = \"blue\")\n\n\n\nCodeggplot(df, aes(sample = TestMean))  + stat_qq() + stat_qq_line()\n\n\n\nCodemean(testF) - mean(testM)\n\n[1] 83.0692\n\nCodemean(TestMean)\n\n[1] 83.21201\n\nCodesd(TestMean)\n\n[1] 29.59292\n\nCodequantile(TestMean,c(0.025,0.975))\n\n     2.5%     97.5% \n 25.55393 140.54042 \n\nCodemean(TestMean)- observed  #bias\n\n[1] 0.1428033\n\n\nPermutation test for Skateboard means\n\nCodetestAll &lt;- pull(Skateboard, Testosterone)\n#testAll &lt;- Skateboard$Testosterone\n\nN &lt;- 10^4 - 1  #set number of times to repeat this process\n\n#set.seed(99)\nresult &lt;- numeric(N) # space to save the random differences\nfor(i in 1:N)\n  {\n  index &lt;- sample(71, size = nf, replace = FALSE) #sample of numbers from 1:71\n  result[i] &lt;- mean(testAll[index]) - mean(testAll[-index])\n}\n\n(sum(result &gt;= observed)+1)/(N + 1)  #P-value\n\n[1] 0.0064\n\nCodeggplot() + geom_histogram(aes(result), bins = 15) + \n  labs(x = \"xbar1-xbar2\", title=\"Permutation distribution for testosterone levels\") +\n  geom_vline(xintercept = observed, colour = \"blue\")\n\n\n\nCodedf &lt;- data.frame(result)\nggplot(df, aes(sample = result)) + stat_qq() + stat_qq_line()\n\n\n\n\nSection 5.4.1 Matched pairs for Diving data\n\nCodeDiving2017 &lt;- read.csv(\"../../../../../../materials/data/resampling/Diving2017.csv\")\nDiff &lt;- Diving2017 %&gt;% mutate(Diff = Final - Semifinal) %&gt;% pull(Diff)\n#alternatively\n#Diff &lt;- Diving2017$Final - Diving2017$Semifinal\nn &lt;- length(Diff)\n\nN &lt;- 10^5\nresult &lt;- numeric(N)\n\nfor (i in 1:N)\n{\n  dive.sample &lt;- sample(Diff, n, replace = TRUE)\n  result[i] &lt;- mean(dive.sample)\n}\n\nggplot() + geom_histogram(aes(result), bins = 15)\n\n\n\nCodequantile(result, c(0.025, 0.975))\n\n     2.5%     97.5% \n-6.679271 30.920937 \n\n\nExample 5.5 Verizon cont. Bootstrap means for the ILEC data and for the CLEC data\nBootstrap difference of means.\n\nCodeVerizon &lt;- read.csv(\"../../../../../../materials/data/resampling/Verizon.csv\")\n\nTime.ILEC &lt;- Verizon %&gt;% filter(Group == \"ILEC\") %&gt;% pull(Time)\nTime.CLEC &lt;- Verizon %&gt;% filter(Group == \"CLEC\") %&gt;% pull(Time)\n\nobserved &lt;- mean(Time.ILEC) - mean(Time.CLEC)\n\nn.ILEC &lt;- length(Time.ILEC)\nn.CLEC &lt;- length(Time.CLEC)\n\nN &lt;- 10^4\n\ntime.ILEC.boot &lt;- numeric(N)\ntime.CLEC.boot &lt;- numeric(N)\ntime.diff.mean &lt;- numeric(N)\n\nset.seed(100)\nfor (i in 1:N)\n {\n  ILEC.sample &lt;- sample(Time.ILEC, n.ILEC, replace = TRUE)\n  CLEC.sample &lt;- sample(Time.CLEC, n.CLEC, replace = TRUE)\n  time.ILEC.boot[i] &lt;- mean(ILEC.sample)\n  time.CLEC.boot[i] &lt;- mean(CLEC.sample)\n  time.diff.mean[i] &lt;- mean(ILEC.sample) - mean(CLEC.sample)\n}\n\n#bootstrap for ILEC\nggplot() + geom_histogram(aes(time.ILEC.boot), bins = 15) + \n  labs(title = \"Bootstrap distribution of ILEC means\", x = \"means\") + \n  geom_vline(xintercept = mean(Time.ILEC), colour = \"blue\") + \n  geom_vline(xintercept = mean(time.ILEC.boot), colour = \"red\", lty=2)\n\n\n\nCodesummary(time.ILEC.boot)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  7.036   8.156   8.400   8.406   8.642   9.832 \n\nCodedf &lt;- data.frame(x = time.ILEC.boot)\nggplot(df, aes(sample = x)) + stat_qq() + stat_qq_line()\n\n\n\nCode#bootstrap for CLEC\nggplot() + geom_histogram(aes(time.CLEC.boot), bins = 15) + \n  labs(title = \"Bootstrap distribution of CLEC means\", x = \"means\") + \n  geom_vline(xintercept = mean(Time.CLEC), colour = \"blue\") + \n  geom_vline(xintercept = mean(time.CLEC.boot), colour = \"red\", lty = 2)\n\n\n\nCodedf &lt;- data.frame(x = time.CLEC.boot)\nggplot(df, aes(sample = x)) + stat_qq() + stat_qq_line()\n\n\n\nCode#Different in means\nggplot() + geom_histogram(aes(time.diff.mean), bins = 15) + \n  labs(title = \"Bootstrap distribution of difference in means\", x = \"means\") +\n  geom_vline(xintercept = mean(time.diff.mean), colour = \"blue\") + \n  geom_vline(xintercept = mean(observed), colour = \"red\", lty = 2)\n\n\n\nCodedf &lt;- data.frame(x = time.diff.mean)\nggplot(df, aes(sample = x)) + stat_qq() + stat_qq_line()\n\n\n\nCodemean(time.diff.mean)\n\n[1] -8.096489\n\nCodequantile(time.diff.mean, c(0.025, 0.975))\n\n      2.5%      97.5% \n-16.970068  -1.690859 \n\n\nSection 5.5 Verizon cont.\nBootstrap difference in trimmed means\n\nCodeTime.ILEC &lt;- Verizon %&gt;% filter(Group == \"ILEC\") %&gt;% pull(Time)\nTime.CLEC &lt;- Verizon %&gt;% filter(Group == \"CLEC\") %&gt;% pull(Time)\nn.ILEC &lt;- length(Time.ILEC)\nn.CLEC &lt;- length(Time.CLEC)\n\nN &lt;- 10^4\ntime.diff.trim &lt;- numeric(N)\n\n#set.seed(100)\nfor (i in 1:N)\n{\n  x.ILEC &lt;- sample(Time.ILEC, n.ILEC, replace = TRUE)\n  x.CLEC &lt;- sample(Time.CLEC, n.CLEC, replace = TRUE)\n  time.diff.trim[i] &lt;- mean(x.ILEC, trim = .25) - mean(x.CLEC, trim = .25)\n}\n\nggplot() + geom_histogram(aes(time.diff.trim), bins = 15) + \n  labs(x = \"difference in trimmed means\") + \n  geom_vline(xintercept = mean(time.diff.trim),colour = \"blue\") + \n  geom_vline(xintercept = mean(Time.ILEC, trim = .25) - mean(Time.CLEC, trim = .25), colour = \"red\", lty = 2)\n\n\n\nCodedf &lt;- data.frame(x = time.diff.trim)\nggplot(df, aes(sample = x)) + stat_qq() + stat_qq_line()\n\n\n\nCodemean(time.diff.trim)\n\n[1] -10.32079\n\nCodequantile(time.diff.trim, c(0.025,0.975))\n\n     2.5%     97.5% \n-15.47049  -4.97130 \n\n\nSection 5.5 Other statistics Verizon cont:\nBootstrap of the ratio of means\nTime.ILEC and Time.CLEC created above.\nn.ILEC, n.CLEC created above\n\nCodeN &lt;- 10^4\ntime.ratio.mean &lt;- numeric(N)\n\n#set.seed(100)\nfor (i in 1:N)\n{\n  ILEC.sample &lt;- sample(Time.ILEC, n.ILEC, replace = TRUE)\n  CLEC.sample &lt;- sample(Time.CLEC, n.CLEC, replace = TRUE)\n  time.ratio.mean[i] &lt;- mean(ILEC.sample)/mean(CLEC.sample)\n}\n\nggplot() + geom_histogram(aes(time.ratio.mean), bins = 12) + \n  labs(title = \"bootstrap distribution of ratio of means\", x = \"ratio of means\") +\n  geom_vline(xintercept = mean(time.ratio.mean), colour = \"red\", lty = 2) + \n  geom_vline(xintercept  = mean(Time.ILEC)/mean(Time.CLEC), col = \"blue\")\n\n\n\nCodedf &lt;- data.frame(x = time.ratio.mean)\nggplot(df, aes(sample = x)) + stat_qq() + stat_qq_line()\n\n\n\nCodemean(time.ratio.mean)\n\n[1] 0.5429164\n\nCodesd(time.ratio.mean)\n\n[1] 0.1354238\n\nCodequantile(time.ratio.mean, c(0.025, 0.975))\n\n     2.5%     97.5% \n0.3283862 0.8517156 \n\n\nExample 5.7 Relative risk example\n\nCodehighbp &lt;- rep(c(1,0),c(55,3283))   #high blood pressure\nlowbp &lt;- rep(c(1,0),c(21,2655))    #low blood pressure\n\nN &lt;- 10^4\nboot.rr &lt;- numeric(N)\nhigh.prop &lt;- numeric(N)\nlow.prop &lt;- numeric(N)\n\nfor (i in 1:N)\n{\n   x.high &lt;- sample(highbp,3338, replace = TRUE)\n   x.low  &lt;- sample(lowbp, 2676, replace = TRUE)\n   high.prop[i] &lt;- sum(x.high)/3338\n   low.prop[i]  &lt;- sum(x.low)/2676\n   boot.rr[i] &lt;- high.prop[i]/low.prop[i]\n}\n\nci &lt;- quantile(boot.rr, c(0.025, 0.975))\n\nggplot() + geom_histogram(aes(boot.rr), bins = 15) + \n  labs(title = \"Bootstrap distribution of relative risk\", x = \"relative risk\") +\n  geom_vline(aes(xintercept = mean(boot.rr), colour = \"mean of bootstrap\")) +\n  geom_vline(aes(xintercept = 2.12, colour=\"observed rr\"), lty = 2) + \n  scale_colour_manual(name=\"\", values = c(\"mean of bootstrap\"=\"blue\", \"observed rr\" = \"red\"))\n\n\n\nCodetemp &lt;- ifelse(high.prop &lt; 1.31775*low.prop, 1, 0)\ntemp2 &lt;- ifelse(high.prop &gt; 3.687*low.prop, 1, 0)\ntemp3 &lt;- temp + temp2\n\ndf &lt;- data.frame(y=high.prop, x=low.prop, temp, temp2, temp3)\ndf1 &lt;- df %&gt;% filter(temp == 1)\ndf2 &lt;- df %&gt;% filter (temp2 == 1)\ndf3 &lt;- df %&gt;% filter(temp3 == 0)\n\nggplot(df, aes(x=x, y = y)) + \n  geom_point(data =df1, aes(x= x, y = y), colour = \"green\") + \n  geom_point(data = df2, aes(x = x, y = y), colour = \"green\") + \n  geom_point(data = df3, aes(x = x, y = y), colour = \"red\") + \n  geom_vline(aes(xintercept = mean(low.prop)), colour = \"red\") +\n  geom_hline(yintercept = mean(high.prop), colour = \"red\") + \n  geom_abline(aes(intercept = 0, slope = 2.12, colour = \"observed rr\"), lty = 2, lwd = 1) + \n  geom_abline(aes(intercept = 0, slope = ci[1], colour = \"bootstrap CI\"), lty = 2, lwd = 1) + \n  geom_abline(intercept = 0, slope = ci[2], colour = \"blue\", lty = 2, lwd = 1) +\n  scale_colour_manual(name=\"\", values=c(\"observed rr\"=\"black\", \"bootstrap CI\" = \"blue\")) +\n  labs(x = \"Proportion in low blood pressure group\", y = \"Proportion in high blood pressure group\")\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/files/sampling-tutorial.html",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/files/sampling-tutorial.html",
    "title": "Sampling",
    "section": "",
    "text": "Continuing to treat the NHANES dataset as a population, We will try to replicate the process of sampling and CLT for another variable in the NHANES variable, AlcoholYear.\n\n\nTry sample sizes of 25, 50, 100, 500.\n\n\n\nWrite your observations here!"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/files/sampling-tutorial.html#sampling-alcoholyear",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/files/sampling-tutorial.html#sampling-alcoholyear",
    "title": "Sampling",
    "section": "",
    "text": "Try sample sizes of 25, 50, 100, 500."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/files/sampling-tutorial.html#conclusion",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/files/sampling-tutorial.html#conclusion",
    "title": "Sampling",
    "section": "",
    "text": "Write your observations here!"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/single-prop.html",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/single-prop.html",
    "title": "🃏 Testing a Single Proportion",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE,message = TRUE,warning = TRUE, fig.align = \"center\")\noptions(digits=2)\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(ggformula)\nlibrary(ggmosaic) # plotting mosaic plots for Categorical Data\nlibrary(vcd) # Creating Tables and plotting mosaic charts\n\n## Datasets from Chihara and Hesterberg's book (Second Edition)\nlibrary(resampledata)\n\n## Datasets from Cetinkaya-Rundel and Hardin's book (First Edition)\nlibrary(openintro)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/single-prop.html#iconify-noto-v1-package-setting-up-r-packages",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/single-prop.html#iconify-noto-v1-package-setting-up-r-packages",
    "title": "🃏 Testing a Single Proportion",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE,message = TRUE,warning = TRUE, fig.align = \"center\")\noptions(digits=2)\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(ggformula)\nlibrary(ggmosaic) # plotting mosaic plots for Categorical Data\nlibrary(vcd) # Creating Tables and plotting mosaic charts\n\n## Datasets from Chihara and Hesterberg's book (Second Edition)\nlibrary(resampledata)\n\n## Datasets from Cetinkaya-Rundel and Hardin's book (First Edition)\nlibrary(openintro)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/single-prop.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/single-prop.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "title": "🃏 Testing a Single Proportion",
    "section": "\n Introduction",
    "text": "Introduction\nWe saw from the diagram created by Allen Downey that there is only one test! We will now use this philosophy to develop a technique that allows us to mechanize several Statistical Models in that way, with nearly identical code.\nWe will use two packages in R, mosaic to develop our intuition for what are called permutation based statistical tests. (There is also a more recent package called infer in R which can do pretty much all of this, including visualization. In my opinion, the code is a little too high-level and does not offer quite the detailed insight that the mosaic package does)."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/single-prop.html#iconify-svg-spinners-blocks-shuffle-3-permutation-visually-demonstrated",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/single-prop.html#iconify-svg-spinners-blocks-shuffle-3-permutation-visually-demonstrated",
    "title": "🃏 Testing a Single Proportion",
    "section": "\n Permutation Visually Demonstrated",
    "text": "Permutation Visually Demonstrated\nWe will look visually at a permutation exercise. We will create dummy data that contains the following case study:\n\nA set of identical resumes was sent to male and female evaluators. The candidates in the resumes were of both genders. We wish to see if there was difference in the way resumes were evaluated, by male and female evaluators. (We use just one male and one female evaluator here, to keep things simple!)\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n\n\n         M \n-0.3333333 \n\n\n\n\n\n\nSo, we have a solid disparity in percentage of selection between the two evaluators!\n\n Permutation\nNow we pretend that there is no difference between the selections made by either set of evaluators. So we can just:\n\nPool up all the evaluations\n\nArbitrarily re-assign a given candidate(selected or rejected) to either of the two sets of evaluators, by permutation.\n\n\nHow would that pooled shuffled set of evaluations look like?\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nAs can be seen, the ratio is different! We can now do this many many times, to check out our Hypothesis that there is no bias. We can plot the distribution of the differences in selection ratio and see how that artificially created distribution compares with mother-nature, the originally observed figure.\n\nnull_dist &lt;- do(5000) * diff(mean(candidate ~ shuffle(evaluator), \n                                   data = data))\n# null_dist %&gt;% names()\nnull_dist %&gt;% gf_histogram( ~ M, \n                  fill = ~ (M &lt;= obs_difference), \n                  bins = 25,show.legend = FALSE,\n                  xlab = \"Bias Proportion\", \n                  ylab = \"How Often?\",\n                  title = \"Permutation Test on Diffence between Groups\",\n                  subtitle = \"\") %&gt;% \n  gf_vline(xintercept = ~ obs_difference, color = \"red\" ) %&gt;% \n  gf_label(500 ~ obs_difference, label = \"Observed\\n Bias\", \n           show.legend = FALSE) %&gt;% \n  gf_theme(theme_classic())\nmean(~ M&lt;= obs_difference, data = null_dist)\n\n\n\n\n\n\n \n\n\n[1] 0.0146\n\n\n\n\nWe see that the artificial data can hardly ever (\\(p = 0.012\\)) mimic what the real world experiment is showing. Hence we had good reason to reject our NULL Hypothesis that there is no bias."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/single-prop.html#iconify-fluent-mdl2-decision-solid-conclusion",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/single-prop.html#iconify-fluent-mdl2-decision-solid-conclusion",
    "title": "🃏 Testing a Single Proportion",
    "section": "\n Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/single-prop.html#iconify-ooui-references-ltr-references",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/single-prop.html#iconify-ooui-references-ltr-references",
    "title": "🃏 Testing a Single Proportion",
    "section": "\n References",
    "text": "References\n\nOpenIntro Modern Statistics: Chapter 17\nChihara and Hesterberg"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/150-Correlation/single_corr.html",
    "href": "content/courses/Analytics/Inference/Modules/150-Correlation/single_corr.html",
    "title": "Inference for Correlation",
    "section": "",
    "text": "knitr::opts_chunk$set(fig.align = \"center\", fig.width = 7, fig.height = 5)\noptions(scipen = 8, digits = 3)\n# CRAN Packages\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(mosaic)\nlibrary(mosaicCore)\nlibrary(mosaicData)\n\nlibrary(openintro) # datasets and methods\nlibrary(resampledata3) # datasets\nlibrary(statsExpressions) # datasets and methods\nlibrary(ggstatsplot) # special stats plots\nlibrary(ggExtra)\n\n# Non-CRAN Packages\n# remotes::install_github(\"easystats/easystats\")\nlibrary(easystats)\n\nggplot2::theme_set(theme_classic())"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/150-Correlation/single_corr.html#iconify-noto-v1-package-setting-up-r-packages",
    "href": "content/courses/Analytics/Inference/Modules/150-Correlation/single_corr.html#iconify-noto-v1-package-setting-up-r-packages",
    "title": "Inference for Correlation",
    "section": "",
    "text": "knitr::opts_chunk$set(fig.align = \"center\", fig.width = 7, fig.height = 5)\noptions(scipen = 8, digits = 3)\n# CRAN Packages\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(mosaic)\nlibrary(mosaicCore)\nlibrary(mosaicData)\n\nlibrary(openintro) # datasets and methods\nlibrary(resampledata3) # datasets\nlibrary(statsExpressions) # datasets and methods\nlibrary(ggstatsplot) # special stats plots\nlibrary(ggExtra)\n\n# Non-CRAN Packages\n# remotes::install_github(\"easystats/easystats\")\nlibrary(easystats)\n\nggplot2::theme_set(theme_classic())"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/150-Correlation/single_corr.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "href": "content/courses/Analytics/Inference/Modules/150-Correlation/single_corr.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "title": "Inference for Correlation",
    "section": "\n Introduction",
    "text": "Introduction\nLet us recap a few basic definitions:\nWe have already encountered the variance of a variable:\n\\[\n\\begin{align*}\nvar_x &= \\frac{\\sum_{i=1}^{n}(x_i - \\mu_x)^2}{(n-1)}\\\\\nwhere ~ \\mu_x &= mean(x)\\\\\nn &= sample\\ size\n\\end{align*}\n\\] The standard deviation is: $$\n_x &= \\\n$$\nThe covariance of two variables is defined as $$ \\[\\begin{align*}\ncov(x,y) &= \\frac{\\sum_{i = 1}^{n}(x_i - \\mu_x)*(y_i - \\mu_y)}{n-1}\\\\\n&= \\frac{\\sum{x_i *y_i}}{n-1} - \\frac{\\sum{x_i *\\mu_y}}{n-1} - \\frac{\\sum{y_i *\\mu_x}}{n-1} + \\frac{\\sum{\\mu_x *\\mu_y}}{n-1}\\\\\n&= \\frac{\\sum{x_i *y_i}}{n-1} - \\frac{\\sum{\\mu_x *\\mu_y}}{n-1}\\\\\n\n\\end{align*}\\] $$ Hence covariance is the expectation of the product minus the product of the expectations of the two variables.\n\n\n\n\n\n\nCovariance uses z-scores!\n\n\n\nNote that in both cases we are dealing with z-scores: variable minus its mean, \\(x_i - \\mu_x\\), which we have seen when dealing with the CLT and the Gaussian Distribution.\n\n\nSo, finally, the coefficient of **correlation* between two variables is defined as:\n\\[\n\\begin{align*}\ncorrelation ~ r &= \\frac{cov(x,y)}{\\sigma_x * \\sigma_y}\n\\\\\n&= \\frac{cov(x,y)}{\\sqrt{var_x} * \\sqrt{var_y}}\n\\end{align*}\n\\] Thus correlation coefficient is the covariance scaled by the geometric mean of the variances. Correlations define how one variables varies with another. One of the basic Questions we would have of our data is: Does some variable have a significant correlation score with another in some way? Does \\(y\\) vary with \\(x\\)? A Correlation Test is designed to answer exactly this question. The block diagram depicts the statistical procedures available to test for the significance of correlation scores between two variables.\n\nflowchart TD\n    A[Inference for Correlation] --&gt;|Check Assumptions| B[Normality: Shapiro-Wilk Test shapiro.test\\n Variances: Fisher F-test var.test]\n    B --&gt; C{OK?}\n    C --&gt;|Yes, both\\n Parametric| D[t.test]\n    D &lt;--&gt;F[Linear Model\\n Method] \n    C --&gt;|Yes, but not variance\\n Parametric| W[t.test with\\n Welch Correction]\n    W&lt;--&gt;F\n    C --&gt;|No\\n Non-Parametric| E[wilcox.test]\n    E &lt;--&gt; G[Linear Model\\n with\\n Ranked Data]\n    C --&gt;|No\\n Non-Parametric| P[Bootstrap\\n or\\n Permutation]\n    P &lt;--&gt; Q[Linear Model\\n with Ranked Data\\n and Permutation]\n\n\n\nflowchart TD\n    A[Inference for Correlation] --&gt;|Check Assumptions| B[Normality: Shapiro-Wilk Test shapiro.test\\n Variances: Fisher F-test var.test]\n    B --&gt; C{OK?}\n    C --&gt;|Yes, both\\n Parametric| D[t.test]\n    D &lt;--&gt;F[Linear Model\\n Method] \n    C --&gt;|Yes, but not variance\\n Parametric| W[t.test with\\n Welch Correction]\n    W&lt;--&gt;F\n    C --&gt;|No\\n Non-Parametric| E[wilcox.test]\n    E &lt;--&gt; G[Linear Model\\n with\\n Ranked Data]\n    C --&gt;|No\\n Non-Parametric| P[Bootstrap\\n or\\n Permutation]\n    P &lt;--&gt; Q[Linear Model\\n with Ranked Data\\n and Permutation]"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/150-Correlation/single_corr.html#iconify-pajamas-issue-type-test-case-case-study-1-a-simple-data-set-with-two-quant-variables",
    "href": "content/courses/Analytics/Inference/Modules/150-Correlation/single_corr.html#iconify-pajamas-issue-type-test-case-case-study-1-a-simple-data-set-with-two-quant-variables",
    "title": "Inference for Correlation",
    "section": "\n Case Study #1: A Simple Data set with Two Quant Variables",
    "text": "Case Study #1: A Simple Data set with Two Quant Variables\nLet us now see how a Correlation Test can be re-formulated as a Linear Model + Hypothesis Test."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/150-Correlation/single_corr.html#the-linear-model",
    "href": "content/courses/Analytics/Inference/Modules/150-Correlation/single_corr.html#the-linear-model",
    "title": "Inference for Correlation",
    "section": "The Linear Model",
    "text": "The Linear Model\nThe premise here is that many common statistical tests are special cases of the linear model. A linear model estimates the relationship between dependent variable or “response” variable (\\(y\\)) and an explanatory variable or “predictor” (\\(x\\)). It is assumed that the relationship is linear. \\(\\beta_0\\) is the intercept and \\(\\beta_1\\) is the slope of the linear fit, that predicts the value of y based the value of x.\n\\[\ny = \\beta_0 + \\beta_1 *x\n\\]\nSome Toy Data\nMost examples in this exposition are based on three “imaginary” samples, \\(x, y1, y2\\). Each is normally distributed and made up of 50 observations. The means and the sds are, respectively:\n\nrnorm_fixed  &lt;- function(N, mu = 0, sd = 1) {\n  scale(rnorm(N))* sd + mu\n}\nparams &lt;- tibble(mu = c(0, 0.3, 0.5), sd = c(1,2,1.5))\nparams \n\n\n\n  \n\n\n\n\nset.seed(40) # for replication\n\n# Data as vectors ( for t.tests etc)\nx &lt;- rnorm_fixed(50, mu = 0.0, sd = 1) #explanatory\ny1 &lt;- rnorm_fixed(50, mu = 0.3, sd = 2) # dependent #1\ny2 &lt;- rnorm_fixed(50, mu = 0.5, sd = 1.5) # dependent #2\n\n# Make a tibble with all variables\nmydata_wide &lt;- tibble(x = x, y1 = y1, y2 = y2)\n\n# Long form data\nmydata_long &lt;- \n  mydata_wide %&gt;%\n  pivot_longer(., cols = c(x,y1,y2), \n               names_to = \"group\", \n               values_to = \"value\")\n\n# Long form data with only dependent variables\nmydata_long_y &lt;- \n  mydata_wide %&gt;% \n  select(-x) %&gt;% \n  pivot_longer(., cols = c(y1,y2), \n               names_to = \"group\", \n               values_to = \"value\")\n\nLet us look at our toy data in three ways:\n\nAll three variables:\n\n\nmydata_wide \n\n\n\n  \n\n\n\n\nVariables stacked and labelled (Note: group is now a Qual variable !!)\n\n\nmydata_long \n\n\n\n  \n\n\n\n\nSame as 2, but only for the dependent y variables:\n\n\nmydata_long_y"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/150-Correlation/single_corr.html#pearson-correlation",
    "href": "content/courses/Analytics/Inference/Modules/150-Correlation/single_corr.html#pearson-correlation",
    "title": "Inference for Correlation",
    "section": "Pearson Correlation",
    "text": "Pearson Correlation\nModel\nThe model for Pearson Correlation tests is exactly the Linear Model:\n\\[\n\\begin{aligned}\ny = \\beta_0 + \\beta_1 \\times x\\\\\n\\\\\nH_0: Null\\ Hypothesis\\ =&gt; \\beta_1 = 0\\\\\\\nH_a: Alternate\\ Hypothesis\\ =&gt; \\beta_1 \\ne 0\\\\\n\\end{aligned}\n\\]\n\n# Pearson (built-in test)\ncor &lt;- cor.test(y1,x,method = \"pearson\") %&gt;% \n  broom::tidy() %&gt;% mutate(term = \"Pearson Correlation r\") %&gt;% select(term, estimate, p.value) \ncor \n\n\n\n  \n\n\n\nUsing the linear model method we get:\n\n# Linear Model\nlin &lt;- lm(y1 ~ 1 + x, data = mydata_wide) %&gt;% \n  broom::tidy() %&gt;% mutate(term = c(\"beta_0\", \"beta_1\")) %&gt;% select(term, estimate, p.value)\nlin \n\n\n\n  \n\n\n\nWhy are \\(r\\) and \\(\\beta_1\\) different, though the p-value is suspiciously the same!?\nDid we miss a factor of \\(\\frac{-0.463}{-0.231} = 2\\) somewhere…??\nLet us scale the variables to within {-1, +1} : (subtract the mean and divide by sd) and re-do the Linear Model with scaled versions \\(x\\) and \\(y\\):\n\n# Scaled linear model\nlin_scl &lt;- lm(scale(y1) ~ 1 + scale(x), data = mydata_wide) %&gt;% \n  broom::tidy() %&gt;% mutate(term = c(\"beta_0\", \"beta_1\")) %&gt;% select(term, estimate, p.value) %&gt;% select(term, estimate, p.value)\nlin_scl \n\n\n\n  \n\n\n\nSo we conclude:\n\nWhen both x and y have the same standard deviation, the slope from the linear model and the Pearson correlation are the same. Here, since x has twice the sd of y, the ratio of slope = -0.4635533 to r = -0.2317767 is 0.5.\nThere is this relationship between the slope in the linear model and Pearson correlation:\n\n\\[\nSlope\\ \\beta_1 = \\frac{sd_y}{sd_x} * r\n\\]\nThe slope is usually much more interpretable and informative than the correlation coefficient.\n\nHence a linear model using scale() for both variables will show slope = r.\n\nSlope_Scaled: -0.2317767 = Correlation: -0.2317767\n\nFinally, the p-value for Pearson Correlation and that for the slope in the linear model is the same (\\(0.1053\\)). Which means we cannot reject the NULL hypothesis of “no relationship”.\nExample\nTBD"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/150-Correlation/single_corr.html#spearman-correlation",
    "href": "content/courses/Analytics/Inference/Modules/150-Correlation/single_corr.html#spearman-correlation",
    "title": "Inference for Correlation",
    "section": "Spearman Correlation",
    "text": "Spearman Correlation\nModel\nIn some cases the LINE assumptions may not hold.\nNonlinear relationships, non-normally distributed data ( with large outliers ) and working with ordinal rather than continuous data: these situations necessitate the use of Spearman’s ranked correlation scores. (Ranked, not sign-ranked.).\nSee the example below: We choose to look at the gpa_study_hours dataset. It has two numeric columns gpa and study_hours:\n\nglimpse(gpa_study_hours)\n\nRows: 193\nColumns: 2\n$ gpa         &lt;dbl&gt; 4.000, 3.800, 3.930, 3.400, 3.200, 3.520, 3.680, 3.400, 3.…\n$ study_hours &lt;dbl&gt; 10, 25, 45, 10, 4, 10, 24, 40, 10, 10, 30, 7, 15, 60, 10, …\n\n\nWe can plot this:\n\nggplot(gpa_study_hours, aes(x = study_hours, y = gpa)) + geom_point() + geom_smooth() + theme_minimal()\n\n\n\n# ggstatsplot::ggscatterstats(data = gpa_study_hours, \n#                             x = study_hours, \n#                             y = gpa,\n#                             marginal = TRUE,\n#                             title = \"GPA vs Study Hours\")\n\nHmm…not normally distributed, and there is a sort of increasing relationship, however is it linear? And there is some evidence of heteroscedasticity, so the LINE assumptions are clearly in violation. Pearson correlation would not be the best idea here.\nLet us quickly try it anyway, using a Linear Model for the scaled gpa and study_hours variables, from where we get:\n\n# Pearson Correlation as Linear Model\nmodel_gpa &lt;-\n  lm(scale(gpa) ~ 1 + scale(study_hours), data = gpa_study_hours)\n\nmodel_gpa %&gt;%\n  broom::tidy() %&gt;% \n  mutate(term = c(\"beta_0\", \"beta_1\")) %&gt;% \n  cbind(confint(model_gpa) %&gt;%                                              as_tibble()) %&gt;%  \n  select(term, estimate, p.value, `2.5 %`, `97.5 %`)\n\n\n\n  \n\n\n\nThe correlation estimate is \\(0.133\\); the p-value is \\(0.065\\) (and the confidence interval includes \\(0\\)).\nHence we fail to reject the NULL hypothesis that study_hours and gpa have no relationship. But can this be right?\nShould we use another test, that does not need the LINE assumptions?"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/150-Correlation/single_corr.html#signed-rank-values",
    "href": "content/courses/Analytics/Inference/Modules/150-Correlation/single_corr.html#signed-rank-values",
    "title": "Inference for Correlation",
    "section": "“Signed Rank” Values",
    "text": "“Signed Rank” Values\nMost statistical tests use the actual values of the data variables. However, in some non-parametric statistical tests, the data are used in rank-transformed sense/order. (In some cases the signed-rank of the data values is used instead of the data itself.)\nSigned Rank is calculated as follows:\n\nTake the absolute value of each observation in a sample\nPlace the ranks in order of (absolute magnitude). The smallest number has rank = 1 and so on. This gives is ranked data.\nGive each of the ranks the sign of the original observation ( + or -). This gives us signed ranked data.\n\n\nsigned_rank &lt;- function(x) {sign(x) * rank(abs(x))}"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/150-Correlation/single_corr.html#plotting-original-and-signed-rank-data",
    "href": "content/courses/Analytics/Inference/Modules/150-Correlation/single_corr.html#plotting-original-and-signed-rank-data",
    "title": "Inference for Correlation",
    "section": "Plotting Original and Signed Rank Data",
    "text": "Plotting Original and Signed Rank Data\nLet us see how this might work by comparing data and its signed-rank version…A quick set of plots:\n\np1 &lt;- ggplot(mydata_long,aes(x = group, y = value)) +\n  geom_jitter(width = 0.02, height = 0,aes(colour = group), size = 4) +\n  geom_segment(data = mydata_wide, aes(y = 0, yend = 0, \n                                       x = .75, \n                                       xend = 1.25 )) + \n  geom_text(aes(x = 1, y = 0.5, label = \"0\")) +\n  geom_segment(data = mydata_wide, aes(y = 0.3, yend = 0.3, \n                                       x = 1.75 , \n                                       xend = 2.25 )) + \n  geom_text(aes(x = 2, y = 0.6, label = \"0.3\")) +\n  geom_segment(data = mydata_wide, aes(y = 0.5, yend = 0.5, \n                                       x = 2.75, \n                                       xend = 3.25 )) + \n  geom_text(aes(x = 3, y = 0.8, label = \"0.5\")) +\n  labs(title = \"Original Data\", subtitle = \"Black Lines show Means\") +\n  ylab(\"Response Variable\")\n\np2 &lt;- mydata_long %&gt;% \n  group_by(group) %&gt;% \n  mutate( s_value = signed_rank(value)) %&gt;% \n  ggplot(., aes(x = group, y = s_value)) + \n  geom_jitter(width = 0.02, height = 0,aes(colour = group), size = 4) + \n  stat_summary(fun = \"mean\", geom = \"point\", colour = \"red\", \n               size = 8) + \n  labs(title = \"Signed Rank of Data\", subtitle = \"Red Points are means of Ranks!\") +\n  ylab(\"Signed Rank of Response Variable\")\n\npatchwork::wrap_plots(p1,p2, nrow = 1, guides = \"collect\")\n\n\n\n\nSo the means of the ranks three separate variables seem to be in the same order as the means of the data variables themselves.\nHow about associations between data? Do ranks reflect well what the data might?\n\n# Plot the data\np1 &lt;- ggplot(mydata_wide, aes(x, y1)) + \n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  ggtitle(\" Pearson Correlation\\n and Linear Models\")\n\n# Plot ranked data\np2 &lt;- mydata_wide %&gt;% \n  mutate(x_rank = rank(x),\n         y_rank = rank(y1)) %&gt;%\n  ggplot(.,aes(x_rank, y_rank)) + \n  geom_point(shape = 15, size = 2) +\n  geom_smooth(method = \"lm\") + \n  ggtitle(\" Spearman Ranked Correlation\\n and Linear Models\")\n\npatchwork::wrap_plots(p1,p2, nrow = 1, guides = \"collect\")\n\n\n\n\nThe slopes are almost identical, \\(0.25\\) for both original data and ranked data for \\(y1\\sim x\\). So maybe ranked and even sign_ranked data could work, and if it can work despite LINE assumptions not being satisfied, that would be nice!"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/150-Correlation/single_corr.html#how-does-sign-rank-data-work",
    "href": "content/courses/Analytics/Inference/Modules/150-Correlation/single_corr.html#how-does-sign-rank-data-work",
    "title": "Inference for Correlation",
    "section": "How does Sign-Rank data work?",
    "text": "How does Sign-Rank data work?\nTBD: need to add some explanation here.\nSpearman correlation = Pearson correlation using the rank of the data observations. Let’s check how this holds for a our x and y1 data:\nSo the Linear Model for the Ranked Data would be:\n\\[\n\\begin{aligned}\ny = \\beta_0 + \\beta_1 \\times rank(x)\\\\\n\\\\\nH_0: Null\\ Hypothesis\\ =&gt; \\beta_1 = 0\\\\\\\nH_a: Alternate\\ Hypothesis\\ =&gt; \\beta_1 \\ne 0\\\\\n\\end{aligned}\n\\]\nCode\n\n# Spearman\ncor1 &lt;- cor.test(x,y1, method = \"spearman\") %&gt;% \n  broom::tidy() %&gt;% mutate(term = \"Spearman Correlation \") %&gt;% select(term, estimate, p.value)\ncor1\n\n\n\n  \n\n\n\n\n# Pearson using ranks\ncor2 &lt;- cor.test(rank(y1), rank(x), method = \"pearson\") %&gt;% \nbroom::tidy() %&gt;% select(estimate, p.value)\ncor2\n\n\n\n  \n\n\n\n\n# Linear Models using rank\ncor3 &lt;- lm(rank(y1) ~ 1 + rank(x),data = mydata_wide) %&gt;% \n  broom::tidy() %&gt;% select(estimate, p.value)\ncor3\n\n\n\n  \n\n\n\nNotes:\n\nWhen ranks are used, the slope of the linear model (\\(\\beta_1\\)) has the same value as the Spearman correlation coefficient ( \\(\\rho\\) ).\nNote that the slope from the linear model now has an intuitive interpretation: the number of ranks y changes for each change in rank of x. ( Ranks are “independent” of sd )\nExample\nWe examine the cars93 data, where the numeric variables of interest are weight and price.\n\ncars93 %&gt;% \n  ggplot(aes(weight, price)) + \n  geom_point() + geom_smooth(method = \"lm\", se = FALSE, lty = 2) + \n  labs(title = \"Car Weight and Car Price have a nonlinear relationship\")\n\n\n\n\nLet us try a Spearman Correlation score for these variables, since the data are not linearly related and the variance of price also is not constant over weight\n\ncor.test(cars93$price, cars93$weight, method = \"spearman\") %&gt;% broom::tidy()\n\n\n\n  \n\n\n# Using linear Model\nlm(rank(price) ~ rank(weight), data = cars93) %&gt;% summary()\n\n\nCall:\nlm(formula = rank(price) ~ rank(weight), data = cars93)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-20.0676  -3.0135   0.7815   3.6926  20.4099 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   3.22074    2.05894   1.564    0.124    \nrank(weight)  0.88288    0.06514  13.554   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.46 on 52 degrees of freedom\nMultiple R-squared:  0.7794,    Adjusted R-squared:  0.7751 \nF-statistic: 183.7 on 1 and 52 DF,  p-value: &lt; 2.2e-16\n\n# Stats Plot\nggstatsplot::ggscatterstats(data = cars93, x = weight, \n                            y = price,\n                            type = \"nonparametric\",\n                            title = \"Cars93: Weight vs Price\",\n                            subtitle = \"Spearman Correlation\")\n\n\n\n\nWe see that using ranks of the price variable, we obtain a Spearman’s \\(\\rho = 0.882\\) with a p-value that is very small. Hence we are able to reject the NULL hypothesis and state that there is a relationship between these two variables. The linear relationship is evaluated as a correlation of 0.882.\n\n# Other ways using other packages\nmosaic::cor_test(gpa ~ study_hours, data = gpa_study_hours) %&gt;% broom:: tidy() %&gt;% select(estimate, p.value, conf.low, conf.high)\n\n\n\n  \n\n\n\n\nstatsExpressions::corr_test(data = gpa_study_hours, \n                            x = study_hours, \n                            y = gpa)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/paired-means.html",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/paired-means.html",
    "title": "🃏 Inference for Comparing Two Paired Means",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = FALSE,message = TRUE, warning = TRUE, fig.align = \"center\")\noptions(scipen=5)\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(broom)\nlibrary(MKinfer) # Confidence Interval Computation\nlibrary(resampledata) ### Datasets from Chihara and Hesterberg's book \nlibrary(gt) # for tables"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/paired-means.html#iconify-noto-v1-package-setting-up-r-packages",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/paired-means.html#iconify-noto-v1-package-setting-up-r-packages",
    "title": "🃏 Inference for Comparing Two Paired Means",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = FALSE,message = TRUE, warning = TRUE, fig.align = \"center\")\noptions(scipen=5)\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(broom)\nlibrary(MKinfer) # Confidence Interval Computation\nlibrary(resampledata) ### Datasets from Chihara and Hesterberg's book \nlibrary(gt) # for tables"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/paired-means.html#iconify-carbon-chart-3d-inspecting-and-charting-data",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/paired-means.html#iconify-carbon-chart-3d-inspecting-and-charting-data",
    "title": "🃏 Inference for Comparing Two Paired Means",
    "section": "\n Inspecting and Charting Data",
    "text": "Inspecting and Charting Data\n\n\n\n\n  \n\n\n  \n\n\n  \n\n\n\nThe data is made up of paired observations per swimmer, one for the semi-final and one for the final race. There are 12 swimmers and therefore 12 paired records. How can we quickly visualize this data?\nFirst, histograms and densities of the two variables at hand:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe see that:\n\nThe data are not normally distributed. With just such few readings (n &lt; 30) it was just possible…more readings would have helped. We will verify this aspect formally very shortly. \nThere is no immediately identifiable trend in score changes from one race to the other.\n\n\nA.  Check for Normality\nLet us also complete a check for normality: the shapiro.wilk test checks whether a Quant variable is from a normal distribution; the NULL hypothesis is that the data are from a normal distribution.\n\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  Diving2017$Final\nW = 0.9184, p-value = 0.273\n\n\n    Shapiro-Wilk normality test\n\ndata:  Diving2017$Semifinal\nW = 0.86554, p-value = 0.05738\n\n\n\nHmmm….the Shapiro-Wilk test suggests that both scores are normally distributed, though Semifinal is probably marginally so.\nCan we check this with plots? We can plot Q-Q plots for both variables, and also compare both data with normally-distributed data generated with the same means and standard deviations:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhile the boxplots are not very evocative, we see in the QQ-plots that the Final scores are closer to the straight line than the Semifinal scores. But it is perhaps still hard to accept the data as normally distributed…hmm.\nB.  Check for Variances\nLet us check if the two variables have similar variances: the var.test does this for us, with a NULL hypothesis that the variances are not significantly different:\n\n\n\n\n\n  \n\n\n\n[1] 3.473699\n\n\n\nThe variances are not significantly different, as seen by the \\(p.value = 0.08\\).\nSo to summarise our data checks:\n- data are normally distributed\n- variances are not significantly different"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/paired-means.html#iconify-academicons-hypothesis-hypothesis",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/paired-means.html#iconify-academicons-hypothesis-hypothesis",
    "title": "🃏 Inference for Comparing Two Paired Means",
    "section": "\n Hypothesis",
    "text": "Hypothesis\nBased on the graph, how would we formulate our Hypothesis? We wish to infer whether there is any change in performance, per swimmer between the two races. So accordingly:\n\\[\nH_0: \\mu_{semifinal} = \\mu_{final}\\\\\n\\]\n\\[\nH_a: \\mu_{semifinal} \\ne \\mu_{final}\\\n\\]"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/paired-means.html#iconify-academicons-hypothesis-observed-and-test-statistic",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/paired-means.html#iconify-academicons-hypothesis-observed-and-test-statistic",
    "title": "🃏 Inference for Comparing Two Paired Means",
    "section": "\n Observed and Test Statistic",
    "text": "Observed and Test Statistic\nWhat would be the test statistic we would use? The difference in means. Is the observed difference in the means between the two groups of scores non-zero? We use the diffmean function, with the argument only.2=FALSE to allow for paired data:\n\n\ndiffmean \n -11.975"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/paired-means.html#iconify-fluent-mdl2-insights-inference",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/paired-means.html#iconify-fluent-mdl2-insights-inference",
    "title": "🃏 Inference for Comparing Two Paired Means",
    "section": "\n Inference",
    "text": "Inference\nType help(t.test) in your Console.\n\nUsing the paired t.test\nUsing non-parametric paired Wilcoxon test\nUsing the Linear Model Method\nUsing Permutation Tests\n\n\n\nSince the data variables satisfy the assumption of being normally distributed, and the variances are not significantly different, we may attempt the classical t.test with paired data. (we will use the mosaic variant).  Our model would be:\n\\[\nmean(Final(i) - Semifinal(i)) = \\beta_0 \\\\\n\\]\nAnd that \\[\nH_0: \\beta_0 = 0;\\\\\n\\\\\nH_a: \\beta_0 \\ne 0\n\\]\n\n\n\n\n  \n\n\n\nThe confidence interval spans the zero value, and the p.value is a high \\(0.259\\), so there is no reason to accept alternative hypothesis that the means are different. Hence we say that there is no evidence of a difference between SemiFinal and Final scores.\n\n\nWell, we might consider ( based on knowledge of the sport ) that at least one of the variables does not meet the normality criteria, and though their variances are not significantly different. So we would attempt a non-parametric Wilcoxon test, that uses the signed-rank of the paired data differences, instead of the data variables. Our model would be:\n\\[\nmean(\\ sign.rank[\\ Final(i) - Semifinal(i)\\ ]\\ ) = \\beta_0 \\\\\n\\]\n\\[\nH_0 : \\beta_0 = 0;\\\\\nH_a: \\beta_0 \\ne 0\n\\]\n\n\n\n\n  \n\n\n\nHere also with the p.value being \\(0.3804\\), we have no reason to accept the Alternative Hypothesis. The parametric t.test and the non-parametric wilcox.test agree in their inferences.\n\n\nWe can apply the linear-model-as-inference interpretation both to the original data and to the sign.rank data: \n\\[\nlm(y_i - x_i \\sim 1) = \\beta_0\\\\\n\\\\ and\\\\\nlm(\\ sign.rank[\\ Final(i) - Semifinal(i)\\ ] \\sim 1) = \\beta_0\\\\\n\\]\nAnd the Hypothesis for both interpretations would be:\\[\nH_0: \\beta_0 = 0\\\\\n\\\\\\\nH_a: \\beta_0 \\ne 0\\\\\n\\]\n\n\n\n\n  \n\n\n  \n\n\n\nWe observe that using the linear model method for the original scores and the sign-rank scores both sdo not permit us to reject the \\(H_0\\) Null Hypothesis, since p.values are high, and the confidence.intervals straddle \\(0\\).\n\n\nWe saw from the diagram created by Allen Downey that there is only one test! We will now use this philosophy to develop a technique that allows us to mechanize several Statistical Models in that way, with nearly identical code. For the specific data at hand, we need to shuffle the records between Semifinal and Final on a per Swimmer basis and take the test statistic (difference between the two swim records for each swimmer). Another way to look at this is to take the differences between Semifinal and Final scores and shuffle the differences to either polarity. We will follow this method in the code below:\n\n\n\n [1]  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1\n\n\n\n\n  \n\n\n\n\nLet us plot the NULL distribution and compare it with the actual observed differences in the race times:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprop_TRUE \n   0.1254 \n\n\n\nHmm…so by generating 9999 shuffles of score-difference polarities, it does appear that we can not only obtain the current observed difference but even surpass it frequently. So it does seem that there is no difference in means between Semi-Final and Final swimming scores.\n\n\n\nAll Tests Together\nWe can put all the test results together to get a few more insights about the tests:\n\n\n\n\n\n\n\n\nt.test\n    \n\nestimate\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n\n\n-11.975\n-1.190339\n0.2589684\n11\n-34.11726\n10.16726\nPaired t-test\ntwo.sided\n\n\n\n\n\n\n\n\nLinear Model\n    \n\nterm\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n\n\n(Intercept)\n-11.975\n10.06016\n-1.190339\n0.2589684\n-34.11726\n10.16726\n\n\n\n\n\n\n\n\nWilcoxon test\n    \n\nstatistic\n      p.value\n      method\n      alternative\n    \n\n\n27\n0.3803711\nWilcoxon signed rank exact test\ntwo.sided\n\n\n\n\n\n\n\n\nLinear Model with sign.rank\n    \n\nterm\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n\n\n(Intercept)\n-2\n2.135558\n-0.9365236\n0.3691097\n-6.70033\n2.70033\n\n\n\n\n\nThe linear model and the t.test are nearly identical in performance; the p.values are the same. The same is also true of the wilcox.test and the linear model with sign-rank data differences. This is of course not surprising!"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/paired-means.html#iconify-carbon-chart-3d-inspecting-and-charting-data-1",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/paired-means.html#iconify-carbon-chart-3d-inspecting-and-charting-data-1",
    "title": "🃏 Inference for Comparing Two Paired Means",
    "section": "\n Inspecting and Charting Data",
    "text": "Inspecting and Charting Data\n\n\n\n\n  \n\n\n  \n\n\n  \n\n\n\nThere are just 30 prices for each vendor….just barely enough to get an idea of what the distribution might be. Let us plot histograms/densities of the two variables that we wish to compare. We will also overlay a Gaussian distribution for comparison:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNot close to the Gaussian…there is clearly some skew to the right, with some items being very costly compared to the rest. More when we check the assumptions on data for the tests.\nHow about price differences, what we are interested in? Let us plot the prices for the products, as box plots after pivoting the data to long form, 1 and as bar charts:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe see that the price difference between Walmart and Target prices is highest for the Product named Quaker Oats Life Cereal Original. Apart from this Product, the rest have no discernible trend either way. Let us check observed statistic (the mean difference in prices)\n\n\n\n\n\n  diffmean \n0.05666667 \n\n\n\n\n Hypothesis\nBased on the graph, how would we formulate our Hypothesis? We wish to infer whether there is any change in prices, per product between the two Store chains. So accordingly:\n\\[\nH_0: \\mu_{Walmart} = \\mu_{Target}\\\\\n\\]\n\\[\nH_a: \\mu_{Walmart} \\ne \\mu_{Target}\\\n\\]\nTesting for Assumptions on the Data\nThere are a few checks we need to make of our data, to decide what test procedure to use.\nA.  Check for Normality\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  Groceries$Walmart\nW = 0.78662, p-value = 0.00003774\n\n\n    Shapiro-Wilk normality test\n\ndata:  Groceries$Target\nW = 0.79722, p-value = 0.00005836\n\n\nFor both tests, we see that the p.value is very small, indicating that the data are unlikely to be normally distributed. This means we cannot apply a standard paired t.test and need to use the non-parametric wilcox.test, that does not rely on the assumption of normality.\nB.  Check for Variances\nLet us check if the two variables have similar variances:\n\n\n\n    F test to compare two variances\n\ndata:  Groceries$Walmart and Groceries$Target\nF = 0.97249, num df = 29, denom df = 29, p-value = 0.9406\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.4628695 2.0431908\nsample estimates:\nratio of variances \n         0.9724868 \n\n\nIt appears from the \\(p.value = 0.9\\) and the \\(Confidence Interval = [0.4629, 2.0432]\\) that we cannot reject the NULL Hypothesis that the variances are not significantly different.\n\n Inference\n\n\nUsing paired t.test\nUsing non-parametric paired Wilcoxon test\nUsing the Linear Model Method\nUsing Permutation Tests\n\n\n\nWell, the variables are not normally distributed, so a standard t.test is not advised, even if the variances are similar. We can still try:\n\n\n\n\n  \n\n\n\nThe p.value is \\(0.64\\) ! And the Confidence Interval straddles \\(0\\). So the t.test gives us no reason to reject the Null Hypothesis that the means are similar. But can we really believe this, given the non-normality of data?\n\n\nHowever, we have seen that the data variables are not normally distributed. So a Wilcoxon Test, using signed-ranks, is indicated: (recall the model!)\n\n\n\n\n  \n\n\n\nThe Wilcoxon test result is very interesting: the p.value says there is a significant difference between the two store prices, and the confidence.interval also is unipolar…\n\n\nAs before we can do the linear model for both the original data and the sign.rank data. The test statistic is again the difference between the two variables:\n\n\n\n\n  \n\n\n  \n\n\n\nVery interesting results, but confirming what we saw earlier: The Linear Model with the original data reports no significant difference, but the linear model with sign-ranks, suggests there is a significant difference in means prices between stores!\n\n\nLet us perform the pair-wise permutation test on prices, by shuffling the two store names:\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\nprop_-0.302 \n     0.0002 \n\n\nDoes not seem to be any significant difference in prices…\n\n\n\nAll Tests Together\nWe can put all the test results together to get a few more insights about the tests:\n\n\n\n\n\n\n\n\nt.test\n    \n\nestimate\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n\n\n-0.05666667\n-0.4704556\n0.6415488\n29\n-0.3030159\n0.1896825\nPaired t-test\ntwo.sided\n\n\n\n\n\n\n\n\nLinear Model\n    \n\nterm\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n\n\n(Intercept)\n0.05666667\n0.1204506\n0.4704556\n0.6415488\n-0.1896825\n0.3030159\n\n\n\n\n\n\n\n\nWilcoxon Test\n    \n\nestimate\n      statistic\n      p.value\n      conf.low\n      conf.high\n      method\n      alternative\n    \n\n\n-0.104966\n95\n0.01431746\n-0.1750051\n-0.03005987\nWilcoxon signed rank test with continuity correction\ntwo.sided\n\n\n\n\n\n\n\n\nLinear Model with Sign.Ranks\n    \n\nterm\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n\n\n(Intercept)\n8.533333\n2.888834\n2.953902\n0.006167464\n2.625004\n14.44166\n\n\n\n\n\nClearly, the parametric tests do not detect a significant difference in prices, whereas the non-parametric tests do.\nSuppose we knock off the Quaker Cereal data item…(note the spaces in the product name)\n\n\n\n\n\n  \n\n\n\n[1] 0.1558621\n\n\n\n\n\n\n\n\n\n\n[1] 0.01390139\n\n\n\nWe see that removing the Quaker Oats product item from the data does give a significant difference in mean prices !!! That one price difference was in the opposite direction compared to the general trend in differences, so when it was removed, we obtained a truer picture of price differences.\nTry to do a regular parametric t.test with this reduced data!"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/paired-means.html#iconify-fluent-mdl2-decision-solid-conclusion",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/paired-means.html#iconify-fluent-mdl2-decision-solid-conclusion",
    "title": "🃏 Inference for Comparing Two Paired Means",
    "section": "\n Conclusion",
    "text": "Conclusion\nWe have learnt how to perform inference for paired-means. We have looked at the conditions that make the regular t.test possible, and learnt what to do if the conditions of normality and equal variance are not met. We have also looked at how these tests can be understood as manifestations of the linear model, with data and sign-ranked data. It should also be fairly clear now that we can test for the equivalence of two paired means, using a very simple permutation tests. Given computing power, we can always mechanize this test very quickly to get our results. And that performing this test yields reliable results without having to rely on any assumption relating to underlying distributions and so on."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/paired-means.html#iconify-ooui-references-ltr-references",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/paired-means.html#iconify-ooui-references-ltr-references",
    "title": "🃏 Inference for Comparing Two Paired Means",
    "section": "\n References",
    "text": "References\n\nRandall Pruim, Nicholas J. Horton, Daniel T. Kaplan, Start Teaching with R\n\nhttps://bcs.wiley.com/he-bcs/Books?action=index&itemId=111941654X&bcsId=11307\nhttps://statsandr.com/blog/wilcoxon-test-in-r-how-to-compare-2-groups-under-the-non-normality-assumption/"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/paired-means.html#footnotes",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/paired-means.html#footnotes",
    "title": "🃏 Inference for Comparing Two Paired Means",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://raw.githubusercontent.com/gadenbuie/tidyexplain/main/images/tidyr-pivoting.gif↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/inf_for_numerical_data.html",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/inf_for_numerical_data.html",
    "title": "Inference for numerical data",
    "section": "",
    "text": "In this lab, we will explore and visualize the data using the tidyverse suite of packages, and perform statistical inference using infer. The data can be found in the companion package for OpenIntro resources, openintro.\nLet’s load the packages.\n\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(openintro)\nlibrary(infer)\nlibrary(skimr)\n\n\nTo create your new lab report, in RStudio, go to New File -&gt; R Markdown… Then, choose From Template and then choose Lab Report for OpenIntro Statistics Labs from the list of templates.\n\nEvery two years, the Centers for Disease Control and Prevention conduct the Youth Risk Behavior Surveillance System (YRBSS) survey, where it takes data from high schoolers (9th through 12th grade), to analyze health patterns. You will work with a selected group of variables from a random sample of observations during one of the years the YRBSS was conducted.\nLoad the yrbss data set into your workspace.\n\ndata(yrbss)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/inf_for_numerical_data.html#getting-started",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/inf_for_numerical_data.html#getting-started",
    "title": "Inference for numerical data",
    "section": "",
    "text": "In this lab, we will explore and visualize the data using the tidyverse suite of packages, and perform statistical inference using infer. The data can be found in the companion package for OpenIntro resources, openintro.\nLet’s load the packages.\n\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(openintro)\nlibrary(infer)\nlibrary(skimr)\n\n\nTo create your new lab report, in RStudio, go to New File -&gt; R Markdown… Then, choose From Template and then choose Lab Report for OpenIntro Statistics Labs from the list of templates.\n\nEvery two years, the Centers for Disease Control and Prevention conduct the Youth Risk Behavior Surveillance System (YRBSS) survey, where it takes data from high schoolers (9th through 12th grade), to analyze health patterns. You will work with a selected group of variables from a random sample of observations during one of the years the YRBSS was conducted.\nLoad the yrbss data set into your workspace.\n\ndata(yrbss)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/inf_for_numerical_data.html#exploratory-data-analysis",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/inf_for_numerical_data.html#exploratory-data-analysis",
    "title": "Inference for numerical data",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nThere are observations on 13 different variables, some categorical and some numerical. The meaning of each variable can be found by bringing up the help file: type this in your console\n\n\n\n\n\n\nNote\n\n\n\nhelp(yrbss)\n\n\n\n\n\n\n\n\nNote\n\n\n\n1 . What are the cases in this data set? How many cases are there in our sample?\n\n\nYou will first start with analyzing the weight of the participants in kilograms: weight.\nUsing visualization and summary statistics, describe the distribution of weights. The inspect() function from the mosaic package produces nice summaries of the variables in the dataset, separating categorical (character) variables from quantitative variables.\n\nmosaic::inspect(yrbss)\n\n\ncategorical variables:  \n                      name     class levels     n missing\n1                   gender character      2 13571      12\n2                    grade character      5 13504      79\n3                 hispanic character      2 13352     231\n4                     race character      5 10778    2805\n5               helmet_12m character      6 13272     311\n6   text_while_driving_30d character      8 12665     918\n7  hours_tv_per_school_day character      7 13245     338\n8 school_night_hours_sleep character      7 12335    1248\n                                   distribution\n1 male (51.2%), female (48.8%)                 \n2 9 (26.6%), 12 (26.3%), 11 (23.6%) ...        \n3 not (74.4%), hispanic (25.6%)                \n4 White (59.5%) ...                            \n5 never (52.6%), did not ride (34.3%) ...      \n6 0 (37.8%), did not drive (36.7%) ...         \n7 2 (20.4%), &lt;1 (16.4%), 3 (16.1%) ...         \n8 7 (28.1%), 8 (21.8%), 6 (21.5%) ...          \n\nquantitative variables:  \n                  name   class   min    Q1 median    Q3    max      mean\n1                  age integer 12.00 15.00  16.00 17.00  18.00 16.157041\n2               height numeric  1.27  1.60   1.68  1.78   2.11  1.691241\n3               weight numeric 29.94 56.25  64.41 76.20 180.99 67.906503\n4 physically_active_7d integer  0.00  2.00   4.00  7.00   7.00  3.903005\n5 strength_training_7d integer  0.00  0.00   3.00  5.00   7.00  2.949948\n          sd     n missing\n1  1.2637373 13506      77\n2  0.1046973 12579    1004\n3 16.8982128 12579    1004\n4  2.5641046 13310     273\n5  2.5768522 12407    1176\n\n\nNext, consider the possible relationship between a high schooler’s weight and their physical activity. Plotting the data is a useful first step because it helps us quickly visualize trends, identify strong associations, and develop research questions.\nFirst, let’s create a new variable physical_3plus, which will be coded as either “yes” if the student is physically active for at least 3 days a week, and “no” if not. Recall that we have several missing data in that column, so we will (sadly) drop these before generating the new variable:\n\nyrbss &lt;- yrbss %&gt;% \n  drop_na() %&gt;% \n  mutate(physical_3plus = if_else(physically_active_7d &gt;= 2, \"yes\", \"no\"),\n         physical_3plus = factor(physical_3plus, \n                                 labels = c(\"yes\", \"no\"),\n                                 levels = c(\"yes\", \"no\")))\n# Let us check\nyrbss %&gt;% count(physical_3plus)\n\n\n\n  \n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nMake a side-by-side violin box plots of physical_3plus and weight.\nIs there a relationship between these two variables? What did you expect and why?\n\n\n\n\ngf_boxplot(weight ~ physical_3plus, \n          fill = ~ physical_3plus,\n          data = yrbss,\n          draw_quantiles = TRUE)\n\n\n\n\nThe box plots show how the medians of the two distributions compare, but we can also compare the means of the distributions using the following to first group the data by the physical_3plus variable, and then calculate the mean weight in these groups using the mean function while ignoring missing values by setting the na.rm argument to TRUE.\n\nyrbss %&gt;%\n  group_by(physical_3plus) %&gt;%\n  summarise(mean_weight = mean(weight, na.rm = TRUE))\n\n\n\n  \n\n\n\nThere is an observed difference, but is this difference large enough to deem it “statistically significant”? In order to answer this question we will conduct a hypothesis test."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/inf_for_numerical_data.html#inference",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/inf_for_numerical_data.html#inference",
    "title": "Inference for numerical data",
    "section": "Inference",
    "text": "Inference\n\n\n\n\n\n\nImportant\n\n\n\nAre all conditions necessary for inference satisfied? Comment on each. You can compute the group sizes with the summarize command above by defining a new variable with the definition n().\n\n\n\n\n\n\n\n\nNote\n\n\n\nWrite the hypotheses for testing if the average weights are different for those who exercise at least times a week and those who don’t.\nWrite here !\n\n\nWe will do this in two ways, just for fun: one using mosaic and the other using infer.\nBut first, we need to initialize the test, which we will save as obs_diff.\n\nobs_diff_infer &lt;- yrbss %&gt;%\n  specify(weight ~ physical_3plus) %&gt;%\n  calculate(stat = \"diff in means\", order = c(\"yes\", \"no\"))\nobs_diff_infer\n\n\n\n  \n\n\nobs_diff_mosaic &lt;- diffmean(~ weight | physical_3plus, data = yrbss)\nobs_diff_mosaic\n\n diffmean \n-1.694383 \n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that obs_diff_infer is a 1 X 1 dataframe; obs_diff_mosaic is a scalar!!\n\n\n\n\nInference Using `infer`\nInference Using `mosaic`\n\n\n\nNext, we will work through creating a permutation distribution using tools from the infer package.\nRecall that the specify() function is used to specify the variables you are considering (notated y ~x), and you can use the calculate() function to specify the statistic you want to calculate and the order of subtraction you want to use. For this hypothesis, the statistic you are searching for is the difference in means, with the order being yes - no.\nAfter you have calculated your observed statistic, you need to create a permutation distribution. This is the distribution that is created by shuffling the observed weights into new physical_3plus groups, labeled “yes” and “no”.\nWe will save the permutation distribution as null_dist.\n\nnull_dist &lt;- yrbss %&gt;%\n  specify(weight ~ physical_3plus) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  generate(reps = 1000, type = \"permute\") %&gt;%\n  calculate(stat = \"diff in means\", order = c(\"yes\", \"no\"))\n\nThe hypothesize() function is used to declare what the null hypothesis is. Here, we are assuming that student’s weight is independent of whether they exercise at least 3 days or not.\nWe should also note that the type argument within generate() is set to \"permute\". This ensures that the statistics calculated by the calculate() function come from a reshuffling of the data (not a resampling of the data)! Finally, the specify() and calculate() steps should look familiar, since they are the same as what we used to find the observed difference in means!\nWe can visualize this null distribution with the following code:\n\ngf_histogram(data = null_dist, ~ stat)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nAdd a vertical red line to the plot above, demonstrating where the observed difference in means (obs_diff_mosaic) falls on the distribution.\nHow many of these null_dist permutations have a difference at least as large (or larger) as obs_diff_mosaic?\n\n\n\nNow that you have calculated the observed statistic and generated a permutation distribution, you can calculate the p-value for your hypothesis test using the function get_p_value() from the infer package.\n\nnull_dist %&gt;%\n  get_p_value(obs_stat = obs_diff_infer, direction = \"two_sided\")\n\nWarning: Please be cautious in reporting a p-value of 0. This result is an\napproximation based on the number of `reps` chosen in the `generate()` step.\nSee `?get_p_value()` for more information.\n\n\n\n\n  \n\n\n\n\n\n\n\nWhat warning message do you get? Why do you think you get this warning message?\nConstruct and record a confidence interval for the difference between the weights of those who exercise at least three times a week and those who don’t, and interpret this interval in context of the data.\n\n\n\n\nWe already have the observed difference, obs_diff_mosaic. Now we generate the null distribution using permutation, with mosaic:\n\nnull_dist_mosaic &lt;- do(1000) * diffmean(~ weight | shuffle(physical_3plus), data = yrbss)\n\nWe can also generate the histogram of the null distribution, compare that with the observed diffrence and compute the p-value and confidence intervals:\n\ngf_histogram(~ diffmean, data = null_dist_mosaic) %&gt;% \n  gf_vline(xintercept = obs_diff_mosaic, colour = \"red\")\n\n\n\n# p-value\nprop(~ diffmean &gt;= obs_diff_mosaic, data = null_dist_mosaic)\n\nprop_TRUE \n        1 \n\n# Confidence Intervals for p = 0.95\nmosaic::cdata(~ diffmean, p = 0.95, data = null_dist_mosaic)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/inf_for_numerical_data.html#more-practice",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/inf_for_numerical_data.html#more-practice",
    "title": "Inference for numerical data",
    "section": "More Practice",
    "text": "More Practice\n\nCalculate a 95% confidence interval for the average height in meters (height) and interpret it in context.\nCalculate a new confidence interval for the same parameter at the 90% confidence level. Comment on the width of this interval versus the one obtained in the previous exercise.\nConduct a hypothesis test evaluating whether the average height is different for those who exercise at least three times a week and those who don’t.\nNow, a non-inference task: Determine the number of different options there are in the dataset for the hours_tv_per_school_day there are.\nCome up with a research question evaluating the relationship between height or weight and sleep. Formulate the question in a way that it can be answered using a hypothesis test and/or a confidence interval. Report the statistical results, and also provide an explanation in plain language. Be sure to check all assumptions, state your \\(\\alpha\\) level, and conclude in context."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/two-means-tutorial.html",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/two-means-tutorial.html",
    "title": "Permutation Tests for Two Means",
    "section": "",
    "text": "library(ggplot2)\nlibrary(dplyr)\nlibrary(mosaic)\n\nlibrary(resampledata)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/two-means-tutorial.html#setting-up-the-packages",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/two-means-tutorial.html#setting-up-the-packages",
    "title": "Permutation Tests for Two Means",
    "section": "",
    "text": "library(ggplot2)\nlibrary(dplyr)\nlibrary(mosaic)\n\nlibrary(resampledata)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/two-means-tutorial.html#case-study-1-verizon",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/two-means-tutorial.html#case-study-1-verizon",
    "title": "Permutation Tests for Two Means",
    "section": "Case Study-1: Verizon",
    "text": "Case Study-1: Verizon\nDoes Verizon create a difference in Repair Times between ILEC and CLEC systems?\n\ndata(\"Verizon\")\ninspect(Verizon)\n\n\ncategorical variables:  \n   name  class levels    n missing\n1 Group factor      2 1687       0\n                                   distribution\n1 ILEC (98.6%), CLEC (1.4%)                    \n\nquantitative variables:  \n  name   class min   Q1 median   Q3   max     mean       sd    n missing\n1 Time numeric   0 0.75   3.63 7.35 191.6 8.522009 14.78848 1687       0\n\n\nDescribe the Variables!\nHypothesis Specification\nWrite the Null and Alternate hypotheses here.\nNull Distribution Computation\nVerizon Conclusion"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/two-means-tutorial.html#case-story-2-recidivism",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/two-means-tutorial.html#case-story-2-recidivism",
    "title": "Permutation Tests for Two Means",
    "section": "Case Story-2: Recidivism",
    "text": "Case Story-2: Recidivism\nDo criminals released after a jail term commit crimes again? Does recidivism depend upon age?\n\ndata(\"Recidivism\")\ninspect(Recidivism)\n\n\ncategorical variables:  \n     name  class levels     n missing\n1  Gender factor      2 17019       3\n2     Age factor      5 17019       3\n3   Age25 factor      2 17019       3\n4    Race factor     10 16988      34\n5 Offense factor      2 17022       0\n6   Recid factor      2 17022       0\n7    Type factor      3 17022       0\n                                   distribution\n1 M (87.7%), F (12.3%)                         \n2 25-34 (36.6%), 35-44 (23.7%) ...             \n3 Over 25 (81.9%), Under 25 (18.1%)            \n4 White-NonHispanic (67%) ...                  \n5 Felony (80.6%), Misdemeanor (19.4%)          \n6 No (68.4%), Yes (31.6%)                      \n7 No Recidivism (68.4%), New (20.2%) ...       \n\nquantitative variables:  \n  name   class min  Q1 median  Q3  max     mean       sd    n missing\n1 Days integer   0 241    418 687 1095 473.3275 283.1393 5386   11636\n\n\nDescribe the variables!\nHypothesis Specification\nLet us see if the indidence of recidivism is dependent upon whether a person is aged less than or more than 25 years. Write the Null and Alternate hypotheses here.\n\\[\nH_0: \\mu_{recid-age-25-minus}\\ = \\mu_{recid-age-25-plus}\\\\\n\\]\n\\[\nH_a:\\mu_{recid-age-25-minus}\\ \\ne\\mu_{recid-age-25-plus}\\\\\n\\]\n\nRecidivism\n\n\n\n  \n\n\ninspect(Recidivism)\n\n\ncategorical variables:  \n     name  class levels     n missing\n1  Gender factor      2 17019       3\n2     Age factor      5 17019       3\n3   Age25 factor      2 17019       3\n4    Race factor     10 16988      34\n5 Offense factor      2 17022       0\n6   Recid factor      2 17022       0\n7    Type factor      3 17022       0\n                                   distribution\n1 M (87.7%), F (12.3%)                         \n2 25-34 (36.6%), 35-44 (23.7%) ...             \n3 Over 25 (81.9%), Under 25 (18.1%)            \n4 White-NonHispanic (67%) ...                  \n5 Felony (80.6%), Misdemeanor (19.4%)          \n6 No (68.4%), Yes (31.6%)                      \n7 No Recidivism (68.4%), New (20.2%) ...       \n\nquantitative variables:  \n  name   class min  Q1 median  Q3  max     mean       sd    n missing\n1 Days integer   0 241    418 687 1095 473.3275 283.1393 5386   11636\n\n\nAlso, the variable Recid is a factor variable coded “Yes” or “No”. We ought to convert it to a numeric variable of 1’s and 0’s. Why?\nNull Distribution for Recidivism\nRecidivism Conclusion\nCase Study #3: Flight Delays\nLaGuardia Airport (LGA) is one of three major airports that serves the New York City metropolitan area. In 2008, over 23 million passengers and over 375 000 planes flew in or out of LGA. United Airlines and America Airlines are two major airlines that schedule services at LGA. The data set FlightDelays contains information on all 4029 departures of these two airlines from LGA during May and June 2009.\n\ndata(\"FlightDelays\")\ninspect(FlightDelays)\n\n\ncategorical variables:  \n         name  class levels    n missing\n1     Carrier factor      2 4029       0\n2 Destination factor      7 4029       0\n3  DepartTime factor      5 4029       0\n4         Day factor      7 4029       0\n5       Month factor      2 4029       0\n6   Delayed30 factor      2 4029       0\n                                   distribution\n1 AA (72.1%), UA (27.9%)                       \n2 ORD (44.3%), DFW (22.8%), MIA (15.1%) ...    \n3 8-Noon (26.1%), Noon-4pm (26%) ...           \n4 Fri (15.8%), Mon (15.6%), Tue (15.6%) ...    \n5 June (50.4%), May (49.6%)                    \n6 No (85.2%), Yes (14.8%)                      \n\nquantitative variables:  \n          name   class min   Q1 median   Q3  max      mean         sd    n\n1           ID integer   1 1008   2015 3022 4029 2015.0000 1163.21645 4029\n2     FlightNo integer  71  371    691  787 2255  827.1035  551.30939 4029\n3 FlightLength integer  68  155    163  228  295  185.3011   41.78783 4029\n4        Delay integer -19   -6     -3    5  693   11.7379   41.63050 4029\n  missing\n1       0\n2       0\n3       0\n4       0\n\n\nThe variables in the FlightDelays dataset are:\nHypothesis Specification\nLet us compute the proportion of times that each carrier’s flights was delayed more than 20 min. We will conduct a two-sided test to see if the difference in these proportions is statistically significant.\nNull Distribution for FlightDelays\n\nwhich is very small. Hence we reject the null Hypothesis that there is no difference between carriers on delay times."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html",
    "title": "🃏 Inference for a Single Mean",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE,message = TRUE,warning = TRUE, fig.align = \"center\")\noptions(digits=2)\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(infer)\n### Dataset from Chihara and Hesterberg's book (Second Edition)\nlibrary(resampledata)\nlibrary(openintro) # datasets"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html#iconify-noto-v1-package-setting-up-r-packages",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html#iconify-noto-v1-package-setting-up-r-packages",
    "title": "🃏 Inference for a Single Mean",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE,message = TRUE,warning = TRUE, fig.align = \"center\")\noptions(digits=2)\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(infer)\n### Dataset from Chihara and Hesterberg's book (Second Edition)\nlibrary(resampledata)\nlibrary(openintro) # datasets"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html#introduction",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html#introduction",
    "title": "🃏 Inference for a Single Mean",
    "section": "Introduction",
    "text": "Introduction\nWe saw from the diagram created by Allen Downey that there is only one test 1! We will now use this philosophy to develop a technique that allows us to mechanize several Statistical Models in that way, with nearly identical code.\nWe will use two packages in R, mosaic to develop our intuition for what are called bootstrap randomization based statistical tests. (There is also a more recent package called infer in R which can do pretty much all of this, including visualization. In my opinion, the code is a little too high-level and does not offer quite the detailed insight that the mosaic package does)."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html#iconify-pajamas-issue-type-test-case-case-study-1-toy-data",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html#iconify-pajamas-issue-type-test-case-case-study-1-toy-data",
    "title": "🃏 Inference for a Single Mean",
    "section": "\n Case Study #1: Toy data",
    "text": "Case Study #1: Toy data\nFirst we will use a toy dataset with three “imaginary” samples, \\(x, y, y2\\). Each is normally distributed and made up of 50 observations.\nWe start by creating a function that will allow us to produce samples of a given size (N) with a specified mean (mu) and standard deviation (sd).\n\nrnorm_fixed  &lt;- function(N, mu = 0, sd = 1) {\n  scale(rnorm(N))* sd + mu\n}\n\nWe create three variables: x ( explanatory) and y, y2 ( dependent ).\n\nset.seed(40) # for replication\n\n# Data as vectors ( for t.tests etc)\nx &lt;- rnorm_fixed(50, mu = 0.0, sd = 1) #explanatory\ny &lt;- rnorm_fixed(50, mu = 0.3, sd = 2) # dependent #1\ny2 &lt;- rnorm_fixed(50, mu = 0.5, sd = 1.5) # dependent #2\n\n# Make a tibble with all variables\nmydata_wide &lt;- tibble(x = x, y = y, y2 = y2)\n\n# Long form data\nmydata_long &lt;- \n  mydata_wide %&gt;%\n  pivot_longer(., cols = c(x,y,y2), \n               names_to = \"group\", \n               values_to = \"value\")\n\n# Long form data with only dependent variables\nmydata_long_y &lt;- \n  mydata_wide %&gt;% \n  select(-x) %&gt;% \n  pivot_longer(., cols = c(y,y2), \n               names_to = \"group\", \n               values_to = \"value\")\nmydata_wide\nmydata_long\nmydata_long_y"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html#signed-rank-values",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html#signed-rank-values",
    "title": "🃏 Inference for a Single Mean",
    "section": "“Signed Rank” Values",
    "text": "“Signed Rank” Values\nMost statistical tests use the actual values of the data variables. However, in some non-parametric statistical tests, the data are used in rank-transformed sense/order. In some cases the signed-rank of the data values is used instead of the data itself.\nSigned Rank is calculated as follows:\n1. Take the absolute value of each observation in a sample\n2. Place the ranks in order of (absolute magnitude). The smallest number has rank = 1 and so on.\n3. Give each of the ranks the sign of the original observation ( + or - )\n\nsigned_rank &lt;- function(x) {sign(x) * rank(abs(x))}"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html#iconify-openmoji-japanese-symbol-for-beginner-introduction-to-inference-for-a-single-mean",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html#iconify-openmoji-japanese-symbol-for-beginner-introduction-to-inference-for-a-single-mean",
    "title": "🃏 Inference for a Single Mean",
    "section": "\n Introduction to Inference for a Single Mean",
    "text": "Introduction to Inference for a Single Mean\nA series of tests deal with one mean value of a sample. The idea is to evaluate whether that mean is representative of the mean of the underlying population. Depending upon the nature of the (single) variable, the test that can be used are as follows:\n\nflowchart TD\n    A[Inference for Single Mean] --&gt;|Check Assumptions| B[Normality: Shapiro-Wilk Test shapiro.test\\n]\n    B --&gt; C{OK?}\n    C --&gt;|Yes\\n Parametric| D[t.test]\n    D &lt;--&gt;F[Linear Model\\n with Data] \n    C --&gt;|No\\n Non-Parametric| E[wilcox.test]\n    E &lt;--&gt; G[Linear Model\\n with\\n Signed-Ranks of Data]\n    C --&gt;|No\\n Non-Parametric| P[Bootstrap]\n    P &lt;--&gt; Q[Linear Model\\n with Signed-Rank\\n with Bootstrap]\n\n\n\nflowchart TD\n    A[Inference for Single Mean] --&gt;|Check Assumptions| B[Normality: Shapiro-Wilk Test shapiro.test\\n]\n    B --&gt; C{OK?}\n    C --&gt;|Yes\\n Parametric| D[t.test]\n    D &lt;--&gt;F[Linear Model\\n with Data] \n    C --&gt;|No\\n Non-Parametric| E[wilcox.test]\n    E &lt;--&gt; G[Linear Model\\n with\\n Signed-Ranks of Data]\n    C --&gt;|No\\n Non-Parametric| P[Bootstrap]\n    P &lt;--&gt; Q[Linear Model\\n with Signed-Rank\\n with Bootstrap]"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html#iconify-carbon-chart-3d-inspecting-and-charting-data",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html#iconify-carbon-chart-3d-inspecting-and-charting-data",
    "title": "🃏 Inference for a Single Mean",
    "section": "\n Inspecting and Charting Data",
    "text": "Inspecting and Charting Data\nTesting Assumptions in the Data\nInference\n\n\nThe Student’s t-test with one sample\nWilcoxon’s Signed-Rank Test\nLinear Model\nUsing bootstrap\nPlots\n\n\n\nA. Model\nA single number predicts y:\n\\[\ny = \\beta_0 + \\beta_1*x \\\\\n\\] \\[\n\\\\and\\ further \\\\\n\\] \\[\ny = \\beta_0\\\n\\]\nand the second term vanishes, since “there is no x”: all the x-values are made equal to zero in the linear model for a single variable !! The NULL Hypothesis therefore is:\n\\[\n\\ H_0: \\beta_0 = 0\n\\]\nNote that if we want the NULL hypothesis to be that the mean is other than zero, we can use\n\\[\nH_0:\\ \\beta_0 = \\mu \\ne 0\n\\] the lm(...., mu = some_number, ..) parameter in the command in the code.\nB. Code\nIf we compare the t.test with the appropriate lm model:\n\n# t-test\nt1 &lt;- t.test(y, mu = 0, alternative = \"two.sided\")\nprint(t1)\n\n\n    One Sample t-test\n\ndata:  y\nt = 1.0607, df = 49, p-value = 0.294\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.2683937  0.8683937\nsample estimates:\nmean of x \n      0.3 \n\n\nSo even though y has a mean of 0.3, the confidence intervals straddle zero, and hence we cannot reject the NULL hypothesis that the true population, of which y is a sample, could have mean=0.\n\n\nSince we are dealing with the mean, the sign of the rank becomes important to use, in the case of a non-parametric single mean test.\nA. Model\n\\[\nsigned\\_rank(y) = \\beta_0 \\\\\nH_0: \\beta_0 = 0\n\\]\nB. Code\n\n# Standard Wilcoxon Signed_Rank Test\nw1 &lt;- wilcox.test(y)\nw1\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  y\nV = 754, p-value = 0.2628\nalternative hypothesis: true location is not equal to 0\n\n# Wilcoxon test with lm\nw2 &lt;- lm(signed_rank(y) ~ 1 , data = mydata_wide)\nw2\n\n\nCall:\nlm(formula = signed_rank(y) ~ 1, data = mydata_wide)\n\nCoefficients:\n(Intercept)  \n       4.66  \n\n# t-test with signed_rank data\nw3 &lt;- t.test(signed_rank(y))\nw3\n\n\n    One Sample t-test\n\ndata:  signed_rank(y)\nt = 1.1277, df = 49, p-value = 0.265\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -3.644491 12.964491\nsample estimates:\nmean of x \n     4.66 \n\n\n\n\n\n# linear model\nlm1 &lt;- lm(y ~ 1, data = mydata_wide)\nlm1 %&gt;% summary()\n\n\nCall:\nlm(formula = y ~ 1, data = mydata_wide)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5554 -1.4845 -0.0392  1.5559  4.5119 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   0.3000     0.2828   1.061    0.294\n\nResidual standard error: 2 on 49 degrees of freedom\n\nlm1 %&gt;% confint()\n\n                 2.5 %    97.5 %\n(Intercept) -0.2683937 0.8683937\n\n\nThe confidence intervals for both the t.test and the lm model are identical.\nt-test confidence intervals: -0.2683937, 0.8683937\nlinear model confidence intervals: -0.2683937, 0.8683937\n\n\n\n\n\nWe can plot the y data both original and ranked to see where the mean lies in each case. The approximation to the true \\(\\beta_0\\) ( is good when the number of observations \\(n &gt;=50\\). Lindoloev has a simulation on this.. We can also plot the model using lm for both the original data and the sign-ranked data.\n\np1 &lt;- ggplot(mydata_wide, aes( x = 0, y = y)) +\n  geom_point(alpha = 0.4) +\n  geom_segment(aes(y = t1$estimate, \n                   yend = t1$estimate, \n                   x = -0.2, xend = 0.2)) + \n  labs(title = \"Student's\\n t-Test\")\n\n# t-test using linear model\np2 &lt;- ggplot(mydata_wide, aes( x = 0, y = y)) +\n  geom_point(alpha = 0.4) +\n  geom_segment(aes(y = lm(y ~ 1)$coefficient, \n                   yend = lm(y ~ 1)$coefficient, \n                   x = -0.2, xend = 0.2)) + \n  labs(title = \"Student's\\n t-Test \\n using lm\")\n\n# Wilcoxon test, using signed-ranks of data\np3 &lt;- ggplot(mydata_wide, aes( x = 0, y = signed_rank(y))) +\n  geom_point(alpha = 0.4) +\n  geom_segment(aes(y = mean(signed_rank(y)), yend = mean(signed_rank(y)), x = -0.2, xend = 0.2)) + \n  labs(title = \"Wilcoxon \\nSigned-Rank\\n Test\")\n\n# Wilcoxon test, using signed-ranks of data, and lm\np4 &lt;- ggplot(mydata_wide, aes( x = 0, y = signed_rank(y))) +\n  geom_point(alpha = 0.4) +\n  geom_segment(aes(y = lm(signed_rank(y) ~1)$coefficient, \n                   yend = lm(signed_rank(y) ~1)$coefficient, \n                   x = -0.2, xend = 0.2)) + \n  labs(title = \"Wilcoxon \\n Signed-Rank \\n Test with lm\")\n\n\npatchwork::wrap_plots(p1,p2,p3,p4, nrow = 1, guides = \"collect\")"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html#iconify-pajamas-issue-type-test-case-case-study-2-exam-data",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html#iconify-pajamas-issue-type-test-case-case-study-2-exam-data",
    "title": "🃏 Inference for a Single Mean",
    "section": "\n Case Study #2: Exam data",
    "text": "Case Study #2: Exam data\nLet us now choose a dataset from the openintro package:\n\ndata(\"exam_grades\")\nexam_grades\n\n\n\n  \n\n\n\n\n Inspecting and Charting Data\nTesting Assumptions in the Data\nInference\n\n\nt.test\nWilcoxon test\nLinear Model\nUsing Bootstrap\nPlots"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html#iconify-fluent-mdl2-decision-solid-conclusion",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html#iconify-fluent-mdl2-decision-solid-conclusion",
    "title": "🃏 Inference for a Single Mean",
    "section": "\n Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html#iconify-ooui-references-ltr-references",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html#iconify-ooui-references-ltr-references",
    "title": "🃏 Inference for a Single Mean",
    "section": "{{ < iconify ooui references-ltr >}} References",
    "text": "{{ &lt; iconify ooui references-ltr &gt;}} References\n\nOpenIntro Modern Statistics: Chapter 17"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html#footnotes",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html#footnotes",
    "title": "🃏 Inference for a Single Mean",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://allendowney.blogspot.com/2016/06/there-is-still-only-one-test.html↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/10-Intro/intro-inference.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "href": "content/courses/Analytics/Inference/Modules/10-Intro/intro-inference.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "title": "🧭 Basics of Statistical Inference",
    "section": " Introduction",
    "text": "Introduction\nIn this set of modules we will explore Data, understand what types of data variables there are, and the kinds of statistical tests and visualizations we can create with them."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/10-Intro/intro-inference.html#the-big-ideas-in-stats",
    "href": "content/courses/Analytics/Inference/Modules/10-Intro/intro-inference.html#the-big-ideas-in-stats",
    "title": "🧭 Basics of Statistical Inference",
    "section": "The Big Ideas in Stats",
    "text": "The Big Ideas in Stats\nSteven Stigler(Stigler 2016) is the author of the book “The Seven Pillars of Statistical Wisdom”. The Big Ideas in Statistics from that book are:\n\nAggregation\n\nThe first pillar I will call Aggregation, although it could just as well be given the nineteenth-century name, “The Combination of Observations,” or even reduced to the simplest example, taking a mean. Those simple names are misleading, in that I refer to an idea that is now old but was truly revolutionary in an earlier day—and it still is so today, whenever it reaches into a new area of application. How is it revolutionary? By stipulating that, given a number of observations, you can actually gain information by throwing information away! In taking a simple arithmetic mean, we discard the individuality of the measures, subsuming them to one summary.\n\n\n\nInformation\n\nIn the early eighteenth century it was discovered that in many situations the amount of information in a set of data was only proportional to the square root of the number n of observations, not the number n itself.\n\n\n\nLikelihood\n\nBy the name I give to the third pillar, Likelihood, I mean the calibration of inferences with the use of probability. The simplest form for this is in significance testing and the common P-value.\n\n\n\nIntercomparison\n\nIt represents what was also once a radical idea and is now commonplace: that statistical comparisons do not need to be made with respect to an exterior standard but can often be made in terms interior to the data themselves. The most commonly encountered examples of intercomparisons are Student’s t-tests and the tests of the analysis of variance.\n\n\n\nRegression\n\nI call the fifth pillar Regression, after Galton’s revelation of 1885, explained in terms of the bivariate normal distribution. Galton arrived at this by attempting to devise a mathematical framework for Charles Darwin’s theory of natural selection, overcoming what appeared to Galton to be an intrinsic contradiction in the theory: selection required increasing diversity, in contradiction to the appearance of the population stability needed for the definition of species.\n\n\n\nDesign of Experiments and Observations\n\nThe sixth pillar is Design, as in “Design of Experiments,” but conceived of more broadly, as an ideal that can discipline our thinking in even observational settings.Starting in the late nineteenth century, a new understanding of the topic appeared, as Charles S. Peirce and then Fisher discovered the extraordinary role randomization could play in inference.\n\n\n\nResiduals\n\nThe most common appearances in Statistics are our model diagnostics (plotting residuals), but more important is the way we explore high-dimensional spaces by fitting and comparing nested models.\n\nIn our work with Statistical Models, we will be working with all except Idea 6 above."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/10-Intro/intro-inference.html#what-is-a-statistical-model",
    "href": "content/courses/Analytics/Inference/Modules/10-Intro/intro-inference.html#what-is-a-statistical-model",
    "title": "🧭 Basics of Statistical Inference",
    "section": "What is a Statistical Model?",
    "text": "What is a Statistical Model?\nFrom Daniel Kaplan’s book:\n\n“Modeling” is a process of asking questions. “Statistical” refers in part to data – the statistical models you will construct will be rooted in data. But it refers also to a distinctively modern idea: that you can measure what you don’t know and that doing so contributes to your understanding.\n\nThe conclusions you reach from data depend on the specific questions you ask. The word “modeling” highlights that your goals, your beliefs, and your current state of knowledge all influence your analysis of data. You examine your data to see whether they are consistent with the hypotheses that frame your understanding of the system under study."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/10-Intro/intro-inference.html#types-of-statistical-models-based-on-purpose",
    "href": "content/courses/Analytics/Inference/Modules/10-Intro/intro-inference.html#types-of-statistical-models-based-on-purpose",
    "title": "🧭 Basics of Statistical Inference",
    "section": "Types of Statistical Models Based on Purpose",
    "text": "Types of Statistical Models Based on Purpose\nThere are three main uses for statistical models. They are closely related, but distinct enough to be worth enumerating.\nDescription. Sometimes you want to describe the range or typical values of a quantity. For example, what’s a “normal” white blood cell count? Sometimes you want to describe the relationship between things. Example: What’s the relationship between the price of gasoline and consumption by automobiles?\nClassification or Prediction. You often have information about some observable traits, qualities, or attributes of a system you observe and want to draw conclusions about other things that you can’t directly observe. For instance, you know a patient’s white blood-cell count and other laboratory measurements and want to diagnose the patient’s illness.\nAnticipating the consequences of interventions. Here, you intend to do something: you are not merely an observer but an active participant in the system. For example, people involved in setting or debating public policy have to deal with questions like these: To what extent will increasing the tax on gasoline reduce consumption? To what extent will paying teachers more increase student performance?\nThe appropriate form of a model depends on the purpose. For example, a model that diagnoses a patient as ill based on an observation of a high number of white blood cells can be sensible and useful. But that same model could give absurd predictions about intervention: Do you really think that lowering the white blood cell count by bleeding a patient will make the patient better?\nTo anticipate correctly the effects of an intervention you need to get the direction of cause (polarity) and effect (magnitude) correct in your models.\n\n\n\n\n\n\nNote\n\n\n\nAn effect size tells how the output of a model changes when a simple change is made to the input.Effect sizes always involve two variables: a response variable and a single explanatory variable. Effect size is always about a model. The model might have one explanatory variable or many explanatory variables. Each explanatory variable will have its own effect size, so a model with multiple explanatory variables will have multiple effect sizes.\n\n\n\n\n\n\n\n\nNote\n\n\n\nBut for a model used for classification or prediction, it may be unnecessary to represent causation correctly. Instead, other issues, e.g., the reliability of data, can be the most important. One of the thorniest issues in statistical modeling – with tremendous consequences for science, medicine, government, and commerce – is how you can legitimately draw conclusions about interventions from models based on data collected without performing these interventions.\n\n\n\nTypes of Models Based on Data Variables\nLet us look at the famous dataset pertaining to Francis Galton’s work on the heights of children and the heights of their parents. We can create 4 kinds of models based on the types of variables in that dataset.\n\n\n\nVariables and Models"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/10-Intro/intro-inference.html#linear-models-everywhere",
    "href": "content/courses/Analytics/Inference/Modules/10-Intro/intro-inference.html#linear-models-everywhere",
    "title": "🧭 Basics of Statistical Inference",
    "section": "Linear Models Everywhere",
    "text": "Linear Models Everywhere\nOur method in this set of modules is to take the modern view that all these models can be viewed from a standpoint of the Linear Model, also called Linear Regression \\(y = \\beta_1 *x + \\beta_0\\) . For example, it is relatively straightforward to imagine Plot B (Quant vs Quant ) as an example of a Linear Model, with the dependent variable modelled as \\(y\\) and the independent one as \\(x\\). We will try to work up to the intuition that this model can be used to understand all the models in the Figure."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/10-Intro/intro-inference.html#a-flowchart-of-statistical-inference-tests",
    "href": "content/courses/Analytics/Inference/Modules/10-Intro/intro-inference.html#a-flowchart-of-statistical-inference-tests",
    "title": "🧭 Basics of Statistical Inference",
    "section": "A Flowchart of Statistical Inference Tests",
    "text": "A Flowchart of Statistical Inference Tests\n\n\n\n\nflowchart TD\n    A[Inference for Means] --&gt;|Check Assumptions|B[Normality: Shapiro-Wilk Test shapiro.test\\n Variances: Fisher F-test var.test\\n Outliers: Box Plots]\n    B --&gt; M[Means]\n\n subgraph Means\n    direction TB\n      subgraph Single-Mean\n        direction LR\n        OM[Single Mean]&lt;--&gt;|p| TT[t.test]\n        TWT[t.test \\n Welch]&lt;--&gt;|p diff var|OM[Single Mean]\n        WT[wilcox.test]&lt;--&gt;|np|OM[Single Mean]\n      end\n      \n      subgraph Paired-Means\n        direction LR\n        TM[Paired Means]&lt;--&gt;|p| TTP[t.test with pairs]\n        WTP[wilcox.test with pairs\\n Mann-Whitney U Test]--&gt;|np|TM\n      end\n      \n      subgraph Multiple-Means\n        direction LR\n        MM[Multiple Means] --&gt;|p| ANO[ANOVA]\n        KW[kruskal.test]&lt;--&gt;|np indep| MM\n        FT[friedman.test]&lt;--&gt;|np dep| MM\n      end\n      \nM --&gt;Single-Mean\nSingle-Mean--&gt;Paired-Means\nPaired-Means--&gt;Multiple-Means\n\nend\n\n%%subgraph LM\n%%  direction BT\n%%  LM[Linear Model]--&gt;Means\n%%end"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/10-Intro/intro-inference.html#references",
    "href": "content/courses/Analytics/Inference/Modules/10-Intro/intro-inference.html#references",
    "title": "🧭 Basics of Statistical Inference",
    "section": "References",
    "text": "References\n\nTihamér von Ghyczy, The Fruitful Flaws of Strategy Metaphors. Harvard Business Review, 2003. https://hbr.org/2003/09/the-fruitful-flaws-of-strategy-metaphors\nDaniel T. Kaplan, Statistical Models (second edition). Available online. https://dtkaplan.github.io/SM2-bookdown/\nDaniel T. Kaplan, Compact Introduction to Classical Inference, 2020. Available Online. https://dtkaplan.github.io/CompactInference/\nDaniel T. Kaplan and Frank Shaw, Statistical Modeling: Computational Technique. Available online https://www.mosaic-web.org/go/SM2-technique/\nJonas Kristoffer Lindeløv, Common statistical tests are linear models (or: how to teach stats) https://lindeloev.github.io/tests-as-linear/"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/90-Space/spatial.html",
    "href": "content/courses/Analytics/Descriptive/Modules/90-Space/spatial.html",
    "title": "🗺 Visualising Spatial Data",
    "section": "",
    "text": "Intro to Spatial Data \n R Tutorial\n\nStatic Maps \n Interactive Maps"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/90-Space/spatial.html#fa-folder-open-slides-and-tutorials",
    "href": "content/courses/Analytics/Descriptive/Modules/90-Space/spatial.html#fa-folder-open-slides-and-tutorials",
    "title": "🗺 Visualising Spatial Data",
    "section": "",
    "text": "Intro to Spatial Data \n R Tutorial\n\nStatic Maps \n Interactive Maps"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/90-Space/spatial.html#iconify-noto-v1-package-setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/90-Space/spatial.html#iconify-noto-v1-package-setting-up-r-packages",
    "title": "🗺 Visualising Spatial Data",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nknitr::opts_chunk$set(echo = TRUE)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(osmdata)\nlibrary(osmplotr)\nlibrary(leaflet)\nlibrary(mapview)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/90-Space/spatial.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/90-Space/spatial.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "title": "🗺 Visualising Spatial Data",
    "section": "\n Introduction",
    "text": "Introduction\nFirst; let us watch a short, noisy video on maps:"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/90-Space/spatial.html#iconify-fxemoji-japanesesymbolforbeginner-what-kind-of-visualizations-will-we-make",
    "href": "content/courses/Analytics/Descriptive/Modules/90-Space/spatial.html#iconify-fxemoji-japanesesymbolforbeginner-what-kind-of-visualizations-will-we-make",
    "title": "🗺 Visualising Spatial Data",
    "section": "\n What kind of visualizations will we make?",
    "text": "What kind of visualizations will we make?\nWe will first understand the structure of spatial data and where to find it. For now, we will deal with vector spatial data; the discussion on raster data will be dealt with in another future module.\nWe will get hands-on with making maps, both static and interactive.\n\n Choropleth Map\nWhat does choropleth1 mean? And what kind of information could this map represent?\n\n\n\n Bubble Map\nWhat information could this map below represent?\n\n\nLet us now look at the slides. Then we will understand how the R packages sf, tmap work to create maps, using data downloadable into R using osmdata and osmplotr. We will also make interactive maps with leaflet and mapview; tmap is also capable of creating interactive maps."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/90-Space/spatial.html#iconify-icon-park-me-your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/90-Space/spatial.html#iconify-icon-park-me-your-turn",
    "title": "🗺 Visualising Spatial Data",
    "section": "\n Your Turn",
    "text": "Your Turn\n\n Animal and Bird Migration\n\nHead off to movebank.org. Look at a few species of interest and choose one.\nDownload the data ( ESRI Shapefile). Note: You will get a .zip file with a good many files in it. Save all of them, but read only the .shp file into R and Orange.\nImport that into R using sf_read()\n\nSee how you can plot locations, tracks and colour by species….based on the data you download.\nFor tutorial info: https://movebankworkshopraleighnc.netlify.app/\n\n\n UFO Sightings\nHere is a UFO Sighting dataset, containing location and text descriptions. https://github.com/planetsig/ufo-reports/blob/master/csv-data/ufo-scrubbed-geocoded-time-standardized.csv\n\n Sales Data from kaggle\nHead off to Kaggle and search for Geographical Sales related data. Make both static and interactive maps with this data. Justify your decisions for type of map."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/90-Space/spatial.html#iconify-ooui-references-ltr-references",
    "href": "content/courses/Analytics/Descriptive/Modules/90-Space/spatial.html#iconify-ooui-references-ltr-references",
    "title": "🗺 Visualising Spatial Data",
    "section": "\n References",
    "text": "References\n\nHadley Wickham, Danielle Navarro, and Thomas Lin Pedersen, ggplot2: Elegant Graphics for Data Analysis https://ggplot2-book.org/maps.html\n“Geocomputation with R” by Robin Lovelace, Jakub Nowosad, Jannes Muenchow. Available Online.\nEmine Fidan, Guide to Creating Interactive Maps in R\nNikita Voevodin, R, Not the Best Practices"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/90-Space/spatial.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/90-Space/spatial.html#footnotes",
    "title": "🗺 Visualising Spatial Data",
    "section": "Footnotes",
    "text": "Footnotes\n\nEtymology. From Ancient Greek χώρα (khṓra, “location”) + πλῆθος (plêthos, “a great number”) + English map. First proposed in 1938 by American geographer John Kirtland Wright to mean “quantity in area,” although maps of the type have been used since the early 19th century.↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/files/evolutions.html",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/files/evolutions.html",
    "title": "Tutorial on Evolutions and Flow",
    "section": "",
    "text": "Tutorial Content to be written up when Arvind has time !!!\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/part-whole.html",
    "href": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/part-whole.html",
    "title": "🍕 Parts of a Whole",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(ggformula)\nlibrary(plotrix) # Fan, Pyramid Chart\nlibrary(ggparliament) # Parliament Chart\nlibrary(ggpol) # Parliament, Arc-Bar and other interesting charts\n\n# library(waffle) # What it says! See note below: need github version\n# install.packages(\"waffle\", repos = \"https://cinc.rud.is\")\n\nlibrary(tidygraph) # Trees, Dendros, and Circle Packings\nlibrary(ggraph) # Trees, Dendros, and Circle Packings\nlibrary(echarts4r) # Interactive Charts\n\nlibrary(patchwork) # Arrange your plots"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/part-whole.html#iconify-noto-v1-package-setting-up-the-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/part-whole.html#iconify-noto-v1-package-setting-up-the-packages",
    "title": "🍕 Parts of a Whole",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(ggformula)\nlibrary(plotrix) # Fan, Pyramid Chart\nlibrary(ggparliament) # Parliament Chart\nlibrary(ggpol) # Parliament, Arc-Bar and other interesting charts\n\n# library(waffle) # What it says! See note below: need github version\n# install.packages(\"waffle\", repos = \"https://cinc.rud.is\")\n\nlibrary(tidygraph) # Trees, Dendros, and Circle Packings\nlibrary(ggraph) # Trees, Dendros, and Circle Packings\nlibrary(echarts4r) # Interactive Charts\n\nlibrary(patchwork) # Arrange your plots"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/part-whole.html#iconify-fxemoji-japanesesymbolforbeginner-what-graphs-will-we-see-today",
    "href": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/part-whole.html#iconify-fxemoji-japanesesymbolforbeginner-what-graphs-will-we-see-today",
    "title": "🍕 Parts of a Whole",
    "section": "\n What Graphs will we see today?",
    "text": "What Graphs will we see today?\nThere are a good few charts available to depict things that constitute other bigger things. We will discuss a few of these: Pie, Fan, and Donuts; Waffle and Parliament charts; Trees, Dendrograms, and Circle Packings. (The last three visuals we will explore along with network diagrams in a later module.)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/part-whole.html#iconify-noto-folding-hand-fan-pies-and-fans",
    "href": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/part-whole.html#iconify-noto-folding-hand-fan-pies-and-fans",
    "title": "🍕 Parts of a Whole",
    "section": "\n Pies and Fans",
    "text": "Pies and Fans\nSo let us start with “eating humble pie”: discussing a Pie chart first.\nA pie chart is a circle divided into sectors that each represent a proportion of the whole. It is often used to show percentage, where the sum of the sectors equals 100%.\nThe problem is that humans are pretty bad at reading angles. This ubiquitous chart is much vilified in the industry and bar charts that we have seen earlier, are viewed as better options. On the other hand, pie charts are ubiquitous in business circles, and are very much accepted! Do also read this spirited defense of pie charts here. https://speakingppt.com/why-tufte-is-flat-out-wrong-about-pie-charts/\nAnd we will also see that there is an attractive, and similar-looking alternative, called a fan chart which we will explore here.\n\n\nUsing Base R\nUsing echarts4r\n\n\n\nBase R has a simple pie command that does the job. Let’s create some toy data first:\n\npie_data &lt;- tibble(\n  sales = c(0.12, 0.3, 0.26, 0.16, 0.04, 0.12), \n  \n  # Labels MUST be character entries for `pie` to work\n  labels = c(\"Blueberry\",\"Cherry\",\"Apple\",\"Boston Cream\",\n             \"Other\",\"Vanilla Cream\")\n  )\npie_data\npie(\n  x = pie_data$sales,\n  labels = pie_data$labels, # Character Vector is a MUST\n\n  # Pie is within a square of 1 X 1 units\n  # Reduce radius if needed to see labels properly\n  radius = 0.95,\n  \n  init.angle = 90, # First slice starts at 12 o'clock position\n  \n  # Change the default colours. Comment this and see what happens. \n  col =  grDevices::hcl.colors(palette = \"Plasma\", n = 6)\n  )\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nHere is a basic interactive pie chart withecharts4r:\n\npie_data &lt;- tibble(\n  sales = c(0.12, 0.3, 0.26, 0.16, 0.04, 0.12), \n\n  labels = c(\"Blueberry\",\"Cherry\",\"Apple\",\"Boston Cream\",\"Other\",\n             \"Vanilla Cream\"))\npie_data %&gt;% \n  e_charts(x = labels) %&gt;% \n  e_pie(serie = sales, clockwise = TRUE, \n        startAngle = 90) %&gt;% \n  e_legend(list(orient = \"vertical\",\n                      left = \"right\")) %&gt;% \n  e_tooltip()\n\n\n\n\n\nWe can add more bells and whistles to the humble-pie chart, and make a Nightingale rosechart out of it:\n\npie_data &lt;- tibble(\n  sales = c(0.12, 0.3, 0.26, 0.16, 0.04, 0.12), \n\n  labels = c(\"Blueberry\",\"Cherry\",\"Apple\",\"Boston Cream\",\"Other\",\n             \"Vanilla Cream\"))\npie_data %&gt;% \n  e_charts(x = labels) %&gt;% \n  e_pie(serie = sales, clockwise = TRUE, \n        startAngle = 90, \n        roseType = \"area\") %&gt;% # try \"radius\"\n  \n  # Lets move the legend\n  e_legend(left = \"right\", orient = \"vertical\") %&gt;% \n  e_tooltip()\npie_data %&gt;% \n  e_charts(x = labels) %&gt;% \n  e_pie(serie = sales, clockwise = TRUE, \n        startAngle = 90, \n        roseType = \"radius\") %&gt;% \n  \n  # Lets move the legend\n  e_legend(left = \"right\", orient = \"vertical\") %&gt;% \n  e_tooltip()\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor more information and customization look at https://echarts.apache.org/en/option.html#series-pie\n\n\n\nThe fan Plot\nThe fan plot (from the plotrix package) displays numerical values as arcs of overlapping sectors. This allows for more effective comparison:\n\nplotrix::fan.plot(\n  x = pie_data$sales,\n  labels = pie_data$labels,\n  \n  col = grDevices::hcl.colors(palette = \"Plasma\", n = 6),\n  shrink = 0.03,\n  # How much to shrink each successive sector\n\n  label.radius = 1.15,\n  main = \"Fan Plot of Ice Cream Flavours\",\n  # ticks = 360,\n  # if we want tick marks on the circumference\n  \n  max.span = pi\n)\n\n\n\n\nThere is no fan plot possible with echarts4r, as far as I know.\nThe Donut Chart\nThe donut chart suffers from the same defects as the pie, so should be used with discretion. The donut chart is essentially a gf_rect from ggformula, plotted on a polar coordinate set of of axes:\n\n\nUsing ggformula\nUsing echarts4r\n\n\n\nLet us make some toy data:\n\n# Data\ndf &lt;- tibble(group = LETTERS[1:3],\n                 value = c(25, 20, 35))\n\ndf &lt;-\n  df %&gt;% \n  dplyr::mutate(fraction = value / sum(value), # percentages\n    ymax = cumsum(fraction), # cumulative percentages\n    ymin = lag(ymax, 1, default = 0),\n    # bottom edge of each\n    label = paste0(group, \"\\n value: \", value),\n    labelPosition = (ymax + ymin) / 2 # labels midway on arcs\n  )\n\ndf\ndf %&gt;%  \n  # gf_rect() formula: ymin + ymax ~ xmin + xmax\n  # Bars with varying thickness (y) proportional to data\n  # Fixed length x (2 to 4)\n  gf_rect(ymin + ymax ~ 2 + 4,\n          fill = ~ group, colour = \"black\") %&gt;%\n  \n  gf_label(labelPosition ~ 3.5, \n           label = ~ label,\n           size = 4) %&gt;%\n\n# When switching to polar coords:\n# x maps to radius\n# y maps to angle theta\n# so we create a \"hole\" in the radius, in x \n  gf_refine(coord_polar(theta = \"y\", \n                        direction = 1)) %&gt;% \n            # Up to here will give us a pie chart\n  \n  # Now to create the hole\n  # try to play with the \"0\"\n  # Recall x = [2,4]\n  gf_refine(xlim(c(-2, 5))) %&gt;% \n\n  \n  gf_theme(theme = theme_void()) %&gt;% \n  gf_theme(legend.position = \"none\")\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nThe donut chart is simply a variant of the pie chart in echarts4r:\n\ndf &lt;- tibble(group = LETTERS[1:3],\n                 value = c(25, 20, 35))\n\ndf &lt;-\n  df %&gt;% \n  dplyr::mutate(fraction = value / sum(value), # percentages\n    ymax = cumsum(fraction), # cumulative percentages\n    ymin = lag(ymax, 1, default = 0),\n    # bottom edge of each\n    label = paste0(group, \"\\n value: \", value),\n    labelPosition = (ymax + ymin) / 2 # labels midway on arcs\n  )\ndf\ndf %&gt;% \n  e_charts(x = group, width = 400) %&gt;% \n  e_pie(serie = value, \n        clockwise = TRUE, \n        startAngle = 90,\n        \n        radius = c(\"50%\", \"70%\")\n        ) %&gt;% \n  \n  e_legend(left = \"right\", orient = \"vertical\") %&gt;% \n  e_tooltip()"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/part-whole.html#iconify-noto-waffle-waffle-charts",
    "href": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/part-whole.html#iconify-noto-waffle-waffle-charts",
    "title": "🍕 Parts of a Whole",
    "section": "\n Waffle Charts",
    "text": "Waffle Charts\nWaffle charts are often called “square pie charts” !\nHere we will need to step outside of ggformula and get into ggplot itself momentarily. (Always remember that ggformula is a simplified and intuitive method that runs on top of ggplot.) We will use the waffle package.\n\n# install.packages(\"waffle\", repos = \"https://cinc.rud.is\")\nlibrary(waffle)\n\n# Data\ndf &lt;- tibble(group = LETTERS[1:3],\n                 value = c(25, 20, 35))\ndf\n\n\n\n  \n\n\n# Waffle plot\n# Using ggplot, sadly not yet ggformula\nggplot(df, aes(fill = group, values = value)) +\n  geom_waffle(\n    n_rows = 8,\n    size = 0.33,\n    colour = \"white\",\n    na.rm = TRUE\n  ) +\n  scale_fill_manual(\n    name = NULL,\n    values = c(\"#BA182A\", \"#FF8288\", \"#FFDBDD\"),\n    labels = c(\"A\", \"B\", \"C\")\n  ) +\n  coord_equal() +\n  theme_void()"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/part-whole.html#iconify-fa6-solid-people-roof-parliament-charts",
    "href": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/part-whole.html#iconify-fa6-solid-people-roof-parliament-charts",
    "title": "🍕 Parts of a Whole",
    "section": "\n Parliament Charts",
    "text": "Parliament Charts\nThe package ggpol offers an interesting visualization in the shape of a array of “seats” in a parliament. (There is also a package called ggparliament which in my opinion is a bit cumbersome, having a two step procedure to convert data into “parliament form” etc. )\n\ndf &lt;- tibble(group = LETTERS[1:3],\n                 value = c(25, 20, 35))\n\n# Parliament Plot\nggplot(df) +\n  ggpol::geom_parliament(aes(seats = value, \n                             fill = group),\n                         r0 = 2, # inner radius\n                         r1 = 4 # Outer radius\n  ) + \n  scale_fill_manual(name = NULL,\n                    values = c(\"#BA182A\", \"#FF8288\", \"#FFDBDD\"),\n                    labels = c(\"A\", \"B\", \"C\")) +\n  coord_equal() +\n  theme_void()"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/part-whole.html#trees-dendrograms-and-circle-packings",
    "href": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/part-whole.html#trees-dendrograms-and-circle-packings",
    "title": "🍕 Parts of a Whole",
    "section": "Trees, Dendrograms, and Circle Packings",
    "text": "Trees, Dendrograms, and Circle Packings\nThere are still more esoteric plots to explore, if you are hell-bent on startling people ! There is an R package called ggraph, that can do these charts, and many more:\n\nggraph is an extension of ggplot2 aimed at supporting relational data structures such as networks, graphs, and trees. While it builds upon the foundation of ggplot2 and its API it comes with its own self-contained set of geoms, facets, etc., as well as adding the concept of layouts to the grammar.\n\nWe will explore these charts when we examine network diagrams. For now, we can quickly see what these diagrams look like. Although the R-code is visible to you, it may not make sense at the moment!\n\n Dendrograms\nFrom the R Graph Gallery Website :\n\nDendrograms can be built from:\n\nHierarchical dataset: think about a CEO managing team leads managing employees and so on.\nClustering result: clustering divides a set of individuals in group according to their similarity. Its result can be visualized as a tree.\n\n\n\n# create an edge list data frame giving the hierarchical structure of your individuals\nd1 &lt;- tibble(from = \"origin\", to = paste(\"group\", seq(1,5), sep = \"\"))\nd2 &lt;- tibble(from = rep(d1$to, each=5), to = paste(\"subgroup\", seq(1,25), sep=\"_\"))\nedges &lt;- rbind(d1, d2)\nedges\n\n\n\n  \n\n\n# Create a graph object \nmygraph1 &lt;- tidygraph::as_tbl_graph( edges )\n \n# Basic tree\np1 &lt;- ggraph(mygraph1, layout = 'dendrogram', circular = TRUE) + \n  geom_edge_diagonal() +\n  geom_node_point() +\n  theme_void()\n\n\n# create a data frame \ndata &lt;- tibble(\n  level1=\"CEO\",\n  level2=c( rep(\"boss1\",4), rep(\"boss2\",4)),\n  level3=paste0(\"mister_\", letters[1:8])\n)\n \n# transform it to a edge list!\nedges_level1_2 &lt;- data %&gt;% \n  select(level1, level2) %&gt;% unique %&gt;% rename(from=level1, to=level2)\n\nedges_level2_3 &lt;- data %&gt;% \n  select(level2, level3) %&gt;% unique %&gt;% rename(from=level2, to=level3)\n\nedge_list &lt;- rbind(edges_level1_2, edges_level2_3)\nedge_list\n\n\n\n  \n\n\n# Now we can plot that\nmygraph2 &lt;- as_tbl_graph(edge_list)\np2 &lt;- ggraph(mygraph2, layout = 'dendrogram', circular = FALSE) + \n  geom_edge_diagonal() +\n  geom_node_point() +\n  theme_void()\n\n\np1 + p2 + theme(aspect.ratio = 1)\n\n\n\n\nCircle Packing\n\nlibrary(tidygraph)\nlibrary(ggraph)\ngraph &lt;- tbl_graph(flare$vertices, flare$edges)\nset.seed(1)\nggraph(graph, 'circlepack', weight = size) + \n  geom_node_circle(aes(fill = as_factor(depth)), size = 0.25, n = 50) + \n  coord_fixed() +\n  scale_fill_discrete(name = \"Depth\") +\n  theme_void()"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/part-whole.html#iconify-bi-person-up-your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/part-whole.html#iconify-bi-person-up-your-turn",
    "title": "🍕 Parts of a Whole",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nUse the penguins dataset from the palmerpenguins package and plot pies, fans, and donuts as appropriate.\nLook at the whigs and highschool datasets in the package ggraph. Plot Pies, Fans and if you are feeling confident, Trees, Dendrograms, and Circle Packings as appropriate for these."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/part-whole.html#iconify-ooui-references-ltr-references",
    "href": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/part-whole.html#iconify-ooui-references-ltr-references",
    "title": "🍕 Parts of a Whole",
    "section": "\n References",
    "text": "References\n\nhttps://datavizstory.com/a-parliament-diagram-in-r/\nggpolGuide by Frederik Tiedemann, https://erocoar.github.io/ggpol/\nThomas Lin Pedersen, ggraph:A grammar of graphics for relational data, https://ggraph.data-imaginist.com/\nVenn Diagrams in R, Venn diagram in ggplot2 | R CHARTS (r-charts.com)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html",
    "title": "🕔 Time Series",
    "section": "",
    "text": "library(tidyverse)\nlibrary(lubridate)  # Deal with dates\nlibrary(mosaic)\nlibrary(ggformula) # Our Formula based graphing package\nlibrary(timetk) # Tidy Time series analysis and plots\nlibrary(sweep) # New (07/2023) package to bring broom-like features to time series models\n\nlibrary(fpp3) # Robert Hyndman's textbook package, Loads all the core time series packages, see messages\n\n# devtools::install_github(\"FinYang/tsdl\")\nlibrary(tsdl) # Time Series Data Library from Rob Hyndman\n\nlibrary(tsbox) # \"new kid on the block\"\nlibrary(TSstudio) # Each Plots, Decomposition, and Modelling with Time Series\n\nlibrary(gt) # to make tables!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#iconify-noto-v1-package-setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#iconify-noto-v1-package-setting-up-r-packages",
    "title": "🕔 Time Series",
    "section": "",
    "text": "library(tidyverse)\nlibrary(lubridate)  # Deal with dates\nlibrary(mosaic)\nlibrary(ggformula) # Our Formula based graphing package\nlibrary(timetk) # Tidy Time series analysis and plots\nlibrary(sweep) # New (07/2023) package to bring broom-like features to time series models\n\nlibrary(fpp3) # Robert Hyndman's textbook package, Loads all the core time series packages, see messages\n\n# devtools::install_github(\"FinYang/tsdl\")\nlibrary(tsdl) # Time Series Data Library from Rob Hyndman\n\nlibrary(tsbox) # \"new kid on the block\"\nlibrary(TSstudio) # Each Plots, Decomposition, and Modelling with Time Series\n\nlibrary(gt) # to make tables!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "title": "🕔 Time Series",
    "section": "\n Introduction",
    "text": "Introduction\nAny metric that is measured over regular time intervals forms a time series. Analysis of Time Series is commercially important because of industrial need and relevance, especially with respect to Forecasting (Weather data, sports scores, population growth figures, stock prices, demand, sales, supply…). For example, in the graph shown below are the temperatures over time in two US cities:\n\n\nWhat can we do with Time Series? As with other datasets, we have to begin by answering fundamental questions, such as:\n\nWhat are the types of time series?\nHow do we visualize time series?\nHow might we summarize time series to get aggregate numbers, say by week, month, quarter or year?\nHow do we decompose the time series into level, trend, and seasonal components?\nHoe might we make a model of the underlying process that creates these time series?\nHow do we make useful forecasts with the data we have?\n\nWe will first look at the multiple data formats for time series in R. Alongside we will look at the R packages that work with these formats and create graphs and measures using those objects. Then we examine data wrangling of time series, where we look at packages that offer dplyr-like ability to group and summarize timee series using the timevariable. We will finally look at obtaining the components of the time series and try our hand at modelling and forecasting."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#iconify-game-icons-time-synchronization-time-series-data-formats",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#iconify-game-icons-time-synchronization-time-series-data-formats",
    "title": "🕔 Time Series",
    "section": "\n Time Series Data Formats",
    "text": "Time Series Data Formats\nThere are multiple formats for time series data. The ones that we are likely to encounter most are:\n\nThe tibble format: the simplest and most familiar data format is of course the standard tibble/data frame, with or without an explicit time column/variable to indicate that the other variables vary with time. The standard tibble object is used by many packages, e.g. timetk & modeltime\nThe ts format: We may simply have a single series of measurements that are made over time, stored as a numerical vector. The stats::ts() function will convert a numeric vector into an R time series ts object, which is the most basic time series object in R. The base-R ts object is used by established packages forecast and is also supported by newer packages such as tsbox.\nThe modern tsibble format: this is a new modern format for time series analysis. The special tsibble object (“time series tibble”) is used by fable, feasts and others from the tidyverts set of packages.\n\nThere are many other time-oriented data formats too…probably too many,\n\n\nStandards\n\nsuch a tibbletime and TimeSeries objects. For now the best way to deal with these, should you encounter them, is to convert them (Using tsbox) to a tibble or a tsibble and work with these."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#iconify-flat-color-icons-line-chart-time-series-formats-conversion-and-plotting",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#iconify-flat-color-icons-line-chart-time-series-formats-conversion-and-plotting",
    "title": "🕔 Time Series",
    "section": "\n Time Series Formats, Conversion, and Plotting",
    "text": "Time Series Formats, Conversion, and Plotting\nIn this first example, we will use simple ts data first, and then do another with tibble format that we can plot as is. We will then do more after conversion to tsibble format, and then a third example with a ground-up tsibble dataset.\n\n Base-R ts format data\nThere are a few datasets in base R that are in ts format already.\n\nAirPassengers\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1949 112 118 132 129 121 135 148 148 136 119 104 118\n1950 115 126 141 135 125 149 170 170 158 133 114 140\n1951 145 150 178 163 172 178 199 199 184 162 146 166\n1952 171 180 193 181 183 218 230 242 209 191 172 194\n1953 196 196 236 235 229 243 264 272 237 211 180 201\n1954 204 188 235 227 234 264 302 293 259 229 203 229\n1955 242 233 267 269 270 315 364 347 312 274 237 278\n1956 284 277 317 313 318 374 413 405 355 306 271 306\n1957 315 301 356 348 355 422 465 467 404 347 305 336\n1958 340 318 362 348 363 435 491 505 404 359 310 337\n1959 360 342 406 396 420 472 548 559 463 407 362 405\n1960 417 391 419 461 472 535 622 606 508 461 390 432\n\nstr(AirPassengers)\n\n Time-Series [1:144] from 1949 to 1961: 112 118 132 129 121 135 148 148 136 119 ...\n\n\nThis can be easily plotted using base R and other more recent packages:\n\n# Base R\nplot(AirPassengers)\n# tsbox static plot\ntsbox::ts_plot(AirPassengers,ylab = \"Passengers\")\n\n\n\n\n\n\n\n\n\n\n\n\n# TSstudio interactive plot\nTSstudio::ts_plot(AirPassengers,Xtitle = \"Time\", Ytitle = \"Passengers\")\n\n\n\n\n\nOne can see that there is an upward trend and also seasonal variations that also increase over time. This is an example of a multiplicative time series, which we will discuss later.\nLet us take data that is “time oriented” but not in ts format. We use the command ts to convert a numeric vector to ts format: the syntax of ts() is:\nSyntax: objectName &lt;- ts(data, start, end, frequency), where,\n\n\ndata : represents the data vector\n\nstart : represents the first observation in time series\n\nend : represents the last observation in time series\n\nfrequency : represents number of observations per unit time. For example 1=annual, 4=quarterly, 12=monthly, 7=weekly, etc.\n\nWe will pick simple numerical vector data ( i.e. not a time series ) ChickWeight:\n\nChickWeight %&gt;% head()\n\n\n\n  \n\n\n# Filter for Chick #1 and for Diet #1\nChickWeight_ts &lt;- ChickWeight %&gt;% \n  filter(Chick == 1, Diet ==1) %&gt;% \n  select(weight, Time)\n\nChickWeight_ts &lt;- stats::ts(ChickWeight_ts$weight, frequency = 2) \nstr(ChickWeight_ts)\n\n Time-Series [1:12] from 1 to 6.5: 42 51 59 64 76 93 106 125 149 171 ...\n\n\nNow we can plot this in many ways:\n\nplot(ChickWeight_ts) # Using base-R\n# ts_boxable(ChickWeight_ts)\n# Using tsbox\ntsbox::ts_plot(ChickWeight_ts,\n               ylab = \"Weight of Chick #1\")\n\n\n\n\n\n\n\n\n\n\n\n\n# Using TSstudio\nTSstudio::ts_plot(ChickWeight_ts,\n                  Xtitle = \"Time\", \n                  Ytitle = \"Weight of Chick #1\")\n\n\n\n\n\nWe see that the weights of a young chick specimen increases over time.\n\ntibble data\nThe ts data format can handle only one time series. If we want multiple time series, based on say Qualitative variables, we need other data formats. Using the familiar tibble structure opens up new possibilities.\n\nWe can have multiple time series within a tibble (think of numerical time-series data like GDP, Population, Imports, Exports for multiple countries as with the gapminder1data we saw earlier).\n\nIt also allows for data processing with dplyr such as filtering and summarizing.\n\n\n\ngapminder data\n\n\n\n\n  \n\n\n\nLet us read and inspect in the US births data from 2000 to 2014. Download this data by clicking on the icon below, and saving the downloaded file in a sub-folder called data inside your project.\n Download the US Births data \nRead this data in:\n\nbirths_2000_2014 &lt;- read_csv(\"data/US_births_2000-2014_SSA.csv\")\ninspect(births_2000_2014)\n\n\nquantitative variables:  \n           name   class  min   Q1 median    Q3   max         mean          sd\n1          year numeric 2000 2003   2007  2011  2014  2006.999270    4.321085\n2         month numeric    1    4      7    10    12     6.522723    3.449075\n3 date_of_month numeric    1    8     16    23    31    15.730243    8.801151\n4   day_of_week numeric    1    2      4     6     7     3.999817    2.000502\n5        births numeric 5728 8740  12343 13082 16081 11350.068261 2325.821049\n     n missing\n1 5479       0\n2 5479       0\n3 5479       0\n4 5479       0\n5 5479       0\n\nbirths_2000_2014\n\n\n\n  \n\n\n\nThis is just a tibble containing a single data variable births that varies over time. All other variables, although depicting time, are numerical columns. There are no Qualitative variables (yet!).\nPlotting tibble time series\n\n\nUsing ggformula\nUsing tsbox and TSstudio\n\n\n\nWe will now plot this using ggformula. Using the separate year/month/week and day_of_week / day_of_month columns, we can plot births over time, colouring by day_of_week, for example:\n\n# grouping by day_of_week\nbirths_2000_2014 %&gt;% \n  gf_line(births ~ year, \n          group = ~ day_of_week, \n          color = ~ day_of_week) %&gt;% \n  gf_point(title = \"By Day of Week\") %&gt;% \n  gf_theme(scale_colour_distiller(palette = \"Paired\")) %&gt;% \n  gf_theme(theme_classic())\n# Grouping by date_of_month\nbirths_2000_2014 %&gt;% \n  gf_line(births ~ year, \n          group = ~ date_of_month, \n          color = ~ date_of_month) %&gt;% \n  gf_point(title = \"By Date of Month\") %&gt;% \n  gf_theme(scale_colour_distiller(palette = \"Paired\")) %&gt;% \n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\n\n\n\nNot particularly illuminating. This is because the data is daily and we have considerable variation over time, and here we have too much data to visualize. Summaries will help, so we could calculate the the mean births on a month basis in each year and plot that:\n\nbirths_2000_2014_monthly &lt;- births_2000_2014 %&gt;% \n# Convert month to factor/Qual variable!\n# So that we can have discrete colours for each month\n# Using base::factor()\n# Could use forcats::as_factor() also\n  mutate(month = base::factor(month, labels = month.abb)) %&gt;%\n  group_by(year, month) %&gt;% \n  summarise(mean_monthly_births = mean(births, na.rm = TRUE))\nbirths_2000_2014_monthly\nbirths_2000_2014_monthly %&gt;% \n  gf_line(mean_monthly_births ~ year, \n          group = ~ month, \n          colour = ~ month) %&gt;% \n  gf_point(title = \"Summaries of Monthly Births over the years\") %&gt;% \n    # palette for 12 colours\n  gf_theme(scale_colour_brewer(palette = \"Paired\")) %&gt;% \n  gf_theme(theme_classic())\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nNote that month.abb is a built-in dataset containing names of months.\n\n\n\n\n\n\nNote\n\n\n\nThese are graphs for the same month each year: we have a January graph and a February graph and so on. So…average births per month were higher in all months during 2005 to 2007 and have dropped since.\n\n\nWe can do similar graphs using day_of_week as our basis for grouping, instead of month:\n\nbirths_2000_2014_weekly &lt;- births_2000_2014 %&gt;% \n  mutate(day_of_week = base::factor(day_of_week,\n          levels = c(1,2,3,4,5,6,7), \n          labels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"))) %&gt;% \n  group_by(year, day_of_week) %&gt;% \n  summarise(mean_daily_births = mean(births, na.rm = TRUE))\nbirths_2000_2014_weekly\nbirths_2000_2014_weekly %&gt;%   \n  gf_line(mean_daily_births ~ year, \n             group = ~ day_of_week, \n             colour = ~ day_of_week, data = .) %&gt;% \n  gf_point() %&gt;% \n  # palette for 12 colours\n  gf_theme(scale_colour_brewer(palette = \"Paired\")) %&gt;% \n  gf_theme(theme_classic())\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy are fewer babies born on weekends?\n\n\n\nLooks like an interesting story here…there are significantly fewer births on average on Sat and Sun, over the years! Why? Should we watch Grey’s Anatomy ?\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that this is still using just tibble data, without converting it or using it as a time series. So far we are simply treating the year/month/day variables are simple variables and using dplyr to group and summarize. We have not created an explicit time or date variable.\n\n\n\n\nLet us create a time variable in our dataset now:\n\n\ntsbox::ts_plot needs just the date and the births columns to plot with and not be confused by the other numerical columns, so let us create a single date column from these three, but retain them for now.\n\nTSstudio::ts_plot also needs a date column.\n\nSo there are several numerical variables for year, month, and day_of_month, day_of_week, and of course the births on a daily basis.\nWe use the lubridate package from the tidyverse:\n\nbirths_timeseries &lt;- \n  births_2000_2014 %&gt;% \n  mutate(date = lubridate::make_date(year = year,\n                                     month = month,\n                                     day = date_of_month)) %&gt;% \n  select(date, births, year, month,date_of_month, day_of_week)\n\nbirths_timeseries\n\n\n\n  \n\n\n\n\n\n\n\n\n\nExtract from help(tsbox)\n\n\n\nIn data frames, i.e., in a data.frame, a data.table, or a tibble, tsbox stores one or multiple time series in the ‘long’ format. tsbox detects a value, a time column, and zero, one or several id columns. Column detection is done in the following order:\n\nStarting on the right, the first first numeric or integer column is used as value column.\n\nUsing the remaining columns and starting on the right again, the first Date, POSIXct, numeric or character column is used as time column. character strings are parsed by anytime::anytime(). The timestamp, time, indicates the beginning of a period.\n\n\nAll remaining columns are id columns. Each unique combination of id columns points to a (unique) time series.\n\nAlternatively, the time column and the value column to be explicitly named as time and value. If explicit names are used, the column order will be ignored. If columns are detected automatically, a message is returned.\n\n\nPlotting this directly, after selecting the relevant variables, so that they will be auto-detected:\n\nbirths_timeseries %&gt;% \n  select(date, births) %&gt;% \n  tsbox::ts_plot()\n\n[time]: 'date' [value]: 'births' \n\nbirths_timeseries %&gt;% \n  select(date, births) %&gt;% \n  TSstudio::ts_plot()\n\n\n\n\n\n\n\n\n\n\n\n\nQuite messy, as before. We need use the summarised data, as before. We will do this in the next section. We will do this shortly.\n\n\n\n\ntsibble data\nFinally, we have tsibble (“time series tibble”) format data, which contains three main components:\n\nan index variable that defines time;\na set of key variables, usually categorical, that define sets of observations, over time. This allows for each combination of the categorical variables to define a separate time series.\na set of quantitative variables, that represent the quantities that vary over time (i.e index)\n\nHere is Robert Hyndman’s video introducing tsibbles:\n\nThe package tsibbledata contains several ready made tsibble format data.  Let us try PBS, which is a dataset containing Monthly Medicare prescription data in Australia.Run data(package = \"tsibbledata\") in your Console to find out about these.\n\ndata(\"PBS\")\n# inspect(PBS) # does not work since mosaic cannot handle tsibbles\nPBS\n\n\n\n  \n\n\n\nData Description: This is a large-ish dataset:Run PBS in your console\n\n67K observations\n336 combinations of key variables (Concession, Type, ATC1, ATC2) which are categorical, as foreseen.\nData appears to be monthly, as indicated by the 1M.\nthe time index variable is called Month, formatted as yearmonth, a new type of variable introduced in the tsibble package\n\nNote that there are multiple Quantitative variables (Scripts,Cost), each sliced into 336 time-series, a feature which is not supported in the ts format, but is supported in a tsibble. The Qualitative Variables are described below. Type help(\"PBS\") in your Console.\nThe data is dis-aggregated/grouped using four keys:\n- Concession: Concessional scripts are given to pensioners, unemployed, dependents, and other card holders\n- Type: Co-payments are made until an individual’s script expenditure hits a threshold ($290.00 for concession, $1141.80 otherwise). Safety net subsidies are provided to individuals exceeding this amount.\n- ATC1: Anatomical Therapeutic Chemical index (level 1). 15 types\n- ATC2: Anatomical Therapeutic Chemical index (level 2). 84 types, nested inside ATC1.\nLet us simply plot Cost over time:\n\nPBS %&gt;% \n  gf_point(Cost ~ Month, data = .) %&gt;% \n  gf_line(title = \"PBS Costs vs time\") %&gt;% \n  gf_theme(theme_classic())\n\n\n\n\nThis basic plot is quite messy, and it is now time(sic!) for us to look at summaries of the data using dplyr-like verbs."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#iconify-eos-icons-data-mining-time-series-wrangling",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#iconify-eos-icons-data-mining-time-series-wrangling",
    "title": "🕔 Time Series",
    "section": "\n Time-Series Wrangling",
    "text": "Time-Series Wrangling\nWe have now arrived at the need to filter, group, and summarize time-series data. We can do this in two ways, with two packages:\n\n\n\n\n\n\ntsibble has dplyr-like functions\n\n\n\nUsing tsibble data, the tsibble package has specialized filter and group_by functions to do with the index (i.e time) variable and the key variables, such as index_by() and group_by_key().\nFiltering based on Qual variables can be done with dplyr. We can use dplyr functions such as group_by, mutate(), filter(), select() and summarise() to work with tsibble objects.\n\n\n\n\n\n\n\n\ntimetk also has dplyr-like functions!\n\n\n\nUsing tibbles, timetk provides functions such as summarize_by_time, filter_by_time and slidify that are quite powerful. Again, as with tsibble, dplyr can always be used for other variables (i.e non-time).\n\n\nLet us first see how many observations there are for each combo of keys:\n\nPBS %&gt;% \n  count()\n# Grouped Counts\nPBS %&gt;% \n  tsibble::group_by_key(ATC1, ATC2, Concession, Type) %&gt;% \n  dplyr::count()\n# dplyr grouping\nPBS %&gt;% \n  dplyr::group_by(ATC1, ATC2) %&gt;% \n  dplyr::count()\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n\n\n\n\n  \n\n\n\n\n\nWe have 336 combinations of Qualitative variables, each combo containing 204 observations (except some! Take a look!): so let us filter for a few such combinations and plot:\n\n# Costs\nPBS %&gt;% \n  tsibble::group_by_key(ATC1, ATC2, Concession, Type) %&gt;%\n  gf_line(Cost ~ Month, \n          colour = ~ Type, \n          data = .) %&gt;% \n  gf_point(title = \"Costs, per Month\") %&gt;% \n  gf_theme(theme_classic())\n# Scripts\nPBS %&gt;% \n  tsibble::group_by_key(ATC1, ATC2, Concession, Type) %&gt;%\n  gf_line(Scripts ~ Month, \n          colour = ~ Type, \n          data = .) %&gt;% \n  gf_point(title = \"Scripts, per Month\") %&gt;% \n  gf_theme(theme_classic())\n# Costs variable for a specific combo of Qual variables(keys)\nPBS %&gt;% \n  dplyr::filter(Concession == \"General\", \n                      ATC1 == \"A\",\n                      ATC2 == \"A10\") %&gt;% \n  gf_line(Cost ~ Month, \n          colour = ~ Type, \n          data = .) %&gt;% \n  gf_point(title = \"Costs, per Month for General/A/A10 category patients\") %&gt;%\n  gf_theme(theme_classic())\n# Scripts variable for a specific combo of Qual variables(keys)\nPBS %&gt;% \n  dplyr::filter(Concession == \"General\", \n                      ATC1 == \"A\",\n                      ATC2 == \"A10\") %&gt;% \n  gf_line(Scripts ~ Month, \n          colour = ~ Type, \n          data = .) %&gt;% \n  gf_point(title = \"Scripts, per Month for General/A/A10 category patients\") %&gt;% \n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs can be seen, very different time patterns based on the two Types of payment methods, and also with Costs and Scripts. Strongly seasonal for both, with seasonal variation increasing over the years, a clear sign of a multiplicative time series. There is a strong upward trend with both types of subsidies, Safety net and Co-payments. But these trends are somewhat different in magnitude for specific combinations of ATC1 and ATC2 categories.\nWe can use tsibble’s dplyr-like commands to develop summaries by year, quarter, month(original data): Look carefully at the new time variable created each time:\n\n\n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n\nFinally, it may be a good idea to convert some tibble into a tsibble to leverage some of functions that tsibble offers:\n\nbirths_tsibble &lt;- births_2000_2014 %&gt;% \n  mutate(date = lubridate::make_date(year = year,\n                                     month = month,\n                                     day = date_of_month)) %&gt;%\n  # Convert to tsibble\n  tsibble::as_tsibble(index = date) # Time Variable\n\nbirths_tsibble\n\n\n\n  \n\n\n\nThis is DAILY data of course. Let us say we want to group by month and plot mean monthly births as before, but now using tsibble and the index variable:\n\n\ntsibble vs timetk: Basic Plot\ntsibble vs timetk: Grouped Plot 1\ntsibble vs timetk: Grouped Plot 2\n\n\n\n\nbirths_tsibble %&gt;%\n  gf_line(births ~ date, \n          data = ., \n          title = \"Basic tsibble plotted with ggformula\") %&gt;% \n  gf_theme(theme_classic())\n# timetk **can** plot tsibbles. \nbirths_tsibble %&gt;% \n  timetk::plot_time_series(.date_var = date, \n                           .value = births,\n                           .title = \"Tsibble Plotted with timetk\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbirths_tsibble %&gt;% \n  tsibble::index_by(month_index = ~ tsibble::yearmonth(.)) %&gt;% \n  dplyr::summarise(mean_births = mean(births, na.rm = TRUE)) %&gt;% \n  gf_point(mean_births ~ month_index, \n           data = ., \n           title = \"Monthly Aggregate with tsibble\") %&gt;% \n  gf_line() %&gt;% \n  gf_smooth(se = FALSE, method = \"loess\") %&gt;% \n  gf_theme(theme_minimal())\nbirths_timeseries %&gt;% \n  # timetk cannot wrangle tsibbles\n  # timetk needs tibble or data frame\n  timetk::summarise_by_time(.date_var = date, \n                            .by = \"month\", \n                            mean = mean(births)) %&gt;% \n  timetk::plot_time_series(date, mean,\n                           .title = \"Monthly aggregate births with timetk\",\n                           .x_lab = \"year\", \n                           .y_lab = \"Mean Monthly Births\")\n\n\n\n\n\n\n\n\n\n\n\n\nApart from the bump during in 2006-2007, there are also seasonal trends that repeat each year, which we glimpsed earlier.\n\n\n\nbirths_tsibble %&gt;% \n  tsibble::index_by(year_index = ~ lubridate::year(.)) %&gt;% \n  dplyr::summarise(mean_births = mean(births, na.rm = TRUE)) %&gt;%\n  gf_point(mean_births ~ year_index, data = .) %&gt;% \n  gf_line() %&gt;% \n  gf_smooth(se = FALSE, method = \"loess\") %&gt;% \n  gf_theme(theme_minimal())\nbirths_timeseries %&gt;% \n  timetk::summarise_by_time(.date_var = date, \n                            .by = \"year\", \n                            mean = mean(births)) %&gt;% \n  timetk::plot_time_series(date, mean,\n                           .title = \"Yearly aggregate births with timetk\",\n                           .x_lab = \"year\", \n                           .y_lab = \"Mean Yearly Births\")"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#iconify-game-icons-candles-candle-stick-plots",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#iconify-game-icons-candles-candle-stick-plots",
    "title": "🕔 Time Series",
    "section": "\n Candle-Stick Plots",
    "text": "Candle-Stick Plots\nHmm…can we try to plot boxplots over time (Candle-Stick Plots)? Over month / quarter or year?\n\n Monthly Box Plots\n\nbirths_tsibble %&gt;%\n  index_by(month_index = ~ yearmonth(.)) %&gt;% \n  # 15 years\n  # No need to summarise, since we want boxplots per year / month\n  gf_boxplot(births ~ date, \n             group =  ~ month_index, \n             fill = ~ month_index, data = .) %&gt;%  \n  # plot the groups\n  # 180 plots!!\n  gf_theme(theme_minimal())\nbirths_timeseries %&gt;% \n  # timetk::summarise_by_time(.date_var = date, \n  #                           .by = \"month\", \n  #                           mean = mean(births)) %&gt;% \n  timetk::plot_time_series_boxplot(date, births,\n                           .title = \"Monthly births with timetk\",\n                           .x_lab = \"year\", .period = \"month\",\n                           .y_lab = \"Mean Monthly Births\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n Quarterly boxplots\n\nbirths_tsibble %&gt;%\n  index_by(qrtr_index = ~ yearquarter(.)) %&gt;% # 60 quarters over 15 years\n  # No need to summarise, since we want boxplots per year / month\n  gf_boxplot(births ~ date, \n             group = ~ qrtr_index,\n             fill = ~ qrtr_index,\n             data = .) %&gt;%  # 60 plots!!\n  gf_theme(theme_minimal())\n\n\n\nbirths_timeseries %&gt;% \n  timetk::plot_time_series_boxplot(date, births,\n                           .title = \"Quarterly births with timetk\",\n                           .x_lab = \"year\", .period = \"quarter\",\n                           .y_lab = \"Mean Monthly Births\")\n\n\n\n\n\n\n Yearwise boxplots\n\nbirths_tsibble %&gt;% \n  index_by(year_index = ~ lubridate::year(.)) %&gt;% # 15 years, 15 groups\n    # No need to summarise, since we want boxplots per year / month\n\n  gf_boxplot(births ~ date, \n              group = ~ year_index, \n              fill = ~ year_index, \n             data = .) %&gt;%  # plot the groups 15 plots\n  gf_theme(scale_fill_distiller(palette = \"Spectral\")) %&gt;% \n  gf_theme(theme_minimal())\n\n\n\nbirths_timeseries %&gt;% \n  timetk::plot_time_series_boxplot(date, births,\n                           .title = \"Yearly aggregate births with timetk\",\n                           .x_lab = \"year\", .period = \"year\",\n                           .y_lab = \"Births\")\n\n\n\n\n\nAlthough the graphs are very busy, they do reveal seasonality trends at different periods.\n\nHow about a heatmap? We can cook up a categorical variable based on the number of births (low, fine, high) and use that to create a heatmap:\n\nlibrary(ggformula)\n\nbirths_2000_2014 %&gt;%  \n  mutate(birthrate = case_when(births &gt;=10000 ~ \"high\", \n                               births &lt;= 8000 ~ \"low\", \n                               TRUE ~ \"fine\")) %&gt;% \n  \n  gf_tile(data = ., year ~ month, fill = ~ birthrate, color = \"black\") %&gt;%\n  \n  gf_theme(scale_x_time(breaks = 1:12, \n                        labels = c(\"Jan\", \"Feb\", \"Mar\",\"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"))) %&gt;% \n  \n  gf_theme(theme_classic())"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#iconify-fluent-mdl2-decision-solid-conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#iconify-fluent-mdl2-decision-solid-conclusion",
    "title": "🕔 Time Series",
    "section": "\n Conclusion",
    "text": "Conclusion\nWe have seen a good few data formats for time series, and how to work with them and plot them. We have also seen how to decompose time series into periodic and aperiodic components, which can be used to make business decisions.\nIn the Tutorial @sec–slides-and-tutorials, we will explore modelling and forecasting of time series."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#iconify-icon-park-me-your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#iconify-icon-park-me-your-turn",
    "title": "🕔 Time Series",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nChoose some of the datasets in the tsdl and in the tsibbledata packages. Plot basic, filtered and model-based graphs for these and interpret."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#iconify-ooui-references-ltr-references",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#iconify-ooui-references-ltr-references",
    "title": "🕔 Time Series",
    "section": "\n References",
    "text": "References\n\nRobert Hyndman, Forecasting: Principles and Practice (Third Edition). available online\nTime Series Analysis at Our Coding Club"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#iconify-ep-reading-readings",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#iconify-ep-reading-readings",
    "title": "🕔 Time Series",
    "section": "\n Readings",
    "text": "Readings\n\nThe Nuclear Threat—The Shadow Peace, part 1\n11 Ways to Visualize Changes Over Time – A Guide\nWhat is seasonal adjustment and why is it used?\nThe start-at-zero rule\nKeeping one’s appetite after touring the sausage factory\nHow Common is Your Birthday? This Visualization Might Surprise You\nThe Fallen of World War II\nVisualizing Statistical Mix Effects and Simpson’s Paradox\nHow To Fix a Toilet (And Other Things We Couldn’t Do Without Search)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#footnotes",
    "title": "🕔 Time Series",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://www.gapminder.org/data/↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html",
    "title": "🐉 Visualizing Categorical Data",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE)\nlibrary(tidyverse)\nlibrary(mosaic) # Our trusted friend\nlibrary(GGally) \nlibrary(skimr)\nlibrary(vcd) # Michael Friendly's package, Visualizing Categorical Data\nlibrary(vcdExtra) # Categorical Data Sets\nlibrary(ggmosaic) # Mosaic Plots\nlibrary(resampledata) # More datasets\n\nlibrary(sjPlot) # Likert Scale Plots\nlibrary(sjlabelled) # Creating Labelled Data for Likert Plots\n\nlibrary(ggpubr) # Colours, Themes and new geometries in ggplot\nlibrary(ca) # Correspondence Analysis, for use some day\n\n\n## Making Tables\nlibrary(kableExtra) # html styled tables\nlibrary(gt) # Making Cool Tables\nlibrary(patchwork) # To arrange plots on a grid and other things\n\n## Plot Theme\ntheme_set(new = theme_classic())"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#sec-setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#sec-setting-up-r-packages",
    "title": "🐉 Visualizing Categorical Data",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE)\nlibrary(tidyverse)\nlibrary(mosaic) # Our trusted friend\nlibrary(GGally) \nlibrary(skimr)\nlibrary(vcd) # Michael Friendly's package, Visualizing Categorical Data\nlibrary(vcdExtra) # Categorical Data Sets\nlibrary(ggmosaic) # Mosaic Plots\nlibrary(resampledata) # More datasets\n\nlibrary(sjPlot) # Likert Scale Plots\nlibrary(sjlabelled) # Creating Labelled Data for Likert Plots\n\nlibrary(ggpubr) # Colours, Themes and new geometries in ggplot\nlibrary(ca) # Correspondence Analysis, for use some day\n\n\n## Making Tables\nlibrary(kableExtra) # html styled tables\nlibrary(gt) # Making Cool Tables\nlibrary(patchwork) # To arrange plots on a grid and other things\n\n## Plot Theme\ntheme_set(new = theme_classic())"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "title": "🐉 Visualizing Categorical Data",
    "section": "\n Introduction",
    "text": "Introduction\nTo recall, a categorical variable is one for which the possible measured or assigned values consist of a discrete set of categories, which may be ordered or unordered. Some typical examples are:\n\nGender, with categories “Male,” “Female.”\nMarital status, with categories “Never married,” “Married,” “Separated,” “Divorced,” “Widowed.”\nFielding position (in baseball cricket), with categories “Slips,”Cover “,”Mid-off “Deep Fine Leg”, “Close-in”, “Deep”…\nSide effects (in a pharmacological study), with categories “None,” “Skin rash,” “Sleep disorder,” “Anxiety,” . . ..\nPolitical attitude, with categories “Left,” “Center,” “Right.”\nParty preference (in India), with categories “BJP” “Congress,” “AAP,” “TMC”…\nTreatment outcome, with categories “no improvement,” “some improvement,” or “marked improvement.”\nAge, with categories “0–9,” “10–19,” “20–29,” “30–39,” . . . .\nNumber of children, with categories 0, 1, 2, . . . .\n\nAs these examples suggest, categorical variables differ in the number of categories: we often distinguish binary variables (or dichotomous variables) such as Gender from those with more than two categories (called polytomous variables)."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#iconify-mdi-category-plus-outline-categorical-data",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#iconify-mdi-category-plus-outline-categorical-data",
    "title": "🐉 Visualizing Categorical Data",
    "section": "\n Categorical Data",
    "text": "Categorical Data\nFrom the {vcd package} vignette:\n\nThe first thing you need to know is that categorical data can be represented in three different forms in R, and it is sometimes necessary to convert from one form to another, for carrying out statistical tests, fitting models or visualizing the results.\n\n\nCase Data\nFrequency Data\nCross-Tabular Count Data\n\nLet us first see examples of each.\n\n\nCase Form\nFrequency Data Form\nTable form\n\n\n\nFrom Michael Friendly Discrete Data Analysis and Visualization :\n\nIn many circumstances, data is recorded on each individual or experimental unit. Data in this form is called case data, or data in case form. Containing individual observations with one or more categorical factors, used as classifying variables. The total number of observations is nrow(X), and the number of variables is ncol(X).\n\n\nclass(Arthritis)\n# Tibble as HTML for presentation\nArthritis %&gt;%  \n  head(10) %&gt;% \n  kbl(caption = \"Arthritis Treatments and Effects&lt;br&gt; First 10 Observations\",centering = TRUE) %&gt;%\n  kable_classic_2(html_font = \"Cambria\", full_width = F)\n\n\n\n[1] \"data.frame\"\n\n\n\nArthritis Treatments and Effects\nFirst 10 Observations\n\nID\nTreatment\nSex\nAge\nImproved\n\n\n\n57\nTreated\nMale\n27\nSome\n\n\n46\nTreated\nMale\n29\nNone\n\n\n77\nTreated\nMale\n30\nNone\n\n\n17\nTreated\nMale\n32\nMarked\n\n\n36\nTreated\nMale\n46\nMarked\n\n\n23\nTreated\nMale\n58\nMarked\n\n\n75\nTreated\nMale\n59\nNone\n\n\n39\nTreated\nMale\n59\nMarked\n\n\n33\nTreated\nMale\n63\nNone\n\n\n55\nTreated\nMale\n63\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Arthritis data set has three factors and two integer variables. One of the three factors Improved is an ordered factor.\n\nID\nTreatment: a factor; Placebo or Treated\nSex: a factor, M / F\nAge: integer\nImproved: Ordinal factor; None &lt; Some &lt; Marked\n\nEach row in the Arthritis dataset is a separate case or observation.\n\n\nData in frequency form has already been tabulated and aggregated by counting over the (combinations of ) categories of the table variables. When the data are in case form, we can always trace any observation back to its individual identifier or data record, since each row is a unique observation or case; the reverse, with the Frequency Form is rarely possible.\nFrequency Data is usually a data frame, with columns of categorical variables and at least one column containing frequency or count information.\n\nstr(GSS)\nGSS \n# Tibble as HTML for presentation\nGSS %&gt;%\n  kbl(caption = \"General Social Survey\",centering = TRUE) %&gt;%\n  kable_classic_2(html_font = \"Cambria\", full_width = F)\n\n\n\n'data.frame':   6 obs. of  3 variables:\n $ sex  : Factor w/ 2 levels \"female\",\"male\": 1 2 1 2 1 2\n $ party: Factor w/ 3 levels \"dem\",\"indep\",..: 1 1 2 2 3 3\n $ count: num  279 165 73 47 225 191\n\n\n\n\n  \n\n\n\n\nGeneral Social Survey\n\nsex\nparty\ncount\n\n\n\nfemale\ndem\n279\n\n\nmale\ndem\n165\n\n\nfemale\nindep\n73\n\n\nmale\nindep\n47\n\n\nfemale\nrep\n225\n\n\nmale\nrep\n191\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRespondents in the GSS survey were classified by sex and party identification. As can be seen, there is a count for every combination of the two categorical variables, sex and party.\n\n\nTable Form Data can be a matrix, array or table object, whose elements are the frequencies in an n-way table. The variable names (factors) and their levels are given by dimnames(X).\n\nHairEyeColor\n\n, , Sex = Male\n\n       Eye\nHair    Brown Blue Hazel Green\n  Black    32   11    10     3\n  Brown    53   50    25    15\n  Red      10   10     7     7\n  Blond     3   30     5     8\n\n, , Sex = Female\n\n       Eye\nHair    Brown Blue Hazel Green\n  Black    36    9     5     2\n  Brown    66   34    29    14\n  Red      16    7     7     7\n  Blond     4   64     5     8\n\nclass(HairEyeColor)\n\n[1] \"table\"\n\n\nHairEyeColor is a “two-way” table, consisting of two tables, one for Sex = Female and the other for Sex = Male. The total number of observations is sum(X). The number of dimensions of the table is length(dimnames(X)), and the table sizes are given by sapply(dimnames(X), length). The data looks like a n-dimensional cube and needs n-way tables to represent.\n\nsum(HairEyeColor)\n\n[1] 592\n\ndimnames(HairEyeColor)\n\n$Hair\n[1] \"Black\" \"Brown\" \"Red\"   \"Blond\"\n\n$Eye\n[1] \"Brown\" \"Blue\"  \"Hazel\" \"Green\"\n\n$Sex\n[1] \"Male\"   \"Female\"\n\nsapply(dimnames(HairEyeColor), length)\n\nHair  Eye  Sex \n   4    4    2 \n\n\nA good way to think of tabular data is to think of a Rubik’s Cube.\n\n\nRubik’s Cube model for Multi-Table Data\n\n\n\n\n\n\n\nRubik’s Cube and Categorical Data Tables\n\n\n\nEach of the edges is an Ordinal Variable, each segment represents a level in the variable. So each face of the Cube represents two ordinal variables. Any segment is at the intersection of two (independent) levels of two variables, and the colour may be visualized as a count. This array of counts on a face is a 2D or 2-Way Table. ( More on this later )\n\n\nSince we can only print 2D tables, we hold one face in front and the image we see is a 2-Way Table. Turning the Cube by 90 degrees gives us another face with 2 variables, with one variable in common with the previous face. If we consider two faces together, we get two 2-way tables, effectively allowing us to contemplate 3 categorical variables.\nMultiple 2-Way tables can be flattened into a single long table that contains all counts for all combinations of categorical variables. This can be visualized as “opening up” and laying flat the Rubik’s cube, as with a cardboard model of it.\n\nftable(HairEyeColor)\n\n            Sex Male Female\nHair  Eye                  \nBlack Brown       32     36\n      Blue        11      9\n      Hazel       10      5\n      Green        3      2\nBrown Brown       53     66\n      Blue        50     34\n      Hazel       25     29\n      Green       15     14\nRed   Brown       10     16\n      Blue        10      7\n      Hazel        7      7\n      Green        7      7\nBlond Brown        3      4\n      Blue        30     64\n      Hazel        5      5\n      Green        8      8\n\n\nFinally, we may need to convert the (multiple) tables into a data frame or tibble:\n\n## Convert the two tables into a data frame\nHairEyeColor %&gt;% \n  as_tibble() \n# Tibble as HTML for presentation\nHairEyeColor %&gt;% \n  as_tibble() %&gt;%  # Convert\n  kbl(caption = \"Hair Eye and Color\") %&gt;% \n  kable_classic_2(html_font = \"Cambria\", full_width = F)\n\n\n\n\n\n  \n\n\n\n\nHair Eye and Color\n\nHair\nEye\nSex\nn\n\n\n\nBlack\nBrown\nMale\n32\n\n\nBrown\nBrown\nMale\n53\n\n\nRed\nBrown\nMale\n10\n\n\nBlond\nBrown\nMale\n3\n\n\nBlack\nBlue\nMale\n11\n\n\nBrown\nBlue\nMale\n50\n\n\nRed\nBlue\nMale\n10\n\n\nBlond\nBlue\nMale\n30\n\n\nBlack\nHazel\nMale\n10\n\n\nBrown\nHazel\nMale\n25\n\n\nRed\nHazel\nMale\n7\n\n\nBlond\nHazel\nMale\n5\n\n\nBlack\nGreen\nMale\n3\n\n\nBrown\nGreen\nMale\n15\n\n\nRed\nGreen\nMale\n7\n\n\nBlond\nGreen\nMale\n8\n\n\nBlack\nBrown\nFemale\n36\n\n\nBrown\nBrown\nFemale\n66\n\n\nRed\nBrown\nFemale\n16\n\n\nBlond\nBrown\nFemale\n4\n\n\nBlack\nBlue\nFemale\n9\n\n\nBrown\nBlue\nFemale\n34\n\n\nRed\nBlue\nFemale\n7\n\n\nBlond\nBlue\nFemale\n64\n\n\nBlack\nHazel\nFemale\n5\n\n\nBrown\nHazel\nFemale\n29\n\n\nRed\nHazel\nFemale\n7\n\n\nBlond\nHazel\nFemale\n5\n\n\nBlack\nGreen\nFemale\n2\n\n\nBrown\nGreen\nFemale\n14\n\n\nRed\nGreen\nFemale\n7\n\n\nBlond\nGreen\nFemale\n8"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#sec-creating-contingency-tables",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#sec-creating-contingency-tables",
    "title": "🐉 Visualizing Categorical Data",
    "section": "\n Creating Contingency Tables",
    "text": "Creating Contingency Tables\nMost plots for Categorical Data ( as we shall see ) require that the data be converted into a Contingency Table ; even Statistical tests for Proportions ( the \\(\\chi^2\\) test ) need Contingency Tables. The Frequency Table we encountered earlier is very close to being a full-fledged Contingency Table.\nFrom Wolfram Alpha:\n\nA contingency table, sometimes called a two-way frequency table, is a tabular mechanism with at least two rows and two columns used in statistics to present categorical data in terms of frequency counts. More precisely, an \\(r \\times c\\) contingency table shows the observed frequency of two variables the observed frequencies of which are arranged into \\(r\\) rows and \\(c\\) columns. The intersection of a row and a column of a contingency table is called a cell.\n\nIn this section we understand how to make Contingency Tables from each of the three forms. We will use vcd, mosaic and the tidyverse packages for our purposes. Then we will see how it can be visualized.\n\n\nUsing base R\nUsing the vcd package\nUsing the mosaic package\nUsing the tidyverse\n\n\n\n\n# One Way Table ( one variable )\ntable(Arthritis$Treatment) # Contingency Table\n# 1-way Contingency Table\ntable(Arthritis$Treatment) %&gt;% addmargins() # Contingency Table with margins\n# 2-Way Contingency Tables\n\n# Choosing Treatment and Improved\ntable(Arthritis$Treatment, Arthritis$Improved) %&gt;% addmargins() \n# Choosing Treatment and Sex\ntable(Arthritis$Sex, Arthritis$Improved) %&gt;% addmargins()\n\n\n\n\nPlacebo Treated \n     43      41 \n\n\n\nPlacebo Treated     Sum \n     43      41      84 \n\n\n\n\n         \n          None Some Marked Sum\n  Placebo   29    7      7  43\n  Treated   13    7     21  41\n  Sum       42   14     28  84\n\n\n        \n         None Some Marked Sum\n  Female   25   12     22  59\n  Male     17    2      6  25\n  Sum      42   14     28  84\n\n\n\n\nWe can use table() ( and also xtabs() ) to generate multi-dimensional tables too (More than 2-way) These will be printed out as a series of 2D tables, one for each value of the “third” parameter. We can then flatten this set of tables using ftable() and add margins to convert into a Contingency Table:\n\nmytable &lt;- table(Arthritis$Treatment, Arthritis$Sex, Arthritis$Improved)\nmytable\n# Now flatten \nftable(mytable) \nftable(mytable) %&gt;% addmargins()\n\n\n\n, ,  = None\n\n         \n          Female Male\n  Placebo     19   10\n  Treated      6    7\n\n, ,  = Some\n\n         \n          Female Male\n  Placebo      7    0\n  Treated      5    2\n\n, ,  = Marked\n\n         \n          Female Male\n  Placebo      6    1\n  Treated     16    5\n\n\n                None Some Marked\n                                \nPlacebo Female    19    7      6\n        Male      10    0      1\nTreated Female     6    5     16\n        Male       7    2      5\n\n\n             Sum\n    19  7  6  32\n    10  0  1  11\n     6  5 16  27\n     7  2  5  14\nSum 42 14 28  84\n\n\n\n\nA bit strange that the column labels disappear in the ftable when margins are added…\n\n\nThe vcd ( Visualize Categorical Data ) package by Michael Friendly has a convenient function to create Contingency Tables: structable(); this function produces a ‘flat’ representation of a high-dimensional contingency table constructed by recursive splits (similar to the construction of mosaic charts/graphs). structable tends to render flat tables, of the kind that can be thought of as a “text representation” of the vcd::mosaic plot:\nThe arguments of structable are:\n\na formula \\(y + p \\sim x + z\\) which shows which variables are to be included as columns and rows respectively on a table\na data argument, which can indicate a data frame\n\n\n\n# Three Way!!\narth_vcd &lt;- vcd::structable(data = Arthritis, Treatment ~ Improved + Sex)\narth_vcd\nclass(arth_vcd)\n\n\n\n                Treatment Placebo Treated\nImproved Sex                             \nNone     Female                19       6\n         Male                  10       7\nSome     Female                 7       5\n         Male                   0       2\nMarked   Female                 6      16\n         Male                   1       5\n[1] \"structable\" \"ftable\"    \n\n\n\n\n\n# With Margins\narth_vcd %&gt;% as.matrix() %&gt;% addmargins()\n\n# We can convert this to a tibble, unlike the `table` earlier!\narth_vcd %&gt;% as.matrix() %&gt;% addmargins() %&gt;% as_tibble()\n\n\n\n               Treatment\nImproved_Sex    Placebo Treated Sum\n  None_Female        19       6  25\n  None_Male          10       7  17\n  Some_Female         7       5  12\n  Some_Male           0       2   2\n  Marked_Female       6      16  22\n  Marked_Male         1       5   6\n  Sum                43      41  84\n\n\n\n\n  \n\n\n\n\n\n\n# HairEyeColor is in multiple table form\n# structable flattens these into one, as for a mosaic chart\n\nHairEyeColor\nvcd::structable(HairEyeColor)\n\n\n\n, , Sex = Male\n\n       Eye\nHair    Brown Blue Hazel Green\n  Black    32   11    10     3\n  Brown    53   50    25    15\n  Red      10   10     7     7\n  Blond     3   30     5     8\n\n, , Sex = Female\n\n       Eye\nHair    Brown Blue Hazel Green\n  Black    36    9     5     2\n  Brown    66   34    29    14\n  Red      16    7     7     7\n  Blond     4   64     5     8\n\n\n             Eye Brown Blue Hazel Green\nHair  Sex                              \nBlack Male          32   11    10     3\n      Female        36    9     5     2\nBrown Male          53   50    25    15\n      Female        66   34    29    14\nRed   Male          10   10     7     7\n      Female        16    7     7     7\nBlond Male           3   30     5     8\n      Female         4   64     5     8\n\n\n\n\nUCBAdmissions is already in Frequency Form i.e. a Contingency Table. But it is a set of (two-way) Contingency Tables:\n\nUCBAdmissions\nvcd::structable(UCBAdmissions)\nstructable(UCBAdmissions) %&gt;% as.matrix() %&gt;% addmargins()\n\n\n\n, , Dept = A\n\n          Gender\nAdmit      Male Female\n  Admitted  512     89\n  Rejected  313     19\n\n, , Dept = B\n\n          Gender\nAdmit      Male Female\n  Admitted  353     17\n  Rejected  207      8\n\n, , Dept = C\n\n          Gender\nAdmit      Male Female\n  Admitted  120    202\n  Rejected  205    391\n\n, , Dept = D\n\n          Gender\nAdmit      Male Female\n  Admitted  138    131\n  Rejected  279    244\n\n, , Dept = E\n\n          Gender\nAdmit      Male Female\n  Admitted   53     94\n  Rejected  138    299\n\n, , Dept = F\n\n          Gender\nAdmit      Male Female\n  Admitted   22     24\n  Rejected  351    317\n\n\n              Gender Male Female\nAdmit    Dept                   \nAdmitted A            512     89\n         B            353     17\n         C            120    202\n         D            138    131\n         E             53     94\n         F             22     24\nRejected A            313     19\n         B            207      8\n         C            205    391\n         D            279    244\n         E            138    299\n         F            351    317\n\n\n            Gender\nAdmit_Dept   Male Female  Sum\n  Admitted_A  512     89  601\n  Admitted_B  353     17  370\n  Admitted_C  120    202  322\n  Admitted_D  138    131  269\n  Admitted_E   53     94  147\n  Admitted_F   22     24   46\n  Rejected_A  313     19  332\n  Rejected_B  207      8  215\n  Rejected_C  205    391  596\n  Rejected_D  279    244  523\n  Rejected_E  138    299  437\n  Rejected_F  351    317  668\n  Sum        2691   1835 4526\n\n\n\n\nNote that structable does not permit the adding of margins directly; it needs to be converted to a matrix for addmargins() to do its work.\n\n\nI think this is the simplest and most elegant way of obtaining Contingency Tables:\n\n# One Way Table\nmosaicCore::tally( ~ substance, data = HELPrct, margins = TRUE)\n# Two-Way Tables\n# Two ways of producing the same result\ntally( sex ~ substance, data = HELPrct, margins = TRUE)\ntally(~ sex | substance, data = HELPrct, margins = TRUE)\n\n\n\nsubstance\nalcohol cocaine  heroin   Total \n    177     152     124     453 \n\n\n        substance\nsex      alcohol cocaine heroin\n  female      36      41     30\n  male       141     111     94\n  Total      177     152    124\n\n\n        substance\nsex      alcohol cocaine heroin\n  female      36      41     30\n  male       141     111     94\n  Total      177     152    124\n\n\n\n\nSo far these packages give Contingency Tables that are easy to see for humans; some of these structures are also capable being passed directly to commands such as stats::chisq.test() or janitor::chisq.test().\nOften we need Contingency Tables that are in tibble form and we need to perform some data processing using dplyr to get there:\n\n\nDoing this with the tidyverse set of packages may seem counter-intuitively long-winded, but the workflow is easily understandable.\nFirst we develop the counts:\n\ndiamonds %&gt;% count(cut)\ndiamonds %&gt;% count(clarity)\ndiamonds %&gt;% \n  group_by(cut, clarity) %&gt;% \n  dplyr::summarise( count = n())\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n\n\n\n\n  \n\n\n\n\n\nWe need to have the individual levels of cut as rows and the individual levels of clarity as columns. This means that we need to pivot this from “long” to wide” to obtain a Contingency Table:\n\ndiamonds %&gt;% \n  group_by(cut, clarity) %&gt;% \n  dplyr::summarise( count = n()) %&gt;% \n  \n  pivot_wider(id_cols = cut, \n              names_from = clarity, \n              values_from = count) %&gt;% \n  \n  # Now add the row and column totals using the `janitor` package\n  janitor::adorn_totals(where = c(\"row\", \"col\")) %&gt;%\n  \n  # Recover to tibble since janitor gives a \"tabyl\" format ( which can be useful too !)\n  as_tibble()"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#iconify-carbon-chart-3d-plots-for-categorical-data",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#iconify-carbon-chart-3d-plots-for-categorical-data",
    "title": "🐉 Visualizing Categorical Data",
    "section": "\n Plots for Categorical Data",
    "text": "Plots for Categorical Data\nNow that we have Contingency Tables, we can plot these. We have already seen bar plots, which allow us to plot counts of categorical data. These can be used for say 2 or 3 Categorical variables, with not too many levels. However, for more complex data, if there are a large number of categorical variables, or if the categorical variables have many levels, the bar plot is not adequate.\nFrom Michael Friendly:\n\nThe familiar techniques for displaying raw data are often disappointing when applied to categorical data. The simple scatterplot, for example, widely used to show the relation between quantitative response and predictors, when applied to discrete variables, gives a display of the category combinations, with all identical values overplotted, and no representation of their frequency. (AV: Scatter plots do not do counting internally!)\n\n\nInstead, frequencies of categorical variables are often best represented graphically using areas rather than as position along a scale. Using the visual attribute:\n\n\\[\\pmb{area \\sim frequency}\\]\n\nallows creating novel graphical displays of frequency data for special circumstances.\n\nLet us not look at some sample plots that embody this area-frequency principle.\n\n Mosaic Plots\nA mosaic plot is basically an area-proportional visualization of (typically observed) frequencies, consisting of tiles (corresponding to the cells) created by vertically and horizontally splitting a rectangle recursively. Thus, the area of each tile is proportional to the corresponding cell entry given the dimensions of previous splits.\n\n\nUsing vcd\nUsing ggmosaic\nUsing ggformula\n\n\n\nThe vcd::mosaic() function needs the data in contingency table form. We will use our vcd::structable() function to construct one:\n\nart &lt;- vcd::structable(~ Treatment + Improved, data = Arthritis)\nart\nvcd::mosaic(art, gp = shading_max, main = \"Arthritis Treatment Dataset\")\n\n\n\n          Improved None Some Marked\nTreatment                          \nPlacebo              29    7      7\nTreated              13    7     21\n\n\n\n\n\n\n\n\ndata(\"GSS2002\", package = \"resampledata\")\n\ngss2002 &lt;- GSS2002 %&gt;% \n  # select two categorical variables from the dataset\n  dplyr::select(Education, DeathPenalty) %&gt;% \n  drop_na(Education, DeathPenalty)\n\ngss_table &lt;- tally(DeathPenalty ~ Education, data = gss2002)\ngss_table %&gt;% addmargins()\n\n            Education\nDeathPenalty Left HS   HS Jr Col Bachelors Graduate  Sum\n      Favor      117  511     71       135       64  898\n      Oppose      72  200     16        71       50  409\n      Sum        189  711     87       206      114 1307\n\nvcd::mosaic(gss_table, gp = shading_hsv)\n\n\n\n\n\n\nThis is perhaps the simplest way, but does use a different package and also does not use the formula notation: there is no gf_mosaic command yet!\n\n#library(ggmosaic)\n\nggplot(data = gss2002) +\n  geom_mosaic(aes(x = product(DeathPenalty, Education), \n                  fill = DeathPenalty))\n\n\n\n\n\n\nThis needs quite some work, to convert the Contingency Table into a mosaic plot; perhaps not the most intuitive of methods either. This code has been developed using this Stackoverflow post.\n\n# Reference\n# https://stackoverflow.com/questions/19233365/how-to-create-a-marimekko-mosaic-plot-in-ggplot2\n\ngss_summary &lt;- gss2002 %&gt;% \n  dplyr::group_by(Education, DeathPenalty) %&gt;%\n  dplyr::summarise(count = n()) %&gt;% # This is good for a chisq test\n  \n  # Data is still grouped by `Education`\n  # Add two more columns to facilitate mosaic Plot\n  # These two columns are quite unusual...\n  mutate(edu_count = sum(count), \n         edu_prop = count / sum(count)) %&gt;%\n  ungroup()\ngss_summary\n\n\n\n  \n\n\n# This works but is not very intuitive...\ngf_col(edu_prop ~ Education, data = gss_summary,\n       width = ~ edu_count, # Not typically used in a column chart\n       fill = ~ DeathPenalty,\n       stat = \"identity\", \n       position = \"fill\", \n       color = \"black\") %&gt;% \n  \n  gf_text(edu_prop ~ Education, \n          label = ~ scales::percent(edu_prop),\n          position = position_stack(vjust = 0.5)) %&gt;% \n  \n  gf_facet_grid(~ Education, \n                scales = \"free_x\", \n                space = \"free_x\") %&gt;% \n  \n  gf_theme(scale_fill_manual(values = c(\"orangered\", \"palegreen3\"))) %&gt;% \n  gf_theme(theme_void())"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#iconify-fluent-mdl2-balloons-balloon-plots",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#iconify-fluent-mdl2-balloons-balloon-plots",
    "title": "🐉 Visualizing Categorical Data",
    "section": "\n Balloon Plots",
    "text": "Balloon Plots\nThere is another visualization of Categorical Data, called a Balloon Plot. We will use the housetasks dataset from the package ggpubr. This data is already in Contingency Table form (without the margin totals)!\n\nhousetasks &lt;- read.delim(\n  system.file(\"demo-data/housetasks.txt\", \n              package = \"ggpubr\"),\n  row.names = 1\n  )\nhead(housetasks, 4)\nggpubr::ggballoonplot(housetasks, fill = \"value\") +\n  scale_fill_viridis_c(option = \"C\") +\n  labs(title = \"A Balloon Plot for Categorical Data\")\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAnd repeat with the familiar HairEyeColor dataset:\n\ndf &lt;- as_tibble(HairEyeColor)\ndf\nggballoonplot(df, x = \"Hair\", y = \"Eye\", size = \"n\",\n              fill = \"n\",\n              ggtheme = theme_bw()) +\n  scale_fill_viridis_c(option = \"C\") + \n  labs(title = \"Balloon Plot\")\n# Balloon Plot with facetting\nggballoonplot(df, x = \"Hair\", y = \"Eye\", size = \"n\",\n              fill = \"n\", facet.by = \"Sex\",\n              ggtheme = theme_bw()) +\n  scale_fill_viridis_c(option = \"C\") + \n  labs(title = \"Balloon Plot with Facetting\")\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nNote the somewhat different syntax with ggballoonplot: the variable names are enclosed in quotes.\n\n Plots for Likert Data\nIn many business situations, we perform surveys to get Likert Scale data, where several respondents rate a product or a service on a scale of Very much like, somewhat like, neutral, Dislike and Very much dislike, for example. Such data may look for example as follows:\n\ndata(efc)\nhead(efc, 20)\n\n\n\n  \n\n\n\nefc is a German data set from a European study on family care of older people. Following a common protocol, data were collected from national samples of approximately 1,000 family carers (i.e. caregivers) per country and clustered into comparable subgroups to facilitate cross-national analysis. The research questions in this EUROFAM study were:\n\n\nTo what extent do family carers of older people use support services or receive financial allowances across Europe? What kind of supports and allowances do they mainly use?\nWhat are the main difficulties carers experience accessing the services used? What prevents carers from accessing unused supports that they need? What causes them to stop using still-needed services?\nIn order to improve support provision, what can be understood about the service characteristics considered crucial by carers, and how far are these needs met? and,\nWhich channels or actors can provide the greatest help in underpinning future policy efforts to improve access to services/supports?\n\n\nWe will select the variables from the efc data set that related to coping (on part of care-givers) and plot their responses after inspecting them:\n\nefc %&gt;% select(dplyr::contains(\"cop\")) %&gt;% str()\n\n'data.frame':   908 obs. of  9 variables:\n $ c82cop1: num  3 3 2 4 3 2 4 3 3 3 ...\n  ..- attr(*, \"label\")= chr \"do you feel you cope well as caregiver?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"never\" \"sometimes\" \"often\" \"always\"\n $ c83cop2: num  2 3 2 1 2 2 2 2 2 2 ...\n  ..- attr(*, \"label\")= chr \"do you find caregiving too demanding?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"Never\" \"Sometimes\" \"Often\" \"Always\"\n $ c84cop3: num  2 3 1 3 1 3 4 2 3 1 ...\n  ..- attr(*, \"label\")= chr \"does caregiving cause difficulties in your relationship with your friends?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"Never\" \"Sometimes\" \"Often\" \"Always\"\n $ c85cop4: num  2 3 4 1 2 3 1 1 2 2 ...\n  ..- attr(*, \"label\")= chr \"does caregiving have negative effect on your physical health?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"Never\" \"Sometimes\" \"Often\" \"Always\"\n $ c86cop5: num  1 4 1 1 2 3 1 1 2 1 ...\n  ..- attr(*, \"label\")= chr \"does caregiving cause difficulties in your relationship with your family?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"Never\" \"Sometimes\" \"Often\" \"Always\"\n $ c87cop6: num  1 1 1 1 2 2 2 1 1 1 ...\n  ..- attr(*, \"label\")= chr \"does caregiving cause financial difficulties?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"Never\" \"Sometimes\" \"Often\" \"Always\"\n $ c88cop7: num  2 3 1 1 1 2 4 2 3 1 ...\n  ..- attr(*, \"label\")= chr \"do you feel trapped in your role as caregiver?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"Never\" \"Sometimes\" \"Often\" \"Always\"\n $ c89cop8: num  3 2 4 2 4 1 1 3 1 1 ...\n  ..- attr(*, \"label\")= chr \"do you feel supported by friends/neighbours?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"never\" \"sometimes\" \"often\" \"always\"\n $ c90cop9: num  3 2 3 4 4 1 4 3 3 3 ...\n  ..- attr(*, \"label\")= chr \"do you feel caregiving worthwhile?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"never\" \"sometimes\" \"often\" \"always\"\n\n\nThe coping related variables have responses on the Likert Scale (1,2,3,4) which correspond to (never, sometimes, often, always), and each variable also has a label defining each variable. We can plot this data using the plot_likert function from package sjPlot:\n\nefc %&gt;% select(dplyr::contains(\"cop\")) %&gt;% \n  sjPlot::plot_likert(title = \"Caregiver Survey from EUROFAM\")\n\n\n\n\n\n Labelled Data\nNote how the y-axis has been populated with labels: this is an example of a labelled dataset, where not only do the variables have names i.e. column names, but also have longish text labels that add information to the data variables. A simple example is a survey dataset, where the column names can be the Likert scale(Like/Dislike/Strongly Dislike OR never/sometimes/often/always) and the labels are the survey questions themselves. Let us manually create one such dataset, since this is a common-enough situation1. We will use the R package sjlabelled to label our data.2.\n\n\n\n\n\n\nVariable Labels and Value Labels\n\n\n\nVariable label is human readable description of the variable. R supports rather long variable names and these names can contain even spaces and punctuation but short variables names make coding easier. Variable label can give a nice, long description of variable. With this description it is easier to remember what those variable names refer to.Value labels are similar to variable labels, but value labels are descriptions of the values a variable can take. Labeling values means we don’t have to remember if 1=Extremely poor and 7=Excellent or vice-versa. We can easily get dataset description and variables summary with info function.\n\n\n\n#library(sjlabelled)\nlikert_labels = c(\"never\", \"sometimes\",\"often\",\"always\") # numerically 1:4\nvariable_labels &lt;- c(\"Do you practice Analytics?\",\n                            \"Do you code in R?\",\n                            \"Have you published your R Code?\",\n                            \"Do you use Quarto as your Workflow in R?\",\n                            \"Will you use R at Work?\")\nmy_survey_data &lt;- \n  # Create toy survey data\n  # 200 responses to 5 questions\n  # responses on Likert Scale\n  # 1:4 = \"never\", \"sometimes\",\"often\",\"always\")\n  # \n  tibble(q1 = mosaic::sample(1:4, replace = TRUE, size = 200),\n         q2 = mosaic::sample(1:4, replace = TRUE, size = 200),\n         q3 = mosaic::sample(1:4, replace = TRUE, size = 200),\n         q4 = mosaic::sample(1:4, replace = TRUE, size = 200),\n         q5 = mosaic::sample(1:4, replace = TRUE, size = 200)) %&gt;%\n  \n  # Set VARIABLE labels\n  sjlabelled::set_label(x = .,\n                        label = variable_labels) %&gt;%\n  \n  # Now set VALUE labels\n  sjlabelled::set_labels(x = ., labels = likert_labels)\n\nstr(my_survey_data)\nplot_likert(my_survey_data, title = \"Summary of Analytics Questionnaire\")\n\n\n\ntibble [200 × 5] (S3: tbl_df/tbl/data.frame)\n $ q1: int [1:200] 3 2 1 2 3 1 1 2 2 2 ...\n  ..- attr(*, \"label\")= Named chr \"Do you practice Analytics?\"\n  .. ..- attr(*, \"names\")= chr \"q1\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"never\" \"sometimes\" \"often\" \"always\"\n $ q2: int [1:200] 1 1 3 2 1 2 1 1 2 4 ...\n  ..- attr(*, \"label\")= Named chr \"Do you code in R?\"\n  .. ..- attr(*, \"names\")= chr \"q2\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"never\" \"sometimes\" \"often\" \"always\"\n $ q3: int [1:200] 4 4 1 1 4 1 1 2 3 1 ...\n  ..- attr(*, \"label\")= Named chr \"Have you published your R Code?\"\n  .. ..- attr(*, \"names\")= chr \"q3\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"never\" \"sometimes\" \"often\" \"always\"\n $ q4: int [1:200] 1 1 2 4 2 2 4 3 3 3 ...\n  ..- attr(*, \"label\")= Named chr \"Do you use Quarto as your Workflow in R?\"\n  .. ..- attr(*, \"names\")= chr \"q4\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"never\" \"sometimes\" \"often\" \"always\"\n $ q5: int [1:200] 3 3 1 3 3 2 4 2 4 3 ...\n  ..- attr(*, \"label\")= Named chr \"Will you use R at Work?\"\n  .. ..- attr(*, \"names\")= chr \"q5\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"never\" \"sometimes\" \"often\" \"always\"\n\n\n\n\n\n\n\nSo there we are with Categorical data ! There are a few other plots with this type of data, which are useful in very specialized circumstances. One example of this is the agreement plot which captures the agreement between two (sets) of evaluators, on ratings given on a shared ordinal scale to a set of items. An example from the field of medical diagnosis is the opinions of two specialists on a common set of patients. However, that is for a more advanced course!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#iconify-fluent-mdl2-decision-solid-conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#iconify-fluent-mdl2-decision-solid-conclusion",
    "title": "🐉 Visualizing Categorical Data",
    "section": "\n Conclusion",
    "text": "Conclusion\nHow are these bar plots different from histograms? Why don’t “regular” scatter plots simply work for Categorical data? Discuss!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#your-turn",
    "title": "🐉 Visualizing Categorical Data",
    "section": "Your Turn",
    "text": "Your Turn\n\nTake some of the categorical datasets from the vcd and vcdExtra packages and recreate the plots from this module."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#iconify-ooui-references-ltr-references",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#iconify-ooui-references-ltr-references",
    "title": "🐉 Visualizing Categorical Data",
    "section": "\n References",
    "text": "References\n\nChapter 4, https://openintro-ims.netlify.app/explore-categorical.html from An Introduction to Modern Statistics by Mine Cetinkaya-Rundel and Johanna Hardin.\nUsing the strcplot command from vcd, https://cran.r-project.org/web/packages/vcd/vignettes/strucplot.pdf\nCreating Frequency Tables with vcd, https://cran.r-project.org/web/packages/vcdExtra/vignettes/A_creating.html\nCreating mosaic plots with vcd, https://cran.r-project.org/web/packages/vcdExtra/vignettes/D_mosaics.html\nMichael Friendly, Corrgrams: Exploratory displays for correlation matrices. The American Statistician August 19, 2002 (v1.5). https://www.datavis.ca/papers/corrgram.pdf\nVisualizing Categorical Data in R"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#footnotes",
    "title": "🐉 Visualizing Categorical Data",
    "section": "Footnotes",
    "text": "Footnotes\n\nPiping Hot Data: Leveraging Labelled Data in R, https://www.pipinghotdata.com/posts/2020-12-23-leveraging-labelled-data-in-r/&gt;↩︎\nLabel Support in R:https://cran.r-project.org/web/packages/sjlabelled/index.html↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html",
    "title": "Tutorial on Correlations in R",
    "section": "",
    "text": "We will create Tables for Correlations, and graphs for Correlations in R. As always, we will consistently use the Project Mosaic ecosystem of packages in R (mosaic, mosaicData and ggformula).\n\n\nlibrary(tidyverse)\nlibrary(mosaic) # package for stats, simulations, and basic plots\nlibrary(ggformula) # package for professional looking plots, that use the formula interface from mosaic\nlibrary(skimr)\nlibrary(GGally)\nlibrary(corrplot) # For Correlogram plots\nlibrary(broom) # to properly format stat test results\n\nlibrary(mosaicData) # package containing datasets\nlibrary(NHANES) # survey data collected by the US National Center for Health Statistics (NCHS)\n\nAll R functions seen in the code are clickable links that take you to online documentation about the function. Try!\n\n\n\n\n\n\nThe Formula interface\n\n\n\nNote the standard method for all commands from the mosaic package:\ngoal( y ~ x | z, data = mydata, …)\nWith ggformula, one can create any graph/chart using:\ngf_geometry(y ~ x | z, data = mydata)\nOR\nmydata %&gt;% gf_geometry( y ~ x | z )\nThe second method may be preferable, especially if you have done some data manipulation first! More about this later!\n\n\n\nLet us inspect what datasets are available in the package mosaicData. Run this command in your Console:\n\n# Run in Console\ndata(package = \"mosaicData\")\n\nThe popup tab shows a lot of datasets we could use. Let us continue to use the famous Galton dataset and inspect it:\n\ndata(\"Galton\")\n\n\nThe inspect command already gives us a series of statistical measures of different variables of interest. As discussed previously, we can retain the output of inspect and use it in our reports: (there are ways of dressing up these tables too)\n\ngalton_describe &lt;- inspect(Galton)\n\ngalton_describe$categorical\n\n\n\n  \n\n\ngalton_describe$quantitative\n\n\n\n  \n\n\n\nTry help(\"Galton\") in your Console. The dataset is described as:\n\nA data frame with 898 observations on the following variables.\n- family a factor with levels for each family\n- father the father’s height (in inches)\n- mother the mother’s height (in inches)\n- sex the child’s sex: F or M\n- height the child’s height as an adult (in inches)\n- nkids the number of adult children in the family, or, at least, the number whose heights Galton recorded.\n\nThere is a lot of Description generated by the mosaic::inspect() command ! Let us also look at the output of skim:\n\nskimr::skim(Galton)\n\n\nData summary\n\n\nName\nGalton\n\n\nNumber of rows\n898\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nfamily\n0\n1\nFALSE\n197\n185: 15, 166: 11, 66: 11, 130: 10\n\n\nsex\n0\n1\nFALSE\n2\nM: 465, F: 433\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nfather\n0\n1\n69.23\n2.47\n62\n68\n69.0\n71.0\n78.5\n▁▅▇▂▁\n\n\nmother\n0\n1\n64.08\n2.31\n58\n63\n64.0\n65.5\n70.5\n▂▅▇▃▁\n\n\nheight\n0\n1\n66.76\n3.58\n56\n64\n66.5\n69.7\n79.0\n▁▇▇▅▁\n\n\nnkids\n0\n1\n6.14\n2.69\n1\n4\n6.0\n8.0\n15.0\n▃▇▆▂▁\n\n\n\n\n\nWhat can we say about the dataset and its variables? How big is the dataset? How many variables? What types are they, Quant or Qual? If they are Qual, what are the levels? Are they ordered levels? Which variables could have relationships with others? Why? Write down these Questions!\n\nWhat Questions might we have, that we could answer with a Statistical Measure, or Correlation chart?\n\n\n\n\n\n\nQuestions\n\n\n\nHow does children’s height correlate with that of father and mother? Is this relationship also affected by sex of the child?\nWith this question, height becomes our target variable, which we should always plot on the dependent y-axis.\n\n\n\n# Pulling out the list of Quant variables from NHANES\ngalton_quant &lt;- galton_describe$quantitative\ngalton_quant$name\n\n[1] \"father\" \"mother\" \"height\" \"nkids\" \n\nGGally::ggpairs(\n  Galton,\n  \n  # Choose the variables we want to plot for\n  columns = c(\"father\", \"mother\", \"height\", \"nkids\"),\n  \n  switch = \"both\", # axis labels in more traditional locations\n  progress = FALSE, # no compute progress messages needed\n  \n  # Choose the diagonal graphs (always single variable! Think!)\n  diag = list(continuous = \"barDiag\"), # choosing histogram,not density\n  \n  # Choose lower triangle graphs, two-variable graphs\n  lower = list(continuous = wrap(\"smooth\", alpha = 0.1)),\n  \n  title = \"Galton Data Correlations Plot\"\n) + \n  \n  theme_bw()\n\n\n\n\nWe note that children’s height is correlated with that of father and mother. The correlations are both positive, and that with father seems to be the larger of the two. ( Look at the slopes of the lines and the values of the correlation scores. )\n\n\n\n\n\n\nQuestion\n\n\n\nWhat if we group the Quant variables based on a Qual variable, like sex of the child?\n\n\n\n# Pulling out the list of Quant variables from NHANES\ngalton_quant &lt;- galton_describe$quantitative\ngalton_quant$name\n\n[1] \"father\" \"mother\" \"height\" \"nkids\" \n\nGGally::ggpairs(\n  Galton,\n  \n  mapping = aes(colour = sex), # Colour by `sex`\n\n  # Choose the variables we want to plot for\n  columns = c(\"father\", \"mother\", \"height\", \"nkids\"),\n  switch = \"both\", # axis labels in more traditional locations\n  progress = FALSE, # no compute progress messages needed\n  \n  diag = list(continuous = \"barDiag\"),\n  \n  # Choose lower triangle graphs, two-variable graphs\n  lower = list(continuous = wrap(\"smooth\", alpha = 0.1)),\n  \n  title = \"Galton Data Correlations Plot\"\n) + \n  \n  theme_bw()\n\n\n\n\nThe split scatter plots are useful, as is the split histogram for height: Clearly the correlation of children’s height with father and mother is positive for both sex-es. The other plots, and even some of the correlations scores are not all useful! Just shows everything we can compute is not necessarily useful immediately.\nIn later modules we will see how to plot correlations when the number of variables is larger still.\n\n\n\n\n\n\nQuestion\n\n\n\nCan we plot a Correlogram for this dataset?\n\n\n\n#library(corrplot)\n\ngalton_num_var &lt;- Galton %&gt;% select(father, mother, height, nkids)\ngalton_cor &lt;- cor(galton_num_var)\ngalton_cor %&gt;%\n  corrplot(method = \"ellipse\",\n           type = \"lower\",\n           main = \"Correlogram for Galton dataset\")\n\n\n\n\nClearly height is positively correlated to father and mother; interestingly, height is negatively correlated ( slightly) with nkids.\n\n\n\n\n\n\nQuestion\n\n\n\nLet us confirm with a correlation test:\n\n\nWe will use the mosaic function cor_test to get these results:\n\nmosaic::cor_test(height ~ father, data = Galton) %&gt;% \n  broom::tidy() %&gt;% \n  knitr::kable(digits = 2,\n               caption = \"Children vs Fathers\")\n\n\nChildren vs Fathers\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n0.28\n8.57\n0\n896\n0.21\n0.33\nPearson’s product-moment correlation\ntwo.sided\n\n\n\n\n\nmosaic::cor_test(height ~ mother, data = Galton) %&gt;% \n  broom::tidy() %&gt;% \n    knitr::kable(digits = 2,\n               caption = \"Children vs Mothers\")\n\n\nChildren vs Mothers\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n0.2\n6.16\n0\n896\n0.14\n0.26\nPearson’s product-moment correlation\ntwo.sided\n\n\n\n\n\n\n\n\n\n\nCorrelation Scores and Uncertainty\n\n\n\nNote how the mosaic::cor_test() reports a correlation score estimate and the p-value for the same. There is also a confidence interval reported for the correlation score, an interval within which we are 95% sure that the true correlation value is to be found.\nNote that GGally::ggpairs() too reports the significance of the correlation scores estimates using *** or **. This indicates the p-value in the scores obtained by GGally; Presumably, there is an internal cor_test that is run for each pair of variables and the p-value and confidence levels are also computed internally.\n\n\nIn both cases, we used the formula \\(height \\sim other-variable\\), in keeping with our idea of height being the dependent, target variable..\nWe also see the p.value for the estimateed correlation is negligible, and the conf.low/conf.high interval does not straddle \\(0\\). These attest to the significance of the correlation score.\n\n\n\n\n\n\nQuestion\n\n\n\nWhat does this correlation look when split by sex of Child?\n\n\n\n# For the sons\n\nmosaic::cor_test(height ~ father,\n                 data = Galton %&gt;% filter(sex == \"M\")) %&gt;% \n  broom::tidy() %&gt;% knitr::kable(digits = 2,\n                                 caption = \"Sons vs Fathers\")\ncor_test(height ~ mother, \n         data = Galton %&gt;% filter(sex == \"M\")) %&gt;% \n  broom::tidy() %&gt;% knitr::kable(digits = 2,\n                                 caption = \"Sons vs Mothers\")\n\n# For the daughters\ncor_test(height ~ father, \n         data = Galton %&gt;% filter(sex == \"F\")) %&gt;% \n  broom::tidy() %&gt;% knitr::kable(digits = 2,\n                                 caption = \"Daughters vs Fathers\")\ncor_test(height ~ mother, \n         data = Galton %&gt;% filter(sex == \"F\")) %&gt;% \n  broom::tidy() %&gt;% knitr::kable(digits = 2,\n                                 caption = \"Daughters vs Mothers\")\n\n\nSons vs Fathers\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n0.39\n9.15\n0\n463\n0.31\n0.47\nPearson’s product-moment correlation\ntwo.sided\n\n\n\nSons vs Mothers\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n0.33\n7.63\n0\n463\n0.25\n0.41\nPearson’s product-moment correlation\ntwo.sided\n\n\n\nDaughters vs Fathers\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n0.46\n10.72\n0\n431\n0.38\n0.53\nPearson’s product-moment correlation\ntwo.sided\n\n\n\nDaughters vs Mothers\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n0.31\n6.86\n0\n431\n0.23\n0.4\nPearson’s product-moment correlation\ntwo.sided\n\n\n\n\nThe same observation as made above ( p.value and confidence intervals) applies here too and tells us that the estimated correlations are significant.\n\nWe can also visualize this uncertainty and the confidence levels in a plot too, using gf_errorbar and a handy set of functions within purrr which is part of the tidyverse. Assuming heights is the target variable we want to correlate every other (quantitative) variable against, we can proceed very quickly as follows: we will first plot correlation uncertainty for one pair of variables to develop the intuition, and then for all variables against the one target variable:\n\nmosaic::cor_test(height ~ mother, data = Galton) %&gt;% \n  broom::tidy() %&gt;% \n\n# We need a graph not a table \n# So comment out this line from the earlier code\n#knitr::kable(digits = 2,caption = \"Children vs Mothers\")\n\nrowid_to_column(var = \"index\") %&gt;% # Need an index to plot with\n  \n  # Uncertainty as error-bars\n  gf_errorbar(conf.high + conf.low ~ index, linewidth = 2) %&gt;% \n  \n  # Estimate as a point\n  gf_point(estimate ~ index, color = \"red\", size = 6) %&gt;% \n  \n  # Labels\n  gf_text(estimate ~ index - 0.2, \n             label = \"Correlation Score = estimate\") %&gt;% \n  gf_text(conf.high*0.98 ~ index - 0.25, \n           label = \"Upper Limit = estimate + conf.high\") %&gt;%   \n  gf_text(conf.low*1.04 ~ index - 0.25, \n           label = \"Lower Limit = estimate - conf.low\") %&gt;% \n  gf_theme(theme_bw())\n\n\n\n\nWe can now do this for all variables against the target variable height, which we identified in our research question. We will use the iteration capabilities offered by the tidyverse package, purrr:\n\nall_corrs &lt;- Galton %&gt;% \n  select(where(is.numeric)) %&gt;% \n  \n  # leave off height to get all the remaining ones\n  select(- height) %&gt;%  \n  \n  # perform a cor.test for all variables against height\n  purrr::map(.x = .,\n             .f = \\(x) cor.test(x, Galton$height)) %&gt;%\n  \n  # tidy up the cor.test outputs into a tidy data frame\n  map_dfr(broom::tidy, .id = \"predictor\") \n\nall_corrs\n\n\n\n  \n\n\nall_corrs %&gt;% \n  \n  # arrange the predictors in order of their correlation scores\n  # with the target variable (`height`)\n  # Add errorbars to show uncertainty ranges / confidence intervals\n  # Use errorbar width and linewidth fo emphasis\n  gf_errorbar(conf.high + conf.low ~ reorder(predictor, estimate),\n              color = ~ estimate,\n              width = 0.2,\n              linewidth = ~ -log10(p.value)) %&gt;% \n  \n  # All correlation estimates as points\n  gf_point(estimate ~ reorder(predictor, estimate), \n           color = \"black\") %&gt;% \n  \n  # Reference line at zero correlation score\n  gf_hline(yintercept = 0, color = \"grey\", linewidth = 2) %&gt;% \n  \n  # Themes,Titles, and Scales\n  gf_labs(x = NULL, y = \"Correlation with height in Galton\", \n          caption = \"Significance = - log10(p.value)\") %&gt;% \n  \n  gf_refine(\n    \n    # Scale for colour\n scale_colour_distiller(\"Correlation\", type = \"div\", palette = \"RdBu\"),\n            \n    # Scale for dumbbells!!\n    scale_linewidth_continuous(\"significance\",\n                                       range = c(0.5,4))) %&gt;% \n  gf_refine(guides(linewidth = guide_legend(reverse = TRUE))) %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\nWe can clearly see the size of the correlations and the confidence intervals marked in this plot. father has somewhat greater correlation with children’s height, as compared to mother. nkids seems to matter very little. This kind of plot will be very useful when we pursue linear regression models.\n\n\n\n\n\n\nQuestion\n\n\n\nHow can we show this correlation in a set of Scatter Plots + Regression Lines? Can we recreate Galton’s famous diagram?\n\n\n\n# For the sons\ngf_point(height ~ father, \n         data = Galton %&gt;% filter(sex == \"M\"),\n         title = \"Soms and Fathers\") %&gt;%\n  gf_smooth(method = \"lm\") %&gt;%\n  gf_theme(theme_minimal())\ngf_point(height ~ mother, \n         data = Galton %&gt;% filter(sex == \"M\"),\n         title = \"Sons and Mothers\") %&gt;%\n  gf_smooth(method = \"lm\") %&gt;%\n  gf_theme(theme_minimal())\n# For the daughters\ngf_point(height ~ father, \n         data = Galton %&gt;% filter(sex == \"F\"),\n         title = \"Daughters and Fathers\") %&gt;%\n  gf_smooth(method = \"lm\") %&gt;%\n  gf_theme(theme_minimal())\ngf_point(height ~ mother, \n         data = Galton %&gt;% filter(sex == \"F\"),\n         title = \"Daughters and Mothers\") %&gt;%\n  gf_smooth(method = \"lm\") %&gt;% \n  gf_theme(theme_minimal())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn approximation to Galton’s famous plot1 (see Wikipedia):\n\n\n\n\n\n\ngf_point(height ~ (father + mother)/2, data = Galton) %&gt;% \n  gf_smooth(method = \"lm\") %&gt;% \n  gf_density_2d(n = 8) %&gt;% \n  gf_abline(slope = 1) %&gt;% \n  gf_theme(theme_minimal())\ngf_point(height ~ (father + mother)/2, data = Galton) %&gt;% \n  gf_smooth(method = \"lm\") %&gt;% \n  gf_ellipse(level = 0.95, color = \"red\") %&gt;% \n    gf_ellipse(level = 0.75, color = \"blue\") %&gt;% \n    gf_ellipse(level = 0.5, color = \"green\") %&gt;% \n  gf_abline(slope = 1) %&gt;% \n  gf_theme(theme_minimal())\n\n\n\n\n\n\n\n\n\n\n\nHow would you interpret this plot2?\n\nLet us look at the NHANES dataset from the package NHANES:\n\ndata(\"NHANES\")\n\n\n\nNHANES_describe &lt;- inspect(NHANES)\n\nNHANES_describe$categorical\nNHANES_describe$quantitative\nNHANES\nskimr::skim(NHANES)\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n\n\n\n\n  \n\n\n\n\n\n\nData summary\n\n\nName\nNHANES\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n76\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n31\n\n\nnumeric\n45\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nSurveyYr\n0\n1.00\nFALSE\n2\n200: 5000, 201: 5000\n\n\nGender\n0\n1.00\nFALSE\n2\nfem: 5020, mal: 4980\n\n\nAgeDecade\n333\n0.97\nFALSE\n8\n40: 1398, 0-: 1391, 10: 1374, 20: 1356\n\n\nRace1\n0\n1.00\nFALSE\n5\nWhi: 6372, Bla: 1197, Mex: 1015, Oth: 806\n\n\nRace3\n5000\n0.50\nFALSE\n6\nWhi: 3135, Bla: 589, Mex: 480, His: 350\n\n\nEducation\n2779\n0.72\nFALSE\n5\nSom: 2267, Col: 2098, Hig: 1517, 9 -: 888\n\n\nMaritalStatus\n2769\n0.72\nFALSE\n6\nMar: 3945, Nev: 1380, Div: 707, Liv: 560\n\n\nHHIncome\n811\n0.92\nFALSE\n12\nmor: 2220, 750: 1084, 250: 958, 350: 863\n\n\nHomeOwn\n63\n0.99\nFALSE\n3\nOwn: 6425, Ren: 3287, Oth: 225\n\n\nWork\n2229\n0.78\nFALSE\n3\nWor: 4613, Not: 2847, Loo: 311\n\n\nBMICatUnder20yrs\n8726\n0.13\nFALSE\n4\nNor: 805, Obe: 221, Ove: 193, Und: 55\n\n\nBMI_WHO\n397\n0.96\nFALSE\n4\n18.: 2911, 30.: 2751, 25.: 2664, 12.: 1277\n\n\nDiabetes\n142\n0.99\nFALSE\n2\nNo: 9098, Yes: 760\n\n\nHealthGen\n2461\n0.75\nFALSE\n5\nGoo: 2956, Vgo: 2508, Fai: 1010, Exc: 878\n\n\nLittleInterest\n3333\n0.67\nFALSE\n3\nNon: 5103, Sev: 1130, Mos: 434\n\n\nDepressed\n3327\n0.67\nFALSE\n3\nNon: 5246, Sev: 1009, Mos: 418\n\n\nSleepTrouble\n2228\n0.78\nFALSE\n2\nNo: 5799, Yes: 1973\n\n\nPhysActive\n1674\n0.83\nFALSE\n2\nYes: 4649, No: 3677\n\n\nTVHrsDay\n5141\n0.49\nFALSE\n7\n2_h: 1275, 1_h: 884, 3_h: 836, 0_t: 638\n\n\nCompHrsDay\n5137\n0.49\nFALSE\n7\n0_t: 1409, 0_h: 1073, 1_h: 1030, 2_h: 589\n\n\nAlcohol12PlusYr\n3420\n0.66\nFALSE\n2\nYes: 5212, No: 1368\n\n\nSmokeNow\n6789\n0.32\nFALSE\n2\nNo: 1745, Yes: 1466\n\n\nSmoke100\n2765\n0.72\nFALSE\n2\nNo: 4024, Yes: 3211\n\n\nSmoke100n\n2765\n0.72\nFALSE\n2\nNon: 4024, Smo: 3211\n\n\nMarijuana\n5059\n0.49\nFALSE\n2\nYes: 2892, No: 2049\n\n\nRegularMarij\n5059\n0.49\nFALSE\n2\nNo: 3575, Yes: 1366\n\n\nHardDrugs\n4235\n0.58\nFALSE\n2\nNo: 4700, Yes: 1065\n\n\nSexEver\n4233\n0.58\nFALSE\n2\nYes: 5544, No: 223\n\n\nSameSex\n4232\n0.58\nFALSE\n2\nNo: 5353, Yes: 415\n\n\nSexOrientation\n5158\n0.48\nFALSE\n3\nHet: 4638, Bis: 119, Hom: 85\n\n\nPregnantNow\n8304\n0.17\nFALSE\n3\nNo: 1573, Yes: 72, Unk: 51\n\n\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nID\n0\n1.00\n61944.64\n5871.17\n51624.00\n56904.50\n62159.50\n67039.00\n71915.00\n▇▇▇▇▇\n\n\nAge\n0\n1.00\n36.74\n22.40\n0.00\n17.00\n36.00\n54.00\n80.00\n▇▇▇▆▅\n\n\nAgeMonths\n5038\n0.50\n420.12\n259.04\n0.00\n199.00\n418.00\n624.00\n959.00\n▇▇▇▆▃\n\n\nHHIncomeMid\n811\n0.92\n57206.17\n33020.28\n2500.00\n30000.00\n50000.00\n87500.00\n100000.00\n▃▆▃▁▇\n\n\nPoverty\n726\n0.93\n2.80\n1.68\n0.00\n1.24\n2.70\n4.71\n5.00\n▅▅▃▃▇\n\n\nHomeRooms\n69\n0.99\n6.25\n2.28\n1.00\n5.00\n6.00\n8.00\n13.00\n▂▆▇▂▁\n\n\nWeight\n78\n0.99\n70.98\n29.13\n2.80\n56.10\n72.70\n88.90\n230.70\n▂▇▂▁▁\n\n\nLength\n9457\n0.05\n85.02\n13.71\n47.10\n75.70\n87.00\n96.10\n112.20\n▁▃▆▇▃\n\n\nHeadCirc\n9912\n0.01\n41.18\n2.31\n34.20\n39.58\n41.45\n42.92\n45.40\n▁▂▇▇▅\n\n\nHeight\n353\n0.96\n161.88\n20.19\n83.60\n156.80\n166.00\n174.50\n200.40\n▁▁▁▇▂\n\n\nBMI\n366\n0.96\n26.66\n7.38\n12.88\n21.58\n25.98\n30.89\n81.25\n▇▆▁▁▁\n\n\nPulse\n1437\n0.86\n73.56\n12.16\n40.00\n64.00\n72.00\n82.00\n136.00\n▂▇▃▁▁\n\n\nBPSysAve\n1449\n0.86\n118.15\n17.25\n76.00\n106.00\n116.00\n127.00\n226.00\n▃▇▂▁▁\n\n\nBPDiaAve\n1449\n0.86\n67.48\n14.35\n0.00\n61.00\n69.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nBPSys1\n1763\n0.82\n119.09\n17.50\n72.00\n106.00\n116.00\n128.00\n232.00\n▂▇▂▁▁\n\n\nBPDia1\n1763\n0.82\n68.28\n13.78\n0.00\n62.00\n70.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys2\n1647\n0.84\n118.48\n17.49\n76.00\n106.00\n116.00\n128.00\n226.00\n▃▇▂▁▁\n\n\nBPDia2\n1647\n0.84\n67.66\n14.42\n0.00\n60.00\n68.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys3\n1635\n0.84\n117.93\n17.18\n76.00\n106.00\n116.00\n126.00\n226.00\n▃▇▂▁▁\n\n\nBPDia3\n1635\n0.84\n67.30\n14.96\n0.00\n60.00\n68.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nTestosterone\n5874\n0.41\n197.90\n226.50\n0.25\n17.70\n43.82\n362.41\n1795.60\n▇▂▁▁▁\n\n\nDirectChol\n1526\n0.85\n1.36\n0.40\n0.39\n1.09\n1.29\n1.58\n4.03\n▅▇▂▁▁\n\n\nTotChol\n1526\n0.85\n4.88\n1.08\n1.53\n4.11\n4.78\n5.53\n13.65\n▂▇▁▁▁\n\n\nUrineVol1\n987\n0.90\n118.52\n90.34\n0.00\n50.00\n94.00\n164.00\n510.00\n▇▅▂▁▁\n\n\nUrineFlow1\n1603\n0.84\n0.98\n0.95\n0.00\n0.40\n0.70\n1.22\n17.17\n▇▁▁▁▁\n\n\nUrineVol2\n8522\n0.15\n119.68\n90.16\n0.00\n52.00\n95.00\n171.75\n409.00\n▇▆▃▂▁\n\n\nUrineFlow2\n8524\n0.15\n1.15\n1.07\n0.00\n0.48\n0.76\n1.51\n13.69\n▇▁▁▁▁\n\n\nDiabetesAge\n9371\n0.06\n48.42\n15.68\n1.00\n40.00\n50.00\n58.00\n80.00\n▁▂▆▇▂\n\n\nDaysPhysHlthBad\n2468\n0.75\n3.33\n7.40\n0.00\n0.00\n0.00\n3.00\n30.00\n▇▁▁▁▁\n\n\nDaysMentHlthBad\n2466\n0.75\n4.13\n7.83\n0.00\n0.00\n0.00\n4.00\n30.00\n▇▁▁▁▁\n\n\nnPregnancies\n7396\n0.26\n3.03\n1.80\n1.00\n2.00\n3.00\n4.00\n32.00\n▇▁▁▁▁\n\n\nnBabies\n7584\n0.24\n2.46\n1.32\n0.00\n2.00\n2.00\n3.00\n12.00\n▇▅▁▁▁\n\n\nAge1stBaby\n8116\n0.19\n22.65\n4.77\n14.00\n19.00\n22.00\n26.00\n39.00\n▆▇▅▂▁\n\n\nSleepHrsNight\n2245\n0.78\n6.93\n1.35\n2.00\n6.00\n7.00\n8.00\n12.00\n▁▅▇▁▁\n\n\nPhysActiveDays\n5337\n0.47\n3.74\n1.84\n1.00\n2.00\n3.00\n5.00\n7.00\n▇▇▃▅▅\n\n\nTVHrsDayChild\n9347\n0.07\n1.94\n1.43\n0.00\n1.00\n2.00\n3.00\n6.00\n▇▆▂▂▂\n\n\nCompHrsDayChild\n9347\n0.07\n2.20\n2.52\n0.00\n0.00\n1.00\n6.00\n6.00\n▇▁▁▁▃\n\n\nAlcoholDay\n5086\n0.49\n2.91\n3.18\n1.00\n1.00\n2.00\n3.00\n82.00\n▇▁▁▁▁\n\n\nAlcoholYear\n4078\n0.59\n75.10\n103.03\n0.00\n3.00\n24.00\n104.00\n364.00\n▇▁▁▁▁\n\n\nSmokeAge\n6920\n0.31\n17.83\n5.33\n6.00\n15.00\n17.00\n19.00\n72.00\n▇▂▁▁▁\n\n\nAgeFirstMarij\n7109\n0.29\n17.02\n3.90\n1.00\n15.00\n16.00\n19.00\n48.00\n▁▇▂▁▁\n\n\nAgeRegMarij\n8634\n0.14\n17.69\n4.81\n5.00\n15.00\n17.00\n19.00\n52.00\n▂▇▁▁▁\n\n\nSexAge\n4460\n0.55\n17.43\n3.72\n9.00\n15.00\n17.00\n19.00\n50.00\n▇▅▁▁▁\n\n\nSexNumPartnLife\n4275\n0.57\n15.09\n57.85\n0.00\n2.00\n5.00\n12.00\n2000.00\n▇▁▁▁▁\n\n\nSexNumPartYear\n5072\n0.49\n1.34\n2.78\n0.00\n1.00\n1.00\n1.00\n69.00\n▇▁▁▁▁\n\n\n\n\n\n\n\nTry help(\"NHANES\") in your Console.\n\nThis is survey data collected by the US National Center for Health Statistics (NCHS) which has conducted a series of health and nutrition surveys since the early 1960’s. Since 1999 approximately 5,000 individuals of all ages are interviewed in their homes every year and complete the health examination component of the survey. The health examination is conducted in a mobile examination centre (MEC).\n\nThe dataset is described as: A data frame with 100000 observations on 76 variables. Some of these are:\n- Race1 and Race2: factors with 5 and 6 levels respectively\n- Education a factor with 5 levels\n- HHIncomeMid Total annual gross income for the household in US dollars.\n- Age\n- BMI: Body mass index (weight/height2 in kg/m2)\n- Height: Standing height in cm.\n- Weight: Weight in kg &gt; &gt; - Testosterone: Testosterone total (ng/dL) - PhysActiveDays: Number of days in a typical week that participant does moderate or vigorous-intensity activity.\n- CompHrsDay: Number of hours per day on average participant used a computer or gaming device over the past 30 days.\n\n\n\n\n\n\nMissing Data\n\n\n\nWhy do so many of the variables have missing entries? What could be your guess about the Experiment/Survey`?\n\n\nLet us make some counts of the data, since we have so many factors:\n\nNHANES %&gt;% count(Gender)\n\n\n\n  \n\n\nNHANES %&gt;% count(Race1)\n\n\n\n  \n\n\nNHANES %&gt;% count(Race3)\n\n\n\n  \n\n\nNHANES %&gt;% count(Education)\n\n\n\n  \n\n\nNHANES %&gt;% count(MaritalStatus)\n\n\n\n  \n\n\n\nThere is a good mix of factors and counts.\nNow we articulate our Research Questions:\n\n\n\n\n\n\nResearch Question\n\n\n\n\nDoes Testosterone have a relationship with parameters such as BMI, Weight, Height, PhysActiveDays CompHrsDay and Age?\nDoes HHIncomeMid have a relationship with these same parameters? And with Gender?\nAre there any other pairwise correlations that we should note? (This is especially useful in choosing independent variables for multiple regression)\n\n\n\n( Yes we are concerned with men more than with the women, sadly.)\n\n\nGGally::ggpairs(NHANES, \n                # Choose the variables we want to plot for\n                columns = c(\"HHIncomeMid\", \"Weight\", \"Height\", \n                            \"BMI\", \"Gender\"), \n                \n                # LISTs of graphs needed at different locations\n                # For different combinations of variables \n                diag = list(continuous = \"barDiag\"),\n                lower = list(continuous = wrap(\"smooth\", alpha = 0.01)),\n                upper = list(continuous = \"cor\"),\n                \n                switch = \"both\", # axis labels in more traditional locations\n                progress = FALSE ) + # No compute progress bars needed\n  theme_bw()\n\n\n\n\nWe see that HHIncomeMid is Quantitative, discrete valued variable, since it is based on a set of median incomes for different ranges of income. BMI, Weight, Height are continuous Quant variables.\nHHIncomeMid also seems to be relatively unaffected by Weight; And is only mildly correlated with Height and BMI, as seen both by the correlation score magnitudes and the slopes of the trend lines.\nThere is a difference in the median income by Gender, but we will defer that test for later, when we do Statistical Inference.\nUnsurprisingly, BMI and Weight have a strong relationship, as do Height and Weight; the latter is of course non-linear, since the Height levels off at a point.\n\nGGally::ggpairs(NHANES, \n                columns = c(\"Testosterone\", \"Weight\", \"Height\", \"BMI\"), \n                \n                diag = list(continuous = \"barDiag\"),\n                lower = list(continuous = wrap(\"smooth\", alpha = 0.01)),\n                upper = list(continuous = \"cor\"),\n                \n                switch = \"both\",\n                progress = FALSE ) +\n  theme_bw()\n\n\n\n\nIt is clear that Testosterone has strong relationships with Height and Weight but not so much with BMI.\n\nSince the pairs plot is fairly clear for both target variables, let us head to visualizing the significance and uncertainty in the correlation estimates.\n\nHHIncome_corrs &lt;- NHANES %&gt;% \n  select(where(is.numeric)) %&gt;% \n  \n  # leave off height to get all the remaining ones\n  select(- HHIncomeMid) %&gt;%  \n  \n  # perform a cor.test for all variables against height\n  purrr::map(.x = .,\n             .f = \\(x) cor.test(x, NHANES$HHIncomeMid)) %&gt;%\n  \n  # tidy up the cor.test outputs into a tidy data frame\n  map_dfr(broom::tidy, .id = \"predictor\") \n\nHHIncome_corrs\n\n\n\n  \n\n\nHHIncome_corrs %&gt;% \n  \n  # arrange the predictors in order of their correlation scores\n  # with the target variable (`height`)\n  # Add errorbars to show uncertainty ranges / confidence intervals\n  # Use errorbar width and linewidth fo emphasis\n  gf_errorbar(conf.high + conf.low ~ reorder(predictor, estimate),\n              color = ~ -log10(p.value + 0.001),\n              width = 0.2,\n              linewidth = ~ -log10(p.value + 0.001)) %&gt;% \n  \n  # All correlation estimates as points\n  gf_point(estimate ~ reorder(predictor, estimate), \n           color = \"black\") %&gt;% \n  \n  # Reference line at zero correlation score\n  gf_hline(yintercept = 0, color = \"grey\", linewidth = 2) %&gt;% \n  \n  # Themes,Titles, and Scales\n  gf_labs(x = NULL, y = \"Correlations with HouseHold Median Income\", \n          caption = \"Significance = - log10(p.value)\") %&gt;% \n  \n  gf_refine(\n    # Scale for colour\n    scale_colour_gradient(\"significance\",low = \"purple\", \n                                 high = \"red\"),\n            \n    # Scale for dumbbells!!\n    scale_linewidth_continuous(\"significance\",\n                                       range = c(0.5,4))) %&gt;% \n  gf_theme(theme_classic()) %&gt;% \n  gf_refine(theme(axis.text.x = element_text(angle = 60, size = 6, hjust = 1)))\n\n\n\n\nIf we select just the variables from our Research Question:\n\nHHIncome_corrs_select &lt;- NHANES %&gt;% \n  select(Height, Weight, BMI) %&gt;% # Only change is here!\n  \n  # leave off height to get all the remaining ones\n  #select(- HHIncomeMid) %&gt;%  \n  \n  # perform a cor.test for all variables against height\n  purrr::map(.x = .,\n             .f = \\(x) cor.test(x, NHANES$HHIncomeMid)) %&gt;%\n  \n  # tidy up the cor.test outputs into a tidy data frame\n  map_dfr(broom::tidy, .id = \"predictor\") \n\nHHIncome_corrs_select\n\n\n\n  \n\n\nHHIncome_corrs_select %&gt;% \n  \n  # arrange the predictors in order of their correlation scores\n  # with the target variable (`height`)\n  # Add errorbars to show uncertainty ranges / confidence intervals\n  # Use errorbar width and linewidth fo emphasis\n  gf_errorbar(conf.high + conf.low ~ reorder(predictor, estimate),\n              color = ~ -log10(p.value + 0.001),\n              width = 0.2,\n              linewidth = ~ -log10(p.value + 0.001)) %&gt;% \n  \n  # All correlation estimates as points\n  gf_point(estimate ~ reorder(predictor, estimate), \n           color = \"black\") %&gt;% \n  \n  # Reference line at zero correlation score\n  gf_hline(yintercept = 0, color = \"grey\", linewidth = 2) %&gt;% \n  \n  # Themes,Titles, and Scales\n  gf_labs(x = NULL, y = \"Correlations with HouseHold Median Income\", \n          caption = \"Significance = - log10(p.value)\") %&gt;% \n  \n  gf_refine(\n    # Scale for colour\n    scale_colour_gradient(\"significance\",low = \"purple\", \n                                 high = \"red\"),\n            \n    # Scale for dumbbells!!\n    scale_linewidth_continuous(\"significance\",\n                                       range = c(0.5,4))) %&gt;% \n  gf_theme(theme_classic()) %&gt;% \n  gf_refine(theme(axis.text.x = element_text(angle = 60, size = 10, hjust = 1)))\n\n\n\n\nSo we might say taller people make more money? And fatter people make slightly less money?\nLet us look at the Testosterone variable: trying all variables shows some paucity of observations ( due to missing data), so we will stick with our chosen variables:\n\nTestosterone_corrs &lt;- NHANES %&gt;% \n  select(Height, Weight, BMI) %&gt;% \n  \n  # leave off height to get all the remaining ones\n  #select(- Testosterone) %&gt;%  \n  \n  # perform a cor.test for all variables against height\n  purrr::map(.x = .,\n             .f = \\(x) cor.test(x, NHANES$Testosterone)) %&gt;%\n  \n  # tidy up the cor.test outputs into a tidy data frame\n  map_dfr(broom::tidy, .id = \"predictor\") \n\nTestosterone_corrs\n\n\n\n  \n\n\nTestosterone_corrs %&gt;% \n  \n  # arrange the predictors in order of their correlation scores\n  # with the target variable (`height`)\n  # Add errorbars to show uncertainty ranges / confidence intervals\n  # Use errorbar width and linewidth fo emphasis\n  gf_errorbar(conf.high + conf.low ~ reorder(predictor, estimate),\n              color = ~ -log10(p.value + 0.001),\n              width = 0.2,\n              linewidth = ~ -log10(p.value + 0.001)) %&gt;% \n  \n  # All correlation estimates as points\n  gf_point(estimate ~ reorder(predictor, estimate), \n           color = \"black\") %&gt;% \n  \n  # Reference line at zero correlation score\n  gf_hline(yintercept = 0, color = \"grey\", linewidth = 2) %&gt;% \n  \n  # Themes,Titles, and Scales\n  gf_labs(x = NULL, y = \"Correlations with Testosterone Levels\",\n          caption = \"Significance = - log10(p.value)\") %&gt;% \n  \n  gf_refine(\n    # Scale for colour\n    scale_colour_gradient(\"significance\",low = \"purple\", \n                                 high = \"red\"),\n            \n    # Scale for dumbbells!!\n    scale_linewidth_continuous(\"significance\",\n                                       range = c(0.5,4))) %&gt;% \n  gf_theme(theme_classic()) %&gt;% \n  gf_refine(theme(axis.text.x = element_text(angle = 60, size = 10, hjust = 1)))\n\n\n\n\n\nWe have a decent Correlations related workflow in R:\n\nLoad the dataset\n\ninspect the dataset, identify Quant and Qual variables\nDevelop Pair-Wise plots + Correlations using GGally::ggpairs()\n\nDevelop Correlogram corrplot::corrplot\n\nCheck everything with a cor_test\n\nUse purrr + cor.test to plot correlations and confidence intervals for multiple Quant variables\nPlot scatter plots using gf_point.\nAdd extra lines using gf_abline() to compare hypotheses that you may have."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#iconify-noto-v1-package-setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#iconify-noto-v1-package-setting-up-r-packages",
    "title": "Tutorial on Correlations in R",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic) # package for stats, simulations, and basic plots\nlibrary(ggformula) # package for professional looking plots, that use the formula interface from mosaic\nlibrary(skimr)\nlibrary(GGally)\nlibrary(corrplot) # For Correlogram plots\nlibrary(broom) # to properly format stat test results\n\nlibrary(mosaicData) # package containing datasets\nlibrary(NHANES) # survey data collected by the US National Center for Health Statistics (NCHS)\n\nAll R functions seen in the code are clickable links that take you to online documentation about the function. Try!\n\n\n\n\n\n\nThe Formula interface\n\n\n\nNote the standard method for all commands from the mosaic package:\ngoal( y ~ x | z, data = mydata, …)\nWith ggformula, one can create any graph/chart using:\ngf_geometry(y ~ x | z, data = mydata)\nOR\nmydata %&gt;% gf_geometry( y ~ x | z )\nThe second method may be preferable, especially if you have done some data manipulation first! More about this later!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#iconify-pajamas-issue-type-test-case-case-study-1-galton-dataset-from-mosaicdata",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#iconify-pajamas-issue-type-test-case-case-study-1-galton-dataset-from-mosaicdata",
    "title": "Tutorial on Correlations in R",
    "section": "",
    "text": "Let us inspect what datasets are available in the package mosaicData. Run this command in your Console:\n\n# Run in Console\ndata(package = \"mosaicData\")\n\nThe popup tab shows a lot of datasets we could use. Let us continue to use the famous Galton dataset and inspect it:\n\ndata(\"Galton\")\n\n\nThe inspect command already gives us a series of statistical measures of different variables of interest. As discussed previously, we can retain the output of inspect and use it in our reports: (there are ways of dressing up these tables too)\n\ngalton_describe &lt;- inspect(Galton)\n\ngalton_describe$categorical\n\n\n\n  \n\n\ngalton_describe$quantitative\n\n\n\n  \n\n\n\nTry help(\"Galton\") in your Console. The dataset is described as:\n\nA data frame with 898 observations on the following variables.\n- family a factor with levels for each family\n- father the father’s height (in inches)\n- mother the mother’s height (in inches)\n- sex the child’s sex: F or M\n- height the child’s height as an adult (in inches)\n- nkids the number of adult children in the family, or, at least, the number whose heights Galton recorded.\n\nThere is a lot of Description generated by the mosaic::inspect() command ! Let us also look at the output of skim:\n\nskimr::skim(Galton)\n\n\nData summary\n\n\nName\nGalton\n\n\nNumber of rows\n898\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nfamily\n0\n1\nFALSE\n197\n185: 15, 166: 11, 66: 11, 130: 10\n\n\nsex\n0\n1\nFALSE\n2\nM: 465, F: 433\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nfather\n0\n1\n69.23\n2.47\n62\n68\n69.0\n71.0\n78.5\n▁▅▇▂▁\n\n\nmother\n0\n1\n64.08\n2.31\n58\n63\n64.0\n65.5\n70.5\n▂▅▇▃▁\n\n\nheight\n0\n1\n66.76\n3.58\n56\n64\n66.5\n69.7\n79.0\n▁▇▇▅▁\n\n\nnkids\n0\n1\n6.14\n2.69\n1\n4\n6.0\n8.0\n15.0\n▃▇▆▂▁\n\n\n\n\n\nWhat can we say about the dataset and its variables? How big is the dataset? How many variables? What types are they, Quant or Qual? If they are Qual, what are the levels? Are they ordered levels? Which variables could have relationships with others? Why? Write down these Questions!\n\nWhat Questions might we have, that we could answer with a Statistical Measure, or Correlation chart?\n\n\n\n\n\n\nQuestions\n\n\n\nHow does children’s height correlate with that of father and mother? Is this relationship also affected by sex of the child?\nWith this question, height becomes our target variable, which we should always plot on the dependent y-axis.\n\n\n\n# Pulling out the list of Quant variables from NHANES\ngalton_quant &lt;- galton_describe$quantitative\ngalton_quant$name\n\n[1] \"father\" \"mother\" \"height\" \"nkids\" \n\nGGally::ggpairs(\n  Galton,\n  \n  # Choose the variables we want to plot for\n  columns = c(\"father\", \"mother\", \"height\", \"nkids\"),\n  \n  switch = \"both\", # axis labels in more traditional locations\n  progress = FALSE, # no compute progress messages needed\n  \n  # Choose the diagonal graphs (always single variable! Think!)\n  diag = list(continuous = \"barDiag\"), # choosing histogram,not density\n  \n  # Choose lower triangle graphs, two-variable graphs\n  lower = list(continuous = wrap(\"smooth\", alpha = 0.1)),\n  \n  title = \"Galton Data Correlations Plot\"\n) + \n  \n  theme_bw()\n\n\n\n\nWe note that children’s height is correlated with that of father and mother. The correlations are both positive, and that with father seems to be the larger of the two. ( Look at the slopes of the lines and the values of the correlation scores. )\n\n\n\n\n\n\nQuestion\n\n\n\nWhat if we group the Quant variables based on a Qual variable, like sex of the child?\n\n\n\n# Pulling out the list of Quant variables from NHANES\ngalton_quant &lt;- galton_describe$quantitative\ngalton_quant$name\n\n[1] \"father\" \"mother\" \"height\" \"nkids\" \n\nGGally::ggpairs(\n  Galton,\n  \n  mapping = aes(colour = sex), # Colour by `sex`\n\n  # Choose the variables we want to plot for\n  columns = c(\"father\", \"mother\", \"height\", \"nkids\"),\n  switch = \"both\", # axis labels in more traditional locations\n  progress = FALSE, # no compute progress messages needed\n  \n  diag = list(continuous = \"barDiag\"),\n  \n  # Choose lower triangle graphs, two-variable graphs\n  lower = list(continuous = wrap(\"smooth\", alpha = 0.1)),\n  \n  title = \"Galton Data Correlations Plot\"\n) + \n  \n  theme_bw()\n\n\n\n\nThe split scatter plots are useful, as is the split histogram for height: Clearly the correlation of children’s height with father and mother is positive for both sex-es. The other plots, and even some of the correlations scores are not all useful! Just shows everything we can compute is not necessarily useful immediately.\nIn later modules we will see how to plot correlations when the number of variables is larger still.\n\n\n\n\n\n\nQuestion\n\n\n\nCan we plot a Correlogram for this dataset?\n\n\n\n#library(corrplot)\n\ngalton_num_var &lt;- Galton %&gt;% select(father, mother, height, nkids)\ngalton_cor &lt;- cor(galton_num_var)\ngalton_cor %&gt;%\n  corrplot(method = \"ellipse\",\n           type = \"lower\",\n           main = \"Correlogram for Galton dataset\")\n\n\n\n\nClearly height is positively correlated to father and mother; interestingly, height is negatively correlated ( slightly) with nkids.\n\n\n\n\n\n\nQuestion\n\n\n\nLet us confirm with a correlation test:\n\n\nWe will use the mosaic function cor_test to get these results:\n\nmosaic::cor_test(height ~ father, data = Galton) %&gt;% \n  broom::tidy() %&gt;% \n  knitr::kable(digits = 2,\n               caption = \"Children vs Fathers\")\n\n\nChildren vs Fathers\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n0.28\n8.57\n0\n896\n0.21\n0.33\nPearson’s product-moment correlation\ntwo.sided\n\n\n\n\n\nmosaic::cor_test(height ~ mother, data = Galton) %&gt;% \n  broom::tidy() %&gt;% \n    knitr::kable(digits = 2,\n               caption = \"Children vs Mothers\")\n\n\nChildren vs Mothers\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n0.2\n6.16\n0\n896\n0.14\n0.26\nPearson’s product-moment correlation\ntwo.sided\n\n\n\n\n\n\n\n\n\n\nCorrelation Scores and Uncertainty\n\n\n\nNote how the mosaic::cor_test() reports a correlation score estimate and the p-value for the same. There is also a confidence interval reported for the correlation score, an interval within which we are 95% sure that the true correlation value is to be found.\nNote that GGally::ggpairs() too reports the significance of the correlation scores estimates using *** or **. This indicates the p-value in the scores obtained by GGally; Presumably, there is an internal cor_test that is run for each pair of variables and the p-value and confidence levels are also computed internally.\n\n\nIn both cases, we used the formula \\(height \\sim other-variable\\), in keeping with our idea of height being the dependent, target variable..\nWe also see the p.value for the estimateed correlation is negligible, and the conf.low/conf.high interval does not straddle \\(0\\). These attest to the significance of the correlation score.\n\n\n\n\n\n\nQuestion\n\n\n\nWhat does this correlation look when split by sex of Child?\n\n\n\n# For the sons\n\nmosaic::cor_test(height ~ father,\n                 data = Galton %&gt;% filter(sex == \"M\")) %&gt;% \n  broom::tidy() %&gt;% knitr::kable(digits = 2,\n                                 caption = \"Sons vs Fathers\")\ncor_test(height ~ mother, \n         data = Galton %&gt;% filter(sex == \"M\")) %&gt;% \n  broom::tidy() %&gt;% knitr::kable(digits = 2,\n                                 caption = \"Sons vs Mothers\")\n\n# For the daughters\ncor_test(height ~ father, \n         data = Galton %&gt;% filter(sex == \"F\")) %&gt;% \n  broom::tidy() %&gt;% knitr::kable(digits = 2,\n                                 caption = \"Daughters vs Fathers\")\ncor_test(height ~ mother, \n         data = Galton %&gt;% filter(sex == \"F\")) %&gt;% \n  broom::tidy() %&gt;% knitr::kable(digits = 2,\n                                 caption = \"Daughters vs Mothers\")\n\n\nSons vs Fathers\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n0.39\n9.15\n0\n463\n0.31\n0.47\nPearson’s product-moment correlation\ntwo.sided\n\n\n\nSons vs Mothers\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n0.33\n7.63\n0\n463\n0.25\n0.41\nPearson’s product-moment correlation\ntwo.sided\n\n\n\nDaughters vs Fathers\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n0.46\n10.72\n0\n431\n0.38\n0.53\nPearson’s product-moment correlation\ntwo.sided\n\n\n\nDaughters vs Mothers\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n0.31\n6.86\n0\n431\n0.23\n0.4\nPearson’s product-moment correlation\ntwo.sided\n\n\n\n\nThe same observation as made above ( p.value and confidence intervals) applies here too and tells us that the estimated correlations are significant.\n\nWe can also visualize this uncertainty and the confidence levels in a plot too, using gf_errorbar and a handy set of functions within purrr which is part of the tidyverse. Assuming heights is the target variable we want to correlate every other (quantitative) variable against, we can proceed very quickly as follows: we will first plot correlation uncertainty for one pair of variables to develop the intuition, and then for all variables against the one target variable:\n\nmosaic::cor_test(height ~ mother, data = Galton) %&gt;% \n  broom::tidy() %&gt;% \n\n# We need a graph not a table \n# So comment out this line from the earlier code\n#knitr::kable(digits = 2,caption = \"Children vs Mothers\")\n\nrowid_to_column(var = \"index\") %&gt;% # Need an index to plot with\n  \n  # Uncertainty as error-bars\n  gf_errorbar(conf.high + conf.low ~ index, linewidth = 2) %&gt;% \n  \n  # Estimate as a point\n  gf_point(estimate ~ index, color = \"red\", size = 6) %&gt;% \n  \n  # Labels\n  gf_text(estimate ~ index - 0.2, \n             label = \"Correlation Score = estimate\") %&gt;% \n  gf_text(conf.high*0.98 ~ index - 0.25, \n           label = \"Upper Limit = estimate + conf.high\") %&gt;%   \n  gf_text(conf.low*1.04 ~ index - 0.25, \n           label = \"Lower Limit = estimate - conf.low\") %&gt;% \n  gf_theme(theme_bw())\n\n\n\n\nWe can now do this for all variables against the target variable height, which we identified in our research question. We will use the iteration capabilities offered by the tidyverse package, purrr:\n\nall_corrs &lt;- Galton %&gt;% \n  select(where(is.numeric)) %&gt;% \n  \n  # leave off height to get all the remaining ones\n  select(- height) %&gt;%  \n  \n  # perform a cor.test for all variables against height\n  purrr::map(.x = .,\n             .f = \\(x) cor.test(x, Galton$height)) %&gt;%\n  \n  # tidy up the cor.test outputs into a tidy data frame\n  map_dfr(broom::tidy, .id = \"predictor\") \n\nall_corrs\n\n\n\n  \n\n\nall_corrs %&gt;% \n  \n  # arrange the predictors in order of their correlation scores\n  # with the target variable (`height`)\n  # Add errorbars to show uncertainty ranges / confidence intervals\n  # Use errorbar width and linewidth fo emphasis\n  gf_errorbar(conf.high + conf.low ~ reorder(predictor, estimate),\n              color = ~ estimate,\n              width = 0.2,\n              linewidth = ~ -log10(p.value)) %&gt;% \n  \n  # All correlation estimates as points\n  gf_point(estimate ~ reorder(predictor, estimate), \n           color = \"black\") %&gt;% \n  \n  # Reference line at zero correlation score\n  gf_hline(yintercept = 0, color = \"grey\", linewidth = 2) %&gt;% \n  \n  # Themes,Titles, and Scales\n  gf_labs(x = NULL, y = \"Correlation with height in Galton\", \n          caption = \"Significance = - log10(p.value)\") %&gt;% \n  \n  gf_refine(\n    \n    # Scale for colour\n scale_colour_distiller(\"Correlation\", type = \"div\", palette = \"RdBu\"),\n            \n    # Scale for dumbbells!!\n    scale_linewidth_continuous(\"significance\",\n                                       range = c(0.5,4))) %&gt;% \n  gf_refine(guides(linewidth = guide_legend(reverse = TRUE))) %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\nWe can clearly see the size of the correlations and the confidence intervals marked in this plot. father has somewhat greater correlation with children’s height, as compared to mother. nkids seems to matter very little. This kind of plot will be very useful when we pursue linear regression models.\n\n\n\n\n\n\nQuestion\n\n\n\nHow can we show this correlation in a set of Scatter Plots + Regression Lines? Can we recreate Galton’s famous diagram?\n\n\n\n# For the sons\ngf_point(height ~ father, \n         data = Galton %&gt;% filter(sex == \"M\"),\n         title = \"Soms and Fathers\") %&gt;%\n  gf_smooth(method = \"lm\") %&gt;%\n  gf_theme(theme_minimal())\ngf_point(height ~ mother, \n         data = Galton %&gt;% filter(sex == \"M\"),\n         title = \"Sons and Mothers\") %&gt;%\n  gf_smooth(method = \"lm\") %&gt;%\n  gf_theme(theme_minimal())\n# For the daughters\ngf_point(height ~ father, \n         data = Galton %&gt;% filter(sex == \"F\"),\n         title = \"Daughters and Fathers\") %&gt;%\n  gf_smooth(method = \"lm\") %&gt;%\n  gf_theme(theme_minimal())\ngf_point(height ~ mother, \n         data = Galton %&gt;% filter(sex == \"F\"),\n         title = \"Daughters and Mothers\") %&gt;%\n  gf_smooth(method = \"lm\") %&gt;% \n  gf_theme(theme_minimal())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn approximation to Galton’s famous plot1 (see Wikipedia):\n\n\n\n\n\n\ngf_point(height ~ (father + mother)/2, data = Galton) %&gt;% \n  gf_smooth(method = \"lm\") %&gt;% \n  gf_density_2d(n = 8) %&gt;% \n  gf_abline(slope = 1) %&gt;% \n  gf_theme(theme_minimal())\ngf_point(height ~ (father + mother)/2, data = Galton) %&gt;% \n  gf_smooth(method = \"lm\") %&gt;% \n  gf_ellipse(level = 0.95, color = \"red\") %&gt;% \n    gf_ellipse(level = 0.75, color = \"blue\") %&gt;% \n    gf_ellipse(level = 0.5, color = \"green\") %&gt;% \n  gf_abline(slope = 1) %&gt;% \n  gf_theme(theme_minimal())\n\n\n\n\n\n\n\n\n\n\n\nHow would you interpret this plot2?"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#iconify-pajamas-issue-type-test-case-case-study2-dataset-from-nhanes",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#iconify-pajamas-issue-type-test-case-case-study2-dataset-from-nhanes",
    "title": "Tutorial on Correlations in R",
    "section": "",
    "text": "Let us look at the NHANES dataset from the package NHANES:\n\ndata(\"NHANES\")\n\n\n\nNHANES_describe &lt;- inspect(NHANES)\n\nNHANES_describe$categorical\nNHANES_describe$quantitative\nNHANES\nskimr::skim(NHANES)\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n\n\n\n\n  \n\n\n\n\n\n\nData summary\n\n\nName\nNHANES\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n76\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n31\n\n\nnumeric\n45\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nSurveyYr\n0\n1.00\nFALSE\n2\n200: 5000, 201: 5000\n\n\nGender\n0\n1.00\nFALSE\n2\nfem: 5020, mal: 4980\n\n\nAgeDecade\n333\n0.97\nFALSE\n8\n40: 1398, 0-: 1391, 10: 1374, 20: 1356\n\n\nRace1\n0\n1.00\nFALSE\n5\nWhi: 6372, Bla: 1197, Mex: 1015, Oth: 806\n\n\nRace3\n5000\n0.50\nFALSE\n6\nWhi: 3135, Bla: 589, Mex: 480, His: 350\n\n\nEducation\n2779\n0.72\nFALSE\n5\nSom: 2267, Col: 2098, Hig: 1517, 9 -: 888\n\n\nMaritalStatus\n2769\n0.72\nFALSE\n6\nMar: 3945, Nev: 1380, Div: 707, Liv: 560\n\n\nHHIncome\n811\n0.92\nFALSE\n12\nmor: 2220, 750: 1084, 250: 958, 350: 863\n\n\nHomeOwn\n63\n0.99\nFALSE\n3\nOwn: 6425, Ren: 3287, Oth: 225\n\n\nWork\n2229\n0.78\nFALSE\n3\nWor: 4613, Not: 2847, Loo: 311\n\n\nBMICatUnder20yrs\n8726\n0.13\nFALSE\n4\nNor: 805, Obe: 221, Ove: 193, Und: 55\n\n\nBMI_WHO\n397\n0.96\nFALSE\n4\n18.: 2911, 30.: 2751, 25.: 2664, 12.: 1277\n\n\nDiabetes\n142\n0.99\nFALSE\n2\nNo: 9098, Yes: 760\n\n\nHealthGen\n2461\n0.75\nFALSE\n5\nGoo: 2956, Vgo: 2508, Fai: 1010, Exc: 878\n\n\nLittleInterest\n3333\n0.67\nFALSE\n3\nNon: 5103, Sev: 1130, Mos: 434\n\n\nDepressed\n3327\n0.67\nFALSE\n3\nNon: 5246, Sev: 1009, Mos: 418\n\n\nSleepTrouble\n2228\n0.78\nFALSE\n2\nNo: 5799, Yes: 1973\n\n\nPhysActive\n1674\n0.83\nFALSE\n2\nYes: 4649, No: 3677\n\n\nTVHrsDay\n5141\n0.49\nFALSE\n7\n2_h: 1275, 1_h: 884, 3_h: 836, 0_t: 638\n\n\nCompHrsDay\n5137\n0.49\nFALSE\n7\n0_t: 1409, 0_h: 1073, 1_h: 1030, 2_h: 589\n\n\nAlcohol12PlusYr\n3420\n0.66\nFALSE\n2\nYes: 5212, No: 1368\n\n\nSmokeNow\n6789\n0.32\nFALSE\n2\nNo: 1745, Yes: 1466\n\n\nSmoke100\n2765\n0.72\nFALSE\n2\nNo: 4024, Yes: 3211\n\n\nSmoke100n\n2765\n0.72\nFALSE\n2\nNon: 4024, Smo: 3211\n\n\nMarijuana\n5059\n0.49\nFALSE\n2\nYes: 2892, No: 2049\n\n\nRegularMarij\n5059\n0.49\nFALSE\n2\nNo: 3575, Yes: 1366\n\n\nHardDrugs\n4235\n0.58\nFALSE\n2\nNo: 4700, Yes: 1065\n\n\nSexEver\n4233\n0.58\nFALSE\n2\nYes: 5544, No: 223\n\n\nSameSex\n4232\n0.58\nFALSE\n2\nNo: 5353, Yes: 415\n\n\nSexOrientation\n5158\n0.48\nFALSE\n3\nHet: 4638, Bis: 119, Hom: 85\n\n\nPregnantNow\n8304\n0.17\nFALSE\n3\nNo: 1573, Yes: 72, Unk: 51\n\n\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nID\n0\n1.00\n61944.64\n5871.17\n51624.00\n56904.50\n62159.50\n67039.00\n71915.00\n▇▇▇▇▇\n\n\nAge\n0\n1.00\n36.74\n22.40\n0.00\n17.00\n36.00\n54.00\n80.00\n▇▇▇▆▅\n\n\nAgeMonths\n5038\n0.50\n420.12\n259.04\n0.00\n199.00\n418.00\n624.00\n959.00\n▇▇▇▆▃\n\n\nHHIncomeMid\n811\n0.92\n57206.17\n33020.28\n2500.00\n30000.00\n50000.00\n87500.00\n100000.00\n▃▆▃▁▇\n\n\nPoverty\n726\n0.93\n2.80\n1.68\n0.00\n1.24\n2.70\n4.71\n5.00\n▅▅▃▃▇\n\n\nHomeRooms\n69\n0.99\n6.25\n2.28\n1.00\n5.00\n6.00\n8.00\n13.00\n▂▆▇▂▁\n\n\nWeight\n78\n0.99\n70.98\n29.13\n2.80\n56.10\n72.70\n88.90\n230.70\n▂▇▂▁▁\n\n\nLength\n9457\n0.05\n85.02\n13.71\n47.10\n75.70\n87.00\n96.10\n112.20\n▁▃▆▇▃\n\n\nHeadCirc\n9912\n0.01\n41.18\n2.31\n34.20\n39.58\n41.45\n42.92\n45.40\n▁▂▇▇▅\n\n\nHeight\n353\n0.96\n161.88\n20.19\n83.60\n156.80\n166.00\n174.50\n200.40\n▁▁▁▇▂\n\n\nBMI\n366\n0.96\n26.66\n7.38\n12.88\n21.58\n25.98\n30.89\n81.25\n▇▆▁▁▁\n\n\nPulse\n1437\n0.86\n73.56\n12.16\n40.00\n64.00\n72.00\n82.00\n136.00\n▂▇▃▁▁\n\n\nBPSysAve\n1449\n0.86\n118.15\n17.25\n76.00\n106.00\n116.00\n127.00\n226.00\n▃▇▂▁▁\n\n\nBPDiaAve\n1449\n0.86\n67.48\n14.35\n0.00\n61.00\n69.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nBPSys1\n1763\n0.82\n119.09\n17.50\n72.00\n106.00\n116.00\n128.00\n232.00\n▂▇▂▁▁\n\n\nBPDia1\n1763\n0.82\n68.28\n13.78\n0.00\n62.00\n70.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys2\n1647\n0.84\n118.48\n17.49\n76.00\n106.00\n116.00\n128.00\n226.00\n▃▇▂▁▁\n\n\nBPDia2\n1647\n0.84\n67.66\n14.42\n0.00\n60.00\n68.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys3\n1635\n0.84\n117.93\n17.18\n76.00\n106.00\n116.00\n126.00\n226.00\n▃▇▂▁▁\n\n\nBPDia3\n1635\n0.84\n67.30\n14.96\n0.00\n60.00\n68.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nTestosterone\n5874\n0.41\n197.90\n226.50\n0.25\n17.70\n43.82\n362.41\n1795.60\n▇▂▁▁▁\n\n\nDirectChol\n1526\n0.85\n1.36\n0.40\n0.39\n1.09\n1.29\n1.58\n4.03\n▅▇▂▁▁\n\n\nTotChol\n1526\n0.85\n4.88\n1.08\n1.53\n4.11\n4.78\n5.53\n13.65\n▂▇▁▁▁\n\n\nUrineVol1\n987\n0.90\n118.52\n90.34\n0.00\n50.00\n94.00\n164.00\n510.00\n▇▅▂▁▁\n\n\nUrineFlow1\n1603\n0.84\n0.98\n0.95\n0.00\n0.40\n0.70\n1.22\n17.17\n▇▁▁▁▁\n\n\nUrineVol2\n8522\n0.15\n119.68\n90.16\n0.00\n52.00\n95.00\n171.75\n409.00\n▇▆▃▂▁\n\n\nUrineFlow2\n8524\n0.15\n1.15\n1.07\n0.00\n0.48\n0.76\n1.51\n13.69\n▇▁▁▁▁\n\n\nDiabetesAge\n9371\n0.06\n48.42\n15.68\n1.00\n40.00\n50.00\n58.00\n80.00\n▁▂▆▇▂\n\n\nDaysPhysHlthBad\n2468\n0.75\n3.33\n7.40\n0.00\n0.00\n0.00\n3.00\n30.00\n▇▁▁▁▁\n\n\nDaysMentHlthBad\n2466\n0.75\n4.13\n7.83\n0.00\n0.00\n0.00\n4.00\n30.00\n▇▁▁▁▁\n\n\nnPregnancies\n7396\n0.26\n3.03\n1.80\n1.00\n2.00\n3.00\n4.00\n32.00\n▇▁▁▁▁\n\n\nnBabies\n7584\n0.24\n2.46\n1.32\n0.00\n2.00\n2.00\n3.00\n12.00\n▇▅▁▁▁\n\n\nAge1stBaby\n8116\n0.19\n22.65\n4.77\n14.00\n19.00\n22.00\n26.00\n39.00\n▆▇▅▂▁\n\n\nSleepHrsNight\n2245\n0.78\n6.93\n1.35\n2.00\n6.00\n7.00\n8.00\n12.00\n▁▅▇▁▁\n\n\nPhysActiveDays\n5337\n0.47\n3.74\n1.84\n1.00\n2.00\n3.00\n5.00\n7.00\n▇▇▃▅▅\n\n\nTVHrsDayChild\n9347\n0.07\n1.94\n1.43\n0.00\n1.00\n2.00\n3.00\n6.00\n▇▆▂▂▂\n\n\nCompHrsDayChild\n9347\n0.07\n2.20\n2.52\n0.00\n0.00\n1.00\n6.00\n6.00\n▇▁▁▁▃\n\n\nAlcoholDay\n5086\n0.49\n2.91\n3.18\n1.00\n1.00\n2.00\n3.00\n82.00\n▇▁▁▁▁\n\n\nAlcoholYear\n4078\n0.59\n75.10\n103.03\n0.00\n3.00\n24.00\n104.00\n364.00\n▇▁▁▁▁\n\n\nSmokeAge\n6920\n0.31\n17.83\n5.33\n6.00\n15.00\n17.00\n19.00\n72.00\n▇▂▁▁▁\n\n\nAgeFirstMarij\n7109\n0.29\n17.02\n3.90\n1.00\n15.00\n16.00\n19.00\n48.00\n▁▇▂▁▁\n\n\nAgeRegMarij\n8634\n0.14\n17.69\n4.81\n5.00\n15.00\n17.00\n19.00\n52.00\n▂▇▁▁▁\n\n\nSexAge\n4460\n0.55\n17.43\n3.72\n9.00\n15.00\n17.00\n19.00\n50.00\n▇▅▁▁▁\n\n\nSexNumPartnLife\n4275\n0.57\n15.09\n57.85\n0.00\n2.00\n5.00\n12.00\n2000.00\n▇▁▁▁▁\n\n\nSexNumPartYear\n5072\n0.49\n1.34\n2.78\n0.00\n1.00\n1.00\n1.00\n69.00\n▇▁▁▁▁\n\n\n\n\n\n\n\nTry help(\"NHANES\") in your Console.\n\nThis is survey data collected by the US National Center for Health Statistics (NCHS) which has conducted a series of health and nutrition surveys since the early 1960’s. Since 1999 approximately 5,000 individuals of all ages are interviewed in their homes every year and complete the health examination component of the survey. The health examination is conducted in a mobile examination centre (MEC).\n\nThe dataset is described as: A data frame with 100000 observations on 76 variables. Some of these are:\n- Race1 and Race2: factors with 5 and 6 levels respectively\n- Education a factor with 5 levels\n- HHIncomeMid Total annual gross income for the household in US dollars.\n- Age\n- BMI: Body mass index (weight/height2 in kg/m2)\n- Height: Standing height in cm.\n- Weight: Weight in kg &gt; &gt; - Testosterone: Testosterone total (ng/dL) - PhysActiveDays: Number of days in a typical week that participant does moderate or vigorous-intensity activity.\n- CompHrsDay: Number of hours per day on average participant used a computer or gaming device over the past 30 days.\n\n\n\n\n\n\nMissing Data\n\n\n\nWhy do so many of the variables have missing entries? What could be your guess about the Experiment/Survey`?\n\n\nLet us make some counts of the data, since we have so many factors:\n\nNHANES %&gt;% count(Gender)\n\n\n\n  \n\n\nNHANES %&gt;% count(Race1)\n\n\n\n  \n\n\nNHANES %&gt;% count(Race3)\n\n\n\n  \n\n\nNHANES %&gt;% count(Education)\n\n\n\n  \n\n\nNHANES %&gt;% count(MaritalStatus)\n\n\n\n  \n\n\n\nThere is a good mix of factors and counts.\nNow we articulate our Research Questions:\n\n\n\n\n\n\nResearch Question\n\n\n\n\nDoes Testosterone have a relationship with parameters such as BMI, Weight, Height, PhysActiveDays CompHrsDay and Age?\nDoes HHIncomeMid have a relationship with these same parameters? And with Gender?\nAre there any other pairwise correlations that we should note? (This is especially useful in choosing independent variables for multiple regression)\n\n\n\n( Yes we are concerned with men more than with the women, sadly.)\n\n\nGGally::ggpairs(NHANES, \n                # Choose the variables we want to plot for\n                columns = c(\"HHIncomeMid\", \"Weight\", \"Height\", \n                            \"BMI\", \"Gender\"), \n                \n                # LISTs of graphs needed at different locations\n                # For different combinations of variables \n                diag = list(continuous = \"barDiag\"),\n                lower = list(continuous = wrap(\"smooth\", alpha = 0.01)),\n                upper = list(continuous = \"cor\"),\n                \n                switch = \"both\", # axis labels in more traditional locations\n                progress = FALSE ) + # No compute progress bars needed\n  theme_bw()\n\n\n\n\nWe see that HHIncomeMid is Quantitative, discrete valued variable, since it is based on a set of median incomes for different ranges of income. BMI, Weight, Height are continuous Quant variables.\nHHIncomeMid also seems to be relatively unaffected by Weight; And is only mildly correlated with Height and BMI, as seen both by the correlation score magnitudes and the slopes of the trend lines.\nThere is a difference in the median income by Gender, but we will defer that test for later, when we do Statistical Inference.\nUnsurprisingly, BMI and Weight have a strong relationship, as do Height and Weight; the latter is of course non-linear, since the Height levels off at a point.\n\nGGally::ggpairs(NHANES, \n                columns = c(\"Testosterone\", \"Weight\", \"Height\", \"BMI\"), \n                \n                diag = list(continuous = \"barDiag\"),\n                lower = list(continuous = wrap(\"smooth\", alpha = 0.01)),\n                upper = list(continuous = \"cor\"),\n                \n                switch = \"both\",\n                progress = FALSE ) +\n  theme_bw()\n\n\n\n\nIt is clear that Testosterone has strong relationships with Height and Weight but not so much with BMI.\n\nSince the pairs plot is fairly clear for both target variables, let us head to visualizing the significance and uncertainty in the correlation estimates.\n\nHHIncome_corrs &lt;- NHANES %&gt;% \n  select(where(is.numeric)) %&gt;% \n  \n  # leave off height to get all the remaining ones\n  select(- HHIncomeMid) %&gt;%  \n  \n  # perform a cor.test for all variables against height\n  purrr::map(.x = .,\n             .f = \\(x) cor.test(x, NHANES$HHIncomeMid)) %&gt;%\n  \n  # tidy up the cor.test outputs into a tidy data frame\n  map_dfr(broom::tidy, .id = \"predictor\") \n\nHHIncome_corrs\n\n\n\n  \n\n\nHHIncome_corrs %&gt;% \n  \n  # arrange the predictors in order of their correlation scores\n  # with the target variable (`height`)\n  # Add errorbars to show uncertainty ranges / confidence intervals\n  # Use errorbar width and linewidth fo emphasis\n  gf_errorbar(conf.high + conf.low ~ reorder(predictor, estimate),\n              color = ~ -log10(p.value + 0.001),\n              width = 0.2,\n              linewidth = ~ -log10(p.value + 0.001)) %&gt;% \n  \n  # All correlation estimates as points\n  gf_point(estimate ~ reorder(predictor, estimate), \n           color = \"black\") %&gt;% \n  \n  # Reference line at zero correlation score\n  gf_hline(yintercept = 0, color = \"grey\", linewidth = 2) %&gt;% \n  \n  # Themes,Titles, and Scales\n  gf_labs(x = NULL, y = \"Correlations with HouseHold Median Income\", \n          caption = \"Significance = - log10(p.value)\") %&gt;% \n  \n  gf_refine(\n    # Scale for colour\n    scale_colour_gradient(\"significance\",low = \"purple\", \n                                 high = \"red\"),\n            \n    # Scale for dumbbells!!\n    scale_linewidth_continuous(\"significance\",\n                                       range = c(0.5,4))) %&gt;% \n  gf_theme(theme_classic()) %&gt;% \n  gf_refine(theme(axis.text.x = element_text(angle = 60, size = 6, hjust = 1)))\n\n\n\n\nIf we select just the variables from our Research Question:\n\nHHIncome_corrs_select &lt;- NHANES %&gt;% \n  select(Height, Weight, BMI) %&gt;% # Only change is here!\n  \n  # leave off height to get all the remaining ones\n  #select(- HHIncomeMid) %&gt;%  \n  \n  # perform a cor.test for all variables against height\n  purrr::map(.x = .,\n             .f = \\(x) cor.test(x, NHANES$HHIncomeMid)) %&gt;%\n  \n  # tidy up the cor.test outputs into a tidy data frame\n  map_dfr(broom::tidy, .id = \"predictor\") \n\nHHIncome_corrs_select\n\n\n\n  \n\n\nHHIncome_corrs_select %&gt;% \n  \n  # arrange the predictors in order of their correlation scores\n  # with the target variable (`height`)\n  # Add errorbars to show uncertainty ranges / confidence intervals\n  # Use errorbar width and linewidth fo emphasis\n  gf_errorbar(conf.high + conf.low ~ reorder(predictor, estimate),\n              color = ~ -log10(p.value + 0.001),\n              width = 0.2,\n              linewidth = ~ -log10(p.value + 0.001)) %&gt;% \n  \n  # All correlation estimates as points\n  gf_point(estimate ~ reorder(predictor, estimate), \n           color = \"black\") %&gt;% \n  \n  # Reference line at zero correlation score\n  gf_hline(yintercept = 0, color = \"grey\", linewidth = 2) %&gt;% \n  \n  # Themes,Titles, and Scales\n  gf_labs(x = NULL, y = \"Correlations with HouseHold Median Income\", \n          caption = \"Significance = - log10(p.value)\") %&gt;% \n  \n  gf_refine(\n    # Scale for colour\n    scale_colour_gradient(\"significance\",low = \"purple\", \n                                 high = \"red\"),\n            \n    # Scale for dumbbells!!\n    scale_linewidth_continuous(\"significance\",\n                                       range = c(0.5,4))) %&gt;% \n  gf_theme(theme_classic()) %&gt;% \n  gf_refine(theme(axis.text.x = element_text(angle = 60, size = 10, hjust = 1)))\n\n\n\n\nSo we might say taller people make more money? And fatter people make slightly less money?\nLet us look at the Testosterone variable: trying all variables shows some paucity of observations ( due to missing data), so we will stick with our chosen variables:\n\nTestosterone_corrs &lt;- NHANES %&gt;% \n  select(Height, Weight, BMI) %&gt;% \n  \n  # leave off height to get all the remaining ones\n  #select(- Testosterone) %&gt;%  \n  \n  # perform a cor.test for all variables against height\n  purrr::map(.x = .,\n             .f = \\(x) cor.test(x, NHANES$Testosterone)) %&gt;%\n  \n  # tidy up the cor.test outputs into a tidy data frame\n  map_dfr(broom::tidy, .id = \"predictor\") \n\nTestosterone_corrs\n\n\n\n  \n\n\nTestosterone_corrs %&gt;% \n  \n  # arrange the predictors in order of their correlation scores\n  # with the target variable (`height`)\n  # Add errorbars to show uncertainty ranges / confidence intervals\n  # Use errorbar width and linewidth fo emphasis\n  gf_errorbar(conf.high + conf.low ~ reorder(predictor, estimate),\n              color = ~ -log10(p.value + 0.001),\n              width = 0.2,\n              linewidth = ~ -log10(p.value + 0.001)) %&gt;% \n  \n  # All correlation estimates as points\n  gf_point(estimate ~ reorder(predictor, estimate), \n           color = \"black\") %&gt;% \n  \n  # Reference line at zero correlation score\n  gf_hline(yintercept = 0, color = \"grey\", linewidth = 2) %&gt;% \n  \n  # Themes,Titles, and Scales\n  gf_labs(x = NULL, y = \"Correlations with Testosterone Levels\",\n          caption = \"Significance = - log10(p.value)\") %&gt;% \n  \n  gf_refine(\n    # Scale for colour\n    scale_colour_gradient(\"significance\",low = \"purple\", \n                                 high = \"red\"),\n            \n    # Scale for dumbbells!!\n    scale_linewidth_continuous(\"significance\",\n                                       range = c(0.5,4))) %&gt;% \n  gf_theme(theme_classic()) %&gt;% \n  gf_refine(theme(axis.text.x = element_text(angle = 60, size = 10, hjust = 1)))"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#iconify-fluent-mdl2-decision-solid-conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#iconify-fluent-mdl2-decision-solid-conclusion",
    "title": "Tutorial on Correlations in R",
    "section": "",
    "text": "We have a decent Correlations related workflow in R:\n\nLoad the dataset\n\ninspect the dataset, identify Quant and Qual variables\nDevelop Pair-Wise plots + Correlations using GGally::ggpairs()\n\nDevelop Correlogram corrplot::corrplot\n\nCheck everything with a cor_test\n\nUse purrr + cor.test to plot correlations and confidence intervals for multiple Quant variables\nPlot scatter plots using gf_point.\nAdd extra lines using gf_abline() to compare hypotheses that you may have."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#footnotes",
    "title": "Tutorial on Correlations in R",
    "section": "Footnotes",
    "text": "Footnotes\n\n,http://euclid.psych.yorku.ca/SCS/Gallery/images/galton-corr.jpg&gt;↩︎\nhttps://www.researchgate.net/figure/Galtons-smoothed-correlation-diagram-for-the-data-on-heights-of-parents-and-children_fig15_226400313↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html",
    "title": "📊 Distributions",
    "section": "",
    "text": "R (Static Viz)  \n  R (Interactive Viz)\n\n  Radiant Tutorial \n  Datasets"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#fa-folder-open-slides-and-tutorials",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#fa-folder-open-slides-and-tutorials",
    "title": "📊 Distributions",
    "section": "",
    "text": "R (Static Viz)  \n  R (Interactive Viz)\n\n  Radiant Tutorial \n  Datasets"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#iconify-noto-v1-package-setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#iconify-noto-v1-package-setting-up-r-packages",
    "title": "📊 Distributions",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\n```{r}\n#| label: setup\n#| include: true\n#| message: false\n#| warning: false\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(palmerpenguins)\nlibrary(ggformula)\nlibrary(ggridges)\n\nggplot2::theme_set(theme_classic())\n```"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#iconify-fxemoji-japanesesymbolforbeginner-what-graphs-will-we-see-today",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#iconify-fxemoji-japanesesymbolforbeginner-what-graphs-will-we-see-today",
    "title": "📊 Distributions",
    "section": "\n What graphs will we see today?",
    "text": "What graphs will we see today?\nSome of the very basic and commonly used plots for data are:\n\nBar and Column Charts\nHistograms and Frequency Distributions\nBox Plots\n2D Hexbins Plots and 2D Frequency Distributions\nRidge Plots ( Quant + Qual variables)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#iconify-icon-park-outline-chart-histogram-histograms-and-frequency-distributions",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#iconify-icon-park-outline-chart-histogram-histograms-and-frequency-distributions",
    "title": "📊 Distributions",
    "section": "\n Histograms and Frequency Distributions",
    "text": "Histograms and Frequency Distributions\nHistograms are best to show the distribution of raw quantitative data,by displaying the number of values that fall within defined ranges, often called buckets or bins.\nAlthough histograms may look similar to column charts, the two are different. First, histograms show continuous data, and usually you can adjust the bucket ranges to explore frequency patterns. For example, you can shift histogram buckets from 0-1, 1-2, 2-3, etc. to 0-2, 2-4, etc. By contrast, column charts show categorical data, such as the number of apples, bananas, carrots, etc. Second, histograms do not usually show spaces between buckets because these are continuous values, while column charts show spaces to separate each category.\nLet us listen to the late great Hans Rosling from the Gapminder Project, which aims at telling stories of the world with data, to remove systemic biases about poverty, income and gender related issues.\n\n\n Examine the Data\nLet us look at the popular mpg dataset ( from R ) using mosaic::inspect(). We get two new pieces of output (i.e. two new dataframes), describing the Qual and Quant variables separately:\n\n```{r}\n#| label: mpg-inspect-1\ninspect(mpg)\n```\n\n\ncategorical variables:  \n          name     class levels   n missing\n1 manufacturer character     15 234       0\n2        model character     38 234       0\n3        trans character     10 234       0\n4          drv character      3 234       0\n5           fl character      5 234       0\n6        class character      7 234       0\n                                   distribution\n1 dodge (15.8%), toyota (14.5%) ...            \n2 caravan 2wd (4.7%) ...                       \n3 auto(l4) (35.5%), manual(m5) (24.8%) ...     \n4 f (45.3%), 4 (44%), r (10.7%)                \n5 r (71.8%), p (22.2%), e (3.4%) ...           \n6 suv (26.5%), compact (20.1%) ...             \n\nquantitative variables:  \n   name   class    min     Q1 median     Q3  max        mean       sd   n\n1 displ numeric    1.6    2.4    3.3    4.6    7    3.471795 1.291959 234\n2  year integer 1999.0 1999.0 2003.5 2008.0 2008 2003.500000 4.509646 234\n3   cyl integer    4.0    4.0    6.0    8.0    8    5.888889 1.611534 234\n4   cty integer    9.0   14.0   17.0   19.0   35   16.858974 4.255946 234\n5   hwy integer   12.0   18.0   24.0   27.0   44   23.440171 5.954643 234\n  missing\n1       0\n2       0\n3       0\n4       0\n5       0\n\n\n\n\nWe can save and see the outputs separately:\n\n```{r mpg-inspect-2, eval=FALSE}\nmpg_describe &lt;- inspect(mpg)\n\nmpg_describe$categorical\nmpg_describe$quantitative\n```\n\nThere is a lot of Description generated by the mosaic::inspect() command ! What can we say about the dataset and its variables? How big is the dataset? How many variables? What types are they, Quant or Qual? If they are Qual, what are the levels? Are they ordered levels? Discuss!\n\n\n\n\n\n\nTip\n\n\n\nNote the standard method for all commands from the mosaic package:\ngoal( y ~ x | z, data = mydata, …)\nWith ggformula, one can create any graph/chart using:\ngf_geometry(y ~ x | z, data = mydata)\nOR\nmydata %&gt;% gf_geometry( y ~ x | z)\nThe second method may be preferable, especially if you have done some data manipulation first! More later!\n\n\n\n Some Sample Charts for Quant Data Distributions\n\n```{r}\n#| label: charts\n#| layout-ncol: 2\n\nmpg &lt;- mpg %&gt;% mutate(drv= as_factor(drv))\n\ngf_histogram(~ hwy, data = mpg) %&gt;%\n  gf_labs(title = \"Histogram of Highway Mileage\")\ngf_histogram(~ hwy, fill = ~ drv, alpha = 0.3, data = mpg) %&gt;%\n  gf_labs(title = \"Histogram of Highway Mileage by Drive Train\")\n\ngf_density(~ hwy, data = mpg) %&gt;% \n  gf_labs(title = \"Frequency Density of Highway Mileage\")\ngf_density(~ hwy, fill = ~ drv, data = mpg) %&gt;% \n  gf_labs(title = \"Frequency Density by Drive Train\")\n\n\ngf_boxplot(hwy ~ drv, fill = ~ drv, data = mpg) %&gt;% \n  gf_labs(title = \"Boxplot\")\n\ngf_violin(hwy ~ drv, fill = ~ drv, data = mpg) %&gt;% \n    gf_labs(title = \"Violin\")\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```{r}\n#| label: more-charts\n#| layout-nrow: 2\n\nmpg &lt;- mpg %&gt;% mutate(drv= as_factor(drv))\n\ngf_density_ridges(drv ~ hwy, fill = ~ drv, alpha = 0.3, \n                        rel_min_height = 0.005, data = mpg) %&gt;% \n  gf_refine(scale_y_discrete(expand = c(0.01, 0)),\n            scale_x_continuous(expand = c(0.01, 0))) %&gt;% \n  gf_theme(theme_minimal()) %&gt;% \n  gf_labs(title = \"Ridge Plot\")\n\ngf_hex(cty ~ hwy, fill = ~ drv, data = mpg) %&gt;% \n  gf_theme(theme_minimal()) %&gt;% \n  gf_labs(title = \"Hex Bin Plot\")\n\ngf_density_2d(cty ~ hwy, data = mpg) %&gt;% \n  gf_theme(theme_minimal()) %&gt;% \n    gf_labs(title = \"2D Density Plot\")\n```"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#iconify-fluent-mdl2-decision-solid-conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#iconify-fluent-mdl2-decision-solid-conclusion",
    "title": "📊 Distributions",
    "section": "\n Conclusion",
    "text": "Conclusion\n\nHistograms, Frequency Distributions, and Box Plots are used for Quantitative data variables\nHistograms “dwell upon” counts, ranges, means and standard deviations\n\nFrequency Density plots “dwell upon” probabilities and densities\n\nBox Plots “dwell upon” medians and Quartiles\n\nQualitative data variables can be plotted as counts, using Bar Charts, or using Heat Maps\n2D density plots are used for describing two quant variables\nRidge Plots are density plots used for describing one Quant and one Qual variable (by inherent splitting)\nWe can split all these plots on the basis of another Qualitative variable.(Ridge Plots are already split)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#your-turn",
    "title": "📊 Distributions",
    "section": "Your Turn",
    "text": "Your Turn\n  Datasets\n\nClick on the Dataset Icon above, and unzip that archive. Try to make distribution plots with each of the three tools.\nA dataset from calmcode.io https://calmcode.io/datasets.html\n\nOld Faithful Data in R (sFind it!)\n\ninspect the dataset in each case and develop a set of Questions, that can be answered by appropriate stat measures, or by using a chart to show the distribution."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#iconify-ooui-references-rtl-references",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#iconify-ooui-references-rtl-references",
    "title": "📊 Distributions",
    "section": "\n References",
    "text": "References\n\nSee the scrolly animation for a histogram at this website: Exploring Histograms, an essay by Aran Lunzer and Amelia McNamara https://tinlizzie.org/histograms/?s=09\n\nMinimal R using mosaic. https://cran.r-project.org/web/packages/mosaic/vignettes/MinimalRgg.pdf\n\nPlotting multiple plots using purrr::map and ggplot. Sebastian Sauer"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions-interactive.html",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions-interactive.html",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "",
    "text": "# options(tibble.print_min = 4L, tibble.print_max = 4L,digits = 3)\nlibrary(tidyverse)\nlibrary(mosaic) # package for stats, simulations, and basic plots\nlibrary(mosaicData) # package containing datasets\nlibrary(ggformula) # package for professional looking plots, that use the formula interface from mosaic\nlibrary(skimr) # Summary statistics about variables in data frames\nlibrary(NHANES) # survey data collected by the US National Center for Health Statistics (NCHS)\n\nlibrary(echarts4r) # Interactive graphs using Javascript in R\nlibrary(plotly) # An older more established package for interactive graphs using Javascript in R"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions-interactive.html#iconify-noto-v1-package-setup-the-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions-interactive.html#iconify-noto-v1-package-setup-the-packages",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "",
    "text": "# options(tibble.print_min = 4L, tibble.print_max = 4L,digits = 3)\nlibrary(tidyverse)\nlibrary(mosaic) # package for stats, simulations, and basic plots\nlibrary(mosaicData) # package containing datasets\nlibrary(ggformula) # package for professional looking plots, that use the formula interface from mosaic\nlibrary(skimr) # Summary statistics about variables in data frames\nlibrary(NHANES) # survey data collected by the US National Center for Health Statistics (NCHS)\n\nlibrary(echarts4r) # Interactive graphs using Javascript in R\nlibrary(plotly) # An older more established package for interactive graphs using Javascript in R"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions-interactive.html#iconify-fxemoji-japanesesymbolforbeginner-introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions-interactive.html#iconify-fxemoji-japanesesymbolforbeginner-introduction",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Introduction",
    "text": "Introduction\nWe will query our dataset, developing insights and new questions as each Table or Bar/Histogram chart yields new information. This process of exploration is iterative, structured, and intuitive. Intermediate results may on occasion be messy or not very insightful!\nWe will consistently use the Project Mosaic ecosystem of packages in R (mosaic, mosaicData and ggformula).\n\n\n\n\n\n\nFormula Interface\n\n\n\nNote the standard method for all commands from the mosaic package:goal( y ~ x | z, data = mydata, …) With ggformula, one can create any graph/chart using:gf_geometry(y ~ x | z, data = mydata)\nORmydata %&gt;% gf_geometry( y ~ x | z)\nThe second method may be preferable, especially if you have done some data manipulation first! More later! ggformula supports many types of plots (using geometry), such as scatter, bar, histogram, density, boxplots, maps and many other statistical plots.\n\n\n\n\n\n\n\n\nInteractive Graphs with echarts4r\n\n\n\nWe will also start using echarts4r side by side for interactive graphs.\n\nEvery function in the package starts with e_.\nYou start coding a visualization by creating an echarts object with the e_charts() function. That takes your data frame and x-axis column as arguments.\nNext, you add a function for the type of chart (e_line(), e_bar(), etc.) with the y-axis series column name as an argument.\nThe rest is mostly customization! echarts4r takes some effort in getting used to, but it totally worth it!\n\n\n\nThe website for echarts4r is https://echarts4r.john-coene.com/articles/get_started.html. You should also quickly view this short introductory video on echarts4r:"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions-interactive.html#iconify-pajamas-issue-type-test-case-case-study-1-galton-dataset-from-mosaicdata",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions-interactive.html#iconify-pajamas-issue-type-test-case-case-study-1-galton-dataset-from-mosaicdata",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Case Study-1: Galton Dataset from mosaicData\n",
    "text": "Case Study-1: Galton Dataset from mosaicData\n\nLet us choose the famous Galton dataset:\n\ndata(\"Galton\")\nGalton &lt;- as_tibble(Galton)\n\n\n Look at the Data:\n\nskim(Galton)\n\n\nData summary\n\n\nName\nGalton\n\n\nNumber of rows\n898\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nfamily\n0\n1\nFALSE\n197\n185: 15, 166: 11, 66: 11, 130: 10\n\n\nsex\n0\n1\nFALSE\n2\nM: 465, F: 433\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nfather\n0\n1\n69.23\n2.47\n62\n68\n69.0\n71.0\n78.5\n▁▅▇▂▁\n\n\nmother\n0\n1\n64.08\n2.31\n58\n63\n64.0\n65.5\n70.5\n▂▅▇▃▁\n\n\nheight\n0\n1\n66.76\n3.58\n56\n64\n66.5\n69.7\n79.0\n▁▇▇▅▁\n\n\nnkids\n0\n1\n6.14\n2.69\n1\n4\n6.0\n8.0\n15.0\n▃▇▆▂▁\n\n\n\n\n\nWhat can we say about the dataset and its variables? How big is the dataset? How many variables? What types are they, Quant or Qual? What are the means, medians and inter-quartile ranges for the Quant variables? If they are Qual, what are the levels? Are they ordered levels?\nThere is a lot of Description generated by the skimr::skim command (and equivalently by the mosaic::inspect() command)! Try both and see which output suits you. The first table above describes the Qual variables: family and sex. The second table describes the Quant variables, and gives us their statistical summaries as well and a neat little histogram to boot. The data are described as: Type help(Galton) in your Console\n\nA data frame with 898 observations on the following variables.\n\n\nfamily an ID for each family, a factor with levels for each family\n\nfather the father’s height (in inches)\n\nmother the mother’s height (in inches)\n\nsex the child’s sex: F or M\n\nheight the child’s height as an adult (in inches)\n\nnkids the number of adult children in the family, or, at least, the number whose heights Galton recorded.\n\n\n\n Counts, and Charts with Counts\nNow that we know the variables, let us look at counts of data observations(rows). We know from our examination of variable types that counting of observations must be done on the basis of Qualitative variables. So let us count and plot the counts in bar charts.\n\n\n\n\n\n\nQuestion\n\n\n\nQ.1 How many families in the data for each value of nkids(i.e. Count of families by size)?\n\n\n\n\nComputations\nUsing ggformula\nUsing echarts4r\nUsing plotly\n\n\n\n\nGalton_counts &lt;- Galton %&gt;%\n  group_by(nkids) %&gt;% \n  summarise(children = n()) %&gt;% \n  # just to check\n  mutate(\n    No_of_families = as.integer(children/nkids),\n    # Why do we divide\n    \n    running_count_of_children = cumsum(children),\n    running_count_of_families = cumsum(No_of_families)) \nGalton_counts\n\n\n\n  \n\n\n\n\n\n\nGalton_counts %&gt;% \n  gf_col(No_of_families ~ nkids) %&gt;% \n  gf_theme(theme_classic())\n\n\n\n\n\n\n\nGalton_counts %&gt;% \n  e_charts(nkids) %&gt;% \n  e_bar(No_of_families,\n        colorBy = \"data\",\n        legend = FALSE) %&gt;% # Or \"series\"\n  \n# https://echarts4r.john-coene.com/articles/grid.html\n# echarts4r does not \"automatically\" name the axes!\n# And look at the \"categorical\" x-axis below!\n  e_x_axis(name = \"Family Size\", nameLocation = \"center\", \n           nameGap = 25, type = \"category\") %&gt;% \n  e_y_axis(name = \"Count\",nameLocation = \"center\", nameGap = 25,) %&gt;% \n  \n  e_tooltip(trigger = \"item\") %&gt;% \n  e_title(\"No of Families of each size\")\n\n\n\n\n\n\n\n\nGalton_counts %&gt;% \n  plot_ly(x = ~nkids, y = ~No_of_families) %&gt;% \n  add_bars()\n\n\n\n\n\n\n\n\nInsight: There are 32 1-kid families; and \\(128/8 = 16\\) 8-kid families! There is one great great 15-kid family. (Did you get the idea behind why we divide here?)\n\n\n\n\n\n\nQuestion\n\n\n\nQ.2. What is the count of Children by sex of the child and by family size nkids?\n\n\n\n\nUsing ggformula\nUsing echarts4r\n\n\n\n\n\n\n\nGalton_counts_by_sex &lt;- Galton %&gt;% \n  mutate(family = as.integer(family)) %&gt;% \n  group_by(nkids, sex) %&gt;% \n  summarise(count_by_sex = n()) %&gt;% \n  ungroup() %&gt;% \n  group_by(sex) \nGalton_counts_by_sex\n\n\n\n  \n\n\nGalton_counts_by_sex%&gt;% \n  e_charts(nkids) %&gt;% \n  e_bar(count_by_sex) %&gt;% \n\n  e_x_axis(name = \"Family Size (nkids)\", nameLocation = \"center\",\n           nameGap = 20, type = \"category\") %&gt;%\n  e_y_axis(name = \"How Many Children?\",\n           nameGap = 20,\n           nameTextStyle = list(align = \"center\"),\n           nameLocation = \"center\") %&gt;%\n  e_legend(right = 25, orient = \"vertical\") %&gt;% \n  e_facet(cols = 2,rows = 1) %&gt;% \n  e_tooltip(trigger = \"item\") %&gt;% \n  e_title(\"Child Counts by Sex over Family Size\")\n\n\n\n\n\n\n\n\nInsight: Hmm…decent gender balance overall, across family sizes nkids.\n\n\n\n\n\n\nFollow-up Question\n\n\n\nFollow up Question: How would we look for “gender balance” in individual families? Should we look at the family column ?\n\n\n\nGalton %&gt;% \n  mutate(family = as.integer(family)) %&gt;% \n  group_by(family, sex) %&gt;% \n  summarise(count_by_sex = n()) %&gt;% \n  ungroup() %&gt;% \n  group_by(sex) %&gt;% \n  \n  e_charts(family) %&gt;% \n  e_bar(count_by_sex) %&gt;% \n  \n  e_x_axis(name = \"nkids\", nameLocation = \"center\",\n           nameGap = 25, type = \"category\") %&gt;% \n  e_y_axis(name = \"How Many Children?\", \n           nameGap = 25, nameLocation = \"center\") %&gt;% \n  e_legend(right = 5) %&gt;% \n  e_facet(cols = 2,rows = 1) %&gt;% \n  e_tooltip(trigger = \"item\") %&gt;% \n  e_title(\"Child Counts by Sex over Family ID\")\n\n\n\n\n\nInsight: The No of Children were distributed similarly across family sizenkids… However, this plot is too crowded and does not lead to any great insight. Using family ID was silly to plot against, wasn’t it? Not all exploratory plots will be “necessary” in the end. But they are part of the journey of getting better acquainted with the data!\n\n Stat Summaries and Distributions\nOK, on to the Quantitative variables now! What Questions might we have, that could relate not to counts by Qual variables, but to the numbers in Quant variables. Stat measures, like their ranges, max and min? Means, medians, distributions? And how these vary on the basis of Qual variables? All this using histograms and densities.\n\n\n\n\n\n\nSummary Stats\n\n\n\nAs Stigler(Stigler 2016) said, summaries are the first thing to look at in data. skimr::skim has already given us a lot summary data for Quant variables. We can now use mosaic::favstats to develop these further, by slicing / facetting these wrt other Qual variables. Let us tabulate some quick stat summaries of the important variables in Galton.\n\n\n\n# summaries facetted by sex of child\nmeasures &lt;- favstats(~ height | sex, data = Galton)\nmeasures\n\n\n\n  \n\n\n\nInsight: We saw earlier that the mean height of the Children was 66 inches. However, are Sons taller than Daughters? Difference in mean height is 5 inches! AND…that was the same difference between fathers and mothers mean heights! Is it so simple then?\n\n\n\n\n\n\nQuestion\n\n\n\nQ.4 How are the heights of the children distributed? Here is where we need a e_histogram…\n\n\n\nGalton %&gt;% \n  e_charts() %&gt;% \n  e_histogram(serie = height) %&gt;% \n  e_tooltip(trigger = \"item\") %&gt;% \n  \n  e_mark_line(data = list(xAxis = mean(Galton$height)),\n              label = list(label = \"Mean Height\",\n                           label.position = \"end\"),\n              lineStyle = list(color = \"red\", width = 1.5, \n                               type = \"solid\")) %&gt;% \n# See https://echarts.apache.org/en/option.html#series-line.markLine\n\n  e_x_axis(name = \"Height\", nameLocation = \"center\") %&gt;% \n  e_y_axis(name = \"Counts\", nameLocation = \"center\", nameGap = 30) %&gt;% \n  e_title(\"Distribution of Heights in Galton\")\n\n\n\n\n\nInsight: Fairly symmetric distribution…but there are a few very short and some very tall children! Try to change the no. of bins to check of we are missing some pattern. This is not completely easy with echarts4r which uses the “Sturges” algorithm to set the number of bins. Need to figure this out from the echarts Apache API docs.\n\n\n\n\n\n\nQuestion\n\n\n\nQ.5 Is there a difference in height distributions between Male and Female children?(Quant variable sliced by Qual variable)\n\n\nWe will use the raw Galton data and previously-computed measures:\n\nGalton %&gt;%\n  group_by(sex) %&gt;% \n  e_charts(height = 300) %&gt;% \n  e_density(height) %&gt;% \n  e_mark_line(data = list(xAxis = measures %&gt;% filter(sex == \"M\") %&gt;% \n                            select(mean) %&gt;% as.numeric()),\n  # This code colours both v-lines red...how?\n              lineStyle = list(color = \"red\", width = 1.5, \n                               type = \"solid\")) %&gt;% \n  # Upto here gives one line in red colour, correctly\n\n  e_mark_line(data = list(xAxis = measures %&gt;% \n                            filter(sex == \"F\") %&gt;%\n                            select(mean) %&gt;% as.numeric()),\n              \n  # This piece of code has no effect...wonder why not?\n  # BOTH lines are in red ...why??\n              lineStyle = list(color = \"black\", width = 1.5, \n                               type = \"solid\")) %&gt;% \n  e_title(\"Distributions of Height by Sex in Galton\") %&gt;% \n  e_x_axis(name = \"Height\", nameLocation = \"center\") %&gt;% \n  e_legend(right = 5)\n\n\n\n\n\nInsight: There is a visible difference in average heights between girls and boys. Is that significant, however? We will need a statistical inference test to figure that out!! Claus Wilke1 says comparisons of Quant variables across groups are best made between densities and not histograms…\n\n\n\n\n\n\nQuestion\n\n\n\nQ.6 Are Mothers generally shorter than fathers?\n\n\n\nGalton %&gt;% \n  e_charts(height = 300) %&gt;% \n  e_density(father) %&gt;% \n  e_density(mother) %&gt;% \n  e_mark_line(data = list(xAxis = mean(Galton$mother)),\n              lineStyle = list(color = \"red\", width = 1.5, \n                               type = \"solid\")) %&gt;% \n  e_mark_line(data = list(xAxis = mean(Galton$father),\n              lineStyle = list(color = \"black\", width = 1.5, \n                               type = \"solid\"))) %&gt;% \n  e_legend(right = 10)\n\n\n\n\n\nInsight: Yes, moms are on average shorter than dads in this dataset. Again, is this difference statistically significant? We will find out in when we do Inference.\n\n\n\n\n\n\nQuestion\n\n\n\nQ.7a. Are heights of children different based on the number of kids in the family? And For Male and Female children?\n\n\n\nGalton %&gt;%\n  group_by(nkids) %&gt;% \n  e_charts(height = 400) %&gt;% \n  e_boxplot(height,colorBy = \"data\",\n            itemStyle = list(borderWidth = 3)) %&gt;% \n  e_y_axis(max = 80, min = 50, name = \"height\", nameLocation = \"center\",\n           nameGap = 25, margin = 5) %&gt;% # adds +/- 5 to y-axis limits\n  \n e_x_axis(name = \"Family Size\", \n           nameLocation = \"center\", \n           nameGap = 25, type = \"category\") %&gt;% # makes a category axis showing factors\n\n  e_tooltip() %&gt;% \n  e_title(\"Heights over Family Size\")\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.7b. Are heights of children different for Male and Female children?\n\n\n\n# Can do better at colouring/filling and facetting...\nGalton %&gt;%\n  group_by(nkids,sex) %&gt;% \n  e_charts(height = 400) %&gt;% # no x-variable needed for boxplots\n  e_boxplot(height,colorBy = \"data\",\n            itemStyle = list(borderWidth = 3)) %&gt;% \n  e_y_axis(max = 80, min = 50, name = \"height\", nameLocation = \"center\",\n           nameGap = 25, margin = 5) %&gt;% # adds +/- 5 to y-axis limits\n  \n  e_x_axis(name = \"Family Size\", \n           nameLocation = \"center\", \n           nameGap = 25, type = \"category\") %&gt;% # makes a category axis showing factors\n\n  e_tooltip() %&gt;% \n  e_title(\"Heights by Sex over Family Size\")\n\n\n\n\n\nInsight: So, at all family “strengths”, the male children are taller than the female children. Box plots are used to show distributions of numeric data values and compare them between multiple groups (i.e Categorical Data, here sex and nkids).\n\n\n\n\n\n\nQuestion\n\n\n\nQ.8 Does the mean height of children in a family vary with the number of children in the family? (family size)?\n\n\n\nGalton %&gt;% \n  group_by(nkids) %&gt;% \n  summarise(mean_height = mean(height)) %&gt;% \n  e_charts(nkids,height = 300) %&gt;% \n  e_bar(mean_height,colorBy = \"data\", legend = FALSE) %&gt;% \n  e_x_axis(name = \"nkids\",nameLocation = \"center\", nameGap = 25,\n           type = \"category\") %&gt;% \n  e_y_axis(name = \"mean height\", nameLocation = \"center\", nameGap = 25) %&gt;% \n  e_tooltip(trigger = \"item\")\n\n\n\n\n\nInsight: Hmm…The graph shows that mean heights do not vary much with family size nkids. We saw this with the box plots earlier. This would be useful information in a Modelling and Prediction exercise.\n\n\n\n\n\n\nFollow-up Question\n\n\n\nQ. 8a. Is height difference between sons and daughters related to height difference between father and mother?\nDifferences between father and mother heights influencing height…this would be like height ~ (father-mother). This would be a relationship between two Quant variables. A histogram would not serve here and we plot this as a Scatter Plot:\n\n\n\nGalton %&gt;% \n  group_by(family,sex) %&gt;% \n  \n  # Parental Height Difference\n  mutate(diff_height = father - mother) %&gt;% \n  select(family, sex, height, diff_height) %&gt;% \n  ungroup() %&gt;% \n  group_by(sex) %&gt;% \n\n  e_charts(diff_height, height = 300) %&gt;% \n  e_scatter(height, symbol_size = 8) %&gt;%\n  \n  # Fit a trend line\n  e_lm(height ~ diff_height,\n       name = c(\"Female\", \"Male\")) %&gt;% \n  e_x_axis(max = 18, min = -5,\n           name = \"Father - Mother Height\", \n           nameLocation = \"center\", nameGap = 25) %&gt;% \n  e_y_axis(max = 80, min = 50,  \n           name = \"Children's Heights\", \n           nameLocation = \"center\", nameGap = 25) %&gt;% \n  e_tooltip(axisPointer = list(type = \"cross\"))\n\n\n\n\n\nInsight: There seems no relationship, or a very small one, between children’s heights on the y-axis and the difference in parental height differences on the x-axis…\nAnd so on…..we can proceed from simple visualizations based on Questions to larger questions that demand inference and modelling. We hinted briefly on these in the above Case Study."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions-interactive.html#iconify-pajamas-issue-type-test-case-case-study-2-dataset-from-nhanes",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions-interactive.html#iconify-pajamas-issue-type-test-case-case-study-2-dataset-from-nhanes",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Case Study-2: Dataset from NHANES\n",
    "text": "Case Study-2: Dataset from NHANES\n\nLet us try the NHANES dataset. Try help(NHANES) in your Console.\n\ndata(\"NHANES\")\n\n\n Look at the Data\n\nskim(NHANES)\n\n\nData summary\n\n\nName\nNHANES\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n76\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n31\n\n\nnumeric\n45\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nSurveyYr\n0\n1.00\nFALSE\n2\n200: 5000, 201: 5000\n\n\nGender\n0\n1.00\nFALSE\n2\nfem: 5020, mal: 4980\n\n\nAgeDecade\n333\n0.97\nFALSE\n8\n40: 1398, 0-: 1391, 10: 1374, 20: 1356\n\n\nRace1\n0\n1.00\nFALSE\n5\nWhi: 6372, Bla: 1197, Mex: 1015, Oth: 806\n\n\nRace3\n5000\n0.50\nFALSE\n6\nWhi: 3135, Bla: 589, Mex: 480, His: 350\n\n\nEducation\n2779\n0.72\nFALSE\n5\nSom: 2267, Col: 2098, Hig: 1517, 9 -: 888\n\n\nMaritalStatus\n2769\n0.72\nFALSE\n6\nMar: 3945, Nev: 1380, Div: 707, Liv: 560\n\n\nHHIncome\n811\n0.92\nFALSE\n12\nmor: 2220, 750: 1084, 250: 958, 350: 863\n\n\nHomeOwn\n63\n0.99\nFALSE\n3\nOwn: 6425, Ren: 3287, Oth: 225\n\n\nWork\n2229\n0.78\nFALSE\n3\nWor: 4613, Not: 2847, Loo: 311\n\n\nBMICatUnder20yrs\n8726\n0.13\nFALSE\n4\nNor: 805, Obe: 221, Ove: 193, Und: 55\n\n\nBMI_WHO\n397\n0.96\nFALSE\n4\n18.: 2911, 30.: 2751, 25.: 2664, 12.: 1277\n\n\nDiabetes\n142\n0.99\nFALSE\n2\nNo: 9098, Yes: 760\n\n\nHealthGen\n2461\n0.75\nFALSE\n5\nGoo: 2956, Vgo: 2508, Fai: 1010, Exc: 878\n\n\nLittleInterest\n3333\n0.67\nFALSE\n3\nNon: 5103, Sev: 1130, Mos: 434\n\n\nDepressed\n3327\n0.67\nFALSE\n3\nNon: 5246, Sev: 1009, Mos: 418\n\n\nSleepTrouble\n2228\n0.78\nFALSE\n2\nNo: 5799, Yes: 1973\n\n\nPhysActive\n1674\n0.83\nFALSE\n2\nYes: 4649, No: 3677\n\n\nTVHrsDay\n5141\n0.49\nFALSE\n7\n2_h: 1275, 1_h: 884, 3_h: 836, 0_t: 638\n\n\nCompHrsDay\n5137\n0.49\nFALSE\n7\n0_t: 1409, 0_h: 1073, 1_h: 1030, 2_h: 589\n\n\nAlcohol12PlusYr\n3420\n0.66\nFALSE\n2\nYes: 5212, No: 1368\n\n\nSmokeNow\n6789\n0.32\nFALSE\n2\nNo: 1745, Yes: 1466\n\n\nSmoke100\n2765\n0.72\nFALSE\n2\nNo: 4024, Yes: 3211\n\n\nSmoke100n\n2765\n0.72\nFALSE\n2\nNon: 4024, Smo: 3211\n\n\nMarijuana\n5059\n0.49\nFALSE\n2\nYes: 2892, No: 2049\n\n\nRegularMarij\n5059\n0.49\nFALSE\n2\nNo: 3575, Yes: 1366\n\n\nHardDrugs\n4235\n0.58\nFALSE\n2\nNo: 4700, Yes: 1065\n\n\nSexEver\n4233\n0.58\nFALSE\n2\nYes: 5544, No: 223\n\n\nSameSex\n4232\n0.58\nFALSE\n2\nNo: 5353, Yes: 415\n\n\nSexOrientation\n5158\n0.48\nFALSE\n3\nHet: 4638, Bis: 119, Hom: 85\n\n\nPregnantNow\n8304\n0.17\nFALSE\n3\nNo: 1573, Yes: 72, Unk: 51\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nID\n0\n1.00\n61944.64\n5871.17\n51624.00\n56904.50\n62159.50\n67039.00\n71915.00\n▇▇▇▇▇\n\n\nAge\n0\n1.00\n36.74\n22.40\n0.00\n17.00\n36.00\n54.00\n80.00\n▇▇▇▆▅\n\n\nAgeMonths\n5038\n0.50\n420.12\n259.04\n0.00\n199.00\n418.00\n624.00\n959.00\n▇▇▇▆▃\n\n\nHHIncomeMid\n811\n0.92\n57206.17\n33020.28\n2500.00\n30000.00\n50000.00\n87500.00\n100000.00\n▃▆▃▁▇\n\n\nPoverty\n726\n0.93\n2.80\n1.68\n0.00\n1.24\n2.70\n4.71\n5.00\n▅▅▃▃▇\n\n\nHomeRooms\n69\n0.99\n6.25\n2.28\n1.00\n5.00\n6.00\n8.00\n13.00\n▂▆▇▂▁\n\n\nWeight\n78\n0.99\n70.98\n29.13\n2.80\n56.10\n72.70\n88.90\n230.70\n▂▇▂▁▁\n\n\nLength\n9457\n0.05\n85.02\n13.71\n47.10\n75.70\n87.00\n96.10\n112.20\n▁▃▆▇▃\n\n\nHeadCirc\n9912\n0.01\n41.18\n2.31\n34.20\n39.58\n41.45\n42.92\n45.40\n▁▂▇▇▅\n\n\nHeight\n353\n0.96\n161.88\n20.19\n83.60\n156.80\n166.00\n174.50\n200.40\n▁▁▁▇▂\n\n\nBMI\n366\n0.96\n26.66\n7.38\n12.88\n21.58\n25.98\n30.89\n81.25\n▇▆▁▁▁\n\n\nPulse\n1437\n0.86\n73.56\n12.16\n40.00\n64.00\n72.00\n82.00\n136.00\n▂▇▃▁▁\n\n\nBPSysAve\n1449\n0.86\n118.15\n17.25\n76.00\n106.00\n116.00\n127.00\n226.00\n▃▇▂▁▁\n\n\nBPDiaAve\n1449\n0.86\n67.48\n14.35\n0.00\n61.00\n69.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nBPSys1\n1763\n0.82\n119.09\n17.50\n72.00\n106.00\n116.00\n128.00\n232.00\n▂▇▂▁▁\n\n\nBPDia1\n1763\n0.82\n68.28\n13.78\n0.00\n62.00\n70.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys2\n1647\n0.84\n118.48\n17.49\n76.00\n106.00\n116.00\n128.00\n226.00\n▃▇▂▁▁\n\n\nBPDia2\n1647\n0.84\n67.66\n14.42\n0.00\n60.00\n68.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys3\n1635\n0.84\n117.93\n17.18\n76.00\n106.00\n116.00\n126.00\n226.00\n▃▇▂▁▁\n\n\nBPDia3\n1635\n0.84\n67.30\n14.96\n0.00\n60.00\n68.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nTestosterone\n5874\n0.41\n197.90\n226.50\n0.25\n17.70\n43.82\n362.41\n1795.60\n▇▂▁▁▁\n\n\nDirectChol\n1526\n0.85\n1.36\n0.40\n0.39\n1.09\n1.29\n1.58\n4.03\n▅▇▂▁▁\n\n\nTotChol\n1526\n0.85\n4.88\n1.08\n1.53\n4.11\n4.78\n5.53\n13.65\n▂▇▁▁▁\n\n\nUrineVol1\n987\n0.90\n118.52\n90.34\n0.00\n50.00\n94.00\n164.00\n510.00\n▇▅▂▁▁\n\n\nUrineFlow1\n1603\n0.84\n0.98\n0.95\n0.00\n0.40\n0.70\n1.22\n17.17\n▇▁▁▁▁\n\n\nUrineVol2\n8522\n0.15\n119.68\n90.16\n0.00\n52.00\n95.00\n171.75\n409.00\n▇▆▃▂▁\n\n\nUrineFlow2\n8524\n0.15\n1.15\n1.07\n0.00\n0.48\n0.76\n1.51\n13.69\n▇▁▁▁▁\n\n\nDiabetesAge\n9371\n0.06\n48.42\n15.68\n1.00\n40.00\n50.00\n58.00\n80.00\n▁▂▆▇▂\n\n\nDaysPhysHlthBad\n2468\n0.75\n3.33\n7.40\n0.00\n0.00\n0.00\n3.00\n30.00\n▇▁▁▁▁\n\n\nDaysMentHlthBad\n2466\n0.75\n4.13\n7.83\n0.00\n0.00\n0.00\n4.00\n30.00\n▇▁▁▁▁\n\n\nnPregnancies\n7396\n0.26\n3.03\n1.80\n1.00\n2.00\n3.00\n4.00\n32.00\n▇▁▁▁▁\n\n\nnBabies\n7584\n0.24\n2.46\n1.32\n0.00\n2.00\n2.00\n3.00\n12.00\n▇▅▁▁▁\n\n\nAge1stBaby\n8116\n0.19\n22.65\n4.77\n14.00\n19.00\n22.00\n26.00\n39.00\n▆▇▅▂▁\n\n\nSleepHrsNight\n2245\n0.78\n6.93\n1.35\n2.00\n6.00\n7.00\n8.00\n12.00\n▁▅▇▁▁\n\n\nPhysActiveDays\n5337\n0.47\n3.74\n1.84\n1.00\n2.00\n3.00\n5.00\n7.00\n▇▇▃▅▅\n\n\nTVHrsDayChild\n9347\n0.07\n1.94\n1.43\n0.00\n1.00\n2.00\n3.00\n6.00\n▇▆▂▂▂\n\n\nCompHrsDayChild\n9347\n0.07\n2.20\n2.52\n0.00\n0.00\n1.00\n6.00\n6.00\n▇▁▁▁▃\n\n\nAlcoholDay\n5086\n0.49\n2.91\n3.18\n1.00\n1.00\n2.00\n3.00\n82.00\n▇▁▁▁▁\n\n\nAlcoholYear\n4078\n0.59\n75.10\n103.03\n0.00\n3.00\n24.00\n104.00\n364.00\n▇▁▁▁▁\n\n\nSmokeAge\n6920\n0.31\n17.83\n5.33\n6.00\n15.00\n17.00\n19.00\n72.00\n▇▂▁▁▁\n\n\nAgeFirstMarij\n7109\n0.29\n17.02\n3.90\n1.00\n15.00\n16.00\n19.00\n48.00\n▁▇▂▁▁\n\n\nAgeRegMarij\n8634\n0.14\n17.69\n4.81\n5.00\n15.00\n17.00\n19.00\n52.00\n▂▇▁▁▁\n\n\nSexAge\n4460\n0.55\n17.43\n3.72\n9.00\n15.00\n17.00\n19.00\n50.00\n▇▅▁▁▁\n\n\nSexNumPartnLife\n4275\n0.57\n15.09\n57.85\n0.00\n2.00\n5.00\n12.00\n2000.00\n▇▁▁▁▁\n\n\nSexNumPartYear\n5072\n0.49\n1.34\n2.78\n0.00\n1.00\n1.00\n1.00\n69.00\n▇▁▁▁▁\n\n\n\n\n\nAgain, lots of data from skim, about the Quant and Qual variables. Spend a little time looking through this output.\n\nWhich variables could have been data that was given/stated by each respondent?\nAnd which ones could have been measured dependent data variables? Why do you think so?\nWhy is there so much missing data? Which variable are the most affected by this?\n\n\n Counts, and Charts with Counts\n\n\n\n\n\n\nQuestion\n\n\n\nQ.1 What are the Education levels and the counts of people with those levels?\n\n\n\nNHANES %&gt;% \n  group_by(Education) %&gt;% \n  summarise(total = n())\n\n\n\n  \n\n\n# This also works\n# tally(~Education, data = NHANES) %&gt;% as_tibble()\n\nInsight: The count goes up as we go from lower Education levels to higher. Need to keep that in mind. How do we understand the large number of NA entries?\n\n\n\n\n\n\nQuestion\n\n\n\nQ.2 How do counts of Education vs Work-status look like?\n\n\n\nNHANES %&gt;% \n  mutate(Education = as.factor(Education)) %&gt;% \n  group_by(Work,Education) %&gt;% \n  summarise(count = n())\nNHANES %&gt;% \n  group_by(Work, Education) %&gt;% \n  summarise(count = n()) %&gt;% \n  e_charts(Education, height = 300) %&gt;% \n  e_bar(count) %&gt;% \n  e_y_axis(max = 1750) %&gt;% \n  e_x_axis(type = \"category\")  %&gt;% e_tooltip()\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\nInsight: Clear increase in the number of Working people as Education goes from 8th Grade to College. No surprise. Are the NotWorking counts a surprise?\n\n Stat Summaries, Histograms, and Densities\n\n\n\n\n\n\nQuestion\n\n\n\nQ.3. What is the distribution of Physical Activity Days, across Gender? Across Education?\n\n\n\n# NHANES %&gt;% gf_histogram( ~ PhysActiveDays | Education, fill = ~ Education)\nNHANES %&gt;% \n  group_by(Gender) %&gt;% \n  e_charts(PhysActiveDays,height = 350) %&gt;%  \n  e_histogram(PhysActiveDays) %&gt;% \n  e_x_axis(max = 8) %&gt;% \n  e_facet(cols = 2) %&gt;% e_tooltip()\nNHANES %&gt;% \n  group_by(Education) %&gt;% \n  e_charts(PhysActiveDays) %&gt;% \n  e_histogram(PhysActiveDays) %&gt;% \n  e_x_axis(max = 8) %&gt;% \n  e_facet(rows = 2, cols = 3) %&gt;% e_tooltip()\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsight: Can we conclude anything here? The populations in each category are different, as indicated by the different y-axis scales, so what do we need to do? Take percentages or ratios of course, per-capita! How would one do that?\n\n\n\n\n\n\nQuestion\n\n\n\nQ.3a. What is the distribution of Physical Activity Days, across Education and Sex, per capita?\n\n\n\nNHANES %&gt;% \n  group_by(Gender) %&gt;% \n  summarize(mean_active = mean(PhysActiveDays,na.rm = TRUE))\nNHANES %&gt;% \n  group_by(Education) %&gt;% \n  summarize(mean_active = mean(PhysActiveDays,na.rm = TRUE))\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n\n\n\n\nInsight: Hmm..no great differences in per-capita physical activity. Females are marginally more active than males. No need to even plot this.\n::: {.callout-note title=“Question”} Q.4. How are people Ages distributed across levels of Education?\n\n# Recall there are missing data\n# gf_boxplot(Age ~ Education, \n#            fill = ~ Education, # Always a good idea to fill boxes\n#            data = NHANES) %&gt;% \n#   gf_theme(theme_classic()) %&gt;% plotly::ggplotly()\n\nNHANES %&gt;% \n  mutate(Education = as.factor(Education)) %&gt;% \n  group_by(Education) %&gt;% \n  e_charts(height = 300) %&gt;% # Should not mention x-variable!!!\n  e_boxplot(Age, colorBy = \"data\",\n             itemStyle = list(borderWidth = 3)) %&gt;% \n  e_y_axis(name = \"Age\", nameLocation = \"middle\", max = 100, min = 0, nameGap = 25) %&gt;% \n  e_x_axis(type = \"category\", axisTick = list(alignWithLabel = TRUE), \n           axisLabel = list(interval = 0)) %&gt;% # ensures all tick labels on x-axis\n  e_tooltip()\n\n\n\n\n\n\n\n\n\nInsight: Older age groups are somewhat more heavily represented in groups with lower educational status. But College Graduates also have slightly older age distributions…So do College Educated people live longer? That is a nice Question for some Inferential Modelling. And how to interpret the NA group?\n\n\n\n\n\n\nQuestion\n\n\n\nQ.5. How is Education distributed over Race?\n\n\n\nNHANES_by_Race1 &lt;- NHANES %&gt;% \n  group_by(Race1) %&gt;% \n  summarize(population = n())\nNHANES_by_Race1\nNHANES %&gt;% group_by(Education, Race1) %&gt;% \n  summarize( n = n()) %&gt;% \n  left_join(NHANES_by_Race1, by = c(\"Race1\" = \"Race1\")) %&gt;% \n  mutate(percapita_educated = (n/population)*100) %&gt;% \n  ungroup() %&gt;%  \n  group_by(Race1) %&gt;% # Aesthetic 1\n  e_charts(Education, height = 350) %&gt;%  # Aesthetic #2\n  e_bar(percapita_educated) %&gt;% # Aesthetic #3\n  \n  e_x_axis(type = \"category\", axisTick = list(alignWithLabel = TRUE), \n           axisLabel = list(interval = 0)) %&gt;% \n  e_y_axis(max = 35) %&gt;% \n  e_facet(rows = 2,cols = 3) %&gt;% \n  e_flip_coords()\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\nInsight: Blacks, Hispanics, and Mexicans tend to have fewer people with college degrees, as a percentage of their population. Asians and other immigrants have a significant tendency towards higher education!\n\n\n\n\n\n\nQuestion\n\n\n\nQ.6. What is the distribution of people’s BMI, split by Gender? By Race1?\n\n\n\n# One can also plot both histograms and densities in an overlay fashion,\n\nNHANES %&gt;% group_by(Gender) %&gt;% \n  e_charts(height = 300) %&gt;% \n  e_density(BMI)\nNHANES %&gt;% group_by(Race1) %&gt;% \n  e_charts(height = 350) %&gt;% \n  e_density(BMI) %&gt;% \n  e_facet(rows = 2, cols = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsight: Non-white races tend to have larger portions of their populations with larger BMI. So these races perhaps tend to obesity. By and large BMI distributions are normal.\n\n\n\n\n\n\nQuestion\n\n\n\nQ.7. What is the distribution of people’s Testosterone level vs BMI? Split By Race1?\n\n\n\nNHANES %&gt;%  \n  gf_density2d(Testosterone ~ BMI | Race1) %&gt;% \n  gf_theme(theme_classic()) %&gt;% \nplotly::ggplotly()\n\n\n\n\n\nInsight: Low testosterone levels exist across all BMI values, but healthy levels of T exists only over a smaller range of BMI.\nNote: echarts4r does not seem to provide a 2D-density plot…yet!!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions-interactive.html#iconify-pajamas-issue-type-test-case-case-study-3-a-complete-example-with-banned-books",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions-interactive.html#iconify-pajamas-issue-type-test-case-case-study-3-a-complete-example-with-banned-books",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Case Study #3: A complete example with Banned Books",
    "text": "Case Study #3: A complete example with Banned Books\nHere is a dataset from Jeremy Singer-Vine’s blog, Data Is Plural. This is a list of all books banned in schools across the US.\n Download the data \n\n Look at the Data\n\nbanned &lt;- readxl::read_xlsx(path = \"data/banned.xlsx\",\n                            sheet = \"Sorted by Author & Title\")\nskim(banned)\n\n\nData summary\n\n\nName\nbanned\n\n\nNumber of rows\n1586\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n10\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nAuthor\n0\n1.00\n7\n29\n0\n797\n0\n\n\nTitle\n0\n1.00\n2\n155\n0\n1145\n0\n\n\nType of Ban\n0\n1.00\n21\n36\n0\n4\n0\n\n\nSecondary Author(s)\n1488\n0.06\n9\n187\n0\n61\n0\n\n\nIllustrator(s)\n1222\n0.23\n8\n35\n0\n192\n0\n\n\nTranslator(s)\n1576\n0.01\n14\n25\n0\n9\n0\n\n\nState\n0\n1.00\n4\n14\n0\n26\n0\n\n\nDistrict\n0\n1.00\n4\n40\n0\n86\n0\n\n\nDate of Challenge/Removal\n0\n1.00\n5\n15\n0\n15\n0\n\n\nOrigin of Challenge\n0\n1.00\n13\n16\n0\n2\n0\n\n\n\n\n\nInsight: Clearly the variables are all Qualitative, except perhaps for Date of Challenge/Removal, (which in this case has been badly mangled by Excel) So we need to make counts based on the* levels* of the Qual variables and plot Bar/Column charts. We will not find a use for histograms or densities.\nLet us try to answer this question, about counts:\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is the count of banned books by type and by US state?\n\n\n\nbanned_by_state &lt;- \n  banned %&gt;% \n  group_by(State) %&gt;% \n  summarise(total = n()) %&gt;% \n  ungroup()\nbanned_by_state\n\n\n\n  \n\n\nbanned %&gt;% \n  group_by(State, `Type of Ban`) %&gt;% \n  summarise(count = n()) %&gt;% \n  ungroup() %&gt;% \n  left_join(., banned_by_state, by = c(\"State\" = \"State\")) %&gt;% \n  \n #  pivot_wider(.,id_cols = State,\n #              names_from = `Type of Ban`,\n #              values_from = count) %&gt;% janitor::clean_names() %&gt;% \n #  replace_na(list(banned_from_libraries_and_classrooms = 0,\n #                  banned_from_libraries = 0,\n #                  banned_pending_investigation = 0,\n #                  banned_from_classrooms = 0)) %&gt;% \n # mutate(total = sum(across(where(is.integer)))) %&gt;%\ngf_col(count ~ reorder(State, total), \n          fill = ~ `Type of Ban`) %&gt;% \n  gf_labs(x = \"Count of Banned Books\",\n          y = \"State\") %&gt;% \n  gf_refine(coord_flip()) %&gt;% \n  gf_theme(theme = theme_minimal())\n\n\n\n\nInsight: Do you want to live in Texas? If you are both illiterate and interested in horses, perhaps."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions-interactive.html#iconify-fluent-mdl2-decision-solid-conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions-interactive.html#iconify-fluent-mdl2-decision-solid-conclusion",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Conclusion",
    "text": "Conclusion\nAnd that is a wrap!! Try to work with this procedure:\n\nInspect the data using skim or inspect\n\nIdentify Qualitative and Quantitative variables\n\nNotice variables that have missing data\n\nDevelop Counts of Observations for combinations of Qualitative variables (factors)\n\nDevelop Histograms and Densities, and slice them by Qualitative variables to develop facetted plots as needed\nAt each step record the insight and additional questions!!\n\nContinue with other Descriptive Graphs as needed\n\nAnd then on the inference and modelling!!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions-interactive.html#iconify-ooui-references-rtl-references",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions-interactive.html#iconify-ooui-references-rtl-references",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n References",
    "text": "References\n\nSharon Machlis, Plot in R with echarts4r, InfoWorld https://www.infoworld.com/article/3607068/plot-in-r-with-echarts4r.html\n\nA detailed analysis of the NHANES dataset, https://awagaman.people.amherst.edu/stat230/Stat230CodeCompilationExampleCodeUsingNHANES.pdf"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions-interactive.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions-interactive.html#footnotes",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "Footnotes",
    "text": "Footnotes\n\nFundamentals of Data Visualization (clauswilke.com)↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html",
    "title": "🕸 Networks",
    "section": "",
    "text": "Orange Tutorial\n\n Networks in R"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html#fa-folder-open-slides-and-tutorials",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html#fa-folder-open-slides-and-tutorials",
    "title": "🕸 Networks",
    "section": "",
    "text": "Orange Tutorial\n\n Networks in R"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html#iconify-noto-v1-package-setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html#iconify-noto-v1-package-setting-up-r-packages",
    "title": "🕸 Networks",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nknitr::opts_chunk$set(echo = TRUE)\nlibrary(tidyverse)\nlibrary(tidygraph)\nlibrary(ggraph)\nlibrary(graphlayouts)\nlibrary(ggraph)\nlibrary(graphlayouts)\nlibrary(igraphdata)\nlibrary(sand)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "title": "🕸 Networks",
    "section": "\n Introduction",
    "text": "Introduction\nNetwork graphs show relationships between entities: what sort they are, how strong they are, and even of they change over time.\nWe will examine data structures pertaining both to the entities and the relationships between them and look at the data object that can combine these aspects together. Then we will see how these are plotted, what the structure of the plot looks like. There are also metrics that we can calculate for the network, based on its structure. We will of course examine geometric metaphors that can represent various classes of entities and their relationships.\nNetwork graphs can be rendered both as static and interactive and we will examine R packages that render both kinds of plots.\nThere is a another kind of structure: one that combines spatial and network data in one. We will defer that for a future module !"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html#what-kind-network-graphs-will-we-make",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html#what-kind-network-graphs-will-we-make",
    "title": "🕸 Networks",
    "section": "What kind Network graphs will we make?",
    "text": "What kind Network graphs will we make?\nHere is a network map of the characters in Victor Hugo’s Les Miserables:\n\n\nAnd this: the well known Zachary’s Karate Club dataset visualized as a network"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html#references",
    "title": "🕸 Networks",
    "section": "References",
    "text": "References\n\nHadley Wickham, Danielle Navarro, and Thomas Lin Pedersen, ggplot2: Elegant Graphics for Data Analysis. https://ggplot2-book.org/networks.html\nOmar Lizardo and Isaac Jilbert, Social Networks: An Introduction. https://bookdown.org/omarlizardo/_main/\nMark Hoffman, Methods for Network Analysis. https://bookdown.org/markhoff/social_network_analysis/\nStatistical Analysis of Network Data with R, 2nd Edition.https://github.com/kolaczyk/sand"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-NatureData/nature-data.html",
    "href": "content/courses/Analytics/Descriptive/Modules/10-NatureData/nature-data.html",
    "title": "🕶 Science, Human Experience, Experiments, and Data",
    "section": "",
    "text": "library(tidyverse) # Data processing with tidy principles\nlibrary(mosaic) # Our go-to package for almost everything\nlibrary(ggformula) # Out plotting package\n\nlibrary(Lock5withR) # Some neat little datasets from a lovely textbook"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-NatureData/nature-data.html#iconify-noto-v1-package-setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/10-NatureData/nature-data.html#iconify-noto-v1-package-setting-up-r-packages",
    "title": "🕶 Science, Human Experience, Experiments, and Data",
    "section": "",
    "text": "library(tidyverse) # Data processing with tidy principles\nlibrary(mosaic) # Our go-to package for almost everything\nlibrary(ggformula) # Out plotting package\n\nlibrary(Lock5withR) # Some neat little datasets from a lovely textbook"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-NatureData/nature-data.html#fa-envelope-titlean-envelope-where-does-data-come-from",
    "href": "content/courses/Analytics/Descriptive/Modules/10-NatureData/nature-data.html#fa-envelope-titlean-envelope-where-does-data-come-from",
    "title": "🕶 Science, Human Experience, Experiments, and Data",
    "section": "\n Where does Data come from?",
    "text": "Where does Data come from?\nWe will need to form a basic understanding of basic scientific enterprise. Let us look at the slides."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-NatureData/nature-data.html#fa-chart-simple-why-visualize",
    "href": "content/courses/Analytics/Descriptive/Modules/10-NatureData/nature-data.html#fa-chart-simple-why-visualize",
    "title": "🕶 Science, Human Experience, Experiments, and Data",
    "section": "\n Why Visualize?",
    "text": "Why Visualize?\n\nWe can digest information more easily when it is pictorial\nOur Working Memories are both short-term and limited in capacity. So a picture abstracts the details and presents us with an overall summary, an insight, or a story that is both easy to recall and easy on retention.\nData Viz includes shapes that carry strong cultural memories; and impressions for us. These cultural memories help us to use data viz in a universal way to appeal to a wide variety of audiences. (Do humans have a gene for geometry?1);\nIt helps sift facts and mere statements: for example:\n\n\n\nRape Capital\n\n\n\nData Reveals Crime"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-NatureData/nature-data.html#iconify-mdi-category-plus-what-are-data-types",
    "href": "content/courses/Analytics/Descriptive/Modules/10-NatureData/nature-data.html#iconify-mdi-category-plus-what-are-data-types",
    "title": "🕶 Science, Human Experience, Experiments, and Data",
    "section": "\n What are Data Types?",
    "text": "What are Data Types?\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Data\n\n\n\nEach variable is a column; a column contains one kind of data. Each observation or case is a row."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-NatureData/nature-data.html#sec-data-types",
    "href": "content/courses/Analytics/Descriptive/Modules/10-NatureData/nature-data.html#sec-data-types",
    "title": "🕶 Science, Human Experience, Experiments, and Data",
    "section": "\n How do we Spot Data Variable Types?",
    "text": "How do we Spot Data Variable Types?\nBy asking questions!. Shown below is a table of different kinds of questions you could use to query a dataset. The variable or variables that “answer” the question would be in the category indicated by the question.\n\n\n\n\n\nNo\nPronoun\nAnswer\nVariable/Scale\nExample\nWhat Operations?\n\n\n\n1\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities, with Scale and a Zero Value.Differences and Ratios /Products are meaningful.\nQuantitative/Ratio\nLength,Height,Temperature in Kelvin,Activity,Dose Amount,Reaction Rate,Flow Rate,Concentration,Pulse,Survival Rate\nCorrelation\n\n\n2\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities with Scale. Differences are meaningful, but not products or ratios\nQuantitative/Interval\npH,SAT score(200-800),Credit score(300-850),SAT score(200-800),Year of Starting College\nMean,Standard Deviation\n\n\n3\nHow, What Kind, What Sort\nA Manner / Method, Type or Attribute from a list, with list items in some \" order\" ( e.g. good, better, improved, best..)\nQualitative/Ordinal\nSocioeconomic status (Low income, Middle income, High income),Education level (HighSchool, BS, MS, PhD),Satisfaction rating(Very much Dislike, Dislike, Neutral, Like, Very Much Like)\nMedian,Percentile\n\n\n4\nWhat, Who, Where, Whom, Which\nName, Place, Animal, Thing\nQualitative/Nominal\nName\nCount no. of cases,Mode\n\n\n\n\n\n\nAs you go from Qualitative to Quantitative data types in the table, I hope you can detect a movement from fuzzy groups/categories to more and more crystallized numbers.\n\n\nType of Variables\n\nEach variable/scale can be subjected to the operations of the previous group. In the words of S.S. Stevens\n\nthe basic operations needed to create each type of scale is cumulative: to an operation listed opposite a particular scale must be added all those operations preceding it."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-NatureData/nature-data.html#some-examples-of-data-variables",
    "href": "content/courses/Analytics/Descriptive/Modules/10-NatureData/nature-data.html#some-examples-of-data-variables",
    "title": "🕶 Science, Human Experience, Experiments, and Data",
    "section": "Some Examples of Data Variables",
    "text": "Some Examples of Data Variables\nExample 1: AllCountries\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\nQuestions\n\n\n\nQ1. How many people in Andorra have internet access?\nA1. This leads to the Internet variable, which is a Quantitative variable, a proportion.2 The answer is \\(70.5\\%\\).\n\n\nExample 2:StudentSurveys\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\nQuestions\n\n\n\nQ.1. What kind of students are these?\nA.1. The variables Gender, and Year both answer to this Question. And they are both Qualitative/Categorical variables, of course.\nQ.2. What is their status in their respective families?\nA.2. Hmm…they are either first-born, or second-born, or third…etc. While this is recorded as a number, it is still a Qualitative variable3! Think! Can you do math operations with BirthOrder? Like mean or median?\nQ.3.How big are the families?\nA.3. Clearly, the variable that answers is Siblings and since the question is synonymous with “how many”, this is a Quantitative variable."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-NatureData/nature-data.html#iconify-material-symbols-auto-graph-what-is-a-data-visualization",
    "href": "content/courses/Analytics/Descriptive/Modules/10-NatureData/nature-data.html#iconify-material-symbols-auto-graph-what-is-a-data-visualization",
    "title": "🕶 Science, Human Experience, Experiments, and Data",
    "section": "\n What is a Data Visualization?",
    "text": "What is a Data Visualization?\n\n Data Viz = Data -&gt; Geometry\n\n Shapes\nData Visualization is the act of “mapping” a geometric aspect/aesthetic to a variable in data. The aesthetic then varies in accordance with the data variable, creating (part of) a chart.\nWhat might be the geometric aesthetics available to us?\n\n\nCommon Geometric Aesthetics in Charts\n\n\n Mapping\nWhat does this “mapping” mean? That the geometric aesthetics are used to represent qualitative or quantitative variables from your data, by varying in accordance to the data variable. For instance, length or height of a bar can be made proportional to theage or income of a person. Colour of points can be mapped to gender, with a unique colour for each gender. Position along an axis( x or y) can vary in accordance with a bodyWeight variable.\n\n\n\n\n\nA chart may use more than one aesthetic: position, shape, colour, height and angle,pattern or texture to name several. Usually, each aesthetic is mapped to just one variable to ensure there is no cognitive error. There is of course a choice and you should be able to map any kind of variable to any geometric aspect/aesthetic that may be available.\n\n\n\n\n\n\nA Natural Mapping\n\n\n\nNote that here is also a “natural” mapping between aesthetic and kind of variable@sec-data-types, Quantitative or Qualitative. For instance, shape is rarely mapped to a Quantitative variable; we understand this because the nature of variation between the Quantitative variable and the shape aesthetic is not similar (i.e. not continuous). Bad choices may lead to bad, or worse, misleading charts!\n\n\nIn the above chart, it is pretty clear what kind of variable is plotted on the x-axis and the y-axis. What about colour? Could this be considered a z-axis in the chart? There are also other aspects that you can choose (not explicitly shown here) such as the plot theme(colours, fonts, backgrounds etc), which may not be mapped to data, but are nonetheless choices to be made. We will get acquainted with this aspect as we build charts."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-NatureData/nature-data.html#sec-data-viz",
    "href": "content/courses/Analytics/Descriptive/Modules/10-NatureData/nature-data.html#sec-data-viz",
    "title": "🕶 Science, Human Experience, Experiments, and Data",
    "section": "\n Basic Types of Charts",
    "text": "Basic Types of Charts\nWe can think of simple visualizations as combinations of aesthetics, mapped to combinations of variables. Some examples:\n\n\nGeometries , Combinations, and Graphs\n\n\n\n\n\n\n\nVariable#1\nVariable#2\nChart Names\nShape\n\n\n\nQuant\nNone\nHistogram and Density\n\n\n\nQual\nNone\nBar Chart\n\n\n\nQuant\nQuant\nScatter Plot, Line Chart, Bubble Plot, Area Chart\n\n\n\n\n\nQuant\nQual\nPie Chart, Donut Chart, Column Chart, Box-Whisker Plot, Radar Chart, Bump Chart, Tree Diagram\n\n\n\n\n\nQual\nQual\nStacked Bar Chart, Mosaic Chart, Sankey, Chord Diagram, Network Diagram"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-NatureData/nature-data.html#iconify-fluent-mdl2-decision-solid-conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/10-NatureData/nature-data.html#iconify-fluent-mdl2-decision-solid-conclusion",
    "title": "🕶 Science, Human Experience, Experiments, and Data",
    "section": "\n Conclusion",
    "text": "Conclusion\nSo there we have it:\n\nQuestions lead to Types of Variables (Quant and Qual)\n\nFurther Questions lead to relationships between them, which we describe using Data Visualizations\n\nData Visualizations are Data mapped onto Geometry \nMultiple Variable-to-Geometry Mappings = A Complete Data Visualization\n\n\nYou might think of all these Questions, Answers, Mapping as being equivalent to metaphors as a language in itself. And indeed, in R we use a philosophy called the Grammar of Graphics! We will use this grammar in the R graphics packages that we will encounter."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-NatureData/nature-data.html#iconify-ooui-references-ltr-references",
    "href": "content/courses/Analytics/Descriptive/Modules/10-NatureData/nature-data.html#iconify-ooui-references-ltr-references",
    "title": "🕶 Science, Human Experience, Experiments, and Data",
    "section": "\n References",
    "text": "References\n\nOpen Intro Stats: Types of Variables\nLock, Lock, Lock, Lock, and Lock. Statistics: Unlocking the Power of Data, Third Edition, Wiley, 2021. &lt;Statistics: Unlocking the Power of Data, 3rd Edition | Wiley&gt;\nClaus Wilke: Fundamentals of Data Visualization"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-NatureData/nature-data.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/10-NatureData/nature-data.html#footnotes",
    "title": "🕶 Science, Human Experience, Experiments, and Data",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://www.xcode.in/genes-and-personality/how-genes-influence-your-math-ability/↩︎\nHow might this data have been obtained? By asking people in a survey and getting Yes/No answers!↩︎\nQualitative variables are called Factor variables in R, and are stored, internally, as numeric variables together with their levels. The actual values of the numeric variable are 1, 2, and so on.↩︎"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Applied Metaphorics",
    "section": "",
    "text": "Hi, I’m Arvind Venkatadri.\nI’m an Adjunct Professor at DSU, Bangalore, INDIA, with a passion for R, Data Visualization, Complexity Science, Literature, and Creative Thinking and Problem Solving with TRIZ. On this blog, I share and teach what I learn.\nTo get started, you can check out my courses. You can find me on Twitter or GitHub and YouTube. Feel free to reach out to me via mail !\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/listing.html",
    "href": "content/courses/Analytics/Descriptive/listing.html",
    "title": "Descriptive Analytics",
    "section": "",
    "text": "🕶 Science, Human Experience, Experiments, and Data\n\n\nWhy do we visualize data\n\n\n\n\nScientific Inquiry\n\n\nExperiments\n\n\nObservations\n\n\nNature of Data\n\n\nExperience\n\n\nMeasurement\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2021\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\n📊 Distributions\n\n\nDistributions\n\n\n\n\nQual Variables\n\n\nQuant Variables\n\n\nBar Charts\n\n\nColumn Charts\n\n\nHistograms\n\n\nDensity Plots\n\n\nBox Plots\n\n\n\n\nQuant Variable Graphs and their Siblings\n\n\n\n\n\n\nNov 15, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\n📎 Correlations\n\n\nCorrelations\n\n\n\n\nCorrelations\n\n\nScatter Plots\n\n\nBubble Plots\n\n\n2D Density Plots\n\n\nHeatmaps\n\n\nRegression Lines\n\n\n\n\nHow one variable changes with another\n\n\n\n\n\n\nNov 22, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\n🐉 Visualizing Categorical Data\n\n\n\n\n\n\n\nProportions\n\n\nFrequency Tables\n\n\nContingency Tables\n\n\nNumerical Data in Groups\n\n\nMargins\n\n\nLikert Scale data\n\n\nBar Plots (for Contingency Tables)\n\n\nMosaic Plots\n\n\nBalloon Plots\n\n\nPie Charts\n\n\nCorrespondence Analysis\n\n\n\n\nTypes, Categories, and Counts\n\n\n\n\n\n\nDec 27, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\n🕔 Time Series\n\n\nTime Series\n\n\n\n\nCandleStick Graphs\n\n\nHeatmap Graphs (over time)\n\n\nLine Graphs\n\n\nTime Series\n\n\n\n\nEvents, Trends, Seasons, and Changes over Time\n\n\n\n\n\n\nDec 15, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\n🍕 Parts of a Whole\n\n\nParts of a Whole\n\n\n\n\nPie Charts\n\n\nFan Charts\n\n\nDonut Charts\n\n\nGrouping\n\n\nStacking\n\n\nCircular Bar Charts\n\n\nDot Plots\n\n\nMosaic Charts\n\n\nParliament Charts\n\n\nWaffle Charts\n\n\n\n\nSlices, Portions, Counts, and Aggregates of Data\n\n\n\n\n\n\nNov 25, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\n🕸 Evolution and Flow\n\n\nEvolution and Flow\n\n\n\n\nLine and Area Plots\n\n\nStream Charts\n\n\nParallel Set Plots\n\n\nAlluvial Plots\n\n\nSankey Diagrams\n\n\nChord Diagrams\n\n\nWaterfall Plots\n\n\nBump Charts\n\n\n\n\nChanges in Information over space and time\n\n\n\n\n\n\nNov 22, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\n🖏 Ratings and Rankings\n\n\nRanking\n\n\n\n\nBar Charts\n\n\nLollipop Charts\n\n\nRadar Charts\n\n\nWord Clouds\n\n\nBump Charts\n\n\n\n\nComparisons between observations and between variables\n\n\n\n\n\n\nFeb 10, 2023\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\n🗺 Visualising Spatial Data\n\n\nMaps\n\n\n\n\nSpatial Data\n\n\nMaps\n\n\nStatic\n\n\nInteractive\n\n\nChoropleth Maps\n\n\nBubble Plots\n\n\nCartograms\n\n\n\n\nGeospatial Data and how to use it with intent\n\n\n\n\n\n\nAug 15, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\n🕸 Networks\n\n\nHow things are connected\n\n\nNetworks and Connections and what happens over them\n\n\n\n\n\n\nNov 21, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\n📚 Miscellaneous Graphing Tools, and References\n\n\n\n\n\n\n\nOnline Tools\n\n\nNo Code\n\n\nData Viz Guides\n\n\n\n\nMiscellaneous Graphs and Tools\n\n\n\n\n\n\nNov 11, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html",
    "title": "The Grammar of Networks",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE, warning = FALSE)\n\n# Fonts\n# Run these next few commands IN YOUR CONSOLE once. \n#install.packages(\"extrafontdb\")\n#library(extrafont)\n#extrafont::font_import(paths = NULL, recursive = TRUE, prompt = TRUE,pattern = NULL)\n\n########################################\n# For General Data Manipulation\nlibrary(tidyverse)\n\n########################################\n# Network Analysis Library (Handle data and Viz)\nlibrary(igraph)\n\n########################################\n# For Network \"Manipulation\"\nlibrary(tidygraph)\n\n# For Network Visualization\nlibrary(ggraph)\nlibrary(graphlayouts)\nlibrary(visNetwork)\n\n# For \"Network\" Datasets\nlibrary(igraphdata)\n\n# Fonts\nlibrary(ggtext) # Claus Wilke's package\nlibrary(showtext)\n\n# For repeatable layouts\nset.seed(12345)\n\n\n## Loading Google fonts (https://fonts.google.com/)\nfont_add_google(name = \"Fira Sans Extra Condensed\", family = \"fira\")\nfont_add_google(name = \"Roboto Condensed\", family = \"roboto_condensed\")\nfont_add_google(name = \"Poppins\", family = \"poppins\")\nfont_add_google(\"Gochi Hand\", \"gochi\")\nfont_add_google(\"Schoolbell\", \"bell\")\nfont_add_google(\"Merriweather Sans\", \"merri\")\n\nshowtext_opts(dpi = 300, regular.wt = 300, bold.wt = 800)\nshowtext_auto(enable = TRUE)\n\n# theme_set(new = theme_graph(base_family = \"merri\"))"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#iconify-noto-v1-package-setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#iconify-noto-v1-package-setting-up-r-packages",
    "title": "The Grammar of Networks",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE, warning = FALSE)\n\n# Fonts\n# Run these next few commands IN YOUR CONSOLE once. \n#install.packages(\"extrafontdb\")\n#library(extrafont)\n#extrafont::font_import(paths = NULL, recursive = TRUE, prompt = TRUE,pattern = NULL)\n\n########################################\n# For General Data Manipulation\nlibrary(tidyverse)\n\n########################################\n# Network Analysis Library (Handle data and Viz)\nlibrary(igraph)\n\n########################################\n# For Network \"Manipulation\"\nlibrary(tidygraph)\n\n# For Network Visualization\nlibrary(ggraph)\nlibrary(graphlayouts)\nlibrary(visNetwork)\n\n# For \"Network\" Datasets\nlibrary(igraphdata)\n\n# Fonts\nlibrary(ggtext) # Claus Wilke's package\nlibrary(showtext)\n\n# For repeatable layouts\nset.seed(12345)\n\n\n## Loading Google fonts (https://fonts.google.com/)\nfont_add_google(name = \"Fira Sans Extra Condensed\", family = \"fira\")\nfont_add_google(name = \"Roboto Condensed\", family = \"roboto_condensed\")\nfont_add_google(name = \"Poppins\", family = \"poppins\")\nfont_add_google(\"Gochi Hand\", \"gochi\")\nfont_add_google(\"Schoolbell\", \"bell\")\nfont_add_google(\"Merriweather Sans\", \"merri\")\n\nshowtext_opts(dpi = 300, regular.wt = 300, bold.wt = 800)\nshowtext_auto(enable = TRUE)\n\n# theme_set(new = theme_graph(base_family = \"merri\"))"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "title": "The Grammar of Networks",
    "section": "\n Introduction",
    "text": "Introduction\nThis Quarto document is part of my workshop course on R . The material is based on A Layered Grammar of Graphics by Hadley Wickham. The intent of this Course is to build Skill in coding in R, and also appreciate R as a way to metaphorically visualize information of various kinds, using predominantly geometric figures and structures.\nAll Quarto document files combine code, text, web-images, and figures developed using code. Everything is text; code chunks are enclosed in fences (```)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#iconify-game-icons-stairs-goal-goals",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#iconify-game-icons-stairs-goal-goals",
    "title": "The Grammar of Networks",
    "section": "\n Goals",
    "text": "Goals\nAt the end of this Lab session, we should:\n\nknow the types and structures of network data and be able to work with them\nunderstand the basics of modern network packages in R\nbe able to create network visualizations using tidygraph, ggraph( static visualizations ) and visNetwork (interactive visualizations)\nsee directions for how the network metaphor applies in a variety of domains (e.g. biology/ecology, ideas/influence, technology, transportation, to name a few)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#iconify-game-icons-journey-pedagogical-note",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#iconify-game-icons-journey-pedagogical-note",
    "title": "The Grammar of Networks",
    "section": "\n Pedagogical Note",
    "text": "Pedagogical Note\nThe method followed will be based on PRIMM:\n\n\nPREDICT Inspect the code and guess at what the code might do, write predictions\n\n\nRUN the code provided and check what happens\n\nINFER what the parameters of the code do and write comments to explain. What bells and whistles can you see?\n\nMODIFY the parameters code provided to understand the options available. Write comments to show what you have aimed for and achieved.\n\nMAKE : take an idea/concept of your own, and graph it."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#iconify-cib-graphql-graph-metaphors",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#iconify-cib-graphql-graph-metaphors",
    "title": "The Grammar of Networks",
    "section": "\n Graph Metaphors",
    "text": "Graph Metaphors\nNetwork graphs are characterized by two key terms: nodes and edges\n\n\nNodes : Entities\n\nMetaphors: Individual People? Things? Ideas? Places? to be connected in the network.\nSynonyms: vertices. Nodes have IDs.\n\n\n\nEdges: Connections\n\nMetaphors: Interactions? Relationships? Influence? Letters sent and received? Dependence? between the entities.\nSynonyms: links, ties.\n\n\n\nIn R, we create network representations using node and edge information. One way in which these could be organized are:\n\n\nNode list: a data frame with a single column listing the node IDs found in the edge list. You can also add attribute columns to the data frame such as the names of the nodes or grouping variables. ( Type? Class? Family? Country? Subject? Race? )\n\n\nNode Table\n\n\n\n\n\n\n\nID\nNode Name\nAttribute? Qualities?Categories? Family? Country?Planet?\n\n\n1\nNed\nNursery School Teacher\n\n\n2\nJaguar Paw\nMain Character, Apocalypto\n\n\n3\nJohn Snow\nEpidemiologist\n\n\n\n\n\nEdge list: data frame containing two columns: source node and destination node of an edge. Source and Destination have node IDs.\n\nWeighted network graph: An edge list can also contain additional columns describing attributes of the edges such as a magnitude aspect for an edge. If the edges have a magnitude attribute the graph is considered weighted.\n\n\nEdges Table\n\nFrom\nTo\nRelationship\nWeightage\n\n\n\n1\n3\nFinancial Dealings\n6\n\n\n2\n1\nHistory Lessons\n2\n\n\n2\n3\nVaccination\n15\n\n\n\n\n\nLayout: A geometric arrangement of nodes and edges.\n\nMetaphors: Location? Spacing? Distance? Coordinates? Colour? Shape? Size? Provides visual insight due to the arrangement.\n\n\n\nLayout Algorithms : Method to arranges nodes and edges with the aim of optimizing some metric .\n\nMetaphors: Nodes are masses and edges are springs. The Layout algorithm minimizes the stretching and compressing of all springs.(BTW, are the Spring Constants K the same for all springs?…)\n\n\nDirected and undirected network graph: If the distinction between source and target is meaningful, the network is directed. If the distinction is not meaningful, the network is undirected. Directed edges represent an ordering of nodes, like a relationship extending from one node to another, where switching the direction would change the structure of the network. Undirected edges are simply links between nodes where order does not matter.\n\n\n\n\n\n\n\nExamples\n\n\n\n\nThe World Wide Web is an example of a directed network because hyperlinks connect one Web page to another, but not necessarily the other way around.\nCo-authorship networks represent examples of un-directed networks, where nodes are authors and they are connected by an edge if they have written a publication together\nWhen people send e-mail to each other, the distinction between the sender (source) and the recipient (target) is clearly meaningful, therefore the network is directed.\n\n\n\n\n\nConnected and Disconnected graphs: If there is some path from any node to any other node, the Networks is said to be Connected. Else, Disconnected."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#predictruninfer-1",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#predictruninfer-1",
    "title": "The Grammar of Networks",
    "section": "Predict/Run/Infer-1",
    "text": "Predict/Run/Infer-1\nUsing tidygraph and ggraph\n\ntidygraph and ggraph are modern R packages for network data. Graph Data setup and manipulation is done in tidygraph and graph visualization with ggraph.\n\n\ntidygraph Data -&gt; “Network Object” in R.\n\nggraph Network Object -&gt; Plots using a chosen layout/algo.\n\nBoth leverage the power of igraph, which is the Big Daddy of all network packages. We will be using the Grey’s Anatomy dataset in our first foray into networks.\nStep1. Read the data\nDownload these two datasets into your current project-&gt; data folder.\n Grey’s Anatomy Nodes \n Grey’s Anatomy Edges \n\ngrey_nodes &lt;- read_csv(\"data/grey_nodes.csv\")\ngrey_edges &lt;- read_csv(\"data/grey_edges.csv\")\n\ngrey_nodes\ngrey_edges\n\n\n\n\n\n  \n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #1\n\n\n\nLook at the output thumbnails. What attributes (i.e. extra information) are seen for Nodes and Edges?\n\n\nStep 2.Create a network object using tidygraph:\nKey function:\n\n\ntbl_graph(): (aka “tibble graph”). Key arguments: nodes, edges and directed. Note this is a very versatile command and can take many input forms, such as data structures that result from other packages. Type ?tbl_graph in the Console and see the Usage section.\n\n\nga &lt;- tbl_graph(nodes = grey_nodes, \n                edges = grey_edges, \n                directed = FALSE)\nga\n\n# A tbl_graph: 54 nodes and 57 edges\n#\n# An undirected simple graph with 4 components\n#\n# A tibble: 54 × 7\n  name               sex   race  birthyear position  season sign  \n  &lt;chr&gt;              &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt; \n1 Addison Montgomery F     White      1967 Attending      1 Libra \n2 Adele Webber       F     Black      1949 Non-Staff      2 Leo   \n3 Teddy Altman       F     White      1969 Attending      6 Pisces\n4 Amelia Shepherd    F     White      1981 Attending      7 Libra \n5 Arizona Robbins    F     White      1976 Attending      5 Leo   \n6 Rebecca Pope       F     White      1975 Non-Staff      3 Gemini\n# ℹ 48 more rows\n#\n# A tibble: 57 × 4\n   from    to weight type    \n  &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt;   \n1     5    47      2 friends \n2    21    47      4 benefits\n3     5    46      1 friends \n# ℹ 54 more rows\n\n\n\n\n\n\n\n\nQuestions and Inferences #2\n\n\n\nWhat information does the graph object contain? What attributes do the nodes have? What about the edges?\n\n\nStep 3. Plot using ggraph\n\n3a. Quick Plot: autograph() This is to check quickly is the data is imported properly and to decide upon going on to a more elaborate plotting.\n\nautograph(ga)\n\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #3\n\n\n\nDescribe this graph, in simple words here. Try to use some of the new domain words we have just acquired: nodes/edges, connected/disconnected, directed/undirected.\n\n\n3b. More elaborate plot\nKey functions:\n\n\nggraph(layout = \"......\"): Create classic node-edge diagrams; i.e. Sets up the graph. Rather like ggplot for networks!\n\nTwo kinds of geom: one set for nodes, and another for edges\n\ngeom_node_point(aes(.....)): Draws node as “points”. Alternatives are circle / arc_bar / tile / voronoi. Remember the geoms that we have seen before in Grammar of Graphics!\ngeom_edge_link0(aes(.....)): Draws edges as “links”. Alternatives are arc / bend / elbow / hive / loop / parallel / diagonal / point / span /tile.\ngeom_node_text(aes(label = ......), repel = TRUE): Adds text labels (non-overlapping). Alternatives are label /...\nlabs(title = \"....\", subtitle = \"....\", caption = \"....\"): Change main titles, axis labels and legend titles. We know this from our work with ggplot.\n\n\n# Write Comments next to each line \n# About what that line does for the overall graph\n\nggraph(graph = ga,  layout = \"kk\") +\n  #\n  geom_edge_link0(width = 2, color = \"pink\") +\n  #\n  geom_node_point(shape = 21, size = 8,\n                  fill = \"blue\",\n                  color = \"green\",\n                  stroke = 2) +\n  \n  labs(title = \"Whoo Hoo! My First Silly Grey's Anatomy graph in R!\",\n       subtitle = \"Why did I ever get in this course...\",\n       caption = \"Bro, they are doing cool things in the other classes...\\n And the show is even more cool!\") +\n  \n  set_graph_style(family = \"merri\")\n\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #4:\n\n\n\nWhat parameters have been changed here, compared to the earlier graph? Where do you see these changes in the code above?\n\n\nLet us Play with this graph and see if we can make some small changes. Colour? Fill? Width? Size? Stroke? Labs? Of course!\n\n# Change the parameters in each of the commands here to new ones\n# Use fixed values for colours or sizes...etc. \n\nggraph(graph = ga,  layout = \"kk\") + \n  geom_edge_link0(width = 2) + \n  geom_node_point(shape = 21, size = 4, \n                  fill = \"moccasin\", \n                  color = \"firebrick\", \n                  stroke = 2) +\n  labs(title = \"Whoo Hoo! My next silly Grey's Anatomy graph in R!\",\n       subtitle = \"Why did I ever get in this course...\",\n       caption = \"Bro, they are doing cool things in the other classes...\") + set_graph_style(family = \"merri\")\n\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #5\n\n\n\nWhat did the shape parameter achieve? What are the possibilities with shape? How about including alpha?\n\n\n3c. Aesthetic Mapping from Node and Edge attribute columns\nUp to now, we have assigned specific numbers to geometric aesthetics such as shape and size. Now we are ready ( maybe ?) change the meaning and significance of the entire graph and each element within it, and use aesthetics / metaphoric mappings to achieve new meanings or insights. Let us try using aes() inside each geom to map a variable to a geometric aspect.\nDon’t try to use more than 2 aesthetic mappings simultaneously!!\nThe node elements we can tweak are:\n\nTypes of Nodes: geom_node_****()\n\nNode Parameters: inside geom_node_****(aes(...............))\n-aes(alpha  = node-variable) : opacity; a value between 0 and 1\n-aes(shape  = node-variable) : node shape\n-aes(colour = node-variable) : node colour\n-aes(fill   = node-variable) : fill colour for node\n-aes(size   = node-variable) : size of node\n\nThe edge elements we can tweak are:\n\nType of Edges” geom_edge_****()\n\nEdge Parameters: inside geom_edge_****(aes(...............))\n-aes(colour = edge-variable) : colour of the edge\n-aes(width  = edge-variable) : width of the edge\n-aes(label  = some_variable) : labels for the edge\n\nType ?geom_node_point and ?geom-edge_link in your Console for more information.\n\nggraph(graph = ga, layout = \"fr\") +\n  geom_edge_link0(aes(width = weight)) + # change variable here\n  \n  geom_node_point(aes(color = race), size = 6) + # change variable here\n\n  labs(title = \"Whoo Hoo! Yet another Grey's Anatomy graph in R!\",\n       subtitle = \"Colouring Nodes by Attribute\",\n       caption = \"Grey's Anatomy\") +\n  \n  scale_edge_width(range = c(0.2,2)) +\n  set_graph_style(family = \"merri\")\n\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #6\n\n\n\nDescribe some of the changes here. What types of edges worked? Which variables were you able to use for nodes and edges and how? What did not work with either of the two?"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#predictreuseinfer-2",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#predictreuseinfer-2",
    "title": "The Grammar of Networks",
    "section": "Predict/Reuse/Infer-2",
    "text": "Predict/Reuse/Infer-2\n\n# Arc diagram\n\nggraph(ga, layout = \"linear\") +\n  geom_edge_arc0(aes(width = weight), alpha = 0.8) +\n  scale_edge_width(range = c(0.2, 2)) +\n  geom_node_point(size = 2, colour = \"red\") +\n  labs(edge_width = \"Weight\",title = \"Grey's Anatomy\", subtitle = \"Arc Layout\") +\n  set_graph_style(family = \"merri\")\n\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #7\n\n\n\nHow does this graph look “metaphorically” different? Do you see a difference in the relationships between people here? Why?\n\n\n\n# Coord diagram, circular\nggraph(ga, layout = \"linear\", circular = TRUE) + # Note the layout!\n  geom_edge_arc0(aes(width = weight), alpha = 0.8) + \n  scale_edge_width(range = c(0.2, 2)) +\n  \n  geom_node_point(size = 4,colour = \"red\") + \n  geom_node_text(aes(label = name),\n                 repel = TRUE, size = 3,check_overlap = TRUE, \n                 max.overlaps = 25) +\n  labs(edge_width = \"Weight\")  +\n  theme(aspect.ratio = 1) +\n  set_graph_style(family = \"merri\")\n\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #8\n\n\n\nHow does this graph look “metaphorically” different? Do you see a difference in the relationships between people here? Why?"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#hierarchical-layouts",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#hierarchical-layouts",
    "title": "The Grammar of Networks",
    "section": "Hierarchical layouts",
    "text": "Hierarchical layouts\nThese provide for some alternative metaphorical views of networks. Note that not all layouts are possible for all datasets!!\n\n# set_graph_style()\n\n# This dataset contains the graph that describes the class \n# hierarchy for the Flare visualization library.\n# Type ?flare in your Console\nhead(flare$vertices)\n\n\n\n  \n\n\nhead(flare$edges)\n\n\n\n  \n\n\n# flare class hierarchy\ngraph &lt;-  tbl_graph(edges = flare$edges, nodes = flare$vertices)\n\n\nset_graph_style(family = \"merri\")\n# dendrogram\nggraph(graph, layout = \"dendrogram\") + \n  geom_edge_diagonal() + \n  labs(title = \"Dendrogram\") \n\n# circular dendrogram\nggraph(graph, layout = \"dendrogram\", circular = TRUE) + \n  geom_edge_diagonal0() + \n  geom_node_point(aes(filter = leaf)) + \n  coord_fixed()+ \n  labs(title = \"Circular Dendrogram\")\n\n\n# rectangular tree map\nggraph(graph, layout = \"treemap\", weight = size) + \n  geom_node_tile(aes(fill = depth), size = 0.25) + \n  labs(title = \"Rectangular Tree Map\")\n\n\n# circular tree map\nggraph(graph, layout = \"circlepack\", weight = size) + \n  geom_node_circle(aes(fill = depth), size = 0.25, n = 50) + \n  coord_fixed() + \n  labs(title = \"Circular Tree Map\")\n\n\n# icicle\nggraph(graph, layout = \"partition\") + \n  geom_node_tile(aes(y = -y, fill = depth))\n\n# sunburst (circular icicle)\nggraph(graph, layout = \"partition\", circular = TRUE) +\n  geom_node_arc_bar(aes(fill = depth)) +\n  coord_fixed() + \n  labs(title = \"Circular Icicle\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #9\n\n\n\nHow do graphs look “metaphorically” different? Do they reveal different aspects of the group? How?"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#faceting",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#faceting",
    "title": "The Grammar of Networks",
    "section": "Faceting",
    "text": "Faceting\nFaceting allows to create sub-plots according to the values of a qualitative attribute on nodes or edges.\n\nset_graph_style(family = \"merri\")\n\n# facet edges by type\nggraph(ga,layout = \"linear\", circular = TRUE) + \n  geom_edge_link0(aes(color = type)) + \n  geom_node_point() +\n  facet_edges(~ type) +\n  th_foreground(border = TRUE) +\n  theme(aspect.ratio = 1)\n# facet nodes by sex\nggraph(ga,layout = \"linear\", circular = TRUE) + \n  geom_edge_link0() + \n  geom_node_point() +\n  facet_nodes(~race) +\n  th_foreground(border = TRUE) +\n  theme(aspect.ratio = 1)\n# facet both nodes and edges\nggraph(ga,layout = \"linear\", circular = TRUE) + \n  geom_edge_link0(aes(color = type)) + \n  geom_node_point() +\n  facet_graph(type ~ race) + \n  th_foreground(border = TRUE) +\n  theme(aspect.ratio = 1, legend.position = \"right\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #10\n\n\n\nDoes splitting up the main graph into sub-networks give you more insight? Describe some of these."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#network-analysis-with-tidygraph",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#network-analysis-with-tidygraph",
    "title": "The Grammar of Networks",
    "section": "Network analysis with tidygraph",
    "text": "Network analysis with tidygraph\nThe data frame graph representation can be easily augmented with metrics or statistics computed on the graph. Remember how we computed counts with the penguin dataset in Grammar of Graphics.\nBefore computing a metric on nodes or edges use the activate() function to activate either node or edge data frames. Use dplyr verbs (filter, arrange, mutate) to achieve your computation in the proper way.\nNetwork Centrality: Go-To and Go-Through People!\nCentrality is a an “ill-defined” metric of node and edge importance in a network. It is therefore calculated in many ways. Type ?centrality in your Console.\n\n\nStandards\n\nLet’s add a few columns to the nodes and edges based on network centrality measures:\n\nga %&gt;% \n  activate(nodes) %&gt;% \n  \n  # Node with  the most connections?\n  mutate(degree = centrality_degree(mode = c(\"in\"))) %&gt;% \n  filter(degree &gt; 0) %&gt;% \n  \n  activate(edges) %&gt;% \n  # \"Busiest\" edge?\n  mutate(betweenness = centrality_edge_betweenness())\n\n# A tbl_graph: 54 nodes and 57 edges\n#\n# An undirected simple graph with 4 components\n#\n# A tibble: 57 × 5\n   from    to weight type     betweenness\n  &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;\n1     5    47      2 friends         20.3\n2    21    47      4 benefits        44.7\n3     5    46      1 friends         39  \n4     5    41      1 friends         66.3\n5    18    41      6 friends         39  \n6    21    41     12 benefits        91.5\n# ℹ 51 more rows\n#\n# A tibble: 54 × 8\n  name               sex   race  birthyear position  season sign   degree\n  &lt;chr&gt;              &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1 Addison Montgomery F     White      1967 Attending      1 Libra       3\n2 Adele Webber       F     Black      1949 Non-Staff      2 Leo         1\n3 Teddy Altman       F     White      1969 Attending      6 Pisces      4\n# ℹ 51 more rows\n\n\nPackages tidygraph and ggraph can be pipe-lined to perform analysis and visualization tasks in one go.\n\nset_graph_style(family = \"merri\")\n\n  ggraph(ga,layout = \"nicely\") +\n    geom_edge_link0(aes(alpha = centrality_edge_betweenness())) + \n    \n    geom_node_point(aes(colour = centrality_degree(), \n                        size = centrality_degree())) +\n    \n    geom_node_text(aes(label = name), repel = TRUE, size = 2.5) +\n    \n    scale_size(name = \"Degree\", range = c(0.5, 5)) + \n    \n    scale_color_gradient(name = \"Degree\", # SAME NAME!!\n                         low = \"blue\", high = \"red\", \n                         aesthetics = c(\"colour\", \"fill\"), \n                         guide = guide_legend(reverse = FALSE)) + \n    \n    scale_edge_alpha(name = \"Betweenness\", range = c(0.05, 1)) +\n  labs(title = \"Grey's Anatomy\", subtitle = \"Nodes Scaled by Degree\\n Edges shaded by Betweenness\")\n\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #11\n\n\n\nHow do the Centrality Measures show up in the graph? Would you “agree” with the way we have done it? Try to modify the aesthetics by copy-pasting this chunk below and see how you can make an alternative representation.\n\n\nAnalysis and Visualizing Network Communities\nWho is close to whom? Which are the groups you can see?\n\nset_graph_style(family = \"merri\")\n\n# visualize communities of nodes\nga %&gt;% \n  activate(nodes) %&gt;%\n  mutate(community = as.factor(group_louvain())) %&gt;% \n  ggraph(layout = \"graphopt\") + \n  geom_edge_link0() + \n  geom_node_point(aes(color = community), size = 5) +\n  labs(title = \"Grey's Anatomy\", subtitle = \"Nodes Coloured by Community Detection Algorithm (Louvain)\")\n\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #12\n\n\n\nIs the Community depiction clear? How would you do it, with which aesthetic? Copy Paste this chunk below and try.\n\n\nInteractive Graphs with visNetwork\n\nExploring the VisNetwork package. Make graphs wiggle and shake using tidy commands! The package implements interactivity using the physical metaphor of weights and springs we discussed earlier.\nThe visNetwork() function uses a nodes list and edges list to create an interactive graph. The nodes list must include an “id” column, and the edge list must have “from” and “to” columns. The function also plots the labels for the nodes, using the names of the cities from the “label” column in the node list.\n\nlibrary(visNetwork)\n\n# Prepare the data for plotting by visNetwork\ngrey_nodes\ngrey_edges\n# Relabel greys anatomy nodes and edges for VisNetwork\ngrey_nodes_vis &lt;- grey_nodes %&gt;% \n  rowid_to_column(var = \"id\") %&gt;% \n  rename(\"label\" = name) %&gt;% \n  mutate(sex = case_when(sex == \"F\" ~ \"Female\",\n                         sex == \"M\" ~ \"Male\")) %&gt;% \n  replace_na(., list(sex = \"Transgender?\")) %&gt;% \n  rename(\"group\" = sex)\ngrey_nodes_vis\ngrey_edges_vis &lt;- grey_edges %&gt;% \n  select(from, to) %&gt;% \n  left_join(., grey_nodes_vis, \n            by = c(\"from\" = \"label\")) %&gt;% \n  left_join(., grey_nodes_vis, \n            by = c(\"to\" = \"label\")) %&gt;%\n  select(\"from\"= id.x, \"to\" = id.y)\ngrey_edges_vis\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n\n\n\n\nUsing fontawesome icons\n\ngrey_nodes_vis %&gt;%\n\n  visNetwork(nodes = ., edges = grey_edges_vis) %&gt;% \n  visNodes(font = list(size = 40)) %&gt;% \n  \n  # Colour and icons for each of the gender-groups\n  visGroups(groupname = \"Female\", shape = \"icon\", \n            icon = list(code = \"f182\", size = 75, color = \"tomato\"),\n            shadow = list(enabled = TRUE)) %&gt;% \n  \n  visGroups(groupname = \"Male\", shape = \"icon\", \n            icon = list(code = \"f183\", size = 75, color = \"slateblue\"), \n            shadow = list(enabled = TRUE)) %&gt;% \n  \n  visGroups(groupname = \"Transgender?\", shape = \"icon\", \n            icon = list(code = \"f22c\", size = 75, color = \"fuchsia\"), \n            shadow = list(enabled = TRUE)) %&gt;% \n  \n  #visLegend() %&gt;%\n  #Add the fontawesome icons!!\n  addFontAwesome(version = \"4.7.0\") %&gt;% \n  \n  # Add Interaction Controls\n  visInteraction(navigationButtons = TRUE,\n                 hover = TRUE,\n                 selectConnectedEdges = TRUE,\n                 hoverConnectedEdges = TRUE,\n                 zoomView = TRUE)\n\n\n\n\n\nThere is another family of icons available in visNetwork, called ionicons. Let’s see how they look:\n\ngrey_nodes_vis %&gt;%\n\n  visNetwork(nodes = ., edges = grey_edges_vis,) %&gt;%\n  visLayout(randomSeed = 12345) %&gt;%\n  visNodes(font = list(size = 50)) %&gt;%\n  visEdges(color = \"green\") %&gt;%\n  visGroups(\n    groupname = \"Female\",\n    shape = \"icon\",\n    icon = list(\n      face = 'Ionicons',\n      code = \"f25d\",\n      color = \"fuchsia\",\n      size = 125\n    )\n  ) %&gt;%\n  \n  visGroups(\n    groupname = \"Male\",\n    shape = \"icon\",\n    icon = list(\n      face = 'Ionicons',\n      code = \"f202\",\n      color = \"green\",\n      size = 125\n    )\n  ) %&gt;%\n  \n  visGroups(\n    groupname = \"Transgender?\",\n    shape = \"icon\",\n    icon = list(\n      face = 'Ionicons',\n      code = \"f233\",\n      color = \"dodgerblue\",\n      size = 125\n    )\n  ) %&gt;%\n  visLegend() %&gt;%\n  addIonicons() %&gt;%\n  visInteraction(\n    navigationButtons = TRUE,\n    hover = TRUE,\n    selectConnectedEdges = TRUE,\n    hoverConnectedEdges = TRUE,\n    zoomView = TRUE\n  )\n\n\n\n\n\nSome idea of interactivity and controls with visNetwork:\n Star Wars Nodes \n Star Wars Edges \n\n# let's look again at the data\nstarwars_nodes &lt;- read_csv(\"data/star-wars-network-nodes.csv\")\nstarwars_edges &lt;- read_csv(\"data/star-wars-network-edges.csv\")\n\n\n# We need to rename starwars nodes dataframe and edge dataframe columns for visNetwork\nstarwars_nodes_vis &lt;- \n  starwars_nodes %&gt;% \n  rename(\"label\" = name)\n\n# Convert from and to columns to **node ids**\nstarwars_edges_vis &lt;- \n  starwars_edges %&gt;% \n  \n  # Matching Source &lt;- Source Node id (\"id.x\")\n  left_join(., starwars_nodes_vis, by = c(\"source\" = \"label\")) %&gt;% \n  \n  # Matching Target &lt;- Target Node id (\"id.y\")\n  left_join(., starwars_nodes_vis, by = c(\"target\" = \"label\")) %&gt;% \n  \n# Select \"id.x\" and \"id.y\" ONLY\n# Rename them as \"from\" and \"to\"\n# keep \"weight\" column for aesthetics of edges\n  select(\"from\" = id.x, \"to\" = id.y, \"value\" = weight)\n\n# Check everything once\nstarwars_nodes_vis\nstarwars_edges_vis\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n\n\n\n\nOk, let’s make things move and shake!!\n\nvisNetwork(nodes = starwars_nodes_vis,\n           edges = starwars_edges_vis) %&gt;% \n  visNodes(font = list(size = 30)) %&gt;% \n  visEdges(color = \"red\")\n\n\n\n\n\n\nvisNetwork(nodes = starwars_nodes_vis,\n           edges = starwars_edges_vis) %&gt;% \n  visNodes(font = list(size = 30), shape = \"icon\", \n           icon = list(code = \"f1e3\", size = 75)) %&gt;% \n  visEdges(color = list(color = \"red\", hover = \"green\", highlight = \"black\")) %&gt;% \n  visInteraction(hover = TRUE) %&gt;% \n  addFontAwesome(version = \"4.7.0\")"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#your-assignments",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#your-assignments",
    "title": "The Grammar of Networks",
    "section": "Your Assignments:",
    "text": "Your Assignments:\nMake-1 : With a ready made dataset\nStep 0. Fire up a New Project! Always!\nStep 1. Fire up a new Quarto document. Fill in the YAML header.\nStep 2. Take any one of the “Make1-Datasets” datasets described below.\nStep 3. Document contents:\n\nIntroduce / Inspect in R your data and describe\nIntroduce your Purpose\nCreate graph objects\nTry different layouts\nWrite comments in the code\nWrite narrative in text with sections, bold ,italic etc.\n\nStep 4. Knit before you submit. Submit only your renderable .qmd file.\nMake1 - Datasets:\n\nAirline Data:\n\n Airlines Nodes \n Airlines Edges \n\nStart with this bit of code in your second chunk, after set up\n\n\n\nairline_nodes &lt;-\n  read_csv(\"./mydatafolder/AIRLINES-NODES.csv\") %&gt;% \n  mutate(Id = Id + 1)\n\nairline_edges &lt;-\n  read_csv(\"./mydatafolder/AIRLINES-EDGES.csv\") %&gt;%\n  mutate(Source = Source + 1, Target = Target + 1)\n\n\nThe Famous Zachary Karate Club dataset\n\n\n\nStart with pulling this data into your Rmarkdown:\ndata(\"karate\",package= \"igraphdata\")\nkarate\n\nTry ?karate in the console\nNote that this is not a set of nodes, nor edges, but already a graph-object!\nSo no need to create a graph object using tbl_graph.\nYou will need to just go ahead and plot using ggraph.\n\n\nGame of Thrones:\n\n GoT Networks \n\nStart with pulling this data into your Rmarkdown:\n\n\nGoT &lt;- read_rds(\"data/GoT.RDS\")\n\n\nNote that this is a list of 7 graphs from Game of Thrones.\nSelect one using GoT[[index]] where index = 1…7 and then plot directly.\nTry to access the nodes and edges and modify them using any attribute data\n\n\nAny other graph dataset from igraphdata (type ?igraphdata in console)\n\n\nAsk me for help if you need any\nMake-2: Literary Network with TV Show / Book / Story / Play\nYou need to create a Network Graph for your favourite Book, play, TV serial or Show. (E.g. Friends, BBT, or LB or HIMYM, B99, TGP, JTV…or Hamlet, Little Women , Pride and Prejudice, or LoTR)\nStep 1. Go to: Literary Networks for instructions.\nStep 2. Make your data using the instructions.\n\nIn the nodes excel, use id and names as your columns. Any other details in other columns to the right.\nIn your edges excel, use from and to are your first columns. Entries in these columns can be names or ids but be consistent and don’t mix.\n\nStep 3. Decide on 3 answers that you to seek and plan to make graphs for.\nStep 4. Create graph objects. Say 3 visualizations.\nStep 5. Write comments/answers in the code and narrative text. Add pictures from the web using Markdown syntax.\nStep 6. Write Reflection ( ok, a short one!) inside your Quarto document. Make sure it renders !!\nStep 7. Group Submission: Submit the render-able .qmd file AND the data. Quarto Markdown with joint authorship. Each person submits on their Assignments. All get the same grade on this one.\nAsk me for clarifications on what to do after you have read the Instructions in your group."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#read-more",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#read-more",
    "title": "The Grammar of Networks",
    "section": "Read more",
    "text": "Read more\n\nThomas Lin Pedersen - 1 giraffe, 2 giraffe,GO!\nIgraph: Network Analysis and Visualization. https://CRAN.R-project.org/package=igraph.\nPedersen, Thomas Lin. 2017a. Ggraph: An Implementation of Grammar of Graphics for Graphs and Networks. https://CRAN.R-project.org/package=ggraph.\n———. 2017b. Tidygraph: A Tidy Api for Graph Manipulation. https://CRAN.R-project.org/package=tidygraph.\nTyner, Sam, François Briatte, and Heike Hofmann. 2017. “Network Visualization with ggplot2.” The R Journal 9 (1): 27–59. https://journal.r-project.org/archive/2017/RJ-2017-023/index.html.\nNetwork Datasets https://icon.colorado.edu/#!/networks\nYunran Chen, Introduction to Network Analysis Using R"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/180-RefMat/other-tools.html",
    "href": "content/courses/Analytics/Descriptive/Modules/180-RefMat/other-tools.html",
    "title": "📚 Miscellaneous Graphing Tools, and References",
    "section": "",
    "text": "https://rawgraphs.io\n\n\n\nhttps://datawrapper.de\n\n\n\nhttps://hdlab.stanford.edu/palladio/\n\n\n\nhttps://infogram.com/\n\n\n\nhttps://www.visme.co/chart-maker/\n\n\n\nhttps://flourish.studio/ https://www.figma.com/"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/180-RefMat/other-tools.html#what-other-free-tools-are-there-on-the-web",
    "href": "content/courses/Analytics/Descriptive/Modules/180-RefMat/other-tools.html#what-other-free-tools-are-there-on-the-web",
    "title": "📚 Miscellaneous Graphing Tools, and References",
    "section": "",
    "text": "https://rawgraphs.io\n\n\n\nhttps://datawrapper.de\n\n\n\nhttps://hdlab.stanford.edu/palladio/\n\n\n\nhttps://infogram.com/\n\n\n\nhttps://www.visme.co/chart-maker/\n\n\n\nhttps://flourish.studio/ https://www.figma.com/"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/180-RefMat/other-tools.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/180-RefMat/other-tools.html#references",
    "title": "📚 Miscellaneous Graphing Tools, and References",
    "section": "References",
    "text": "References\n\nGetting started with Flourish & Figma to create beautiful custom charts, https://inside.mediahack.co.za/getting-started-with-flourish-figma-to-create-beautiful-custom-charts-34e4efb8fd3d\nFlowing Data Chart Types https://flowingdata.com/chart-types/\nGeeks for Geeks: Chart Types https://www.geeksforgeeks.org/r-charts-and-graphs/\nFinancial Times Visual Vocabulary (Interactive) https://ft-interactive.github.io/visual-vocabulary/\nFinancial Times Visual Vocabulary (PDF) https://github.com/Financial-Times/chart-doctor/blob/main/visual-vocabulary/FT4schools_RGS.pdf\nFinancial Times Data Journalism Visuals https://www.ft.com/visual-and-data-journalism\nSeverino Ribecca and John Schwabish , The Graphic Continuum https://www.severinoribecca.one/portfolio-item/the-graphic-continuum/\nWeb based tools for Dataviz https://policyviz.com/resources/data-viz-tools/\nNightingale Data Visualization Society Blog: How to visualize categorical data: https://nightingaledvs.com/endless-river-an-overview-of-dataviz-for-categorical-data/\nJohn Schwabish’s policyviz Data Viz catalogue: https://datastudio.google.com/s/quUUlgosF4U"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/180-RefMat/other-tools.html#papers",
    "href": "content/courses/Analytics/Descriptive/Modules/180-RefMat/other-tools.html#papers",
    "title": "📚 Miscellaneous Graphing Tools, and References",
    "section": "Papers",
    "text": "Papers\n1.Christopher G. Healey Department of Computer Science, North Carolina State University. Perception in Visualization"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html",
    "title": "EDA: Exploring Static Graphs for Distributions in R",
    "section": "",
    "text": "options(digits = 3)\nlibrary(tidyverse)\nlibrary(mosaic) # package for stats, simulations, and basic plots\nlibrary(mosaicData) # package containing datasets\nlibrary(ggformula) # package for professional looking plots, that use the formula interface from mosaic\nlibrary(skimr) # Summary statistics about variables in data frames\nlibrary(NHANES) # survey data collected by the US National Center for Health Statistics (NCHS)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html#iconify-noto-v1-package-setup-the-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html#iconify-noto-v1-package-setup-the-packages",
    "title": "EDA: Exploring Static Graphs for Distributions in R",
    "section": "",
    "text": "options(digits = 3)\nlibrary(tidyverse)\nlibrary(mosaic) # package for stats, simulations, and basic plots\nlibrary(mosaicData) # package containing datasets\nlibrary(ggformula) # package for professional looking plots, that use the formula interface from mosaic\nlibrary(skimr) # Summary statistics about variables in data frames\nlibrary(NHANES) # survey data collected by the US National Center for Health Statistics (NCHS)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html#iconify-fxemoji-japanesesymbolforbeginner-introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html#iconify-fxemoji-japanesesymbolforbeginner-introduction",
    "title": "EDA: Exploring Static Graphs for Distributions in R",
    "section": "\n Introduction",
    "text": "Introduction\nWe will create Distributions for data in R. As always, we will consistently use the Project Mosaic ecosystem of packages in R (mosaic, mosaicData and ggformula).\n\n\n\n\n\n\nTip\n\n\n\nNote the standard method for all commands from the mosaic package: goal( y ~ x | z, data = mydata, …)\nWith ggformula, one can create any graph/chart using: gf_geometry(y ~ x | z, data = mydata) OR mydata %&gt;% gf_geometry( y ~ x | z)\nThe second method may be preferable, especially if you have done some data manipulation first! More later!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html#iconify-pajamas-issue-type-test-case-case-study-1-galton-dataset-from-mosaicdata",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html#iconify-pajamas-issue-type-test-case-case-study-1-galton-dataset-from-mosaicdata",
    "title": "EDA: Exploring Static Graphs for Distributions in R",
    "section": "\n Case Study #1: Galton Dataset from mosaicData\n",
    "text": "Case Study #1: Galton Dataset from mosaicData\n\nLet us inspect what datasets are available in the package mosaicData. Type data(package = \"mosaicData\") in your Console to see what datasets are available.\nLet us choose the famous Galton dataset:\n\n```{r}\ndata(\"Galton\")\nGalton_inspect &lt;- inspect(Galton)\nGalton_inspect\n```\n\n\ncategorical variables:  \n    name  class levels   n missing\n1 family factor    197 898       0\n2    sex factor      2 898       0\n                                   distribution\n1 185 (1.7%), 166 (1.2%), 66 (1.2%) ...        \n2 M (51.8%), F (48.2%)                         \n\nquantitative variables:  \n    name   class min Q1 median   Q3  max  mean   sd   n missing\n1 father numeric  62 68   69.0 71.0 78.5 69.23 2.47 898       0\n2 mother numeric  58 63   64.0 65.5 70.5 64.08 2.31 898       0\n3 height numeric  56 64   66.5 69.7 79.0 66.76 3.58 898       0\n4  nkids integer   1  4    6.0  8.0 15.0  6.14 2.69 898       0\n\n\nThe data is described as:\n\nA data frame with 898 observations on the following variables.\n\n\nfamily a factor with levels for each family\n\nfather the father’s height (in inches)\n\nmother the mother’s height (in inches)\n\nsex the child’s sex: F or M\n\nheight the child’s height as an adult (in inches)\n\nnkids the number of adult children in the family, or, at least, the number whose heights Galton recorded.\n\n\nThere is a lot of Description generated by the mosaic::inspect() command ! What can we say about the dataset and its variables? How big is the dataset? How many variables? What types are they, Quant or Qual? If they are Qual, what are the levels? Are they ordered levels? Discuss!\n\n Counts, and Charts with Counts\nNow that we know the variables, let us look at counts of data observations(rows). We know from our examination of variable types that counting of observations must be done on the basis of Qualitative variables. So let us count and plot the counts in bar charts.\n\n\n\n\n\n\nQuestion\n\n\n\nQ.1 How many families in the data for each value of nkids(i.e. Count of families by size)?\n\n```{r}\n#| layout-ncol: 2\nGalton_counts &lt;- Galton %&gt;%\n  group_by(nkids) %&gt;% \n  summarise(children = n()) %&gt;% \n  # just to check\n  mutate(\n    No_of_families = as.integer(children/nkids),\n    # Why do we divide\n    \n    running_count_of_children = cumsum(children),\n    running_count_of_families = cumsum(No_of_families)) \nGalton_counts\n\nGalton_counts %&gt;% \n  gf_col(No_of_families ~ nkids) %&gt;% \n  gf_theme(theme_classic())\n```\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nInsight: There are 32 1-kid families; and \\(128/8 = 16\\) 8-kid families! There is one great great 15-kid family. (Did you get the idea behind why we divide here?)\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.2. What is the count of Children by sex of the child and by family size nkids?\n\n```{r}\n#| layout-ncol: 2\n#| message: false\n#| warning: false\nGalton_counts_by_sex &lt;- Galton %&gt;% \n  mutate(family = as.integer(family)) %&gt;% \n  group_by(nkids, sex) %&gt;% \n  summarise(count_by_sex = n()) %&gt;% \n  ungroup() %&gt;% \n  group_by(sex) \nGalton_counts_by_sex\n\nGalton_counts_by_sex %&gt;% \n  gf_col(count_by_sex ~ nkids | sex, fill = ~sex) %&gt;% \n  gf_theme(theme_classic())\n```\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nInsight: Hmm…decent gender balance overall, across family sizes nkids.\n\n\n\n\n\n\n\n\nFollow-up Question\n\n\n\nFollow up Question: How would we look for “gender balance” in individual families? Should we look at the family column ?\n\n```{r}\n#| message: false\n#| warning: false\n#| layout-ncol: 2\n\nGalton_gender_family &lt;- Galton %&gt;% \n  mutate(family = as.integer(family)) %&gt;% \n  group_by(family, sex) %&gt;% \n  summarise(count_by_sex = n()) %&gt;% \n  ungroup() %&gt;% \n  group_by(sex)\nGalton_gender_family\nGalton_gender_family %&gt;% \n  gf_col(count_by_sex ~ family | sex, fill = ~ sex) %&gt;% \n  gf_theme(theme_minimal())\n```\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nInsight: The No of Children were distributed similarly across family sizenkids… However, this plot is too crowded and does not lead to any great insight. Using family ID was silly to plot against, wasn’t it? Not all exploratory plots will be “necessary” in the end. But they are part of the journey of getting better acquainted with the data!\n\n\n\n Stat Summaries and Distributions\nOK, on to the Quantitative variables now! What Questions might we have, that could relate not to counts by Qual variables, but to the numbers in Quant variables. Stat measures, like their ranges, max and min? Means, medians, distributions? And how these vary on the basis of Qual variables? All this using histograms and densities.\n\n\n\n\n\n\nSummary Stats\n\n\n\nAs Stigler[@stigler2016] said, summaries are the first thing to look at in data. skimr::skim has already given us a lot summary data for Quant variables. We can now use mosaic::favstats to develop these further, by slicing / facetting these wrt other Qual variables. Let us tabulate some quick stat summaries of the important variables in Galton.\n\n```{r}\n# summaries facetted by sex of child\nGalton_measures &lt;- favstats(~ height | sex, data = Galton)\nGalton_measures\n```\n\n\n\n  \n\n\n\nInsight: We saw earlier that the mean height of the Children was 66 inches. However, are Sons taller than Daughters? Difference in mean height is 5 inches! AND…that was the same difference between fathers and mothers mean heights! Is it so simple then?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.4 How are the heights of the children distributed? Here is where we need a gf_histogram…\n\n```{r}\nGalton %&gt;% \n  gf_histogram(~ height,bins = 30) %&gt;% # ALWAYS try several settings for \"bins\"\n  gf_vline(xintercept = mean(Galton$height), color = \"red\") %&gt;% \n  gf_label(label = glue::glue(\"Mean Value\\n {mean(Galton$height)}\"),\n           gformula = 85 ~ 70) %&gt;% # Where do we want the label (y ~ x) %&gt;% \n  gf_labs(title = \"Galton Dataset\",\n          subtitle = \"\",\n          x = \"Heights of Children\",\n          y = \"Counts\",\n          caption = \"Using `ggformula` and Classic Theme\") %&gt;% \n  gf_theme(theme_classic())\n```\n\n\n\n\nInsight: Fairly symmetric distribution…but there are a few very short and some very tall children! Always try to change the no. of bins to check of we are missing some pattern.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.5 Is there a difference in height distributions between Male and Female children?(Quant variable sliced by Qual variable)\n\n```{r}\nmean_daughters &lt;- Galton_measures %&gt;% \n             filter(sex == \"F\") %&gt;% select(mean) %&gt;% as.numeric()\nmean_sons &lt;- Galton_measures %&gt;% \n             filter(sex == \"M\") %&gt;% select(mean) %&gt;% as.numeric()\nGalton %&gt;% \n  gf_density(~ height, group = ~sex, fill = ~ sex, alpha = 0.5) %&gt;% \n  gf_vline(xintercept = mean_daughters) %&gt;%\n  gf_vline(xintercept = mean_sons) %&gt;% \n  \n   gf_label(label = glue::glue(\"Mean Daughters' heights\\n {mean_daughters}\"),\n           gformula = 0.15 ~ 59, # Where do we want the label (y ~ x) \n           show.legend = FALSE,\n           fill = \"white\") %&gt;% \n  \n   gf_label(label = glue::glue(\"Mean Sons' heights \\n {mean_sons}\"),\n           gformula = 0.15 ~ 75,  # Where do we want the label (y ~ x) \n           show.legend = FALSE,\n           fill = \"white\") %&gt;% \n  gf_theme(theme_classic())\n```\n\n\n\n\nInsight: There is a visible difference in average heights between girls and boys. Is that significant, however? We will need a statistical inference test to figure that out!! Claus Wilke1 says comparisons of Quant variables across groups are best made between densities and not histograms…\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.6 Are Mothers generally shorter than fathers?\n\n```{r}\nmean_fathers &lt;- Galton %&gt;% mosaic::mean(~ father, data = .) %&gt;% as.numeric()\n\nmean_mothers &lt;- Galton%&gt;% mean(~ mother, data = .) %&gt;% as.numeric()\n\nGalton %&gt;% \n  gf_density(~ father, fill = \"dodgerblue\", alpha = 0.3) %&gt;% \n  gf_density(~ mother, fill = \"red\", alpha = 0.3) %&gt;% \n    gf_vline(xintercept = mean_mothers) %&gt;%\n  gf_vline(xintercept = mean_fathers) %&gt;% \n  gf_labs(x = \"Parents' Heights\") %&gt;% \n    \n   gf_label(label = glue::glue(\" Average Mother \\n {mean_mothers}\"),\n           gformula = 0.15 ~ 60, # Where do we want the label (y ~ x) \n           show.legend = FALSE,\n           fill = \"white\") %&gt;% \n  \n   gf_label(label = glue::glue(\"Average Father\\n {mean_fathers}\"),\n           gformula = 0.15 ~ 75,  # Where do we want the label (y ~ x) \n           show.legend = FALSE,\n           fill = \"white\") %&gt;% \n  \n  gf_theme(theme_classic())\n```\n\n\n\n\nInsight: Yes, moms are on average shorter than dads in this dataset. Again, is this difference statistically significant? We will find out in when we do Inference.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.7a. Are heights of children different based on the number of kids in the family? And For Male and Female children?\n\n```{r}\n# Boxplot series for daughters\ngf_boxplot(height ~ nkids, group = ~ nkids, fill = ~ sex, \n           data = Galton %&gt;% filter(sex == \"F\")) %&gt;% \n# Boxplot series for sons\n  gf_boxplot(height ~ nkids, group = ~ nkids, fill = ~ sex, \n             data = Galton %&gt;% filter(sex == \"M\")) %&gt;% \n  gf_labs(title = \"Daughters vs Sons: Who is Taller?\",\n          subtitle = \"Family Size does not change height disparity ;-D\",\n          x = \"Family Size\", y = \"Heights of Children\",\n          caption = \"Made with `ggformula` and Classic Theme\") %&gt;% \n  \n  gf_theme(theme_classic())\n```\n\n\n\n\nInsight: So, at all family “strengths” (nkids), the sons are taller than the daughters, based on distributions. Box plots are used to show distributions of numeric data values and compare them between multiple groups (i.e Categorical Data, here sex and nkids).\n\n\n\n\n\n\n\n\nFollow-up Question\n\n\n\nQ. 8a. Is height difference between sons and daughters related to height difference between father and mother?\nDifferences between father and mother heights influencing height…this would be like height ~ (father-mother). This would be a relationship between two Quant variables. A histogram would not serve here and we plot this as a Scatter Plot:\n\n```{r}\n#| layout: [[40], [60]]\nGalton_parents_diff &lt;- Galton %&gt;% \n  group_by(family,sex) %&gt;% \n  \n  # Parental Height Difference\n  mutate(diff_height = father - mother) %&gt;% \n  select(family, sex, height, diff_height) %&gt;% \n  ungroup() %&gt;% \n  group_by(sex)\nGalton_parents_diff\n\nGalton_parents_diff %&gt;% \n  gf_point(height ~ diff_height, color = ~ sex, data = Galton_parents_diff) %&gt;%\n  gf_lims(y = c(55,82)) %&gt;% # note: Y-AXIS does not go to zero!!\n  \n  gf_rect(55 + 80 ~ 0 + 18, fill = \"grey80\", \n          color = \"white\", alpha = 0.02) %&gt;%\n  gf_rect(55 + 80 ~ -5 + 0, fill = \"moccasin\", \n          color = \"white\", alpha = 0.02) %&gt;%   \n  \n  # Note the repetition!\n  gf_point(height ~ diff_height, color = ~ sex, data = Galton_parents_diff) %&gt;%\n  gf_smooth(method = \"lm\", se = FALSE) %&gt;% \n  gf_labs(x = \"Parental Height Difference\",\n          y = \"Children's Heights\") %&gt;% \n  \n  gf_label(label = glue::glue(\"Taller Mothers\"),\n           gformula = 75 ~ -2.5, # Where do we want the label (y ~ x) \n           show.legend = FALSE,\n           fill = \"white\") %&gt;% \n  \n  gf_label(label = glue::glue(\"Taller Fathers\"),\n           gformula = 75 ~ 15,  # Where do we want the label (y ~ x) \n           show.legend = FALSE,\n           fill = \"white\") %&gt;% \n  \n  gf_vline(xintercept = 0, linewidth = 2, \n           title = \"Does Parental Height Difference affect \n                    Children's Height?\",\n           caption = \"Note Y-axis does not go to zero\") %&gt;% \n  \n  gf_theme(theme_classic())\n```\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nInsight: There seems no relationship, or a very small one, between children’s heights on the y-axis and the difference in parental height differences (father - mother) on the x-axis…\n\n\nAnd so on…..we can proceed from simple visualizations based on Questions to larger questions that demand inference and modelling. We hinted briefly on these in the above Case Study."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html#iconify-pajamas-issue-type-test-case-case-study-2-dataset-from-nhanes",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html#iconify-pajamas-issue-type-test-case-case-study-2-dataset-from-nhanes",
    "title": "EDA: Exploring Static Graphs for Distributions in R",
    "section": "\n Case Study-2: Dataset from NHANES\n",
    "text": "Case Study-2: Dataset from NHANES\n\nLet us try the NHANES dataset. Try help(NHANES) in your Console.\n\n```{r}\ndata(\"NHANES\")\n```\n\n\n Look at the Data\n\n```{r}\n#| column: body-outset-right\nskimr::skim(NHANES)\n```\n\n\nData summary\n\n\nName\nNHANES\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n76\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n31\n\n\nnumeric\n45\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nSurveyYr\n0\n1.00\nFALSE\n2\n200: 5000, 201: 5000\n\n\nGender\n0\n1.00\nFALSE\n2\nfem: 5020, mal: 4980\n\n\nAgeDecade\n333\n0.97\nFALSE\n8\n40: 1398, 0-: 1391, 10: 1374, 20: 1356\n\n\nRace1\n0\n1.00\nFALSE\n5\nWhi: 6372, Bla: 1197, Mex: 1015, Oth: 806\n\n\nRace3\n5000\n0.50\nFALSE\n6\nWhi: 3135, Bla: 589, Mex: 480, His: 350\n\n\nEducation\n2779\n0.72\nFALSE\n5\nSom: 2267, Col: 2098, Hig: 1517, 9 -: 888\n\n\nMaritalStatus\n2769\n0.72\nFALSE\n6\nMar: 3945, Nev: 1380, Div: 707, Liv: 560\n\n\nHHIncome\n811\n0.92\nFALSE\n12\nmor: 2220, 750: 1084, 250: 958, 350: 863\n\n\nHomeOwn\n63\n0.99\nFALSE\n3\nOwn: 6425, Ren: 3287, Oth: 225\n\n\nWork\n2229\n0.78\nFALSE\n3\nWor: 4613, Not: 2847, Loo: 311\n\n\nBMICatUnder20yrs\n8726\n0.13\nFALSE\n4\nNor: 805, Obe: 221, Ove: 193, Und: 55\n\n\nBMI_WHO\n397\n0.96\nFALSE\n4\n18.: 2911, 30.: 2751, 25.: 2664, 12.: 1277\n\n\nDiabetes\n142\n0.99\nFALSE\n2\nNo: 9098, Yes: 760\n\n\nHealthGen\n2461\n0.75\nFALSE\n5\nGoo: 2956, Vgo: 2508, Fai: 1010, Exc: 878\n\n\nLittleInterest\n3333\n0.67\nFALSE\n3\nNon: 5103, Sev: 1130, Mos: 434\n\n\nDepressed\n3327\n0.67\nFALSE\n3\nNon: 5246, Sev: 1009, Mos: 418\n\n\nSleepTrouble\n2228\n0.78\nFALSE\n2\nNo: 5799, Yes: 1973\n\n\nPhysActive\n1674\n0.83\nFALSE\n2\nYes: 4649, No: 3677\n\n\nTVHrsDay\n5141\n0.49\nFALSE\n7\n2_h: 1275, 1_h: 884, 3_h: 836, 0_t: 638\n\n\nCompHrsDay\n5137\n0.49\nFALSE\n7\n0_t: 1409, 0_h: 1073, 1_h: 1030, 2_h: 589\n\n\nAlcohol12PlusYr\n3420\n0.66\nFALSE\n2\nYes: 5212, No: 1368\n\n\nSmokeNow\n6789\n0.32\nFALSE\n2\nNo: 1745, Yes: 1466\n\n\nSmoke100\n2765\n0.72\nFALSE\n2\nNo: 4024, Yes: 3211\n\n\nSmoke100n\n2765\n0.72\nFALSE\n2\nNon: 4024, Smo: 3211\n\n\nMarijuana\n5059\n0.49\nFALSE\n2\nYes: 2892, No: 2049\n\n\nRegularMarij\n5059\n0.49\nFALSE\n2\nNo: 3575, Yes: 1366\n\n\nHardDrugs\n4235\n0.58\nFALSE\n2\nNo: 4700, Yes: 1065\n\n\nSexEver\n4233\n0.58\nFALSE\n2\nYes: 5544, No: 223\n\n\nSameSex\n4232\n0.58\nFALSE\n2\nNo: 5353, Yes: 415\n\n\nSexOrientation\n5158\n0.48\nFALSE\n3\nHet: 4638, Bis: 119, Hom: 85\n\n\nPregnantNow\n8304\n0.17\nFALSE\n3\nNo: 1573, Yes: 72, Unk: 51\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nID\n0\n1.00\n61944.64\n5871.17\n51624.00\n56904.50\n62159.50\n67039.00\n7.19e+04\n▇▇▇▇▇\n\n\nAge\n0\n1.00\n36.74\n22.40\n0.00\n17.00\n36.00\n54.00\n8.00e+01\n▇▇▇▆▅\n\n\nAgeMonths\n5038\n0.50\n420.12\n259.04\n0.00\n199.00\n418.00\n624.00\n9.59e+02\n▇▇▇▆▃\n\n\nHHIncomeMid\n811\n0.92\n57206.17\n33020.28\n2500.00\n30000.00\n50000.00\n87500.00\n1.00e+05\n▃▆▃▁▇\n\n\nPoverty\n726\n0.93\n2.80\n1.68\n0.00\n1.24\n2.70\n4.71\n5.00e+00\n▅▅▃▃▇\n\n\nHomeRooms\n69\n0.99\n6.25\n2.28\n1.00\n5.00\n6.00\n8.00\n1.30e+01\n▂▆▇▂▁\n\n\nWeight\n78\n0.99\n70.98\n29.13\n2.80\n56.10\n72.70\n88.90\n2.31e+02\n▂▇▂▁▁\n\n\nLength\n9457\n0.05\n85.02\n13.71\n47.10\n75.70\n87.00\n96.10\n1.12e+02\n▁▃▆▇▃\n\n\nHeadCirc\n9912\n0.01\n41.18\n2.31\n34.20\n39.58\n41.45\n42.92\n4.54e+01\n▁▂▇▇▅\n\n\nHeight\n353\n0.96\n161.88\n20.19\n83.60\n156.80\n166.00\n174.50\n2.00e+02\n▁▁▁▇▂\n\n\nBMI\n366\n0.96\n26.66\n7.38\n12.88\n21.58\n25.98\n30.89\n8.12e+01\n▇▆▁▁▁\n\n\nPulse\n1437\n0.86\n73.56\n12.16\n40.00\n64.00\n72.00\n82.00\n1.36e+02\n▂▇▃▁▁\n\n\nBPSysAve\n1449\n0.86\n118.15\n17.25\n76.00\n106.00\n116.00\n127.00\n2.26e+02\n▃▇▂▁▁\n\n\nBPDiaAve\n1449\n0.86\n67.48\n14.35\n0.00\n61.00\n69.00\n76.00\n1.16e+02\n▁▁▇▇▁\n\n\nBPSys1\n1763\n0.82\n119.09\n17.50\n72.00\n106.00\n116.00\n128.00\n2.32e+02\n▂▇▂▁▁\n\n\nBPDia1\n1763\n0.82\n68.28\n13.78\n0.00\n62.00\n70.00\n76.00\n1.18e+02\n▁▁▇▆▁\n\n\nBPSys2\n1647\n0.84\n118.48\n17.49\n76.00\n106.00\n116.00\n128.00\n2.26e+02\n▃▇▂▁▁\n\n\nBPDia2\n1647\n0.84\n67.66\n14.42\n0.00\n60.00\n68.00\n76.00\n1.18e+02\n▁▁▇▆▁\n\n\nBPSys3\n1635\n0.84\n117.93\n17.18\n76.00\n106.00\n116.00\n126.00\n2.26e+02\n▃▇▂▁▁\n\n\nBPDia3\n1635\n0.84\n67.30\n14.96\n0.00\n60.00\n68.00\n76.00\n1.16e+02\n▁▁▇▇▁\n\n\nTestosterone\n5874\n0.41\n197.90\n226.50\n0.25\n17.70\n43.82\n362.41\n1.80e+03\n▇▂▁▁▁\n\n\nDirectChol\n1526\n0.85\n1.36\n0.40\n0.39\n1.09\n1.29\n1.58\n4.03e+00\n▅▇▂▁▁\n\n\nTotChol\n1526\n0.85\n4.88\n1.08\n1.53\n4.11\n4.78\n5.53\n1.36e+01\n▂▇▁▁▁\n\n\nUrineVol1\n987\n0.90\n118.52\n90.34\n0.00\n50.00\n94.00\n164.00\n5.10e+02\n▇▅▂▁▁\n\n\nUrineFlow1\n1603\n0.84\n0.98\n0.95\n0.00\n0.40\n0.70\n1.22\n1.72e+01\n▇▁▁▁▁\n\n\nUrineVol2\n8522\n0.15\n119.68\n90.16\n0.00\n52.00\n95.00\n171.75\n4.09e+02\n▇▆▃▂▁\n\n\nUrineFlow2\n8524\n0.15\n1.15\n1.07\n0.00\n0.48\n0.76\n1.51\n1.37e+01\n▇▁▁▁▁\n\n\nDiabetesAge\n9371\n0.06\n48.42\n15.68\n1.00\n40.00\n50.00\n58.00\n8.00e+01\n▁▂▆▇▂\n\n\nDaysPhysHlthBad\n2468\n0.75\n3.33\n7.40\n0.00\n0.00\n0.00\n3.00\n3.00e+01\n▇▁▁▁▁\n\n\nDaysMentHlthBad\n2466\n0.75\n4.13\n7.83\n0.00\n0.00\n0.00\n4.00\n3.00e+01\n▇▁▁▁▁\n\n\nnPregnancies\n7396\n0.26\n3.03\n1.80\n1.00\n2.00\n3.00\n4.00\n3.20e+01\n▇▁▁▁▁\n\n\nnBabies\n7584\n0.24\n2.46\n1.32\n0.00\n2.00\n2.00\n3.00\n1.20e+01\n▇▅▁▁▁\n\n\nAge1stBaby\n8116\n0.19\n22.65\n4.77\n14.00\n19.00\n22.00\n26.00\n3.90e+01\n▆▇▅▂▁\n\n\nSleepHrsNight\n2245\n0.78\n6.93\n1.35\n2.00\n6.00\n7.00\n8.00\n1.20e+01\n▁▅▇▁▁\n\n\nPhysActiveDays\n5337\n0.47\n3.74\n1.84\n1.00\n2.00\n3.00\n5.00\n7.00e+00\n▇▇▃▅▅\n\n\nTVHrsDayChild\n9347\n0.07\n1.94\n1.43\n0.00\n1.00\n2.00\n3.00\n6.00e+00\n▇▆▂▂▂\n\n\nCompHrsDayChild\n9347\n0.07\n2.20\n2.52\n0.00\n0.00\n1.00\n6.00\n6.00e+00\n▇▁▁▁▃\n\n\nAlcoholDay\n5086\n0.49\n2.91\n3.18\n1.00\n1.00\n2.00\n3.00\n8.20e+01\n▇▁▁▁▁\n\n\nAlcoholYear\n4078\n0.59\n75.10\n103.03\n0.00\n3.00\n24.00\n104.00\n3.64e+02\n▇▁▁▁▁\n\n\nSmokeAge\n6920\n0.31\n17.83\n5.33\n6.00\n15.00\n17.00\n19.00\n7.20e+01\n▇▂▁▁▁\n\n\nAgeFirstMarij\n7109\n0.29\n17.02\n3.90\n1.00\n15.00\n16.00\n19.00\n4.80e+01\n▁▇▂▁▁\n\n\nAgeRegMarij\n8634\n0.14\n17.69\n4.81\n5.00\n15.00\n17.00\n19.00\n5.20e+01\n▂▇▁▁▁\n\n\nSexAge\n4460\n0.55\n17.43\n3.72\n9.00\n15.00\n17.00\n19.00\n5.00e+01\n▇▅▁▁▁\n\n\nSexNumPartnLife\n4275\n0.57\n15.09\n57.85\n0.00\n2.00\n5.00\n12.00\n2.00e+03\n▇▁▁▁▁\n\n\nSexNumPartYear\n5072\n0.49\n1.34\n2.78\n0.00\n1.00\n1.00\n1.00\n6.90e+01\n▇▁▁▁▁\n\n\n\n\n\nAgain, lots of data from skim, about the Quant and Qual variables. Spend a little time looking through this output.\n\nWhich variables could have been data that was given/stated by each respondent?\nAnd which ones could have been measured dependent data variables? Why do you think so?\nWhy is there so much missing data? Which variable are the most affected by this?\n\n\n Counts, and Charts with Counts\n\n\n\n\n\n\nQuestion\n\n\n\nQ.1 What are the Education levels and the counts of people with those levels?\n\n```{r}\nNHANES %&gt;% \n  group_by(Education) %&gt;% \n  summarise(total = n())\n\n# This also works\n# tally(~Education, data = NHANES) %&gt;% as_tibble()\n```\n\n\n\n  \n\n\n\nInsight: The count goes up as we go from lower Education levels to higher. Need to keep that in mind. How do we understand the large number of NA entries? What could have led to these entries?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.2 How do counts of Education vs Work-status look like?\n\n```{r}\n#| message: false\n#| layout: [[30], [70]]\nNHANES %&gt;% \n  mutate(Education = as.factor(Education)) %&gt;% \n  group_by(Work,Education) %&gt;% \n  summarise(count = n())\n\nNHANES %&gt;% \n  group_by(Work, Education) %&gt;% \n  summarise(count = n()) %&gt;% \n  gf_col(count ~ Education, fill = ~ Work, position = \"dodge\") %&gt;% \n  gf_theme(theme_classic())\n```\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nInsight: Clear increase in the number of Working people as Education goes from 8th Grade to College. No surprise. Are the NotWorking counts a surprise?\n\n\n\n Stat Summaries, Histograms, and Densities\n\n\n\n\n\n\nQuestion\n\n\n\nQ.3. What is the distribution of Physical Activity Days, across Gender? Across Education?\n\n```{r}\n#| layout-ncol: 2\n#| column: body-outset-right\n#| warning: false\n\nNHANES %&gt;% \n  gf_histogram(~ PhysActiveDays | Gender, fill = ~Gender) %&gt;% \n  gf_theme(theme_classic())\n\nNHANES %&gt;% \n  gf_histogram(~ PhysActiveDays | Education, fill = ~ Education) %&gt;% \n  gf_theme(theme_classic())\n```\n\n\n\n\n\n\n\n\n\n\n\nInsight: Can we conclude anything here? The populations in each category are different, as indicated by the different heights of the bars, so what do we need to do? Take percentages or ratios of course, per-capita! How would one do that?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.3a. What is the distribution of Physical Activity Days, across Education and Sex, per capita?\n\n```{r}\n#| layout-ncol: 2\n#| message: false\nNHANES %&gt;% \n  group_by(Gender) %&gt;% \n  summarize(mean_active = mean(PhysActiveDays,na.rm = TRUE))\n\nNHANES %&gt;% \n  group_by(Education, Work) %&gt;% \n  summarize(mean_active = mean(PhysActiveDays,na.rm = TRUE))\n```\n\n\n\n\n\n  \n\n\n  \n\n\n\n\n\nInsight: Hmm..no great differences in per-capita physical activity. Females are marginally more active than males. No need to even plot this.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.4. How are people’s Ages distributed across levels of Education?\n\n```{r}\n#| warning: false\n#| layout-nrow: 1\n# Recall there are missing data\ngf_boxplot(Age ~ Education,\n           fill = ~ Education, # Always a good idea to fill boxes\n           data = NHANES) %&gt;%\n  gf_theme(theme_classic()) %&gt;% \n  \n  # And to turn this into an interactive plot\n  plotly::ggplotly()\n```\n\n\n\n\n\n\n\n\n\nInsight: Older age groups are somewhat more heavily represented in groups with lower educational status. College Graduates also have slightly narrow age….That is a nice Question for some Inferential Modelling. And how to interpret the NA group?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.5. How is Education distributed over Race?\n\n```{r}\n#| warning: false\n#| message: false\n#| layout: [[25], [75]]\n\nNHANES_by_Race1 &lt;- NHANES %&gt;% \n  group_by(Race1) %&gt;% \n  summarize(population = n())\nNHANES_by_Race1\n\nNHANES %&gt;% group_by(Education, Race1) %&gt;% \n  summarize( n = n()) %&gt;% \n  left_join(NHANES_by_Race1, by = c(\"Race1\" = \"Race1\")) %&gt;% \n  mutate(percapita_educated = (n/population)*100) %&gt;% \n  ungroup() %&gt;%  \n  group_by(Race1) %&gt;%\n  gf_col(percapita_educated ~ Education | Race1, fill = ~ Race1) %&gt;%\n  gf_refine(coord_flip()) %&gt;%  \n  gf_theme(theme_classic()) %&gt;% \n\n  # And to turn this into an interactive plot\n  plotly::ggplotly()\n```\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\nInsight: Blacks, Hispanics, and Mexicans tend to have fewer people with college degrees, as a percentage of their population. Asians and other immigrants have a significant tendency towards higher education!\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.6. What is the distribution of people’s BMI, split by Gender? By Race1?\n\n```{r}\n#| warning: false\n#| message: false\n#| layout-ncol: 2\n# One can also plot both histograms and densities in an overlay fashion,\n\nNHANES %&gt;% \n  group_by(Gender) %&gt;% \n  gf_density(~ BMI | Gender,fill = ~ Gender , alpha = 0.4) %&gt;% \n  gf_fitdistr(dist = \"dnorm\",color =  ~ Gender) %&gt;% \n  gf_theme(theme_classic()) %&gt;% \n\n  # And to turn this into an interactive plot\n  plotly::ggplotly()\n\nNHANES %&gt;% group_by(Race1) %&gt;% \n  gf_density(~ BMI | Race1, fill = ~ Race1) %&gt;% \n  gf_fitdistr(dist = \"dnorm\") %&gt;% \n  gf_theme(theme_classic()) %&gt;% \n\n  # And to turn this into an interactive plot\n  plotly::ggplotly()\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsight: Blacks tend to have larger portions of their populations with larger BMI. So these races perhaps tend to obesity. By and large BMI distributions are normal.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.7. What is the distribution of people’s Testosterone level vs BMI? Split By Race1?\n\n```{r}\n#| warning: false\nNHANES %&gt;%  \n  gf_density_2d(Testosterone ~ BMI | Race1) %&gt;%\n  gf_theme(theme_classic()) %&gt;% \nplotly::ggplotly()\n```\n\n\n\n\n\nInsight: Low testosterone levels exist across all BMI values, but healthy levels of T exists only over a smaller range of BMI."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html#iconify-pajamas-issue-type-test-case-case-study3-a-complete-example",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html#iconify-pajamas-issue-type-test-case-case-study3-a-complete-example",
    "title": "EDA: Exploring Static Graphs for Distributions in R",
    "section": "\n Case Study#3: A complete example",
    "text": "Case Study#3: A complete example\nHere is a dataset from Jeremy Singer-Vine’s blog, Data Is Plural. This is a list of all books banned in schools across the US.\n Download the data \n\n```{r}\n#| label: Banned Books\nbanned &lt;- readxl::read_xlsx(path = \"data/banned.xlsx\",\n                            sheet = \"Sorted by Author & Title\")\nbanned\nnames(banned)\n```\n\n\n\n  \n\n\n\n [1] \"Author\"                    \"Title\"                    \n [3] \"Type of Ban\"               \"Secondary Author(s)\"      \n [5] \"Illustrator(s)\"            \"Translator(s)\"            \n [7] \"State\"                     \"District\"                 \n [9] \"Date of Challenge/Removal\" \"Origin of Challenge\"      \n\n\nClearly the variables are all Qualitative, except perhaps for Date of Challenge/Removal, (which in this case has been badly mangled by Excel) So we need to make counts based on the levels* of the Qual variables and plot Bar/Column charts.\nLet us quickly make some Stat Summaries ( using inspect)\n\n```{r}\nmosaic::inspect(banned)\n```\n\n\ncategorical variables:  \n                        name     class levels    n missing\n1                     Author character    797 1586       0\n2                      Title character   1145 1586       0\n3                Type of Ban character      4 1586       0\n4        Secondary Author(s) character     61   98    1488\n5             Illustrator(s) character    192  364    1222\n6              Translator(s) character      9   10    1576\n7                      State character     26 1586       0\n8                   District character     86 1586       0\n9  Date of Challenge/Removal character     15 1586       0\n10       Origin of Challenge character      2 1586       0\n                                    distribution\n1  Kobabe, Maia (1.9%) ...                      \n2  Gender Queer: A Memoir (1.9%) ...            \n3  Banned Pending Investigation (46.1%) ...     \n4  Cast, Kristin (12.2%) ...                    \n5  Aly, Hatem (4.7%) ...                        \n6  Mlawer, Teresa (20%) ...                     \n7  Texas (45%), Pennsylvania (28.8%) ...        \n8  Central York (27.8%) ...                     \n9  44440 (28.8%), 44531 (28.3%) ...             \n10 Administrator (95.6%) ...                    \n\n\nInsight: Clearly the variables are all Qualitative, except perhaps for Date of Challenge/Removal, (which in this case has been badly mangled by Excel). So we need to make counts based on the* levels* of the Qual variables and plot Bar/Column charts. We will not find much use for histograms or densities.\nLet us try to answer this question:\n\n\n\n\n\n\nQuestion\n\n\n\nQ.1.What is the count of banned books by type? By US state?\n\n```{r}\n#| message: false\n#| layout: [[60],[40]]\nbanned %&gt;% count(`Type of Ban`, sort = TRUE)\nbanned %&gt;% count(State, sort = TRUE)\n```\n\n\n\n\n\n  \n\n\n  \n\n\n\n\n\nInsight: There are four types of book bans, and Banned Pending Investigation is the most frequent, but other types of bans are frequent too. Texas is the leader in book bans!\nFrom: https://en.wikipedia.org/wiki/Bible_Belt\n\nThe Bible Belt is a region of the Southern United States in which socially conservative Protestant Christianity plays a strong role in society. Church attendance across the denominations is generally higher than the nation’s average. The region contrasts with the religiously diverse Midwest and Great Lakes, and the Mormon corridor in Utah and southern Idaho.\n\n\n\nBible Belt\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.2. What is the count of banned books by type and by US state?*\n\n```{r}\n#| message: false\n\nbanned %&gt;% group_by(State, `Type of Ban`) %&gt;% \n  count(State, sort = TRUE) %&gt;% \n  slice_max(n = 10, order_by = n, with_ties = TRUE) %&gt;% \n  gf_col(reorder(State, n) ~ n, fill = ~`Type of Ban`,\n         xlab = \"No of Banned Books\", ylab = \"State\") %&gt;% \n  gf_refine(scale_fill_brewer(palette = \"Set2\")) %&gt;% \n  gf_theme(theme_classic())\n```\n\n\n\n\nInsight: Clearly Texas is the leading state in book bans in schools. California clearly doesn’t bother itself with these things!!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html#iconify-fluent-mdl2-decision-solid-conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html#iconify-fluent-mdl2-decision-solid-conclusion",
    "title": "EDA: Exploring Static Graphs for Distributions in R",
    "section": "\n Conclusion",
    "text": "Conclusion\nAnd that is a wrap!! Try to work with this procedure:\n\nInspect the data using skim or inspect\n\nIdentify Qualitative and Quantitative variables\nNotice variables that have missing data and decide if that matters\nDevelop Counts of Observations for combinations of Qualitative variables (factors)\nDevelop Histograms and Densities, and slice them by Qualitative variables to develop faceted plots as needed\nContinue with other Descriptive Graphs as needed\nAt each step record the insight and additional questions!!\n\nOnwards with Correlations and other visuals…\nAnd then on the inference and modelling!!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html#iconify-ooui-references-rtl-references",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html#iconify-ooui-references-rtl-references",
    "title": "EDA: Exploring Static Graphs for Distributions in R",
    "section": "\n References",
    "text": "References\n\nA detailed analysis of the NHANES dataset, https://awagaman.people.amherst.edu/stat230/Stat230CodeCompilationExampleCodeUsingNHANES.pdf"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html#footnotes",
    "title": "EDA: Exploring Static Graphs for Distributions in R",
    "section": "Footnotes",
    "text": "Footnotes\n\nFundamentals of Data Visualization (clauswilke.com)↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html",
    "title": "EDA: Interactive Correlation Graphs in R",
    "section": "",
    "text": "We will create Tables for Correlations, and graphs for Correlations in R. As always, we will consistently use the Project Mosaic ecosystem of packages in R (mosaic, mosaicData and ggformula).\n\n\nlibrary(tidyverse)\nlibrary(mosaic) # package for stats, simulations, and basic plots\nlibrary(mosaicData) # package containing datasets\nlibrary(ggformula) # package for professional looking plots, that use the formula interface from mosaic\nlibrary(NHANES) # survey data collected by the US National Center for Health Statistics (NCHS)\nlibrary(corrplot) # For Correlogram plots\nlibrary(plotly)\nlibrary(echarts4r)\n\n\n\n\n\n\n\nInteractive Graphs with echarts4r\n\n\n\nWe will also start using echarts4r side by side for interactive graphs.\n\nEvery function in the package starts with e_.\nYou start coding a visualization by creating an echarts object with the e_charts() function. That takes your data frame and x-axis column as arguments.\nNext, you add a function for the type of chart (e_line(), e_bar(), etc.) with the y-axis series column name as an argument.\nThe rest is mostly customization! echarts4r takes some effort in getting used to, but it totally worth it!\n\n\n\n\nLet us inspect what datasets are available in the package mosaicData. Run this command in your Console: data(package = “mosaicData”)\nThe popup tab shows a lot of datasets we could use. Let us continue to use the famous Galton dataset and inspect it: (We will save the inspect output as an R object for use later)\n\ndata(\"Galton\")\ngalton_describe &lt;- inspect(Galton)\ngalton_describe$categorical\n\n\n\n  \n\n\ngalton_describe$quantitative\n\n\n\n  \n\n\n\nThe inspect command already gives us a series of statistical measures of different variables of interest. As discussed previously, we can retain the output of inspect and use it in our reports: (there are ways of dressing up these tables too)\nThe dataset is described as:Try help(\"Galton\") in your Console.\n\nA data frame with 898 observations on the following variables.\n- family a factor with levels for each family\n- father the father’s height (in inches)\n- mother the mother’s height (in inches)\n- sex the child’s sex: F or M\n- height the child’s height as an adult (in inches)\n- nkids the number of adult children in the family, or, at least, the number whose heights Galton recorded.\n\nThere is a lot of Description generated by the mosaic::inspect() command ! What can we say about the dataset and its variables? How big is the dataset? How many variables? What types are they, Quant or Qual? If they are Qual, what are the levels? Are they ordered levels? Discuss!\n\nWhat Questions might we have, that we could answer with a Statistical Measure, or Correlation chart?\n\n\n\n\n\n\nPair-wise Correlation Plot\n\n\n\nQ.1 Which are the variables that have significant pair-wise correlations? What polarity are these correlations?\n\n# Pulling out the list of Quant variables from NHANES\ngalton_quant &lt;- galton_describe$quantitative\ngalton_quant$name\n\n[1] \"father\" \"mother\" \"height\" \"nkids\" \n\nGGally::ggpairs(\n  Galton,\n  columns = c(\"father\", \"mother\", \"height\", \"nkids\"),\n  diag = list(\"densityDiag\"),\n  title = \"Galton Data Correlations Plot\"\n) %&gt;% \n  plotly::ggplotly()\n\n\n\n\n\nInsight: There are significant, but low value correlations in the Galton dataset. height is best correlated with father (\\(0.275\\)). The Scatter Plots shown in the plot also visually demonstrate the (lack of) large value correlations.\nWe cannot have too many variables in this kind of plot. We will shortly see how to plot correlations when there are a large number of variables.\n\n\n\n\n\n\n\n\nHeatmap\n\n\n\necharts4r does not have a comprehensive combination plot like what GGally offers. However, we can plot a Correlation Heatmap using echarts4r:\n\nGalton %&gt;% select(where(is.numeric)) %&gt;% \n  mosaic::cor() %&gt;% \n  e_charts(height = 300) %&gt;% \n  e_correlations(order = \"hclust\", visual_map = TRUE) %&gt;% \n  e_title(\"Galton Correlations Heatmap\")\n\n\n\n\n\nInsight: Moving the cursor over the heatmap gives us the an indication of the correlation scores between variables. The visual map slider moves automatically to indicate the scores. We can also move the slider ourselves to “filter” the heatmap!\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.2: Can we plot a Correlogram for this dataset?\n\n#library(corrplot)\n\ngalton_num_var &lt;- Galton %&gt;% select(father, mother, height, nkids)\ngalton_cor &lt;- cor(galton_num_var)\ngalton_cor %&gt;%\n  corrplot(method = \"ellipse\",\n           type = \"lower\",\n           main = \"Correlogram for Galton dataset\")\n\n\n\n\nInsight: Again, height is positively correlated to father and mother as depicted by the rightward-sloping blue ellipses. And height is negatively correlated (very slightly) with nkids, with leftward-sloping reddish ellipses. (See the color palette + legend below the figure).\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.3: What do the correlation tests tell us?\n\nmosaic::cor_test(height ~ father, data = Galton)\nmosaic::cor_test(height ~ mother, data = Galton)\n\n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and father\nt = 8.5737, df = 896, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2137851 0.3347455\nsample estimates:\n      cor \n0.2753548 \n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and mother\nt = 6.1628, df = 896, p-value = 1.079e-09\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.1380554 0.2635982\nsample estimates:\n      cor \n0.2016549 \n\n\n\n\nInsight: The tests give us the same values seen before, along with the confidence intervals for the correlation estimate. These represent the uncertainty that exists in our estimates.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.4: What does this correlation look when split by sex of Child?\nWe will use the mosaic function cor_test to get these results:\n\n# For the sons\nmosaic::cor_test(height ~ father, \n                 data = Galton %&gt;% filter(sex == \"M\"))\ncor_test(height ~ mother, data = Galton %&gt;% \n           filter(sex == \"M\"))\n\n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and father\nt = 9.1498, df = 463, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3114667 0.4656805\nsample estimates:\n      cor \n0.3913174 \n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and mother\nt = 7.628, df = 463, p-value = 1.367e-13\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2508178 0.4125305\nsample estimates:\n      cor \n0.3341309 \n\n\n\n\n\n# For the daughters\ncor_test(height ~ father, \n         data = Galton %&gt;% filter(sex == \"F\"))\ncor_test(height ~ mother, \n         data = Galton %&gt;% filter(sex == \"F\"))\n\n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and father\nt = 10.719, df = 431, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3809944 0.5300812\nsample estimates:\n      cor \n0.4587605 \n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and mother\nt = 6.8588, df = 431, p-value = 2.421e-11\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2261463 0.3962226\nsample estimates:\n      cor \n0.3136984 \n\n\n\n\nInsight: Son’s heights are correlated more with father than with mother. This trend is even more so for daughters! Hmmm…mother’s influence on children is clearly not with height.\n\n\n\n\n\n\n\n\n\nCorrelation Tests and Uncertainty\n\n\n\nNote how the cor.test reports a correlation score and the p-value for the same. There is also a confidence interval reported for the correlation score, an interval within which we are 95% sure that the true correlation value is to be found. Note that GGally too reports the significance of the correlation scores using *** or **. This indicates the p-value in the scores obtained by GGally; Presumably, there is an internal cor.test that is run for each pair of variables and the p-value and confidence levels are also computed internally.\nWe can also visualise this uncertainty and the confidence levels in a plot too, using gf_errorbar and a handy set of functions within purrr which is part of the tidyverse: Assuming heights is the target variable we want to correlate every other (quantitative) variable against, we can proceed very quickly as follows:\n\nall_corrs &lt;- Galton %&gt;% \n  select(where(is.numeric)) %&gt;% \n  \n  # leave off height to get all the remaining ones\n  select(- height) %&gt;%  \n  \n  # perform a cor.test for all variables against height\n  purrr::map(.x = .,\n             .f = \\(x) cor.test(x, Galton$height)) %&gt;%\n  \n  # tidy up the cor.test outputs into a tidy data frame\n  map_dfr(broom::tidy, .id = \"predictor\") \n\nall_corrs\n\n\n\n  \n\n\n\n\nall_corrs %&gt;% \n  e_charts(predictor) %&gt;% \n  e_bar(estimate, colorBy = \"data\", legend = FALSE) %&gt;% \n  e_error_bar(lower = conf.low, upper = conf.high) %&gt;% \n  \n  e_y_axis(name = \"Correlation with `height`\", \n           nameLocation = \"middle\", nameGap = 35) %&gt;% \n  e_x_axis(name = \"Parameter\", nameLocation = \"center\",\n           nameGap = 35, type = \"category\") %&gt;% \n  e_tooltip()\n\n\n\n\nall_corrs %&gt;% \n  mutate(sd = (conf.high-conf.low)/2) %&gt;% \n  plot_ly() %&gt;% add_bars(y = ~estimate, x = ~predictor, \n                         error_y = ~ list(array = sd, color = \"black\"))\n\n\n\n\n\nInsight: We can clearly see the size of the correlations and the confidence intervals marked in this plot. father has somewhat greater correlation with children’s height, as compared to mother. nkids seems to matter very slightly, in a negative way.\nThis kind of plot will be very useful when we pursue linear regression models.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.5. How can we show this correlation in a set of Scatter Plots + Regression Lines? Can we recreate Galton’s famous diagram?\n\n# For the father\nGalton %&gt;% \n  group_by(sex) %&gt;% \n  e_charts(father, height = 300) %&gt;% \n  e_scatter(height,symbol_size = 8) %&gt;% \n  e_lm(height ~ father,legend = FALSE) %&gt;% \n  e_x_axis(name = \"father\", nameLocation = \"middle\", nameGap = 35,\n           min = 60, max = 80) %&gt;% \n  e_y_axis(name = \"height\", nameLocation = \"middle\", nameGap = 35,\n           min = 50, max = 80) %&gt;% \n  e_tooltip()\n# for the mother\nGalton %&gt;% \n  group_by(sex) %&gt;% \n  e_charts(mother, height = 300) %&gt;% \n  e_scatter(height,symbol_size = 8) %&gt;% \n  e_lm(height ~ mother,legend = FALSE) %&gt;% \n  e_x_axis(name = \"mother\", nameLocation = \"middle\", nameGap = 35,\n           min = 55, max = 75) %&gt;% \n  e_y_axis(name = \"height\", nameLocation = \"middle\", nameGap = 35,\n           min = 50, max = 80) %&gt;% \n  e_tooltip()\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsight: Visibly the scatter plots are slightly tilted upward to the right, showing a positive correlation for both sons’ and daughters’ heights with that of the father and mother.\n\n\n\n\n\n\n\n\nGalton’s Plot\n\n\n\nAn approximation to Galton’s famous plot (see Wikipedia):\n\ngf_point(height ~ (father + mother)/2, data = Galton) %&gt;% \n  gf_smooth(method = \"lm\") %&gt;% \n  gf_density_2d(n = 8) %&gt;% \n  gf_abline(slope = 1) %&gt;% \n  gf_theme(theme_minimal())\n\n\n\n\nInsight: How would you interpret this plot1? As yet we are not able to reproduce this with charts4r.\n\n\n\nWe will “live code” this in class!\n\nWe have a decent Correlations related workflow in R:\n- load the dataset\n- inspect the dataset, identify Quant and Qual variables\n- Develop Pair-Wise plots + Correlations using GGally::ggpairs()\n- Develop Correlogram corrplot::corrplot\n- Check everything with a cor_test\n- Use purrr + cor.test to plot correlations and confidence intervals for multiple Quant variables\n- Plot scatter plots using gf_point.\n- Add extra lines using gf_abline() to compare hypotheses that you may have."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#iconify-noto-v1-package-setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#iconify-noto-v1-package-setting-up-r-packages",
    "title": "EDA: Interactive Correlation Graphs in R",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic) # package for stats, simulations, and basic plots\nlibrary(mosaicData) # package containing datasets\nlibrary(ggformula) # package for professional looking plots, that use the formula interface from mosaic\nlibrary(NHANES) # survey data collected by the US National Center for Health Statistics (NCHS)\nlibrary(corrplot) # For Correlogram plots\nlibrary(plotly)\nlibrary(echarts4r)\n\n\n\n\n\n\n\nInteractive Graphs with echarts4r\n\n\n\nWe will also start using echarts4r side by side for interactive graphs.\n\nEvery function in the package starts with e_.\nYou start coding a visualization by creating an echarts object with the e_charts() function. That takes your data frame and x-axis column as arguments.\nNext, you add a function for the type of chart (e_line(), e_bar(), etc.) with the y-axis series column name as an argument.\nThe rest is mostly customization! echarts4r takes some effort in getting used to, but it totally worth it!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#iconify-pajamas-issue-type-test-case-case-study-1-dataset-from-mosaicdata",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#iconify-pajamas-issue-type-test-case-case-study-1-dataset-from-mosaicdata",
    "title": "EDA: Interactive Correlation Graphs in R",
    "section": "",
    "text": "Let us inspect what datasets are available in the package mosaicData. Run this command in your Console: data(package = “mosaicData”)\nThe popup tab shows a lot of datasets we could use. Let us continue to use the famous Galton dataset and inspect it: (We will save the inspect output as an R object for use later)\n\ndata(\"Galton\")\ngalton_describe &lt;- inspect(Galton)\ngalton_describe$categorical\n\n\n\n  \n\n\ngalton_describe$quantitative\n\n\n\n  \n\n\n\nThe inspect command already gives us a series of statistical measures of different variables of interest. As discussed previously, we can retain the output of inspect and use it in our reports: (there are ways of dressing up these tables too)\nThe dataset is described as:Try help(\"Galton\") in your Console.\n\nA data frame with 898 observations on the following variables.\n- family a factor with levels for each family\n- father the father’s height (in inches)\n- mother the mother’s height (in inches)\n- sex the child’s sex: F or M\n- height the child’s height as an adult (in inches)\n- nkids the number of adult children in the family, or, at least, the number whose heights Galton recorded.\n\nThere is a lot of Description generated by the mosaic::inspect() command ! What can we say about the dataset and its variables? How big is the dataset? How many variables? What types are they, Quant or Qual? If they are Qual, what are the levels? Are they ordered levels? Discuss!\n\nWhat Questions might we have, that we could answer with a Statistical Measure, or Correlation chart?\n\n\n\n\n\n\nPair-wise Correlation Plot\n\n\n\nQ.1 Which are the variables that have significant pair-wise correlations? What polarity are these correlations?\n\n# Pulling out the list of Quant variables from NHANES\ngalton_quant &lt;- galton_describe$quantitative\ngalton_quant$name\n\n[1] \"father\" \"mother\" \"height\" \"nkids\" \n\nGGally::ggpairs(\n  Galton,\n  columns = c(\"father\", \"mother\", \"height\", \"nkids\"),\n  diag = list(\"densityDiag\"),\n  title = \"Galton Data Correlations Plot\"\n) %&gt;% \n  plotly::ggplotly()\n\n\n\n\n\nInsight: There are significant, but low value correlations in the Galton dataset. height is best correlated with father (\\(0.275\\)). The Scatter Plots shown in the plot also visually demonstrate the (lack of) large value correlations.\nWe cannot have too many variables in this kind of plot. We will shortly see how to plot correlations when there are a large number of variables.\n\n\n\n\n\n\n\n\nHeatmap\n\n\n\necharts4r does not have a comprehensive combination plot like what GGally offers. However, we can plot a Correlation Heatmap using echarts4r:\n\nGalton %&gt;% select(where(is.numeric)) %&gt;% \n  mosaic::cor() %&gt;% \n  e_charts(height = 300) %&gt;% \n  e_correlations(order = \"hclust\", visual_map = TRUE) %&gt;% \n  e_title(\"Galton Correlations Heatmap\")\n\n\n\n\n\nInsight: Moving the cursor over the heatmap gives us the an indication of the correlation scores between variables. The visual map slider moves automatically to indicate the scores. We can also move the slider ourselves to “filter” the heatmap!\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.2: Can we plot a Correlogram for this dataset?\n\n#library(corrplot)\n\ngalton_num_var &lt;- Galton %&gt;% select(father, mother, height, nkids)\ngalton_cor &lt;- cor(galton_num_var)\ngalton_cor %&gt;%\n  corrplot(method = \"ellipse\",\n           type = \"lower\",\n           main = \"Correlogram for Galton dataset\")\n\n\n\n\nInsight: Again, height is positively correlated to father and mother as depicted by the rightward-sloping blue ellipses. And height is negatively correlated (very slightly) with nkids, with leftward-sloping reddish ellipses. (See the color palette + legend below the figure).\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.3: What do the correlation tests tell us?\n\nmosaic::cor_test(height ~ father, data = Galton)\nmosaic::cor_test(height ~ mother, data = Galton)\n\n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and father\nt = 8.5737, df = 896, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2137851 0.3347455\nsample estimates:\n      cor \n0.2753548 \n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and mother\nt = 6.1628, df = 896, p-value = 1.079e-09\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.1380554 0.2635982\nsample estimates:\n      cor \n0.2016549 \n\n\n\n\nInsight: The tests give us the same values seen before, along with the confidence intervals for the correlation estimate. These represent the uncertainty that exists in our estimates.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.4: What does this correlation look when split by sex of Child?\nWe will use the mosaic function cor_test to get these results:\n\n# For the sons\nmosaic::cor_test(height ~ father, \n                 data = Galton %&gt;% filter(sex == \"M\"))\ncor_test(height ~ mother, data = Galton %&gt;% \n           filter(sex == \"M\"))\n\n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and father\nt = 9.1498, df = 463, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3114667 0.4656805\nsample estimates:\n      cor \n0.3913174 \n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and mother\nt = 7.628, df = 463, p-value = 1.367e-13\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2508178 0.4125305\nsample estimates:\n      cor \n0.3341309 \n\n\n\n\n\n# For the daughters\ncor_test(height ~ father, \n         data = Galton %&gt;% filter(sex == \"F\"))\ncor_test(height ~ mother, \n         data = Galton %&gt;% filter(sex == \"F\"))\n\n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and father\nt = 10.719, df = 431, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3809944 0.5300812\nsample estimates:\n      cor \n0.4587605 \n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and mother\nt = 6.8588, df = 431, p-value = 2.421e-11\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2261463 0.3962226\nsample estimates:\n      cor \n0.3136984 \n\n\n\n\nInsight: Son’s heights are correlated more with father than with mother. This trend is even more so for daughters! Hmmm…mother’s influence on children is clearly not with height."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#correlation-tests-and-uncertainty",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#correlation-tests-and-uncertainty",
    "title": "EDA: Interactive Correlation Graphs in R",
    "section": "",
    "text": "Correlation Tests and Uncertainty\n\n\n\nNote how the cor.test reports a correlation score and the p-value for the same. There is also a confidence interval reported for the correlation score, an interval within which we are 95% sure that the true correlation value is to be found. Note that GGally too reports the significance of the correlation scores using *** or **. This indicates the p-value in the scores obtained by GGally; Presumably, there is an internal cor.test that is run for each pair of variables and the p-value and confidence levels are also computed internally.\nWe can also visualise this uncertainty and the confidence levels in a plot too, using gf_errorbar and a handy set of functions within purrr which is part of the tidyverse: Assuming heights is the target variable we want to correlate every other (quantitative) variable against, we can proceed very quickly as follows:\n\nall_corrs &lt;- Galton %&gt;% \n  select(where(is.numeric)) %&gt;% \n  \n  # leave off height to get all the remaining ones\n  select(- height) %&gt;%  \n  \n  # perform a cor.test for all variables against height\n  purrr::map(.x = .,\n             .f = \\(x) cor.test(x, Galton$height)) %&gt;%\n  \n  # tidy up the cor.test outputs into a tidy data frame\n  map_dfr(broom::tidy, .id = \"predictor\") \n\nall_corrs\n\n\n\n  \n\n\n\n\nall_corrs %&gt;% \n  e_charts(predictor) %&gt;% \n  e_bar(estimate, colorBy = \"data\", legend = FALSE) %&gt;% \n  e_error_bar(lower = conf.low, upper = conf.high) %&gt;% \n  \n  e_y_axis(name = \"Correlation with `height`\", \n           nameLocation = \"middle\", nameGap = 35) %&gt;% \n  e_x_axis(name = \"Parameter\", nameLocation = \"center\",\n           nameGap = 35, type = \"category\") %&gt;% \n  e_tooltip()\n\n\n\n\nall_corrs %&gt;% \n  mutate(sd = (conf.high-conf.low)/2) %&gt;% \n  plot_ly() %&gt;% add_bars(y = ~estimate, x = ~predictor, \n                         error_y = ~ list(array = sd, color = \"black\"))\n\n\n\n\n\nInsight: We can clearly see the size of the correlations and the confidence intervals marked in this plot. father has somewhat greater correlation with children’s height, as compared to mother. nkids seems to matter very slightly, in a negative way.\nThis kind of plot will be very useful when we pursue linear regression models.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.5. How can we show this correlation in a set of Scatter Plots + Regression Lines? Can we recreate Galton’s famous diagram?\n\n# For the father\nGalton %&gt;% \n  group_by(sex) %&gt;% \n  e_charts(father, height = 300) %&gt;% \n  e_scatter(height,symbol_size = 8) %&gt;% \n  e_lm(height ~ father,legend = FALSE) %&gt;% \n  e_x_axis(name = \"father\", nameLocation = \"middle\", nameGap = 35,\n           min = 60, max = 80) %&gt;% \n  e_y_axis(name = \"height\", nameLocation = \"middle\", nameGap = 35,\n           min = 50, max = 80) %&gt;% \n  e_tooltip()\n# for the mother\nGalton %&gt;% \n  group_by(sex) %&gt;% \n  e_charts(mother, height = 300) %&gt;% \n  e_scatter(height,symbol_size = 8) %&gt;% \n  e_lm(height ~ mother,legend = FALSE) %&gt;% \n  e_x_axis(name = \"mother\", nameLocation = \"middle\", nameGap = 35,\n           min = 55, max = 75) %&gt;% \n  e_y_axis(name = \"height\", nameLocation = \"middle\", nameGap = 35,\n           min = 50, max = 80) %&gt;% \n  e_tooltip()\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsight: Visibly the scatter plots are slightly tilted upward to the right, showing a positive correlation for both sons’ and daughters’ heights with that of the father and mother.\n\n\n\n\n\n\n\n\nGalton’s Plot\n\n\n\nAn approximation to Galton’s famous plot (see Wikipedia):\n\ngf_point(height ~ (father + mother)/2, data = Galton) %&gt;% \n  gf_smooth(method = \"lm\") %&gt;% \n  gf_density_2d(n = 8) %&gt;% \n  gf_abline(slope = 1) %&gt;% \n  gf_theme(theme_minimal())\n\n\n\n\nInsight: How would you interpret this plot1? As yet we are not able to reproduce this with charts4r."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#iconify-pajamas-issue-type-test-case-case-study-2-dataset-from-nhanes",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#iconify-pajamas-issue-type-test-case-case-study-2-dataset-from-nhanes",
    "title": "EDA: Interactive Correlation Graphs in R",
    "section": "",
    "text": "We will “live code” this in class!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#iconify-fluent-mdl2-decision-solid-conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#iconify-fluent-mdl2-decision-solid-conclusion",
    "title": "EDA: Interactive Correlation Graphs in R",
    "section": "",
    "text": "We have a decent Correlations related workflow in R:\n- load the dataset\n- inspect the dataset, identify Quant and Qual variables\n- Develop Pair-Wise plots + Correlations using GGally::ggpairs()\n- Develop Correlogram corrplot::corrplot\n- Check everything with a cor_test\n- Use purrr + cor.test to plot correlations and confidence intervals for multiple Quant variables\n- Plot scatter plots using gf_point.\n- Add extra lines using gf_abline() to compare hypotheses that you may have."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#footnotes",
    "title": "EDA: Interactive Correlation Graphs in R",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://www.researchgate.net/figure/Galtons-smoothed-correlation-diagram-for-the-data-on-heights-of-parents-and-children_fig15_226400313↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html",
    "title": "📎 Correlations",
    "section": "",
    "text": "Tutorial   \n  R (Interactive Graphs"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#fa-folder-open-slides-and-tutorials",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#fa-folder-open-slides-and-tutorials",
    "title": "📎 Correlations",
    "section": "",
    "text": "Tutorial   \n  R (Interactive Graphs"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#iconify-noto-v1-package-setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#iconify-noto-v1-package-setting-up-r-packages",
    "title": "📎 Correlations",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(tidyverse) # Tidy data processing and plotting\nlibrary(ggformula) # Formula based plots\nlibrary(mosaic) # Our go to package\nlibrary(GGally) # Corr plots\nlibrary(corrplot) # More corrplots"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#iconify-tabler-exchange-what-is-correlation",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#iconify-tabler-exchange-what-is-correlation",
    "title": "📎 Correlations",
    "section": "\n What is Correlation?",
    "text": "What is Correlation?\nOne of the basic Questions we would have of our data is: Does some variable depend upon another in some way? Does \\(y\\) vary with \\(x\\)? A Correlation Test is designed to exactly answer this question.\nThe word correlation is used in everyday life to denote some form of association. We might say that we have noticed a correlation between rainy days and reduced sales at supermarkets. However, in statistical terms we use correlation to denote association between two quantitative variables. We also assume that the association is linear, that one variable increases or decreases a fixed amount for a unit increase or decrease in the other. The other technique that is often used in these circumstances is regression, which involves estimating the best straight line to summarise the association.\n\n Correlation coefficient\nThe degree of association is measured by a correlation coefficient, denoted by r. It is sometimes called Pearson’s correlation coefficient after its originator and is a measure of linear association. (If a curved line is needed to express the relationship, other and more complicated measures of the correlation must be used.)\nThe correlation coefficient is measured on a scale that varies from + 1 through 0 to – 1. Complete correlation between two variables is expressed by either + 1 or -1. When one variable increases as the other increases the correlation is positive; when one decreases as the other increases it is negative.\nIn formal terms, the correlation between two variables \\(x\\) and \\(y\\) is defined as\n\\[\n\\rho = E\\left[\\frac{(x - \\mu_{x}) * (y - \\mu_{y})}{(\\sigma_x)*(\\sigma_y)}\\right]\n\\]\nwhere \\(E\\) is the expectation operator ( i.e taking mean ). Think of this as the average of the products of two scaled variables. (We can see \\((x-\\mu_x)/\\sigma_x\\) is a centering and scaling of the variable \\(x\\). It is called the z-score of x.)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#iconify-material-symbols-add-chart-outline-what-graphs-will-we-see-today",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#iconify-material-symbols-add-chart-outline-what-graphs-will-we-see-today",
    "title": "📎 Correlations",
    "section": "\n What graphs will we see today?",
    "text": "What graphs will we see today?\nSome of the very basic and commonly used plots for data are:\n\nScatter Plot\nContour Plot\nCorrelogram\nHeatmap\n\nLet us look at the mpg dataset from R. Here we have hwy and cty mileages for cars of different types, makes, and models."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#iconify-carbon-chart-3d-inspecting-and-charting-data",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#iconify-carbon-chart-3d-inspecting-and-charting-data",
    "title": "📎 Correlations",
    "section": "\n Inspecting and Charting Data",
    "text": "Inspecting and Charting Data\n\n\n\n\n  \n\n\n\n\n\n  \n\n\n\nThe mosaic::inspect command provides the schema of the mpg dataset in table form, one table for each kind of variable (Qual and Quant).\nThere are 6 Qual variables, and Quant variables.\n\n\n\n\n\n\nResearch Questions\n\n\n\nIs there a relationship between the cty and hwy variables in the mpg dataset? Does this relationship vary based on groupings by other Qual variables, such as drv and cyl?\n\n\n\n Scatter Plot\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt does appear from the scatter plots that there is a near-linear relationship between cty and hwy and that it is positive. This relationship also holds when we group the data by drv.\n\n\n\n\n\n\nIndependent and Dependent Variables\n\n\n\nNote that we have rather arbitrarily taken hwy as the independent variable, to be plotted on the x-axis, and cty on the y-axis. It could easily have been the other way around, based on our Research Question.\n\n\nHow do we quantize this relationship, into a correlation score? There are two ways: using the GGally and corplot packages, and doing a formal correlation test with the mosaic package.\nPairwise Correlation Plot with GGally\n\nBy default, GGally::ggpairs() provides:\n\ntwo different comparisons of each pair of columns\ndisplays either the density or count of the respective variable along the diagonal.\nWith different parameter settings, the diagonal can be replaced with the axis values and variable labels.\n\n\n\n\n\n\nPairwise Correlogram with corrplot\n\nIn this chart, the correlation between pairs of variables is shown symbolically as coloured shapes or colours. Circles, Squares, and Ellipse for example. - The size, colour, and “orientation” of the shapes in question symbolically represent the strength and polarity of the correlation scores. E.g. direction of the semi-major axis + the colour of the ellipse indicate whether the correlation score is positive or negative; And the more eccentric the ellipse, the higher is the correlation score in value.\nWhereas GGally computes the correlation scores, corplot “merely” displays them in an evocative way. We need to compute the correlations a priori. Note however:\n\nR package corrplot provides a visual exploratory tool on correlation matrix that supports automatic variable reordering to help detect hidden patterns among variables. corrplot is very easy to use and provides a rich array of plotting options in visualization method, graphic layout, color, legend, text labels, etc. It also provides p-values and confidence intervals to help users determine the statistical significance of the correlations.\n\n\n\n\n\nCorrelation Scores Matrix\n\n\nhwy\ncty\ndispl\ncyl\n\n\n\nhwy\n1.0000000\n0.9559159\n-0.7660200\n-0.7619124\n\n\ncty\n0.9559159\n1.0000000\n-0.7985240\n-0.8057714\n\n\ndispl\n-0.7660200\n-0.7985240\n1.0000000\n0.9302271\n\n\ncyl\n-0.7619124\n-0.8057714\n0.9302271\n1.0000000"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#iconify-ooui-references-ltr-references",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#iconify-ooui-references-ltr-references",
    "title": "📎 Correlations",
    "section": "\n References",
    "text": "References\n\nUsing the GGally package &lt;ggally_*(): List of available high-level plots • GGally (ggobi.github.io)&gt;\nMinimal R using mosaic. https://cran.r-project.org/web/packages/mosaic/vignettes/MinimalRgg.pdf"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries.html",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries.html",
    "title": "Analysis of Time Series in R",
    "section": "",
    "text": "library(tidyverse) # For tidy data processing and plotting\nlibrary(lubridate)  # Deal with dates\n\nlibrary(mosaic) # Out go to package for everything\n\nlibrary(fpp3) # Robert Hyndman's time series analysis package\nlibrary(timetk)  # Convert data frames to time series-specific objects\nlibrary(forecast)  # Make forecasts and decompose time series\n\n# devtools::install_github(\"FinYang/tsdl\")\nlibrary(tsdl) # Time Series Data Library from Rob Hyndman"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries.html#setup-the-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries.html#setup-the-packages",
    "title": "Analysis of Time Series in R",
    "section": "",
    "text": "library(tidyverse) # For tidy data processing and plotting\nlibrary(lubridate)  # Deal with dates\n\nlibrary(mosaic) # Out go to package for everything\n\nlibrary(fpp3) # Robert Hyndman's time series analysis package\nlibrary(timetk)  # Convert data frames to time series-specific objects\nlibrary(forecast)  # Make forecasts and decompose time series\n\n# devtools::install_github(\"FinYang/tsdl\")\nlibrary(tsdl) # Time Series Data Library from Rob Hyndman"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries.html#introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries.html#introduction",
    "title": "Analysis of Time Series in R",
    "section": "Introduction",
    "text": "Introduction\nWe will see how a time series can be broken down to its components so as to systematically understand, analyze, model and forecast it. We have to begin by answering fundamental questions such as:\n\nWhat are the types of time series?\nHow does one process and analyze time series data?\nHow does one plot time series?\nHow to decompose it? How to extract a level, a trend, and seasonal components from a time series?\nWhat is auto correlation etc.\n\nWhat is a stationary time series?"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries.html#case-study--1-walmart-sales-dataset-from-timetk",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries.html#case-study--1-walmart-sales-dataset-from-timetk",
    "title": "Analysis of Time Series in R",
    "section": "Case Study -1: Walmart Sales Dataset from timetk\n",
    "text": "Case Study -1: Walmart Sales Dataset from timetk\n\nLet us inspect what datasets are available in the package timetk. Type data(package = \"timetk\") in your Console to see what datasets are available.\nLet us choose the Walmart Sales dataset. See here for more details: Walmart Recruiting - Store Sales Forecasting |Kaggle\n\ndata(\"walmart_sales_weekly\")\nwalmart_sales_weekly\n\n\n\n  \n\n\ninspect(walmart_sales_weekly)\n\n\ncategorical variables:  \n       name     class levels    n missing\n1        id    factor   3331 1001       0\n2 IsHoliday   logical      2 1001       0\n3      Type character      1 1001       0\n                                   distribution\n1 1_1 (14.3%), 1_3 (14.3%), 1_8 (14.3%) ...    \n2 FALSE (93%), TRUE (7%)                       \n3 A (100%)                                     \n\nDate variables:  \n  name class      first       last min_diff max_diff    n missing\n1 Date  Date 2010-02-05 2012-10-26   0 days   7 days 1001       0\n\nquantitative variables:  \n           name   class         min          Q1      median          Q3\n1         Store numeric      1.0000      1.0000      1.0000      1.0000\n2          Dept numeric      1.0000      3.0000     13.0000     93.0000\n3  Weekly_Sales numeric   6165.7300  28257.3000  39886.0600  77943.5700\n4          Size numeric 151315.0000 151315.0000 151315.0000 151315.0000\n5   Temperature numeric     35.4000     57.7900     69.6400     80.4900\n6    Fuel_Price numeric      2.5140      2.7590      3.2900      3.5940\n7     MarkDown1 numeric    410.3100   4039.3900   6154.1400  10121.9700\n8     MarkDown2 numeric      0.5000     40.4800    144.8700   1569.0000\n9     MarkDown3 numeric      0.2500      6.0000     25.9650    101.6400\n10    MarkDown4 numeric      8.0000    577.1400   1822.5500   3750.5900\n11    MarkDown5 numeric    554.9200   3127.8800   4325.1900   6222.2500\n12          CPI numeric    210.3374    211.5312    215.4599    220.6369\n13 Unemployment numeric      6.5730      7.3480      7.7870      7.8380\n           max         mean           sd    n missing\n1       1.0000 1.000000e+00 0.000000e+00 1001       0\n2      95.0000 3.585714e+01 3.849159e+01 1001       0\n3  148798.0500 5.464634e+04 3.627627e+04 1001       0\n4  151315.0000 1.513150e+05 0.000000e+00 1001       0\n5      91.6500 6.830678e+01 1.420767e+01 1001       0\n6       3.9070 3.219699e+00 4.260286e-01 1001       0\n7   34577.0600 8.090766e+03 6.550983e+03  357     644\n8   46011.3800 2.941315e+03 7.873661e+03  294     707\n9   55805.5100 1.225400e+03 7.811934e+03  350     651\n10  32403.8700 3.746085e+03 5.948867e+03  357     644\n11  20475.3200 5.018655e+03 3.254071e+03  357     644\n12    223.4443 2.159969e+02 4.337818e+00 1001       0\n13      8.1060 7.610420e+00 3.825958e-01 1001       0\n\nglimpse(walmart_sales_weekly)\n\nRows: 1,001\nColumns: 17\n$ id           &lt;fct&gt; 1_1, 1_1, 1_1, 1_1, 1_1, 1_1, 1_1, 1_1, 1_1, 1_1, 1_1, 1_…\n$ Store        &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ Dept         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ Date         &lt;date&gt; 2010-02-05, 2010-02-12, 2010-02-19, 2010-02-26, 2010-03-…\n$ Weekly_Sales &lt;dbl&gt; 24924.50, 46039.49, 41595.55, 19403.54, 21827.90, 21043.3…\n$ IsHoliday    &lt;lgl&gt; FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA…\n$ Type         &lt;chr&gt; \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A…\n$ Size         &lt;dbl&gt; 151315, 151315, 151315, 151315, 151315, 151315, 151315, 1…\n$ Temperature  &lt;dbl&gt; 42.31, 38.51, 39.93, 46.63, 46.50, 57.79, 54.58, 51.45, 6…\n$ Fuel_Price   &lt;dbl&gt; 2.572, 2.548, 2.514, 2.561, 2.625, 2.667, 2.720, 2.732, 2…\n$ MarkDown1    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ MarkDown2    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ MarkDown3    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ MarkDown4    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ MarkDown5    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CPI          &lt;dbl&gt; 211.0964, 211.2422, 211.2891, 211.3196, 211.3501, 211.380…\n$ Unemployment &lt;dbl&gt; 8.106, 8.106, 8.106, 8.106, 8.106, 8.106, 8.106, 8.106, 7…\n\n# Try this in your Console\n# help(\"walmart_sales_weekly\")\n\nThe data is described as:\n\nA tibble: 9,743 x 3\n\n\nid Factor. Unique series identifier (4 total)\n\nStore Numeric. Store ID.\n\nDept Numeric. Department ID.\n\nDate Date. Weekly timestamp.\n\nWeekly_Sales Numeric. Sales for the given department in the given store.\n\nIsHoliday Logical. Whether the week is a “special” holiday for the store.\n\nType Character. Type identifier of the store.\n\nSize Numeric. Store square-footage\n\nTemperature Numeric. Average temperature in the region.\n\nFuel_Price Numeric. Cost of fuel in the region.\n\nMarkDown1, MarkDown2, MarkDown3, MarkDown4, MarkDown5 Numeric. Anonymized data related to promotional markdowns that Walmart is running. MarkDown data is only available after Nov 2011, and is not available for all stores all the time. Any missing value is marked with an NA.\n\nCPI Numeric. The consumer price index.\n\nUnemployment Numeric. The unemployment rate in the region.\n\n\nVery cool to know that mosaic::inspect() identifies date variables separately!\n\n\n\n\n\n\nNote\n\n\n\nNOTE:\nThis is still a tibble, with a time-oriented variable of course, but not yet a time-series object. The data frame has the YMD columns repeated for each Dept, giving us what is called “long” form data. To deal with this repetition, we will always need to split the Weekly_Sales by the Dept column before we plot or analyze.\n\n\nSince our sales are weekly, we will convert Date to yearweek format:\n\n#|label: walmart sales tsibble\nwalmart_time &lt;- walmart_sales_weekly %&gt;% \n  # mutate(Date = as.Date(Date)) %&gt;% \n  as_tsibble(index = Date, # Time Variable\n             key = Dept)\n             \n  #  Identifies unique \"subject\" who are measures\n  #  All other variables such as Weekly_sales become \"measured variable\"\n  #  Each observation should be uniquely identified by index and key\n\nwalmart_time\n\n\n\n  \n\n\n\nBasic Time Series Plots\nThe easiest way is to use autoplot from the feasts package. You may need to specify the actual measured variable, if there is more than one numerical column:\n\nautoplot(walmart_time,\n         .vars = Weekly_Sales)\n\n\n\n\ntimetk gives us interactive plots that may be more evocative than the static plot above. The basic plot function with timetk is plot_time_series. There are arguments for the date variable, the value you want to plot, colours, groupings etc.\nLet us explore this dataset using timetk, using our trusted method of asking Questions:\nQ.1 How are the weekly sales different for each Department?\nThere are 7 number of Departments. So we should be fine plotting them and also facetting with them, as we will see in a bit:\n\nwalmart_time %&gt;% timetk::plot_time_series(.date_var = Date, \n                                            .value = Weekly_Sales,\n                   .color_var = Dept, \n                   .legend_show = TRUE,\n                   .title = \"Walmart Sales Data by Department\",\n                   .smooth = FALSE)\n\n\n\n\n\nQ.2. What do the sales per Dept look like during the month of December (Christmas time) in 2012? Show the individual Depts as facets.\nWe can of course zoom into the interactive plot above, but if we were to plot it anyway:\n\n# Only include rows from  1 to December 31, 2011\n# Data goes only up to Oct 2012\n\nwalmart_time %&gt;% \n  # Each side of the time_formula is specified as the character 'YYYY-MM-DD HH:MM:SS',\n  timetk::filter_by_time(.date_var = Date,\n                         .start_date = \"2011-12-01\",\n                         .end_date = \"2011-12-31\") %&gt;%\n\n  plot_time_series(.date_var = Date, \n                   .value = Weekly_Sales, \n                   .color_var = Dept, \n                   .facet_vars = Dept, \n                   .facet_ncol = 2,\n                   .smooth = FALSE) # Only 4 points per graph\n\n\n\n\n\nClearly the “unfortunate” Dept#13 has seen something of a Christmas drop in sales, as has Dept#38 ! The rest, all is well, it seems…\nToo much noise? How about some averaging?\nQ.3 How do we smooth out some of the variations in the time series to be able to understand it better?\nSometimes there is too much noise in the time series observations and we want to take what is called a rolling average. For this we will use the function timetk::slidify to create an averaging function of our choice, and then apply it to the time series using regular dplyr::mutate\n\n# Let's take the average of Sales for each month in each Department.\n# Our **function** will be named \"rolling_avg_month\": \n\nrolling_avg_month = slidify(.period = 4, # every 4 weeks\n                            .f = mean, # The funtion to average\n                            .align = \"center\", # Aligned with middle of month\n                            .partial = TRUE) # TO catch any leftover half weeks\nrolling_avg_month\n\nfunction (...) \n{\n    slider_2(..., .slider_fun = slider::pslide, .f = .f, .period = .period, \n        .align = .align, .partial = .partial, .unlist = .unlist)\n}\n&lt;bytecode: 0x00000145a131e670&gt;\n&lt;environment: 0x00000145a131dd40&gt;\n\n\nOK, slidify creates a function! Let’s apply it to the Walmart Sales time series…\n\nwalmart_time %&gt;% \n  # group_by(Dept) %&gt;% \n  mutate(avg_monthly_sales = rolling_avg_month(Weekly_Sales)) %&gt;% \n  # ungroup() %&gt;% \n  timetk::plot_time_series(Date, avg_monthly_sales,.color_var = Dept, .smooth = FALSE)\n\n\n\n\n\nCurves are smoother now. Need to check whether the averaging was done on a per-Dept basis…should we have had a group_by(Dept) before the averaging, and ungroup() before plotting? Try it !!\nDecomposing Time Series: Trends, Seasonal Patterns, and Cycles\nEach data point (\\(Y_t\\)) at time \\(t\\) in a Time Series can be expressed as either a sum or a product of 4 components, namely, Seasonality(\\(S_t\\)), Trend(\\(T_t\\)), Cyclic, and Error(\\(e_t\\)) (a.k.a White Noise).\n\nTrend: pattern exists when there is a long-term increase or decrease in the data.\nSeasonal: pattern exists when a series is influenced by seasonal factors (e.g., the quarter of the year, the month, or day of the week).\nCyclic: pattern exists when data exhibit rises and falls that are not of fixed period (duration usually of at least 2 years). Often combined with Trend into “Trend-Cycle”.\nError or Noise: Random component\n\nDecomposing non-seasonal datas means breaking it up into trend and irregular components. To estimate the trend component of a non-seasonal time series that can be described using an additive model, it is common to use a smoothing method, such as calculating the simple moving average of the time series.\ntimetk has the ability to achieve this: Let us plot the trend, seasonal, cyclic and irregular aspects of Weekly_Sales for Dept 38:\n\nwalmart_time %&gt;% \n  filter(Dept == \"38\") %&gt;% \n  timetk::plot_stl_diagnostics(.data = .,\n                               .date_var = Date, \n                               .value = Weekly_Sales)\n\n\n\n\n\nWe can do this for all Dept using fable and fabletools:\n\nwalmart_decomposed &lt;- \n  walmart_time %&gt;% \n  \n  # If we want to filter, we do it here\n  filter(Dept == \"38\") %&gt;% \n  # \n\nfabletools::model(stl = STL(Weekly_Sales))\n\nfabletools::components(walmart_decomposed)\n\n\n\n  \n\n\nautoplot(components((walmart_decomposed)))"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries.html#case-study-2-dataset-from-nycflights13",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries.html#case-study-2-dataset-from-nycflights13",
    "title": "Analysis of Time Series in R",
    "section": "Case Study-2: Dataset from nycflights13\n",
    "text": "Case Study-2: Dataset from nycflights13\n\nLet us try the flights dataset from the package nycflights13. Try data(package = \"nycflights13\") in your Console.\nWe have the following datasets innycflights13:\nData sets in package nycflights13:\n\n\nairlines Airline names.\n\nairports Airport metadata\n\nflights Flights data\n\nplanes Plane metadata.\n\nweather Hourly weather data\n\nLet us analyze the flights data:\n\ndata(\"flights\", package = \"nycflights13\")\nmosaic::inspect(flights)\n\n\ncategorical variables:  \n     name     class levels      n missing\n1 carrier character     16 336776       0\n2 tailnum character   4043 334264    2512\n3  origin character      3 336776       0\n4    dest character    105 336776       0\n                                   distribution\n1 UA (17.4%), B6 (16.2%), EV (16.1%) ...       \n2 N725MQ (0.2%), N722MQ (0.2%) ...             \n3 EWR (35.9%), JFK (33%), LGA (31.1%)          \n4 ORD (5.1%), ATL (5.1%), LAX (4.8%) ...       \n\nquantitative variables:  \n             name   class  min   Q1 median   Q3  max        mean          sd\n1            year integer 2013 2013   2013 2013 2013 2013.000000    0.000000\n2           month integer    1    4      7   10   12    6.548510    3.414457\n3             day integer    1    8     16   23   31   15.710787    8.768607\n4        dep_time integer    1  907   1401 1744 2400 1349.109947  488.281791\n5  sched_dep_time integer  106  906   1359 1729 2359 1344.254840  467.335756\n6       dep_delay numeric  -43   -5     -2   11 1301   12.639070   40.210061\n7        arr_time integer    1 1104   1535 1940 2400 1502.054999  533.264132\n8  sched_arr_time integer    1 1124   1556 1945 2359 1536.380220  497.457142\n9       arr_delay numeric  -86  -17     -5   14 1272    6.895377   44.633292\n10         flight integer    1  553   1496 3465 8500 1971.923620 1632.471938\n11       air_time numeric   20   82    129  192  695  150.686460   93.688305\n12       distance numeric   17  502    872 1389 4983 1039.912604  733.233033\n13           hour numeric    1    9     13   17   23   13.180247    4.661316\n14         minute numeric    0    8     29   44   59   26.230100   19.300846\n        n missing\n1  336776       0\n2  336776       0\n3  336776       0\n4  328521    8255\n5  336776       0\n6  328521    8255\n7  328063    8713\n8  336776       0\n9  327346    9430\n10 336776       0\n11 327346    9430\n12 336776       0\n13 336776       0\n14 336776       0\n\ntime variables:  \n       name   class               first                last min_diff   max_diff\n1 time_hour POSIXct 2013-01-01 05:00:00 2013-12-31 23:00:00   0 secs 25200 secs\n       n missing\n1 336776       0\n\n\nWe have time-related columns; Apart from year, month, day we have time_hour; and time-event numerical data such as arr_delay (arrival delay) and dep_delay (departure delay). We also have categorical data such as carrier, origin, dest, flight and tailnum of the aircraft. It is also a large dataset containing 330K entries. Enough to play with!!\nLet us replace the NAs in arr_delay and dep_delay with zeroes for now, and convert it into a time-series object with tsibble:\n\nflights_delay_ts &lt;- flights %&gt;% \n  \n  mutate(arr_delay = replace_na(arr_delay, 0), \n         dep_delay = replace_na(dep_delay, 0)) %&gt;% \n  \n  select(time_hour, arr_delay, dep_delay, \n         carrier, origin, dest, \n         flight, tailnum) %&gt;% \n  \n  tsibble::as_tsibble(index = time_hour, \n                      # All the remaining identify unique entries\n                      # Along with index\n                      # Many of these variables are common\n                      # Need *all* to make unique entries!\n                      key = c(carrier, origin, dest,flight, tailnum), \n                      validate = TRUE) # Making sure each entry is unique\n\n\nflights_delay_ts\n\n\n\n  \n\n\n\nQ.1. Plot the monthly average arrival delay by carrier\n\nmean_arr_delays_by_carrier &lt;- \n  flights_delay_ts %&gt;%\n  group_by(carrier) %&gt;% \n  \n  index_by(month = ~ yearmonth(.)) %&gt;% \n  # index_by uses (year, yearquarter, yearmonth, yearweek, as.Date)\n  # to create a new column to show the time-grouping\n  # year / quarter / month/ week, or day...\n  # which IS different from traditional dplyr\n  \n  summarise(mean_arr_delay = \n              mean(arr_delay, na.rm = TRUE)\n  )\n\nmean_arr_delays_by_carrier\n\n\n\n  \n\n\nmean_arr_delays_by_carrier %&gt;%\n  timetk::plot_time_series(\n    .data = .,\n    .date_var = month,\n    .value = mean_arr_delay,\n    .facet_vars = carrier,\n    .smooth = FALSE,\n    # .smooth_degree = 1,\n    \n    # keep .smooth off since it throws warnings if there are too few points\n    # Like if we do quarterly or even yearly summaries\n    # Use only for smaller values of .smooth_degree (0,1)\n    #\n    .facet_ncol = 4,\n    .title = \"Average Monthly Arrival Delays by Carrier\"\n  )\n\n\n\n\n\nQ.2. Plot a candlestick chart for total flight delays by month for each carrier\n\nflights_delay_ts %&gt;% \n  mutate(total_delay = arr_delay + dep_delay) %&gt;%\n  timetk::plot_time_series_boxplot(\n    .date_var = time_hour,\n    .value = total_delay,\n    .color_var = origin,\n    .facet_vars = origin,\n    .period = \"month\",\n  # same warning again\n    .smooth = FALSE\n  )"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/files/parts.html",
    "href": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/files/parts.html",
    "title": "Part of a Whole in R",
    "section": "",
    "text": "Introduction\nWe will create Data Visualizations in R to show Parts of a Whole. As always, we will consistently use the Project Mosaic ecosystem of packages in R (mosaic, mosaicData and ggformula). Some specialized plots ( e.g. Fan Plots) may require us to load other R Packages. These will be introduced appropriately.\n\nRecall the standard method for all commands from the mosaic package:\ngoal( y ~ x | z, data = mydata, …)\n\n\n\n Back to top"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/evol-flow.html",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/evol-flow.html",
    "title": "🕸 Evolution and Flow",
    "section": "",
    "text": "R Tutorial\n  Orange Tutorial\n  Radiant Tutorial \n   Datasets"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/evol-flow.html#fa-folder-open-slides-and-tutorials",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/evol-flow.html#fa-folder-open-slides-and-tutorials",
    "title": "🕸 Evolution and Flow",
    "section": "",
    "text": "R Tutorial\n  Orange Tutorial\n  Radiant Tutorial \n   Datasets"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/evol-flow.html#iconify-noto-v1-package-setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/evol-flow.html#iconify-noto-v1-package-setting-up-r-packages",
    "title": "🕸 Evolution and Flow",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(tidyverse)\nlibrary(ggstream)\n\n# remotes::install_github(\"corybrunson/ggalluvial@main\", build_vignettes = TRUE)\nlibrary(ggalluvial)\nlibrary(ggformula)\n\nlibrary(echarts4r) # Interactive graphs"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/evol-flow.html#iconify-entypo-line-graph-what-time-evolution-charts-can-we-plot",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/evol-flow.html#iconify-entypo-line-graph-what-time-evolution-charts-can-we-plot",
    "title": "🕸 Evolution and Flow",
    "section": "\n What Time Evolution Charts can we plot?",
    "text": "What Time Evolution Charts can we plot?\nIn these cases, the x-axis is typically time…and we chart the variable of another Quant variable with respect to time, using a line geometry.\nLet is take a healthcare budget dataset from Our World in Data: We will plot graphs for 5 countries (India, China, Brazil, Russia, Canada ). Download this data by clicking on the button below:\n Download the Health data \n\nhealth &lt;-\n  read_csv(\"data/public-health-expenditure-share-GDP-OWID.csv\")\n\nhealth_filtered &lt;- health %&gt;%\n  filter(Entity %in% c(\n    \"India\",\n    \"China\",\n    \"United States\",\n    \"United Kingdom\",\n    \"Russia\",\n    \"Sweden\"\n  ))\n\n\n\nUsing ggformula\nUsing echarts4r\n\n\n\n\ngf_point(data = health_filtered,\n             public_health_expenditure_pc_gdp ~ Year, \n             colour = ~ Entity, \n             ylab = \"Healthcare Budget\\n as % of GDP\",\n         title = \"Line Charts to show Evolution (over Time )\") %&gt;% \n  gf_theme(theme_minimal())\ngf_area(data = health_filtered,\n          public_health_expenditure_pc_gdp ~ Year, \n          fill = ~ Entity, alpha = 0.3, \n          ylab = \"Healthcare Budget\\n as % of GDP\",\n        title = \"Area Charts to show Evolution (over Time )\") %&gt;% \n  gf_line(colour = ~ Entity) %&gt;% \n  gf_theme(theme_minimal())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhealth_filtered %&gt;% group_by(Entity) %&gt;% \n  e_charts(Year) %&gt;% \n  e_scatter(public_health_expenditure_pc_gdp) %&gt;% \n  e_line(public_health_expenditure_pc_gdp) %&gt;% \n  e_x_axis(name = \"Year\", min = 1850, max = 2100) %&gt;% \n  e_y_axis(name = \"Public Health Expenditure\", \n           nameLocation = \"middle\", nameGap = 25) %&gt;% \n  e_tooltip()\nhealth_filtered %&gt;% group_by(Entity) %&gt;% \n  e_charts(Year) %&gt;% \n  e_scatter(public_health_expenditure_pc_gdp) %&gt;% \n  e_area(public_health_expenditure_pc_gdp) %&gt;% \n  e_x_axis(name = \"Year\", min = 1850, max = 2100) %&gt;% \n  e_y_axis(name = \"Public Health Expenditure\", \n           nameLocation = \"middle\",nameGap = 25) %&gt;% \n  e_tooltip()"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/evol-flow.html#iconify-carbon-sankey-diagram-alt-what-space-evolution-charts-can-we-plot",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/evol-flow.html#iconify-carbon-sankey-diagram-alt-what-space-evolution-charts-can-we-plot",
    "title": "🕸 Evolution and Flow",
    "section": "\n What Space Evolution Charts can we plot?",
    "text": "What Space Evolution Charts can we plot?\nHere, the space can be any Qual variable, and we can chart another Quant or Qual variable move across levels of the first chosen Qual variable.\nFor instance we can contemplate enrollment at a University, and show how students move from course to course in a University. Or how customers drift from one category of products or brands to another….or the movement of cricket players from one IPL Team to another !!\nHere is what Thomas Lin Pedersen says:\n\nA parallel sets diagram is a type of visualisation showing the interaction between multiple categorical variables. If the variables have an intrinsic order the representation can be thought of as a Sankey Diagram. If each variable is a point in time it will resemble an Alluvial diagram.\n\n\n\n\n\n\nThe Qualitative variables being connected are mapped to stages/axes\n\nEach level within a Qual variable is mapped to nodes / strata / lodes;\nAnd the connections between the strata of the axes are called flows / links / alluvia.\n\n\nSuch diagrams are best used when you want to show a many-to-many mapping between two domains or multiple paths through a set of stages E.g Students going through multiple courses during a semester of study.\nHere is an example of a Sankey Diagram: This diagram show how energy is converted or transmitted before being consumed or lost: supplies are on the left, and demands are on the right. (Data: Department of Energy & Climate Change via Tom Counsell)1:\n\n\n\n\n\n\n\n\nSwitching to ggplot here\n\n\n\nFor the next few charts, there are (as yet) no equivalents in ggformula. Hence we will use ggplot.\n\n\n\n\n\n\n\n\nAnd introducing echarts4r\n\n\n\nWe will also build interactive versions of these charts using echarts4r!\n\n\n\n Case Study-1: Titanic Dataset\n\n# library(ggalluvial)\ndata(\"Titanic\")\nTitanic &lt;- Titanic %&gt;% as_tibble()\nTitanic\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nTable Form Data\n\n\n\nNote that this data is in tidy wide / table form, with separate columns for each Qualitative variable and a separate count column, which we saw when we examined Categorical Data. This is, in my opinion, intuitively the best form of data to plot a Sankey plot with. But there are other forms such as the tidy long form which we have been using practically all this while. You will find examples of on the ggalluvial website using tidy long form data. https://corybrunson.github.io/ggalluvial/\n\n\n\n\nUsing ggplot\nUsing echarts4r\n\n\n\n\nTitanic %&gt;% ggplot(data = .,\n    \n# Select the Categorical Variables for the vertical Axes / Stages\n       aes(axis1 = Class, \n           axis2 = Sex, \n           axis3 = Age,\n           axis4 = Survived,\n           y = n), fill = \"white\") +\n  \n# Alluvials between Categorical Axes\n  geom_alluvium(aes(fill = Survived), \n                colour = \"black\", \n                linewidth = 0.25) +\n  \n# Vertical segments for each Categorical Variable2 \n  geom_stratum(colour = \"black\", \n               linewidth = 1, \n               fill = \"white\") + \n  \n# Labels for each \"level\" of the Categorical Axes\n  geom_text(stat = \"stratum\", \n            aes(label = after_stat(stratum))) +\n  \n\n  \n# Scales and Colours\n  scale_x_discrete(limits = c(\"Class\", \"Sex\", \"Age\"), \n                   expand = c(.2, .05)) +\n  \n  scale_fill_manual(values = c(\"red\", \"green\")) + \n  \n  xlab(\"Demographic\") +\n  theme_minimal() +\n  ggtitle(\"Passengers on the maiden voyage of the Titanic\",\n          \"Stratified by demographics and survival\")\n\n\n\n\n\nHere is how the package ggalluvial defines the elements of a typical alluvial plot:\n\nAn axis is a dimension (variable) along which the data are vertically arranged at a fixed horizontal position. The plot above uses three categorical axes: Class, Sex, and Age.\nThe groups at each axis are depicted as opaque blocks called strata. For example, the Class axis contains four strata: 1st, 2nd, 3rd, and Crew.\nHorizontal (x-) splines called alluvia span the entire width of the plot. In this plot, each alluvium corresponds to a fixed strata value of each axis variable, indicated by its vertical position at the axis, as well as of the Survived variable, indicated by its fill color.\nThe segments of the alluvia between pairs of adjacent axes are flows.\nThe alluvia intersect the strata at lodes. The lodes are not visualized in the above plot, but they can be inferred as filled rectangles extending the flows through the strata at each end of the plot or connecting the flows on either side of the center stratum.\n\n\n\n\nLet us make an interactive graph for this dataset using echarts4.\n\nClassSex &lt;- \n  Titanic %&gt;% group_by(Class, Sex) %&gt;% summarise(cs = sum(n)) %&gt;% \n  ungroup() %&gt;% \n  rename(\"source\" = Class, \"target\" = Sex, \"value\" = cs)\n            \nSexAge &lt;- \n  Titanic %&gt;% group_by(Sex,Age) %&gt;% summarise(sa = sum(n)) %&gt;% \n  ungroup() %&gt;% \n  rename(\"source\" = Sex, \"target\" = Age, \"value\" = sa)\n\nAgeSurvived &lt;- \n  Titanic %&gt;% group_by(Age,Survived) %&gt;% summarise(as = sum(n)) %&gt;% \n  ungroup() %&gt;% \n  rename(\"source\" = Age, \"target\" = Survived, \"value\" = as)\n\nCombo &lt;- rbind(ClassSex, SexAge, AgeSurvived)\nCombo\n\n\n\n  \n\n\nCombo %&gt;% e_charts() %&gt;% \n  e_sankey(source, target, value) %&gt;% \n  e_title(\"Titanic: Who lived, and who didn't?\") %&gt;% \n  e_tooltip()\n\n\n\n\n\nThe process with echarts4r is quite different, since the data structure used by this package is different:\n\nThe echarts4r package needs to have source and target columns for axes, along with a value to determine the width of the alluvium. \nThe names in the source and target can repeat, and can appear in both source and target columns in order to create a multi-axis diagram. Hence the data needs to be inherently in long form.\nHowever, for the values, we need to manually calculate the aggregate totals for alluvia between each consecutive pairs of axes (i.e Qual variables). This is not done automatically in echarts4r, but it is with ggalluvial.\nSo we create grouped aggregate summaries for each pair of Qualitative variables that we wish to plot consecutively ( i.e as axis1, axis2…)\nStack these pair-wise alluvia totals into one combo data frame using rbind(), after renaming the variables to “source”, “target” and “value”.\n\nPhew! seems like too much work to do…I wonder if good, old-fashioned pivot-longer will get us here…"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/evol-flow.html#iconify-icon-park-outline-chart-histogram-chord-diagram",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/evol-flow.html#iconify-icon-park-outline-chart-histogram-chord-diagram",
    "title": "🕸 Evolution and Flow",
    "section": "\n Chord Diagram",
    "text": "Chord Diagram\n\n\n\n\nWe will explore this diagram when we explore network graphs with the tidygraph and ggraph packages."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/evol-flow.html#iconify-fluent-mdl2-decision-solid-conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/evol-flow.html#iconify-fluent-mdl2-decision-solid-conclusion",
    "title": "🕸 Evolution and Flow",
    "section": "\n Conclusion",
    "text": "Conclusion\nWe see that we can visualize “evolutions” over time and space. The evolutions can represent changes in the quantities of things, or their categorical affiliations or groups.\nWhat business data would you depict in this way? Revenue streams? Employment? Expenditures over time and market? There are many possibilities!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/evol-flow.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/evol-flow.html#your-turn",
    "title": "🕸 Evolution and Flow",
    "section": "Your Turn",
    "text": "Your Turn\n\nWithin the ggalluvial package are two datasets, majors and vaccinations. Plot alluvial charts for both of these.\nGo to the American Life Panel Website where you will find many public datasets. Try to take one and make charts from it that we have learned in this Module."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/evol-flow.html#iconify-ooui-references-ltr-references",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/evol-flow.html#iconify-ooui-references-ltr-references",
    "title": "🕸 Evolution and Flow",
    "section": "\n References",
    "text": "References\n\nGlobal Migration, https://download.gsb.bund.de/BIB/global_flow/ A good example of the use of a Chord Diagram.\nCory Brunson, ggalluvial, https://corybrunson.github.io/ggalluvial/\nJohn Coene, Sankey plots with echarts4r, https://echarts4r.john-coene.com/articles/chart_types.html#sankey\nOther packages: Sankey plot | the R Graph Gallery (r-graph-gallery.com)\nAnother package: Sankey diagrams in ggplot2 with ggsankey | R CHARTS (r-charts.com)\nSankey Charts using networkD3: http://christophergandrud.github.io/networkD3"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/evol-flow.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/evol-flow.html#footnotes",
    "title": "🕸 Evolution and Flow",
    "section": "Footnotes",
    "text": "Footnotes\n\nD3 JavaScript Network Graphs from R: christophergandrud.github.io/networkD3/↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Ranking/ranking.html",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Ranking/ranking.html",
    "title": "🖏 Ratings and Rankings",
    "section": "",
    "text": "TBD."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Ranking/ranking.html#fa-folder-open-slides-and-tutorials",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Ranking/ranking.html#fa-folder-open-slides-and-tutorials",
    "title": "🖏 Ratings and Rankings",
    "section": "",
    "text": "TBD."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Ranking/ranking.html#iconify-noto-v1-package-setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Ranking/ranking.html#iconify-noto-v1-package-setting-up-r-packages",
    "title": "🖏 Ratings and Rankings",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nknitr::opts_chunk$set(echo = TRUE)\nlibrary(tidyverse) # includes ggplot for plotting\nlibrary(mosaic)\nlibrary(ggformula)\n\nlibrary(ggbump) # Bump Charts\n\nlibrary(ggiraphExtra) # Radar, Spine, Donut and Donut-Pie combo charts !!\n\n# install.packages(\"devtools\")\n# devtools::install_github(\"ricardo-bion/ggradar\")\nlibrary(ggradar) # Radar Plots"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Ranking/ranking.html#iconify-fxemoji-japanesesymbolforbeginner-what-graphs-are-we-going-to-see-today",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Ranking/ranking.html#iconify-fxemoji-japanesesymbolforbeginner-what-graphs-are-we-going-to-see-today",
    "title": "🖏 Ratings and Rankings",
    "section": "\n What graphs are we going to see today?",
    "text": "What graphs are we going to see today?\nWhen we wish to compare the size of things and rank them, there are quite a few ways to do it.\nBar Charts and Lollipop Charts are immediately obvious when we wish to rank things on one aspect or parameter.\nWhen we wish to rank the same set of objects against multiple aspects or parameters, then we can use Bump Charts and Radar Charts."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Ranking/ranking.html#iconify-openmoji-poppy-lollipop-charts",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Ranking/ranking.html#iconify-openmoji-poppy-lollipop-charts",
    "title": "🖏 Ratings and Rankings",
    "section": "\n Lollipop Charts",
    "text": "Lollipop Charts\n\n# Sample data set\nset.seed(1)\ndf1 &lt;- tibble(product = LETTERS[1:10],\n                 rank = sample(20:35, 10, replace = TRUE))\ndf1\n\n\n\n  \n\n\ngf_segment(0 + rank ~ product + product, data = df1) %&gt;% \n# A formula with shape y + yend ~ x + xend.\n  \n  gf_point(rank ~ product, colour = ~ product, \n           size = 5,\n           ylab = \"Rank\",\n           xlab = \"Product\")\n\n\n\n# Compare with ggplot code:\n# ggplot(df1) + \n#   geom_segment(aes(x = x, xend = x, y = 0, yend = y)) + \n#   geom_point(aes(x = x, y = y, colour = x), size = 5)\n\nWe can flip this horizontally and reorder the \\(x\\) categories in order of decreasing ( or increasing ) \\(y\\), using forcats::fct_reorder:\n\ngf_segment(0 + rank ~ fct_reorder(product, - rank) + \n             fct_reorder(product, - rank), \n           data = df1) %&gt;%\n  # A formula with shape y + yend ~ x + xend.\n  \n  gf_point(rank ~ product, colour = ~ product, size = 5) %&gt;%\n  \n  gf_refine(coord_flip()) %&gt;%\n  gf_labs(x = \"Product\", y = \"Rank\") %&gt;%\n  gf_theme(theme_minimal())\n\n\n\n# Compare with ggplot code\n# ggplot(df1) + \n#   geom_segment(aes(x = fct_reorder(x, -y), # in decreasing order of y\n#                    xend = fct_reorder(x, -y), \n#                    y = 0, \n#                    yend = y)) + \n#   geom_point(aes(x = x, y = y, colour = x), size = 5) +\n#   coord_flip() + \n#   xlab(\"Group\") +\n#   ylab(\"\") +\n#   theme_minimal()"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Ranking/ranking.html#iconify-teenyicons-curved-connector-outline-bump-charts",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Ranking/ranking.html#iconify-teenyicons-curved-connector-outline-bump-charts",
    "title": "🖏 Ratings and Rankings",
    "section": "\n Bump Charts",
    "text": "Bump Charts\nBump Charts track the ranking of several objects based on other parameters, such as time/month or even category. For instance, what is the opinion score of a set of products across various categories of users?\n\nyear &lt;- rep(2019:2021, 4)\nposition &lt;- c(4, 2, 2, 3, 1, 4, 2, 3, 1, 1, 4, 3)\nproduct &lt;- c(\"A\", \"A\", \"A\",\n            \"B\", \"B\", \"B\", \n            \"C\", \"C\", \"C\",\n            \"D\", \"D\", \"D\")\n\ndf2 &lt;- tibble(year,position,product)\n\ndf2\n\n\n\n  \n\n\n\n\n\n\n\n\n\nggbump uses ggplot syntax\n\n\n\nWe need to use a new package called, what else, ggbump to create our Bump Charts: Here again we do not yet have a ggformula equivalent . Note the + syntax with ggplot code!!\n\n\n\nlibrary(ggbump)\n\nggplot(df2) +\n  geom_bump(aes(x = year, y = position, color = product)) +\n  geom_point(aes(x = year, y = position, color = product),\n             size = 6) +\n    theme_minimal() +\n    xlab(\"Year\") +\n    ylab(\"Rank\") +\n    scale_color_brewer(palette = \"RdBu\") # Change Colour Scale\n\n\n\n\nWe can add labels along the “bump lines” and remove the legend altogether:\n\nggplot(df2) +\n  geom_bump(aes(x = year, y = position, color = product)) +\n  geom_point(aes(x = year, y = position, color = product),\n             size = 6) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"RdBu\") + # Change Colour Scale\n# Same as before up to here\n# Add the labels at start and finish\n\n  geom_text(data = df2 %&gt;% filter(year == min(year)),\n            aes(x = year - 0.1, label = product, y = position),\n            size = 5, hjust = 1) +\n  geom_text(data = df2 %&gt;% filter(year == max(year)),\n            aes(x = year + 0.1, label = product, y = position),\n            size = 5, hjust = 0) +\n  xlab(\"Year\") +\n  ylab(\"Rank\") +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Ranking/ranking.html#iconify-ant-design-radar-chart-outlined-radar-charts",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Ranking/ranking.html#iconify-ant-design-radar-chart-outlined-radar-charts",
    "title": "🖏 Ratings and Rankings",
    "section": "\n Radar Charts",
    "text": "Radar Charts\nWhat if your marketing folks had rated some products along several different desirable criteria? Such data, where a certain set of items (Qualitative!!) are rated (Quantitative !!) against another set (Qualitative again!!) can be plotted on a roughly circular set of axes, with the radial distance defining the rank against each axes.\nOf course, we will use the aptly named ggradar, which is at this time (Feb 2023) a development version and not yet part of CRAN. We will still try it, and another package ggiraphExtra which IS a part of CRAN ( and has some other capabilities too, which are worth exploring!)\n\n#library(ggradar)\n\nset.seed(4)\ndf3 &lt;- tibble(Product = c(\"G1\", \"G2\", \"G3\"),\n              Power = runif(3), \n              Cost = runif(3),\n              Harmony = runif(3),\n              Style = runif(3),\n              Size = runif(3),\n              Manufacturability = runif(3),\n              Durability = runif(3),\n              Universality = runif(3))\ndf3\n\n\n\n  \n\n\n\n\n\n\n\n\n\nWide Form Data\n\n\n\nLook at the data: this is wide form data. The columns pertaining to each of the Product-Features would normally be stacked into two columns, one with the Feature and the other with the score. Note the trio: Qual(Product) + Quant(Scores) + Qual(Criteria, multi-column)\n\n\n\n Data: Wide to Long\n\ndf3 %&gt;% \n  pivot_longer(cols = c(-Product), \n               names_to = \"Feature\", \n               values_to = \"Score\") %&gt;% \n  arrange(Product)\n\n\n\n  \n\n\n\n\n Using ggradar\n\n\nggradar::ggradar(plot.data = df3,\n                 axis.label.size = 3, # Titles of Params\n                 grid.label.size = 4, # Score Values/Circles\n                 group.point.size = 3,# Product Points Sizes\n                 group.line.width = 1, # Product Line Widths\n                 fill = TRUE, # fill the radar polygons\n                 fill.alpha = 0.3, # Not too dark, Arvind\n                 legend.title = \"Product\") +\n  theme_void()\n\n\n\n\n\n Using ggiraphExtra\n\nFrom the ggiraphExtra website:\n\nPackage ggiraphExtra contains many useful functions for exploratory plots. These functions are made by both ‘ggplot2’ and ‘ggiraph’ packages. You can make a static ggplot or an interactive ggplot by setting the parameter interactive=TRUE.\n\n\n# library(ggiraphExtra)\n\nggiraphExtra::ggRadar(data = df3,\n        aes(colour = Product),\n        rescale = FALSE,\n        title = \"Using ggiraphExtra\"\n          )  + # recale = TRUE makes it look different...try!!\n  theme_minimal()\n\n\n\n\nBoth render very similar-looking radar charts and the syntax is not too intimidating!!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Ranking/ranking.html#iconify-icon-park-me-your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Ranking/ranking.html#iconify-icon-park-me-your-turn",
    "title": "🖏 Ratings and Rankings",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nTake the HELPrct dataset from our well used mosaicData package. Plot ranking charts using each of the public health issues that you can see in that dataset. What choice will you make for the the axes?\nTry the SaratogaHouses dataset also from mosaicData."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Ranking/ranking.html#iconify-ooui-references-ltr-references",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Ranking/ranking.html#iconify-ooui-references-ltr-references",
    "title": "🖏 Ratings and Rankings",
    "section": "\n References",
    "text": "References\n\nKeon-Woong Moon, R Package ggiraphExtra, https://cran.r-project.org/web/packages/ggiraphExtra/vignettes/introduction.html"
  },
  {
    "objectID": "content/courses/Analytics/Inference/listing.html",
    "href": "content/courses/Analytics/Inference/listing.html",
    "title": "Statistical Inference",
    "section": "",
    "text": "Important\n\n\n\nStatistical inference is the process of drawing conclusions about the entire population based on the information in a sample.\n\n\nIn this Section we will examine samples from populations and find procedures for estimating parameters such as means and sd. We will also devise procedures for comparing means and variances across more than one population. The conditions that make these procedures possible and accurate will also be studied and we will find alternative methods when those assumptions breakdown.\nBased on our ideas of data and types of variables, here is a table of what we may infer, based on the underlying data:\n\nData Types and Inference\n\n\n\n\n\n\n\n\nVariable(s)\nEstimating:\nPopulation Parameter\nSample Statistic\n\n\n\n\nSingle Qual variable\nProportion\np\n\\(\\hat{p}\\)\n\n\nSingle Quant variable\nMean\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\n\nTwo Qual Variables\nDifference in Proportions\n\\(p_1 -p_2\\)\n\\(\\hat{p_1} - \\hat{p_2}\\)\n\n\nOne Qual, one Quant\nDifference in Means\n\\(\\mu_1 - \\mu_2\\)\n\\(\\bar{x_1}-\\bar{x_2}\\)\n\n\nTwo Quant variables\nCorrelation\n\\(\\rho\\)\nr\n\n\n\nWe will examine inference procedures for all these cases."
  },
  {
    "objectID": "content/courses/Analytics/Inference/listing.html#what-is-inference",
    "href": "content/courses/Analytics/Inference/listing.html#what-is-inference",
    "title": "Statistical Inference",
    "section": "",
    "text": "Important\n\n\n\nStatistical inference is the process of drawing conclusions about the entire population based on the information in a sample.\n\n\nIn this Section we will examine samples from populations and find procedures for estimating parameters such as means and sd. We will also devise procedures for comparing means and variances across more than one population. The conditions that make these procedures possible and accurate will also be studied and we will find alternative methods when those assumptions breakdown.\nBased on our ideas of data and types of variables, here is a table of what we may infer, based on the underlying data:\n\nData Types and Inference\n\n\n\n\n\n\n\n\nVariable(s)\nEstimating:\nPopulation Parameter\nSample Statistic\n\n\n\n\nSingle Qual variable\nProportion\np\n\\(\\hat{p}\\)\n\n\nSingle Quant variable\nMean\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\n\nTwo Qual Variables\nDifference in Proportions\n\\(p_1 -p_2\\)\n\\(\\hat{p_1} - \\hat{p_2}\\)\n\n\nOne Qual, one Quant\nDifference in Means\n\\(\\mu_1 - \\mu_2\\)\n\\(\\bar{x_1}-\\bar{x_2}\\)\n\n\nTwo Quant variables\nCorrelation\n\\(\\rho\\)\nr\n\n\n\nWe will examine inference procedures for all these cases."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html",
    "title": "🃏 Permutation Test for Two Proportions",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(ggmosaic) # plotting mosaic plots for Categorical Data\n\n### Dataset from Chihara and Hesterberg's book (Second Edition)\nlibrary(resampledata)\n\n\nAttaching package: 'resampledata'\n\nThe following object is masked from 'package:datasets':\n\n    Titanic"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#setting-up-the-packages",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#setting-up-the-packages",
    "title": "🃏 Permutation Test for Two Proportions",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(ggmosaic) # plotting mosaic plots for Categorical Data\n\n### Dataset from Chihara and Hesterberg's book (Second Edition)\nlibrary(resampledata)\n\n\nAttaching package: 'resampledata'\n\nThe following object is masked from 'package:datasets':\n\n    Titanic"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#introduction",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#introduction",
    "title": "🃏 Permutation Test for Two Proportions",
    "section": "Introduction",
    "text": "Introduction\nWe saw from the diagram created by Allen Downey that there is only one test! We will now use this philosophy to develop a technique that allows us to mechanize several Statistical Models in that way, with nearly identical code.\nWe will use two packages in R, mosaic and the relatively new infer package, to develop our intuition for what are called permutation based statistical tests."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#testing-for-two-or-more-proportions",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#testing-for-two-or-more-proportions",
    "title": "🃏 Permutation Test for Two Proportions",
    "section": "Testing for Two or More Proportions",
    "text": "Testing for Two or More Proportions\nLet us try a dataset with Qualitative / Categorical data. This is the General Social Survey GSS dataset, and we have people with different levels of Education stating their opinion on the Death Penalty. We want to know if these two Categorical variables have a correlation, i.e. can the opinions in favour of the Death Penalty be explained by the Education level?\nSince data is Categorical ( both variables ), we need to take counts in a table, and then implement a chi-square test. In the test, we will permute the Education variable to see if we can see how significant its effect size is.\n\ndata(GSS2002)\ninspect(GSS2002)\n\n\ncategorical variables:  \n            name  class levels    n missing\n1         Region factor      7 2765       0\n2         Gender factor      2 2765       0\n3           Race factor      3 2765       0\n4      Education factor      5 2760       5\n5        Marital factor      5 2765       0\n6       Religion factor     13 2746      19\n7          Happy factor      3 1369    1396\n8         Income factor     24 1875     890\n9       PolParty factor      8 2729      36\n10      Politics factor      7 1331    1434\n11     Marijuana factor      2  851    1914\n12  DeathPenalty factor      2 1308    1457\n13        OwnGun factor      3  924    1841\n14        GunLaw factor      2  916    1849\n15 SpendMilitary factor      3 1324    1441\n16     SpendEduc factor      3 1343    1422\n17      SpendEnv factor      3 1322    1443\n18      SpendSci factor      3 1266    1499\n19        Pres00 factor      5 1749    1016\n20      Postlife factor      2 1211    1554\n                                    distribution\n1  North Central (24.7%) ...                    \n2  Female (55.6%), Male (44.4%)                 \n3  White (79.1%), Black (14.8%) ...             \n4  HS (53.8%), Bachelors (16.1%) ...            \n5  Married (45.9%), Never Married (25.6%) ...   \n6  Protestant (53.2%), Catholic (24.5%) ...     \n7  Pretty happy (57.3%) ...                     \n8  40000-49999 (9.1%) ...                       \n9  Ind (19.3%), Not Str Dem (18.9%) ...         \n10 Moderate (39.2%), Conservative (15.8%) ...   \n11 Not legal (64%), Legal (36%)                 \n12 Favor (68.7%), Oppose (31.3%)                \n13 No (65.5%), Yes (33.5%) ...                  \n14 Favor (80.5%), Oppose (19.5%)                \n15 About right (46.5%) ...                      \n16 Too little (73.9%) ...                       \n17 Too little (60%) ...                         \n18 About right (49.7%) ...                      \n19 Bush (50.6%), Gore (44.7%) ...               \n20 Yes (80.5%), No (19.5%)                      \n\nquantitative variables:  \n  name   class min  Q1 median   Q3  max mean       sd    n missing\n1   ID integer   1 692   1383 2074 2765 1383 798.3311 2765       0\n\n\nNote how all variables are Categorical !! Education has five levels:\n\nGSS2002 %&gt;% count(Education)\n\n\n\n  \n\n\nGSS2002 %&gt;% count(DeathPenalty)\n\n\n\n  \n\n\n\nLet us drop NA entries in Education and Death Penalty. And set up a table for the chi-square test.\n\ngss2002 &lt;- GSS2002 %&gt;% \n  dplyr::select(Education, DeathPenalty) %&gt;% \n  tidyr::drop_na(., c(Education, DeathPenalty))\ndim(gss2002)\n\n[1] 1307    2\n\ngss_summary &lt;- gss2002 %&gt;%\n  mutate(\n    Education = factor(\n      Education,\n      levels = c(\"Bachelors\", \"Graduate\", \"Jr Col\", \"HS\", \"Left HS\"),\n      labels = c(\"Bachelors\", \"Graduate\", \"Jr Col\", \"HS\", \"Left HS\")\n    ),\n    DeathPenalty = as.factor(DeathPenalty)\n  ) %&gt;%\n  group_by(Education, DeathPenalty) %&gt;%\n  summarise(count = n()) %&gt;% # This is good for a chisq test\n  \n  # Add two more columns to facilitate mosaic/Marrimekko Plot\n  # \n  mutate(edu_count = sum(count), \n         edu_prop = count / sum(count)) %&gt;%\n  ungroup() \n\ngss_summary"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#sec-table-plots",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#sec-table-plots",
    "title": "🃏 Permutation Test for Two Proportions",
    "section": "Table Plots",
    "text": "Table Plots\nWe can plot a heatmap-like mosaic chart for this table.\nUsing ggplot\n\n\n# https://stackoverflow.com/questions/19233365/how-to-create-a-marimekko-mosaic-plot-in-ggplot2\n\nggplot(data = gss_summary, aes( x = Education, y = edu_prop)) +\n  \n  geom_bar(aes(width = edu_count, fill = DeathPenalty), \n           stat = \"identity\", \n           position = \"fill\", \n           colour = \"black\") +\n  \n  geom_text(aes(label = scales::percent(edu_prop)), \n            position = position_stack(vjust = 0.5)) +\n\n\n# if labels are desired\n facet_grid(~ Education, scales = \"free_x\", space = \"free_x\") + \n  theme(scale_fill_brewer(palette = \"RdYlGn\")) + \n  # theme(panel.spacing.x = unit(0, \"npc\")) + # if no spacing preferred between bars\n  theme_void() \n\n\n\n\nUsing ggmosaic\n\n\n#library(ggmosaic)\n\nggplot(data = gss2002) +\n  geom_mosaic(aes(x = product(DeathPenalty, Education), fill = DeathPenalty))"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#section",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#section",
    "title": "🃏 Permutation Test for Two Proportions",
    "section": "",
    "text": "Observed Statistic: the X^2 metric\nWhen there are multiple proportions involved, the X^2 test is what is used.\nLet us now perform the base chisq test: We need a table and then the chisq test:\n\ngss_table &lt;- tally(DeathPenalty ~ Education, data = gss2002)\ngss_table\n\n            Education\nDeathPenalty Left HS  HS Jr Col Bachelors Graduate\n      Favor      117 511     71       135       64\n      Oppose      72 200     16        71       50\n\n# Get the observed chi-square statistic\nobservedChi2 &lt;- mosaic::chisq(tally(DeathPenalty ~ Education, data = gss2002))\nobservedChi2\n\nX.squared \n 23.45093 \n\n# Actual chi-square test\nstats::chisq.test(tally(DeathPenalty ~ Education, data = gss2002))\n\n\n    Pearson's Chi-squared test\n\ndata:  tally(DeathPenalty ~ Education, data = gss2002)\nX-squared = 23.451, df = 4, p-value = 0.0001029\n\n\nWhat would our Hypotheses be?\n$$ H_0: Education Does Not affect Votes on Death Penalty\\\nH_a: Education affects Votes on Death Penalty\n$$\nWe should now repeat the test with permutations on Education:\n\nnull_chisq &lt;- do(10000) * chisq.test(tally(DeathPenalty ~ shuffle(Education), data = gss2002))\n\nhead(null_chisq)\n\n\n\n  \n\n\ngf_histogram( ~ X.squared, data = null_chisq) %&gt;% \n  gf_vline(xintercept = observedChi2, color = \"red\")\n\n\n\nprop1(~ X.squared &gt;= observedChi2, data = null_chisq)\n\nprop_TRUE \n9.999e-05 \n\n\nThe p-value is well below our threshold of \\(0.05%\\), so we would conclude that Education has a significant effect on DeathPenalty opinion!"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#conclusion",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#conclusion",
    "title": "🃏 Permutation Test for Two Proportions",
    "section": "Conclusion",
    "text": "Conclusion\nSo, what do you think?"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html",
    "title": "Inference for Two Independent Means",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE, message = TRUE, warning = TRUE, fig.align = \"center\")\nlibrary(tidyverse)\nlibrary(mosaic) # Our go-to package\nlibrary(infer) # An alternative package for inference using tidy data\nlibrary(broom) # Clean test results in tibble form\nlibrary(skimr) # data inspection\n\nlibrary(resampledata) # Datasets from Chihara and Hesterberg's book\nlibrary(openintro) # datasets\nlibrary(gt) # for tables\n\n\n\nflowchart TD\n    A[Inference for Independent Means] --&gt;|Check Assumptions| B[Normality: Shapiro-Wilk Test shapiro.test\\n Variances: Fisher F-test var.test]\n    B --&gt; C{OK?}\n    C --&gt;|Yes, both\\n Parametric| D[t.test]\n    D &lt;--&gt;F[Linear Model\\n Method] \n    C --&gt;|Yes, but not variance\\n Parametric| W[t.test with\\n Welch Correction]\n    W&lt;--&gt;F\n    C --&gt;|No\\n Non-Parametric| E[wilcox.test]\n    E &lt;--&gt; G[Linear Model\\n with\\n Signed-Ranks]\n    C --&gt;|No\\n Non-Parametric| P[Bootstrap\\n or\\n Permutation]\n    P &lt;--&gt; Q[Linear Model\\n with Signed-Rank\\n with Permutation]\n\n\n\nflowchart TD\n    A[Inference for Independent Means] --&gt;|Check Assumptions| B[Normality: Shapiro-Wilk Test shapiro.test\\n Variances: Fisher F-test var.test]\n    B --&gt; C{OK?}\n    C --&gt;|Yes, both\\n Parametric| D[t.test]\n    D &lt;--&gt;F[Linear Model\\n Method] \n    C --&gt;|Yes, but not variance\\n Parametric| W[t.test with\\n Welch Correction]\n    W&lt;--&gt;F\n    C --&gt;|No\\n Non-Parametric| E[wilcox.test]\n    E &lt;--&gt; G[Linear Model\\n with\\n Signed-Ranks]\n    C --&gt;|No\\n Non-Parametric| P[Bootstrap\\n or\\n Permutation]\n    P &lt;--&gt; Q[Linear Model\\n with Signed-Rank\\n with Permutation]"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "title": "Inference for Two Independent Means",
    "section": "",
    "text": "flowchart TD\n    A[Inference for Independent Means] --&gt;|Check Assumptions| B[Normality: Shapiro-Wilk Test shapiro.test\\n Variances: Fisher F-test var.test]\n    B --&gt; C{OK?}\n    C --&gt;|Yes, both\\n Parametric| D[t.test]\n    D &lt;--&gt;F[Linear Model\\n Method] \n    C --&gt;|Yes, but not variance\\n Parametric| W[t.test with\\n Welch Correction]\n    W&lt;--&gt;F\n    C --&gt;|No\\n Non-Parametric| E[wilcox.test]\n    E &lt;--&gt; G[Linear Model\\n with\\n Signed-Ranks]\n    C --&gt;|No\\n Non-Parametric| P[Bootstrap\\n or\\n Permutation]\n    P &lt;--&gt; Q[Linear Model\\n with Signed-Rank\\n with Permutation]\n\n\n\nflowchart TD\n    A[Inference for Independent Means] --&gt;|Check Assumptions| B[Normality: Shapiro-Wilk Test shapiro.test\\n Variances: Fisher F-test var.test]\n    B --&gt; C{OK?}\n    C --&gt;|Yes, both\\n Parametric| D[t.test]\n    D &lt;--&gt;F[Linear Model\\n Method] \n    C --&gt;|Yes, but not variance\\n Parametric| W[t.test with\\n Welch Correction]\n    W&lt;--&gt;F\n    C --&gt;|No\\n Non-Parametric| E[wilcox.test]\n    E &lt;--&gt; G[Linear Model\\n with\\n Signed-Ranks]\n    C --&gt;|No\\n Non-Parametric| P[Bootstrap\\n or\\n Permutation]\n    P &lt;--&gt; Q[Linear Model\\n with Signed-Rank\\n with Permutation]"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#iconify-carbon-chart-3d-inspecting-and-charting-data",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#iconify-carbon-chart-3d-inspecting-and-charting-data",
    "title": "Inference for Two Independent Means",
    "section": "\n Inspecting and Charting Data",
    "text": "Inspecting and Charting Data\nA.  Check for Normality\nStatistical tests for means usually require a couple of checks1 2:\n\nAre the data normally distributed?\n\nAre the data variances similar?:\n\nLet us also complete a check for normality: the shapiro.wilk test checks whether a Quant variable is from a normal distribution; the NULL hypothesis is that the data are from a normal distribution.\nB.  Check for Variances\n\n\n\n\n\n\nConditions:\n\n\n\n\nThe two variables are not normally distributed.\nThe two variances are also significantly different."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#iconify-academicons-hypothesis-hypothesis",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#iconify-academicons-hypothesis-hypothesis",
    "title": "Inference for Two Independent Means",
    "section": "\n Hypothesis",
    "text": "Hypothesis"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#iconify-academicons-hypothesis-observed-and-test-statistic",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#iconify-academicons-hypothesis-observed-and-test-statistic",
    "title": "Inference for Two Independent Means",
    "section": "\n Observed and Test Statistic",
    "text": "Observed and Test Statistic"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#iconify-fluent-mdl2-insights-inference",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#iconify-fluent-mdl2-insights-inference",
    "title": "Inference for Two Independent Means",
    "section": "\n Inference",
    "text": "Inference\n::: panel-tabset\nUsing the Parametric t.test\n\nUsing the non-parametric wilcox.test\n\nUsing the Linear Model Interpretation\nUsing the Permutation Test\nAll Tests Together"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#iconify-carbon-chart-3d-inspecting-and-charting-data-1",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#iconify-carbon-chart-3d-inspecting-and-charting-data-1",
    "title": "Inference for Two Independent Means",
    "section": "\n Inspecting and Charting Data",
    "text": "Inspecting and Charting Data\n\ndata(yrbss)\nyrbss\nyrbss_inspect &lt;- inspect(yrbss)\nyrbss_inspect$categorical\nyrbss_inspect$quantitative\n\n\n\n  \n\n\n  \n\n\n  \n\n\n\nWe have 13K data entries, and with 13 different variables, some Qual and some Quant. Many entries are missing too, typical of real-world data and something we will have to account for in our computations. The meaning of each variable can be found by bringing up the help file. Type this in your console: help(yrbss)\nIn this tutorial, our research question is:\n\n\n\n\n\n\nResearch Question\n\n\n\nDoes weight of highschoolers in this dataset vary with gender?"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#iconify-carbon-chart-3d-inspecting-and-charting-data-2",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#iconify-carbon-chart-3d-inspecting-and-charting-data-2",
    "title": "Inference for Two Independent Means",
    "section": "\n Inspecting and Charting Data",
    "text": "Inspecting and Charting Data\nFirst, histograms and densities of the variable we are interested in:\n\nyrbss_select_gender &lt;- yrbss %&gt;% \n  select(weight, gender, physically_active_7d) %&gt;% \n  drop_na(weight) # Sadly dropping off NA data\n\nyrbss_select_gender %&gt;%\n  gf_density( ~ weight,\n              fill = ~ gender,\n              alpha = 0.5,\n              title = \"Highschoolers' Weights by Gender\") %&gt;%\n  gf_theme(theme_classic())\nyrbss_select_gender %&gt;%\n  gf_boxplot(weight ~ gender,\n              fill = ~ gender,\n              alpha = 0.5,\n              title = \"Highschoolers' Weights by Gender\") %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverlapped Distribution plot shows some difference in the means; and the Boxplots show visible difference in the medians.\nA.  Check for Normality\nAs stated before, statistical tests for means usually require a couple of checks:\n\nAre the data normally distributed?\n\nAre the data variances similar?\n\nLet us also complete a visual check for normality,with plots since we cannot do a shapiro.test:\n\n\n\n\n\n\nShapiro-Wilks Test\n\n\n\nThe longest data it can take (in R) is 5000. Since our data is longer, we will cannot use this procedure and have to resort to visual means.\n\n\n\nmale_student_weights &lt;- yrbss_select_gender %&gt;% filter(gender == \"male\") %&gt;% select(weight)\nfemale_student_weights &lt;- yrbss_select_gender %&gt;% filter(gender == \"female\") %&gt;% select(weight)\n#shapiro.test(male_student_weights$weight)\n#shapiro.test(female_student_weights$weight)\n\nyrbss_select_gender %&gt;%\n  gf_density( ~ weight,\n              fill = ~ gender,\n              alpha = 0.5,\n              title = \"Highschoolers' Weights by Gender\") %&gt;%\n  gf_facet_grid(~ gender) %&gt;% \n  gf_fitdistr(dist = \"dnorm\") %&gt;% \n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\n\n\n\nDistributions are not too close to normal…perhaps a hint of a rightward skew, suggesting that there are some obese students.\nWe can plot Q-Q plots3 for both variables, and also compare both data with normally-distributed data generated with the same means and standard deviations:\n\nyrbss_select_gender %&gt;% \n  gf_qq(~ weight | gender) %&gt;% \n  gf_qqline(ylab = \"scores\") %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\n\n\n\nNo real evidence (visually) of the variables being normally distributed.\nB.  Check for Variances\nLet us check if the two variables have similar variances: the var.test does this for us, with a NULL hypothesis that the variances are not significantly different:\n\nvar.test( weight ~  gender, data = yrbss_select_gender, \n          conf.int = TRUE,\n          conf.level = 0.95) %&gt;% \n  broom::tidy()\n\n#qf(0.975,6164, 6413)\n\n\n\n  \n\n\n\nThe p.value being so small, we are able to reject the NULL Hypothesis that the variances of weight are nearly equal across the two exercise regimes.\n\n\n\n\n\n\nConditions\n\n\n\n\nThe two variables are not normally distributed.\nThe two variances are also significantly different.\n\n\n\nThis means that the parametric t.test must be eschewed in favour of the non-parametric wilcox.test. We will use that, and also attempt linear models with rank data, and a final permutation test."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#iconify-academicons-hypothesis-hypothesis-1",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#iconify-academicons-hypothesis-hypothesis-1",
    "title": "Inference for Two Independent Means",
    "section": "\n Hypothesis",
    "text": "Hypothesis\nBased on the graphs, how would we formulate our Hypothesis? We wish to infer whether there is difference in mean weight across gender. So accordingly:\n\\[\nH_0: \\mu_{male} = \\mu_{female}\\\\\n\\\\\\\nH_a: \\mu_{male} \\ne \\mu_{female}\\\n\\]"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#iconify-academicons-hypothesis-observed-and-test-statistic-1",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#iconify-academicons-hypothesis-observed-and-test-statistic-1",
    "title": "Inference for Two Independent Means",
    "section": "\n Observed and Test Statistic",
    "text": "Observed and Test Statistic\nWhat would be the test statistic we would use? The difference in means. Is the observed difference in the means between the two groups of scores non-zero? We use the diffmean function, from mosaic:\n\nobs_diff_gender &lt;- diffmean(weight ~ gender, data = yrbss_select_gender) \n\nobs_diff_gender\n\n\n\ndiffmean \n11.70089"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#iconify-fluent-mdl2-insights-inference-1",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#iconify-fluent-mdl2-insights-inference-1",
    "title": "Inference for Two Independent Means",
    "section": "\n Inference",
    "text": "Inference\nType help(wilcox.test) in your Console.\n\nUsing the wilcox.test\nUsing the Linear Model\nUsing the Permutation Test\n\n\n\nSince the data variables do not satisfy the assumption of being normally distributed, and the variances are significantly different, we use the classical wilcox.test, which implements what we need here: the Mann-Whitney U test:4\n\nThe Mann-Whitney test as a test of mean ranks. It first ranks all your values from high to low, computes the mean rank in each group, and then computes the probability that random shuffling of those values between two groups would end up with the mean ranks as far apart as, or further apart, than you observed. No assumptions about distributions are needed so far. (emphasis mine)\n\nWe will use the mosaic variant).  Our model would be:\n\\[\nmean(rank(Weight_{male})) - mean(rank(Weight_{female})) =\n\\beta_0\n\\\\\\\nH_0: \\beta_0 = 0;\\\\\n\\\\\\\nH_a: \\beta_0 \\ne 0\n\\]\n\nwilcox.test(weight ~ gender, data = yrbss_select_gender, \n            conf.int = TRUE, \n            conf.level = 0.95) %&gt;% \n  broom::tidy()\n\n\n\n  \n\n\n\nThe p.value is negligible and we are able to reject the NULL hypothesis that the means are equal.\n\n\nWe can apply the linear-model-as-inference interpretation to the ranked data data to implement the non-parametric test as a Linear Model:\n\\[\nlm(rank(weight) \\sim  gender) = \\beta_0 + \\beta_1 * gender\n\\\\\nH_0: \\beta_1 = 0\\\\\n\\\\\\\nH_a: \\beta_1 \\ne 0\\\\\n\\]\n\n# Create a sign-rank function\n#signed_rank &lt;- function(x) {sign(x) * rank(abs(x))}\n\nlm(rank(weight) ~ gender, \n   data = yrbss_select_gender) %&gt;% \n  broom::tidy(conf.int = TRUE,\n              conf.level = 0.95)\n\n\n\n  \n\n\n\n\n\n\n\n\n\nDummy Variables in lm\n\n\n\nNote how the Qual variable was used here in Linear Regression! The gender variable was treated as a binary “dummy” variable5.\n\n\n\n\nWe saw from the diagram created by Allen Downey that there is only one test6! We will now use this philosophy to develop a technique that allows us to mechanize several Statistical Models in that way, with nearly identical code. For the specific data at hand, we need to shuffle the records between Semifinal and Final on a per Swimmer basis and take the test statistic (difference between the two swim records for each swimmer). Another way to look at this is to take the differences between Semifinal and Final scores and shuffle the differences to either polarity. We will follow this method in the code below:\n\nnull_dist_weight &lt;- \n  do(9999) * diffmean(data = yrbss_select_gender, weight ~ shuffle(gender))\nnull_dist_weight\ngf_histogram(data = null_dist_weight, ~ diffmean, bins = 25) %&gt;%\n  gf_vline(xintercept = obs_diff_gender, colour = \"red\") %&gt;% \n  gf_theme(theme_classic())\ngf_ecdf(data = null_dist_weight, ~ diffmean) %&gt;%\n  gf_vline(xintercept = obs_diff_gender, colour = \"red\")  %&gt;% \n  gf_theme(theme_classic())\nprop1(~ diffmean &lt;= obs_diff_gender, data = null_dist_weight)\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprop_TRUE \n        1 \n\n\n\n\nClearly the observed_diff_weight is much beyond anything we can generate with permutations with gender! And hence there is a significant difference in weights across gender!\n\n\n\nAll Tests Together\nWe can put all the test results together to get a few more insights about the tests:\n\nwilcox.test(weight ~ gender, data = yrbss_select_gender, \n            conf.int = TRUE, \n            conf.level = 0.95) %&gt;% \n  broom::tidy() %&gt;% \n  gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"cyan\"), cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)) %&gt;% \n  tab_header(title = \"wilcox.test\")\n\nlm(rank(weight) ~ gender, \n   data = yrbss_select_gender) %&gt;% \n  broom::tidy(conf.int = TRUE,\n              conf.level = 0.95) %&gt;% \n  gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"cyan\"),cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)) %&gt;% \n  tab_header(title = \"Linear Model with Ranked Data\")\n\n\n\n\n\n\n\nwilcox.test\n    \n\nestimate\n      statistic\n      p.value\n      conf.low\n      conf.high\n      method\n      alternative\n    \n\n\n-11.33999\n10808213\n0\n-11.34003\n-10.87994\nWilcoxon rank sum test with continuity correction\ntwo.sided\n\n\n\n\n\n\n\n\nLinear Model with Ranked Data\n    \n\nterm\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n\n\n\n(Intercept)\n4836.157\n42.52745\n113.71848\n0\n4752.797\n4919.517\n\n\ngendermale\n2851.246\n59.55633\n47.87478\n0\n2734.507\n2967.986\n\n\n\n\n\n\nThe wilcox.test and the linear model with rank data offer the same results. This is of course not surprising!"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#iconify-carbon-chart-3d-inspecting-and-charting-data-3",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#iconify-carbon-chart-3d-inspecting-and-charting-data-3",
    "title": "Inference for Two Independent Means",
    "section": "\n Inspecting and Charting Data",
    "text": "Inspecting and Charting Data\nWe can make distribution plots for weight by physical_3plus:\n\ngf_boxplot(weight ~ physical_3plus, \n          fill = ~ physical_3plus,\n          data = yrbss_select_phy, xlab = \"Days of Exercise &gt;=3 \") %&gt;% \n  gf_theme(theme_classic())\n\n\n\n\n\n\ngf_density(~ weight,\n          fill = ~ physical_3plus,\n          data = yrbss_select_phy) %&gt;% \n  gf_theme(theme_classic())\n\n\n\n\n\n\n\nThe box plots show how the medians of the two distributions compare, but we can also compare the means of the distributions using the following to first group the data by the physical_3plus variable, and then calculate the mean weight in these groups using the mean function while ignoring missing values by setting the na.rm argument to TRUE.\n\nyrbss_select_phy %&gt;%\n  group_by(physical_3plus) %&gt;%\n  summarise(mean_weight = mean(weight, na.rm = TRUE))\n\n\n\n  \n\n\n\nThere is an observed difference, but is this difference large enough to deem it “statistically significant”? In order to answer this question we will conduct a hypothesis test. But before that a few more checks on the data:\nA.  Check for Normality\nAs stated before, statistical tests for means usually require a couple of checks:\n\nAre the data normally distributed?\n\nAre the data variances similar?\n\nLet us also complete a visual check for normality,with plots since we cannot do a shapiro.test:\n\nyrbss_select_phy %&gt;%\n  gf_density( ~ weight,\n              fill = ~ physical_3plus,\n              alpha = 0.5,\n              title = \"Highschoolers' Weights by Exercise Frequency\") %&gt;%\n  gf_facet_grid(~ physical_3plus) %&gt;% \n  gf_fitdistr(dist = \"dnorm\") %&gt;% \n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\n\n\n\nAgain, not normally distributed…\nWe can plot Q-Q plots for both variables, and also compare both data with normally-distributed data generated with the same means and standard deviations:\n\nyrbss_select_phy %&gt;% \n  gf_qq(~ weight | physical_3plus , color = ~ physical_3plus) %&gt;% \n  gf_qqline(ylab = \"Weight\") %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\n\n\n\nThe QQ-plots confirm that he tow data variables are not normally distributed.\nB.  Check for Variances\nLet us check if the two variables have similar variances: the var.test does this for us, with a NULL hypothesis that the variances are not significantly different:\n\nvar.test( weight ~ physical_3plus, data = yrbss_select_phy, \n          conf.int = TRUE,\n          conf.level = 0.95) %&gt;% \n  broom::tidy()\n\n# Critical F value\nqf(0.975,4021, 8341)\n\n\n\n\n\n  \n\n\n\n[1] 1.054398\n\n\n\n\nThe p.value states the probability of the data being what it is, assuming the NULL hypothesis that variances were similar. It being so small, we are able to reject this NULL Hypothesis that the variances of weight are nearly equal across the two exercise frequencies. (Compare the statistic in the var.test with the critical F-value)\n\n\n\n\n\n\nConditions\n\n\n\n\nThe two variables are not normally distributed.\nThe two variances are also significantly different.\n\n\n\nHence we will have to use non-parametric tests to infer if the means are similar."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#iconify-academicons-hypothesis-hypothesis-2",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#iconify-academicons-hypothesis-hypothesis-2",
    "title": "Inference for Two Independent Means",
    "section": "\n Hypothesis",
    "text": "Hypothesis\nBased on the graphs, how would we formulate our Hypothesis? We wish to infer whether there is difference in mean weight across physical_3plus. So accordingly:\n\\[\nH_0: \\mu_{physical-3plus-Yes} = \\mu_{physical-3plus-No}\\\\\n\\\\\\\nH_a: \\mu_{physical-3plus-Yes} \\ne \\mu_{physical-3plus-No}\\\\\n\\]"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#iconify-academicons-hypothesis-observed-and-test",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#iconify-academicons-hypothesis-observed-and-test",
    "title": "Inference for Two Independent Means",
    "section": "\n Observed and Test",
    "text": "Observed and Test\nStatistic\nWhat would be the test statistic we would use? The difference in means. Is the observed difference in the means between the two groups of scores non-zero? We use the diffmean function, from mosaic:\n\nobs_diff_phy &lt;- diffmean(weight ~ physical_3plus, data = yrbss_select_phy) \n\nobs_diff_phy\n\n\n\n diffmean \n-1.774584 \n\n\n\n\n\n Inference\n::: panel-tabset\nUsing parametric t.test\n\nWell, the variables are not normally distributed, and the variances are significantly different so a standard t.test is not advised. We can still try:\n\nmosaic::t_test(weight ~ physical_3plus,\n               var.equal = FALSE, # Welch Correction\n               data = yrbss_select_phy) %&gt;% \n  broom::tidy()\n\n\n\n  \n\n\n\nThe p.value is \\(8.9e-08\\) ! And the Confidence Interval is clear of \\(0\\). So the t.test gives us good reason to reject the Null Hypothesis that the means are similar. But can we really believe this, given the non-normality of data?\nUsing non-parametric paired Wilcoxon test\nHowever, we have seen that the data variables are not normally distributed. So a Wilcoxon Test, using signed-ranks, is indicated: (recall the model!)\n\n# For stability reasons, it may be advisable to use rounded data or to set digits.rank = 7, say, \n# such that determination of ties does not depend on very small numeric differences (see the example).\n\nwilcox.test(weight ~ physical_3plus,\n            conf.int = TRUE,\n            conf.level = 0.95,\n            data = yrbss_select_phy) %&gt;% \n  broom::tidy()\n\n\n\n  \n\n\n\nThe nonparametric wilcox.test also suggests that the means for weight across physical_3plus are significantly different.\nUsing the Linear Model Interpretation\nWe can apply the linear-model-as-inference interpretation to the ranked data data to implement the non-parametric test as a Linear Model:\n\\[\nlm(rank(weight) \\sim  physical.3plus) = \\beta_0 + \\beta_1 \\times physical.3plus\n\\\\\nH_0: \\beta_1 = 0\\\\\n\\\\\\\nH_a: \\beta_1 \\ne 0\\\\\n\\]\n\nlm(rank(weight) ~ physical_3plus, \n   data = yrbss_select_phy) %&gt;% \n  broom::tidy(conf.int = TRUE,\n              conf.level = 0.95)\n\n\n\n  \n\n\n\nHere too, the linear model using rank data arrives at a conclusion similar to that of the Mann-Whitney U test.\nUsing Permutation Tests\nWe will do this in two ways, just for fun: one using mosaic and the other using infer.\nBut first, we need to initialize the test, which we will save as obs_diff.\n\nobs_diff_infer &lt;- yrbss_select_phy %&gt;%\n  infer::specify(weight ~ physical_3plus) %&gt;%\n  infer::calculate(stat = \"diff in means\", order = c(\"yes\", \"no\"))\nobs_diff_infer\nobs_diff_mosaic &lt;- mosaic::diffmean(~ weight | physical_3plus, data = yrbss_select_phy)\nobs_diff_mosaic\nobs_diff_phy\n\n\n\n\n\n  \n\n\n\n diffmean \n-1.774584 \n\n\n diffmean \n-1.774584 \n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that obs_diff_infer is a 1 X 1 dataframe; obs_diff_mosaic is a scalar!!\n\n\n\nInference Using mosaic\n\n\nWe already have the observed difference, obs_diff_mosaic. Now we generate the null distribution using permutation, with mosaic:\n\nnull_dist_mosaic &lt;- do(999) * diffmean(~ weight | shuffle(physical_3plus), data = yrbss_select_phy)\n\nWe can also generate the histogram of the null distribution, compare that with the observed diffrence and compute the p-value and confidence intervals:\n\ngf_histogram(~ diffmean, data = null_dist_mosaic) %&gt;% \n  gf_vline(xintercept = obs_diff_mosaic, colour = \"red\")\n\n\n\n\n\n\n# p-value\nprop(~ diffmean != obs_diff_mosaic, data = null_dist_mosaic)\n\nprop_TRUE \n        1 \n\n# Confidence Intervals for p = 0.95\nmosaic::cdata(~ diffmean, p = 0.95, data = null_dist_mosaic)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#your-turn",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#your-turn",
    "title": "Inference for Two Independent Means",
    "section": "Your Turn",
    "text": "Your Turn\n\nCalculate a 95% confidence interval for the average height in meters (height) and interpret it in context.\nCalculate a new confidence interval for the same parameter at the 90% confidence level. Comment on the width of this interval versus the one obtained in the previous exercise.\nConduct a hypothesis test evaluating whether the average height is different for those who exercise at least three times a week and those who don’t.\nNow, a non-inference task: Determine the number of different options there are in the dataset for the hours_tv_per_school_day there are.\nCome up with a research question evaluating the relationship between height or weight and sleep. Formulate the question in a way that it can be answered using a hypothesis test and/or a confidence interval. Report the statistical results, and also provide an explanation in plain language. Be sure to check all assumptions, state your \\(\\alpha\\) level, and conclude in context."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#footnotes",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#footnotes",
    "title": "Inference for Two Independent Means",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://stats.stackexchange.com/questions/2492/is-normality-testing-essentially-useless↩︎\nhttps://www.allendowney.com/blog/2023/01/28/never-test-for-normality/↩︎\nhttps://stats.stackexchange.com/questions/92374/testing-large-dataset-for-normality-how-and-is-it-reliable↩︎\nhttps://stats.stackexchange.com/q/113337↩︎\nhttps://en.wikipedia.org/wiki/Dummy_variable_(statistics)↩︎\nhttps://allendowney.blogspot.com/2016/06/there-is-still-only-one-test.html↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html",
    "title": "Inference for Two Independent Means",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic) # Our go-to package\nlibrary(ggformula)\nlibrary(infer) # An alternative package for inference using tidy data\nlibrary(broom) # Clean test results in tibble form\nlibrary(skimr) # data inspection\n\nlibrary(resampledata3) # Datasets from Chihara and Hesterberg's book\nlibrary(openintro) # datasets\nlibrary(gt) # for tables\n\nggplot2::theme_set(theme_classic())\n\n\n\nflowchart TD\n    A[Inference for Independent Means] --&gt;|Check Assumptions| B[Normality: Shapiro-Wilk Test shapiro.test\\n Variances: Fisher F-test var.test]\n    B --&gt; C{OK?}\n    C --&gt;|Yes, both\\n Parametric| D[t.test]\n    D &lt;--&gt;F[Linear Model\\n Method] \n    C --&gt;|Yes, but not variance\\n Parametric| W[t.test with\\n Welch Correction]\n    W&lt;--&gt;F\n    C --&gt;|No\\n Non-Parametric| E[wilcox.test]\n    E &lt;--&gt; G[Linear Model\\n with\\n Ranked Data]\n    C --&gt;|No\\n Non-Parametric| P[Bootstrap\\n or\\n Permutation]\n    P &lt;--&gt; Q[Linear Model\\n with Ranked Data\\n and Permutation]\n\n\n\nflowchart TD\n    A[Inference for Independent Means] --&gt;|Check Assumptions| B[Normality: Shapiro-Wilk Test shapiro.test\\n Variances: Fisher F-test var.test]\n    B --&gt; C{OK?}\n    C --&gt;|Yes, both\\n Parametric| D[t.test]\n    D &lt;--&gt;F[Linear Model\\n Method] \n    C --&gt;|Yes, but not variance\\n Parametric| W[t.test with\\n Welch Correction]\n    W&lt;--&gt;F\n    C --&gt;|No\\n Non-Parametric| E[wilcox.test]\n    E &lt;--&gt; G[Linear Model\\n with\\n Ranked Data]\n    C --&gt;|No\\n Non-Parametric| P[Bootstrap\\n or\\n Permutation]\n    P &lt;--&gt; Q[Linear Model\\n with Ranked Data\\n and Permutation]"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "title": "Inference for Two Independent Means",
    "section": "",
    "text": "flowchart TD\n    A[Inference for Independent Means] --&gt;|Check Assumptions| B[Normality: Shapiro-Wilk Test shapiro.test\\n Variances: Fisher F-test var.test]\n    B --&gt; C{OK?}\n    C --&gt;|Yes, both\\n Parametric| D[t.test]\n    D &lt;--&gt;F[Linear Model\\n Method] \n    C --&gt;|Yes, but not variance\\n Parametric| W[t.test with\\n Welch Correction]\n    W&lt;--&gt;F\n    C --&gt;|No\\n Non-Parametric| E[wilcox.test]\n    E &lt;--&gt; G[Linear Model\\n with\\n Ranked Data]\n    C --&gt;|No\\n Non-Parametric| P[Bootstrap\\n or\\n Permutation]\n    P &lt;--&gt; Q[Linear Model\\n with Ranked Data\\n and Permutation]\n\n\n\nflowchart TD\n    A[Inference for Independent Means] --&gt;|Check Assumptions| B[Normality: Shapiro-Wilk Test shapiro.test\\n Variances: Fisher F-test var.test]\n    B --&gt; C{OK?}\n    C --&gt;|Yes, both\\n Parametric| D[t.test]\n    D &lt;--&gt;F[Linear Model\\n Method] \n    C --&gt;|Yes, but not variance\\n Parametric| W[t.test with\\n Welch Correction]\n    W&lt;--&gt;F\n    C --&gt;|No\\n Non-Parametric| E[wilcox.test]\n    E &lt;--&gt; G[Linear Model\\n with\\n Ranked Data]\n    C --&gt;|No\\n Non-Parametric| P[Bootstrap\\n or\\n Permutation]\n    P &lt;--&gt; Q[Linear Model\\n with Ranked Data\\n and Permutation]"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#iconify-carbon-chart-3d-inspecting-and-charting-data",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#iconify-carbon-chart-3d-inspecting-and-charting-data",
    "title": "Inference for Two Independent Means",
    "section": "\n Inspecting and Charting Data",
    "text": "Inspecting and Charting Data\n\ndata(\"MathAnxiety\")\nMathAnxiety\nMathAnxiety_inspect &lt;- inspect(MathAnxiety)\nMathAnxiety_inspect$categorical\nMathAnxiety_inspect$quantitative\n\n\n\n  \n\n\n  \n\n\n  \n\n\n\nWe have ~600 data entries, and with 4 Quant variables; Age,AMAS, RCMAS, and AMAS; and two Qual variables, Gender and Grade. A simple dataset, with enough entries to make it worthwhile to explore as our first example.\n\n\n\n\n\n\nResearch Question\n\n\n\nIs there a difference between boy and girl “anxiety” levels for AMAS (test) in the MathAnxiety dataset?\n\n\nFirst, histograms, densities and counts of the variable we are interested in, after converting data into long format:\n\nMathAnxiety %&gt;% \n  gf_density(~ AMAS,\n              fill = ~ Gender,\n              alpha = 0.5,\n              title = \"\") %&gt;%\n  gf_theme(theme_classic())\nMathAnxiety %&gt;% \n  gf_boxplot(AMAS ~ Gender,\n              fill = ~ Gender,\n              alpha = 0.5,\n              title = \"\") %&gt;%\n  gf_theme(theme_classic())\nMathAnxiety %&gt;% count(Gender)\nMathAnxiety %&gt;% group_by(Gender) %&gt;% summarise(mean = mean(AMAS))\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n\n\n\n\nThe distributions for anxiety scores for boys and girls overlap considerably and are very similar, though the boxplot for boys shows a significant outlier. Are they close to being normal distributions too? We should check.\nA.  Check for Normality\nStatistical tests for means usually require a couple of checks1 2:\n\nAre the data normally distributed?\n\nAre the data variances similar?\n\nLet us complete a check for normality: the shapiro.wilk test checks whether a Quant variable is from a normal distribution; the NULL hypothesis is that the data are from a normal distribution. We will also look at Q-Q plots for both variables:\n\nMathAnxiety %&gt;%\n  gf_density( ~ AMAS,\n              fill = ~ Gender,\n              alpha = 0.5,\n              title = \"Math Anxiety scores for boys and girls\") %&gt;%\n  gf_facet_grid(~ Gender) %&gt;% \n  gf_fitdistr(dist = \"dnorm\") \nMathAnxiety%&gt;% \n  gf_qq(~ AMAS, color = ~ Gender) %&gt;% \n  gf_qqline(ylab = \"Math Anxiety Score..are they Normally Disributed?\") %&gt;%\n  gf_facet_wrap(~ Gender)  # independent y-axis\nboys_AMAS &lt;- MathAnxiety %&gt;% filter(Gender== \"Boy\") %&gt;% select(AMAS)\ngirls_AMAS &lt;- MathAnxiety %&gt;% filter(Gender== \"Girl\") %&gt;% select(AMAS)\n\nshapiro.test(boys_AMAS$AMAS)\nshapiro.test(girls_AMAS$AMAS)\n\n\n\n\n\n\n\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  boys_AMAS$AMAS\nW = 0.99043, p-value = 0.03343\n\n\n    Shapiro-Wilk normality test\n\ndata:  girls_AMAS$AMAS\nW = 0.99074, p-value = 0.07835\n\n\n\n\nThe distributions for anxiety scores for boys and girls are almost normal, visually speaking. With the Shapiro-Wilk test we find that the scores for girls are normally distributed, but the boys scores are not so.\n\n\n\n\n\n\nNote\n\n\n\nThe p.value obtained in the shapiro.wilk test suggests the chances of the data, given the Assumption that they are normally distributed.\n\n\nWe see that MathAnxiety contains discrete-level scores for anxiety for the two variables (for Boys and Girls) anxiety scores. The boys score has a significant outlier, which we saw earlier and perhaps that makes that variable lose out, perhaps narrowly.\nB.  Check for Variances\nLet us check if the two variables have similar variances: the var.test does this for us, with a NULL hypothesis that the variances are not significantly different:\n\nvar.test(AMAS ~ Gender, data = MathAnxiety, \n         conf.int = TRUE,conf.level = 0.95) %&gt;% \n  broom::tidy()\nqf(0.975,275,322)\n\n\n\n\n\n  \n\n\n\n[1] 1.254823\n\n\n\n\nThe variances are quite similar as seen by the \\(p.value = 0.82\\). We also saw it visually when we plotted the overlapped distributions earlier.\n\n\n\n\n\n\nConditions:\n\n\n\n\nThe two variables are not both normally distributed.\nThe two variances are significantly similar."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#iconify-academicons-hypothesis-hypothesis",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#iconify-academicons-hypothesis-hypothesis",
    "title": "Inference for Two Independent Means",
    "section": "\n Hypothesis",
    "text": "Hypothesis\nBased on the graphs, how would we formulate our Hypothesis? We wish to infer whether there is any difference in the mean anxiety score between Girls and Boys in the dataset MathAnxiety. So accordingly:\n\\[\nH_0: \\mu_{Boys} = \\mu_{Girls}\\\\\n\\] \\[\nH_a: \\mu_{Girls} \\ne \\mu_{Boys}\\\\\n\\]"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#iconify-academicons-hypothesis-observed-and-test-statistic",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#iconify-academicons-hypothesis-observed-and-test-statistic",
    "title": "Inference for Two Independent Means",
    "section": "\n Observed and Test Statistic",
    "text": "Observed and Test Statistic\nWhat would be the test statistic we would use? The difference in means. Is the observed difference in the means between the two groups of scores non-zero? We use the diffmean function:\n\nobs_diff_amas &lt;- diffmean(AMAS ~ Gender, data = MathAnxiety) \nobs_diff_amas\n\ndiffmean \n  1.7676 \n\n\nGirls’ AMAS anxiety scores are, on average, \\(1.76\\) points higher than those for Boys.\n\n\n\n\n\n\nOn Observed Difference Estimates\n\n\n\nDifferent tests here will show the difference as positive or negative, but with the same value! This depends upon the way the factor variable Gender is used, i.e. Boy-Girl or Girl-Boy…"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#iconify-fluent-mdl2-insights-inference",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#iconify-fluent-mdl2-insights-inference",
    "title": "Inference for Two Independent Means",
    "section": "\n Inference",
    "text": "Inference\nType help(wilcox.test) in your Console.\n\n\n\nUsing the Parametric t.test\nUsing the Mann-Whitney Test\nUsing the Linear Model Interpretation\nUsing the Permutation Test\n\n\n\nSince the data are not both normally distributed, though the variances similar, we typically cannot use a parametric t.test. However, we can still examine the results:\n\nmosaic::t_test(AMAS ~ Gender, \n               data = MathAnxiety) %&gt;% \n  broom::tidy()\n\n\n\n  \n\n\n\nThe p.value is \\(0.001\\) ! And the Confidence Interval does not straddle \\(0\\). So the t.test gives us good reason to reject the Null Hypothesis that the means are similar and that there is a significant difference between Boys and Girls when it comes to AMAS anxiety. But can we really believe this, given the non-normality of data?\n\n\nSince the data variables do not satisfy the assumption of being normally distributed, and though the variances are similar, we use the classical wilcox.test, which implements what we need here: the Mann-Whitney U test:3\n\nThe Mann-Whitney test as a test of mean ranks. It first ranks all your values from high to low, computes the mean rank in each group, and then computes the probability that random shuffling of those values between two groups would end up with the mean ranks as far apart as, or further apart, than you observed. No assumptions about distributions are needed so far. (emphasis mine)\n\n\\[\nmean(rank(AMAS_{Girls})) - mean(rank(AMAS_{Boys})) = \\beta_1;\\\\\n\\] \\[\nH_0: \\beta_1 = 0;\\\\\n\\\\\n\\] \\[\nH_a: \\beta_1 \\ne 0\n\\]\n\nwilcox.test(AMAS ~ Gender, data = MathAnxiety, \n            conf.int = TRUE, \n            conf.level = 0.95) %&gt;% \n  broom::tidy()\n\n\n\n  \n\n\n\nThe p.value is very similar, \\(0.00077\\), and again the Confidence Interval does not straddle \\(0\\), and we are hence able to reject the NULL hypothesis that the means are equal and accept the alternative hypothesis that there is a significant difference in mean anxiety scores between Boys and Girls.\n\n\nWe can apply the linear-model-as-inference interpretation to the ranked data data to implement the non-parametric test as a Linear Model:\n\\[\nlm(rank(AMAS) \\sim  gender) = \\beta_0 + \\beta_1 * gender\n\\] \\[\nH_0: \\beta_1 = 0\\\\\n\\] \\[\nH_a: \\beta_1 \\ne 0\\\\\n\\]\n\n\n\n\nlm(rank(AMAS) ~ Gender, \n   data = MathAnxiety) %&gt;% \n  broom::tidy(conf.int = TRUE,\n              conf.level = 0.95)\n\n\n\n  \n\n\n\n\n\n\n\n\n\nDummy Variables in lm\n\n\n\nNote how the Qual variable was used here in Linear Regression! The Gender variable was treated as a binary “dummy” variable4.\n\n\nHere too we see that the p.value for the slope term (“GenderGirl”) is significant at \\(7.4*10^{-4}\\).\n\n\nWe saw from the diagram created by Allen Downey that there is only one test5! We will now use this philosophy to develop a technique that allows us to mechanize several Statistical Models in that way, with nearly identical code.\nWe pretend that Gender has not effect on the AMAS anxiety scores. If this is our position, then the Gender labels are essentially meaningless, and we can pretend that any AMAS score belongs to a Boy or a Girl. This means we can mosaic::shuffle (permute) the Gender labels and see how uncommon our real data is. And we do not have to really worry about whether the data are normally distributed, or if their variances are nearly equal.\n\n\n\n\n\n\nImportant\n\n\n\nThe “pretend” position is exactly the NULL Hypothesis!! The “uncommon” part is the p.value under NULL!!\n\n\n\nnull_dist_amas &lt;- \n  do(9999) * diffmean(data = MathAnxiety, AMAS ~ shuffle(Gender))\nnull_dist_amas\ngf_histogram(data = null_dist_amas, ~ diffmean, bins = 25) %&gt;%\n  gf_vline(xintercept = obs_diff_amas, colour = \"red\") \ngf_ecdf(data = null_dist_amas, ~ diffmean) %&gt;%\n  gf_vline(xintercept = obs_diff_amas, colour = \"red\") \n1-prop1(~ diffmean &lt;= obs_diff_amas, data = null_dist_amas)\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\nprop_TRUE \n    7e-04 \n\n\n\n\nClearly the observed_diff_amas is much beyond anything we can generate with permutations with gender! And hence there is a significant difference in weights across gender!\n\n\n\nAll Tests Together\nWe can put all the test results together to get a few more insights about the tests:\n\nmosaic::t_test(AMAS ~ Gender, \n               data = MathAnxiety) %&gt;% \n  broom::tidy() %&gt;% \n  gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"cyan\"), cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)) %&gt;% \n  tab_header(title = \"t.test\")\n\nwilcox.test(AMAS ~ Gender, \n               data = MathAnxiety) %&gt;% \n  broom::tidy() %&gt;% \n  gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"cyan\"), cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)) %&gt;% \n  tab_header(title = \"wilcox.test\")\n\nlm(AMAS ~ Gender, \n   data = MathAnxiety) %&gt;% \n  broom::tidy(conf.int = TRUE,\n              conf.level = 0.95) %&gt;% \n  gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"cyan\"),cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)) %&gt;% \n  tab_header(title = \"Linear Model with Original Data\")\n\nlm(rank(AMAS) ~ Gender, \n   data = MathAnxiety) %&gt;% \n  broom::tidy(conf.int = TRUE,\n              conf.level = 0.95) %&gt;% \n  gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"cyan\"),cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)) %&gt;% \n  tab_header(title = \"Linear Model with Ranked Data\")\n\n\n\n\n\n\n\nt.test\n    \n\nestimate\n      estimate1\n      estimate2\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n\n\n-1.7676\n21.16718\n22.93478\n-3.291843\n0.001055808\n580.2004\n-2.822229\n-0.7129706\nWelch Two Sample t-test\ntwo.sided\n\n\n\n\n\n\n\n\nwilcox.test\n    \n\nstatistic\n      p.value\n      method\n      alternative\n    \n\n\n37483\n0.0007736219\nWilcoxon rank sum test with continuity correction\ntwo.sided\n\n\n\n\n\n\n\n\nLinear Model with Original Data\n    \n\nterm\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n\n\n\n(Intercept)\n21.16718\n0.3641315\n58.130602\n5.459145e-248\n20.4520482\n21.882317\n\n\nGenderGirl\n1.76760\n0.5364350\n3.295087\n1.042201e-03\n0.7140708\n2.821129\n\n\n\n\n\n\n\n\n\nLinear Model with Ranked Data\n    \n\nterm\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n\n\n\n(Intercept)\n278.04644\n9.535561\n29.158898\n6.848992e-117\n259.31912\n296.7738\n\n\nGenderGirl\n47.64559\n14.047696\n3.391701\n7.405210e-04\n20.05668\n75.2345\n\n\n\n\n\n\nAs we can see, all tests are in agreement that there is a significant effect of Gender on the AMAS anxiety scores!"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#iconify-carbon-chart-3d-inspecting-and-charting-data-1",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#iconify-carbon-chart-3d-inspecting-and-charting-data-1",
    "title": "Inference for Two Independent Means",
    "section": "\n Inspecting and Charting Data",
    "text": "Inspecting and Charting Data\n\ndata(yrbss)\nyrbss\nyrbss_inspect &lt;- inspect(yrbss)\nyrbss_inspect$categorical\nyrbss_inspect$quantitative\n\n\n\n  \n\n\n  \n\n\n  \n\n\n\nWe have 13K data entries, and with 13 different variables, some Qual and some Quant. Many entries are missing too, typical of real-world data and something we will have to account for in our computations. The meaning of each variable can be found by bringing up the help file.Type this in your console: help(yrbss)\nIn this Case Study, our research question is:\n\n\n\n\n\n\nResearch Question\n\n\n\nDoes weight of highschoolers in this dataset vary with gender?"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#iconify-carbon-chart-3d-inspecting-and-charting-data-2",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#iconify-carbon-chart-3d-inspecting-and-charting-data-2",
    "title": "Inference for Two Independent Means",
    "section": "\n Inspecting and Charting Data",
    "text": "Inspecting and Charting Data\nFirst, histograms, densities and counts of the variable we are interested in:\n\nyrbss_select_gender &lt;- yrbss %&gt;% \n  select(weight, gender) %&gt;% \n  drop_na(weight) # Sadly dropping off NA data\n\nyrbss_select_gender %&gt;%\n  gf_density( ~ weight,\n              fill = ~ gender,\n              alpha = 0.5,\n              title = \"Highschoolers' Weights by Gender\") %&gt;%\n  gf_theme(theme_classic())\nyrbss_select_gender %&gt;%\n  gf_boxplot(weight ~ gender,\n              fill = ~ gender,\n              alpha = 0.5,\n              title = \"Highschoolers' Weights by Gender\") %&gt;%\n  gf_theme(theme_classic())\nyrbss_select_gender %&gt;% count(gender)\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nOverlapped Distribution plot shows some difference in the means; and the Boxplots show visible difference in the medians.\nA.  Check for Normality\nAs stated before, statistical tests for means usually require a couple of checks:\n\nAre the data normally distributed?\n\nAre the data variances similar?\n\nLet us also complete a visual check for normality, with plots since we cannot do a shapiro.test:\n\n\n\n\n\n\nShapiro-Wilks Test\n\n\n\nThe longest data it can take (in R) is 5000. Since our data is longer, we will cannot use this procedure and have to resort to visual means.\n\n\nLet us plot frequency distribution and Q-Q plots6 for both variables.\n\nmale_student_weights &lt;- yrbss_select_gender %&gt;% \n  filter(gender == \"male\") %&gt;% \n  select(weight)\nfemale_student_weights &lt;- yrbss_select_gender %&gt;% \n  filter(gender == \"female\") %&gt;% \n  select(weight)\n#shapiro.test(male_student_weights$weight)\n#shapiro.test(female_student_weights$weight)\n\nyrbss_select_gender %&gt;%\n  gf_density( ~ weight,\n              fill = ~ gender,\n              alpha = 0.5,\n              title = \"Highschoolers' Weights by Gender\") %&gt;%\n  gf_facet_grid(~ gender) %&gt;% \n  gf_fitdistr(dist = \"dnorm\") %&gt;% \n  gf_theme(theme_classic())\nyrbss_select_gender %&gt;% \n  gf_qq(~ weight | gender) %&gt;% \n  gf_qqline(ylab = \"scores\") %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\n\n\n\nDistributions are not too close to normal…perhaps a hint of a rightward skew, suggesting that there are some obese students.\nNo real evidence (visually) of the variables being normally distributed.\nB.  Check for Variances\nLet us check if the two variables have similar variances: the var.test does this for us, with a NULL hypothesis that the variances are not significantly different:\n\nvar.test( weight ~  gender, data = yrbss_select_gender, \n          conf.int = TRUE,\n          conf.level = 0.95) %&gt;% \n  broom::tidy()\n\n#qf(0.975,6164, 6413)\n\n\n\n  \n\n\n\nThe p.value being so small, we are able to reject the NULL Hypothesis that the variances of weight are nearly equal across the two exercise regimes.\n\n\n\n\n\n\nConditions\n\n\n\n\nThe two variables are not normally distributed.\nThe two variances are also significantly different.\n\n\n\nThis means that the parametric t.test must be eschewed in favour of the non-parametric wilcox.test. We will use that, and also attempt linear models with rank data, and a final permutation test."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#iconify-academicons-hypothesis-hypothesis-1",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#iconify-academicons-hypothesis-hypothesis-1",
    "title": "Inference for Two Independent Means",
    "section": "\n Hypothesis",
    "text": "Hypothesis\nBased on the graphs, how would we formulate our Hypothesis? We wish to infer whether there is difference in mean weight across gender. So accordingly:\n\\[\nH_0: \\mu_{male} = \\mu_{female}\\\\\n\\\\\\\nH_a: \\mu_{male} \\ne \\mu_{female}\\\n\\]"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#iconify-academicons-hypothesis-observed-and-test-statistic-1",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#iconify-academicons-hypothesis-observed-and-test-statistic-1",
    "title": "Inference for Two Independent Means",
    "section": "\n Observed and Test Statistic",
    "text": "Observed and Test Statistic\nWhat would be the test statistic we would use? The difference in means. Is the observed difference in the means between the two groups of scores non-zero? We use the diffmean function, from mosaic:\n\nobs_diff_gender &lt;- diffmean(weight ~ gender, data = yrbss_select_gender) \n\nobs_diff_gender\n\n\n\ndiffmean \n11.70089"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#iconify-fluent-mdl2-insights-inference-1",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#iconify-fluent-mdl2-insights-inference-1",
    "title": "Inference for Two Independent Means",
    "section": "\n Inference",
    "text": "Inference\nType help(wilcox.test) in your Console.\n\nUsing the Mann-Whitney test\nUsing the Linear Model\nUsing the Permutation Test\n\n\n\nSince the data variables do not satisfy the assumption of being normally distributed, and the variances are significantly different, we use the classical wilcox.test, which implements what we need here: the Mann-Whitney U test:7\n\nThe Mann-Whitney test as a test of mean ranks. It first ranks all your values from high to low, computes the mean rank in each group, and then computes the probability that random shuffling of those values between two groups would end up with the mean ranks as far apart as, or further apart, than you observed. No assumptions about distributions are needed so far. (emphasis mine)\n\nOur model would be:\n\\[\nmean(rank(Weight_{male})) - mean(rank(Weight_{female})) = \\beta_1;\\\\\nH_0: \\beta_1 = 0;\\\\\n\\\\\nH_a: \\beta_1 \\ne 0\n\\]\nRecall the earlier graph showing ranks of scores against Gender.\n\nwilcox.test(weight ~ gender, data = yrbss_select_gender, \n            conf.int = TRUE, \n            conf.level = 0.95) %&gt;% \n  broom::tidy()\n\n\n\n  \n\n\n\nThe p.value is negligible and we are able to reject the NULL hypothesis that the means are equal.\n\n\nWe can apply the linear-model-as-inference interpretation to the ranked data data to implement the non-parametric test as a Linear Model:\n\\[\nlm(rank(weight) \\sim  gender) = \\beta_0 + \\beta_1 * gender\n\\\\\nH_0: \\beta_1 = 0\\\\\n\\\\\\\nH_a: \\beta_1 \\ne 0\\\\\n\\]\n\n# Create a sign-rank function\n#signed_rank &lt;- function(x) {sign(x) * rank(abs(x))}\n\nlm(rank(weight) ~ gender, \n   data = yrbss_select_gender) %&gt;% \n  broom::tidy(conf.int = TRUE,\n              conf.level = 0.95)\n\n\n\n  \n\n\n\n\n\n\n\n\n\nDummy Variables in lm\n\n\n\nNote how the Qual variable was used here in Linear Regression! The gender variable was treated as a binary “dummy” variable8.\n\n\n\n\nFor the specific data at hand, we need to shuffle the records between Semifinal and Final on a per Swimmer basis and take the test statistic (difference between the two swim records for each swimmer). Another way to look at this is to take the differences between Semifinal and Final scores and shuffle the differences to either polarity. We will follow this method in the code below:\n\nnull_dist_weight &lt;- \n  do(9999) * diffmean(data = yrbss_select_gender, weight ~ shuffle(gender))\nnull_dist_weight\ngf_histogram(data = null_dist_weight, ~ diffmean, bins = 25) %&gt;%\n  gf_vline(xintercept = obs_diff_gender, colour = \"red\") %&gt;% \n  gf_theme(theme_classic())\ngf_ecdf(data = null_dist_weight, ~ diffmean) %&gt;%\n  gf_vline(xintercept = obs_diff_gender, colour = \"red\")  %&gt;% \n  gf_theme(theme_classic())\nprop1(~ diffmean &lt;= obs_diff_gender, data = null_dist_weight)\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\nprop_TRUE \n        1 \n\n\n\n\nClearly the observed_diff_weight is much beyond anything we can generate with permutations with gender! And hence there is a significant difference in weights across gender!\n\n\n\nAll Tests Together\nWe can put all the test results together to get a few more insights about the tests:\n\nwilcox.test(weight ~ gender, data = yrbss_select_gender, \n            conf.int = TRUE, \n            conf.level = 0.95) %&gt;% \n  broom::tidy() %&gt;% \n  gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"cyan\"), cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)) %&gt;% \n  tab_header(title = \"wilcox.test\")\n\nlm(rank(weight) ~ gender, \n   data = yrbss_select_gender) %&gt;% \n  broom::tidy(conf.int = TRUE,\n              conf.level = 0.95) %&gt;% \n  gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"cyan\"),cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)) %&gt;% \n  tab_header(title = \"Linear Model with Ranked Data\")\n\n\n\n\n\n\n\nwilcox.test\n    \n\nestimate\n      statistic\n      p.value\n      conf.low\n      conf.high\n      method\n      alternative\n    \n\n\n-11.33999\n10808213\n0\n-11.34003\n-10.87994\nWilcoxon rank sum test with continuity correction\ntwo.sided\n\n\n\n\n\n\n\n\nLinear Model with Ranked Data\n    \n\nterm\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n\n\n\n(Intercept)\n4836.157\n42.52745\n113.71848\n0\n4752.797\n4919.517\n\n\ngendermale\n2851.246\n59.55633\n47.87478\n0\n2734.507\n2967.986\n\n\n\n\n\n\nThe wilcox.test and the linear model with rank data offer the same results. This is of course not surprising!"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#iconify-carbon-chart-3d-inspecting-and-charting-data-3",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#iconify-carbon-chart-3d-inspecting-and-charting-data-3",
    "title": "Inference for Two Independent Means",
    "section": "\n Inspecting and Charting Data",
    "text": "Inspecting and Charting Data\nWe can make distribution plots for weight by physical_3plus:\n\ngf_boxplot(weight ~ physical_3plus, \n          fill = ~ physical_3plus,\n          data = yrbss_select_phy, xlab = \"Days of Exercise &gt;=3 \") %&gt;% \n  gf_theme(theme_classic())\n\n\n\ngf_density(~ weight,\n          fill = ~ physical_3plus,\n          data = yrbss_select_phy) %&gt;% \n  gf_theme(theme_classic())\n\n\n\n\nThe box plots show how the medians of the two distributions compare, but we can also compare the means of the distributions using the following to first group the data by the physical_3plus variable, and then calculate the mean weight in these groups using the mean function while ignoring missing values by setting the na.rm argument to TRUE.\n\nyrbss_select_phy %&gt;%\n  group_by(physical_3plus) %&gt;%\n  summarise(mean_weight = mean(weight, na.rm = TRUE))\n\n\n\n  \n\n\n\nThere is an observed difference, but is this difference large enough to deem it “statistically significant”? In order to answer this question we will conduct a hypothesis test. But before that a few more checks on the data:\nA.  Check for Normality\nAs stated before, statistical tests for means usually require a couple of checks:\n\nAre the data normally distributed?\n\nAre the data variances similar?\n\nLet us also complete a visual check for normality,with plots since we cannot do a shapiro.test:\n\nyrbss_select_phy %&gt;%\n  gf_density( ~ weight,\n              fill = ~ physical_3plus,\n              alpha = 0.5,\n              title = \"Highschoolers' Weights by Exercise Frequency\") %&gt;%\n  gf_facet_grid(~ physical_3plus) %&gt;% \n  gf_fitdistr(dist = \"dnorm\") %&gt;% \n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\nAgain, not normally distributed…\nWe can plot Q-Q plots for both variables, and also compare both data with normally-distributed data generated with the same means and standard deviations:\n\nyrbss_select_phy %&gt;% \n  gf_qq(~ weight | physical_3plus , color = ~ physical_3plus) %&gt;% \n  gf_qqline(ylab = \"Weight\") %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\nThe QQ-plots confirm that he tow data variables are not normally distributed.\nB.  Check for Variances\nLet us check if the two variables have similar variances: the var.test does this for us, with a NULL hypothesis that the variances are not significantly different:\n\nvar.test( weight ~ physical_3plus, data = yrbss_select_phy, \n          conf.int = TRUE,\n          conf.level = 0.95) %&gt;% \n  broom::tidy()\n\n# Critical F value\nqf(0.975,4021, 8341)\n\n\n\n\n\n  \n\n\n\n[1] 1.054398\n\n\n\n\nThe p.value states the probability of the data being what it is, assuming the NULL hypothesis that variances were similar. It being so small, we are able to reject this NULL Hypothesis that the variances of weight are nearly equal across the two exercise frequencies. (Compare the statistic in the var.test with the critical F-value)\n\n\n\n\n\n\nConditions\n\n\n\n\nThe two variables are not normally distributed.\nThe two variances are also significantly different.\n\n\n\nHence we will have to use non-parametric tests to infer if the means are similar."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#iconify-academicons-hypothesis-hypothesis-2",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#iconify-academicons-hypothesis-hypothesis-2",
    "title": "Inference for Two Independent Means",
    "section": "\n Hypothesis",
    "text": "Hypothesis\nBased on the graphs, how would we formulate our Hypothesis? We wish to infer whether there is difference in mean weight across physical_3plus. So accordingly:\n\\[\nH_0: \\mu_{physical-3plus-Yes} = \\mu_{physical-3plus-No}\\\\\n\\\\\\\nH_a: \\mu_{physical-3plus-Yes} \\ne \\mu_{physical-3plus-No}\\\\\n\\]"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#iconify-academicons-hypothesis-observed-and-test-statistic-2",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#iconify-academicons-hypothesis-observed-and-test-statistic-2",
    "title": "Inference for Two Independent Means",
    "section": "\n Observed and Test Statistic",
    "text": "Observed and Test Statistic\nWhat would be the test statistic we would use? The difference in means. Is the observed difference in the means between the two groups of scores non-zero? We use the diffmean function, from mosaic:\n\nobs_diff_phy &lt;- diffmean(weight ~ physical_3plus, data = yrbss_select_phy) \n\nobs_diff_phy\n\n\n\n diffmean \n-1.774584 \n\n\n\n\n\n Inference\n::: panel-tabset\nUsing parametric t.test\n\nWell, the variables are not normally distributed, and the variances are significantly different so a standard t.test is not advised. We can still try:\n\nmosaic::t_test(weight ~ physical_3plus,\n               var.equal = FALSE, # Welch Correction\n               data = yrbss_select_phy) %&gt;% \n  broom::tidy()\n\n\n\n  \n\n\n\nThe p.value is \\(8.9e-08\\) ! And the Confidence Interval is clear of \\(0\\). So the t.test gives us good reason to reject the Null Hypothesis that the means are similar. But can we really believe this, given the non-normality of data?\nUsing non-parametric paired Wilcoxon test\nHowever, we have seen that the data variables are not normally distributed. So a Wilcoxon Test, using signed-ranks, is indicated: (recall the model!)\n\n# For stability reasons, it may be advisable to use rounded data or to set digits.rank = 7, say, \n# such that determination of ties does not depend on very small numeric differences (see the example).\n\nwilcox.test(weight ~ physical_3plus,\n            conf.int = TRUE,\n            conf.level = 0.95,\n            data = yrbss_select_phy) %&gt;% \n  broom::tidy()\n\n\n\n  \n\n\n\nThe nonparametric wilcox.test also suggests that the means for weight across physical_3plus are significantly different.\nUsing the Linear Model Interpretation\nWe can apply the linear-model-as-inference interpretation to the ranked data data to implement the non-parametric test as a Linear Model:\n\\[\nlm(rank(weight) \\sim  physical.3plus) = \\beta_0 + \\beta_1 \\times physical.3plus\n\\\\\nH_0: \\beta_1 = 0\\\\\n\\\\\\\nH_a: \\beta_1 \\ne 0\\\\\n\\]\n\nlm(rank(weight) ~ physical_3plus, \n   data = yrbss_select_phy) %&gt;% \n  broom::tidy(conf.int = TRUE,\n              conf.level = 0.95)\n\n\n\n  \n\n\n\nHere too, the linear model using rank data arrives at a conclusion similar to that of the Mann-Whitney U test.\nUsing Permutation Tests\nFor this last Case Study, we will do this in two ways, just for fun: one using our familiar mosaic package, and the other using the package infer.\nBut first, we need to initialize the test, which we will save as obs_diff_**.\n\nobs_diff_infer &lt;- yrbss_select_phy %&gt;%\n  infer::specify(weight ~ physical_3plus) %&gt;%\n  infer::calculate(stat = \"diff in means\", order = c(\"yes\", \"no\"))\nobs_diff_infer\nobs_diff_mosaic &lt;- mosaic::diffmean(~ weight | physical_3plus, data = yrbss_select_phy)\nobs_diff_mosaic\nobs_diff_phy\n\n\n\n\n\n  \n\n\n\n diffmean \n-1.774584 \n\n\n diffmean \n-1.774584 \n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that obs_diff_infer is a 1 X 1 dataframe; obs_diff_mosaic is a scalar!!\n\n\n\nInference Using mosaic\n\n\nWe already have the observed difference, obs_diff_mosaic. Now we generate the null distribution using permutation, with mosaic:\n\nnull_dist_mosaic &lt;- do(999) * diffmean(~ weight | shuffle(physical_3plus), data = yrbss_select_phy)\n\nWe can also generate the histogram of the null distribution, compare that with the observed diffrence and compute the p-value and confidence intervals:\n\ngf_histogram(~ diffmean, data = null_dist_mosaic) %&gt;% \n  gf_vline(xintercept = obs_diff_mosaic, colour = \"red\")\n\n\n\n# p-value\nprop(~ diffmean != obs_diff_mosaic, data = null_dist_mosaic)\n\nprop_TRUE \n        1 \n\n# Confidence Intervals for p = 0.95\nmosaic::cdata(~ diffmean, p = 0.95, data = null_dist_mosaic)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#your-turn",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#your-turn",
    "title": "Inference for Two Independent Means",
    "section": "Your Turn",
    "text": "Your Turn\n\nCalculate a 95% confidence interval for the average height in meters (height) and interpret it in context.\nCalculate a new confidence interval for the same parameter at the 90% confidence level. Comment on the width of this interval versus the one obtained in the previous exercise.\nConduct a hypothesis test evaluating whether the average height is different for those who exercise at least three times a week and those who don’t.\nNow, a non-inference task: Determine the number of different options there are in the dataset for the hours_tv_per_school_day there are.\nCome up with a research question evaluating the relationship between height or weight and sleep. Formulate the question in a way that it can be answered using a hypothesis test and/or a confidence interval. Report the statistical results, and also provide an explanation in plain language. Be sure to check all assumptions, state your \\(\\alpha\\) level, and conclude in context."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#iconify-fluent-mdl2-decision-solid-conclusion",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#iconify-fluent-mdl2-decision-solid-conclusion",
    "title": "Inference for Two Independent Means",
    "section": "\n Conclusion",
    "text": "Conclusion\nWe have learnt how to perform inference for independent means. We have looked at the conditions that make the regular t.test possible, and learnt what to do if the conditions of normality and equal variance are not met. We have also looked at how these tests can be understood as manifestations of the linear model, with data and sign-ranked data. It should also be fairly clear now that we can test for the equivalence of two paired means, using a very simple permutation tests. Given computing power, we can always mechanize this test very quickly to get our results. And that performing this test yields reliable results without having to rely on any assumption relating to underlying distributions and so on."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#iconify-ooui-references-ltr-references",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#iconify-ooui-references-ltr-references",
    "title": "Inference for Two Independent Means",
    "section": "\n References",
    "text": "References\n\nRandall Pruim, Nicholas J. Horton, Daniel T. Kaplan, Start Teaching with R\n\nhttps://bcs.wiley.com/he-bcs/Books?action=index&itemId=111941654X&bcsId=11307\nhttps://statsandr.com/blog/wilcoxon-test-in-r-how-to-compare-2-groups-under-the-non-normality-assumption/"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#footnotes",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#footnotes",
    "title": "Inference for Two Independent Means",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://stats.stackexchange.com/questions/2492/is-normality-testing-essentially-useless↩︎\nhttps://www.allendowney.com/blog/2023/01/28/never-test-for-normality/↩︎\nhttps://stats.stackexchange.com/q/113337↩︎\nhttps://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/intro-linear-models.html#a-linear-model-can-be-fit-to-data-with-continuous-discrete-or-categorical-x-variables↩︎\nhttps://allendowney.blogspot.com/2016/06/there-is-still-only-one-test.html↩︎\nhttps://stats.stackexchange.com/questions/92374/testing-large-dataset-for-normality-how-and-is-it-reliable↩︎\nhttps://stats.stackexchange.com/q/113337↩︎\nhttps://en.wikipedia.org/wiki/Dummy_variable_(statistics)↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/paired-means-tutorial.html",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/paired-means-tutorial.html",
    "title": "Tutorial on Inference for Two Paired Means",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\n\nlibrary(resampledata)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/paired-means-tutorial.html#iconify-noto-v1-package-setting-up-r-packages",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/paired-means-tutorial.html#iconify-noto-v1-package-setting-up-r-packages",
    "title": "Tutorial on Inference for Two Paired Means",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\n\nlibrary(resampledata)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/paired-means-tutorial.html#iconify-pajamas-issue-type-test-case-case-study-1-icecream",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/paired-means-tutorial.html#iconify-pajamas-issue-type-test-case-case-study-1-icecream",
    "title": "Tutorial on Inference for Two Paired Means",
    "section": "\n Case Study-1: IceCream!!",
    "text": "Case Study-1: IceCream!!\nWhat is there to not like about icecreams!! Here is a dataset that has data on Sugar and Calories between Vanilla and Chocolate icecreams, across several brands of icecreams. Is this a sample of paired data? Let us check:\n\n Inspecting and Charting Data\n\ndata(\"IceCream\")\nIceCream\ninspect(IceCream)\n\n\n\n\n\n  \n\n\n\n\n\n\ncategorical variables:  \n   name  class levels  n missing                                  distribution\n1 Brand factor     39 39       0 Baskin Robbins (2.6%) ...                    \n\nquantitative variables:  \n               name   class   min    Q1 median    Q3 max      mean        sd  n\n1   VanillaCalories integer 120.0 140.0    160 240.0 307 191.41026 58.644207 39\n2        VanillaFat numeric   4.5   7.5      9  15.5  21  11.28718  4.431655 39\n3      VanillaSugar numeric  10.0  12.5     17  21.0  27  17.13077  4.841333 39\n4 ChocolateCalories integer 120.0 140.0    170 260.0 320 198.74359 63.063342 39\n5      ChocolateFat numeric   5.0   7.5      9  14.7  21  11.12051  4.597378 39\n6    ChocolateSugar numeric  12.0  15.0     18  22.3  33  18.97436  5.402812 39\n  missing\n1       0\n2       0\n3       0\n4       0\n5       0\n6       0\n\n\n\n\nHmm…the data are about calories, fat, and sugar between two flavours of icecream sold by each brand. There are 39 brands.\nLet us plot the data first:\n\nIceCream %&gt;% \n  gf_col(fct_reorder(Brand, VanillaCalories) ~ VanillaCalories, \n         fill = \"red\") %&gt;% \n  gf_col(fct_reorder(Brand, VanillaCalories) ~ - ChocolateCalories, \n         fill = \"green\",\n         xlab = \"Calories\", ylab = \"Brand\", \n         title = \"Calories across Icecream Brands\",\n         subtitle = \"Vanilla = Red, Green = Chocolate\") %&gt;% \n  gf_theme(theme_classic())\nIceCream %&gt;% \n  gf_col(fct_reorder(Brand, VanillaFat) ~ VanillaFat, \n         fill = \"red\") %&gt;% \n  gf_col(fct_reorder(Brand, VanillaFat) ~ - ChocolateFat, \n         fill = \"green\",\n         xlab = \"Fat\", ylab = \"Brand\", \n         title = \"Calories across Icecream Brands\",\n         subtitle = \"Vanilla = Red, Green = Chocolate\") %&gt;% \n  gf_theme(theme_classic())\nIceCream %&gt;% \n  gf_col(fct_reorder(Brand, VanillaSugar) ~ VanillaSugar, \n         fill = \"red\") %&gt;% \n  gf_col(fct_reorder(Brand, VanillaSugar) ~ - ChocolateSugar, \n                  fill = \"green\",\n         xlab = \"Sugar\", ylab = \"Brand\", \n         title = \"Calories across Icecream Brands\",\n         subtitle = \"Vanilla = Red, Green = Chocolate\") %&gt;% \n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe may hypothesize that say, the fat content in the two flavours might be similar on a per brand basis. That is, if say Baskin Robbins has high sugar in the vanilla flavour, it is likely to have high sugar also in its chocolate flavour.\nLet us see what are the observed differences in the mean values of calories, sugar, and fat across brands:\n\nIceCream %&gt;% \n  mutate(diff_calories = VanillaCalories - ChocolateCalories,\n         diff_fat = VanillaFat - ChocolateFat,\n         diff_sugar = VanillaSugar - ChocolateSugar) %&gt;% \n  summarise(mean_diff_calories = mean(diff_calories),\n            mean_diff_fat = mean(diff_fat),\n            mean_diff_sugar = mean(diff_sugar))\n\n\n\n  \n\n\n\nHmm…while the numbers showing difference in means are quite different, we need to perform tests to infer whether these difference are statistically significant.\n\n Hypothesis\nHow do we specify our Hypotheses? (Of course, there is more than one!)\nWrite the Null and Alternate hypotheses here.\n\n Null Distribution Computations\nHow do we compute the NULL distributions, for each of the three components of the ice creams, using pair-wise analysis?"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/paired-means-tutorial.html#iconify-fluent-mdl2-decision-solid-conclusions",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/paired-means-tutorial.html#iconify-fluent-mdl2-decision-solid-conclusions",
    "title": "Tutorial on Inference for Two Paired Means",
    "section": "\n Conclusions",
    "text": "Conclusions\nSo are there significant differences in sugar, fat, and calorie content across the two flavours?\nIs this conclusion different if you don’t use paired-data, and just treat the data as independent readings?"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE,warning = FALSE,message = FALSE, tidy = TRUE)\nlibrary(tidyverse) # Tidy data processing \nlibrary(ggformula) # Formula based plots\nlibrary(mosaic) # Data inspection and Statistical Inference \nlibrary(infer) # Statistical Inference\nlibrary(cowplot) # Arranging Plots\n\n\n# Let us set a plot theme for Data visualization\nlibrary(showtext)\nlibrary(sysfonts)\nfont_add_google(\"Merriweather Sans\", \"Merri\")\nmy_theme &lt;- function() {\n  # Creating a function\n  theme_classic() +  # Using pre-defined theme as base\n    theme(\n      text = element_text(family = \"Merri\"),\n      plot.title = element_text(face = \"bold\", size = 14),\n      axis.text.x = element_text(size = 10, face = \"bold\"),\n      # Customizing axes text\n      axis.text.y = element_text(size = 10, face = \"bold\"),\n      axis.title = element_text(size = 12, face = \"bold\"),\n      # Customizing axis title\n      panel.grid = element_blank(),\n      # Taking off the default grid\n      plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), units = , \"cm\"),\n      legend.text = element_text(size = 8, face = \"italic\"),\n      # Customizing legend text\n      legend.title = element_text(size = 10, face = \"bold\"),\n      # Customizing legend title\n      legend.position = \"right\",\n      # Customizing legend position\n      plot.caption = element_text(size = 8)\n    )  # Customizing plot caption\n}"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#iconify-noto-v1-package-setting-up-r-packages",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#iconify-noto-v1-package-setting-up-r-packages",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE,warning = FALSE,message = FALSE, tidy = TRUE)\nlibrary(tidyverse) # Tidy data processing \nlibrary(ggformula) # Formula based plots\nlibrary(mosaic) # Data inspection and Statistical Inference \nlibrary(infer) # Statistical Inference\nlibrary(cowplot) # Arranging Plots\n\n\n# Let us set a plot theme for Data visualization\nlibrary(showtext)\nlibrary(sysfonts)\nfont_add_google(\"Merriweather Sans\", \"Merri\")\nmy_theme &lt;- function() {\n  # Creating a function\n  theme_classic() +  # Using pre-defined theme as base\n    theme(\n      text = element_text(family = \"Merri\"),\n      plot.title = element_text(face = \"bold\", size = 14),\n      axis.text.x = element_text(size = 10, face = \"bold\"),\n      # Customizing axes text\n      axis.text.y = element_text(size = 10, face = \"bold\"),\n      axis.title = element_text(size = 12, face = \"bold\"),\n      # Customizing axis title\n      panel.grid = element_blank(),\n      # Taking off the default grid\n      plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), units = , \"cm\"),\n      legend.text = element_text(size = 8, face = \"italic\"),\n      # Customizing legend text\n      legend.title = element_text(size = 10, face = \"bold\"),\n      # Customizing legend title\n      legend.position = \"right\",\n      # Customizing legend position\n      plot.caption = element_text(size = 8)\n    )  # Customizing plot caption\n}"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n Introduction",
    "text": "Introduction\nSuppose we have three sales strategies on our website, to sell a certain product, say men’s shirts. We have observations of customer website interactions over several months. How do we know which strategy makes people buy the fastest ?\nIf there is a University course that is offered in parallel in three different classrooms, is there a difference between the average marks obtained by students in each of the classrooms?\nIn each case we have a set of observations in each category: Interaction Time vs Sales Strategy in the first example, and Student Marks vs Classroom in the second. We can take mean scores in each category and decide to compare them. How do we make the comparisons? One way would be to compare them pair-wise. But with this rapidly becomes intractable and also dangerous: with increasing number of groups, the number of mean-comparisons becomes very large \\(N\\choose 2\\) and with each comparison the possibility of some difference showing up, just by chance, increases! And we end up making the wrong inference and perhaps the wrong decision.\nThe trick is of course to make comparisons all at once and ANOVA is the technique that allows us to do just that. In this tutorial, we will compare the Hatching Time of frog spawn1, at three different lab temperatures.\nIn this tutorial, our research question is:\n\n\n\n\n\n\nResearch Question\n\n\n\nHow does frogspawn hatching time vary with different temperature settings?"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#iconify-flat-color-icons-workflow-workflow-read-the-data",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#iconify-flat-color-icons-workflow-workflow-read-the-data",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n Workflow: Read the Data",
    "text": "Workflow: Read the Data\nDownload the data by clicking the button below.\n Download the frogs data \n\n\n\n\n\n\nData Folder\n\n\n\nSave the CSV it in a subfolder titled “data” inside your R work folder.\n\n\n\nfrogs_orig &lt;- read_csv(\"data/frogs.csv\")\nfrogs_orig\n\n\n\n  \n\n\n\nOur response variable is the hatching Time. Our explanatory variable is a factor, Temperature, with 3 levels: 13°C, 18°C and 25°C. Different samples of spawn were subject to each of these temperatures respectively."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#iconify-material-symbols-pivot-table-chart-workflow-clean-the-data",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#iconify-material-symbols-pivot-table-chart-workflow-clean-the-data",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n Workflow: Clean the Data",
    "text": "Workflow: Clean the Data\nThe data is badly organized, with a separate column for each Temperature, and a common column for Sample ID. There are NA entries since not all samples of spawn are subject to all temperatures. (E.g. Sample ID #1 was maintained at 13°C). Hence, we should pivot_longer() this data, so all Time readings are in one numerical column, all Temperatures are also in a column that we should convert into a factor:\n\nfrogs_long &lt;- frogs_orig %&gt;%\n  pivot_longer(\n    .,\n    cols = starts_with(\"Temperature\"),\n    cols_vary = \"fastest\",\n    # new in pivot_longer\n    names_to = \"Temp\",\n    values_to = \"Time\"\n  ) %&gt;%\n  drop_na() %&gt;%\n  \n  # knock off the unnecessary \"Temperature\" word everywhere\n  # Just keep the digits thereafter\n  separate_wider_regex(\n    cols = Temp,\n    patterns = c(\"Temperature\",\n                 TempFac = \"\\\\d+\"),\n    cols_remove = TRUE\n  ) %&gt;%\n  \n  # Convert Temp into TempFac, a 3-level factor\n  mutate(TempFac = factor(\n    x = TempFac,\n    levels = c(13, 18, 25),\n    labels = c(\"13\", \"18\", \"25\")\n  )) %&gt;%\n  rename(\"Id\" = `Frogspawn sample id`)\n\nfrogs_long\nfrogs_long %&gt;% count(TempFac)\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n\n\n\n\nSo we have cleaned up our data and have 20 samples for Hatching Time per TempFac setting."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#iconify-flat-color-icons-workflow-workflow-eda",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#iconify-flat-color-icons-workflow-workflow-eda",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n Workflow: EDA",
    "text": "Workflow: EDA\nLet us plot some histograms and boxplots of Hatching Time:\n\ngf_histogram(data = frogs_long,\n             ~ Time,\n             fill = ~ TempFac,\n             stat = \"count\") %&gt;%\n  gf_vline(xintercept = ~ mean(Time)) %&gt;%\n  gf_labs(x = \"Hatching Time\", y = \"Count\") %&gt;%\n  gf_text(7 ~ (mean(Time) + 2),\n          label = \"Overall Mean\") %&gt;%\n  gf_theme(theme = my_theme(),\n           guides(fill = guide_legend(title = \"Temperature level (°C)\"))) \ngf_boxplot(data = frogs_long,\n           Time ~ TempFac,\n           fill = ~ TempFac) %&gt;%\n  gf_vline(xintercept = ~ mean(Time)) %&gt;%\n  gf_labs(x = \"Temperature\", y = \"Hatching Time\") %&gt;%\n  gf_theme(theme = my_theme(),\n           guides(fill = guide_legend(title = \"Temperature level (°C)\")))\n\n\n\n\n\n\n\n\n\n\n\nThe histograms look well separated and the box plots also show very little overlap. So we can reasonably hypothesize that Temperature has a significant effect on Hatching Time.\nOne more slightly esoteric plot: Jitter/Scatter with a new categorical x-axis offered by the ggprism package:\n\nlibrary(ggprism)\ngf_jitter(\n  frogs_long,\n  Time ~ TempFac,\n  color = ~ TempFac,\n  xlab = \"Temperature as Factor\",\n  ylab = \"Hatching Time\",\n  caption = \"Using `ggprism` package\"\n) %&gt;%\n  gf_theme(theme_prism(base_family = theme_get()$font$family)) %&gt;%\n  gf_refine(theme(legend.position = \"none\"),\n            scale_x_discrete(guide = \"prism_bracket\"))\n\n\n\n\nLet’s go ahead with our ANOVA test."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#iconify-flat-color-icons-workflow-workflow-anova",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#iconify-flat-color-icons-workflow-workflow-anova",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n Workflow: ANOVA",
    "text": "Workflow: ANOVA\nWe will first execute the ANOVA test with code and evaluate the results. Then we will do an intuitive walk through of the process and finally, hand-calculate entire analysis for clear understanding.\n\n\nANOVA Test with Code\nANOVA Intuitive\n\nANOVA Manually Demonstrated2(Apologies to Spinoza)\n\n\n\nR offers a very simple command to execute an ANOVA test: Note the familiar formula of stating the variables:\n\nfrogs_anova &lt;- aov(Time ~ TempFac, data = frogs_long)\nsummary(frogs_anova)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)    \nTempFac      2 1020.9   510.5   385.9 &lt;2e-16 ***\nResiduals   57   75.4     1.3                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nfrogs_anova %&gt;% broom::tidy()\n\n\n\n  \n\n\nsummary.lm(frogs_anova) %&gt;% broom::tidy()\n\n\n\n  \n\n\nfrogs_anova %&gt;% broom::glance()\n\n\n\n  \n\n\n\nThe effect of Temperature on Hatching time is significant, with a p-value of \\(&lt;2e-16\\). The F-statistic for the ANOVA test is given by \\(385.9\\), which is very high, and the r.squared value ( to be discussed later) is also large, \\(0.931\\). Clearly Temperature has a very significant effect on the hatching Time.\nTo find which specific value of TempFac has the most effect will require pairwise comparison of the group means, using a standard t-test. The confidence level for such repeated comparisons will need what is called Bonferroni correction3 to prevent us from detecting a significant (pair-wise) difference simply by chance. To do this we take \\(\\alpha = 0.05\\), the confidence level used and divide it by \\(K\\), the number of pair-wise comparisons we intend to make. This new value is used to decide on the significance of the estimated parameter. So the pairwise comparisons in our current data will have to use \\(\\alpha/3 = 0.0166\\) as the confidence level.\n\n\nAll that is very well, but what is happening under the hood of the aov() command?\nConsider a data set with a single Quant and a single Qual variable. The Qual variable has two levels, the Quant data has 20 observations per Qual level.\n\nlibrary(ggtext)\nset.seed(42) # for replication\ndata = tibble(\n  index = 1:40,\n  qual = c(rep(x = \"A\", 20), rep(x = \"B\", 20)),\n  quant = c(rnorm(\n    n = 20, mean = 0, sd = 2\n  ),\n  rnorm(\n    n = 20, mean = 10, sd = 2\n  ))\n)\ndata\noverall_mean &lt;- data %&gt;%\n  summarise(overall_mean = mean(quant))\n#overall_mean\n\ngrouped_means &lt;- data %&gt;%\n  group_by(qual) %&gt;%\n  summarise(grouped_means = mean(quant))\n#grouped_means\n\np1 &lt;- gf_segment(\n  data = data,\n  color = \"black\",\n  overall_mean$overall_mean + quant ~ index + index,\n  ylab = \"quant\") %&gt;% \n  gf_point(quant ~ index,\n               color = ~ qual,\n               data = data) %&gt;%\n  gf_hline(yintercept =  ~ overall_mean,\n           data = overall_mean) %&gt;%\n  gf_text(4.25 ~ 30,\n          label = expression(paste(mu, \"_tot\")),\n          inherit = F) %&gt;%\n  gf_theme(my_theme())\n\np2 &lt;- gf_point(\n  quant ~ index,\n  group = ~ qual,\n  colour = ~ qual,\n  data = data\n) %&gt;%\n  gf_hline(\n    yintercept = ~ mean,\n    colour = ~ qual,\n    data = data %&gt;%\n      group_by(qual) %&gt;%\n      summarise(mean = mean(quant))\n  ) %&gt;%\n  gf_segment(data = data %&gt;% filter(qual == \"A\"),\n             grouped_means$grouped_means[1] + quant ~ index + index) %&gt;%\n  gf_segment(data = data %&gt;% filter(qual == \"B\"),\n             grouped_means$grouped_means[2] + quant ~ index + index) %&gt;%\n  gf_text(10.0 ~ 10,\n          label = expression(paste(mu, \"_B\")),\n          inherit = F) %&gt;%\n  gf_text(1 ~ 38,\n          label = expression(paste(mu, \"_A\")),\n          inherit = F) %&gt;%\n  gf_theme(my_theme())\n\n\ncowplot::plot_grid(p1, p2, labels = c(\"Fig A: SST\", \"Fig B: SSE\"))\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nIn Fig A, the horizontal black line is the overall mean of quant, denoted as \\(\\mu_{tot}\\). The vertical black lines to the points show the departures of each point from this overall mean. The sum of squares of these vertical black lines in Fig A is called the Total Sum of Squares (SST).\n\\[\nSST = \\Sigma (y - \\mu_{tot})^2\n\\tag{1}\\]\nIf there are \\(k\\) levels in qual and \\(n\\) observations \\(y_ n\\) for each level, we can also write:\n\\[\nSST =\n\\sum_{i=1}^{kn}y_i^2 - \\frac{ \\left( \\sum_{i=1}^{kn}\ny_i \\right)^2}{kn}\n\\]\nIn Fig B, the horizontal green and red lines are the means of the individual groups, respectively \\(\\mu_A\\) and \\(\\mu_B\\). The green and red vertical lines are the departures, or errors, of each point from its own group-mean. The sum of the squares of the green and red lines is called the Total Error Sum of Squares (SSE).\n\\[\nSSE = \\Sigma [(y - \\mu_i)^2 +... (y - \\mu_k)^2]\n\\tag{2}\\]\nIf the \\(\\mu_A\\) and \\(\\mu_B\\) are different from \\(\\mu_{tot}\\), then what would be the relationship between \\(SSA\\) and \\(SSE\\) ? Clearly if the all means are identical then the \\(SST\\) and \\(SSE\\) are equal, since the two coloured lines would be in the same place as the black line. It should be clear that if \\(\\mu_A\\) and \\(\\mu_B\\) are different from the overall mean, then \\(SSE &lt; SST\\).\nSo, when we desire to detect if the two groups are different in their means, we take the difference:\n\\[\nSSA = SST - SSE\n\\tag{3}\\]\n\\(SSA\\) is called the Treatment Sum of Squares and is a measure the differences in means of observations at different levels of the factor.\n\\(SSA\\) can also directly be re-written in a very symmetric fashion as:\n\\[\n\\frac{\\sum_{i=1}^{k} \\left( \\sum_{j=1}^{n}y_{ij}\\right)^2 }{n} - \\frac{\\left( \\sum_{i=1}^{kn}\ny_i \\right)^2}{kn}\n\\]\nNote that in the first term, we are calculating sums of observations within each group in the inner summation, which is like a per-group mean(without the division). The outer summation takes the sum of squares of these undivided summations and divides by \\(n\\).\nComparing \\(SSA\\) and \\(SSE\\) now provides us with a method that helps us decide whether these means are different. The logic is that we compare global differences and local differences. Since each of these measures uses a different sets of observations, the comparison is done after scaling each of \\(SSA\\) and \\(SSE\\) by the number of observations influencing them.\nThis means that we need to divide each of \\(SSA\\) and \\(SSE\\) by their degrees of freedom, which gives us a ratio of variances, the F-statistic:\n\\[\nF_{stat} = \\frac{SSA / df_{SSA}}{SSE / df_{SSE}}\n\\]\nwhere \\(df_{SSA}\\) and \\(df_{SSE}\\) are respectively the degrees of freedom in \\(SSA\\) and \\(SSE\\). And so we are in effect deciding if means are significantly different by analyzing (a ratio of) variances! Hence AN-alysis O-f VA-riance, ANOVA.\nIn order to find which of the means is significantly different from others, we need to make a pair-wise comparison of the means, applying the Bonferroni correction as stated before. This means we divide the critical p.value we expect by the number of comparisons we make between levels of the Qual variable. More on this shortly.\n\n\nNow that we understand what aov() is doing, let us hand-calculate the numbers for our frogs dataset and check. Let us visualize our calculations first.\n\n\n\n\n\n\n\n\n\n\nHere is the SST:\n\n# Calculate overall sum squares SST\n\n\nfrogs_overall &lt;- frogs_long %&gt;% \n  summarise(mean_time = mean(Time), \n            # Overall mean across all readings\n            # The Black Line\n            \n            SST = sum((Time - mean_time)^2),\n            n = n())\nfrogs_overall\n\nSST &lt;- frogs_overall$SST\nSST\n\n\n\n  \n\n\n\n[1] 1096.333\n\n\nAnd here is the SSE:\n\n# Calculate sums of square errors *within* each group\n# with respect to individual group means\n\nfrogs_within_groups &lt;- frogs_long %&gt;% \n  group_by(TempFac) %&gt;% \n   summarise(mean_time = mean(Time),\n            variance_time = var(Time),\n            group_error_squares = sum((Time - mean_time)^2),\n            n = n())\nfrogs_within_groups\n\nfrogs_SSE &lt;- frogs_within_groups %&gt;% \n  summarise(SSE = sum(group_error_squares))\n\nSSE &lt;- frogs_SSE$SSE\nSSE\n\n\n\n  \n\n\n\n[1] 75.4\n\n\nOK, we have \\(SST\\) and \\(SSE\\), so let’s get \\(SSA\\):\n\nSST\nSSE\nSSA &lt;- SST - SSE\nSSA\n\n[1] 1096.333\n[1] 75.4\n[1] 1020.933\n\n\nWe have \\(SST = 1096\\), \\(SSE = 75.4\\) and therefore \\(SSA = 1020.9\\).\nIn order to calculate the F-Statistic, we need to compute the variances, using these sum of squares. We obtain variances by dividing by their Degrees of Freedom:\n\\[\nF_{stat} = \\frac{SSA / df_{SSA}}{SSE / df_{SSE}}\n\\]\nwhere \\(df_{SSA}\\) and \\(df_{SSE}\\) are respectively the degrees of freedom in SSA and SSE.\nLet us calculate these Degrees of Freedom.\nWith \\(k = 3\\) levels in the factor TempFac, and \\(n = 20\\) points per level, \\(SST\\) clearly has degree of freedom \\(kn-1\\), since it uses all observations but loses one degree to calculate the global mean. (If each level did not have the same number of points \\(n\\), we simply take all observations less one as the degrees of freedom for \\(SST\\)).\n\\(SSE\\) has \\(k*(n-1)\\) as degrees of freedom, since each of the \\(k\\) groups there are \\(n\\) observations and each group loses one degree to calculate its own group mean.\nAnd therefore \\(SSA\\), being their difference, has \\(k-1\\) degrees of freedom.\nWe can still calculate these in R, for the sake of method and clarity:\n\n# Error Sum of Squares SSE\ndf_SSE &lt;- frogs_long %&gt;%\n  \n  # Takes into account \"unbalanced\" situations\n  group_by(TempFac) %&gt;%\n  summarise(per_group_df_SSE = n() - 1) %&gt;%\n  summarise(df_SSE = sum(per_group_df_SSE)) %&gt;% as.numeric()\n\n\n## Overall Sum of Squares SST\ndf_SST &lt;- frogs_long %&gt;%\n  summarise(df_SST = n() - 1) %&gt;% as.integer()\n\n\n# Treatment Sum of Squares SSA\nk &lt;- length(unique(frogs_long$TempFac))\ndf_SSA &lt;- k - 1\n\nThe degrees of freedom for the quantities are:\n\ndf_SST\ndf_SSE\ndf_SSA\n\n\n\n[1] 59\n[1] 57\n[1] 2\n\n\n\n\nNow we are ready to compute the F-statistic: dividing each sum-of-squares byt its degrees of freedom gives us variances which we will compare, using the F-statistic as a ratio:\n\n# Finally F_Stat!\n# Combine the sum-square_error for each level of the factor\n# Weighted by degrees of freedom **per level**\n# Which are of course equal here ;-D\n\nMSE &lt;- frogs_within_groups %&gt;% \n  summarise(mean_square_error = sum(group_error_squares/df_SSE)) %&gt;% \n  as.numeric()\nMSE\n\nMSA &lt;- SSA/df_SSA # This is OK\nMSA\n\nF_stat &lt;- MSA/MSE\nF_stat\n\n[1] 1.322807\n[1] 510.4667\n[1] 385.8966\n\n\nThe F-stat is compared with a critical value of the F-statistic,F_crit which is computed using the formula for the f-distribution in R. As with our hypothesis tests, we set the significance level to \\(\\alpha = 0.95\\), but here with the Bonferroni correction, and quote the two relevant degrees of freedom as parameters to qf() which computes the critical F value as a quartile:\n\nF_crit &lt;-  \n  qf(p = (1 - 0.05/3),  # Significance level is 5% + Bonferroni Correction\n          df1 = df_SSA, # Numerator degrees of freedom \n          df2 = df_SSE  # Denominator degrees of freedom\n     ) \nF_crit\nF_stat\n\n[1] 4.403048\n[1] 385.8966\n\n\nThe F_crit value can also be seen in a plot4,5:\n\ngf_dist(dist = \"f\",\n        params = list(df1 = df_SSA, df2=df_SSE),\n        linewidth  = 1,\n        xlim = c(0.002, 400), title = \"F distribution for Frog ANOVA\") %&gt;% \n  gf_vline(xintercept = F_crit, linetype = \"dotted\", \n           colour = \"red\") %&gt;% \n  gf_vline(xintercept = F_stat, linetype = \"dashed\", \n           color = \"dodgerblue\") %&gt;% \n  gf_text( 0.25 ~ 360, label = \"F_stat\", colour = \"dodgerblue\") %&gt;% \n  gf_text( 0.25 ~ 20, label = \"F_crit\", colour = \"red\")  %&gt;%\n  gf_theme(my_theme())\nmosaic::xpf(q = F_crit, \n            df1 = df_SSA, df2 = df_SSE,\n            log.p = FALSE,lower.tail = TRUE)\n\n\n\n\n\n\n\n\n\n[1] 0.9833333\n\n\n\n\nAny value of F more than the F_crit occurs with smaller probability than \\(0.05/3\\). Our F_stat is much higher than F_crit, by orders of magnitude! And so we can say with confidence that Temperature has a significant effect on spawn Time.\nAnd that is how ANOVA computes!"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#iconify-flat-color-icons-workflow-workflow-checking-anova-assumptions",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#iconify-flat-color-icons-workflow-workflow-checking-anova-assumptions",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n Workflow: Checking ANOVA Assumptions",
    "text": "Workflow: Checking ANOVA Assumptions\nANOVA makes 3 fundamental assumptions:\n\nData (and errors) are normally distributed.\nVariances are equal.\nObservations are independent.\n\nWe can check these using checks and graphs.\n\n Checks for Normality\nThe shapiro.wilk test tests if a vector of numeric data is normally distributed and rejects the hypothesis of normality when the p-value is less than or equal to 0.05. \n\nshapiro.test(x = frogs_long$Time)\n\n\n    Shapiro-Wilk normality test\n\ndata:  frogs_long$Time\nW = 0.92752, p-value = 0.001561\n\n\nThe p-value is very low and we cannot reject the (alternative) hypothesis that the overall data is not normal. How about normality at each level of the factor?\n\nfrogs_grouped &lt;- frogs_long %&gt;% \n  group_by(TempFac) %&gt;% \n  nest(.key = \"list\") # naming the nested column \"list\"\n\n# Checking if we can purrr\nfrogs_grouped %&gt;% \n  pluck(\"list\", 1) %&gt;% \n  select(Time) %&gt;% \n  as_vector() %&gt;% \n  shapiro.test(.)\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.88954, p-value = 0.02638\n\n# OK now we are set for group-wise Shapiro-Wilk testing with purrr:\n\nfrogs_grouped %&gt;% \n  mutate(shaptest = \n           purrr::map(.x = list, # Column name is \"list\"\n                      .f = \\(.x) select(.data = .x, \n                                        Time) %&gt;% \n                                 as_vector() %&gt;% \n                                 shapiro.test(.)),\n         \n         params = map(.x = shaptest,\n                      .f = \\(.x) broom::tidy(.x))) %&gt;% \n  \n  select(TempFac, params) %&gt;% \n  unnest(cols = params)\n\n\n\n  \n\n\n\nThe shapiro.wilk test makes a NULL Hypothesis that the data are normally distributed and estimates the probability that this could have happened by chance. Except for TempFac = 18 the p-values are less than 0.05 and we can reject the NULL hypothesis that each of these is normally distributed. Perhaps this is a sign that we need more than 20 samples per factor level. Let there be more frogs !!!\nWe can also check the residuals post-model:\n\nfrogs_anova$residuals %&gt;% \n  as_tibble() %&gt;% \n  gf_dhistogram(~ value,data = .) %&gt;% \n  gf_fitdistr() %&gt;%\n  gf_theme(my_theme())\nfrogs_anova$residuals %&gt;%\n  as_tibble() %&gt;% \n  gf_qq(~ value, data = .) %&gt;% \n  gf_qqstep() %&gt;% \n  gf_qqline() %&gt;%\n  gf_theme(my_theme())\nshapiro.test(frogs_anova$residuals)\n\n\n\n\n\n\n\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  frogs_anova$residuals\nW = 0.94814, p-value = 0.01275\n\n\n\n\nUnsurprisingly, the residuals are also not normally distributed either.\n\n Check for Similar Variance\nResponse data with different variances at different levels of an explanatory variable are said to exhibit heteroscedasticity. This violates one of the assumptions of ANOVA.\nTo check if the Time readings are similar in variance across levels of TempFac, we can use the Levene Test, or since our per-group observations are not normally distributed, a non-parametric rank-based Fligner-Killeen Test. The NULL hypothesis is that the data are with similar variances. The tests assess how probable this is with the given data assuming this NULL hypothesis:\n\nfrogs_long %&gt;% \n  group_by(TempFac) %&gt;% \n  summarise(variance = var(Time))\n\n# Not too different...OK on with the test\nfligner.test(Time ~ TempFac, data = frogs_long)\n\n\nDescTools::LeveneTest(Time ~ TempFac, data = frogs_long)\n\n\n\n\n\n  \n\n\n\n\n    Fligner-Killeen test of homogeneity of variances\n\ndata:  Time by TempFac\nFligner-Killeen:med chi-squared = 0.53898, df = 2, p-value = 0.7638\n\n\n\n\n\n\n  \n\n\n\n\n\nIt seems that there is no cause for concern here; the data do not have significantly different variances.\n\n Independent Observations\nThis is an experiment design concern; the way the data is gathered must be specified such that data for each level of the factors ( factor combinations if there are more than one) should be independent."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#iconify-flat-color-icons-workflow-workflow-effect-size",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#iconify-flat-color-icons-workflow-workflow-effect-size",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n Workflow: Effect Size",
    "text": "Workflow: Effect Size\nThe simplest way to find the actual effect sizes detected by an ANOVA test is to use (paradoxically) the summary.lm() command:\n\ntidy_anova &lt;- \n  frogs_anova %&gt;% \n  summary.lm() %&gt;% \n  broom::tidy()\ntidy_anova\n\n\n\n  \n\n\n\nIt may take a bit of effort to understand this. First the TempFac is arranged in order of levels, and the mean at the \\(TempFac = 13\\) is titled Intercept. That is \\(26.3\\). The other two means for levels \\(18\\) and \\(25\\) are stated as differences from this intercept, \\(-5.3\\) and \\(-10.1\\) respectively. The p.value for all these effect sizes is well below the desired confidence level of \\(0.05\\).\n\n\n\n\n\n\nStandard Errors\n\n\n\nObserve that the std.error for the intercept is \\(0.257\\) while that for TempFac18 and TempFac25 is \\(0.257 \\times \\sqrt2 = 0.363\\) since the latter are differences in means, while the former is a single mean. The Variance of a difference is the sum of the individual variances, which are equal here.\n\n\nWe can easily plot bar-chart with error bars for the effect size:\n\ntidy_anova %&gt;% \n  mutate(hi = estimate + std.error,\n         lo = estimate - std.error) %&gt;% \n  gf_hline(data = ., yintercept = 0, \n           colour =\"grey\", \n           linewidth = 2) %&gt;% \n  gf_col(estimate ~ term, \n         fill = \"grey\", \n         color = \"black\",\n         width = 0.15) %&gt;% \n  gf_errorbar(hi + lo ~ term,\n              color = \"blue\",\n              width = 0.2) %&gt;% \n  gf_point(estimate ~ term,\n           color = \"red\", \n           size = 3.5) %&gt;% \n  gf_refine(scale_x_discrete(\"Temp Values\", \n                             labels = c(\"13°C\", \"18°C\", \"25°C\")))  %&gt;%\n  gf_theme(my_theme())\n\n\n\n\nIf we want an “absolute value” plot for effect size, it needs just a little bit of work:\n\n# Merging group averages with `std.error`\nfrogs_long %&gt;% \n  group_by(TempFac) %&gt;% \n  summarise(mean = mean(Time)) %&gt;% \n  cbind(std.error = tidy_anova$std.error) %&gt;% \n  mutate(hi = mean + std.error,\n         lo = mean - std.error) %&gt;% \n  gf_hline(data = ., yintercept = 0, \n           colour =\"grey\", \n           linewidth = 2) %&gt;% \n  gf_col(mean ~ TempFac, \n         fill = \"grey\", \n         color = \"black\", width = 0.15) %&gt;% \n  gf_errorbar(hi + lo ~ TempFac,\n                color = \"blue\",\n                width =0.2) %&gt;% \n  gf_point(mean ~ TempFac, \n           color = \"red\", \n           size = 3.5) %&gt;% \n  gf_refine(scale_x_discrete(\"Temp Values\", \n                             labels = c(\"13°C\", \"18°C\", \"25°C\"))) %&gt;%\n  gf_theme(my_theme())\n\n\n\n\nIn both graphs, note the difference in the error-bar heights.\nUsing other packages\n\n\nUsing ggstatsplot\nUsing supernova\n\n\n\nThere is a very neat package called ggstatsplot6 that allows us to plot very comprehensive statistical graphs. Let us quickly do this:\n\nlibrary(ggstatsplot)\nfrogs_long %&gt;%\n  ggstatsplot::ggbetweenstats(x = TempFac, y = Time,\n                              title = \"ANOVA : Frogs Spawn Time vs Temperature Setting\")\n\n\n\n\n\n\nWe can also obtain crisp-looking anova tables from the new supernova package 7, which is based on the methods discussed in Judd et al.@sec–references\n\nlibrary(supernova)\nsupernova::supernova(frogs_anova)\nsupernova::pairwise(frogs_anova)\n\n\n\n Analysis of Variance Table (Type III SS)\n Model: Time ~ TempFac\n\n                               SS df      MS       F   PRE     p\n ----- --------------- | -------- -- ------- ------- ----- -----\n Model (error reduced) | 1020.933  2 510.467 385.897 .9312 .0000\n Error (from model)    |   75.400 57   1.323                    \n ----- --------------- | -------- -- ------- ------- ----- -----\n Total (empty model)   | 1096.333 59  18.582                    \n\n\n\n\n\n\n  group_1 group_2    diff pooled_se       q    df   lower  upper p_adj\n  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 18      13       -5.300     0.257 -20.608    57  -6.175 -4.425 .0000\n2 25      13      -10.100     0.257 -39.272    57 -10.975 -9.225 .0000\n3 25      18       -4.800     0.257 -18.664    57  -5.675 -3.925 .0000"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#iconify-flat-color-icons-workflow-workflow-anova-using-permutation-tests",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#iconify-flat-color-icons-workflow-workflow-anova-using-permutation-tests",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n Workflow: ANOVA using Permutation Tests",
    "text": "Workflow: ANOVA using Permutation Tests\nWe wish to establish the significance of the effect size due to each of the levels in TempFac. From the normality tests conducted earlier we see that except at one level of TempFac, the times are are not normally distributed. Hence we opt for a Permutation Test to check for significance of effect.\nAs remarked in Ernst8, the non-parametric permutation test can be both exact and also intuitively easier for students to grasp. Permutations are easily executed in R, using packages such as mosaic9.\nWe proceed with a Permutation Test for TempFac. We shuffle the levels (13, 18, 25) randomly between the Times and repeat the ANOVA test each time and calculate the F-statistic. The Null distribution is the distribution of the F-statistic over the many permutations and the p-value is given by the proportion of times the F-statistic equals or exceeds that observed.\nWe will use mosaic first, and also try with infer.\n\n\nUsing mosaic\nUsing infer\n\n\n\nmosaic offers an easy and intuitive way of doing a repeated permutation test, using the do() command. We will shuffle the TempFac factor to jumble up the Time observations, 4999 times. Each time we shuffle, we compute the F_statistic and record it. We then plot the 4999 F-statistics and compare that with the real-world observation of F-stat.\n\nobs_F_stat &lt;- \n  frogs_anova %&gt;% \n  broom::tidy() %&gt;% \n  select(statistic)\nobserved_mosaic &lt;- obs_F_stat$statistic[1]\nobserved_mosaic\n\n[1] 385.8966\n\nnull_dist_mosaic &lt;- do(4999) * aov(Time ~ shuffle(TempFac), \n                                    data = frogs_long)\nnull_dist_mosaic %&gt;% head()\n\n\n\n  \n\n\nnull_dist_mosaic %&gt;% drop_na() %&gt;% \n  select(F) %&gt;% \n  gf_histogram(data = ., ~ F, \n               fill = ~ F &gt;= observed_mosaic,\n               title = \"Null Distribution of ANOVA F-statistic\",\n               xlab = \"Simulated F values (using Permutation)\",\n               ylab = \"Count\") %&gt;% \n  gf_vline(xintercept = observed_mosaic) %&gt;%\n  gf_text(750 ~ observed_mosaic - 300, label = \"Observed F\") %&gt;%\n  gf_refine(scale_x_continuous(trans = \"log10\"),\n            scale_fill_discrete(name = \"Simulated F &gt; Observed F ?\")) %&gt;%\n  gf_theme(my_theme())\n\n\n\n\nThe Null distribution of the F_statistic under permutation shows it never crosses the real-world observed value, testifying as to the strength of the effect of TempFac on hatching Time. And the p-value is:\n\np_value &lt;- mean(null_dist_mosaic$F &gt;= observed_mosaic, na.rm = TRUE)\np_value\n\n[1] 0\n\n\n\n\nWe calculate the observed F-stat with infer, which also has a very direct, if verbose, syntax for doing permutation tests:\n\nobserved_infer &lt;- \n  frogs_long %&gt;% \n  specify(Time ~ TempFac) %&gt;% \n  hypothesise(null = \"independence\") %&gt;% \n  calculate(stat = \"F\")\nobserved_infer\n\n\n\n  \n\n\n\nWe see that the observed F-Statistic is of course \\(385.8966\\) as before. Now we use infer to generate a NULL distribution using permutation of the factor TempFac:\n\nnull_dist_infer &lt;- frogs_long %&gt;% \n  specify(Time ~ TempFac) %&gt;% \n  hypothesise(null = \"independence\") %&gt;% \n  generate(reps = 4999 , type = \"permute\") %&gt;% \n  calculate(stat = \"F\")\n\nhead(null_dist_infer)\n\n\n\n  \n\n\nnull_dist_infer %&gt;% \n  visualise(method = \"simulation\") +\n  shade_p_value(obs_stat = observed_infer$stat, direction = \"right\") + \n  scale_x_continuous(trans = \"log10\")\n\n\n\n\nAs seen, the infer based permutation test also shows that the permutationally generated F-statistics are nowhere near that which was observed. The effect of TempFac is very strong."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#iconify-fluent-mdl2-decision-solid-conclusions",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#iconify-fluent-mdl2-decision-solid-conclusions",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n Conclusions",
    "text": "Conclusions\nWe have discussed ANOVA as a means of modelling the effects of a Categorical variable on a Continuous (Quant) variable. ANOVA can be carried out using the standard formula aov when assumptions on distributions, variances, and independence are met. Permutation ANOVA tests can be carried out when these assumptions do not quite hold."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#sec--references",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#sec--references",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n References",
    "text": "References\n\nThe ANOVA tutorial at Our Coding Club.\nMichael Crawley, The R Book,second edition, 2013. Chapter 11.\nDavid C Howell, Permutation Tests for Factorial ANOVA Designs\nMarti Anderson, Permutation tests for univariate or multivariate analysis of variance and regression\nANOVA in R - Stats and R https://statsandr.com/blog/anova-in-r/\nJudd, Charles M., Gary H. McClelland, and Carey S. Ryan. 2017. “Introduction to Data Analysis.” In, 1–9. Routledge. https://doi.org/10.4324/9781315744131-1.\nPatil, I. (2021). Visualizations with statistical details: The ‘ggstatsplot’ approach. Journal of Open Source Software, 6(61), 3167, doi:10.21105/joss.03167"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#footnotes",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#footnotes",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "Footnotes",
    "text": "Footnotes\n\nThe ANOVA tutorial at Our Coding Club.↩︎\nSpinoza: Ethics Geometrically Demonstrated: spinoza1665.pdf (earlymoderntexts.com)↩︎\nhttps://www.openintro.org/go/?id=anova-supplement&referrer=/book/ahss/index.php↩︎\nPruim R, Kaplan DT, Horton NJ (2017). “The mosaic Package: Helping Students to ‘Think with Data’ Using R.” The R Journal, 9(1), 77–102. https://journal.r-project.org/archive/2017/RJ-2017-024/index.html.↩︎\nmosaic::xpf() gives both a graph and the probabilities.↩︎\nggplot2 Based Plots with Statistical Details • ggstatsplot https://indrajeetpatil.github.io/ggstatsplot/↩︎\nhttps://github.com/UCLATALL/supernova↩︎\nErnst, Michael D. 2004. “Permutation Methods: A Basis for Exact Inference.” Statistical Science 19 (4): 676–85. doi:10.1214/088342304000000396.↩︎\nPruim R, Kaplan DT, Horton NJ (2017). “The mosaic Package: Helping Students to ‘Think with Data’ Using R.” The R Journal, 9(1), 77–102. https://journal.r-project.org/archive/2017/RJ-2017-024/index.html.↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html",
    "title": "Tutorial: Permutation Testing for One Proportion",
    "section": "",
    "text": "We will use the datasets that are part of the resampledata package.1\n\nlibrary(tidyverse)\nlibrary(mosaic)\n\nlibrary(resampledata)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html#introduction",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html#introduction",
    "title": "Tutorial: Permutation Testing for One Proportion",
    "section": "",
    "text": "We will use the datasets that are part of the resampledata package.1\n\nlibrary(tidyverse)\nlibrary(mosaic)\n\nlibrary(resampledata)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html#case-study-1-verizon",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html#case-study-1-verizon",
    "title": "Tutorial: Permutation Testing for One Proportion",
    "section": "Case Study-1: Verizon",
    "text": "Case Study-1: Verizon\nDoes Verizon create a difference in Repair Times between ILEC and CLEC systems?\n\ndata(\"Verizon\")\ninspect(Verizon)\n\n\ncategorical variables:  \n   name  class levels    n missing\n1 Group factor      2 1687       0\n                                   distribution\n1 ILEC (98.6%), CLEC (1.4%)                    \n\nquantitative variables:  \n  name   class min   Q1 median   Q3   max     mean       sd    n missing\n1 Time numeric   0 0.75   3.63 7.35 191.6 8.522009 14.78848 1687       0\n\n\nDescribe the Variables!\nHypothesis Specification\nWrite the Null and Alternate hypotheses here.\nNull Distribution Computation\nVerizon Conclusion"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html#case-story-2-recidivism",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html#case-story-2-recidivism",
    "title": "Tutorial: Permutation Testing for One Proportion",
    "section": "Case Story-2: Recidivism",
    "text": "Case Story-2: Recidivism\nDo criminals released after a jail term commit crimes again? Does recidivism depend upon age?\n\ndata(\"Recidivism\")\ninspect(Recidivism)\n\n\ncategorical variables:  \n     name  class levels     n missing\n1  Gender factor      2 17019       3\n2     Age factor      5 17019       3\n3   Age25 factor      2 17019       3\n4    Race factor     10 16988      34\n5 Offense factor      2 17022       0\n6   Recid factor      2 17022       0\n7    Type factor      3 17022       0\n                                   distribution\n1 M (87.7%), F (12.3%)                         \n2 25-34 (36.6%), 35-44 (23.7%) ...             \n3 Over 25 (81.9%), Under 25 (18.1%)            \n4 White-NonHispanic (67%) ...                  \n5 Felony (80.6%), Misdemeanor (19.4%)          \n6 No (68.4%), Yes (31.6%)                      \n7 No Recidivism (68.4%), New (20.2%) ...       \n\nquantitative variables:  \n  name   class min  Q1 median  Q3  max     mean       sd    n missing\n1 Days integer   0 241    418 687 1095 473.3275 283.1393 5386   11636\n\n\nDescribe the variables!\nHypothesis Specification\nLet us see if the incidence of recidivism is dependent upon whether a person is aged less than or more than 25 years. Write the Null and Alternate hypotheses here.\n\nRecidivism\n\n\n\n  \n\n\n\nAlso, the variable Recid is a factor variable coded “Yes” or “No”. We ought to convert it to a numeric variable of 1’s and 0’s. Why?\nNull Distribution for Recidivism\nRecidivism Conclusion\nCase Study #3: Flight Delays\nLaGuardia Airport (LGA) is one of three major airports that serves the New York City metropolitan area. In 2008, over 23 million passengers and over 375 000 planes flew in or out of LGA. United Airlines and America Airlines are two major airlines that schedule services at LGA. The data set FlightDelays contains information on all 4029 departures of these two airlines from LGA during May and June 2009.\n\ndata(\"FlightDelays\")\ninspect(FlightDelays)\n\n\ncategorical variables:  \n         name  class levels    n missing\n1     Carrier factor      2 4029       0\n2 Destination factor      7 4029       0\n3  DepartTime factor      5 4029       0\n4         Day factor      7 4029       0\n5       Month factor      2 4029       0\n6   Delayed30 factor      2 4029       0\n                                   distribution\n1 AA (72.1%), UA (27.9%)                       \n2 ORD (44.3%), DFW (22.8%), MIA (15.1%) ...    \n3 8-Noon (26.1%), Noon-4pm (26%) ...           \n4 Fri (15.8%), Mon (15.6%), Tue (15.6%) ...    \n5 June (50.4%), May (49.6%)                    \n6 No (85.2%), Yes (14.8%)                      \n\nquantitative variables:  \n          name   class min   Q1 median   Q3  max      mean         sd    n\n1           ID integer   1 1008   2015 3022 4029 2015.0000 1163.21645 4029\n2     FlightNo integer  71  371    691  787 2255  827.1035  551.30939 4029\n3 FlightLength integer  68  155    163  228  295  185.3011   41.78783 4029\n4        Delay integer -19   -6     -3    5  693   11.7379   41.63050 4029\n  missing\n1       0\n2       0\n3       0\n4       0\n\n\nThe variables in the FlightDelays dataset are:\nHypothesis Specification\nLet us compute the proportion of times that each carrier’s flights was delayed more than 20 min. We will conduct a two-sided test to see if the difference in these proportions is statistically significant.\nNull Distribution for FlightDelays\n\nwhich is very small. Hence we reject the null Hypothesis that there is no difference between carriers on delay times."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html#references",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html#references",
    "title": "Tutorial: Permutation Testing for One Proportion",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html#footnotes",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html#footnotes",
    "title": "Tutorial: Permutation Testing for One Proportion",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://github.com/rudeboybert/resampledata↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/190-TwoProp/two-props.html",
    "href": "content/courses/Analytics/Inference/Modules/190-TwoProp/two-props.html",
    "title": "🃏 Inferences Test for Two Proportions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(ggmosaic) # plotting mosaic plots for Categorical Data\n\n### Dataset from Chihara and Hesterberg's book (Second Edition)\nlibrary(resampledata)\nlibrary(vcd)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/190-TwoProp/two-props.html#iconify-noto-v1-package-setting-up-r-packages",
    "href": "content/courses/Analytics/Inference/Modules/190-TwoProp/two-props.html#iconify-noto-v1-package-setting-up-r-packages",
    "title": "🃏 Inferences Test for Two Proportions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(ggmosaic) # plotting mosaic plots for Categorical Data\n\n### Dataset from Chihara and Hesterberg's book (Second Edition)\nlibrary(resampledata)\nlibrary(vcd)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/190-TwoProp/two-props.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "href": "content/courses/Analytics/Inference/Modules/190-TwoProp/two-props.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "title": "🃏 Inferences Test for Two Proportions",
    "section": "\n Introduction",
    "text": "Introduction\nWe saw from the diagram created by Allen Downey that there is only one test! We will now use this philosophy to develop a technique that allows us to mechanize several Statistical Models in that way, with nearly identical code.\nWe will use two packages in R, mosaic and the relatively new infer package, to develop our intuition for what are called permutation based statistical tests."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/190-TwoProp/two-props.html#inference-for-two-or-more-proportions",
    "href": "content/courses/Analytics/Inference/Modules/190-TwoProp/two-props.html#inference-for-two-or-more-proportions",
    "title": "🃏 Inferences Test for Two Proportions",
    "section": "Inference for Two or More Proportions",
    "text": "Inference for Two or More Proportions\nLet us try a dataset with Qualitative / Categorical data. This is the General Social Survey GSS dataset, and we have people with different levels of Education stating their opinion on the Death Penalty. We want to know if these two Categorical variables have a correlation, i.e. can the opinions in favour of the Death Penalty be explained by the Education level?\nSince data is Categorical ( both variables ), we need to take counts in a table, and then implement a chi-square test. In the test, we will permute the Education variable to see if we can see how significant its effect size is.\n\ndata(GSS2002)\ninspect(GSS2002)\n\n\ncategorical variables:  \n            name  class levels    n missing\n1         Region factor      7 2765       0\n2         Gender factor      2 2765       0\n3           Race factor      3 2765       0\n4      Education factor      5 2760       5\n5        Marital factor      5 2765       0\n6       Religion factor     13 2746      19\n7          Happy factor      3 1369    1396\n8         Income factor     24 1875     890\n9       PolParty factor      8 2729      36\n10      Politics factor      7 1331    1434\n11     Marijuana factor      2  851    1914\n12  DeathPenalty factor      2 1308    1457\n13        OwnGun factor      3  924    1841\n14        GunLaw factor      2  916    1849\n15 SpendMilitary factor      3 1324    1441\n16     SpendEduc factor      3 1343    1422\n17      SpendEnv factor      3 1322    1443\n18      SpendSci factor      3 1266    1499\n19        Pres00 factor      5 1749    1016\n20      Postlife factor      2 1211    1554\n                                    distribution\n1  North Central (24.7%) ...                    \n2  Female (55.6%), Male (44.4%)                 \n3  White (79.1%), Black (14.8%) ...             \n4  HS (53.8%), Bachelors (16.1%) ...            \n5  Married (45.9%), Never Married (25.6%) ...   \n6  Protestant (53.2%), Catholic (24.5%) ...     \n7  Pretty happy (57.3%) ...                     \n8  40000-49999 (9.1%) ...                       \n9  Ind (19.3%), Not Str Dem (18.9%) ...         \n10 Moderate (39.2%), Conservative (15.8%) ...   \n11 Not legal (64%), Legal (36%)                 \n12 Favor (68.7%), Oppose (31.3%)                \n13 No (65.5%), Yes (33.5%) ...                  \n14 Favor (80.5%), Oppose (19.5%)                \n15 About right (46.5%) ...                      \n16 Too little (73.9%) ...                       \n17 Too little (60%) ...                         \n18 About right (49.7%) ...                      \n19 Bush (50.6%), Gore (44.7%) ...               \n20 Yes (80.5%), No (19.5%)                      \n\nquantitative variables:  \n  name   class min  Q1 median   Q3  max mean       sd    n missing\n1   ID integer   1 692   1383 2074 2765 1383 798.3311 2765       0\n\n\nNote how all variables are Categorical !! Education has five levels, and of course DeathPenalty has three:\n\nGSS2002 %&gt;% count(Education)\n\n\n\n  \n\n\nGSS2002 %&gt;% count(DeathPenalty)\n\n\n\n  \n\n\n\nLet us drop NA entries in Education and Death Penalty and set up a Contingency Table.\n\ngss2002 &lt;- GSS2002 %&gt;% \n  dplyr::select(Education, DeathPenalty) %&gt;% \n  tidyr::drop_na(., c(Education, DeathPenalty))\n\n\n\ngss_table &lt;- tally(DeathPenalty ~ Education, data = gss2002)\ngss_table %&gt;% \n  addmargins()\n\n            Education\nDeathPenalty Left HS   HS Jr Col Bachelors Graduate  Sum\n      Favor      117  511     71       135       64  898\n      Oppose      72  200     16        71       50  409\n      Sum        189  711     87       206      114 1307"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/190-TwoProp/two-props.html#contingency-table-plots",
    "href": "content/courses/Analytics/Inference/Modules/190-TwoProp/two-props.html#contingency-table-plots",
    "title": "🃏 Inferences Test for Two Proportions",
    "section": "Contingency Table Plots",
    "text": "Contingency Table Plots\nThe Contingency Table can be plotted, as we have seen, using a mosaic plot using several packages:\n\n\nUsing ggformula\nUsing vcd\nUsing ggmosaic\n\n\n\nNeed a little more work, to convert the Contigency Table into a tibble:\n\n# https://stackoverflow.com/questions/19233365/how-to-create-a-marimekko-mosaic-plot-in-ggplot2\n\ngss_summary &lt;- gss2002 %&gt;%\n  mutate(\n    Education = factor(\n      Education,\n      levels = c(\"Bachelors\", \"Graduate\", \"Jr Col\", \"HS\", \"Left HS\"),\n      labels = c(\"Bachelors\", \"Graduate\", \"Jr Col\", \"HS\", \"Left HS\")\n    ),\n    DeathPenalty = as.factor(DeathPenalty)\n  ) %&gt;%\n  group_by(Education, DeathPenalty) %&gt;%\n  summarise(count = n()) %&gt;% # This is good for a chisq test\n  \n  # Add two more columns to facilitate mosaic/Marrimekko Plot\n  # \n  mutate(edu_count = sum(count), \n         edu_prop = count / sum(count)) %&gt;%\n  ungroup() \n\ngf_col(edu_prop ~ Education, data = gss_summary,\n       width = ~ edu_count, \n       fill = ~ DeathPenalty,\n       stat = \"identity\", \n       position = \"fill\", \n       color = \"black\") %&gt;% \n  \n  gf_text(edu_prop ~ Education, \n          label = ~ scales::percent(edu_prop),\n          position = position_stack(vjust = 0.5)) %&gt;% \n  \n  gf_facet_grid(~ Education, \n                scales = \"free_x\", \n                space = \"free_x\") %&gt;% \n  \n  gf_theme(scale_fill_manual(values = c(\"orangered\", \"palegreen3\"))) %&gt;% \n  gf_theme(theme_void())\n\n\n\n\n\n\n\nvcd::mosaic(gss_table, gp = shading_hsv)\n\n\n\n\n\n\n\n#library(ggmosaic)\n\nggplot(data = gss2002) +\n  geom_mosaic(aes(x = product(DeathPenalty, Education), \n                  fill = DeathPenalty))"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/190-TwoProp/two-props.html#observed-statistic-the-x2-metric",
    "href": "content/courses/Analytics/Inference/Modules/190-TwoProp/two-props.html#observed-statistic-the-x2-metric",
    "title": "🃏 Inferences Test for Two Proportions",
    "section": "Observed Statistic: the \\(X^2\\) metric",
    "text": "Observed Statistic: the \\(X^2\\) metric\nWhen there are multiple proportions involved, the \\(X^2\\) test is what is used.\n\n\nIntuitive Explanation\nCode\n\n\n\nLet us look at the Contingency Table that we have:\n\n\n\nContigency Table\n\n\nLeft HS\nHS\nJr Col\nBachelors\nGraduate\nSum\n\n\n\nFavor\n117\n511\n71\n135\n64\n898\n\n\nOppose\n72\n200\n16\n71\n50\n409\n\n\nSum\n189\n711\n87\n206\n114\n1307\n\n\n\n\n\nIn the chi-square test, we check whether the two ( or more ) categorical variables are independent. To do this we perform a simple check on the Contingency Table. We first re-compute the totals in each row and column, based on what we could expect if there was independence (NULL Hypothesis). If the two variables were independent, then there should be no difference between real and expected scores.\nHow do we know what scores to expect?\nConsider the entry in location (1,1): 117. The number of expected entries there is probability of an entry landing in that square times the total number of entries:\n\n\\[\\begin{align}\n\n\\text{Expected Value at location[1,1]}\n&= p_{row_1} * p_{col_1} * \\text{Total Scores}\\\\\\\n&= \\frac{\\text{Row-1-Total}}{\\text{Total Scores}} * \\frac{\\text{Col-1-Total}}{\\text{Total Scores}} * \\text{Total Scores}\\\\\\\n&= \\frac{898}{1307} * \\frac{189}{1307} * 1307\\\\\\\n&= 130\n\n\n\\end{align}\\]\n\nProceeding in this way for all the 15 entries in the Contingency Table, we get the “Expected” Contingency Table. Here are both tables for comparison:\n\n\n\nExpected Contigency Table\n\n\nLeft HS\nHS\nJr Col\nBachelors\nGraduate\nSum\n\n\n\nFavor\n130\n489\n60\n142\n78\n898\n\n\nOppose\n59\n222\n27\n64\n36\n409\n\n\nSum\n189\n711\n87\n206\n114\n1307\n\n\n\n\n\n\n\n\nActual Contigency Table\n\n\nLeft HS\nHS\nJr Col\nBachelors\nGraduate\nSum\n\n\n\nFavor\n117\n511\n71\n135\n64\n898\n\n\nOppose\n72\n200\n16\n71\n50\n409\n\n\nSum\n189\n711\n87\n206\n114\n1307\n\n\n\n\n\nThe \\(X^2\\) statistic is sum of squared differences between Observed and Expected scores, scaled by the Expected Scores. For location [1,1] this would be: \\((117-130)^2/130\\). Do try to compute all of these and the \\(X^2\\) statistic by hand !!\n\n\nLet us now perform the base chisq test: We need a table and then the chisq test:\n\n# gss_table &lt;- tally(DeathPenalty ~ Education, data = gss2002)\n# gss_table\n\n# Get the observed chi-square statistic\nobservedChi2 &lt;- mosaic::chisq(tally(DeathPenalty ~ Education, data = gss2002))\nobservedChi2\n\nX.squared \n 23.45093 \n\n# Actual chi-square test\nstats::chisq.test(tally(DeathPenalty ~ Education, data = gss2002))\n\n\n    Pearson's Chi-squared test\n\ndata:  tally(DeathPenalty ~ Education, data = gss2002)\nX-squared = 23.451, df = 4, p-value = 0.0001029\n\n\nWe see that our observed \\(X^2 = 23.45\\).\n\n\n\nHypotheses Definition\nWhat would our Hypotheses be?\n\\(H_0: \\text{Education does not affect votes for Death Penalty}\\) \\(H_a: \\text{Education affects votes for Death Penalty}\\)\nPermutation Test for Education\n\nWe should now repeat the test with permutations on Education:\n\nnull_chisq &lt;- do(10000) * \n  chisq.test(tally(DeathPenalty ~ shuffle(Education), \n                   data = gss2002))\n\nhead(null_chisq)\n\n\n\n  \n\n\ngf_histogram( ~ X.squared, data = null_chisq) %&gt;% \n  \n  gf_vline(xintercept = observedChi2, \n           color = \"red\") %&gt;% \n  gf_theme(theme = theme_classic())\n\n\n\nprop1(~ X.squared &gt;= observedChi2, data = null_chisq)\n\n prop_TRUE \n0.00029997 \n\n\nThe p-value is well below our threshold of \\(0.05\\), so we would conclude that Education has a significant effect on DeathPenalty opinion!"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/190-TwoProp/two-props.html#conclusion",
    "href": "content/courses/Analytics/Inference/Modules/190-TwoProp/two-props.html#conclusion",
    "title": "🃏 Inferences Test for Two Proportions",
    "section": "Conclusion",
    "text": "Conclusion\nWhy would a permutation test be a good idea here?\nIn our basic \\(X^2\\) test, we calculate the test statistic of \\(X^2\\) and look up a theoretical null distribution for that statistic, and see how unlikely our observed value is.\nWith a permutation test, there are no assumptions of the null distribution: this is computed based on real data. We note in passing that, in this case, since the number of cases in each cell of the Contingency Table are fairly high ( &gt;= 5) the resulting NULL distribution is of the \\(X^2\\) variety."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/190-TwoProp/two-props.html#references",
    "href": "content/courses/Analytics/Inference/Modules/190-TwoProp/two-props.html#references",
    "title": "🃏 Inferences Test for Two Proportions",
    "section": "References",
    "text": "References\n\nOpenIntro Modern Statistics: Chapter 17\nExploring the underlying theory of the chi-square test through simulation - part 1 https://www.rdatagen.net/post/a-little-intuition-and-simulation-behind-the-chi-square-test-of-independence/\n\nExploring the underlying theory of the chi-square test through simulation - part 2 https://www.rdatagen.net/post/a-little-intuition-and-simulation-behind-the-chi-square-test-of-independence-part-2/"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "",
    "text": "R Tutorial  \n\n\n  Datasets"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#fa-folder-open-slides-and-tutorials",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#fa-folder-open-slides-and-tutorials",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "",
    "text": "R Tutorial  \n\n\n  Datasets"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#iconify-noto-v1-package-setting-up-r-packages",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#iconify-noto-v1-package-setting-up-r-packages",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nset.seed(123456) # TO get repeatable graphs!\n\nlibrary(tidyverse) # Data Processing in R\nlibrary(mosaic) # Our workhorse for stats, sampling\nlibrary(skimr) # Good to Examine data\nlibrary(ggformula) # Formula interface for graphs\n\n# load the NHANES data library\nlibrary(NHANES)\n\nlibrary(infer) # tidy workflow for statistical inference\nlibrary(gt) # Create tidy tables to report data\nlibrary(cowplot) # ggplot themes and stacking of plots"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#iconify-clarity-group-solid-what-is-a-population",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#iconify-clarity-group-solid-what-is-a-population",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "\n What is a Population?",
    "text": "What is a Population?\nA population is a collection of individuals or observations we are interested in. This is also commonly denoted as a study population. We mathematically denote the population’s size using upper-case N.\nA population parameter is some numerical summary about the population that is unknown but you wish you knew. For example, when this quantity is a mean like the average height of all Bangaloreans, the population parameter of interest is the population mean.\nA census is an exhaustive enumeration or counting of all N individuals in the population. We do this in order to compute the population parameter’s value exactly. Of note is that as the number N of individuals in our population increases, conducting a census gets more expensive (in terms of time, energy, and money).\n\n\n\n\n\n\n Parameters\n\n\n\nPopulations Parameters are usually indicated by Greek Letters."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#iconify-game-icons-card-pickup-what-is-a-sample",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#iconify-game-icons-card-pickup-what-is-a-sample",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "\n What is a Sample?",
    "text": "What is a Sample?\nSampling is the act of collecting a sample from the population, which we generally do when we can’t perform a census. We mathematically denote the sample size using lower case n, as opposed to upper case N which denotes the population’s size. Typically the sample size n is much smaller than the population size N. Thus sampling is a much cheaper alternative than performing a census.\nA sample statistic, also known as a point estimate, is a summary statistic like a mean or standard deviation that is computed from a sample.\n\n\n\n\n\n\nWhy do we sample?\n\n\n\nBecause we cannot conduct a census ( not always ) — and sometimes we won’t even know how big the population is — we take samples. And we still want to do useful work for/with the population, after estimating its parameters, an act of generalizing from sample to population. So the question is, can we estimate useful parameters of the population, using just samples? Can point estimates serve as useful guides to population parameters?\nThis act of generalizing from sample to population is at the heart of statistical inference.\n\n\n\n\n\n\n\n\nAn Alliterative Mnemonic\n\n\n\nNOTE: there is an alliterative mnemonic here: Samples have Statistics; Populations have Parameters."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#iconify-fluent-mdl2-test-parameter-population-parameters-and-sample-statistics",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#iconify-fluent-mdl2-test-parameter-population-parameters-and-sample-statistics",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "\n Population Parameters and Sample Statistics",
    "text": "Population Parameters and Sample Statistics\n\nParameters and Statistics\n\n\nPopulation Parameter\nSample Statistic\n\n\n\nMean\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\n\nStandard Deviation\n\\(\\sigma\\)\ns\n\n\nProportion\np\n\\(\\hat{p}\\)\n\n\nCorrelation\n\\(\\rho\\)\nr\n\n\nSlope (Regression)\n\\(\\beta_1\\)\n\\(b_1\\)\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.1. What is the mean commute time for workers in a particular city?\nA.1. The parameter is the mean commute time \\(\\mu\\) for a population containing all workers who work in the city. We estimate it using \\(\\bar{x}\\), the mean of the random sample of people who work in the city.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.2. What is the correlation between the size of dinner bills and the size of tips at a restaurant?\nA.2. The parameter is \\(\\rho\\) , the correlation between bill amount and tip size for a population of all dinner bills at that restaurant. We estimate it using r, the correlation from a random sample of dinner bills.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.3. How much difference is there in the proportion of 30 to 39-year-old residents who have only a cell phone (no land line phone) compared to 50 to 59-year-olds in the country?\nA.3. The population is all citizens of the country, and the parameter is \\(p_1 - p_2\\), the difference in proportion of 30 to 39-year-old residents who have only a cell phone (\\(p_1\\)) and the proportion with the same property among all 50 to 59-year olds (\\(p_2\\)). We estimate it using (\\(\\hat{p_1} - \\hat{p_2}\\)), the difference in sample proportions computed from random samples taken from each group.\n\n\nSample statistics vary and in the following we will estimate this uncertainty and decide how reliable they might be as estimates of population parameters."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#iconify-pajamas-issue-type-test-case-case-study-1-sampling-the-nhanes-dataset",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#iconify-pajamas-issue-type-test-case-case-study-1-sampling-the-nhanes-dataset",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "\n Case Study #1: Sampling the NHANES dataset",
    "text": "Case Study #1: Sampling the NHANES dataset\nWe will first execute some samples from a known dataset. We load up the NHANES dataset and inspect it.\n\ndata(\"NHANES\")\n#mosaic::inspect(NHANES)\nskimr::skim(NHANES)\n\n\n\n\nData summary\n\n\nName\nNHANES\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n76\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n31\n\n\nnumeric\n45\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nSurveyYr\n0\n1.00\nFALSE\n2\n200: 5000, 201: 5000\n\n\nGender\n0\n1.00\nFALSE\n2\nfem: 5020, mal: 4980\n\n\nAgeDecade\n333\n0.97\nFALSE\n8\n40: 1398, 0-: 1391, 10: 1374, 20: 1356\n\n\nRace1\n0\n1.00\nFALSE\n5\nWhi: 6372, Bla: 1197, Mex: 1015, Oth: 806\n\n\nRace3\n5000\n0.50\nFALSE\n6\nWhi: 3135, Bla: 589, Mex: 480, His: 350\n\n\nEducation\n2779\n0.72\nFALSE\n5\nSom: 2267, Col: 2098, Hig: 1517, 9 -: 888\n\n\nMaritalStatus\n2769\n0.72\nFALSE\n6\nMar: 3945, Nev: 1380, Div: 707, Liv: 560\n\n\nHHIncome\n811\n0.92\nFALSE\n12\nmor: 2220, 750: 1084, 250: 958, 350: 863\n\n\nHomeOwn\n63\n0.99\nFALSE\n3\nOwn: 6425, Ren: 3287, Oth: 225\n\n\nWork\n2229\n0.78\nFALSE\n3\nWor: 4613, Not: 2847, Loo: 311\n\n\nBMICatUnder20yrs\n8726\n0.13\nFALSE\n4\nNor: 805, Obe: 221, Ove: 193, Und: 55\n\n\nBMI_WHO\n397\n0.96\nFALSE\n4\n18.: 2911, 30.: 2751, 25.: 2664, 12.: 1277\n\n\nDiabetes\n142\n0.99\nFALSE\n2\nNo: 9098, Yes: 760\n\n\nHealthGen\n2461\n0.75\nFALSE\n5\nGoo: 2956, Vgo: 2508, Fai: 1010, Exc: 878\n\n\nLittleInterest\n3333\n0.67\nFALSE\n3\nNon: 5103, Sev: 1130, Mos: 434\n\n\nDepressed\n3327\n0.67\nFALSE\n3\nNon: 5246, Sev: 1009, Mos: 418\n\n\nSleepTrouble\n2228\n0.78\nFALSE\n2\nNo: 5799, Yes: 1973\n\n\nPhysActive\n1674\n0.83\nFALSE\n2\nYes: 4649, No: 3677\n\n\nTVHrsDay\n5141\n0.49\nFALSE\n7\n2_h: 1275, 1_h: 884, 3_h: 836, 0_t: 638\n\n\nCompHrsDay\n5137\n0.49\nFALSE\n7\n0_t: 1409, 0_h: 1073, 1_h: 1030, 2_h: 589\n\n\nAlcohol12PlusYr\n3420\n0.66\nFALSE\n2\nYes: 5212, No: 1368\n\n\nSmokeNow\n6789\n0.32\nFALSE\n2\nNo: 1745, Yes: 1466\n\n\nSmoke100\n2765\n0.72\nFALSE\n2\nNo: 4024, Yes: 3211\n\n\nSmoke100n\n2765\n0.72\nFALSE\n2\nNon: 4024, Smo: 3211\n\n\nMarijuana\n5059\n0.49\nFALSE\n2\nYes: 2892, No: 2049\n\n\nRegularMarij\n5059\n0.49\nFALSE\n2\nNo: 3575, Yes: 1366\n\n\nHardDrugs\n4235\n0.58\nFALSE\n2\nNo: 4700, Yes: 1065\n\n\nSexEver\n4233\n0.58\nFALSE\n2\nYes: 5544, No: 223\n\n\nSameSex\n4232\n0.58\nFALSE\n2\nNo: 5353, Yes: 415\n\n\nSexOrientation\n5158\n0.48\nFALSE\n3\nHet: 4638, Bis: 119, Hom: 85\n\n\nPregnantNow\n8304\n0.17\nFALSE\n3\nNo: 1573, Yes: 72, Unk: 51\n\n\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nID\n0\n1.00\n61944.64\n5871.17\n51624.00\n56904.50\n62159.50\n67039.00\n71915.00\n▇▇▇▇▇\n\n\nAge\n0\n1.00\n36.74\n22.40\n0.00\n17.00\n36.00\n54.00\n80.00\n▇▇▇▆▅\n\n\nAgeMonths\n5038\n0.50\n420.12\n259.04\n0.00\n199.00\n418.00\n624.00\n959.00\n▇▇▇▆▃\n\n\nHHIncomeMid\n811\n0.92\n57206.17\n33020.28\n2500.00\n30000.00\n50000.00\n87500.00\n100000.00\n▃▆▃▁▇\n\n\nPoverty\n726\n0.93\n2.80\n1.68\n0.00\n1.24\n2.70\n4.71\n5.00\n▅▅▃▃▇\n\n\nHomeRooms\n69\n0.99\n6.25\n2.28\n1.00\n5.00\n6.00\n8.00\n13.00\n▂▆▇▂▁\n\n\nWeight\n78\n0.99\n70.98\n29.13\n2.80\n56.10\n72.70\n88.90\n230.70\n▂▇▂▁▁\n\n\nLength\n9457\n0.05\n85.02\n13.71\n47.10\n75.70\n87.00\n96.10\n112.20\n▁▃▆▇▃\n\n\nHeadCirc\n9912\n0.01\n41.18\n2.31\n34.20\n39.58\n41.45\n42.92\n45.40\n▁▂▇▇▅\n\n\nHeight\n353\n0.96\n161.88\n20.19\n83.60\n156.80\n166.00\n174.50\n200.40\n▁▁▁▇▂\n\n\nBMI\n366\n0.96\n26.66\n7.38\n12.88\n21.58\n25.98\n30.89\n81.25\n▇▆▁▁▁\n\n\nPulse\n1437\n0.86\n73.56\n12.16\n40.00\n64.00\n72.00\n82.00\n136.00\n▂▇▃▁▁\n\n\nBPSysAve\n1449\n0.86\n118.15\n17.25\n76.00\n106.00\n116.00\n127.00\n226.00\n▃▇▂▁▁\n\n\nBPDiaAve\n1449\n0.86\n67.48\n14.35\n0.00\n61.00\n69.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nBPSys1\n1763\n0.82\n119.09\n17.50\n72.00\n106.00\n116.00\n128.00\n232.00\n▂▇▂▁▁\n\n\nBPDia1\n1763\n0.82\n68.28\n13.78\n0.00\n62.00\n70.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys2\n1647\n0.84\n118.48\n17.49\n76.00\n106.00\n116.00\n128.00\n226.00\n▃▇▂▁▁\n\n\nBPDia2\n1647\n0.84\n67.66\n14.42\n0.00\n60.00\n68.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys3\n1635\n0.84\n117.93\n17.18\n76.00\n106.00\n116.00\n126.00\n226.00\n▃▇▂▁▁\n\n\nBPDia3\n1635\n0.84\n67.30\n14.96\n0.00\n60.00\n68.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nTestosterone\n5874\n0.41\n197.90\n226.50\n0.25\n17.70\n43.82\n362.41\n1795.60\n▇▂▁▁▁\n\n\nDirectChol\n1526\n0.85\n1.36\n0.40\n0.39\n1.09\n1.29\n1.58\n4.03\n▅▇▂▁▁\n\n\nTotChol\n1526\n0.85\n4.88\n1.08\n1.53\n4.11\n4.78\n5.53\n13.65\n▂▇▁▁▁\n\n\nUrineVol1\n987\n0.90\n118.52\n90.34\n0.00\n50.00\n94.00\n164.00\n510.00\n▇▅▂▁▁\n\n\nUrineFlow1\n1603\n0.84\n0.98\n0.95\n0.00\n0.40\n0.70\n1.22\n17.17\n▇▁▁▁▁\n\n\nUrineVol2\n8522\n0.15\n119.68\n90.16\n0.00\n52.00\n95.00\n171.75\n409.00\n▇▆▃▂▁\n\n\nUrineFlow2\n8524\n0.15\n1.15\n1.07\n0.00\n0.48\n0.76\n1.51\n13.69\n▇▁▁▁▁\n\n\nDiabetesAge\n9371\n0.06\n48.42\n15.68\n1.00\n40.00\n50.00\n58.00\n80.00\n▁▂▆▇▂\n\n\nDaysPhysHlthBad\n2468\n0.75\n3.33\n7.40\n0.00\n0.00\n0.00\n3.00\n30.00\n▇▁▁▁▁\n\n\nDaysMentHlthBad\n2466\n0.75\n4.13\n7.83\n0.00\n0.00\n0.00\n4.00\n30.00\n▇▁▁▁▁\n\n\nnPregnancies\n7396\n0.26\n3.03\n1.80\n1.00\n2.00\n3.00\n4.00\n32.00\n▇▁▁▁▁\n\n\nnBabies\n7584\n0.24\n2.46\n1.32\n0.00\n2.00\n2.00\n3.00\n12.00\n▇▅▁▁▁\n\n\nAge1stBaby\n8116\n0.19\n22.65\n4.77\n14.00\n19.00\n22.00\n26.00\n39.00\n▆▇▅▂▁\n\n\nSleepHrsNight\n2245\n0.78\n6.93\n1.35\n2.00\n6.00\n7.00\n8.00\n12.00\n▁▅▇▁▁\n\n\nPhysActiveDays\n5337\n0.47\n3.74\n1.84\n1.00\n2.00\n3.00\n5.00\n7.00\n▇▇▃▅▅\n\n\nTVHrsDayChild\n9347\n0.07\n1.94\n1.43\n0.00\n1.00\n2.00\n3.00\n6.00\n▇▆▂▂▂\n\n\nCompHrsDayChild\n9347\n0.07\n2.20\n2.52\n0.00\n0.00\n1.00\n6.00\n6.00\n▇▁▁▁▃\n\n\nAlcoholDay\n5086\n0.49\n2.91\n3.18\n1.00\n1.00\n2.00\n3.00\n82.00\n▇▁▁▁▁\n\n\nAlcoholYear\n4078\n0.59\n75.10\n103.03\n0.00\n3.00\n24.00\n104.00\n364.00\n▇▁▁▁▁\n\n\nSmokeAge\n6920\n0.31\n17.83\n5.33\n6.00\n15.00\n17.00\n19.00\n72.00\n▇▂▁▁▁\n\n\nAgeFirstMarij\n7109\n0.29\n17.02\n3.90\n1.00\n15.00\n16.00\n19.00\n48.00\n▁▇▂▁▁\n\n\nAgeRegMarij\n8634\n0.14\n17.69\n4.81\n5.00\n15.00\n17.00\n19.00\n52.00\n▂▇▁▁▁\n\n\nSexAge\n4460\n0.55\n17.43\n3.72\n9.00\n15.00\n17.00\n19.00\n50.00\n▇▅▁▁▁\n\n\nSexNumPartnLife\n4275\n0.57\n15.09\n57.85\n0.00\n2.00\n5.00\n12.00\n2000.00\n▇▁▁▁▁\n\n\nSexNumPartYear\n5072\n0.49\n1.34\n2.78\n0.00\n1.00\n1.00\n1.00\n69.00\n▇▁▁▁▁\n\n\n\n\n\n\n\nLet us create a NHANES (sub)-dataset without duplicated IDs and only adults:\n\nNHANES &lt;-\n  NHANES %&gt;%\n  distinct(ID, .keep_all = TRUE) \n\n#create a dataset of only adults\nNHANES_adult &lt;-  \n  NHANES %&gt;%\n  filter(Age &gt;= 18) %&gt;% drop_na(Height)\n\n\n An “Assumed” Population\n\n\n\n\n\n\nAn “Assumed” Population\n\n\n\nFor now, we will treat this dataset as our Population. So each variable in the dataset is a population for that particular quantity/category, with appropriate population parameters such as means, sd-s, and proportions.\n\n\nLet us calculate the population parameters for the Height data from our “assumed” population:\n\n# NHANES_adult is assumed population\npop_mean_height &lt;- mean(~ Height, data = NHANES_adult)\npop_sd_height &lt;- sd(~ Height, data = NHANES_adult)\n\npop_mean_height\n\n[1] 168.3497\n\npop_sd_height\n\n[1] 10.15705\n\n\n\n Sampling\nNow, we will sample ONCE from the NHANES Height variable. Let us take a sample of sample size 50. We will compare sample statistics with population parameters on the basis of this ONE sample of 50:\n\nsample_height &lt;- sample(NHANES_adult, size = 50) %&gt;% \n  select(Height)\nsample_height\nsample_mean_height &lt;- mean(~ Height, data = sample_height)\nsample_mean_height\n# Plotting the histogram of this sample\nsample_height %&gt;% \n  gf_histogram(~ Height, bins = 10) %&gt;% \n  \n  gf_vline(xintercept = sample_mean_height, \n           color = \"red\") %&gt;% \n  \n  gf_vline(xintercept = pop_mean_height, \n           colour = \"blue\") %&gt;% \n  \n  gf_label(7 ~ (pop_mean_height + 8), \n          label = \"Population Mean Height\", \n          color = \"blue\") %&gt;% \n  \n  gf_label(7 ~ (sample_mean_height - 8), \n          label = \"Sample Mean Height\", color = \"red\") %&gt;% \n  gf_theme(theme_classic())\n\n\n\n\n\n  \n\n\n\n[1] 165.866\n\n\n\n\n\n\n\n\n Repeated Samples and Sample Means\nOK, so the sample_mean_height is not too far from the pop_mean_height. Is this always true? Let us check: we will create 500 samples each of size 50. And calculate their mean as the sample statistic, giving us a data frame containing 500 sample means. We will then see if these 500 means lie close to the pop_mean_height:\n\nsample_height_500 &lt;- do(500) * {\n  sample(NHANES_adult, size = 50) %&gt;%\n    select(Height) %&gt;%\n    summarise(\n      sample_mean_500 = mean(Height),\n      sample_min_500 = min(Height),\n      sample_max_500 = max(Height))\n}\n\nhead(sample_height_500)\ndim(sample_height_500)\nsample_height_500 %&gt;%\n  gf_point(.index ~ sample_mean_500, color = \"red\",\n           title = \"Sample Means are close to the Population Mean\",\n           subtitle = \"Sample Means are Random!\") %&gt;%\n  \n  gf_segment(\n    .index + .index ~ sample_min_500 + sample_max_500,\n    color = \"red\",\n    size = 0.3,\n    alpha = 0.3,\n    ylab = \"Sample Index (1-500)\",\n    xlab = \"Sample Means\"\n  ) %&gt;%\n  \n  gf_vline(xintercept = ~ pop_mean_height, \n           color = \"blue\") %&gt;%\n  \n  gf_label(-15 ~ pop_mean_height, label = \"Population Mean\", \n           color = \"blue\") %&gt;% \n  \n  gf_theme(theme = theme_classic)\n\n\n\n\n\n  \n\n\n\n[1] 500   5\n\n\n\n\n\n\n\n\n\n\n\n\n\nSample Means are a Random Variable\n\n\n\nThe sample-means are a random variable! And hence they will have a mean and sd. Do not get confused ;-D\n\n\nThe sample_means (red dots), are themselves random because the samples are random, of course. It appears that they are generally in the vicinity of the pop_mean (blue line).\n\n Distribution of Sample-Means\nSince the sample-means are themselves random variables, let’s plot the distribution of these 500 sample-means themselves, called a distribution of sample-means. We will also plot the position of the population mean pop_mean_height parameter, the mean of the Height variable.\n\nsample_height_500 %&gt;% \n  gf_dhistogram(~ sample_mean_500,bins = 30, xlab = \"Height\") %&gt;% \n  \n  gf_vline(xintercept = pop_mean_height, \n           color = \"blue\") %&gt;% \n  \n  gf_label(0.01 ~ pop_mean_height, \n            label = \"Population Mean\", \n            color = \"blue\") %&gt;% \n  gf_theme(theme_classic())\n\n# How does this **distribution of sample-means** compare with the\n# overall distribution of the population?\n# \nsample_height_500 %&gt;% \n  gf_dhistogram(~ sample_mean_500, bins = 30,xlab = \"Height\") %&gt;% \n  \n  gf_vline(xintercept = pop_mean_height, \n           color = \"blue\") %&gt;% \n  \n   gf_label(0.01 ~ pop_mean_height, \n            label = \"Population Mean\", \n            color = \"blue\") %&gt;% \n\n  ## Add the population histogram\n  gf_histogram(~ Height, data = NHANES_adult, \n               alpha = 0.2, fill = \"blue\", \n               bins = 30) %&gt;% \n  \n  gf_label(0.025 ~ (pop_mean_height + 20), \n           label = \"Population Distribution\", color = \"blue\") %&gt;% \n  gf_theme(theme_classic())\n\n\n\n\n\nSample\n\n\n\n\n\nSample and Population\n\n\n\n\n\nDistributions\n\n\n\n\n\n Central Limit Theorem\nWe see in the Figure above that\n\nthe distribution of sample-means is centered around the pop_mean.\nThat the standard deviation of the distribution of sample means is less than that of the original population. But exactly what is it?\nAnd what is the kind of distribution?\n\nOne more experiment.\nNow let’s repeatedly sample Height and compute the sample mean, and look at the resulting histograms and Q-Q plots. (Q-Q plots check whether a certain distribution is close to being normal or not.)\nWe will use sample sizes of c(16, 32, 64, 128) and generate 1000 samples each time, take the means and plot these 1000 means:\n\nset.seed(12345)\n\n\nsamples_height_16 &lt;- do(1000) * mean(resample(NHANES_adult$Height, size = 16))\n\nsamples_height_32 &lt;- do(1000) * mean(resample(NHANES_adult$Height, size = 32))\n\nsamples_height_64 &lt;- do(1000) * mean(resample(NHANES_adult$Height, size = 64))\n\nsamples_height_128 &lt;- do(1000) * mean(resample(NHANES_adult$Height, size = 128))\n\n# Quick Check\nhead(samples_height_16)\n\n\n\n  \n\n\n\nNow let’s create separate Q-Q plots for the different sample sizes.\n\n# Now let's create separate Q-Q plots for the different sample sizes.\n#\np1 &lt;- gf_qq( ~ mean,data = samples_height_16,\n             title = \"N = 16\", \n             color = \"cornsilk\") %&gt;%\n  gf_qqline()\n\np2 &lt;- gf_qq( ~ mean,data = samples_height_32,\n            title = \"N = 32\", \n            color = \"sienna\") %&gt;%\n  gf_qqline()\n\np3 &lt;- gf_qq( ~ mean,data = samples_height_32,\n            title = \"N = 64\", \n            color = \"tomato2\") %&gt;%\n  gf_qqline()\n\np4 &lt;- gf_qq( ~ mean,data = samples_height_128,\n            title = \"N = 128\", \n            color = \"violetred\") %&gt;%\n  gf_qqline()\n\ncowplot::plot_grid(p1, p2, p3, p4)\n\n\n\n\nLet us plot their individual histograms to compare them:\n\n# Let us overlay their individual histograms to compare them:\np5 &lt;- gf_dhistogram(~ mean,\n              data = samples_height_16,\n              color = \"black\",\n              fill = \"cornsilk\",title = \"N = 16\") %&gt;%\n  gf_fitdistr() %&gt;%\n  gf_vline(xintercept = pop_mean_height,\n           color = \"blue\") %&gt;%\n  gf_label(-0.01 ~ pop_mean_height, \n           label = \"Population Mean\", \n           color = \"blue\")\n\np6 &lt;- gf_dhistogram(~ mean,\n              data = samples_height_32,\n              color = \"black\",\n              fill = \"sienna\",title = \"N = 32\") %&gt;%\n  gf_fitdistr() %&gt;%\n  gf_vline(xintercept = pop_mean_height,\n           color = \"blue\") %&gt;%\n  gf_label(-.01 ~ pop_mean_height, \n           label = \"Population Mean\", \n           color = \"blue\")\n\np7 &lt;- gf_dhistogram(~ mean,\n                    data = samples_height_64 ,\n                    na.rm = TRUE,\n                    color = \"black\",\n                    fill = \"tomato2\",title = \"N = 64\") %&gt;%\n  gf_fitdistr() %&gt;%\n  gf_vline(xintercept = pop_mean_height,\n           color = \"blue\") %&gt;%\n  gf_label(-.01 ~ pop_mean_height, \n           label = \"Population Mean\", color = \"blue\")\n\np8 &lt;- gf_dhistogram(~ mean, \n                    data = samples_height_128,\n                    na.rm = TRUE,\n                    color = \"black\",\n                    fill = \"violetred\",title = \"N = 128\") %&gt;% \n  gf_fitdistr() %&gt;% \n  gf_vline(xintercept = pop_mean_height,\n         color = \"blue\") %&gt;%\n  gf_label(-.01 ~ pop_mean_height, \n           label = \"Population Mean\", color = \"blue\")\n\ncowplot::plot_grid(p5,p6,p7,p8)\n\n\n\n\nAnd if we overlay the histograms:\n\n\n\n\n\nThe QQ plots show that the results become more normally distributed (i.e. following the straight line) as the samples get larger. From the histograms we learn that the sample-means are normally distributed around the population mean. This feels intuitively right because when we sample from the population, many values will be close to the population mean, and values far away from the mean will be increasingly scarce.\nLet us calculate the mean of the sample-means:\n\nmean(~ mean, data  = samples_height_16)\nmean(~ mean, data  = samples_height_32)\nmean(~ mean, data  = samples_height_64)\nmean(~ mean, data  = samples_height_128)\npop_mean_height\n\n[1] 168.306\n[1] 168.4349\n[1] 168.3184\n[1] 168.366\n[1] 168.3497\n\n\nAnd the sample sds:\n\nsd(~ mean, data  = samples_height_16)\nsd(~ mean, data  = samples_height_32)\nsd(~ mean, data  = samples_height_64)\nsd(~ mean, data  = samples_height_128)\n\n[1] 2.578355\n[1] 1.834979\n[1] 1.280014\n[1] 0.9096318\n\n\n\n\n\n\n\n\nCentral Limit Theorem\n\n\n\nThis is the Central Limit Theorem (CLT)\n\nthe sample-means are normally distributed around the population mean.\nthe sample-means become “more normally distributed” with sample length, as shown by the (small but definite) improvements in the Q-Q plots with sample-size.\nthe sample-mean distributions narrow with sample length, i.e the sd decreases with increasing sample size.\n\nThis is regardless of the distribution of the population parameter itself.1"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#iconify-dashicons-code-standards-standard-error",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#iconify-dashicons-code-standards-standard-error",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "\n Standard Error",
    "text": "Standard Error\nAs we saw above, the standard deviations of the sample-mean distributions reduce with sample size. In fact their SDs are defined by:\nsd = pop_sd/sqrt(sample_size)2 where sample-size here is one of c(16,32,64,128)\nThe standard deviation of the sample-mean distribution is called the Standard Error. This statistic derived from the sample, will help us infer our population parameters with a precise estimate of the uncertainty involved.\n\\[\nStandard\\ Error\\ \\pmb {se} = \\frac{population\\ sd}{\\sqrt[]{sample\\ size}} \\\\\\\n\\pmb {se} = \\frac{\\sigma}{\\sqrt[]{n}}\n\\]\nIn our sampling experiments, the Standard Errors evaluate to:\n\npop_sd_height &lt;- sd(~ Height, data = NHANES_adult)\n\npop_sd_height/sqrt(16)\npop_sd_height/sqrt(32)\npop_sd_height/sqrt(64)\npop_sd_height/sqrt(128)\n\n\n\n[1] 2.539262\n[1] 1.795529\n[1] 1.269631\n[1] 0.8977646\n\n\n\n\nAs seen, these are identical to the Standard Deviations of the individual sample-mean distributions."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#iconify-fluent-auto-fit-width-24-filled-confidence-intervals",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#iconify-fluent-auto-fit-width-24-filled-confidence-intervals",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "\n Confidence intervals",
    "text": "Confidence intervals\nWhen we work with samples, we want to be able to speak with a certain degree of confidence about the population mean, based on the evaluation of one sample mean,not a whole large number of them. Give that sample-means are normally distributed around the population means, we can say that \\(68\\%\\) of all possible sample-mean lie within \\(\\pm SE\\) of the population mean; and further that \\(95 \\%\\) of all possible sample-mean lie within \\(\\pm 2*SE\\) of the population mean.\nThese two intervals \\(sample.mean \\pm SE\\) and \\(sample.mean \\pm 1.5*SE\\) are called the confidence intervals for the population mean, at levels \\(68\\%\\) and \\(95 \\%\\) probability respectively."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#iconify-flat-color-icons-workflow-workflow",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#iconify-flat-color-icons-workflow-workflow",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "\n Workflow",
    "text": "Workflow\nThus if we want to estimate a population parameter:\n\nwe take one random sample from the population\nwe calculate the estimate from the sample\nwe calculate the sample-sd\nwe calculate the Standard Error as \\(\\frac{sample-sd}{\\sqrt[]{n}}\\)\n\nwe calculate 95% confidence intervals for the population parameter based on the formula\n\\(CI_{95\\%}= sample.mean \\pm 2*SE\\).\n\nSince Standard Error decreases with sample size, we need to make our sample of adequate size.( \\(n=30\\) seems appropriate in most cases. Why?)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#iconify-ooui-references-rtl-references",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#iconify-ooui-references-rtl-references",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "\n References",
    "text": "References\n\nDiez, David M & Barr, Christopher D & Çetinkaya-Rundel, Mine, OpenIntro Statistics. https://www.openintro.org/book/os/\nStats Test Wizard. https://www.socscistatistics.com/tests/what_stats_test_wizard.aspx\nDiez, David M & Barr, Christopher D & Çetinkaya-Rundel, Mine: OpenIntro Statistics. Available online https://www.openintro.org/book/os/\nMåns Thulin, Modern Statistics with R: From wrangling and exploring data to inference and predictive modelling http://www.modernstatisticswithr.com/\nJonas Kristoffer Lindeløv, Common statistical tests are linear models (or: how to teach stats) https://lindeloev.github.io/tests-as-linear/\nCheatSheet https://lindeloev.github.io/tests-as-linear/linear_tests_cheat_sheet.pdf\nCommon statistical tests are linear models: a work through by Steve Doogue https://steverxd.github.io/Stat_tests/\nJeffrey Walker “Elements of Statistical Modeling for Experimental Biology”. https://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/\nAdam Loy, Lendie Follett & Heike Hofmann (2016) Variations of Q–Q Plots: The Power of Our Eyes!, The American Statistician, 70:2, 202-214, DOI: 10.1080/00031305.2015.1077728"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#footnotes",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#footnotes",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "Footnotes",
    "text": "Footnotes\n\nThe `Height` variable seems to be normally distributed at population level. We will try other non-normal population variables as an exercise in the tutorials.↩︎\nOnce sample size = population, we have complete access to the population and there is no question of estimation error! So sample_sd = pop_sd!↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/50-Bootstrap/index.html#introduction",
    "href": "content/courses/Analytics/Inference/Modules/50-Bootstrap/index.html#introduction",
    "title": "Bootstrap",
    "section": "Introduction",
    "text": "Introduction\n\n\n\nBootstrap Sampling"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/50-Bootstrap/index.html#datasets",
    "href": "content/courses/Analytics/Inference/Modules/50-Bootstrap/index.html#datasets",
    "title": "Bootstrap",
    "section": "Datasets",
    "text": "Datasets"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/50-Bootstrap/index.html#workflow-in-orange",
    "href": "content/courses/Analytics/Inference/Modules/50-Bootstrap/index.html#workflow-in-orange",
    "title": "Bootstrap",
    "section": "Workflow in Orange",
    "text": "Workflow in Orange"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/50-Bootstrap/index.html#workflow-in-radiant",
    "href": "content/courses/Analytics/Inference/Modules/50-Bootstrap/index.html#workflow-in-radiant",
    "title": "Bootstrap",
    "section": "Workflow in Radiant",
    "text": "Workflow in Radiant"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/50-Bootstrap/index.html#workflow-in-r",
    "href": "content/courses/Analytics/Inference/Modules/50-Bootstrap/index.html#workflow-in-r",
    "title": "Bootstrap",
    "section": "Workflow in R",
    "text": "Workflow in R"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/50-Bootstrap/index.html#conclusion",
    "href": "content/courses/Analytics/Inference/Modules/50-Bootstrap/index.html#conclusion",
    "title": "Bootstrap",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/50-Bootstrap/index.html#references",
    "href": "content/courses/Analytics/Inference/Modules/50-Bootstrap/index.html#references",
    "title": "Bootstrap",
    "section": "References",
    "text": "References\n\nhttps://openintro-ims.netlify.app/foundations-bootstrapping.html#foundations-bootstrapping\ns"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/70-PermTest/files/perm-tutorial.html",
    "href": "content/courses/Analytics/Inference/Modules/70-PermTest/files/perm-tutorial.html",
    "title": "Permutation Tests",
    "section": "",
    "text": "The mosaic package provides the shuffle() function as a synonym for sample(). When used without additional arguments, this will permute its first argument.\n\nCode# library(mosaic)\nshuffle(1:10)\n\n [1]  9  6  3  4  5  1  2  7 10  8\n\n\nApplying shuffle() to an explanatory variable in a model allows us to test the null hypothesis that the explanatory variable has, in fact, no explanatory power. This idea can be used to test\n\nthe equivalence of two or more means,\nthe equivalence of two or more proportions,\nwhether a regression parameter is 0. (Correlations between two variables) For example:\n\nCoupled with mosaic::do() we can repeat a shuffle many times, computing a desired statistic each time we shuffle. The distribution of this computed statistic is a NULL distribution, which can then be compared with the observed statistic to decide upon the Hypothesis Test and p-value."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/70-PermTest/files/perm-tutorial.html#permutation-tests",
    "href": "content/courses/Analytics/Inference/Modules/70-PermTest/files/perm-tutorial.html#permutation-tests",
    "title": "Permutation Tests",
    "section": "Permutation Tests",
    "text": "Permutation Tests\nCase Study-1: Hot Wings Orders vs Gender\nA student conducted a study of hot wings and beer consumption at a Bar. She asked patrons at the bar to record their consumption of hot wings and beer over the course of several hours. She wanted to know if people who ate more hot wings would then drink more beer. In addition, she investigated whether or not gender had an impact on hot wings or beer consumption.\n\nCodeBeerwings &lt;- read.csv(\"../../../../../../materials/data/resampling/Beerwings.csv\")\ninspect(Beerwings)\n\n\ncategorical variables:  \n    name     class levels  n missing\n1 Gender character      2 30       0\n                                   distribution\n1 F (50%), M (50%)                             \n\nquantitative variables:  \n      name   class min    Q1 median    Q3 max     mean        sd  n missing\n1       ID integer   1  8.25   15.5 22.75  30 15.50000  8.803408 30       0\n2 Hotwings integer   4  8.00   12.5 15.50  21 11.93333  4.784554 30       0\n3     Beer integer   0 24.00   30.0 36.00  48 26.20000 11.842064 30       0\n\n\nLet us calculate the observed difference in Hotwings consumption between Males and Females ( Gender)\n\nCodemean(Hotwings ~ Gender, data = Beerwings)\n\n        F         M \n 9.333333 14.533333 \n\nCodeobs_diff_wings &lt;- mosaic::diffmean(data = Beerwings, Hotwings ~ Gender)\nobs_diff_wings \n\ndiffmean \n     5.2 \n\n\n\nCodegf_boxplot(data = Beerwings, Hotwings ~ Gender, title = \"Hotwings Consumption by Gender\")\n\n\n\n\nThe observed difference in mean consumption of Hotwings between Males and Females is 5.2. Could this have occurred by chance? Here is our formulation of the Hypotheses:\n\\[\nNULL\\ Hypothesis\\ H_0 =&gt; No\\ difference\\ between\\ means\\ across\\ groups\\\\\nAlternative\\ Hypothesis\\\nH_a =&gt;Significant\\ difference\\ between\\ the\\ means\\\n\\]\nSo we perform a Permutation Test to check:\n\nCodenull_dist_wings &lt;- do(1000) * diffmean(Hotwings ~ shuffle(Gender), data = Beerwings)\nnull_dist_wings %&gt;% head()\n\n\n\n  \n\n\nCodegf_histogram(data = null_dist_wings, ~ diffmean) %&gt;% \n  gf_vline(xintercept = obs_diff_wings, colour = \"red\")\n\n\n\nCodeprop1(~ diffmean &gt;= obs_diff_wings, data = null_dist_wings)\n\n  prop_TRUE \n0.000999001 \n\n\nThe \\(\\color{red}{red\\ line}\\) shows the actual measured mean difference in Hot Wings consumption. The probability that our Permutation distribution is able to equal or exceed that number is \\(0.001998002\\) and we have to reject the Null Hypothesis that the means are identical.\nTo test whether eating more hotwings would lead to increased beer consumption, we need a regression model, which we can again test with a permutation test.\n\nCodelm(Beer ~ Hotwings, data = Beerwings)\n\n\nCall:\nlm(formula = Beer ~ Hotwings, data = Beerwings)\n\nCoefficients:\n(Intercept)     Hotwings  \n      3.040        1.941  \n\n\nCase Study-2: Verizon\nThe following example is used throughout this article. Verizon was an Incumbent Local Exchange Carrier (ILEC), responsible for maintaining land-line phone service in certain areas. Verizon also sold long-distance service, as did a number of competitors, termed Competitive Local Exchange Carriers (CLEC). When something went wrong, Verizon was responsible for repairs, and was supposed to make repairs as quickly for CLEC long-distance customers as for their own. The New York Public Utilities Commission (PUC) monitored fairness by comparing repair times for Verizon and different CLECs, for different classes of repairs and time periods. In each case a hypothesis test was performed at the 1% significance level, to determine whether repairs for CLEC’s customers were significantly slower than for Verizon’s customers. There were hundreds of such tests. If substantially more than 1% of the tests were significant, then Verizon would pay large penalties. These tests were performed using t tests; Verizon proposed using permutation tests instead.\n\nCodeverizon &lt;- read.csv(\"../../../../../../materials/data/resampling/Verizon.csv\")\ninspect(verizon)\n\n\ncategorical variables:  \n   name     class levels    n missing\n1 Group character      2 1687       0\n                                   distribution\n1 ILEC (98.6%), CLEC (1.4%)                    \n\nquantitative variables:  \n  name   class min   Q1 median   Q3   max     mean       sd    n missing\n1 Time numeric   0 0.75   3.63 7.35 191.6 8.522009 14.78848 1687       0\n\n\n\nCodemean(Time ~ Group, data = verizon)\n\n     CLEC      ILEC \n16.509130  8.411611 \n\nCodeobs_diff_verizon &lt;- diffmean(Time ~ Group, data = verizon)\nobs_diff_verizon\n\ndiffmean \n-8.09752 \n\n\n\nCodenull_dist_verizon &lt;- do(1000) * diffmean(Time ~ shuffle(Group), data = verizon)\ngf_histogram(data = null_dist_verizon, ~ diffmean) %&gt;% \n  gf_vline(xintercept = obs_diff_wings, colour = \"red\")\n\n\n\nCodeprop1(~ diffmean &gt;= obs_diff_wings, data = null_dist_verizon)\n\n prop_TRUE \n0.01098901 \n\n\nCase Story-3: Recidivism\nDo criminals released after a jail term commit crimes again?\n\nCoderecidivism &lt;- read.csv(\"../../../../../../materials/data/resampling/Recidivism.csv\")\ninspect(recidivism)\n\n\ncategorical variables:  \n     name     class levels     n missing\n1  Gender character      2 17019       3\n2     Age character      5 17019       3\n3   Age25 character      2 17019       3\n4 Offense character      2 17022       0\n5   Recid character      2 17022       0\n6    Type character      3 17022       0\n                                   distribution\n1 M (87.7%), F (12.3%)                         \n2 25-34 (36.6%), 35-44 (23.7%) ...             \n3 Over 25 (81.9%), Under 25 (18.1%)            \n4 Felony (80.6%), Misdemeanor (19.4%)          \n5 No (68.4%), Yes (31.6%)                      \n6 No Recidivism (68.4%), New (20.2%) ...       \n\nquantitative variables:  \n  name   class min  Q1 median  Q3  max     mean       sd    n missing\n1 Days integer   0 241    418 687 1095 473.3275 283.1393 5386   11636\n\n\nThere are some missing values in the variable  Age25. The  complete.cases command gives the row numbers where values are not missing. We create a new data frame omitting the rows where there is a missing value in the  ‘Age25’  variable.\n\nCoderecidivism_na &lt;- recidivism %&gt;% tidyr::drop_na(Age25)\n\n\nAlso, the variable Recid is a factor variable coded “Yes” or “No”. We convert it to a numeric variable of 1’s and 0’s.\n\nCoderecidivism_na &lt;- recidivism_na %&gt;% mutate(Recid2 = ifelse(Recid==\"Yes\", 1, 0))\n\nobs_diff_recid &lt;- diffmean( Recid2 ~ Age25, data = recidivism_na)\nobs_diff_recid\n\n  diffmean \n0.05919913 \n\nCodenull_dist_recid &lt;- do(1000) * diffmean( Recid2 ~ shuffle(Age25), data = recidivism_na)\n\ngf_histogram( ~ diffmean, data = null_dist_recid) %&gt;% \n  gf_vline(xintercept = obs_diff_recid, colour = \"red\")\n\n\n\n\nCase Study-4: Matched Pairs: Results from a diving championship.\n\nCodeDiving2017 &lt;- read.csv(\"../../../../../../materials/data/resampling/Diving2017.csv\")\nhead(Diving2017)\n\n\n\n  \n\n\nCodeinspect(Diving2017)\n\n\ncategorical variables:  \n     name     class levels  n missing\n1    Name character     12 12       0\n2 Country character      8 12       0\n                                   distribution\n1  SI Yajie (8.3%) ...                         \n2 Canada (16.7%), China (16.7%) ...            \n\nquantitative variables:  \n       name   class    min       Q1  median      Q3   max    mean       sd  n\n1 Semifinal numeric 313.70 322.2000 325.625 356.575 382.8 338.500 22.94946 12\n2     Final numeric 283.35 318.5875 358.925 387.150 397.5 350.475 40.02204 12\n  missing\n1       0\n2       0\n\n\nThe data is made up of paired observations per swimmer. So we need to take the difference between the two swim records for each swimmer and then shuffle the differences to either polarity. Another way to look at this is to shuffle the records between Semifinal and Final on a per Swimmer basis.\n\nCodeDiving2017\n\n\n\n  \n\n\nCodeDiving2017 %&gt;% diffmean(data = ., Final ~ Semifinal, only.2 = FALSE)\n\n  318.7-313.7  320.55-318.7 322.75-320.55  325.5-322.75  325.75-325.5 \n       12.350       -63.050         5.225        85.125      -114.150 \n   346-325.75    355.15-346 360.85-355.15  367.5-360.85   382.8-367.5 \n      102.200       -54.150        28.600        31.950         4.050 \n\nCodeobs_diff_swim &lt;- mean(~ Final - Semifinal, data = Diving2017)\nobs_diff_swim\n\n[1] 11.975\n\n\n\nCodepolarity &lt;- c(rep(1, 6), rep(-1,6))\npolarity\n\n [1]  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1\n\nCodenull_dist_swim &lt;- do(100000) * mean(data = Diving2017, \n                                    ~(Final - Semifinal) * resample(polarity,\n                                                    replace = TRUE))\nnull_dist_swim %&gt;% head()\n\n\n\n  \n\n\nCodegf_histogram(data = null_dist_swim, ~mean) %&gt;% \n  gf_vline(xintercept = obs_diff_swim, colour = \"red\")\n\n\n\n\nCase Study #5: Flight Delays\nLaGuardia Airport (LGA) is one of three major airports that serves the New York City metropolitan area. In 2008, over 23 million passengers and over 375 000 planes flew in or out of LGA. United Airlines and America Airlines are two major airlines that schedule services at LGA. The data set FlightDelays contains information on all 4029 departures of these two airlines from LGA during May and June 2009.\n\nCodeflightDelays &lt;- read.csv(\"../../../../../../materials/data/resampling/FlightDelays.csv\")\n\ninspect(flightDelays)\n\n\ncategorical variables:  \n         name     class levels    n missing\n1     Carrier character      2 4029       0\n2 Destination character      7 4029       0\n3  DepartTime character      5 4029       0\n4         Day character      7 4029       0\n5       Month character      2 4029       0\n6   Delayed30 character      2 4029       0\n                                   distribution\n1 AA (72.1%), UA (27.9%)                       \n2 ORD (44.3%), DFW (22.8%), MIA (15.1%) ...    \n3 8-Noon (26.1%), Noon-4pm (26%) ...           \n4 Fri (15.8%), Mon (15.6%), Tue (15.6%) ...    \n5 June (50.4%), May (49.6%)                    \n6 No (85.2%), Yes (14.8%)                      \n\nquantitative variables:  \n          name   class min   Q1 median   Q3  max      mean         sd    n\n1           ID integer   1 1008   2015 3022 4029 2015.0000 1163.21645 4029\n2     FlightNo integer  71  371    691  787 2255  827.1035  551.30939 4029\n3 FlightLength integer  68  155    163  228  295  185.3011   41.78783 4029\n4        Delay integer -19   -6     -3    5  693   11.7379   41.63050 4029\n  missing\n1       0\n2       0\n3       0\n4       0\n\n\nThe variables in the flightDelays dataset are:\n\nflightDelay dataset variables\n\n\n\n\n\nVariable\nDescription\n\n\n\nCarrier\nUA=United Airlines, AA=American Airlines\n\n\nFlightNo\nFlight number\n\n\nDestination\nAirport code\n\n\nDepartTime\nScheduled departure time in 4 h intervals\n\n\nDay\nDay of the Week\n\n\nMonth\nMay or June\n\n\nDelay\nMinutes flight delayed (negative indicates early departure)\n\n\nDelayed30\nDeparture delayed more than 30 min? Yes or No\n\n\nFlightLength\nLength of time of flight (minutes)\n\n\n\n\nLet us compute the proportion of times that each carrier’s flights was delayed more than 20 min. We will conduct a two-sided test to see if the difference in these proportions is statistically significant.\n\n\nCodeprop(data = flightDelays, Delay &gt;= 20 ~ Carrier)\n\nprop_TRUE.AA prop_TRUE.UA \n   0.1713696    0.2226180 \n\nCodeobs_diff_delay &lt;- diffprop(data = flightDelays, Delay &gt;= 20 ~ Carrier)\nobs_diff_delay\n\n  diffprop \n0.05124841 \n\n\nWe see carrier AA has a 17.13% chance of delays&gt;= 20, while UA has 22.26% chance. The difference is 5.12%. Is this statistically significant? We take the Delays for both Carriers and perform a permutation test by shuffle on the carrier variable:\n\nCodenull_dist_delay &lt;- do(10000) * diffprop(data = flightDelays, Delay &gt;= 20 ~ shuffle(Carrier))\nnull_dist_delay %&gt;% head()\n\n\n\n  \n\n\nCodegf_histogram(data = null_dist_delay, ~ diffprop) %&gt;% gf_vline(xintercept = obs_diff_delay, color = \"red\")\n\n\n\n\nIt appears that the difference indelay times is significant. We can compute the p-value based on this test:\n\nCode2* mean(null_dist_delay &gt;= obs_diff_delay)\n\n[1] 0\n\n\nwhich is very small. Hence we reject the null Hypothesis that there is no difference between carriers on delay times.\n\nCompute the variance in the flight delay lengths for each carrier. Conduct a test to see if the variance for United Airlines differs from that of American Airlines.\n\n\nCodevar(data = flightDelays, Delay ~ Carrier)\n\n      AA       UA \n1606.457 2037.525 \n\nCode# There is no readymade function in mosaic called `diffvar`...so...we construct one\nobs_diff_var &lt;- diff(var(data = flightDelays, Delay ~ Carrier))\nobs_diff_var\n\n      UA \n431.0677 \n\n\nThe difference in variances in Delay between the two carriers is \\(-431.0677\\). In our Permutation Test, we shuffle the Carrier variable:\n\nCodeobs_diff_var &lt;- diff(var(data = flightDelays, Delay ~ Carrier))\nnull_dist_var &lt;-\n  do(10000) * diff(var(data = flightDelays, Delay ~ shuffle(Carrier)))\nnull_dist_var %&gt;% head()\n\n\n\n  \n\n\nCode# The null distribution variable is called `UA`\ngf_histogram(data = null_dist_var, ~ UA) %&gt;% gf_vline(xintercept = obs_diff_delay, color = \"red\")\n\n\n\nCode2 * mean(null_dist_var &gt;= obs_diff_var)\n\n[1] 0.2806\n\n\nClearly there is no case for a significant difference in variances!\nCase Study #6: Walmart vs Target\nIs there a difference in the price of groceries sold by the two retailers Target and Walmart? The data set Groceries contains a sample of grocery items and their prices advertised on their respective web sites on one specific day.\n\nInspect the data set, then explain why this is an example of matched pairs data.\nCompute summary statistics of the prices for each store.\nConduct a permutation test to determine whether or not there is a difference in the mean prices.\nCreate a histogram bar-chart of the difference in prices. What is unusual about Quaker Oats Life cereal?\nRedo the hypothesis test without this observation. Do you reach the same conclusion?\n\n\nCodegroceries &lt;- read.csv(\"../../../../../../materials/data/resampling/Groceries.csv\") %&gt;% mutate(Product = stringr::str_squish(Product))\nhead(groceries)\n\n\n\n  \n\n\nCodeinspect(groceries)\n\n\ncategorical variables:  \n     name     class levels  n missing\n1 Product character     30 30       0\n2    Size character     24 30       0\n                                   distribution\n1 Annie's Macaroni & Cheese (3.3%) ...         \n2 18oz (10%), 12oz (6.7%) ...                  \n\nquantitative variables:  \n     name   class  min     Q1 median    Q3  max     mean       sd  n missing\n1  Target numeric 0.99 1.8275  2.545 3.140 7.99 2.762333 1.582128 30       0\n2 Walmart numeric 1.00 1.7600  2.340 2.955 6.98 2.705667 1.560211 30       0\n\n\nWe see that the comparison is to be made between two prices for the same product, and hence this is one more example of paired data, as in Case Study #4. Let us plot the prices for the products:\n\nCodegf_col(data = groceries,\n       Target ~ Product,\n       fill = \"#0073C299\",\n       width = 0.5 ) %&gt;% \n  gf_col(data = groceries,\n         -Walmart ~ Product,\n         fill = \"#EFC00099\",\n         ylab = \"Prices\",\n         width = 0.5\n       ) %&gt;% \n  gf_col(data = groceries %&gt;% filter(Product == \"Quaker Oats Life Cereal Original\"), \n         -Walmart ~ Product,\n         fill = \"red\", \n         width = 0.5) %&gt;% \n  gf_theme(theme_classic()) %&gt;%\n  gf_theme(ggplot2::theme(axis.text.x = element_text(\n    size = 8,\n    face = \"bold\",\n    vjust = 0,\n    hjust = 1\n  ))) %&gt;% gf_theme(ggplot2::coord_flip())\n\n\n\n\nWe see that the price difference between Walmart and Target prices is highest for the Product named Quaker Oats Life Cereal Original. Let us check the mean difference in prices:\n\nCodediffmean(data = groceries, Walmart ~ Target, only.2 = FALSE)\n\n   1-0.99    1.22-1 1.42-1.22 1.49-1.42 1.59-1.49 1.62-1.59 1.79-1.62 1.94-1.79 \n-0.580000  0.170000  0.210000 -0.100000  0.190000  0.070000  0.180000  0.160000 \n1.99-1.94 2.12-1.99 2.39-2.12  2.5-2.39  2.59-2.5 2.64-2.59 2.79-2.64 2.82-2.79 \n 0.090000  0.010000  0.200000  0.600000 -0.200000 -0.600000  0.660000  0.040000 \n2.99-2.82 3.19-2.99 3.49-3.19 3.99-3.49 4.79-3.99 7.19-4.79 7.99-7.19 \n 0.220000  1.263333 -1.183333 -0.480000  2.290000  2.190000  0.000000 \n\nCodeobs_diff_price = mean( ~ Walmart - Target, data = groceries)\nobs_diff_price\n\n[1] -0.05666667\n\n\nLet us perform the pair-wise permutation test on prices, by shuffling the two store names:\n\nCodepolarity &lt;- c(rep(1, 15), rep(-1,15))\npolarity\n\n [1]  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n[26] -1 -1 -1 -1 -1\n\nCodenull_dist_price &lt;- do(100000) * mean(data = groceries, \n                                    ~(Walmart-Target) * resample(polarity,\n                                                    replace = TRUE))\nnull_dist_price %&gt;% head()\n\n\n\n  \n\n\nCodegf_histogram(data = null_dist_price, ~mean) %&gt;% \n  gf_vline(xintercept = obs_diff_price, colour = \"red\")\n\n\n\nCode2*(sum(null_dist_price &gt;= obs_diff_price + 1)/(100000+1)) #P-value\n\n[1] 0\n\n\nDoes not seem to be aby significant difference in prices…\nSuppose we knock off the Quaker Cereal data item…\n\nCodewhich(groceries$Product == \"Quaker Oats Life Cereal Original\")\n\n[1] 2\n\nCodegroceries_less &lt;- groceries[-2,]\ngroceries_less\n\n\n\n  \n\n\nCodeobs_diff_price_less = mean( ~ Walmart - Target, data = groceries_less)\nobs_diff_price_less\n\n[1] -0.1558621\n\nCodepolarity_less &lt;- c(rep(1, 15), rep(-1,14)) # Due to resampling this small bias makes no difference\nnull_dist_price_less &lt;- do(100000) * mean(data = groceries_less, \n                                    ~(Walmart-Target) * resample(polarity_less,\n                                                    replace = TRUE))\nnull_dist_price_less %&gt;% head()\n\n\n\n  \n\n\nCodegf_histogram(data = null_dist_price_less, ~mean) %&gt;% \n  gf_vline(xintercept = obs_diff_price_less, colour = \"red\")\n\n\n\nCode1- mean(null_dist_price_less &gt;= obs_diff_price_less) #P-value\n\n[1] 0.01539\n\n\nCase Study 7: Proportions between Categorical Variables\nLet us try a dataset with Qualitative / Categorical data. This is a General Social Survey dataset, and we have people with different levels of Education stating their opinion on the Death Penalty. We want to know if these two Categorical variables have a correlation, i.e. can the opinions in favour of the Death Penalty be explained by the Education level?\nSince data is Categorical, we need to take counts in a table, and then implement a chi-square test. In the test, we will permute the Education variable to see if we can see how significant its effect size is.\n\nCodeGSS2002 &lt;- read.csv(\"../../../../../../materials/data/resampling/GSS2002.csv\")\ninspect(GSS2002)\n\n\ncategorical variables:  \n            name     class levels    n missing\n1         Region character      7 2765       0\n2         Gender character      2 2765       0\n3           Race character      3 2765       0\n4      Education character      5 2760       5\n5        Marital character      5 2765       0\n6       Religion character     13 2746      19\n7          Happy character      3 1369    1396\n8         Income character     24 1875     890\n9       PolParty character      8 2729      36\n10      Politics character      7 1331    1434\n11     Marijuana character      2  851    1914\n12  DeathPenalty character      2 1308    1457\n13        OwnGun character      3  924    1841\n14        GunLaw character      2  916    1849\n15 SpendMilitary character      3 1324    1441\n16     SpendEduc character      3 1343    1422\n17      SpendEnv character      3 1322    1443\n18      SpendSci character      3 1266    1499\n19        Pres00 character      5 1749    1016\n20      Postlife character      2 1211    1554\n                                    distribution\n1  North Central (24.7%) ...                    \n2  Female (55.6%), Male (44.4%)                 \n3  White (79.1%), Black (14.8%) ...             \n4  HS (53.8%), Bachelors (16.1%) ...            \n5  Married (45.9%), Never Married (25.6%) ...   \n6  Protestant (53.2%), Catholic (24.5%) ...     \n7  Pretty happy (57.3%) ...                     \n8  40000-49999 (9.1%) ...                       \n9  Ind (19.3%), Not Str Dem (18.9%) ...         \n10 Moderate (39.2%), Conservative (15.8%) ...   \n11 Not legal (64%), Legal (36%)                 \n12 Favor (68.7%), Oppose (31.3%)                \n13 No (65.5%), Yes (33.5%) ...                  \n14 Favor (80.5%), Oppose (19.5%)                \n15 About right (46.5%) ...                      \n16 Too little (73.9%) ...                       \n17 Too little (60%) ...                         \n18 About right (49.7%) ...                      \n19 Bush (50.6%), Gore (44.7%) ...               \n20 Yes (80.5%), No (19.5%)                      \n\nquantitative variables:  \n  name   class min  Q1 median   Q3  max mean       sd    n missing\n1   ID integer   1 692   1383 2074 2765 1383 798.3311 2765       0\n\n\nNote how all variables are Categorical !! Education has five levels:\n\nCodeGSS2002 %&gt;% count(Education)\n\n\n\n  \n\n\nCodeGSS2002 %&gt;% count(DeathPenalty)\n\n\n\n  \n\n\n\nLet us drop NA entries in Education and Death Penalty. And set up a table for the chi-square test.\n\nCodegss2002 &lt;- GSS2002 %&gt;% \n  dplyr::select(Education, DeathPenalty) %&gt;% \n  tidyr::drop_na(., c(Education, DeathPenalty))\ndim(gss2002)\n\n[1] 1307    2\n\nCodegss_summary &lt;- gss2002 %&gt;%\n  mutate(\n    Education = factor(\n      Education,\n      levels = c(\"Bachelors\", \"Graduate\", \"Jr Col\", \"HS\", \"Left HS\"),\n      labels = c(\"Bachelors\", \"Graduate\", \"Jr Col\", \"HS\", \"Left HS\")\n    ),\n    DeathPenalty = as.factor(DeathPenalty)\n  ) %&gt;%\n  group_by(Education, DeathPenalty) %&gt;%\n  summarise(count = n()) %&gt;% # This is good for a chisq test\n  \n  # Add two more columns to faciltate mosaic/Marrimekko Plot\n  # \n  mutate(edu_count = sum(count), \n         edu_prop = count / sum(count)) %&gt;%\n  ungroup() \n\ngss_summary\n\n\n\n  \n\n\nCode# We can plot a heatmap-like `mosaic chart` for this table, using `ggplot`:\n# https://stackoverflow.com/questions/19233365/how-to-create-a-marimekko-mosaic-plot-in-ggplot2\n\nggplot(data = gss_summary, aes( x = Education, y = edu_prop)) +\n  geom_bar(aes(width = edu_count, fill = DeathPenalty), stat = \"identity\", position = \"fill\", colour = \"black\") +\n  geom_text(aes(label = scales::percent(edu_prop)), position = position_stack(vjust = 0.5)) +\n\n\n# if labels are desired\n facet_grid(~ Education, scales = \"free_x\", space = \"free_x\") + \n  theme(scale_fill_brewer(palette = \"RdYlGn\")) + \n  # theme(panel.spacing.x = unit(0, \"npc\")) + # if no spacing preferred between bars\n  theme_void() \n\n\n\n\nLet us now perform the base chisq test: We need a table and then the chisq test:\n\nCodegss_table &lt;- tally(DeathPenalty ~ Education, data = gss2002)\ngss_table\n\n            Education\nDeathPenalty Bachelors Graduate  HS Jr Col Left HS\n      Favor        135       64 511     71     117\n      Oppose        71       50 200     16      72\n\nCode# Get the observed chi-square statistic\nobservedChi2 &lt;- mosaic::chisq(tally(DeathPenalty ~ Education, data = gss2002))\nobservedChi2\n\nX.squared \n 23.45093 \n\nCode# Actual chi-square test\nstats::chisq.test(tally(DeathPenalty ~ Education, data = gss2002))\n\n\n    Pearson's Chi-squared test\n\ndata:  tally(DeathPenalty ~ Education, data = gss2002)\nX-squared = 23.451, df = 4, p-value = 0.0001029\n\n\nWe should now repeat the test with permutations on Education:\n\nCodenull_chisq &lt;- do(10000) * chisq.test(tally(DeathPenalty ~ shuffle(Education), data = gss2002))\n\nhead(null_chisq)\n\n\n\n  \n\n\nCodegf_histogram( ~ X.squared, data = null_chisq) %&gt;% \n  gf_vline(xintercept = observedChi2, color = \"red\")\n\n\n\nCodegf_histogram( ~ p.value, data = null_chisq, binwidth = 0.1, center = 0.05)\n\n\n\n\nSo we would conclude that Education has a significant effect on DeathPenalty opinion!"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/listing.html",
    "href": "content/courses/Analytics/Modelling/listing.html",
    "title": "Inferential Modelling",
    "section": "",
    "text": "Modelling with Linear Regression\n\n\n\n\n\n\n\nLinear Regression\n\n\nQuantitative Predictor\n\n\nQuantitative Response\n\n\nSum of Squares\n\n\nResiduals\n\n\n\n\nUsing Regression to predict Quantitative Target Variables\n\n\n\n\n\n\nApr 13, 2023\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\nModelling with Logistic Regression\n\n\n\n\n\n\n\nLogistic Regression\n\n\nQualitative Variable\n\n\nProbability\n\n\n\n\n\n\n\n\n\n\n\nApr 13, 2023\n\n\nArvind Venkatadri\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/corr-lm-explorations.html",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/corr-lm-explorations.html",
    "title": "Correlation and Regression Explorations",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(broom)\nlibrary(ggformula)\nggplot2::theme_set(new = theme_classic())"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/corr-lm-explorations.html#packages",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/corr-lm-explorations.html#packages",
    "title": "Correlation and Regression Explorations",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(broom)\nlibrary(ggformula)\nggplot2::theme_set(new = theme_classic())"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/corr-lm-explorations.html#intro",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/corr-lm-explorations.html#intro",
    "title": "Correlation and Regression Explorations",
    "section": "Intro",
    "text": "Intro\nI will work through and “unify” at least two things:\n\nHadley Wickham’s chapter on modelling and his analysis of the linear model for the diamonds dataset\nThe diagnostic aspects of Linear Regression as detailed in Crawley’s book"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/corr-lm-explorations.html#explorations-into-diagnostic-plots",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/corr-lm-explorations.html#explorations-into-diagnostic-plots",
    "title": "Correlation and Regression Explorations",
    "section": "Explorations into Diagnostic Plots",
    "text": "Explorations into Diagnostic Plots\nLet us create dependent y* variables with different sorts of errors:\n\nx &lt;- 0:300\nen &lt;- rnorm(301, mean = 0, sd = 5)\neu &lt;- (runif(n = 301) -0.5) * 20\neb &lt;- rnbinom(n = 301,prob = 0.3,size = 2)\neg &lt;- rgamma(n = 301,shape = 1, rate = 1/x)\nyn &lt;- x + 10 + en\nyu &lt;- x + 10 + eu\nyb &lt;- x + 10 + eb\nyg &lt;- x + 10 + eg\ndata &lt;- tibble(x, yn, yu, yb, yg)\ndata\n\n\n\n  \n\n\n\nNormal Errors\n\nlm_norm_aug &lt;- lm(yn ~ x, data = data) %&gt;% \n  augment()\nlm_norm_aug %&gt;% gf_point(.resid ~ .fitted)\n\n\n\nlm_norm_aug %&gt;% gf_qq(~ .resid) %&gt;% gf_qqline()\n\n\n\n\nUniform Errors\n\nlm_unif_aug &lt;- lm(yu ~ x, data = data) %&gt;% \n  augment()\nlm_unif_aug %&gt;% gf_point(.resid ~ .fitted)\n\n\n\nlm_unif_aug %&gt;% gf_qq(~ .resid, distribution = stats::qnorm) %&gt;% gf_qqline()\n\n\n\n\nNegative Binom Errors\n\nlm_nbinom_aug &lt;- lm(yb ~ x, data = data) %&gt;% \n  augment()\nlm_nbinom_aug %&gt;% gf_point(.resid ~ .fitted)\n\n\n\nlm_nbinom_aug %&gt;% gf_qq(~ .resid, distribution = stats::qnorm) %&gt;% gf_qqline()\n\n\n\n\nGamma Errors\n\nlm_gamm_aug &lt;- lm(yg ~ x, data = data) %&gt;% \n  augment()\nlm_gamm_aug %&gt;% gf_point(.resid ~ .fitted)\n\n\n\nlm_gamm_aug %&gt;% gf_qq(~ .resid, distribution = stats::qnorm) %&gt;% gf_qqline()"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/lin-perm.html",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/lin-perm.html",
    "title": "Permutation Tests for Linear Regression",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE,warning = FALSE,message = FALSE)\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(mosaic)\nlibrary(infer)"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/lin-perm.html#linear-regression-using-permutation-tests",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/lin-perm.html#linear-regression-using-permutation-tests",
    "title": "Permutation Tests for Linear Regression",
    "section": "Linear Regression using Permutation Tests",
    "text": "Linear Regression using Permutation Tests\nWe wish to establish the significance of the effect size due to each of the levels in TempFac. From the normality tests conducted earlier we see that except at one level of TempFac, the times are are not normally distributed. Hence we opt for a Permutation Test to check for significance of effect.\nAs remarked in Ernst[^2], the non-parametric permutation test can be both exact and also intuitively easier for students to grasp. Permutations are easily executed in R, using packages such as mosaic[^3].\nWe proceed with a Permutation Test for TempFac. We shuffle the levels (13, 18, 25) randomly between the Times and repeat the ANOVA test each time and calculate the F-statistic. The Null distribution is the distribution of the F-statistic over the many permutations and the p-value is given by the proportion of times the F-statistic equals or exceeds that observed.\nRead the Data\n\ndata(\"BostonHousing2\", package = \"mlbench\")\nhousing &lt;- BostonHousing2\ninspect(housing)\n\n\ncategorical variables:  \n  name  class levels   n missing                                  distribution\n1 town factor     92 506       0 Cambridge (5.9%) ...                         \n2 chas factor      2 506       0 0 (93.1%), 1 (6.9%)                          \n\nquantitative variables:  \n      name   class       min          Q1     median          Q3       max\n1    tract integer   1.00000 1303.250000 3393.50000 3739.750000 5082.0000\n2      lon numeric -71.28950  -71.093225  -71.05290  -71.019625  -70.8100\n3      lat numeric  42.03000   42.180775   42.21810   42.252250   42.3810\n4     medv numeric   5.00000   17.025000   21.20000   25.000000   50.0000\n5    cmedv numeric   5.00000   17.025000   21.20000   25.000000   50.0000\n6     crim numeric   0.00632    0.082045    0.25651    3.677083   88.9762\n7       zn numeric   0.00000    0.000000    0.00000   12.500000  100.0000\n8    indus numeric   0.46000    5.190000    9.69000   18.100000   27.7400\n9      nox numeric   0.38500    0.449000    0.53800    0.624000    0.8710\n10      rm numeric   3.56100    5.885500    6.20850    6.623500    8.7800\n11     age numeric   2.90000   45.025000   77.50000   94.075000  100.0000\n12     dis numeric   1.12960    2.100175    3.20745    5.188425   12.1265\n13     rad integer   1.00000    4.000000    5.00000   24.000000   24.0000\n14     tax integer 187.00000  279.000000  330.00000  666.000000  711.0000\n15 ptratio numeric  12.60000   17.400000   19.05000   20.200000   22.0000\n16       b numeric   0.32000  375.377500  391.44000  396.225000  396.9000\n17   lstat numeric   1.73000    6.950000   11.36000   16.955000   37.9700\n           mean           sd   n missing\n1  2700.3557312 1.380037e+03 506       0\n2   -71.0563887 7.540535e-02 506       0\n3    42.2164403 6.177718e-02 506       0\n4    22.5328063 9.197104e+00 506       0\n5    22.5288538 9.182176e+00 506       0\n6     3.6135236 8.601545e+00 506       0\n7    11.3636364 2.332245e+01 506       0\n8    11.1367787 6.860353e+00 506       0\n9     0.5546951 1.158777e-01 506       0\n10    6.2846344 7.026171e-01 506       0\n11   68.5749012 2.814886e+01 506       0\n12    3.7950427 2.105710e+00 506       0\n13    9.5494071 8.707259e+00 506       0\n14  408.2371542 1.685371e+02 506       0\n15   18.4555336 2.164946e+00 506       0\n16  356.6740316 9.129486e+01 506       0\n17   12.6530632 7.141062e+00 506       0\n\n\nWe will use mosaic and also try with infer.\n\n\nUsing mosaic\nUsing infer\n\n\n\nmosaic offers an easy and intuitive way of doing a repeated permutation test, using the do() command. We will shuffle the TempFac factor to jumble up the Time observations, 10000 times. Each time we shuffle, we compute the F_statistic and record it. We then plot the 10000 F-statistics and compare that with the real-world observation of F-stat.\nThe Null distribution of the F_statistic under permutation shows it never crosses the real-world observed value, testifying the strength of the effect of TempFac on hatching Time. And the p-value is:\n\n\nWe calculate the observed F-stat with infer, which also has a very direct, if verbose, syntax for doing permutation tests:\nWe see that the observed F-Statistic is of course \\(385.8966\\) as before. Now we use infer to generate a NULL distribution using permutation of the factor TempFac:\nAs seen, the infer based permutation test also shows that the permutationally generated F-statistics are nowhere near that which was observed. The effect of TempFac is very strong."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LogReg/logistic.html",
    "href": "content/courses/Analytics/Modelling/Modules/LogReg/logistic.html",
    "title": "Modelling with Logistic Regression",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE,warning = FALSE,message = FALSE)\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(mosaic)\nlibrary(infer)\nlibrary(regressinator) # pedagogic tool for GLMs\nlibrary(GLMsData)"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LogReg/logistic.html#introduction",
    "href": "content/courses/Analytics/Modelling/Modules/LogReg/logistic.html#introduction",
    "title": "Modelling with Logistic Regression",
    "section": "Introduction",
    "text": "Introduction\nWe saw with the general linear model that it models the mean of a target variable as a linear weighted sum of the predictor variables.\nA general linear model can be stated as:\n\n\\begin{cases}\n& y_i = \\beta_0 + \\beta_1 * x_{1i} + \\beta_2*x_{2i}...+ \\beta_p*x_{pi} + \\epsilon_i\\\\\nwhere\\\\\n& p = number~ of ~ predictors~ x_p\\\\\n& y_i, ~ i = 1, . . . , n~ is ~ the ~ response ~ variable\\\\\n& \\epsilon_i = an~ error~ term, for~ the ~ i^{th} ~ predictions\n\\end{cases}\n\nThis model is considered to be general because of the dependence on potentially more than one explanatory variable, v.s. the simple linear model:1 y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i. The general linear model gives us model “shapes” that start from a simple straight line to a p-dimensional hyperplane.\nThe model is linear in the parameters \\beta_i, e.g. these are OK:\n\n\\color{blue}{\n\\begin{cases}\n& y_i = \\pmb\\beta_0 + \\pmb\\beta_1x_1 + \\pmb\\beta_2x_1^2 + \\epsilon_i\\\\\n& y_1 = \\pmb\\beta_0 + \\pmb\\gamma_1\\pmb\\delta_1x_1 + exp(\\pmb\\beta_2)x_2+ \\epsilon_i\\\\\n\\end{cases}\n}\n\nbut not, for example, these:\n\n\\color{red}{\n\\begin{cases}\n& y_i = \\pmb\\beta_0 + \\pmb\\beta_1x_1^{\\beta_2} + \\epsilon_i\\\\\n& y_i = \\pmb\\beta_0 + exp(\\pmb\\beta_1x_1) + \\epsilon_i\\\\\n\\end{cases}\n}\n\nAlthough a very useful framework, there are some situations where general linear models are not appropriate:\n\nthe range of Y is restricted (e.g. binary, count)\nthe variance of Y depends on the mean(Taylor’s Law)2\n\n\nGeneralized linear models extend the general linear model framework to address both of these issues."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LogReg/logistic.html#generalized-linear-model",
    "href": "content/courses/Analytics/Modelling/Modules/LogReg/logistic.html#generalized-linear-model",
    "title": "Modelling with Logistic Regression",
    "section": "Generalized Linear Model",
    "text": "Generalized Linear Model\n\n\n\n\n\n\nImportant\n\n\n\nA generalized linear model is made up of a linear predictor:\n\n\\eta_i = \\beta_0 + \\beta_1x_{1i} + ... + \\beta_px_{pi}\n\nand two functions:\n\n\na link function that describes how the mean, E(Y_i) = \\mu_i, depends on the linear predictor:\n\ng(\\mu_i) = \\eta_i\n\n\n\na variance function that describes how the variance, var(Y_i) depends on the mean:\n\nvar(Y_i) = \\Phi*V(\\mu_i)\n\n\n\nwhere the dispersion parameter \\Phi is a constant.\n\n\nFor example we can obtain our general linear model with the following choice:\n\n\\begin{align}\n& g(\\mu_i) = \\mu_i\\\\\n& Phi = 1\n\\end{align}\n\nIf now we assume that the target variable Y_i is a binomial, i.e. a two-valued variable:\n\n\\begin{align}\n& Y_i = binom(n_i,p_i)\\\\\n& mean(Y_i) = n_ip_i\\\\\n& var(Y_i) = n_ip_i(1-p_i)\n\\end{align}\n\nNow, we wish to model the proportions Y_i/n_i, as our target. Then we can state that:\n\n\\begin{align}\nmean(Y_i/n_i) = mean(Y_i/n_i) = p_i \\coloneqq \\mu_i\\\\\nvar(Y_i/n_i) = var(Y_i)/n_i^2 = \\frac{p_i(1-p_i)}{n_i} \\coloneqq \\sigma_i^2\\\\\n\\end{align}\n\nInspecting the above, we can write for the target variable:\n\n\\sigma_i^2 = \\frac{\\mu_i(1-\\mu_i)}{n_i}\n\nand since the link function needs to map {[-\\infty, \\infty]} to {[0,1]}, we use the logit function:\n\ng(\\mu_i) = logit(\\mu_i) = log(\\frac{\\mu_i}{1-\\mu_i})"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LogReg/logistic.html#references",
    "href": "content/courses/Analytics/Modelling/Modules/LogReg/logistic.html#references",
    "title": "Modelling with Logistic Regression",
    "section": "References",
    "text": "References\n\nhttps://yury-zablotski.netlify.app/post/how-logistic-regression-works/\nhttps://uc-r.github.io/logistic_regression\nhttps://francisbach.com/self-concordant-analysis-for-logistic-regression/\nhttps://statmath.wu.ac.at/courses/heather_turner/glmCourse_001.pdf"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LogReg/logistic.html#footnotes",
    "href": "content/courses/Analytics/Modelling/Modules/LogReg/logistic.html#footnotes",
    "title": "Modelling with Logistic Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://statmath.wu.ac.at/courses/heather_turner/glmCourse_001.pdf↩︎\nhttps://en.wikipedia.org/wiki/Taylor%27s_law↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html",
    "href": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html",
    "title": "🐉 Intro to Orange",
    "section": "",
    "text": "Orange is a visual drag-and-drop tool for\n\nData visualization\n\nMachine Learning\n\nData mining\n\nand much more. Here is what it looks like:\n\nAll operations are done using a visual menu-driven interface. The visuals can be exported to PNG/SVG/PDF and the entire workflow can be exported into a Report Form and edited for presentation and sharing."
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#introduction-to-orange",
    "href": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#introduction-to-orange",
    "title": "🐉 Intro to Orange",
    "section": "",
    "text": "Orange is a visual drag-and-drop tool for\n\nData visualization\n\nMachine Learning\n\nData mining\n\nand much more. Here is what it looks like:\n\nAll operations are done using a visual menu-driven interface. The visuals can be exported to PNG/SVG/PDF and the entire workflow can be exported into a Report Form and edited for presentation and sharing."
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#installing-orange",
    "href": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#installing-orange",
    "title": "🐉 Intro to Orange",
    "section": "Installing Orange",
    "text": "Installing Orange\nYou can download and install Orange from here:\nhttps://orangedatamining.com/download/"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#basic-usage-of-orange",
    "href": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#basic-usage-of-orange",
    "title": "🐉 Intro to Orange",
    "section": "Basic Usage of Orange",
    "text": "Basic Usage of Orange\n{{% youtube \"HXjnDIgGDuI\" %}}"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#orange-workflows",
    "href": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#orange-workflows",
    "title": "🐉 Intro to Orange",
    "section": "Orange Workflows",
    "text": "Orange Workflows\n{{% youtube \"lb-x36xqJ-E\" %}}"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#widgets-and-channels",
    "href": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#widgets-and-channels",
    "title": "🐉 Intro to Orange",
    "section": "Widgets and Channels",
    "text": "Widgets and Channels\n{{% youtube \"2xS6QjnG714\" %}}"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#loading-data-into-orange",
    "href": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#loading-data-into-orange",
    "title": "🐉 Intro to Orange",
    "section": "Loading data into Orange",
    "text": "Loading data into Orange\n{{% youtube \"MHcGdQeYCMg\" %}} \nWe are good to get started with Orange!!"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#simple-visuals-using-orange",
    "href": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#simple-visuals-using-orange",
    "title": "🐉 Intro to Orange",
    "section": "Simple Visuals using Orange",
    "text": "Simple Visuals using Orange\nLet us create some simple visualizations using Orange.\n\nUse the File Widget to import the iris dataset into your session\nUse the Data Table Widget to look at the data, and note its variable names\nUse the Visualization Widgets ( Scatter Plot, Bar Plot, and Distributions) to look at the properties of the variables, and examine relationships between them."
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#reference",
    "href": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#reference",
    "title": "🐉 Intro to Orange",
    "section": "Reference",
    "text": "Reference\n\nIntroduction to Data Mining-Working notes for the hands-on course with Orange Data Mining. (Download file)"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/files/Random-Forests.html",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/files/Random-Forests.html",
    "title": "Random Forests",
    "section": "",
    "text": "Machine Learning Basics - Random Forest at Shirin’s Playground"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/files/Random-Forests.html#references",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/files/Random-Forests.html#references",
    "title": "Random Forests",
    "section": "",
    "text": "Machine Learning Basics - Random Forest at Shirin’s Playground"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/files/Random-Forests.html#penguin-random-forest-model-withrandomforest",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/files/Random-Forests.html#penguin-random-forest-model-withrandomforest",
    "title": "Random Forests",
    "section": "Penguin Random Forest Model withrandomForest\n",
    "text": "Penguin Random Forest Model withrandomForest\n\nUsing the penguins dataset and Random Forest Classification.\n\npenguins\n\n\n\n  \n\n\nsummary(penguins)\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2                                 \n\npenguins %&gt;% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n344\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nspecies\n0\n1.00\nFALSE\n3\nAde: 152, Gen: 124, Chi: 68\n\n\nisland\n0\n1.00\nFALSE\n3\nBis: 168, Dre: 124, Tor: 52\n\n\nsex\n11\n0.97\nFALSE\n2\nmal: 168, fem: 165\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nbill_length_mm\n2\n0.99\n43.92\n5.46\n32.1\n39.23\n44.45\n48.5\n59.6\n▃▇▇▆▁\n\n\nbill_depth_mm\n2\n0.99\n17.15\n1.97\n13.1\n15.60\n17.30\n18.7\n21.5\n▅▅▇▇▂\n\n\nflipper_length_mm\n2\n0.99\n200.92\n14.06\n172.0\n190.00\n197.00\n213.0\n231.0\n▂▇▃▅▂\n\n\nbody_mass_g\n2\n0.99\n4201.75\n801.95\n2700.0\n3550.00\n4050.00\n4750.0\n6300.0\n▃▇▆▃▂\n\n\nyear\n0\n1.00\n2008.03\n0.82\n2007.0\n2007.00\n2008.00\n2009.0\n2009.0\n▇▁▇▁▇\n\n\n\n\npenguins &lt;- penguins %&gt;% tidyr::drop_na()\n# Spent one hour trying to find `drop-na()` (14 June 2020)\n\n\n# library(corrplot)\ncor &lt;- penguins %&gt;% select(where(is.numeric)) %&gt;% cor() \ncor %&gt;% corrplot(., method = \"ellipse\", order = \"hclust\",tl.cex = 1.0,)\n\n\n\n# try these too:\n# cor %&gt;% corrplot(., method = \"square\", order = \"hclust\",tl.cex = 0.5)\n# cor %&gt;% corrplot(., method = \"color\", order = \"hclust\",tl.cex = 0.5)\n# cor %&gt;% corrplot(., method = \"shade\", order = \"hclust\",tl.cex = 0.5)\n\nNotes: - flipper_length_mm and culmen_depth_mm are negatively correlated at approx (-0.7) - flipper_length_mm and body_mass_g are positively correlated at approx 0.8\nSo we will use steps in the recipe to remove correlated variables.\nPenguin Data Sampling and Recipe\n\n# Data Split\npenguin_split &lt;- initial_split(penguins, prop = 0.6)\npenguin_train &lt;- training(penguin_split)\npenguin_test &lt;- testing(penguin_split)\npenguin_split\n\n&lt;Training/Testing/Total&gt;\n&lt;199/134/333&gt;\n\nhead(penguin_train)\n\n\n\n  \n\n\n# Recipe\npenguin_recipe &lt;- penguins %&gt;% \n  recipe(species ~ .) %&gt;% \n  step_normalize(all_numeric()) %&gt;% # Scaling and Centering\n  step_corr(all_numeric()) %&gt;%  # Handling correlated variables\n  prep()\n\n# Baking the data\npenguin_train_baked &lt;-  penguin_train %&gt;% \n  bake(object = penguin_recipe, new_data = .)\n\npenguin_test_baked &lt;-  penguin_test %&gt;% \n  bake(object = penguin_recipe, new_data = .)\n\nhead(penguin_train_baked)\n\n\n\n  \n\n\n\nPenguin Random Forest Model\n\npenguin_model &lt;- \n  rand_forest(trees = 100) %&gt;% \n  set_engine(\"randomForest\") %&gt;% \n  set_mode(\"classification\")\npenguin_model\n\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  trees = 100\n\nComputational engine: randomForest \n\npenguin_fit &lt;- \n  penguin_model %&gt;% \n  fit(species ~ .,penguin_train_baked)\npenguin_fit\n\nparsnip model object\n\n\nCall:\n randomForest(x = maybe_data_frame(x), y = y, ntree = ~100) \n               Type of random forest: classification\n                     Number of trees: 100\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 1.51%\nConfusion matrix:\n          Adelie Chinstrap Gentoo class.error\nAdelie        82         1      0  0.01204819\nChinstrap      2        42      0  0.04545455\nGentoo         0         0     72  0.00000000\n\n# iris_ranger &lt;- \n#   rand_forest(trees = 100) %&gt;% \n#   set_mode(\"classification\") %&gt;% \n#   set_engine(\"ranger\") %&gt;% \n#   fit(Species ~ ., data = iris_training_baked)\n\nMetrics for the Penguin Random Forest Model\n\n# Predictions\npredict(object = penguin_fit, new_data = penguin_test_baked) %&gt;%  \n  dplyr::bind_cols(penguin_test_baked) %&gt;% \n  glimpse()\n\nRows: 134\nColumns: 9\n$ .pred_class       &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; -0.8946955, -0.8215515, -1.3335592, -0.8581235, -1.3…\n$ bill_depth_mm     &lt;dbl&gt; 0.77955895, 0.11940428, 1.08424573, 1.74440040, 0.32…\n$ flipper_length_mm &lt;dbl&gt; -1.4246077, -1.0678666, -0.5684290, -0.7824736, -1.1…\n$ body_mass_g       &lt;dbl&gt; -0.56762058, -0.50552542, -0.94019151, -0.69181089, …\n$ sex               &lt;fct&gt; male, female, female, male, female, male, female, ma…\n$ year              &lt;dbl&gt; -1.2818130, -1.2818130, -1.2818130, -1.2818130, -1.2…\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n\n# Prediction Accuracy Metrics\npredict(object = penguin_fit, new_data = penguin_test_baked) %&gt;%  \n  dplyr::bind_cols(penguin_test_baked) %&gt;% \n  yardstick::metrics(truth = species, estimate = .pred_class)\n\n\n\n  \n\n\n# Prediction Probabilities\npenguin_fit_probs &lt;- \n  predict(penguin_fit, penguin_test_baked, type = \"prob\") %&gt;%\n  dplyr::bind_cols(penguin_test_baked)\nglimpse(penguin_fit_probs)\n\nRows: 134\nColumns: 11\n$ .pred_Adelie      &lt;dbl&gt; 1.00, 0.97, 0.98, 0.99, 0.98, 1.00, 0.97, 0.99, 0.98…\n$ .pred_Chinstrap   &lt;dbl&gt; 0.00, 0.02, 0.02, 0.01, 0.02, 0.00, 0.02, 0.00, 0.01…\n$ .pred_Gentoo      &lt;dbl&gt; 0.00, 0.01, 0.00, 0.00, 0.00, 0.00, 0.01, 0.01, 0.01…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; -0.8946955, -0.8215515, -1.3335592, -0.8581235, -1.3…\n$ bill_depth_mm     &lt;dbl&gt; 0.77955895, 0.11940428, 1.08424573, 1.74440040, 0.32…\n$ flipper_length_mm &lt;dbl&gt; -1.4246077, -1.0678666, -0.5684290, -0.7824736, -1.1…\n$ body_mass_g       &lt;dbl&gt; -0.56762058, -0.50552542, -0.94019151, -0.69181089, …\n$ sex               &lt;fct&gt; male, female, female, male, female, male, female, ma…\n$ year              &lt;dbl&gt; -1.2818130, -1.2818130, -1.2818130, -1.2818130, -1.2…\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n\n# Confusion Matrix\npenguin_fit$fit$confusion %&gt;% tidy()\n\n\n\n  \n\n\n# Gain Curves\npenguin_fit_probs %&gt;% \n  yardstick::gain_curve(species, .pred_Adelie:.pred_Gentoo) %&gt;%\n  autoplot()\n\n\n\n# ROC Plot\npenguin_fit_probs%&gt;%\n  roc_curve(species, .pred_Adelie:.pred_Gentoo) %&gt;%\n  autoplot()\n\n\n\n\nUsing broom on the penguin model\n\npenguin_split\n\n&lt;Training/Testing/Total&gt;\n&lt;199/134/333&gt;\n\npenguin_split %&gt;% broom::tidy()\n\n\n\n  \n\n\npenguin_recipe %&gt;% broom::tidy()\n\n\n\n  \n\n\n# Following do not work for `random forest models` !! ;-()\n#penguin_model %&gt;% tidy()\n#penguin_fit %&gt;% tidy() \npenguin_model %&gt;% str()\n\nList of 7\n $ args                 :List of 3\n  ..$ mtry : language ~NULL\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_EmptyEnv&gt; \n  ..$ trees: language ~100\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_EmptyEnv&gt; \n  ..$ min_n: language ~NULL\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_EmptyEnv&gt; \n $ eng_args             : Named list()\n  ..- attr(*, \"class\")= chr [1:2] \"quosures\" \"list\"\n $ mode                 : chr \"classification\"\n $ user_specified_mode  : logi TRUE\n $ method               : NULL\n $ engine               : chr \"randomForest\"\n $ user_specified_engine: logi TRUE\n - attr(*, \"class\")= chr [1:2] \"rand_forest\" \"model_spec\"\n\npenguin_test_baked"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/files/Random-Forests.html#iris-random-forest-model-with-ranger",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/files/Random-Forests.html#iris-random-forest-model-with-ranger",
    "title": "Random Forests",
    "section": "Iris Random Forest Model with ranger\n",
    "text": "Iris Random Forest Model with ranger\n\nUsing the iris dataset and Random Forest Classification. This part uses rsample to split the data and the recipes to prep the data for model making.\n\n#set.seed(100)\niris_split &lt;- rsample::initial_split(iris, prop = 0.6)\niris_split\n\n&lt;Training/Testing/Total&gt;\n&lt;90/60/150&gt;\n\niris_split %&gt;% training() %&gt;% glimpse()\n\nRows: 90\nColumns: 5\n$ Sepal.Length &lt;dbl&gt; 6.8, 4.3, 5.4, 5.0, 6.3, 5.2, 5.1, 5.0, 6.6, 6.2, 5.8, 4.…\n$ Sepal.Width  &lt;dbl&gt; 3.0, 3.0, 3.9, 3.6, 3.3, 4.1, 3.5, 3.4, 2.9, 3.4, 2.8, 3.…\n$ Petal.Length &lt;dbl&gt; 5.5, 1.1, 1.7, 1.4, 4.7, 1.5, 1.4, 1.5, 4.6, 5.4, 5.1, 1.…\n$ Petal.Width  &lt;dbl&gt; 2.1, 0.1, 0.4, 0.2, 1.6, 0.1, 0.3, 0.2, 1.3, 2.3, 2.4, 0.…\n$ Species      &lt;fct&gt; virginica, setosa, setosa, setosa, versicolor, setosa, se…\n\niris_split %&gt;% testing() %&gt;% glimpse()\n\nRows: 60\nColumns: 5\n$ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 5.4, 4.8, 5.8, 5.7, 5.4, 5.1, 5.4, 5.1, 4.6, 5.…\n$ Sepal.Width  &lt;dbl&gt; 3.5, 3.1, 3.7, 3.4, 4.0, 4.4, 3.9, 3.8, 3.4, 3.7, 3.6, 3.…\n$ Petal.Length &lt;dbl&gt; 1.4, 1.5, 1.5, 1.6, 1.2, 1.5, 1.3, 1.5, 1.7, 1.5, 1.0, 1.…\n$ Petal.Width  &lt;dbl&gt; 0.2, 0.1, 0.2, 0.2, 0.2, 0.4, 0.4, 0.3, 0.2, 0.4, 0.2, 0.…\n$ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n\nIris Data Pre-Processing: Creating the Recipe\nThe recipes package provides an interface that specializes in data pre-processing. Within the package, the functions that start, or execute, the data transformations are named after cooking actions. That makes the interface more user-friendly. For example:\n\nrecipe() - Starts a new set of transformations to be applied, similar to the ggplot() command. Its main argument is the model’s formula.\nprep() - Executes the transformations on top of the data that is supplied (typically, the training data). Each data transformation is a step() function. ( Recall what we did with the caret package: Centering, Scaling, Removing Correlated variables…)\n\nNote that in order to avoid data leakage (e.g: transferring information from the train set into the test set), data should be “prepped” using the train_tbl only. https://towardsdatascience.com/modelling-with-tidymodels-and-parsnip-bae2c01c131c CRAN: The idea is that the preprocessing operations will all be created using the training set and then these steps will be applied to both the training and test set.\n\n# Pre Processing the Training Data\n\niris_recipe &lt;- \n  training(iris_split) %&gt;% # Note: Using TRAINING data !!\n  recipe(Species ~ .)      # Note: Outcomes ~ Predictors !!\n\n# The data contained in the `data` argument need not be the training set; this data is only used to catalog the names of the variables and their types (e.g. numeric, etc.).\n\nQ: How does the recipe “figure” out which are the outcomes and which are the predictors? A.The recipe command defines Outcomes and Predictors using the formula interface. Not clear how this recipe “figures” out which are the outcomes and which are the predictors, when we have not yet specified them…\nQ. Why is the recipe not agnostic to data set? Is that a meaningful question? A. The use of the training set in the recipe command is just to declare the variables and specify the roles of the data, nothing else. Roles are open-ended and extensible. From https://cran.r-project.org/web/packages/recipes/vignettes/Simple_Example.html :\n\nThis document demonstrates some basic uses of recipes. First, some definitions are required: - variables are the original (raw) data columns in a data frame or tibble. For example, in a traditional formula Y ~ A + B + A:B, the variables are A, B, and Y. - roles define how variables will be used in the model. Examples are: predictor (independent variables), response, and case weight. This is meant to be open-ended and extensible. - terms are columns in a design matrix such as A, B, and A:B. These can be other derived entities that are grouped, such as a set of principal components or a set of columns, that define a basis function for a variable. These are synonymous with features in machine learning. Variables that have predictor roles would automatically be main effect terms.\n\n\n# Apply the transformation steps\niris_recipe &lt;- iris_recipe %&gt;% \n  step_corr(all_predictors()) %&gt;% \n  step_center(all_predictors(), -all_outcomes()) %&gt;% \n  step_scale(all_predictors(), -all_outcomes()) %&gt;% \n  prep()\n\nThis has created the recipe() and prepped it too. We now need to apply it to our datasets:\n\nTake training data and bake() it to prepare it for modelling.\nDo the same for the testing set.\n\n\niris_training_baked &lt;- \n  iris_split %&gt;% \n  training() %&gt;% \n  bake(iris_recipe,.)\niris_training_baked\n\n\n\n  \n\n\niris_testing_baked &lt;- \n  iris_split %&gt;% \n  testing() %&gt;% \n  bake(iris_recipe,.)\niris_testing_baked \n\n\n\n  \n\n\n\nIris Model Training using parsnip\n\nDifferent ML packages provide different interfaces (APIs ) to do the same thing (e.g random forests). The tidymodels package provides a consistent interface to invoke a wide variety of packages supporting a wide variety of models.\nThe parsnip package is a successor to caret.\nTo model with parsnip: 1. Pick a model : 2. Set the engine 3. Set the mode (if needed): Classification or Regression\nCheck here for models available in parsnip.\n\nMode: classification and regression in parsnip, each using a variety of models. ( Which Way). This defines the form of the output.\nEngine: The engine is the R package that is invoked by parsnip to execute the model. E.g glm, glmnet,keras.( How ) parsnip provides wrappers for models from these packages.\nModel: is the specific technique used for the modelling task. E.g linear_reg(), logistic_reg(), mars, decision_tree, nearest_neighbour…(What model).\n\nand models have: - hyperparameters: that are numerical or factor variables that tune the model ( Like the alpha beta parameters for Bayesian priors)\nWe can use the random forest model to classify the iris into species. Here Species is the Outcome variable and the rest are predictor variables. The random forest model is provided by the ranger package, to which tidymodels/parsnip provides a simple and consistent interface.\n\nlibrary(ranger)\niris_ranger &lt;- \n  rand_forest(trees = 100) %&gt;% \n  set_mode(\"classification\") %&gt;% \n  set_engine(\"ranger\") %&gt;% \n  fit(Species ~ ., data = iris_training_baked)\n\nranger can generate random forest models for classification, regression, survival( time series, time to event stuff). Extreme Forests are also supported, wherein all points in the dataset are used ( instead of bootstrap samples) along with feature bagging. We can also run the same model using the randomForest package:\n\nlibrary(randomForest,quietly = TRUE)\niris_rf &lt;- \n  rand_forest(trees = 100) %&gt;% \n  set_mode(\"classification\") %&gt;% \n  set_engine(\"randomForest\") %&gt;% \n  fit(Species ~ ., data = iris_training_baked)\n\nIris Predictions\nThe predict() function run against a parsnip model returns a prediction tibble. By default, the prediction variable is called .pred_class.\n\npredict(object = iris_ranger, new_data = iris_testing_baked) %&gt;%  \n  dplyr::bind_cols(iris_testing_baked) %&gt;% \n  glimpse()\n\nRows: 60\nColumns: 5\n$ .pred_class  &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n$ Sepal.Length &lt;dbl&gt; -0.9654964, -1.2038906, -0.6079052, -1.3230877, -0.131116…\n$ Sepal.Width  &lt;dbl&gt; 1.2088785, 0.1848873, 1.7208741, 0.9528807, 2.4888675, 3.…\n$ Petal.Width  &lt;dbl&gt; -1.4370312, -1.5674070, -1.4370312, -1.4370312, -1.437031…\n$ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n\nIris Classification Model Validation\nWe use metrics() function from the yardstick package to evaluate how good the model is.\n\npredict(iris_ranger, iris_testing_baked) %&gt;%\n  dplyr::bind_cols(iris_testing_baked) %&gt;% \n  yardstick::metrics(truth = Species, estimate = .pred_class)\n\n\n\n  \n\n\n\nWe can also check the metrics for randomForest model:\n\npredict(iris_rf, iris_testing_baked) %&gt;%\n  dplyr::bind_cols(iris_testing_baked) %&gt;% \n  yardstick::metrics(truth = Species, estimate = .pred_class)\n\n\n\n  \n\n\n\nIris Per-Classifier Metrics\nWe can use the parameter type = \"prob\" in the predict() function to obtain a probability score on each prediction. TBD: How is this prob calculated? Possible answer: the Random Forest model outputs its answer by majority voting across n trees. Each of the possible answers( i.e. predictions) for a particular test datum gets a share of the vote, that represents its probability. Hence each dataum in the test vector can show a probability for the “winning” answer. ( Quite possibly we can get the probabilities for all possible outcomes for each test datum)\n\niris_ranger_probs &lt;- \n  predict(iris_ranger, iris_testing_baked, type = \"prob\") %&gt;%\n  dplyr::bind_cols(iris_testing_baked)\nglimpse(iris_ranger_probs)\n\nRows: 60\nColumns: 7\n$ .pred_setosa     &lt;dbl&gt; 0.98333333, 0.96639683, 0.96693651, 0.98002778, 0.780…\n$ .pred_versicolor &lt;dbl&gt; 0.006583333, 0.032353175, 0.025452381, 0.009916667, 0…\n$ .pred_virginica  &lt;dbl&gt; 0.010083333, 0.001250000, 0.007611111, 0.010055556, 0…\n$ Sepal.Length     &lt;dbl&gt; -0.9654964, -1.2038906, -0.6079052, -1.3230877, -0.13…\n$ Sepal.Width      &lt;dbl&gt; 1.2088785, 0.1848873, 1.7208741, 0.9528807, 2.4888675…\n$ Petal.Width      &lt;dbl&gt; -1.4370312, -1.5674070, -1.4370312, -1.4370312, -1.43…\n$ Species          &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos…\n\niris_rf_probs &lt;- \n  predict(iris_rf, iris_testing_baked, type = \"prob\") %&gt;%\n  dplyr::bind_cols(iris_testing_baked)\nglimpse(iris_rf_probs)\n\nRows: 60\nColumns: 7\n$ .pred_setosa     &lt;dbl&gt; 1.00, 0.95, 0.96, 1.00, 0.77, 0.84, 0.96, 1.00, 0.82,…\n$ .pred_versicolor &lt;dbl&gt; 0.00, 0.05, 0.03, 0.00, 0.22, 0.15, 0.03, 0.00, 0.18,…\n$ .pred_virginica  &lt;dbl&gt; 0.00, 0.00, 0.01, 0.00, 0.01, 0.01, 0.01, 0.00, 0.00,…\n$ Sepal.Length     &lt;dbl&gt; -0.9654964, -1.2038906, -0.6079052, -1.3230877, -0.13…\n$ Sepal.Width      &lt;dbl&gt; 1.2088785, 0.1848873, 1.7208741, 0.9528807, 2.4888675…\n$ Petal.Width      &lt;dbl&gt; -1.4370312, -1.5674070, -1.4370312, -1.4370312, -1.43…\n$ Species          &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos…\n\n# Tabulating the probabilities\nftable(iris_rf_probs$.pred_versicolor)\n\n  0 0.01 0.02 0.03 0.04 0.05 0.07 0.08 0.1 0.11 0.12 0.15 0.18 0.22 0.68 0.77 0.78 0.8 0.84 0.85 0.9 0.91 0.92 0.94 0.96 0.97 0.99  1\n                                                                                                                                     \n 12    1    4    3    2    4    1    1   2    2    1    3    2    1    1    1    2   1    1    2   1    1    1    1    3    1    2  3\n\nftable(iris_rf_probs$.pred_virginica)\n\n  0 0.01 0.02 0.03 0.04 0.08 0.09 0.1 0.13 0.15 0.16 0.2 0.22 0.23 0.32 0.85 0.87 0.89 0.9 0.92 0.95 0.96 0.97 0.98  1\n                                                                                                                      \n 18    9    4    1    3    1    1   1    1    2    1   1    1    1    1    2    2    1   2    1    1    1    1    1  2\n\nftable(iris_rf_probs$.pred_setosa)\n\n  0 0.01 0.02 0.06 0.07 0.13 0.77 0.82 0.84 0.88 0.92 0.93 0.95 0.96 0.98 0.99  1\n                                                                                 \n 28    2    1    1    1    2    1    2    1    2    1    1    3    5    1    1  7\n\n\n\n### Iris Classifier: Gain and ROC Curves\n\nWe can plot gain and ROC curves for each of these models\n\n::: {.cell hash='Random-Forests_cache/html/Gain and ROC Curves `ranger`_334a742fbed15719f6ca7a011e06b122'}\n\n```{.r .cell-code}\niris_ranger_probs %&gt;% \n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %&gt;% \n  glimpse()\n\nRows: 134\nColumns: 5\n$ .level          &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"set…\n$ .n              &lt;dbl&gt; 0, 2, 3, 6, 7, 8, 9, 11, 12, 13, 14, 15, 17, 18, 19, 2…\n$ .n_events       &lt;dbl&gt; 0, 2, 3, 6, 7, 8, 9, 11, 12, 13, 14, 15, 17, 18, 19, 2…\n$ .percent_tested &lt;dbl&gt; 0.000000, 3.333333, 5.000000, 10.000000, 11.666667, 13…\n$ .percent_found  &lt;dbl&gt; 0, 8, 12, 24, 28, 32, 36, 44, 48, 52, 56, 60, 68, 72, …\n\niris_ranger_probs %&gt;% \n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %&gt;% \n  autoplot()\n\n\n\niris_ranger_probs %&gt;% \n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %&gt;% \n  glimpse()\n\nRows: 137\nColumns: 4\n$ .level      &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\"…\n$ .threshold  &lt;dbl&gt; -Inf, 0.000000000, 0.001000000, 0.001111111, 0.001250000, …\n$ specificity &lt;dbl&gt; 0.0000000, 0.0000000, 0.2571429, 0.3428571, 0.4285714, 0.4…\n$ sensitivity &lt;dbl&gt; 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00…\n\niris_ranger_probs %&gt;% \n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %&gt;% \n  autoplot()\n\n\n\n:::\n\niris_rf_probs %&gt;% \n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %&gt;% \n  glimpse()\n\nRows: 73\nColumns: 5\n$ .level          &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"set…\n$ .n              &lt;dbl&gt; 0, 7, 8, 9, 14, 17, 18, 19, 21, 22, 24, 25, 27, 28, 29…\n$ .n_events       &lt;dbl&gt; 0, 7, 8, 9, 14, 17, 18, 19, 21, 22, 24, 25, 25, 25, 25…\n$ .percent_tested &lt;dbl&gt; 0.000000, 11.666667, 13.333333, 15.000000, 23.333333, …\n$ .percent_found  &lt;dbl&gt; 0.00000, 28.00000, 32.00000, 36.00000, 56.00000, 68.00…\n\niris_rf_probs %&gt;% \n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %&gt;% \n  autoplot()\n\n\n\niris_rf_probs %&gt;% \n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %&gt;% \n  glimpse()\n\nRows: 76\nColumns: 4\n$ .level      &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\"…\n$ .threshold  &lt;dbl&gt; -Inf, 0.00, 0.01, 0.02, 0.06, 0.07, 0.13, 0.77, 0.82, 0.84…\n$ specificity &lt;dbl&gt; 0.0000000, 0.0000000, 0.8000000, 0.8571429, 0.8857143, 0.9…\n$ sensitivity &lt;dbl&gt; 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0…\n\niris_rf_probs %&gt;% \n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %&gt;% \n  autoplot()\n\n\n\n\nIris Classifier: Metrics\n\npredict(iris_ranger, iris_testing_baked, type = \"prob\") %&gt;% \n  bind_cols(predict(iris_ranger,iris_testing_baked)) %&gt;% \n  bind_cols(select(iris_testing_baked,Species)) %&gt;% \n  glimpse()\n\nRows: 60\nColumns: 5\n$ .pred_setosa     &lt;dbl&gt; 0.98333333, 0.96639683, 0.96693651, 0.98002778, 0.780…\n$ .pred_versicolor &lt;dbl&gt; 0.006583333, 0.032353175, 0.025452381, 0.009916667, 0…\n$ .pred_virginica  &lt;dbl&gt; 0.010083333, 0.001250000, 0.007611111, 0.010055556, 0…\n$ .pred_class      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos…\n$ Species          &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos…\n\npredict(iris_ranger, iris_testing_baked, type = \"prob\") %&gt;% \n  bind_cols(predict(iris_ranger,iris_testing_baked)) %&gt;% \n  bind_cols(select(iris_testing_baked,Species)) %&gt;% \n  yardstick::metrics(data = ., truth = Species, estimate = .pred_class, ... = .pred_setosa:.pred_virginica)\n\n\n\n  \n\n\n# And for the `randomForest`method\n\npredict(iris_rf, iris_testing_baked, type = \"prob\") %&gt;% \n  bind_cols(predict(iris_ranger,iris_testing_baked)) %&gt;% \n  bind_cols(select(iris_testing_baked,Species)) %&gt;% \n  glimpse()\n\nRows: 60\nColumns: 5\n$ .pred_setosa     &lt;dbl&gt; 1.00, 0.95, 0.96, 1.00, 0.77, 0.84, 0.96, 1.00, 0.82,…\n$ .pred_versicolor &lt;dbl&gt; 0.00, 0.05, 0.03, 0.00, 0.22, 0.15, 0.03, 0.00, 0.18,…\n$ .pred_virginica  &lt;dbl&gt; 0.00, 0.00, 0.01, 0.00, 0.01, 0.01, 0.01, 0.00, 0.00,…\n$ .pred_class      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos…\n$ Species          &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos…\n\npredict(iris_rf, iris_testing_baked, type = \"prob\") %&gt;% \n  bind_cols(predict(iris_ranger,iris_testing_baked)) %&gt;% \n  bind_cols(select(iris_testing_baked,Species)) %&gt;% \n  yardstick::metrics(data = ., truth = Species, estimate = .pred_class, ... = .pred_setosa:.pred_virginica)"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html",
    "href": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html",
    "title": "ML - Clustering",
    "section": "",
    "text": "Quoting from http://baoqiang.org/?p=579\n\n\nThese two are arguably the two commonly used cluster methods. One of the reasons is that they are easy to use and also somehow straightforward. So how do they work?\nk-Nearest-Neighbour: Provide N n-dimension entries with known associated classes for each entry, the number of classes is k, that is, \\[\n\\{\\vec{x_i}, y_i\\} ,\\ \\vec{x_i} \\in\\ {\\Re^{n}}\\ , y_i\\ = \\{c_1,...c_k\\},\ni = 1...N\n\\]\nFor a new entry \\(\\vec{v_j}\\), to which class should it belong? We need use a distance measure to get the k closest entries of the new entry , the final decision is simple majority vote based the closest k neighbors. The distance metric could be euclidean or other similar ones.\n\n\nK-means: Given N n-dimension entries and classify them in k classes. At first, we randomly choose k entries and assign them to k clusters. They are the seed classes. Then we calculate the distance between each entry and each class. Each entry will be assigned into one class in terms of the its distance to each class, i.e., assign the entry to its closest class. After the assignment is complete, we then calculate the centroid of each class based on their new members. After the centroid calculation, we go back to the distance calculation and therefore new round classification. We stop the iteration when there is convergence,i.e,, no new centroid and classification.\nThe two methods are all semi-supervised learning algorithms because they do need we provide the number of clusters prior the clustering."
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#introduction",
    "href": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#introduction",
    "title": "ML - Clustering",
    "section": "",
    "text": "Quoting from http://baoqiang.org/?p=579\n\n\nThese two are arguably the two commonly used cluster methods. One of the reasons is that they are easy to use and also somehow straightforward. So how do they work?\nk-Nearest-Neighbour: Provide N n-dimension entries with known associated classes for each entry, the number of classes is k, that is, \\[\n\\{\\vec{x_i}, y_i\\} ,\\ \\vec{x_i} \\in\\ {\\Re^{n}}\\ , y_i\\ = \\{c_1,...c_k\\},\ni = 1...N\n\\]\nFor a new entry \\(\\vec{v_j}\\), to which class should it belong? We need use a distance measure to get the k closest entries of the new entry , the final decision is simple majority vote based the closest k neighbors. The distance metric could be euclidean or other similar ones.\n\n\nK-means: Given N n-dimension entries and classify them in k classes. At first, we randomly choose k entries and assign them to k clusters. They are the seed classes. Then we calculate the distance between each entry and each class. Each entry will be assigned into one class in terms of the its distance to each class, i.e., assign the entry to its closest class. After the assignment is complete, we then calculate the centroid of each class based on their new members. After the centroid calculation, we go back to the distance calculation and therefore new round classification. We stop the iteration when there is convergence,i.e,, no new centroid and classification.\nThe two methods are all semi-supervised learning algorithms because they do need we provide the number of clusters prior the clustering."
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#workflow-using-orange",
    "href": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#workflow-using-orange",
    "title": "ML - Clustering",
    "section": "Workflow using Orange",
    "text": "Workflow using Orange"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#workflow-using-radiant",
    "href": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#workflow-using-radiant",
    "title": "ML - Clustering",
    "section": "Workflow using Radiant",
    "text": "Workflow using Radiant"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#workflow-using-r",
    "href": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#workflow-using-r",
    "title": "ML - Clustering",
    "section": "Workflow using R",
    "text": "Workflow using R"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#conclusion",
    "href": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#conclusion",
    "title": "ML - Clustering",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#references",
    "href": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#references",
    "title": "ML - Clustering",
    "section": "References",
    "text": "References\n\nK-means Cluster Analysis. UC Business Analytics R Programming Guide https://uc-r.github.io/kmeans_clustering#optimal\nThean C Lim. Clustering: k-means, k-means ++ and gganimate. https://theanlim.rbind.io/post/clustering-k-means-k-means-and-gganimate/"
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/listing.html",
    "href": "content/courses/Analytics/Prescriptive/listing.html",
    "title": "Prescriptive Analytics",
    "section": "",
    "text": "📐 Intro to Linear Programming\n\n\n\n\n\n\n\n\n\n\n\n\nNov 10, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\n💭 The Simplex Method - Intuitively\n\n\n\n\n\nWe will look at developing an intuitive understanding of the Simplex Method for Linear Programming.\n\n\n\n\n\n\nNov 14, 2022\n\n\n\n\n\n\n  \n\n\n\n\n📅 The Simplex Method - In Excel\n\n\n\n\n\nWe will look at mechanizing the Simplex Method in Excel\n\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/20-SimplexMethodIntuitive/index.html",
    "href": "content/courses/Analytics/Prescriptive/Modules/20-SimplexMethodIntuitive/index.html",
    "title": "💭 The Simplex Method - Intuitively",
    "section": "",
    "text": "To be written up."
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/20-SimplexMethodIntuitive/index.html#what-is-the-simplex-method",
    "href": "content/courses/Analytics/Prescriptive/Modules/20-SimplexMethodIntuitive/index.html#what-is-the-simplex-method",
    "title": "💭 The Simplex Method - Intuitively",
    "section": "",
    "text": "To be written up."
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/20-SimplexMethodIntuitive/index.html#a-random-walk-with-the-simplex-method",
    "href": "content/courses/Analytics/Prescriptive/Modules/20-SimplexMethodIntuitive/index.html#a-random-walk-with-the-simplex-method",
    "title": "💭 The Simplex Method - Intuitively",
    "section": "A Random Walk with the Simplex Method",
    "text": "A Random Walk with the Simplex Method\nLet us try to form a geometric intuition for the Simplex method.\nWe will define an LP problem, and geometrically traverse the steps the Simplex method might take to solve for the optimum solution.\nLet us define a problem:\n\\[\nMaximise\\ 7.75x_1 + 10x_2\\\\\n\\] \\[\nSubject\\ to\\\\\n  \\begin{cases}\n    C1: -3x_1 + 2x_2 &&lt;= 3\\\\\n    C2: 2x_1 + 4x_2 &&lt;= 27\\\\\n    C3: 9x_1 + 10x_2 &&lt;= 90\\\\\n    x_1, x_2 &gt;= 0\n  \\end{cases}\n\\]\nThe Objective function is: \\(7.75x_1 + 10x_2\\)\nThe Constraints are defined by the three inequalities \\(C1::C3\\). In order to plot these, we convert the inequalities to equalities and plot these as lines. Each line splits the \\(x_1:x_2\\) plane into two half-planes. The inequality part is then taken into account by choosing the appropriate half-plane created by the equation. The intersection of all the half-planes defined by the constraints is the Feasibility Region.\nThe Feasibility region for this LP problem is plotted below:\n\n\n\n\n\n\n\n\nThe corner points of the Feasibility Region are:\n\n\n\n\n  \n\n\n\nRecall that:\n\nThe optimum in an LP problem is found on the boundary, at one of the vertices\nAt each of these vertices one or more constraints (\\(C1::C_n\\)) is tight, i.e. there is no slack."
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/20-SimplexMethodIntuitive/index.html#procedure",
    "href": "content/courses/Analytics/Prescriptive/Modules/20-SimplexMethodIntuitive/index.html#procedure",
    "title": "💭 The Simplex Method - Intuitively",
    "section": "Procedure",
    "text": "Procedure\n\nWe start with an arbitrary point on the edge of the Feasibility Region. \\(A = (0,0)\\) is a common choice. At this point, since all variables are \\(0\\), the objective function is also \\(0\\).\nWe (arbitrarily) decide to move along the boundary of the Feasibility Region, to another FSP. We arbitrarily chose the \\(x_1\\) axis, and set/keep \\(x_2 = 0\\). We now wish to find out the \\(x_1\\) coordinate of the next FSP point. This would be at the intersection of the \\(x_1\\) axis and one of the Constraint lines.\nAll the three Constraint Lines would possibly intersect the \\(x_1\\) axis. We need to choose that intercept point that has the smallest, non-negative \\(x_1\\) intercept value. (Why?)\nSo, which Constraint Line intersects the \\(x_1\\) axis at the smallest value? Is it point B, or point F?\nTo find out, we substitute \\(x_2 = 0\\) in each of the Constraint equations, and solve for the \\(x_1\\):\\[\n\\begin{cases}\nC1: -3x_1 + 2 \\times 0 = 3 \\ =&gt; x_1 = \\color{red}{-1}\\\\\nC2: 2x_1 + 4\\times0 = 27 \\ =&gt; x_1 = 13.5\\ Point\\ F\\\\\n   {\\mathbf{ \\color{lightgreen}{C3}: 9x_1 + 10\\times0 = 90 \\ =&gt; x_1 = 10\\  \\color{lightgreen}{Point\\ B}}}\n\\end{cases}\n\\]\nNegative values for any variable are not permitted. So the smallest value of intercept is \\(x_1 = 10\\) for Constraint \\(C3\\). We therefore move to point \\(B(10,0)\\). At this point the objective function has improved to:\n\n\\[\nObjective = 7.75\\times 10 + 10\\times0 = 77.5\\ at\\ Point\\ B\n\\]\n\nWe now start from Point B, and move to the next nearest point. In identical fashion to Step2, we “imagine” that we move along a new axis defined by:\\[\nIntercept = Point\\ B(10,0)\\\\\n\\] \\[\nEquation = Constraint\\ C3: 9x_1 + 10x_2 = 90\\\\\n\\] We express \\(x_1\\) in terms of \\(x_2\\) with \\(C3\\): \\[\n\\hat C3: x_1 = \\frac{90 - 10x_2}{9}\n\\] As in Step 2, we substitute this equation \\(\\hat C3\\) into the other two constraints, \\(C1\\) and \\(C2\\): \\[\n\\begin{cases}\nC1: -3\\times \\frac{90 - 10x_2}{9} + 2x_2 = 3 \\ =&gt; x_2 = 6.18\\ Point\\ K\\\\\n{\\mathbf{ \\color{lightgreen}{C2}: 2\\times \\frac{90 - 10x_2}{9}+ 4x_2 = 27 =&gt; x_2 = 3.93\\ \\color{lightgreen}{Point\\ C}}}\n\\end{cases}\n\\] As before we choose the smaller of the two intercepts, so \\(x_2 = 3.93\\). Calculating for \\(x_1\\), we get point \\(C(5.63, 3.93)\\). At this point the objective function has improved to:\n\n\\[\n7.75\\times 5.63 + 10\\times 3.93 = 82.9\\ at\\ Point\\ C\n\\]\n\nWe now proceed along the constraint line \\(C2\\) towards the next point. In identical fashion to Step 2 and 3, we “imagine” that we move along a new axis defined by: \\[\nIntercept = Point\\ C(5.63,3.93)\n\\] \\[\nEquation = Constraint\\ C2: 2x_1 + 4x_2 = 27 \\\\\n\\] Again, We express \\(x_1\\) in terms of \\(x_2\\) with \\(C2\\) this time: \\[\n\\hat C2: x_1 = \\frac{27 - 4x_2}{2}\n\\] As in Step 2 and, we substitute this equation \\(\\hat C2\\) into the other constraint, the only remaining \\(C1\\): \\[\n{\\mathbf C1: -3\\times \\frac{27 - 4x_2}{2} + 2x_2 = 3 \\ =&gt; x_2 = 5.44\\ Point\\ D\\\\}\n\\] Calculating for \\(x_1\\), we get point \\(C(2.63, 5.44)\\). At this point the objective function has improved decreased to: \\[\n7.75\\times 2.63 + 10\\times 5.44 = 74.8\\ at\\ Point\\ D\n\\] Since this value for the Objective function is smaller than that at the previous point, our search terminates and we decide that Point \\(C(5.63,3.93)\\) is the optimal point.\nSo the final result is: \\[\n   x_1(max) = 5.63\\\\\n\\] \\[\n   x_2(max) = 3.93\\\\\n\\] \\[\n   Maximum\\ Objective\\ Function\\ Value = 82.9\n\\] The final result is plotted below:"
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/20-SimplexMethodIntuitive/index.html#summary",
    "href": "content/courses/Analytics/Prescriptive/Modules/20-SimplexMethodIntuitive/index.html#summary",
    "title": "💭 The Simplex Method - Intuitively",
    "section": "Summary",
    "text": "Summary\nThe essence of this “intuitive method” can be captured as follows:\n\nStart from a known simple point on the edge of Feasibility Region, e.g. (0,0), since the two coordinate axes frequently form two edges to the Feasibility Region.\n\nMove along one of the axis to find a first adjacent edge point. This adjacent point corresponds to the “tightening” of one or other of the Constraint equations(i.e. slack = 0 for that Constraint)\n\nCalculate the Objective function at that point.\n\nUse this new point as the next starting point and move along the Constraint line from the previous step.\n\nRepeat step 2 and 3, calculating the Objective function each time.\n\nKeep the solution point where the objective function hits a maximum, i.e. when moving to the next point reduces the value of the Objective function."
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html",
    "title": "🐉 Introduction to R and RStudio",
    "section": "",
    "text": "At the end of this Lab, we will:\n\nhave installed R and RStudio on our machines\nunderstood how to add additional R-packages for specific features and graphic capability\nrun code within RStudio and interpret the results\nhave learnt to look for help within R and RStudio\nlearnt to use Quarto in R, which a document format for reproducible report generation"
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#iconify-mdi-goal-goals",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#iconify-mdi-goal-goals",
    "title": "🐉 Introduction to R and RStudio",
    "section": "",
    "text": "At the end of this Lab, we will:\n\nhave installed R and RStudio on our machines\nunderstood how to add additional R-packages for specific features and graphic capability\nrun code within RStudio and interpret the results\nhave learnt to look for help within R and RStudio\nlearnt to use Quarto in R, which a document format for reproducible report generation"
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#iconify-fxemoji-japanesesymbolforbeginner-introduction-to-r-and-rstudio",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#iconify-fxemoji-japanesesymbolforbeginner-introduction-to-r-and-rstudio",
    "title": "🐉 Introduction to R and RStudio",
    "section": "\n Introduction to R and RStudio",
    "text": "Introduction to R and RStudio\nThis guide will lead you through the steps to install and use R, a free and open-source software environment for statistical computing and graphics.\nWhat is R?\n\n\nR is the name of the programming language itself, based off S from Bell Labs, which users access through a command-line interpreter (&gt;)\n\nWhat is RStudio?\n\n\nRStudio is a powerful and convenient user interface that allows you to access the R programming language along with a lot of other bells and whistles that enhance functionality (and sanity).\n\nOur end goal is to get you looking at a screen like this:\n\n\nRStudio Default Window"
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#iconify-grommet-icons-install-install-r",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#iconify-grommet-icons-install-install-r",
    "title": "🐉 Introduction to R and RStudio",
    "section": "\n Install R",
    "text": "Install R\nInstall R from CRAN, the Comprehensive R Archive Network. Please choose a precompiled binary distribution for your operating system.\n\n Check in\nLaunch R by clicking this logo  in your Applications. You should see one console with a command line interpreter. Try typing 2 + 2 and check !\nClose R."
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#iconify-grommet-icons-install-install-rstudio",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#iconify-grommet-icons-install-install-rstudio",
    "title": "🐉 Introduction to R and RStudio",
    "section": "\n Install RStudio",
    "text": "Install RStudio\nInstall the free, open-source edition of RStudio: http://www.rstudio.com/products/rstudio/download/\nRStudio provides a powerful user interface for R, called an integrated development environment. RStudio includes:\n\na console (the standard command line interface: &gt;),\na syntax-highlighting editor that supports direct code execution, and\ntools for plotting, history, debugging and work space management.\n\n\n Check in\nLaunch RStudio. You should get a window similar to the screenshot you see here, but yours will be empty. Look at the bottom left pane: this is the same console window you saw when you opened R in step @Check-In-R\n\nPlace your cursor where you see &gt; and type x &lt;- 2 + 2 again hit enter or return, then type x, and hit enter/return again.\nIf [1] 4 prints to the screen, you have successfully installed R and RStudio, and you can move onto installing packages."
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#iconify-noto-v1-package-install-packages",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#iconify-noto-v1-package-install-packages",
    "title": "🐉 Introduction to R and RStudio",
    "section": "\n Install packages",
    "text": "Install packages\nThe version of R that you just downloaded is considered base R, which provides you with good but basic statistical computing and graphics powers. For analytical and graphical super-powers, you’ll need to install add-on packages, which are user-written, to extend/expand your R capabilities. Packages can live in one of two places:\n\nThey may be carefully curated by CRAN (which involves a thorough submission and review process), and thus are easy install using install.packages(\"name_of_package\", dependencies = TRUE) in your CONSOLE.\nPersonal repositories of packages created by practitioners, which are usually in Github.\n\nPlace your cursor in the CONSOLE again (where you last typed x and [4] printed on the screen). You can use the first method to install the following packages directly from CRAN, all of which we will use:\n\nknitr\ntidyverse\nggformula\n\nbabynames\n\n\n\n\n\n\n\n\nInstallation and Usage of R Packages!\n\n\n\n\nTo install a package, you put the name of the package in quotes as in install.packages(\"name_of_package\"). Mind your use of quotes carefully with packages.\nTo use an already installed package, you must load it first, as in library(name_of_package), leaving the name of the package bare. You only need to do this once per RStudio session.\n\n\n\n\nIf you want help, no quotes are needed: help(name_of_package) or ?name_of_package.\nIf you want the citation for a package (and you should give credit where credit is due), ask R as in citation(\"name_of_package\")."
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#iconify-game-icons-papers-using-quarto",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#iconify-game-icons-papers-using-quarto",
    "title": "🐉 Introduction to R and RStudio",
    "section": "\n Using Quarto",
    "text": "Using Quarto\nWe will get acquainted with the Quarto Document format, which allows us to mix text narrative, code, code-developed figures and items from the web in a seamless document. Quarto can be used to generate multiple formats such as HTML, Word, PDF from the same text/code file. Something that can:\n\nprovide a visualization\nprovide insight\ntell a story\nis reproducible\nbe a call to action or a recommendation\n\n\n\n Setting up Quarto\nQuarto is already installed along with RStudio!! We can check if all is in order by running a check in the Terminal in RStudio. \nThe commands are:\n\nquarto check install\nquarto check knitr\n\nIf these come out with no errors then we are ready to fire up our first Quarto document.\n\n Practice\nLet us now create a brand new Quarto document, create some graphs in R and add some narrative text and see how we can generate our first report!\n\nFire up a new Quarto document by going to: File -&gt; New File -&gt; Quarto Document.\n\nGive a title to your document ( “My First Quarto Document”, for example.\nChange the author name to your own! Keep HTML as your output format\nSwitch to Visual mode, if it is not already there. Use the visual mode tool bar.\n\n\n\n\nClick on the various buttons to see what happens. Try to create Sections, code chunks, embedding images and tables.\n\n\n\n\n\n\nAdd Anything Shortcut\n\n\n\nTry the “add anything” shortcut! Type “/” anywhere in your Quarto Doc, while in Visual Mode, and choose what you want to add from the drop-down menu!\n\n\n\nCreate a code chunk as shown below. You can either use the visual tool bar to create it, or simply hit the copy button in the code chunk display on this website and paste the results into your Quarto document. Check every step!\n\n\n```{r}\n#| label: setup\nknitr::opts_chunk$set(warnings = TRUE, errors = FALSE, messages = TRUE)\nlibrary(knitr) # to use this….document! More later!!\nlibrary(tidyverse) # Data Management and Plotting!!\nlibrary(babynames) # A package containing, yes, Baby Names\n```\n\n\nHit the green “play” button to run this “setup” chunk to include in your R session all the installed packages you need.\nLet us greet our data first !!\n\n\n```{r}\n#| label: babynames data\nglimpse(babynames)\nhead(babynames) \ntail(babynames)\nnames(babynames)\n```\n\nRows: 1,924,665\nColumns: 5\n$ year &lt;dbl&gt; 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880,…\n$ sex  &lt;chr&gt; \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", …\n$ name &lt;chr&gt; \"Mary\", \"Anna\", \"Emma\", \"Elizabeth\", \"Minnie\", \"Margaret\", \"Ida\",…\n$ n    &lt;int&gt; 7065, 2604, 2003, 1939, 1746, 1578, 1472, 1414, 1320, 1288, 1258,…\n$ prop &lt;dbl&gt; 0.07238359, 0.02667896, 0.02052149, 0.01986579, 0.01788843, 0.016…\n\n\n\n\n  \n\n\n  \n\n\n\n[1] \"year\" \"sex\"  \"name\" \"n\"    \"prop\"\n\n\n\nIf you have done the above and produced sane-looking output, you are ready for the next bit. Use the code below to create a new data frame called arvind.\n\n\n```{r}\n#| label: manipulate name data\nmy_name_data &lt;- babynames %&gt;%\n  filter(name == \"Arvind\" | name == \"Aravind\") %&gt;% \n  filter(sex == \"M\") \n```\n\n\nThe first bit makes a new dataset called my_name_data that is a copy of the babynames dataset\nthe %&gt;% (pipe) tells you we are doing some other stuff to it later.1\n\nThe second bit filters our babynames to only keep rows where the name is either Arvind or Aravind (read | as “or”.)\nThe third bit applies another filter to keep only those where sex is male.\n\nLet’s check out the data.\n\n```{r}\nmy_name_data\nglimpse(my_name_data)\n```\n\n\n\n  \n\n\n\nRows: 61\nColumns: 5\n$ year &lt;dbl&gt; 1970, 1972, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983,…\n$ sex  &lt;chr&gt; \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", …\n$ name &lt;chr&gt; \"Arvind\", \"Arvind\", \"Arvind\", \"Arvind\", \"Arvind\", \"Arvind\", \"Arvi…\n$ n    &lt;int&gt; 5, 8, 7, 5, 9, 6, 7, 6, 8, 6, 7, 7, 7, 13, 8, 11, 6, 8, 12, 10, 1…\n$ prop &lt;dbl&gt; 2.620e-06, 4.780e-06, 4.310e-06, 3.060e-06, 5.260e-06, 3.510e-06,…\n\n\n\nAgain, if you have sane-looking output here, move along to plotting the data!\n\n\n```{r}\n#| label:  plot_name_data\n\nplot &lt;- gf_line(prop ~ year, color = ~ name, \n        data = my_name_data)\n```\n\nNow if you did this right, you will not see your plot!\n\nBecause we saved the ggplot with a name (plot), R just saved the object for you. But check out the top right pane in RStudio again: under the Environment pane you should see plot, so it is there, you just have to ask for it. Here’s how:\n\n\n```{r}\nplot \n```\n\n\n\n\n\nNow hit the Render button on your Visual toolbar and see what happens!! Try to use the drop down menu next to it and see if there are more output file options!!\n\n Make a new name plot!\n\nEdit my code above to create a new dataset. Pick 2 names to compare how popular they each are (these could be different spellings of your own name, like I did, but you can choose any 2 names that are present in the dataset), and create a new data object with a new name. Make the new plot, changing the name of the data= argument my_name_data in gf_line to the name of your new dataset.\nWrite narratives comments wherever suitable in your Quarto document. MAke sure you don’t type inside your code chunks. See if you can write your comments in sections which you can create with your visual tool bar, or by using the “add anything” shortcut.\nSave your work ( your Quarto document itself) so you can share your favorite plot with us.\nShare your Plot: You will not like the looks of your plot if you mouse over to Export and save it. Instead, use ggplot2’s command for saving a plot with sensible defaults:\n\nType help(ggsave) in your Console.\n```{r}\n#| label: Saving\n\nggsave(\"file_name_here.pdf\", plot) # please make the filename unique!\n```"
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#iconify-fluent-mdl2-decision-solid-conclusions",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#iconify-fluent-mdl2-decision-solid-conclusions",
    "title": "🐉 Introduction to R and RStudio",
    "section": "\n Conclusions",
    "text": "Conclusions\nWe have installed R, RStudio and created our Quarto document, complete with graphs and narrative text. We also rendered our Quarto doc into HTML and other formats!"
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#iconify-fluent-mdl2-reading-mode-readings",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#iconify-fluent-mdl2-reading-mode-readings",
    "title": "🐉 Introduction to R and RStudio",
    "section": "\n Readings",
    "text": "Readings\n\nR for Data Science, Workflow: Basics Chapter: http://r4ds.had.co.nz/workflow-basics.html\nModern Dive, Getting Started Chapter: http://moderndive.com/2-getting-started.html\nR & RStudio Basics: https://bookdown.org/chesterismay/rbasics/3-rstudiobasics.html\nRStudio IDE Cheatsheet: https://github.com/rstudio/cheatsheets/blob/master/rstudio-ide.pdf"
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#iconify-ooui-references-ltr-references",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#iconify-ooui-references-ltr-references",
    "title": "🐉 Introduction to R and RStudio",
    "section": "\n References",
    "text": "References\n\nhttps://rmarkdown.rstudio.com/index.html\n\nhttps://ysc-rmarkdown.netlify.app/slides/01-basics.html Nice RMarkdown presentation and “code movies” !\nhttps://www.markdowntutorial.com\n\nhttps://quarto.org/docs/authoring/markdown-basics.html How to do more with Quarto HTML format"
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#iconify-fa-solid-tasks-assignments",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#iconify-fa-solid-tasks-assignments",
    "title": "🐉 Introduction to R and RStudio",
    "section": "\n Assignment(s)",
    "text": "Assignment(s)\n\nComplete the markdown tutorial in [reference 1]\nLook through the Slides in [reference 2]\nCreate a fresh Quarto document and use as many as possible of the RMarkdown constructs from the Cheatsheet [reference 1]"
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#footnotes",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#footnotes",
    "title": "🐉 Introduction to R and RStudio",
    "section": "Footnotes",
    "text": "Footnotes\n\nInsert the pipe character using the keyboard shortcutCTRL + SHIFT + M.↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Tools/30-Intro-to-Orange/orange.html",
    "href": "content/courses/Analytics/Tools/30-Intro-to-Orange/orange.html",
    "title": "🐉 Introduction to Orange",
    "section": "",
    "text": "Orange is a visual drag-and-drop tool for\n\nData visualization\nMachine Learning\nData mining\n\nand much more. Here is what it looks like:\n\nAll operations are done using a visual menu-driven interface. The visuals can be exported to PNG/SVG/PDF and the entire workflow can be exported into a Report Form and edited for presentation and sharing."
  },
  {
    "objectID": "content/courses/Analytics/Tools/30-Intro-to-Orange/orange.html#introduction-to-orange",
    "href": "content/courses/Analytics/Tools/30-Intro-to-Orange/orange.html#introduction-to-orange",
    "title": "🐉 Introduction to Orange",
    "section": "",
    "text": "Orange is a visual drag-and-drop tool for\n\nData visualization\nMachine Learning\nData mining\n\nand much more. Here is what it looks like:\n\nAll operations are done using a visual menu-driven interface. The visuals can be exported to PNG/SVG/PDF and the entire workflow can be exported into a Report Form and edited for presentation and sharing."
  },
  {
    "objectID": "content/courses/Analytics/Tools/30-Intro-to-Orange/orange.html#installing-orange",
    "href": "content/courses/Analytics/Tools/30-Intro-to-Orange/orange.html#installing-orange",
    "title": "🐉 Introduction to Orange",
    "section": "Installing Orange",
    "text": "Installing Orange\nYou can download and install Orange from here:\nhttps://orangedatamining.com/download/"
  },
  {
    "objectID": "content/courses/Analytics/Tools/30-Intro-to-Orange/orange.html#basic-usage-of-orange",
    "href": "content/courses/Analytics/Tools/30-Intro-to-Orange/orange.html#basic-usage-of-orange",
    "title": "🐉 Introduction to Orange",
    "section": "Basic Usage of Orange",
    "text": "Basic Usage of Orange"
  },
  {
    "objectID": "content/courses/Analytics/Tools/30-Intro-to-Orange/orange.html#orange-workflows",
    "href": "content/courses/Analytics/Tools/30-Intro-to-Orange/orange.html#orange-workflows",
    "title": "🐉 Introduction to Orange",
    "section": "Orange Workflows",
    "text": "Orange Workflows"
  },
  {
    "objectID": "content/courses/Analytics/Tools/30-Intro-to-Orange/orange.html#widgets-and-channels",
    "href": "content/courses/Analytics/Tools/30-Intro-to-Orange/orange.html#widgets-and-channels",
    "title": "🐉 Introduction to Orange",
    "section": "Widgets and Channels",
    "text": "Widgets and Channels"
  },
  {
    "objectID": "content/courses/Analytics/Tools/30-Intro-to-Orange/orange.html#loading-data-into-orange",
    "href": "content/courses/Analytics/Tools/30-Intro-to-Orange/orange.html#loading-data-into-orange",
    "title": "🐉 Introduction to Orange",
    "section": "Loading data into Orange",
    "text": "Loading data into Orange\n\n\n\nWe are good to get started with Orange!!"
  },
  {
    "objectID": "content/courses/Analytics/Tools/30-Intro-to-Orange/orange.html#simple-visuals-using-orange",
    "href": "content/courses/Analytics/Tools/30-Intro-to-Orange/orange.html#simple-visuals-using-orange",
    "title": "🐉 Introduction to Orange",
    "section": "Simple Visuals using Orange",
    "text": "Simple Visuals using Orange\nLet us create some simple visualizations using Orange.\n\nUse the File Widget to import the iris dataset into your session\nUse the Data Table Widget to look at the data, and note its variable names\nUse the Visualization Widgets ( Scatter Plot, Bar Plot, and Distributions) to look at the properties of the variables, and examine relationships between them."
  },
  {
    "objectID": "content/courses/Analytics/Tools/30-Intro-to-Orange/orange.html#reference",
    "href": "content/courses/Analytics/Tools/30-Intro-to-Orange/orange.html#reference",
    "title": "🐉 Introduction to Orange",
    "section": "Reference",
    "text": "Reference\n\nIntroduction to Data Mining-Working notes for the hands-on course with Orange Data Mining.(Download file)\nOrange Data Mining Widget Catalog: Look here for help and guidance! https://orangedatamining.com/widget-catalog/"
  },
  {
    "objectID": "content/courses/Analytics/Workflow/listing.html",
    "href": "content/courses/Analytics/Workflow/listing.html",
    "title": "Workflow",
    "section": "",
    "text": "Using FlexDashboard in R\n\n\nMaking Business Presentations using Flexdashboards in R\n\n\n\n\nflexdashboard\n\n\nDashboard Layouts\n\n\n\n\nMaking Business Presentations using Flexdashboards in R\n\n\n\n\n\n\nMar 10, 2023\n\n\nArvind Venkatadri\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "content/courses/ISTW/listing.html",
    "href": "content/courses/ISTW/listing.html",
    "title": "In Short, the World",
    "section": "",
    "text": "Title\n\n\nReading Time\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/10-InteratedGames/iteratedGames.html",
    "href": "content/courses/MathModelsDesign/Modules/10-InteratedGames/iteratedGames.html",
    "title": "🕹 Iterated Games!",
    "section": "",
    "text": "How many strangers do you “meet” everyday? Why is the Human Species different from all others? Those two questions are not unrelated !!\nThe Human Species is perhaps unique in that we are able to cooperate with strangers!! Yes! Do you think that building and conducting a business is something like that? No? Well, let us see a few quick stories!!\n\n\n\n\n\n\n\n\n\n\n\nLet us watch a (very) small part of this movie, The Three Musketeers -1973\n\n\n\n\nAnd here is an older, slightly longer version:\n\n\n\n\n\n\n\nThis is the conversation that occurs right before the duel scene we saw in the video above, from the book, the three musketeers.\nChapter 5. THE KING’S MUSKETEERS AND THE CARDINAL’S GUARDS.1 Keep an eye on the conversation!\nAnd an ear open for Frenchi-fied English, I beseech you2!!\n\n\n\n\n\n\n\nWhat about this game3 ?\n\n\n\n\n\n\n\nWhat emotions or motivations are portrayed by the characters in the story?\n\nReward\nRegret aka FOMO\nRisk\nRetribution\n\n\n\n\n\n\n\n\nAre there COSTs to ACTIONs? Who decides?\n\nPlayers\nPayoffs\nRules\nStrategies\n\nWhat strategies are possible as a bowler in Cricket? As a batsman? What strategies are permitted? By whom?\n\n\n\n\n\n\n\n\nAnd finally, let us play a Game, the Game of Trust (Website) !\n\n\n\nThis is a famous study of the Iterated Prisoners’ Dilemma by Robert Axelrod, The Evolution of Cooperation,(PDF)\nAnd here is Robert Axelrod on the RadioLab Podcast:\n\n\nSpotify:\n\n\nTranscript: https://www.wnycstudios.org/podcasts/radiolab/segments/104010-one-good-deed-deserves-another\n\n\n\nThe story of Southwest Airlines’ Early Bird Check-In (Weblink)\n\n\n\nWhy are Hotels and Petrol Pumps located next to each other? (Weblink)\n\n\n\n\nThat gains and pains can be quantified\nRationality\nPlayers have identical and opposed aims (Symmetry)\nIn most cases, the game is zero-sum, but not always!!\nSome form of spoke/unspoken agreement as to what may and may not be done ( i.e Rules of the Game )\n\nHere is a good Summary of modern thinking about Human Cooperation: The Evolution of Human Cooperation, PDF\n\n\n\n\nBachchan vs Warsi: Want a Pepsi - Tit for Tat https://www.youtube.com/watch?v=gc6QZcxbMZE\nThe Gods Must be Crazy (Brinkmanship) https://www.youtube.com/watch?v=9LvViKftRnA\nThe Princess Bride - Battle of Wits - Which Strategy to Use here? (Asymmetric Information) https://www.youtube.com/watch?v=rMz7JBRbmNo\nChris de Burgh: Don’t Pay the Ferryman! https://www.youtube.com/watch?v=Q-a5TAL-IXs\nAbba: The Name of the Game https://www.youtube.com/watch?v=T5Qf_7HM1cM\n\n\n\n\n\nJohn D. Williams, The Compleat Strategyst: Being a Primer on the Theory of Games of Strategy, RAND Corporation, ISBN:9780833042224. This is a very humourous and fun book on Game Theory ! It is available for free online at the RAND Corporation Website.\nKen Binmore, Playing for Real: A Text on Game Theory, ISBN:9780195300574, Oxford University Press, March 2007. Available (here.)\nAvinash Dixit, Susan Skeath, David Reiley, Games of strategy, ISBN: 9780393124446, New York :W.W. Norton & Company, 2015.\nBrams Steven J., 1994. “Game Theory and Literature,” Games and Economic Behavior, Elsevier, vol. 6(1), pages 32-54, January.PDF\nAlexander Mehlmann,The game’s afoot! Game theory in myth and paradox, American Mathematical Society 2000. (AMS Bookstore.)"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/10-InteratedGames/iteratedGames.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/10-InteratedGames/iteratedGames.html#introduction",
    "title": "🕹 Iterated Games!",
    "section": "",
    "text": "How many strangers do you “meet” everyday? Why is the Human Species different from all others? Those two questions are not unrelated !!\nThe Human Species is perhaps unique in that we are able to cooperate with strangers!! Yes! Do you think that building and conducting a business is something like that? No? Well, let us see a few quick stories!!\n\n\n\n\n\n\n\n\n\n\n\nLet us watch a (very) small part of this movie, The Three Musketeers -1973\n\n\n\n\nAnd here is an older, slightly longer version:\n\n\n\n\n\n\n\nThis is the conversation that occurs right before the duel scene we saw in the video above, from the book, the three musketeers.\nChapter 5. THE KING’S MUSKETEERS AND THE CARDINAL’S GUARDS.1 Keep an eye on the conversation!\nAnd an ear open for Frenchi-fied English, I beseech you2!!\n\n\n\n\n\n\n\nWhat about this game3 ?\n\n\n\n\n\n\n\nWhat emotions or motivations are portrayed by the characters in the story?\n\nReward\nRegret aka FOMO\nRisk\nRetribution\n\n\n\n\n\n\n\n\nAre there COSTs to ACTIONs? Who decides?\n\nPlayers\nPayoffs\nRules\nStrategies\n\nWhat strategies are possible as a bowler in Cricket? As a batsman? What strategies are permitted? By whom?\n\n\n\n\n\n\n\n\nAnd finally, let us play a Game, the Game of Trust (Website) !\n\n\n\nThis is a famous study of the Iterated Prisoners’ Dilemma by Robert Axelrod, The Evolution of Cooperation,(PDF)\nAnd here is Robert Axelrod on the RadioLab Podcast:\n\n\nSpotify:\n\n\nTranscript: https://www.wnycstudios.org/podcasts/radiolab/segments/104010-one-good-deed-deserves-another\n\n\n\nThe story of Southwest Airlines’ Early Bird Check-In (Weblink)\n\n\n\nWhy are Hotels and Petrol Pumps located next to each other? (Weblink)\n\n\n\n\nThat gains and pains can be quantified\nRationality\nPlayers have identical and opposed aims (Symmetry)\nIn most cases, the game is zero-sum, but not always!!\nSome form of spoke/unspoken agreement as to what may and may not be done ( i.e Rules of the Game )\n\nHere is a good Summary of modern thinking about Human Cooperation: The Evolution of Human Cooperation, PDF\n\n\n\n\nBachchan vs Warsi: Want a Pepsi - Tit for Tat https://www.youtube.com/watch?v=gc6QZcxbMZE\nThe Gods Must be Crazy (Brinkmanship) https://www.youtube.com/watch?v=9LvViKftRnA\nThe Princess Bride - Battle of Wits - Which Strategy to Use here? (Asymmetric Information) https://www.youtube.com/watch?v=rMz7JBRbmNo\nChris de Burgh: Don’t Pay the Ferryman! https://www.youtube.com/watch?v=Q-a5TAL-IXs\nAbba: The Name of the Game https://www.youtube.com/watch?v=T5Qf_7HM1cM\n\n\n\n\n\nJohn D. Williams, The Compleat Strategyst: Being a Primer on the Theory of Games of Strategy, RAND Corporation, ISBN:9780833042224. This is a very humourous and fun book on Game Theory ! It is available for free online at the RAND Corporation Website.\nKen Binmore, Playing for Real: A Text on Game Theory, ISBN:9780195300574, Oxford University Press, March 2007. Available (here.)\nAvinash Dixit, Susan Skeath, David Reiley, Games of strategy, ISBN: 9780393124446, New York :W.W. Norton & Company, 2015.\nBrams Steven J., 1994. “Game Theory and Literature,” Games and Economic Behavior, Elsevier, vol. 6(1), pages 32-54, January.PDF\nAlexander Mehlmann,The game’s afoot! Game theory in myth and paradox, American Mathematical Society 2000. (AMS Bookstore.)"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/10-InteratedGames/iteratedGames.html#footnotes",
    "href": "content/courses/MathModelsDesign/Modules/10-InteratedGames/iteratedGames.html#footnotes",
    "title": "🕹 Iterated Games!",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDownload The Three Musketeers e-book at Project Gutenberg]↩︎\nListen to the complete book at Librivox↩︎\nSquid Game↩︎"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/30-SchellingPoints/schelling.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/30-SchellingPoints/schelling.html#introduction",
    "title": "Coordination Games and Schelling Focus Points",
    "section": "Introduction",
    "text": "Introduction\nWould you wear a veshti/kurta/angavastram, or silk paavade to college? When you want someone to search the internet, what do you tell them do? How do you show someone that you agree with what they are doing at that moment?\nIn the module on Game Theory we saw how we all are engaged in adversarial transactional Games with many other people, most of whom are strangers to us. The Prisoner’s Dilemma.\nAs remarked before, the Human Species is perhaps unique in that we are able to cooperate with strangers!! So we are not always adversaries or enemies, and we are able to obtain unspoken agreement and correspondence with strangers.\nWhat use would this be? An how do these ideas have anything to with building businesses?\nToday, we will study Coordination Games and Schelling Focus Points, and see how these concepts can help better businesses.\n\nCoordination Games and Schelling Focus Points\nWatch these two videos on the topic of Focal Points as defined by Schelling.\n\n\n\n\nFocal points can be used in Coordination Games: these are activities where people’s actions tend to organize into Patterns based on specific default options. These actions could be\n\nConversation\nDress\nTravel / Meeting\nLocation\nAttention"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/30-SchellingPoints/schelling.html#how-does-this-apply-to-business",
    "href": "content/courses/MathModelsDesign/Modules/30-SchellingPoints/schelling.html#how-does-this-apply-to-business",
    "title": "Coordination Games and Schelling Focus Points",
    "section": "How Does This Apply to Business?",
    "text": "How Does This Apply to Business?\nLet us now look at a few uses of “cultural Schelling Points”!!\n\nCase Study #1: Tanishq Ad\n\n\n\nCase Study #2: Havell’s Ad\n\nSo:\n\nyou want your customers to coordinate towards your product, your ad, your office, and ….you.\nYou will use things or ideas that are culturally salient to your customers to enable that coordination!\nYou location, your ad, your logo, your tag-line, your motto, your message, your company colours…all are capable of being Schelling Points that focus the attention of your customers\n\n\n\nAdditional Readings on Schelling Points\n\nhttps://mindyourdecisions.com/blog/2008/04/01/focal-points-or-schelling-points-how-we-naturally-organize-in-games-of-coordination/\nhttps://medium.com/@richlitt/using-schelling-points-to-perform-better-973243efd989"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/60-Randomness/randomness.html",
    "href": "content/courses/MathModelsDesign/Modules/60-Randomness/randomness.html",
    "title": "Eating Mangoes with Hamlet",
    "section": "",
    "text": "Let us first examine the street traffic data we have gathered for any “model-like” pattern!\n\nWe will use a tool called, umm….WTFcsv. Let us first quickly see what this tool offers us!\n {{% vimeo \"150216437\" %}} \nLet us import our data into the WTFcsv tool (Web Link) and see what patterns lurk beneath our data!\n\nLet us throw some chalk or Lego pieces on the floor at random and see what happens!!"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/60-Randomness/randomness.html#footnotes",
    "href": "content/courses/MathModelsDesign/Modules/60-Randomness/randomness.html#footnotes",
    "title": "Eating Mangoes with Hamlet",
    "section": "Footnotes",
    "text": "Footnotes\n\nWilliam Blake, Auguries of Innocence, https://www.poetryfoundation.org/poems/43650/auguries-of-innocence↩︎"
  },
  {
    "objectID": "content/courses/ML4Artists/1-IntroOrange/index.html",
    "href": "content/courses/ML4Artists/1-IntroOrange/index.html",
    "title": "🐉 Intro to Orange",
    "section": "",
    "text": "Orange is a visual drag-and-drop tool for\n\nData visualization\n\nMachine Learning\n\nData mining\n\nand much more. Here is what it looks like:\n\nAll operations are done using a visual menu-driven interface. The visuals can be exported to PNG/SVG/PDF and the entire workflow can be exported into a Report Form and edited for presentation and sharing."
  },
  {
    "objectID": "content/courses/ML4Artists/1-IntroOrange/index.html#introduction-to-orange",
    "href": "content/courses/ML4Artists/1-IntroOrange/index.html#introduction-to-orange",
    "title": "🐉 Intro to Orange",
    "section": "",
    "text": "Orange is a visual drag-and-drop tool for\n\nData visualization\n\nMachine Learning\n\nData mining\n\nand much more. Here is what it looks like:\n\nAll operations are done using a visual menu-driven interface. The visuals can be exported to PNG/SVG/PDF and the entire workflow can be exported into a Report Form and edited for presentation and sharing."
  },
  {
    "objectID": "content/courses/ML4Artists/1-IntroOrange/index.html#installing-orange",
    "href": "content/courses/ML4Artists/1-IntroOrange/index.html#installing-orange",
    "title": "🐉 Intro to Orange",
    "section": "Installing Orange",
    "text": "Installing Orange\nYou can download and install Orange from here:\nhttps://orangedatamining.com/download/"
  },
  {
    "objectID": "content/courses/ML4Artists/1-IntroOrange/index.html#basic-usage-of-orange",
    "href": "content/courses/ML4Artists/1-IntroOrange/index.html#basic-usage-of-orange",
    "title": "🐉 Intro to Orange",
    "section": "Basic Usage of Orange",
    "text": "Basic Usage of Orange\n{{% youtube \"HXjnDIgGDuI\" %}}"
  },
  {
    "objectID": "content/courses/ML4Artists/1-IntroOrange/index.html#orange-workflows",
    "href": "content/courses/ML4Artists/1-IntroOrange/index.html#orange-workflows",
    "title": "🐉 Intro to Orange",
    "section": "Orange Workflows",
    "text": "Orange Workflows\n{{% youtube \"lb-x36xqJ-E\" %}}"
  },
  {
    "objectID": "content/courses/ML4Artists/1-IntroOrange/index.html#widgets-and-channels",
    "href": "content/courses/ML4Artists/1-IntroOrange/index.html#widgets-and-channels",
    "title": "🐉 Intro to Orange",
    "section": "Widgets and Channels",
    "text": "Widgets and Channels\n{{% youtube \"2xS6QjnG714\" %}}"
  },
  {
    "objectID": "content/courses/ML4Artists/1-IntroOrange/index.html#loading-data-into-orange",
    "href": "content/courses/ML4Artists/1-IntroOrange/index.html#loading-data-into-orange",
    "title": "🐉 Intro to Orange",
    "section": "Loading data into Orange",
    "text": "Loading data into Orange\n{{% youtube \"MHcGdQeYCMg\" %}} \nWe are good to get started with Orange!!"
  },
  {
    "objectID": "content/courses/ML4Artists/1-IntroOrange/index.html#simple-visuals-using-orange",
    "href": "content/courses/ML4Artists/1-IntroOrange/index.html#simple-visuals-using-orange",
    "title": "🐉 Intro to Orange",
    "section": "Simple Visuals using Orange",
    "text": "Simple Visuals using Orange\nLet us create some simple visualizations using Orange.\n\nUse the File Widget to import the iris dataset into your session\nUse the Data Table Widget to look at the data, and note its variable names\nUse the Visualization Widgets ( Scatter Plot, Bar Plot, and Distributions) to look at the properties of the variables, and examine relationships between them."
  },
  {
    "objectID": "content/courses/ML4Artists/1-IntroOrange/index.html#reference",
    "href": "content/courses/ML4Artists/1-IntroOrange/index.html#reference",
    "title": "🐉 Intro to Orange",
    "section": "Reference",
    "text": "Reference\n\nIntroduction to Data Mining-Working notes for the hands-on course with Orange Data Mining. (Download file)"
  },
  {
    "objectID": "content/courses/ML4Artists/3-Classification/index.html",
    "href": "content/courses/ML4Artists/3-Classification/index.html",
    "title": "Basics of Machine Learning - Classification",
    "section": "",
    "text": "Have you played a Childhood Game called 20 Questions? Someone has a “target” entity in mind ( a person or a thing or a literary character) and the others need to discover that entity by asking 20 questions.\n\nHow does one create questions in the game?\n\nCategories?\nNumbers? How?\nComparisons?\n\n\nWhat sort of answers can you expect for each question?"
  },
  {
    "objectID": "content/courses/ML4Artists/3-Classification/index.html#a-childhood-game",
    "href": "content/courses/ML4Artists/3-Classification/index.html#a-childhood-game",
    "title": "Basics of Machine Learning - Classification",
    "section": "",
    "text": "Have you played a Childhood Game called 20 Questions? Someone has a “target” entity in mind ( a person or a thing or a literary character) and the others need to discover that entity by asking 20 questions.\n\nHow does one create questions in the game?\n\nCategories?\nNumbers? How?\nComparisons?\n\n\nWhat sort of answers can you expect for each question?"
  },
  {
    "objectID": "content/courses/ML4Artists/3-Classification/index.html#twenty-questions-game-as-a-play-with-data",
    "href": "content/courses/ML4Artists/3-Classification/index.html#twenty-questions-game-as-a-play-with-data",
    "title": "Basics of Machine Learning - Classification",
    "section": "Twenty Questions Game as a Play with Data…",
    "text": "Twenty Questions Game as a Play with Data…\nAssuming we think of a 20Q Target as say, celebrity singer like Taylor Swift, or a cartoon character like Thomas the Tank Engine, what would an underlying “data structure” look like? We would ask Questions for instance in the following order to find the target of Taylor Swift:\n- Human?(Yes)\n- Living?(Yes)\n- Male?(No)\n- Celebrity?(Yes)\n- Music?(Yes)\n- USA?(Yes)\nOh…Taylor Swift!!!\nLet us try to construct the “datasets” underlying this game!\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nOccupation\nSex\nLiving\nNationality\ngenre\npet\n\n\nTaylor Swift\nSinger\nF\nTRUE\nUSA\ncountry/rock\nScottish Fold Cats\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nType\nLiving\nhuman\nNationality\ncolour\nmaterial\n\n\nThomas, the Tank Engine\nCartoon Character\nFALSE\nFALSE\nUK\nblue\nmetal\n\n\n\n\nIt should be fairly clear that the Questions we ask are based on the COLUMNs in the respective 1-row datasets! The TARGET Column in both cases is the name column."
  },
  {
    "objectID": "content/courses/ML4Artists/3-Classification/index.html#what-is-a-decision-tree",
    "href": "content/courses/ML4Artists/3-Classification/index.html#what-is-a-decision-tree",
    "title": "Basics of Machine Learning - Classification",
    "section": "What is a Decision Tree?",
    "text": "What is a Decision Tree?\nCan you imagine how the 20 Questions Game can be shown as a tree?\n\n\n\n\n\n\nEach Question we ask, based on one of the Feature columns, begets a Yes/NO answer and we turn the left or right accordingly. When we arrive at the leaf, we should be in a position to guess the answer !"
  },
  {
    "objectID": "content/courses/ML4Artists/3-Classification/index.html#twenty-times-20-questions",
    "href": "content/courses/ML4Artists/3-Classification/index.html#twenty-times-20-questions",
    "title": "Basics of Machine Learning - Classification",
    "section": "Twenty times 20 Questions !!",
    "text": "Twenty times 20 Questions !!\nWhat if the dataset we had contained many rows, instead of just one row? How would we play the 20Q Game in this situation? Here is a sample of the famous penguins dataset:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\nAdelie\nDream\n41.1\n18.1\n205\n4300\nmale\n2008\n\n\nAdelie\nDream\n40.8\n18.4\n195\n3900\nmale\n2007\n\n\nAdelie\nTorgersen\n42.1\n19.1\n195\n4000\nmale\n2008\n\n\nAdelie\nDream\n41.1\n19.0\n182\n3425\nmale\n2007\n\n\nAdelie\nBiscoe\n38.2\n18.1\n185\n3950\nmale\n2007\n\n\nAdelie\nDream\n39.5\n16.7\n178\n3250\nfemale\n2007\n\n\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n2007\n\n\nGentoo\nBiscoe\n46.2\n14.4\n214\n4650\nNA\n2008\n\n\nAdelie\nDream\n39.7\n17.9\n193\n4250\nmale\n2009\n\n\nChinstrap\nDream\n42.4\n17.3\n181\n3600\nfemale\n2007\n\n\nGentoo\nBiscoe\n44.5\n14.3\n216\n4100\nNA\n2007\n\n\nAdelie\nTorgersen\n45.8\n18.9\n197\n4150\nmale\n2008\n\n\n\n\n\nAs before, we would need to look at the dataset as containing a TARGET column which we want to predict using several other FEATURE columns. Let us choose species.\nWhen we look at the FEATURE columns, We would need to formulate questions based on entire columns at a time. For instance:\n\n\n“Is the bill_length_mm* greater than 45mm?” considers the entire bill_length_mm* FEATURE column\nIs the sex female? considers the entire sex column\n\nIf the specific FEATURE column is a Numerical (N) variable, the question would use some “thresholding” as shown in the question above, to convert the Numerical Variable into a Categorical variable.\nIf a specific FEATURE column is a Categorical (C) variable, the question would be like a filter operation in Excel.\nEither way, we end up answering with a smaller and smaller subset of rows in the dataset, to which the questions are answered with a Yes. It is as if we played many 20 Questions games in parallel, since there are so many simultaneous “answers”!\nOnce we exhaust all the FEATURE columns, then what remains is a subset (i.e. rows) of the original dataset and we read off the TARGET column, which should now contain a set of identical entries, e.g. “Adelie”. Thus we can extend a single-target 20Q game to a multiple-target one using a larger dataset. ( Note how the multiple targets are all the same: “Adelie”, or “Gentoo”, or “Chinstrap”)\nThis forms the basic intuition for a Machine Learning Algorithm called a Decision Tree.\nDecision Tree in Orange\nLet us visualize this Decision Tree in Orange. Look at the now famous penguins dataset, available here:\nhttps://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\nWe see that there are three species of penguins, that live on three islands. The measurements for each penguin are flipper_length_mm, bill_length_mm, bill_depth_mm, and body_mass_g.\n\nTask 1: Create a few data visualizations for the variables, and pairs of variables from this dataset.\nTask 2: Can you inspect the visualizations and imagine how each of this dataset can be used in a 20 Questions Game, to create a Decision Tree for this dataset as shown below?\n\n\n\nPenguins Decision Tree!\n\nWhat did we learn?\n\nThe 20Q Game can be viewed as a “Decision Tree” of Questions and Answers,\nEach fork in the game is a Question.\nDepending upon whether the current answer is yes or no, we turn in one direction or the other.\nEach of our questions is based on the information available in one or other of the columns!!\nWe arrive at a final “answer” or “target” after a particular sequence of yes/no answers. This is the one of the leaf nodes in the Tree.\nThe island and the species columns are categories and are especially suited to being the targets for a 20 Questions Game.\nWe can therefore use an entire column of data as our 20Questions target, rather than just one entity, person.\n\nThis is how we will use this Game as a Model for our first ML algorithm, classification using Decision Trees."
  },
  {
    "objectID": "content/courses/ML4Artists/3-Classification/index.html#how-do-we-make-predictions-using-our-decision-tree",
    "href": "content/courses/ML4Artists/3-Classification/index.html#how-do-we-make-predictions-using-our-decision-tree",
    "title": "Basics of Machine Learning - Classification",
    "section": "How do we Make Predictions using our Decision Tree",
    "text": "How do we Make Predictions using our Decision Tree\nOur aim is to make predictions. Predictions of what? When we are given new unseen data in the same format, we should be able to predict TARGET variable using the same FEATURE columns.\nNOTE: This that is usually a class/category (We CAN also predict a numerical value with a Decision Tree; but we will deal with that later.)\nIn order to make predictions with completely unseen data, we need to first check if the algorithm is working well with known data. The way to do this is to use a large portion of data to design the tree, and then use the tree to predict some aspect of the remaining, but similar, data. Let us split the penguins dataset into two pieces: a training set to design our tree, and a test set to check how it is working.\nDownload this penguin tree file and open it in Orange.\nHow good are the Predictions? What is the Classification Error Rate?"
  },
  {
    "objectID": "content/courses/ML4Artists/3-Classification/index.html#how-many-trees-do-we-need-enter-the-random-forest",
    "href": "content/courses/ML4Artists/3-Classification/index.html#how-many-trees-do-we-need-enter-the-random-forest",
    "title": "Basics of Machine Learning - Classification",
    "section": "How Many Trees do we Need? Enter the Random Forest!",
    "text": "How Many Trees do we Need? Enter the Random Forest!\nCheck all your individual Decision Trees: do they ask the same Questions? Do they fork in the same way? Yes, they all seem to use the same set of parameters to reach the target. So they are capable of being “biased” and make the same mistakes. So we ask: Does it help to use more than one tree, if all the questions/forks in the Trees are similar?\nNo…we need different Trees to be able to ask different questions, based on different variables or features in the data. That will make the Trees as different as possible and so…unbiased. This is what we also saw when we played 20Q: offbeat questions opened up some avenues for predicting the answer/target.\nA forest of such trees is called the Wild Wood a Random Forest !"
  },
  {
    "objectID": "content/courses/ML4Artists/3-Classification/index.html#an-introduction-to-random-forests",
    "href": "content/courses/ML4Artists/3-Classification/index.html#an-introduction-to-random-forests",
    "title": "Basics of Machine Learning - Classification",
    "section": "An Introduction to Random Forests",
    "text": "An Introduction to Random Forests\nIn the Random Forest method, we do as follows:\n\nSplit the dataset into training and test subsets (70::30 proportion is very common). Keep aside the testing dataset for final testing.\nDecide on a number of trees, say 100-500 in the forest.\nTake the training dataset and repeatedly sample some of the rows in it. Rows can be repeated too; this is called bootstrap sampling.\nGive this sampled training set to each tree. Each tree develops a question from this dataset, in a random fashion, using a randomly chosen variable. E.g. with penguins, if our target is species, then some trees will will use island, some others will use body_mass_g and some others may use bill_length_mm.\nEach tree will “grow its questions” in a unique way !! Since the questions are possibly based on a different variable at each time, the trees will grow in very different ways.\nStop when the required accuracy has been achieved (the sets contain observations/rows from only one species predominantly)\nWith the test set let each tree vote on which species it has decided upon. Take the majority vote.\n\nPhew!!\nLet’s get a visual sense of how all this works:\nhttps://waternova.github.io/random-forest-viz/"
  },
  {
    "objectID": "content/courses/ML4Artists/3-Classification/index.html#random-forest-classification-for-heart-patients",
    "href": "content/courses/ML4Artists/3-Classification/index.html#random-forest-classification-for-heart-patients",
    "title": "Basics of Machine Learning - Classification",
    "section": "Random Forest Classification for Heart Patients",
    "text": "Random Forest Classification for Heart Patients\nDo you want to develop an ML model for heart patients? We have a dataset of heart patients at the University of California, Arvind Irvine ML Dataset Repository\nHeart Patient Data. Import into Orange !!\nWhat are the variables?\n\n(age): age in years\n\n(sex): 1 = male; 0 = female\n\n(cp): chest-pain type( 4 types, 1/2/3/4)\n\n(trestbps): resting blood pressure (in mm Hg on admission to the hospital)\n(chol) : serum cholesterol in mg/dl\n(fbs): (fasting blood sugar &gt; 120 mg/dl) (1 = true; 0 = false)\n(restecg): resting electrocardiograph results (0 = normal; 1= ST-T wave abnormality; 3 = LV hypertrophy)\n(thalach): maximum heart rate achieved\n(exang): exercise induced angina (1 = yes; 0 = no) (remember Puneet Rajkumar)\n\n(oldpeak): ST depression induced by exercise relative to rest\n(slope): the slope of the peak exercise ST segment\nValue 1: upsloping\n\nValue 2: flat\n\nValue 3: downsloping\n\n\n\n(ca): number of major vessels (0-3) colored by fluoroscopy\n\n(thal): 3 = normal; 6 = fixed defect; 7 = reversible defect\n\n(num) : the target attribute, diagnosis of heart disease (angiographic disease status)\nValue 0: &lt; 50% diameter narrowing\n\nValue 1: &gt; 50% diameter narrowing\n(in any major vessel: attributes 59 through 68 are vessels)\n\n\n\nWe will create a Random Forest Model for this dataset, and compare with the Desision Tree for the same dataset."
  },
  {
    "objectID": "content/courses/ML4Artists/3-Classification/index.html#how-good-is-my-random-forest",
    "href": "content/courses/ML4Artists/3-Classification/index.html#how-good-is-my-random-forest",
    "title": "Basics of Machine Learning - Classification",
    "section": "How good is my Random Forest?",
    "text": "How good is my Random Forest?\n\nClassification Error\n\nGini Impurity\n\nCross Entropy"
  },
  {
    "objectID": "content/courses/ML4Artists/3-Classification/index.html#references",
    "href": "content/courses/ML4Artists/3-Classification/index.html#references",
    "title": "Basics of Machine Learning - Classification",
    "section": "References",
    "text": "References\n\nhttps://towardsdatascience.com/data-science-made-easy-data-modeling-and-prediction-using-orange-f451f17061fa\nThe beauty of Random Forests: https://orangedatamining.com/blog/2016/12/22/the-beauty-of-random-forest/\nPythagorean Trees for Random Forests: https://orangedatamining.com/blog/2016/07/29/pythagorean-trees-and-forests/\ndata.tree sample applications, Christoph Glur, 2020-07-31. https://cran.r-project.org/web/packages/data.tree/vignettes/applications.html"
  },
  {
    "objectID": "content/courses/ML4Artists/4-Clustering/index.html",
    "href": "content/courses/ML4Artists/4-Clustering/index.html",
    "title": "Basics of Machine Learning - Clustering",
    "section": "",
    "text": "Introduction\n\n\n Back to top"
  },
  {
    "objectID": "content/courses/NoCode/listing.html",
    "href": "content/courses/NoCode/listing.html",
    "title": "Data Vis with No Code",
    "section": "",
    "text": "Title\n\n\nReading Time\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/10-Basics/index.html#fa-envelope-titlean-envelope-where-does-data-come-from",
    "href": "content/courses/R4Artists/Modules/10-Basics/index.html#fa-envelope-titlean-envelope-where-does-data-come-from",
    "title": "🕶 Lab-1: Science, Human Experience, Experiments, and Data",
    "section": "\n Where does Data come from?",
    "text": "Where does Data come from?\nWe will need to form a basic understanding of basic scientific enterprise. Let us look at the slides."
  },
  {
    "objectID": "content/courses/R4Artists/Modules/10-Basics/index.html#fa-chart-simple-why-visualize",
    "href": "content/courses/R4Artists/Modules/10-Basics/index.html#fa-chart-simple-why-visualize",
    "title": "🕶 Lab-1: Science, Human Experience, Experiments, and Data",
    "section": "\n Why Visualize?",
    "text": "Why Visualize?\n\nWe can digest information more easily when it is pictorial\nOur Working Memories are both short-term and limited in capacity. So a picture abstracts the details and presents us with an overall summary, an insight, or a story that is both easy to recall and easy on retention.\n\nData Viz includes shapes that carry strong cultural memories and impressions for us. These cultural memories help us to use data viz in a universal way to appeal to a wide variety of audiences. (Do humans have a gene for geometry?)\nIt helps sift facts and mere statements: for example:\n\n\n\nRape Capital\n\n\n\nWhat does Data Reveal?"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/10-Basics/index.html#iconify-mdi-category-plus-what-are-data-types",
    "href": "content/courses/R4Artists/Modules/10-Basics/index.html#iconify-mdi-category-plus-what-are-data-types",
    "title": "🕶 Lab-1: Science, Human Experience, Experiments, and Data",
    "section": "\n What are Data Types?",
    "text": "What are Data Types?\n\n\n\nIn more detail:"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/10-Basics/index.html#sec-data-types",
    "href": "content/courses/R4Artists/Modules/10-Basics/index.html#sec-data-types",
    "title": "🕶 Lab-1: Science, Human Experience, Experiments, and Data",
    "section": "\n How do we Spot Data Variable Types?",
    "text": "How do we Spot Data Variable Types?\nBy asking questions!\n\n\n\n\n\nNo\nPronoun\nAnswer\nVariable/Scale\nExample\nWhat Operations?\n\n\n\n1\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities, with Scale and a Zero Value.Differences and Ratios /Products are meaningful.\nQuantitative/Ratio\nLength,Height,Temperature in Kelvin,Activity,Dose Amount,Reaction Rate,Flow Rate,Concentration,Pulse,Survival Rate\nCorrelation\n\n\n2\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities with Scale. Differences are meaningful, but not products or ratios\nQuantitative/Interval\npH,SAT score(200-800),Credit score(300-850),SAT score(200-800),Year of Starting College\nMean,Standard Deviation\n\n\n3\nHow, What Kind, What Sort\nA Manner / Method, Type or Attribute from a list, with list items in some \" order\" ( e.g. good, better, improved, best..)\nQualitative/Ordinal\nSocioeconomic status (Low income, Middle income, High income),Education level (HighSchool, BS, MS, PhD),Satisfaction rating(Very much Dislike, Dislike, Neutral, Like, Very Much Like)\nMedian,Percentile\n\n\n4\nWhat, Who, Where, Whom, Which\nName, Place, Animal, Thing\nQualitative/Nominal\nName\nCount no. of cases,Mode\n\n\n\n\n\n\nAs you go from Qualitative to Quantitative data types in the table, I hope you can detect a movement from fuzzy groups/categories to more and more crystallized numbers.\n\n\nType of Variables\n\nEach variable/scale can be subjected to the operations of the previous group. In the words of S.S. Stevens\n\nthe basic operations needed to create each type of scale is cumulative: to an operation listed opposite a particular scale must be added all those operations preceding it."
  },
  {
    "objectID": "content/courses/R4Artists/Modules/10-Basics/index.html#sec-data-viz",
    "href": "content/courses/R4Artists/Modules/10-Basics/index.html#sec-data-viz",
    "title": "🕶 Lab-1: Science, Human Experience, Experiments, and Data",
    "section": "What Are the Parts of a Data Viz?",
    "text": "What Are the Parts of a Data Viz?"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/10-Basics/index.html#how-to-pick-a-data-viz",
    "href": "content/courses/R4Artists/Modules/10-Basics/index.html#how-to-pick-a-data-viz",
    "title": "🕶 Lab-1: Science, Human Experience, Experiments, and Data",
    "section": "How to pick a Data Viz?",
    "text": "How to pick a Data Viz?\nMost Data Visualizations use one or more of the following geometric attributes or aesthetics. These geometric aesthetics are used to represent qualitative or quantitative variables from your data.\n\n\nCommon Geometric Aesthetics in Charts\n\nWhat does this mean? We can think of simple visualizations as combinations of these aesthetics. Some examples:\n\n\nGeometries , Combinations, and Graphs\n\n\n\n\n\n\nAesthetic #1\nAesthetic #2\nShape\n\n\n\nPosition X = Quant Variable\nPosition Y = Quant Variable\nP oints/Circles with Fixed Size\n\n\nPosition X = Qual Variable\nPosition Y = Count of Qual Variable\nColumns\n\n\nPosition X = Qual Variable\nPosition Y = Qual Variable\nRectangles, with area proportional to joint(X,Y) count\n\n\n\nPosition X = Qual Variable\nPosition Y = Rank-Ordered Quant Variable\nBox + Whisker, Box length proportional to I nter-Quartile Range, w hisker-length proportional to upper and lower quartile resp.\n\n\nPosition X = Quant Variable\nPosition Y = Quant Variable + Qual Variable\n\nLine and Area\nColour for Area"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html",
    "title": "The Grammar of Diagrams",
    "section": "",
    "text": "There are many presentation and drawing tools out there. And these allow the user full control over the diagram so generally result in prettier diagrams that can convey more information to the audience at that point in time.\nBut that point in time passes, and pretty pictures can quickly become out-of-date and, ironically, misinforming if they don’t match the reality of the system they are describing. This is especially so if one team is drawing the pretty pictures, and another team is writing the software/implementing the system.\nHaving diagrams as code that can live beside the system design/code, that the stakeholders are equally comfortable editing and viewing,reduces the gap i.e. “Where system diagrams meet system reality”.\nWe will “explore” two packages to do this: DiagrammeR and nomnoml. Each of these follows a specific grammar so that sets of “sentences” will morph into very different kinds of diagrams."
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#sequence-diagram",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#sequence-diagram",
    "title": "The Grammar of Diagrams",
    "section": "Sequence Diagram",
    "text": "Sequence Diagram\nLook at the code below: What do you think it represents?\n\nCodeDiagrammeR(\"\nsequenceDiagram\nArvind -&gt;&gt; Anamika: Why are you late today?\nAnamika -&gt;&gt; Anamika: Ulp...\nAnamika -&gt;&gt; Arvind: I am sorry... &lt;br&gt; may I come in please?\n\nArvind -&gt;&gt; Komal: And you? What kept you?\nKomal -&gt;&gt; Anamika: (Quietly) He's having a bad day, dude...\nAnamika -&gt;&gt; Komal: (Whisper) Boomer...\n\")\n\n\n\n\n\nThis is a simple Sequence Diagram! Shows a strictly imaginary exchange between a pair of students and an unknown Faculty Member.\nLet us now see how we can embellish this kind of diagram. Can we have a Garden of Forking Paths?\n\nCodeDiagrammeR(\"\n  graph LR\n    A--&gt;B\n    A--&gt;C\n    C--&gt;E\n    B--&gt;D\n    C--&gt;D\n    D--&gt;F\n    E--&gt;F\n\")\n\n\n\n\n\n\nCodeDiagrammeR(\"\n        sequenceDiagram\n        \n        alt Anamika is always punctual\n        Arvind -&gt;&gt; Anamika: Why haven't you put up your Daily Reflection?\n        Anamika -&gt;&gt; Anamika: Ulp...\n        Note right of Anamika : I have had it today..\n        Anamika -&gt;&gt; Arvind: I am sorry... \n        Arvind -&gt;&gt; Anamika: Ok write it today\n        \n        else Anamika is usually tardy\n        Arvind -&gt;&gt; Anamika: Why haven't you put up your Daily Reflection?\n        Anamika -&gt;&gt; Anamika: Ulp...\n        Anamika -&gt;&gt; Arvind: I am sorry... \n        Arvind -&gt;&gt; Anamika: This is not acceptable and will reflect in your grade\n        end\n        \n        Arvind -&gt;&gt; Komal: And you? What kept you?\n        Komal -&gt;&gt; Anamika: (Quietly) He's having a bad day, dude...\n        Anamika -&gt;&gt; Komal: (Whisper) Boomer...\n        Note over Anamika,Komal: Giggle...\n\")\n\n\n\n\n\n\nFrom here: https://cyberhelp.sesync.org/blog/visualization-with-diagrammeR.html\n\nCodegrViz(\"digraph{\n\n      graph[rankdir = LR]\n  \n      node[shape = rectangle, style = filled]\n  \n      node[fillcolor = Coral, margin = 0.2]\n      A[label = 'Figure 1: Map']\n      B[label = 'Figure 2: Metrics']\n  \n      node[fillcolor = Cyan, margin = 0.2]\n      C[label = 'Figures.Rmd']\n  \n      node[fillcolor = Violet, margin = 0.2]\n      D[label = 'Analysis_1.R']\n      E[label = 'Analysis_2.R']\n  \n      subgraph cluster_0 {\n        graph[shape = rectangle]\n        style = rounded\n        bgcolor = Gold\n    \n        label = 'Data Source 1'\n        node[shape = rectangle, fillcolor = LemonChiffon, margin = 0.25]\n        F[label = 'my_dataframe_1.csv']\n        G[label = 'my_dataframe_2.csv']\n      }\n  \n      subgraph cluster_1 {\n         graph[shape = rectangle]\n         style = rounded\n         bgcolor = Gold\n    \n         label = 'Data Source 2'\n         node[shape = rectangle, fillcolor = LemonChiffon, margin = 0.25]\n         H[label = 'my_dataframe_3.csv']\n         I[label = 'my_dataframe_4.csv']\n      }\n  \n      edge[color = black, arrowhead = vee, arrowsize = 1.25]\n      C -&gt; {A B}\n      D -&gt; C\n      E -&gt; C\n      F -&gt; D\n      G -&gt; D\n      H -&gt; E\n      I -&gt; E\n      \n      }\")\n\n\n\n\n\n\nCodemermaid(\"\n        graph BT\n        A((Salinity))\n        A--&gt;B(Barnacles)\n        B-.-&gt;|-0.10|B1{Mussels}\n        A-- 0.30 --&gt;B1\n\n        C[Air Temp]\n        C--&gt;B\n        C-.-&gt;E(Macroalgae)\n        E--&gt;B1\n        C== 0.89 ==&gt;B1\n\n        style A fill:#FFF, stroke:#333, stroke-width:4px\n        style B fill:#9AA, stroke:#9AA, stroke-width:2px\n        style B1 fill:#879, stroke:#333, stroke-width:1px\n        style C fill:#ADF, stroke:#333, stroke-width:2px\n        style E fill:#9C2, stroke:#9C2, stroke-width:2px\n\n        \")\n\n\n\n\n\n\nCodeDiagrammeR(\"\nsequenceDiagram\n  Arvind -&gt;&gt;ticket seller: ask ticket\n  ticket seller-&gt;&gt;database: seats\n  alt tickets available\n    database-&gt;&gt;ticket seller: ok\n    ticket seller-&gt;&gt;customer: confirm\n    Arvind -&gt;&gt;ticket seller: ok\n    ticket seller-&gt;&gt;database: book a seat\n    ticket seller-&gt;&gt;printer: print ticket\n  else sold out\n    database-&gt;&gt;ticket seller: none left\n    ticket seller-&gt;&gt;customer: sorry\n  end\n\")\n\n\n\n\n\n\nCodeDiagrammeR(\n\"graph TB;\nA(Rounded)--&gt;B[Squared];\nB--&gt;C{A Decision};\nC--&gt;D[Square One];\nC--&gt;E[Square Two];\n\n%% Now styling these blocks\nstyle A fill:#E5E25F;  \nstyle B fill:#87AB51; \nstyle C fill:#3C8937;\nstyle D fill:#23772C;  \nstyle E fill:#B6E6E6;\n\"\n)\n\n\n\n\n\n\nCode  grViz(\"\ndigraph boxes_and_circles {\n\n  # a 'graph' statement\n  graph [overlap = true, fontsize = 10,forcelabels = true]\n\n  # several 'node' statements\n  node [shape = box,fontname = Helvetica, color = red, style = filled]\n  A[label = 'This is \\\\n an internal \\\\n label', xlabel = 'This is \\\\nan external \\\\nlabel']; B; C; D; E; F\n\n  node [shape = circle, fixedsize = true, color = palegreen, width = 0.9] // sets as circles\n  1; 2; 3; 4; 5; 6; 7; 8\n\n  # several 'edge' statements\n  A-&gt;{1,2,3,4} B-&gt;2 B-&gt;3 B-&gt;4 C-&gt;A\n  1-&gt;D E-&gt;A 2-&gt;4 1-&gt;5 1-&gt;F\n  E-&gt;6 4-&gt;6 5-&gt;7 6-&gt;7 3-&gt;8 3-&gt;1\n}\n\")"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#sequence-diagram-2",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#sequence-diagram-2",
    "title": "The Grammar of Diagrams",
    "section": "Sequence Diagram-2",
    "text": "Sequence Diagram-2"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#sequence-diagram-3",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#sequence-diagram-3",
    "title": "The Grammar of Diagrams",
    "section": "Sequence Diagram 3",
    "text": "Sequence Diagram 3"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#mindmap",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#mindmap",
    "title": "The Grammar of Diagrams",
    "section": "Mindmap",
    "text": "Mindmap"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#gantt-chart",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#gantt-chart",
    "title": "The Grammar of Diagrams",
    "section": "Gantt Chart",
    "text": "Gantt Chart"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#flow-chart",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#flow-chart",
    "title": "The Grammar of Diagrams",
    "section": "Flow chart",
    "text": "Flow chart"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#some-definitions-on-the-grammar-of-shapes-in-nomnoml",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#some-definitions-on-the-grammar-of-shapes-in-nomnoml",
    "title": "The Grammar of Diagrams",
    "section": "Some definitions on the “grammar of shapes” in nomnoml\n",
    "text": "Some definitions on the “grammar of shapes” in nomnoml\n\n\nAssociation Types: Connectors between blocks( i.e. Classifiers)\nClassifier Types: Kinds of blocks.\nDirective Types: Directives change the nature of the diagram rendered, by affective parameters like colour, direction and margins. ( Ha! VC people!!)\n\nCSS colours https://www.w3schools.com/cssref/css_colors.asp Only these colours are permitted, so use either the names or these specific colour hash codes. Any general hash code will not render.\n\nCode//association-1\n[a] - [b] \n\n//association-2\n[b] -&gt; [c] \n\n//association_3\n[c] &lt;-&gt; [a]\n\n//dependency-1\n[a] &lt;--&gt;[d]\n\n//dependency-2\n#.ell: visual=ellipse fill=#fbfb09 bold\n#.arvind: visual=rhomb fill=#ff2234 bold\n[&lt;ell&gt;e]--&gt;[a]\n//generalization-1\n[c]-:&gt;[&lt;arvind&gt;k]\n\n//implementation --:&gt;\n[k]--:&gt;[d]\n\n\n\n\n\n\nCode//composition +-\n[a]+-[b]\n//composition +-&gt;\n[b]-+[c]\n//aggregation o-\n[c]o-&gt;[d]\n//aggregation o-&gt;\n[d]o-&gt;[a]\n//note --\n[d]--[everything happens;here]\n//hidden -/-\n[d]-/-[f]\n////////////////////////\n//weightless edge _&gt;\n//[k]_&gt;[d] //not working\n//weightless dashed__\n//[d]__[j] //not working\n\n\n\n\n\nClassifier Types\nThese are different kinds of blocks.\n\nCode[class]-&gt;[&lt;abstract&gt; abstract]\n[&lt;abstract&gt; abstract]-:&gt;[&lt;instance&gt; instance]\n[&lt;instance&gt; instance]-:&gt;[&lt;note&gt; note]\n[&lt;note&gt; note]--&gt;[&lt;reference&gt; reference]\n\n\n\n\n\n\nCode[&lt;package&gt; package|components]--&gt;[&lt;frame&gt; frame|]\n[&lt;database&gt; database]--&gt;[&lt;start&gt; start]\n[&lt;end&gt; end]-o&gt;[&lt;state&gt; state]\n\n\n\n\n\n\nCode[&lt;choice&gt; choice]---&gt;[&lt;sync&gt; sync]\n[&lt;input&gt; input]-&gt;[&lt;sender&gt; sender]\n[&lt;receiver&gt; receiver]o-[&lt;transceiver&gt; transceiver]\n\n\n\n\n\n\nCode#direction:down\n#background:lightgrey\n#fill: fuchsia; green; purple\n#fillArrows: false\n#font: Courier\n[class]-&gt;[&lt;abstract&gt; abstract]\n[&lt;abstract&gt; abstract]-:&gt;[&lt;instance&gt; instance]\n[&lt;instance&gt; instance]-:&gt;[&lt;note&gt; note]\n[&lt;note&gt; note]--&gt;[&lt;reference&gt; reference]\n\n\n\n\n\n\nCode#font: CenturySchoolbook\n#fill: lightyellow\n#stroke: green\n\n[&lt;actor&gt; actor]---[&lt;usecase&gt; usecase]\n[&lt;usecase&gt; usecase]&lt;--&gt;[&lt;label&gt; label]\n[&lt;usecase&gt; usecase]-/-[&lt;hidden&gt; hidden]\n\n\n\n\n\n\nCode[&lt;table&gt; table| a | 5 || b | 7]\n\n\n\n\n\n\n\nCode[&lt;table&gt; table| c | 9 ]"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#directives",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#directives",
    "title": "The Grammar of Diagrams",
    "section": "Directives",
    "text": "Directives\nDirectives change the nature of the diagram rendered, by affective parameters like colour, direction and margins."
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#custom-classifier-styles",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#custom-classifier-styles",
    "title": "The Grammar of Diagrams",
    "section": "Custom classifier styles",
    "text": "Custom classifier styles\nA directive that starts with “.” define a classifier’s style. The style is written as a space separated list of modifiers and key/value pairs.\n\nCode#.box: fill=#8f8 dashed\n#.blob: visual=ellipse title=bold\n#.arvind: visual=rhomb title=bold dashed fill=CornFlowerBlue\n[&lt;box&gt; GreenBox]\n[&lt;blob&gt; Blobby]\n[&lt;arvind&gt; Someone]"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#nomnoml-keyvalue-pairs",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#nomnoml-keyvalue-pairs",
    "title": "The Grammar of Diagrams",
    "section": "\nnomnoml Key/value pairs",
    "text": "nomnoml Key/value pairs\n\nfill=(any css color)\nstroke=(any css color)\nalign=center align=left\ndirection=right direction=down\nvisual=actor\nvisual=class\nvisual=database\nvisual=ellipse\nvisual=end\nvisual=frame\nvisual=hidden\nvisual=input\nvisual=none\nvisual=note\nvisual=package\nvisual=receiver\nvisual=rhomb\nvisual=roundrect\nvisual=sender\nvisual=start\nvisual=sync\nvisual=table\nvisual=transceiver"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#text-modifiers",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#text-modifiers",
    "title": "The Grammar of Diagrams",
    "section": "Text modifiers",
    "text": "Text modifiers\nbold center italic left underline\n\nCode# .box: fill=#8f8 dashed\n# .blob: visual=rhomb title=bold fill=#8f8 dashed\n\n[A]-[B]\n[B]--[&lt;usecase&gt;C]\n[C]-[&lt;box&gt; D]\n[B]--[&lt;blob&gt; Jabba;TheHut]\n\n\n\n\n\n\nCode[a] -&gt;[b]\n[b] -:&gt; [c]\n[c]o-&gt;[d]\n[d]-/-[e]\n\n\n\n\n\n\nCode#fill: lightgreen; lightblue; lightyellow; grey; white\n\n[&lt;table&gt; table | c | 9 ]\n\n[R | [&lt;table&gt; Packages |\n         Base R |\n         [ &lt;table&gt; tidyverse| ggplot | tidyr | readr |\n             [&lt;table&gt; dplyr|\n                 magrittr | Others]]]]\n\n\n\n\n\n\n\nCode#fill: lightgreen; lightblue; lightyellow; pink; white\n\n[RStudio | [R | [&lt;table&gt; Packages |\n                   Base R | [ tidyverse |\n                               ggplot | tidyr | readr |\n                               [dplyr]--[magrittr]\n                               [dplyr]--[Others]\n                             | tibble\n                             ]\n                 | lubridate | DiagrammeR | Lattice]]]\n\n\n\n\n\n\n\nCode[Linux]+-[Ubuntu]\n[Linux]+-[Mint]\n[Ubuntu]--[Mint]\n[Linux]+-[Rosa Linux]\n[Linux]+-[Mx Linux]\n[Debian]-+[Linux]\n\n\n[Fedora]-+[Linux]\n[Puppy Linux]-+[Linux]\n[Personal Pups]-+[Puppy Linux]"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/120-time/index.html#introduction",
    "href": "content/courses/R4Artists/Modules/120-time/index.html#introduction",
    "title": "Lab-12: Time is a Him!!",
    "section": "Introduction",
    "text": "Introduction\nTime Series data are important in data visualization where events have a temporal dimension, such as with finance, transportation, music, telecommunications for example.\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(ggformula)\n##########################################\n# Install core TimeSeries Packages\n# library(ctv)\n# ctv::install.views(\"TimeSeries\", coreOnly = TRUE)\n# To update core TimeSeries packages\n# ctv::update.views(\"TimeSeries\")\n# Time Series Core Packages\n##########################################\nlibrary(tsibble)\nlibrary(feasts) # Feature Extraction and Statistics for Time Series\nlibrary(fable) # Forecasting Models for Tidy Time Series\nlibrary(tseries) # Time Series Analysis and Computational Finance\nlibrary(forecast)\nlibrary(zoo)\n##########################################\nlibrary(tsibbledata) # Time Series Demo Datasets"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/120-time/index.html#creating-time-series",
    "href": "content/courses/R4Artists/Modules/120-time/index.html#creating-time-series",
    "title": "Lab-12: Time is a Him!!",
    "section": "Creating time series",
    "text": "Creating time series\nIn this first example, we will use simple ts data, and then do another with a tibble dataset, and then a third example with an tsibble formatted dataset.\n\nts format data\nThere are a few datasets in base R that are in ts format already.\n\nAirPassengers\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1949 112 118 132 129 121 135 148 148 136 119 104 118\n1950 115 126 141 135 125 149 170 170 158 133 114 140\n1951 145 150 178 163 172 178 199 199 184 162 146 166\n1952 171 180 193 181 183 218 230 242 209 191 172 194\n1953 196 196 236 235 229 243 264 272 237 211 180 201\n1954 204 188 235 227 234 264 302 293 259 229 203 229\n1955 242 233 267 269 270 315 364 347 312 274 237 278\n1956 284 277 317 313 318 374 413 405 355 306 271 306\n1957 315 301 356 348 355 422 465 467 404 347 305 336\n1958 340 318 362 348 363 435 491 505 404 359 310 337\n1959 360 342 406 396 420 472 548 559 463 407 362 405\n1960 417 391 419 461 472 535 622 606 508 461 390 432\n\nstr(AirPassengers)\n\n Time-Series [1:144] from 1949 to 1961: 112 118 132 129 121 135 148 148 136 119 ...\n\n\nThis can be easily plotted using base R:\n\nplot(AirPassengers)\n\n\n\n\nLet us take data that is “time oriented” but not in ts format, and convert it to ts: the syntax of ts() is:\nSyntax: objectName &lt;- ts(data, start, end, frequency) where  - data represents the data vector - start represents the first observation in time series\n- end represents the last observation in time series\n- frequency represents number of observations per unit time. For example, frequency=1 for monthly data.\nWe will pick simple numerical vector data ( i.e. not a timeseries ) ChickWeight:\n\nChickWeight %&gt;% head()\n\n\n\n  \n\n\nChickWeight_ts &lt;- ts(ChickWeight$weight, frequency = 2)\nplot(ChickWeight_ts)\n\n\n\n\n\n\n\n\n\n\nThe ts format\n\n\n\nThe ts format is not recommended for new analysis since it does not permit inclusion of multiple time series in one dataset, nor other categorical variables for grouping etc.\n\n\n\ntibble format data\nSome “time-oriented” datasets are available in tibble form. Let us try to plot one, the walmart_sales_weekly dataset from the timetk package:\n\ndata(walmart_sales_weekly, package = \"timetk\")\nwalmart_sales_weekly\n\n\n\n  \n\n\n\nThis dataset is a tibble with a Date column. The Dept column is clearly a categorical column that allows us to distinguish separate time series, i.e. one for each value of Dept. We will convert that to a factor( it is an double precision number ) and then plot the data using this column on the Date on the \\(x\\)-axis:\n\nwalmart_sales_weekly %&gt;% \n  \n  # convert Dept number to a **categorical factor**\n  mutate(Dept = forcats::as_factor(Dept)) %&gt;% \n  \n  gf_point(Weekly_Sales ~ Date, \n           group = ~ Dept, \n           colour = ~ Dept, data = .) %&gt;% \n  gf_line() %&gt;% \n  gf_theme(theme_minimal())\n\n\n\n\nFor more analysis and forecasting etc., it is useful to convert this tibble into a tsibble:\n\nwalmart_tsibble &lt;- as_tsibble(walmart_sales_weekly,\n                         index = Date,\n                         key = c(id, Dept))\nwalmart_tsibble\n\n\n\n  \n\n\n\nThe 7D states the data is weekly. There is a Date column and all the other numerical variables are time-varying quantities. The categorical variables such as id, and Dept allow us to identify separate time series in the data, and these have 7 combinations hence are 7 time series in this data, as indicated.\n\n\nLet us plot Weekly_Sales, colouring the time series by Dept:\n\nwalmart_tsibble %&gt;% \n  gf_line(Weekly_Sales ~ Date, \n          colour = ~ as_factor(Dept), data = .) %&gt;% \n  gf_point() %&gt;% \n  labs(title = \"Weekly Sales by Dept at Walmart\")\n\n[[1]]\n\n\n\n$title\n[1] \"Weekly Sales by Dept at Walmart\"\n\nattr(,\"class\")\n[1] \"labels\"\n\n\n\n\nFigure 1: Walmart Time Series\n\n\n\n\nWe can also do a quick autoplot that seems to offer less control and is also not interactive.\n\nwalmart_tsibble %&gt;% \n  dplyr::group_by(Dept) %&gt;% \n  autoplot(Weekly_Sales)\n\n\n\n\n\n\ntsibble format data\nIn the packages tsibbledata and fpp3 we have a good choice of tsibble format data. Let us pick one:\n\nhh_budget\n\n\n\n  \n\n\n\nThere are 4 keys ( id variables ) here, one for each country. Six other quantitative columns are the individual series. Let us plot the timeseries:"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/120-time/index.html#one-more-example",
    "href": "content/courses/R4Artists/Modules/120-time/index.html#one-more-example",
    "title": "Lab-12: Time is a Him!!",
    "section": "One more example",
    "text": "One more example\nOften we have data in table form, that is time-oriented, with a date like column, and we need to convert it into a tsibble for analysis:\n\nprison &lt;- readr::read_csv(\"https://OTexts.com/fpp3/extrafiles/prison_population.csv\")\nglimpse(prison)\n\nRows: 3,072\nColumns: 6\n$ Date       &lt;date&gt; 2005-03-01, 2005-03-01, 2005-03-01, 2005-03-01, 2005-03-01…\n$ State      &lt;chr&gt; \"ACT\", \"ACT\", \"ACT\", \"ACT\", \"ACT\", \"ACT\", \"ACT\", \"ACT\", \"NS…\n$ Gender     &lt;chr&gt; \"Female\", \"Female\", \"Female\", \"Female\", \"Male\", \"Male\", \"Ma…\n$ Legal      &lt;chr&gt; \"Remanded\", \"Remanded\", \"Sentenced\", \"Sentenced\", \"Remanded…\n$ Indigenous &lt;chr&gt; \"ATSI\", \"Non-ATSI\", \"ATSI\", \"Non-ATSI\", \"ATSI\", \"Non-ATSI\",…\n$ Count      &lt;dbl&gt; 0, 2, 0, 5, 7, 58, 5, 101, 51, 131, 145, 323, 355, 1617, 12…\n\n\nWe have a Date column for the time index, and we have unique key variables like State, Gender, Legal and Indigenous. Count is the value that is variable over time. It also appears that the data is quarterly, since mosaic::inspect reports the max_diff in the Date column as \\(92\\). .Run mosaic::inspect(prison) in your Console\n\nprison_tsibble &lt;- prison %&gt;% \n  mutate(quarter = yearquarter(Date)) %&gt;% \n  select(-Date) %&gt;% # Remove the Date column now that we have quarters\n  as_tsibble(index = quarter, key = c(State, Gender, Legal, Indigenous))\n\nprison_tsibble\n\n\n\n  \n\n\n\n(Here, ATSI stands for Aboriginal or Torres Strait Islander.). We have \\(64\\) time series here, organized quarterly.\nLet us examine the key variables:\n\nprison_tsibble %&gt;% distinct(Indigenous)\n\n\n\n  \n\n\nprison_tsibble %&gt;% distinct(State)\n\n\n\n  \n\n\n\nSo we can plot the time series, faceted / coloured by State:\n\nprison_tsibble %&gt;% \n  tsibble::index_by() %&gt;% \n  group_by(Indigenous, State) %&gt;% \n  #filter(State == \"NSW\") %&gt;% \n  summarise(Total = sum(Count))  %&gt;%\n  ggplot(aes(x = quarter, y = Total, colour = Indigenous, \n             shape = Indigenous)) + \n  geom_point() +\n  geom_line()  + \n  facet_wrap(~ State)\n\n\n\n\nHmm…looks like New South Wales(NSW) as something different going on compared to the rest of the states in Aus."
  },
  {
    "objectID": "content/courses/R4Artists/Modules/120-time/index.html#decomposing-time-series",
    "href": "content/courses/R4Artists/Modules/120-time/index.html#decomposing-time-series",
    "title": "Lab-12: Time is a Him!!",
    "section": "Decomposing Time Series",
    "text": "Decomposing Time Series\nWe can decompose the Weekly_Sales into components representing trends, seasonal events that repeat, and irregular noise. Since each Dept could have a different set of trends, we will do this first for one Dept, say Dept #95:\n\nwalmart_decomposed_season &lt;- walmart_tsibble %&gt;% \n  dplyr::filter(Dept == \"95\") %&gt;% # filter for Dept 95\n  #\n  # feasts depends upon fabletools.\n  # \n  fabletools::model(\n    season = STL(Weekly_Sales ~ season(window = \"periodic\"))) \n\nwalmart_decomposed_ets &lt;- walmart_tsibble %&gt;% \n  dplyr::filter(Dept == \"95\") %&gt;% # filter for Dept 95\n  #\n  # feasts depends upon fabletools.\n  # \n  fabletools::model(\n    ets = ETS(box_cox(Weekly_Sales, 0.3)))\n\nwalmart_decomposed_season %&gt;% fabletools::components()\n\n\n\n  \n\n\nwalmart_decomposed_ets %&gt;% fabletools::components()\n\n\n\n  \n\n\n# walmart_decomposed_arima &lt;- walmart_tsibble %&gt;% \n#   dplyr::filter(Dept == \"95\") %&gt;% # filter for Dept 95\n#     arima = ARIMA(log(Weekly_Sales))\n\n\nwalmart_decomposed_season %&gt;% \n  components() %&gt;% \n  autoplot() + \n  labs( title = \"Seasonal Variations in Weekly Sales, Dept #95\")\n\n\n\nwalmart_decomposed_ets %&gt;% \n  components() %&gt;% \n  autoplot() + \n  labs( title = \"ETS Variations in Weekly Sales, Dept #95\")"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/120-time/index.html#conclusion",
    "href": "content/courses/R4Artists/Modules/120-time/index.html#conclusion",
    "title": "Lab-12: Time is a Him!!",
    "section": "Conclusion",
    "text": "Conclusion\nTBW"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/120-time/index.html#references",
    "href": "content/courses/R4Artists/Modules/120-time/index.html#references",
    "title": "Lab-12: Time is a Him!!",
    "section": "References",
    "text": "References\n\nForecasting: Principles and Practice (3rd ed) Rob J Hyndman and George Athanasopoulos"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/20-intro/index.html#introduction",
    "href": "content/courses/R4Artists/Modules/20-intro/index.html#introduction",
    "title": "Lab-2: Down the R-abbit Hole…",
    "section": "Introduction",
    "text": "Introduction\nWelcome!\nLet’s start our journey to the Garden of Data Visualization, with this terrific presentation by the great ( and sadly late..) Hans Rosling.\nThe best stats you’ve ever seen by Hans Rosling:\n\nWe will run some boiler-plate R code today! Nothing ( almost! ) to code! We will get used to the tools and words of the trade: R, RStudio, installation, packages, libraries…."
  },
  {
    "objectID": "content/courses/R4Artists/Modules/30-quarto/index.html",
    "href": "content/courses/R4Artists/Modules/30-quarto/index.html",
    "title": "Lab-3: Drink Me!",
    "section": "",
    "text": "We will get acquainted with the RMarkdown Quarto Document format, which allows us to mix text narrative, code, code-developed figures and items from the web in a seamless document.Then we can try to create something in the lines of what Hans Rosling did.\nSomething that can:\n\nprovide a visualization\nprovide insight\ntell a story\nis reproducible"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/30-quarto/index.html#introduction",
    "href": "content/courses/R4Artists/Modules/30-quarto/index.html#introduction",
    "title": "Lab-3: Drink Me!",
    "section": "",
    "text": "We will get acquainted with the RMarkdown Quarto Document format, which allows us to mix text narrative, code, code-developed figures and items from the web in a seamless document.Then we can try to create something in the lines of what Hans Rosling did.\nSomething that can:\n\nprovide a visualization\nprovide insight\ntell a story\nis reproducible"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/30-quarto/index.html#setting-up-quarto",
    "href": "content/courses/R4Artists/Modules/30-quarto/index.html#setting-up-quarto",
    "title": "Lab-3: Drink Me!",
    "section": "Setting up Quarto",
    "text": "Setting up Quarto\nQuarto is installed along with RStudio. We can check if all is in order by running a check in the Terminal in RStudio. \nThe commands are:\n\nquarto check install\nquarto check knitr\n\nIf these come out with no errors then we are ready to fire up our first Quarto document."
  },
  {
    "objectID": "content/courses/R4Artists/Modules/30-quarto/index.html#practice",
    "href": "content/courses/R4Artists/Modules/30-quarto/index.html#practice",
    "title": "Lab-3: Drink Me!",
    "section": "Practice",
    "text": "Practice\nFire up a new Quarto document by going to: File -&gt; New File -&gt; Quarto Document.Switch to Visual mode, if it is not already there.\nUse the visual mode tool bar.\nTry to create Sections, code chunks, embedding images and tables.\nHit the Render button to see how the documents is converted into an html document."
  },
  {
    "objectID": "content/courses/R4Artists/Modules/30-quarto/index.html#references",
    "href": "content/courses/R4Artists/Modules/30-quarto/index.html#references",
    "title": "Lab-3: Drink Me!",
    "section": "References:",
    "text": "References:\n\nhttps://rmarkdown.rstudio.com/index.html\nhttps://ysc-rmarkdown.netlify.app/slides/01-basics.html Nice RMarkdown presentation and “code movies” !\nhttps://www.markdowntutorial.com\nhttps://quarto.org/docs/authoring/markdown-basics.html"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/30-quarto/index.html#assignments",
    "href": "content/courses/R4Artists/Modules/30-quarto/index.html#assignments",
    "title": "Lab-3: Drink Me!",
    "section": "Assignment(s)",
    "text": "Assignment(s)\n\nComplete the markdown tutorial in [reference 1]\nCreate a fresh Quarto document and use as many as possible of the RMarkdown constructs from the Cheatsheet [reference 1]"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/30-quarto/index.html#fun-stuff",
    "href": "content/courses/R4Artists/Modules/30-quarto/index.html#fun-stuff",
    "title": "Lab-3: Drink Me!",
    "section": "Fun Stuff",
    "text": "Fun Stuff\n\nhttps://rmarkdown.rstudio.com/lesson-1.html\nDesirée De Leon, Alison Hill: rstudio4edu: A Handbook for Teaching and Learning with R and RStudio, https://rstudio4edu.github.io/rstudio4edu-book/"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/40-working-in-r/index.html",
    "href": "content/courses/R4Artists/Modules/40-working-in-r/index.html",
    "title": "Lab-4: I say what I mean and I mean what I say",
    "section": "",
    "text": "R Tutorial\n Slides"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/40-working-in-r/index.html#fa-folder-open-slides-and-tutorials",
    "href": "content/courses/R4Artists/Modules/40-working-in-r/index.html#fa-folder-open-slides-and-tutorials",
    "title": "Lab-4: I say what I mean and I mean what I say",
    "section": "",
    "text": "R Tutorial\n Slides"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/40-working-in-r/index.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "href": "content/courses/R4Artists/Modules/40-working-in-r/index.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "title": "Lab-4: I say what I mean and I mean what I say",
    "section": " Introduction",
    "text": "Introduction\nWe will get acquainted with data and its representations in R! We will also form a view of how English Grammar ( pronouns, verbs, adjectives, figures of speech..) get metaphorized into the R World!!"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/40-working-in-r/index.html#readings",
    "href": "content/courses/R4Artists/Modules/40-working-in-r/index.html#readings",
    "title": "Lab-4: I say what I mean and I mean what I say",
    "section": "Readings",
    "text": "Readings\n\nR for Data Science, Workflow: Basics Chapter: http://r4ds.had.co.nz/workflow-basics.html\nModern Dive, Getting Started Chapter: http://moderndive.com/2-getting-started.html\nR & RStudio Basics: https://bookdown.org/chesterismay/rbasics/3-rstudiobasics.html\nRStudio IDE Cheatsheet: https://github.com/rstudio/cheatsheets/blob/master/rstudio-ide.pdf"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/60-GoG/index.html",
    "href": "content/courses/R4Artists/Modules/60-GoG/index.html",
    "title": "Lab-6: These Roses have been Painted !!",
    "section": "",
    "text": "R Tutorial  \n  Slides \n  Colour in R \nAdvanced Graphics"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/60-GoG/index.html#fa-folder-open-slides-and-tutorials",
    "href": "content/courses/R4Artists/Modules/60-GoG/index.html#fa-folder-open-slides-and-tutorials",
    "title": "Lab-6: These Roses have been Painted !!",
    "section": "",
    "text": "R Tutorial  \n  Slides \n  Colour in R \nAdvanced Graphics"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/60-GoG/index.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "href": "content/courses/R4Artists/Modules/60-GoG/index.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "title": "Lab-6: These Roses have been Painted !!",
    "section": " Introduction",
    "text": "Introduction\nAh…ggplot ! All those wonderful pictures and graphs, that Alice might have relished!\nMetaphors, aesthetics, geometries…and pictures !! ggplot seems to equate ravens to writing desks in its syntax…and out come graphs!!\nAnd colours: Wes Anderson! Tim Burton! The Economist… and many others!!"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/60-GoG/index.html#fa-asterisk-references",
    "href": "content/courses/R4Artists/Modules/60-GoG/index.html#fa-asterisk-references",
    "title": "Lab-6: These Roses have been Painted !!",
    "section": " References",
    "text": "References\n\nGeorge Lakoff and Mark Johnson, Metaphors We Live By, https://www.youtube.com/watch?v=lYcQcwUfo8c\nWickham and Grolemund, R for Data Science, ggplot chapter: https://r4ds.had.co.nz/data-visualisation.html\nCMDLineTips, 10 Tips to Customize Text Color, Font, Size in ggplot2 with element_text(), https://cmdlinetips.com/2021/05/tips-to-customize-text-color-font-size-in-ggplot2-with-element_text/\nCMDLineTips, How to write a simple custom ggplot theme from scratch, https://cmdlinetips.com/2022/05/how-to-write-a-simple-custom-ggplot-theme-from-scratch/\nAsha Hill @ mode.com, 12 Extensions to ggplot2 for More Powerful R Visualizations, https://mode.com/blog/r-ggplot-extension-packages/\nEmil Hvitfeldt, ggplot Trial and Error, https://www.emilhvitfeldt.com/post/2018-06-12-ggplot2-trial-and-error-us-trade-data/"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/60-GoG/index.html#fa-icons-fun-stuff",
    "href": "content/courses/R4Artists/Modules/60-GoG/index.html#fa-icons-fun-stuff",
    "title": "Lab-6: These Roses have been Painted !!",
    "section": " Fun Stuff",
    "text": "Fun Stuff\n\nYihan Wu, Mapping ggplot geoms and aesthetic parameters, ( An interactive view of which aesthetic parameters work with which ggplot geom!! ) https://www.yihanwu.ca/post/geoms-and-aesthetic-parameters/\nhttps://www.theartstory.org/artist/kandinsky-wassily/"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/index.html#introduction",
    "href": "content/courses/R4Artists/Modules/70-wizardy/index.html#introduction",
    "title": "Lab-7: The Lobster Quadrille",
    "section": "Introduction",
    "text": "Introduction\nWe will add icing and froth to our vanilla ggplots: fonts, annotations, highlights and even pictures!!"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/index.html#using-google-fonts",
    "href": "content/courses/R4Artists/Modules/70-wizardy/index.html#using-google-fonts",
    "title": "Lab-7: The Lobster Quadrille",
    "section": "Using Google Fonts",
    "text": "Using Google Fonts\nWe will want to add a few new fonts to our graphs. The best way (currently) is to use the showtext package (which we loaded above) to bring into our work fonts from Google. To view and select the fonts you might want to work with, spend some time looking over:\n\nGoogle Webfonts Helper App\nGoogle Fonts\n\n\nsysfonts::font_add_google(\"Gochi Hand\", \"gochi\")\nfont_add_google(\"Schoolbell\", \"bell\")\nfont_add_google(\"Galada\", \"galada\")\nfont_add_google(\"Schoolbell\", \"bell\")\nfont_add_google(\"Roboto\", \"roboto\")\nfont_add_google(\"Noto Sans\", \"noto\")\nfont_add_google(\"Uchen\", \"uchen\")\nfont_add_google(\"Ibarra Real Nova\", \"ibarra\")\nfont_add_google(\"Open Sans\", \"open\")\nfont_add_google(\"Anton\", \"anton\")\nfont_add_google(\"Tangerine\", \"tangerine\")\n\n # set the google fonts as default\nshowtext::showtext_auto(enable = TRUE)\n\nWe will work with a familiar dataset, so that we can concentrate on the chart aesthetics, without having to spend time getting used to the data: the penguins dataset again, from the palmerpenguins package.\n\n\n\n\n\n\nggformula and ggplot worlds\n\n\n\nIt seems we can mix `ggformula` code with `ggtext` code, using the `+` sign!! What joy !!! Need to find out if this works for other `ggplot` extensions as well !!!"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/index.html#data",
    "href": "content/courses/R4Artists/Modules/70-wizardy/index.html#data",
    "title": "Lab-7: The Lobster Quadrille",
    "section": "Data",
    "text": "Data\nAlways start your work with a table of the data:\n\npenguins &lt;- penguins %&gt;% drop_na() # remove data containing missing data\npenguins"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/index.html#basic-plot",
    "href": "content/courses/R4Artists/Modules/70-wizardy/index.html#basic-plot",
    "title": "Lab-7: The Lobster Quadrille",
    "section": "Basic Plot",
    "text": "Basic Plot\nA basic scatter plot, which we will progressively dress up.\n\nUsing ggformulaUsing ggplot\n\n\n\n## simple plot: data + mappings + geometry\n## no colour or fill yet\ngf &lt;-  gf_point(bill_depth_mm ~ bill_length_mm, \n                 data = penguins, \n                 alpha = 0.6, size = 3.5)\ngf\n\n\n\n\n\n\n\n## simple plot: data + mappings + geometry\n## no colour or fill yet\ngg &lt;- ggplot(penguins, aes(x = bill_length_mm,\n                           y = bill_depth_mm)) +\n  geom_point(alpha = .6, size = 3.5)\ngg"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/index.html#customized-plot",
    "href": "content/courses/R4Artists/Modules/70-wizardy/index.html#customized-plot",
    "title": "Lab-7: The Lobster Quadrille",
    "section": "Customized Plot",
    "text": "Customized Plot\nLet us set some ggplot theme aspects now!! Here is a handy picture showing (most of) the theme-able aspects of a ggplot plot.\n\n\n\nRosana Ferrero (@RosanaFerrero) on Twitter Sept 11, 2022\n\n\nFor more info, type ?theme in your console.\n\n## change global theme settings (for all following plots)\ntheme_set(theme_classic(base_size = 12, \n                        base_family = \"open\"\n                        ))\n\n## modify plot elements globally (for all following plots)\ntheme_update(\n  axis.ticks = element_line(color = \"grey92\"),\n  axis.ticks.length = unit(.5, \"lines\"),\n  panel.grid.minor = element_blank(),\n  legend.title = element_text(size = 12),\n  legend.text = element_text(color = \"grey30\"),\n  plot.title = element_text(size = 18, face = \"bold\"),\n  plot.subtitle = element_text(size = 12, color = \"grey30\"),\n  plot.caption = element_text(size = 9, margin = margin(t = 15))\n)\n\nSince we know what the basic plot looks like, let’s add titles, labels and colours. We will also set limits and scales.\n\nUsing ggformulaUsing ggplot\n\n\n\ngf1 &lt;- penguins %&gt;% \n  gf_point(bill_depth_mm ~ bill_length_mm, \n           \n           # colour by continuous variable\n           color =  ~ body_mass_g, \n           alpha = .6, size = 3.5) %&gt;% \n\n  \n  ## custom axes scaling\n  gf_refine(\n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)),\n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5), \n                     limits = c(12.5, 22.5)), \n  \n  ## custom colors from the scico package\n  ## using the paletteer super package\n  paletteer:: scale_color_paletteer_c(`\"scico::bamako\"`, \n                                      direction = -1),\n  \n  ## custom labels\n  labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (Pygoscelis)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) PLoS ONE',\n    x = 'Bill Length (mm)', \n    y = 'Bill Depth (mm)',\n    color = 'Body mass (g)' #&lt;1&gt;\n  ))\n\ngf1\n\n\n\n\n\nNote this neat way of naming a scale and the legend!\n\n\n\n\ngg1 &lt;- penguins %&gt;% \n  ggplot(aes(y = bill_depth_mm, x = bill_length_mm), \n         alpha = .6) +\n  geom_point(aes(colour = body_mass_g), size = 3.5) + \n\n  \n  ## custom axes scaling\n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) + \n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5), \n                     limits = c(12.5, 22.5)) + \n  \n  ## custom colors from the scico package\n  paletteer:: scale_color_paletteer_c(`\"scico::bamako\"`, \n                                      direction = -1) + \n  \n  ## custom labels\n  labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (Pygoscelis)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) PLoS ONE',\n    x = 'Bill Length (mm)', \n    y = 'Bill Depth (mm)',\n    color = 'Body mass (g)' \n  )\ngg1"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/index.html#using-ggtext",
    "href": "content/courses/R4Artists/Modules/70-wizardy/index.html#using-ggtext",
    "title": "Lab-7: The Lobster Quadrille",
    "section": "Using {ggtext}",
    "text": "Using {ggtext}\nFrom Claus Wilke’s website → www.wilkelab.org/ggtext\n\nThe ggtext package provides simple Markdown and HTML rendering for ggplot2. Under the hood, the package uses the gridtext package for the actual rendering, and consequently it is limited to the feature set provided by gridtext.\nSupport is provided for Markdown both in theme elements (plot titles, subtitles, captions, axis labels, legends, etc.) and in geoms (similar to geom_text()). In both cases, there are two alternatives, one for creating simple text labels and one for creating text boxes with word wrapping.\n\n\n\n\n\n\n\nWorking with ggtext\n\n\n\nNOTE: on some machines, the ggtext package may not work as expected. In this case, please do as follows, using your Console:\n\nremove gridtext: remove.packages(gridtext).\nInstall development version of gridtext: install.packages(remotes) remotes::install_github(\"wilkelab/gridtext\")\n\n\n\n\nUsing element_markdown()\nWe can use our familiar markdown syntax right inside the titles and captions of the plot. element_markdown() is a theme-ing command made available by the ggtext package.\nelement_markdown() → formatted text elements, e.g. titles, caption, axis text, striptext.\n\nUsing ggformulaUsing ggplot\n\n\n\ngf2 &lt;- penguins %&gt;% gf_point(bill_depth_mm ~ bill_length_mm, \n                            color = ~ body_mass_g, \n                            alpha = 0.6, size = 3.5) %&gt;% \n gf_refine(\n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)),\n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5), \n                     limits = c(12.5, 22.5)), \n  \n  ## custom colors from the scico package\n  paletteer:: scale_color_paletteer_c(\"scico::bamako\", \n                                      direction = -1),\n  \n  ## custom labels using element_markdown()\n   labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) *PLoS ONE*',\n    x = '**Bill Length** (mm)', \n    y = '**Bill Depth** (mm)',\n    color = 'Body mass (g)')\n  ) %&gt;% \n  \n  # New code from here\n  # Enables markdown titles, captions and labels\n  gf_theme(theme(\n    plot.title = ggtext::element_markdown(),\n    plot.caption = ggtext::element_markdown(),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown()\n  ))\n\n gf2\n\n\n\n\n\n\n\ngg2 &lt;- ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5) + \n  \n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5), \n                     limits = c(12.5, 22.5)) +\n   paletteer:: scale_color_paletteer_c(`\"scico::bamako\"`, direction = -1) +\n   \n  ## New code starts here: Two Step Procedure with ggtext\n  ## 1. Markdown formatting of labels and title, using asterisks\n  labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) *PLoS ONE*',\n    x = '**Bill Length** (mm)', \n    y = '**Bill Depth** (mm)',\n    color = 'Body mass (g)'\n  ) +\n   \n  ## 2. Add theme related commands from ggtext\n  ## render respective text elements\n  theme(\n    plot.title = ggtext::element_markdown(),\n    plot.caption = ggtext::element_markdown(),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown()\n  )\ngg2\n\n\n\n\n\n\n\n\n\nelement_markdown() in combination with HTML\nThis allows us to change fonts in titles, labels, and captions.\n\nUsing ggformulaUsing ggplot\n\n\n\n## use HTML syntax to change text color\n## \ngf2 %&gt;% \n  \n  # html in labels\n  gf_labs(title = 'Bill Dimensions of Brush-Tailed Penguins\n          &lt;i style = \"color:#28A87D;\"&gt;Pygoscelis &lt;/i&gt;'\n            ) \n\n\n\n## use HTML syntax to change font and text size\ngf2 %&gt;% \n  gf_labs(title = 'Bill Dimensions of Brush-Tailed Penguins &lt;b style=\"font-size:32pt;font-family:tangerine;\"&gt;Pygoscelis&lt;/b&gt;') \n\n\n\n\n\n\n\n## use HTML syntax to change text color\ngg2 +\n  labs(title = 'Bill Dimensions of Brush-Tailed Penguins &lt;i style=\"color:#28A87D;\"&gt;Pygoscelis&lt;/i&gt;') +\n  theme(plot.margin = margin(t = 15))\n\n\n\n## use HTML syntax to change font and text size\ngg2 +\n  labs(title = 'Bill Dimensions of Brush-Tailed Penguins &lt;b style=\"font-size:32pt;font-family:tangerine;\"&gt;Pygoscelis&lt;/b&gt;')\n\n\n\n\n\n\n\n\n\nAdding images to ggplot\nSave an image from the web in the same folder as your RMarkdown. Use html tags to include it, say as part of your plot title, as shown below.\n\nUsing ggformulaUsing ggplot\n\n\n\n## use HTML syntax to add images to text elements\ngf2 %&gt;% \n  gf_labs(title = 'Bill Dimensions of Brush-Tailed Penguins &nbsp;&nbsp;&nbsp; &lt;img src=\"images/culmen_depth.png\"‚ width=\"480\"/&gt;') \n\n\n\n\n\n\n\n## use HTML syntax to add images to text elements\ngg2 + \n  labs(title = 'Bill Dimensions of Brush-Tailed Penguins &nbsp;&nbsp;&nbsp; &lt;img src=\"images/culmen_depth.png\"‚ width=\"480\"/&gt;')\n\n\n\n\n\n\n\n\n\nAnnotations with geom_richtext() and geom_textbox()\nFurther ggplot annotations can be achieved using geom_richtext() and geom_textbox(). geom_richtext() also allows formatted text labels with 360° rotation. One needs to pass a dataframe to geom_richtext() giving the location, colour, rotation etc of the label annotation.\n\nDesign the labelsUsing ggformulaUsing ggplot\n\n\n\n# Create a label tibble\n# Three rich text labels, \n# so three sets of locations x and y, and angle of rotation\nlabels &lt;- tibble(\n      x = c(34, 56, 54), \n      y = c(20, 18.5, 14.5),\n      angle = c(12, 20, 335),\n      species = c(\"Adelie\", \"Chinstrap\", \"Gentoo\"),\n      \n      lab = c(\"&lt;b style='font-family:anton;font-size:24pt;'&gt;Adélie&lt;/b&gt;&lt;br&gt;&lt;i style='color:darkgrey;'&gt;P. adéliae&lt;/i&gt;\", \n              \"&lt;b style='font-family:anton;font-size:24pt;'&gt;Chinstrap&lt;/b&gt;&lt;br&gt;&lt;i style='color:darkgrey;'&gt;P. antarctica&lt;/i&gt;\", \n              \"&lt;b style='font-family:anton;font-size:24pt;'&gt;Gentoo&lt;/b&gt;&lt;br&gt;&lt;i style='color:darkgrey;'&gt;P. papua&lt;/i&gt;\"))\nlabels\n\n\n\n  \n\n\n\n\n\n\n# Create a label tibble\n# Three rich text labels, \n# so three sets of locations x and y, and angle of rotation\n# labels &lt;- tibble(\n#       x = c(34, 56, 54), \n#       y = c(20, 18.5, 14.5),\n#       angle = c(12, 20, 335),\n#       species = c(\"Adelie\", \"Chinstrap\", \"Gentoo\"),\n#       \n#       lab = c(\"&lt;b style='font-family:anton;font-size:24pt;'&gt;Adélie&lt;/b&gt;&lt;br&gt;&lt;i style='color:darkgrey;'&gt;P. adéliae&lt;/i&gt;\", \n#               \"&lt;b style='font-family:anton;font-size:24pt;'&gt;Chinstrap&lt;/b&gt;&lt;br&gt;&lt;i style='color:darkgrey;'&gt;P. antarctica&lt;/i&gt;\", \n#               \"&lt;b style='font-family:anton;font-size:24pt;'&gt;Gentoo&lt;/b&gt;&lt;br&gt;&lt;i style='color:darkgrey;'&gt;P. papua&lt;/i&gt;\"))\n# labels\n\ngf_rich &lt;- penguins %&gt;% \n  gf_point(bill_depth_mm ~ bill_length_mm, \n                      color = ~ species, \n                      alpha = 0.6, size = 3.5, data = penguins) + # &lt;1&gt;\n  \n  \n    ## add text annotations for each species\n    ggtext::geom_richtext(\n    data = labels,\n    # Now pass the data variables as aesthetics\n    aes(x, y, label = lab, color = species, angle = angle), \n    size = 4, fill = NA, label.color = NA,\n    lineheight = .3) +\n    # show.legend = FALSE else we get some unusual legends!\n    # fill = NA makes the labels' fill transparent\n  \n  \n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5), \n                     limits = c(12.5, 22.5)) +\n  scale_colour_paletteer_d(palette = `\"rcartocolor::Bold\"`, \n                           guide = \"none\") +\n  \n  labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) *PLoS ONE*',\n    x = '**Bill Length** (mm)', \n    y = '**Bill Depth** (mm)',\n    color = 'Body mass (g)'\n  ) +\n  \n  # Use theme and element_markdown() to format axes and titles as usual\n  theme(\n    plot.title = ggtext::element_markdown(),\n    plot.caption = ggtext::element_markdown(),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown(),\n    plot.margin = margin(25, 6, 15, 6)\n  )\n\n\ngf_rich\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote the plus sign usage here!!We are combining the ggformula and ggplot syntax, and it works!\n\n\n\n\n\ngg_rich &lt;- ggplot(penguins, aes(x = bill_length_mm, \n                                y = bill_depth_mm)) +\n  geom_point(aes(color = species), alpha = .6, size = 3.5) + \n  \n  ## add text annotations for each species\n  ggtext::geom_richtext(\n    data = labels,\n    # Now pass the data variables as aesthetics\n    aes(x, y, label = lab, color = species, angle = angle), \n    size = 4, fill = NA, label.color = NA,\n    lineheight = .3) +\n  \n  \n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5), \n                     limits = c(12.5, 22.5)) +\n  scale_colour_paletteer_d(`\"rcartocolor::Bold\"`, guide = \"none\")+\n  \n  labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) *PLoS ONE*',\n    x = '**Bill Length** (mm)', \n    y = '**Bill Depth** (mm)',\n    color = 'Body mass (g)'\n  ) +\n  \n  # Use theme and element_markdown() to format axes and titles as usual\n  theme(\n    plot.title = ggtext::element_markdown(),\n    plot.caption = ggtext::element_markdown(),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown(),\n    plot.margin = margin(25, 6, 15, 6)\n  )\n  \ngg_rich\n\n\n\n\n\n\n\n\n\nFormatted Text boxes on plots\nelement_textbox() and element_textbox_simple() → formatted text boxes with word wrapping.\n\nUsing ggformulaUsing ggplot\n\n\n\ngf_box &lt;- penguins %&gt;% \n  gf_point(bill_depth_mm ~ bill_length_mm, \n                      color = ~ species, \n                      alpha = 0.6, size = 3.5, data = penguins) + # &lt;1&gt;\n  \n  \n    ## add text annotations for each species\n    ggtext::geom_richtext(\n    data = labels,\n    # Now pass the data variables as aesthetics\n    aes(x, y, label = lab, color = species, angle = angle), \n    size = 4, fill = NA, label.color = NA,\n    lineheight = .3) +\n    # show.legend = FALSE else we get some unusual legends!\n    # fill = NA makes the labels' fill transparent\n  \n  \n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5), \n                     limits = c(12.5, 22.5)) +\n  scale_colour_paletteer_d(palette = `\"rcartocolor::Bold\"`, \n                           guide = \"none\") +\n  \n  \n# Now for the Plot Titles and Labels, as before  \n  labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) *PLoS ONE*',\n    x = '**Bill Length** (mm)',\n    y = '**Bill Depth** (mm)',\n    color = 'Body mass (g)'\n  ) +\n  \n# Add the ggtext theme related commands\n  theme(\n    ## turn title into filled textbox\n    plot.title = ggtext::element_textbox_simple(\n      color = \"white\",\n      fill = \"#28A78D\",\n      size = 32,\n      padding = margin(8, 4, 8, 4),\n      margin = margin(b = 5),\n      lineheight = .9\n    ),\n    plot.subtitle = ggtext::element_textbox_simple(\n      size = 10,\n      padding = margin(5.5, 5.5, 5.5, 5.5),\n      margin = margin(0, 0, 5.5, 0),\n      fill = \"orange1\"\n    ),\n    \n    ## add round outline to caption\n    plot.caption = ggtext::element_textbox_simple(\n      width = NULL,\n      linetype = 1,\n      fill = \"grey\",\n      padding = margin(4, 8, 4, 8),\n      margin = margin(t = 15),\n      r = grid::unit(8, \"pt\")\n    ),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown(),\n    plot.margin = margin(25, 6, 15, 6)\n  )\n\ngf_box\n\n\n\n\n\n\n\ngg_box &lt;- ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(aes(color = species), alpha = .6, size = 3.5) +\n  \n     ## add text annotations for each species\n    ggtext::geom_richtext(\n    data = labels,\n    # Now pass the data variables as aesthetics\n    aes(x, y, label = lab, color = species, angle = angle), \n    size = 4, fill = NA, label.color = NA,\n    lineheight = .3) +\n    # show.legend = FALSE else we get some unusual legends!\n    # fill = NA makes the labels' fill transparent\n  \n  \n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5), \n                     limits = c(12.5, 22.5)) +\n  scale_colour_paletteer_d(palette = `\"rcartocolor::Bold\"`, guide = \"none\") +\n  \n  \n# Now for the Plot Titles and Labels, as before  \n  labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) *PLoS ONE*',\n    x = '**Bill Length** (mm)',\n    y = '**Bill Depth** (mm)',\n    color = 'Body mass (g)'\n  ) +\n  \n# Add the ggtext theme related commands\n  theme(\n    ## turn title into filled textbox\n    plot.title = ggtext::element_textbox_simple(\n      color = \"white\",\n      fill = \"#28A78D\",\n      size = 32,\n      padding = margin(8, 4, 8, 4),\n      margin = margin(b = 5),\n      lineheight = .9\n    ),\n    plot.subtitle = ggtext::element_textbox_simple(\n      size = 10,\n      padding = margin(5.5, 5.5, 5.5, 5.5),\n      margin = margin(0, 0, 5.5, 0),\n      fill = \"orange1\"\n    ),\n    \n    ## add round outline to caption\n    plot.caption = ggtext::element_textbox_simple(\n      width = NULL,\n      linetype = 1,\n      fill = \"grey\",\n      padding = margin(4, 8, 4, 8),\n      margin = margin(t = 15),\n      r = grid::unit(8, \"pt\")\n    ),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown(),\n    plot.margin = margin(25, 6, 15, 6)\n  )\n\ngg_box\n\n\n\n\n\n\n\n\n\nUsing geom_texbox() for formatted text boxes with word wrapping\n\nUsing ggformulaUsing ggplot\n\n\n\ntext_box &lt;- tibble(x = 34, y = 13.7, label = \"&lt;span style='font-size:12pt;font-family:anton;'&gt;Lorem Ipsum Dolor Sit Amet&lt;/span&gt;&lt;br&gt;&lt;br&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\")\n\n\ngf_box +\n  ## add textbox with long paragraphs\n  ggtext::geom_textbox(\n    data = text_box,\n    aes(x, y, \n        label = label),\n    size = 2.2, family = \"sans\",\n    fill = \"cornsilk\", \n    \n    colour = \"black\",\n# This is ESSENTIAL !!!\n# It appears that the original colour aesthetic mapping in `gf_box` and a possible colour aesthetic with `geom_textbox` have a clash, *only* with ggformula. No such issues below with the ggplot.\n# So declaring a colour here is essential\n\n    box.color = \"cornsilk3\",\n    #box.padding = c(2,2,2,2),\n    width = unit(11, \"lines\")\n  ) +\n  coord_cartesian(clip = \"off\") # ensure no clipping of labels near the edge\n\n\n\n\n\n\n\ngg_box +\n  ## add textbox with long paragraphs\n  ggtext::geom_textbox(\n    data = text_box,\n    aes(x, y, label = label),\n    size = 2.2, family = \"sans\",\n    fill = \"cornsilk\", box.color = \"cornsilk3\",\n    width = unit(11, \"lines\")\n  ) +\n  coord_cartesian(clip = \"off\") # ensure no clipping of labels near the edge\n\n\n\n\n\n\n\n\n\nUsing {ggforce}\nFrom Thomas Lin Pedersen’s website → www.ggforce.data-imaginist.com\n\nggforce is a package aimed at providing missing functionality to ggplot2 through the extension system introduced with ggplot2 v2.0.0. Broadly speaking ggplot2 has been aimed primarily at explorative data visualization in order to investigate the data at hand, and less at providing utilities for composing custom plots a la D3.js. ggforce is mainly an attempt to address these “shortcoming” (design choices might be a better description). The goal is to provide a repository of geoms, stats, etc. that are as well documented and implemented as the official ones found in ggplot2.\n\nWe will start with the basic plot, with the ggtext related work done up to now:\n\n## use ggtext rendering for the following plots\ntheme_update(\n  plot.title = ggtext::element_markdown(),\n  plot.caption = ggtext::element_markdown(),\n  axis.title.x = ggtext::element_markdown(),\n  axis.title.y = ggtext::element_markdown()\n)\n\n\nUsing ggformula and ggforceUsing ggplot and ggforce\n\n\n\n## plot that we will annotate with ggforce afterwards\ngf3 &lt;- penguins %&gt;% \n  gf_point(bill_depth_mm ~ bill_length_mm,\n           color = ~ body_mass_g,\n           alpha = .6, \n           size = 3.5) + \n  \n  coord_cartesian(xlim = c(25, 65), ylim = c(10, 25)) +\n  \n  # Add Colour scales\n  scale_color_paletteer_c(`\"grDevices::Lajolla\"`, direction = -1) +\n  \n  # Add labels\n  labs(\n    title = \"Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)\",\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE*\",\n    x = \"**Bill Length** (mm)\",\n    y = \"**Bill Depth** (mm)\",\n    color = \"Body mass (g)\",\n    fill = \"Species\"\n  )\n\n\n\n## ellipsoids for all groups\ngf3 +\n  ggforce::geom_mark_ellipse(\n    aes(fill = species, \n        label = species), \n    \n    color = \"black\", \n    # This is good to include\n    # Else ellipses get coloured too\n    \n    alpha = .15, \n    show.legend = FALSE\n  ) \n\n\n\n\n\n\n\n## plot that we will annotate with ggforce afterwards\ngg3 &lt;- ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(aes(color = body_mass_g), \n             alpha = .6, \n             size = 3.5) + \n\n  coord_cartesian(xlim = c(25, 65), ylim = c(10, 25)) +\n  \n  # Add Colour scales\n  scale_color_paletteer_c(`\"grDevices::Lajolla\"`, direction = -1) +\n  # rcartocolor::scale_fill_carto_d(palette = \"Bold\") +\n  \n  # Add labels\n  labs(\n    title = \"Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)\",\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE*\",\n    x = \"**Bill Length** (mm)\",\n    y = \"**Bill Depth** (mm)\",\n    color = \"Body mass (g)\",\n    fill = \"Species\"\n  )\n\n\n## ellipsoids for all groups\ngg3 +\n  ggforce::geom_mark_ellipse(\n    aes(fill = species, \n        label = species), \n    alpha = .15, \n    show.legend = FALSE\n  ) \n\n\n\n\n\n## ellipsoids for specific subset\ngg3 +\n  ggforce::geom_mark_ellipse(\n    aes(fill = species, label = species, \n        filter = species == 'Gentoo'), \n    alpha = 0, show.legend = FALSE\n  ) +\n geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5)\n\n\n\n\n\n## circles\ngg3 +\n  ggforce::geom_mark_circle(\n    aes(fill = species, label = species, \n        filter = species == 'Gentoo'), \n    alpha = 0, show.legend = FALSE\n  ) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5)\n\n\n\n\n\n## rectangles\ngg3 +\n  ggforce::geom_mark_rect(\n    aes(fill = species, label = species, \n        filter = species == 'Gentoo'), \n    alpha = 0, show.legend = FALSE\n  ) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5) \n\n\n\n\n\nlibrary(concaveman)\n## hull\ngg3 +\n  ggforce::geom_mark_hull(\n    aes(fill = species, label = species, \n        filter = species == 'Gentoo'), \n    alpha = 0, show.legend = FALSE\n  ) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5)\n\n\n\n\n\n\n\n\n\nggplot tricks\n\ngg0 &lt;-\n  ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  ggforce::geom_mark_ellipse(aes(fill = species,\n                                 label = species),\n                             alpha = .15,\n                             show.legend = FALSE) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5) +\n  \n  scale_x_continuous(breaks = seq(25, 65, by = 5), \n                     limits = c(25, 65)) +\n  scale_y_continuous(breaks = seq(12, 24, by = 2), \n                     limits = c(12, 24)) +\n  \n  scico::scale_color_scico(palette = \"bamako\", direction = -1) +\n  labs(\n    title = \"Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)\",\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE*\",\n    x = \"Bill Length (mm)\",\n    y = \"Bill Depth (mm)\",\n    color = \"Body mass (g)\"\n  )\ngg0\n\n\n\n\n\nLeft-Aligned Title\n\n(gg1 &lt;- gg0 + theme(plot.title.position = \"plot\"))\n\n\n\n\n\n\nRight-Aligned Caption\n\ngg1b &lt;- gg1 +  theme(plot.caption.position = \"plot\")\ngg1b\n\n\n\n\n\n\nLegend Design\n\ngg1b + theme(legend.position = \"top\")\n\n\n\n#ggsave(\"06a_legend_position.pdf\", width = 9, height = 8, device = cairo_pdf)\n\ngg1b + \n  theme(legend.position = \"top\") +\n  guides(color = guide_colorbar(title.position = \"top\", \n                                title.hjust = .5, \n                                barwidth = unit(20, \"lines\"), \n                                barheight = unit(.5, \"lines\")))\n\n\n\n\n\n\n\nAdd Images\n\n## read PNG file from web\npng &lt;- magick::image_read(\"images/culmen_depth.png\")\n## turn image into `rasterGrob`\nimg &lt;- grid::rasterGrob(png, interpolate = TRUE)\n\ngg5 &lt;- gg2 + \n  annotation_custom(img, ymin = 18, ymax = 28, xmin = 58, xmax = 65) +\n    labs(caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE* &bull; Illustration: Allison Horst\")\ngg5\n\n\n\n\n\n\nUsing {patchwork}\n\nThe goal of patchwork is to make it ridiculously simple to combine separate ggplots into the same graphic. As such it tries to solve the same problem as gridExtra::grid.arrange() and cowplot::plot_grid but using an API that incites exploration and iteration, and scales to arbitrily complex layouts.\n\n→ https://patchwork.data-imaginist.com/\nLet us make two plots and combine them into a single patchwork plot.\n\n## calculate bill ratio\npenguins_stats &lt;- penguins %&gt;% \n  mutate(bill_ratio = bill_length_mm / bill_depth_mm) %&gt;% \n  filter(!is.na(bill_ratio))\n\n## create a second chart\ngg6 &lt;- ggplot(penguins_stats, aes(y = bill_ratio, x = species, fill = species, color = species)) + geom_violin() + \n  labs(\n    y = \"Bill ratio\",\n    x = \"Species\",\n    subtitle = \"\",\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE* &bull; Illustration: Allison Horst\"\n  ) +\n  theme(\n    panel.grid.major.x = element_line(linewidth = .35),\n    panel.grid.major.y = element_blank(),\n    axis.text.y = element_text(size = 13),\n    axis.ticks.length = unit(0, \"lines\"),\n    plot.title.position = 'plot',\n    plot.subtitle = element_text(margin = margin(t = 5, b = 10)),\n    plot.margin = margin(10, 25, 10, 25)\n  )\n\nNow to combine both plots into one using simple operators:\n\nFor the special case of putting plots besides each other or on top of each other patchwork provides 2 shortcut operators. | will place plots next to each other while / will place them on top of each other.\n\nFirst we stack up the graphs side by side:\n\n## combine both plots\ngg5 | (gg6 + labs(title = \"Bill Ratios of Brush-Tailed Penguins\",\n                  subtitle = \"Violin Plots of Bill Ration versus species\"))\n\n\n\n\nWe can place them in one column:\n\ngg5 / (gg6 + labs(title = \"Bill Ratios of Brush-Tailed Penguins\",\n                  subtitle = \"Violin Plots of Bill Ration versus species\")) +\n  plot_layout(heights = c(0.4, 0.4))"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/index.html#references",
    "href": "content/courses/R4Artists/Modules/70-wizardy/index.html#references",
    "title": "Lab-7: The Lobster Quadrille",
    "section": "References",
    "text": "References\n\nThomas Lin Pedersen, https://www.data-imaginist.com/. The creator of ggforce, and patchwork packages.\nClaus Wilke, cowplot – Streamlined plot theme and plot annotations for ggplot2, https://wilkelab.org/cowplot/index.html\nClaus Wilke, Spruce up your ggplot2 visualizations with formatted text, https://clauswilke.com/talk/rstudio_conf_2020/. Slides, Code, and Video !\nRobert Kabacoff, ggplot theme cheatsheet, https://rkabacoff.github.io/datavis/modifyingthemes.pdf"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/index.html#fun-stuff",
    "href": "content/courses/R4Artists/Modules/70-wizardy/index.html#fun-stuff",
    "title": "Lab-7: The Lobster Quadrille",
    "section": "Fun Stuff",
    "text": "Fun Stuff\n\nZuguang Gu, Circular Visualization in R,\n\n\n\n\n\n\n\n\n\n\nhttps://jokergoo.github.io/circlize_book/book/"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/90-GoM/spatial.html",
    "href": "content/courses/R4Artists/Modules/90-GoM/spatial.html",
    "title": "Lab-9: If you please sir…which way to the Secret Garden?",
    "section": "",
    "text": "Alice asks the Catterpillar the Way"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/90-GoM/spatial.html#fa-folder-open-slides-and-tutorials",
    "href": "content/courses/R4Artists/Modules/90-GoM/spatial.html#fa-folder-open-slides-and-tutorials",
    "title": "Lab-9: If you please sir…which way to the Secret Garden?",
    "section": "\n Slides and Tutorials",
    "text": "Slides and Tutorials\n\n\n\n\n\n\n\n\n R Tutorial\n Slides \n Leaflet Tutorial\n Leaflet Tutorial"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/90-GoM/spatial.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "href": "content/courses/R4Artists/Modules/90-GoM/spatial.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "title": "Lab-9: If you please sir…which way to the Secret Garden?",
    "section": "\n Introduction",
    "text": "Introduction\nChoropleth Map\nWhat does choropleth1 mean? And what kind of information could this map represent?\n\n\nBubble Map\nWhat information could this map below represent?\n\n\nWhat is there to not like about maps!!! Let us now look at the slides. Then we will understand how the R packages sf, tmap work to create maps, using data downloadable into R using osmdata, osmplotr and rnaturalearth.\nWe will learn to make static and interactive maps and to show off different kinds of data on them, data that have an inherently “spatial” spread or significance! sf + ggplot and tmap give us great static maps. Interactive maps we will make with leaflet and mapview; tmap is also capable of creating interactive maps.\nTrade Routes? Populations? Street Crime hotspots? Theatre and Food districts and popular Restaurants? Literary Paris, London and Barcelona?\nAll possible !!"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/90-GoM/spatial.html#embedded-tutorials",
    "href": "content/courses/R4Artists/Modules/90-GoM/spatial.html#embedded-tutorials",
    "title": "Lab-9: If you please sir…which way to the Secret Garden?",
    "section": "Embedded Tutorials",
    "text": "Embedded Tutorials\n\nknitr::include_url(\"../../../../labs/r-labs/maps/gram-maps.qmd\")\n\n\n\n\nknitr::include_url(\"../../../../slides/r-slides/spatial/Spatial-Data-in-R.htmls\")"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/90-GoM/spatial.html#references",
    "href": "content/courses/R4Artists/Modules/90-GoM/spatial.html#references",
    "title": "Lab-9: If you please sir…which way to the Secret Garden?",
    "section": "References",
    "text": "References\n\nOSM Basic Maps Vignette\nNikita Voevodin, R, Not the Best Practices\nNico Hahn, Making Maps with R\nEmine Fidan, Interactive Maps in R\nLovelace et al, Geocomputation in R"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/90-GoM/spatial.html#footnotes",
    "href": "content/courses/R4Artists/Modules/90-GoM/spatial.html#footnotes",
    "title": "Lab-9: If you please sir…which way to the Secret Garden?",
    "section": "Footnotes",
    "text": "Footnotes\n\nEtymology. From Ancient Greek χώρα (khṓra, “location”) + πλῆθος (plêthos, “a great number”) + English map. First proposed in 1938 by American geographer John Kirtland Wright to mean “quantity in area,” although maps of the type have been used since the early 19th century.↩︎"
  },
  {
    "objectID": "content/labs/doe/index.html",
    "href": "content/labs/doe/index.html",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "",
    "text": "This project is a quick analysis of the Design of Experiments class carried out in the Order and Chaos course, FSP-2021-2022, at SMI MAHE, Bangalore.\nThe methodology followed was that in A.J. Lawrance’s paper 1 describing a Statistics module based on the method of Design of Experiments. The inquiry relates to Short Term Memory (STM) among students.1 Lawrance, A. J. 1996. “A Design of Experiments Workshop as an Introduction to Statistics.” American Statistician 50 (2): 156–58. doi:10.1080/00031305.1996.10474364."
  },
  {
    "objectID": "content/labs/doe/index.html#introduction",
    "href": "content/labs/doe/index.html#introduction",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "",
    "text": "This project is a quick analysis of the Design of Experiments class carried out in the Order and Chaos course, FSP-2021-2022, at SMI MAHE, Bangalore.\nThe methodology followed was that in A.J. Lawrance’s paper 1 describing a Statistics module based on the method of Design of Experiments. The inquiry relates to Short Term Memory (STM) among students.1 Lawrance, A. J. 1996. “A Design of Experiments Workshop as an Introduction to Statistics.” American Statistician 50 (2): 156–58. doi:10.1080/00031305.1996.10474364."
  },
  {
    "objectID": "content/labs/doe/index.html#structure",
    "href": "content/labs/doe/index.html#structure",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "\n2 Structure",
    "text": "2 Structure\nThe total number of students were 17. Eight Pairs of students were created randomly to create eight different Test tools for Short Term Memory testing.\nThe binary ( two - level ) variables/parameters that were used in the tests, were, following Lawrance:\n\nWL: Word List Length ( 7 and 15 words )\n\nSL: Syllables in the Words ( 2 and 5 syllables )\n\nST: Study Time allowed for the Respondents ( 15 and 30 seconds )\n\nOther parameters considered were a) Language b) Structure/Depiction of the Word Lists ( e.g. word clouds, matrices, columns…), c) Whether the words would be shown or read aloud, and d) whether the respondents had to speak out, or write down, the recollected words. These parameters were discussed and abandoned as too complex to mechanize, though they could have had an impact on the STM scores.\nHence a total of 8 Tests were created by 8 pairs of students, and each team tested the remaining 15 students ( Due to COVID restrictions, this testing was carried out entirely online on MS Teams, using individual breakout rooms for the Test Teams. )\nThe data were entered into a Google Sheet and the STM scores were converted to percentages so as to be comparable across WL.\nThe data was then “flattened” for each of the binary parameters; this was logical to do since for each parameter, the other two parameters were balanced out by the Test structure. For instance, for WL = 5, the SL and ST parameters used all the four combinations ( SL = 5, 15 ) and (ST = 15, 30 ). Hence the “common sense” analysis could proceed for each of the parameters individually. Joint effects were not considered for this preliminary class."
  },
  {
    "objectID": "content/labs/doe/index.html#data",
    "href": "content/labs/doe/index.html#data",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "\n3 Data",
    "text": "3 Data\n\n\n\n\n  \n\n\n\nThe data has scores that have been combined into single columns for each setting for each of the parameters. For example, the column syllable_2 contains STM scores for all tests that used SL = 2-syllables in their tests. The Word Length WL and Study Time ST go through all their combinations in this column. The other columns are constructed similarly.\n\n3.1 Basic Plots\nWe will use Box Plots and Density Plots to compare the STM score distributions for each Parameter. To do this we need to pivot_longer the adjacent columns ( e.g. syllable_2 and syllable_5) and use these names as categorical variables:\n\n3.1.1 Syllable Parameter SL\n\n\n\n\n  \n\n\n\n\n\n\n\n3.1.2 Study Time Parameter ST\n\n\n\n\n  \n\n\n\n\n\n\n\n3.1.3 Word List Length Parameter WL"
  },
  {
    "objectID": "content/labs/doe/index.html#preliminary-observations",
    "href": "content/labs/doe/index.html#preliminary-observations",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "\n4 Preliminary Observations",
    "text": "4 Preliminary Observations\nClearly, based on visual inspection of the Plots, the Word Count seems to have a large effect on STM Test Scores, with fewer words ( 7 ) being easier to recall. Study Time ( 15 and 30 seconds ) also seems to have a more modest positive effect on STM scores, while Syllable Count ( 2 or 5 syllables ) seems to have a modest negative effect on STM scores."
  },
  {
    "objectID": "content/labs/doe/index.html#analysis",
    "href": "content/labs/doe/index.html#analysis",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "\n5 Analysis",
    "text": "5 Analysis\nWe wish to establish the significance of the effect size due to each of the Parameters. Already from the Density Plots, we can see that none of the scores are normally distributed. A quick Shapiro-Wilkes Test for each of them confirms that the scores are not normally distributed.\nHence we go for a Permutation Test to check for significance of effect.\nOn the other hand, as remarked in Ernst2, the non-parametric permutation test can be both exact and also intuitively easier for students to grasp, as I can testify from direct observation in this class. There is no need to discuss sampling distributions and means, t-tests and the like. Permutations are easily executed in R, using packages such as mosaic3.2 Ernst, Michael D. 2004. “Permutation Methods: A Basis for Exact Inference.” Statistical Science 19 (4): 676–85. doi:10.1214/088342304000000396.3 Pruim R, Kaplan DT, Horton NJ (2017). “The mosaic Package: Helping Students to ‘Think with Data’ Using R.” The R Journal, 9(1), 77–102. https://journal.r-project.org/archive/2017/RJ-2017-024/index.html.\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  stm$syllable_2\nW = 0.95508, p-value = 0.02716\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  stm$syllable_5\nW = 0.95321, p-value = 0.02211\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  stm$study_time_15\nW = 0.9068, p-value = 0.0002348\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  stm$study_time_30\nW = 0.95539, p-value = 0.0281\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  stm$list_length_7\nW = 0.90542, p-value = 0.0002085\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  stm$list_length_15\nW = 0.92806, p-value = 0.001645"
  },
  {
    "objectID": "content/labs/doe/index.html#permutation-tests",
    "href": "content/labs/doe/index.html#permutation-tests",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "\n6 Permutation Tests",
    "text": "6 Permutation Tests\nWe proceed with a Permutation Test for each of the Parameters. We start with the Syllable Parameter SL. We shuffle the labels ( SL- = 2 and SL+ = 5) between the scores and determine the null distribution. This is then compared with the difference in mean scores between the unpermuted sets. We continue similarly for the other two parameters.\n\n\n[1] 0.0153731\n\n\n\n\n  \n\n\n\n[1] 0.08526183\n\n\n\n\n  \n\n\n\n[1] 0.2887539"
  },
  {
    "objectID": "content/labs/doe/index.html#conclusions",
    "href": "content/labs/doe/index.html#conclusions",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "\n7 Conclusions",
    "text": "7 Conclusions\nFrom the above null distribution plots obtained using Permutation tests, it is clear that both Study Time ( ST ) and List Word Length ( WL) have significant effects on the Short Term Memory Scores. The probability that the observed value is obtained or exceeded by any permutation of scores is very low in both cases.\nOn the other hand, Syllable Count (SL) does not seem to affect the STM scores significantly."
  },
  {
    "objectID": "content/labs/doe/index.html#references",
    "href": "content/labs/doe/index.html#references",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "\n8 References",
    "text": "8 References"
  },
  {
    "objectID": "content/labs/r-labs/dashboard/flexdashboard.html",
    "href": "content/labs/r-labs/dashboard/flexdashboard.html",
    "title": "ggplotly: various examples",
    "section": "",
    "text": "# This example modifies code from Hadley Wickham (https://gist.github.com/hadley/233134)\n# It also uses data from Nathan Yau's flowingdata site (http://flowingdata.com/)\nunemp &lt;- read.csv(\"http://datasets.flowingdata.com/unemployment09.csv\")\nnames(unemp) &lt;- c(\"id\", \"state_fips\", \"county_fips\", \"name\", \"year\", \n                  \"?\", \"?\", \"?\", \"rate\")\nunemp$county &lt;- tolower(gsub(\" County, [A-Z]{2}\", \"\", unemp$name))\nunemp$state &lt;- gsub(\"^.*([A-Z]{2}).*$\", \"\\\\1\", unemp$name)\ncounty_df &lt;- map_data(\"county\")\nnames(county_df) &lt;- c(\"long\", \"lat\", \"group\", \"order\", \"state_name\", \"county\")\ncounty_df$state &lt;- state.abb[match(county_df$state_name, tolower(state.name))]\ncounty_df$state_name &lt;- NULL\nstate_df &lt;- map_data(\"state\")\nchoropleth &lt;- merge(county_df, unemp, by = c(\"state\", \"county\"))\nchoropleth &lt;- choropleth[order(choropleth$order), ]\nchoropleth$rate_d &lt;- cut(choropleth$rate, breaks = c(seq(0, 10, by = 2), 35))\n\n# provide a custom tooltip to plotly with the county name and actual rate\nchoropleth$text &lt;- with(choropleth, paste0(\"County: \", name, \"Rate: \", rate))\np &lt;- ggplot(choropleth, aes(long, lat, group = group)) +\n  geom_polygon(aes(fill = rate_d, text = text), \n               colour = alpha(\"white\", 1/2), size = 0.2) + \n  geom_polygon(data = state_df, colour = \"white\", fill = NA) +\n  scale_fill_brewer(palette = \"PuRd\") + theme_void()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning in geom_polygon(aes(fill = rate_d, text = text), colour =\nalpha(\"white\", : Ignoring unknown aesthetics: text\n\n# just show the text aesthetic in the tooltip\nggplotly(p, tooltip = \"text\")\n\n\n\n\n\n\n\n\ncrimes &lt;- data.frame(state = tolower(rownames(USArrests)), USArrests)\ncrimesm &lt;- tidyr::gather(crimes, variable, value, -state)\nstates_map &lt;- map_data(\"state\")\ng &lt;- ggplot(crimesm, aes(map_id = state)) +\n  geom_map(aes(fill = value), map = states_map) +\n  expand_limits(x = states_map$long, y = states_map$lat) +\n  facet_wrap( ~ variable) + theme_void()\nggplotly(g)"
  },
  {
    "objectID": "content/labs/r-labs/dashboard/flexdashboard.html#row",
    "href": "content/labs/r-labs/dashboard/flexdashboard.html#row",
    "title": "ggplotly: various examples",
    "section": "",
    "text": "# This example modifies code from Hadley Wickham (https://gist.github.com/hadley/233134)\n# It also uses data from Nathan Yau's flowingdata site (http://flowingdata.com/)\nunemp &lt;- read.csv(\"http://datasets.flowingdata.com/unemployment09.csv\")\nnames(unemp) &lt;- c(\"id\", \"state_fips\", \"county_fips\", \"name\", \"year\", \n                  \"?\", \"?\", \"?\", \"rate\")\nunemp$county &lt;- tolower(gsub(\" County, [A-Z]{2}\", \"\", unemp$name))\nunemp$state &lt;- gsub(\"^.*([A-Z]{2}).*$\", \"\\\\1\", unemp$name)\ncounty_df &lt;- map_data(\"county\")\nnames(county_df) &lt;- c(\"long\", \"lat\", \"group\", \"order\", \"state_name\", \"county\")\ncounty_df$state &lt;- state.abb[match(county_df$state_name, tolower(state.name))]\ncounty_df$state_name &lt;- NULL\nstate_df &lt;- map_data(\"state\")\nchoropleth &lt;- merge(county_df, unemp, by = c(\"state\", \"county\"))\nchoropleth &lt;- choropleth[order(choropleth$order), ]\nchoropleth$rate_d &lt;- cut(choropleth$rate, breaks = c(seq(0, 10, by = 2), 35))\n\n# provide a custom tooltip to plotly with the county name and actual rate\nchoropleth$text &lt;- with(choropleth, paste0(\"County: \", name, \"Rate: \", rate))\np &lt;- ggplot(choropleth, aes(long, lat, group = group)) +\n  geom_polygon(aes(fill = rate_d, text = text), \n               colour = alpha(\"white\", 1/2), size = 0.2) + \n  geom_polygon(data = state_df, colour = \"white\", fill = NA) +\n  scale_fill_brewer(palette = \"PuRd\") + theme_void()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning in geom_polygon(aes(fill = rate_d, text = text), colour =\nalpha(\"white\", : Ignoring unknown aesthetics: text\n\n# just show the text aesthetic in the tooltip\nggplotly(p, tooltip = \"text\")\n\n\n\n\n\n\n\n\ncrimes &lt;- data.frame(state = tolower(rownames(USArrests)), USArrests)\ncrimesm &lt;- tidyr::gather(crimes, variable, value, -state)\nstates_map &lt;- map_data(\"state\")\ng &lt;- ggplot(crimesm, aes(map_id = state)) +\n  geom_map(aes(fill = value), map = states_map) +\n  expand_limits(x = states_map$long, y = states_map$lat) +\n  facet_wrap( ~ variable) + theme_void()\nggplotly(g)"
  },
  {
    "objectID": "content/labs/r-labs/dashboard/flexdashboard.html#row-1",
    "href": "content/labs/r-labs/dashboard/flexdashboard.html#row-1",
    "title": "ggplotly: various examples",
    "section": "\n2 Row",
    "text": "2 Row\n\n2.1 Faithful Eruptions\n\nm &lt;- ggplot(faithful, aes(x = eruptions, y = waiting)) +\n  stat_density_2d() + xlim(0.5, 6) + ylim(40, 110)\nggplotly(m)\n\n\n\n\n\n\n2.2 Faithful Eruptions (polygon)\n\nm &lt;- ggplot(faithful, aes(x = eruptions, y = waiting)) +\n  stat_density_2d(aes(fill = ..level..), geom = \"polygon\") + \n  xlim(0.5, 6) + ylim(40, 110)\nggplotly(m)\n\nWarning: The dot-dot notation (`..level..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(level)` instead.\nℹ The deprecated feature was likely used in the ggplot2 package.\n  Please report the issue at &lt;https://github.com/tidyverse/ggplot2/issues&gt;.\n\n\n\n\n\n\n\n2.3 Faithful Eruptions (hex)\n\nm &lt;- ggplot(faithful, aes(x = eruptions, y = waiting)) + geom_hex() \nggplotly(m)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html",
    "href": "content/labs/r-labs/graphics/colors.html",
    "title": "Lab 05: Colors with Penguins",
    "section": "",
    "text": "knitr::opts_chunk$set(error = TRUE, comment = NA, warning = FALSE, errors = FALSE, message = FALSE, tidy = FALSE, cache = FALSE, fig.path= \"03-figs/\")\n\nlibrary(tidyverse) # Manage data\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.0\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(scales) # Create special ( % or $ ) scales\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n#\nlibrary(palmerpenguins) # source of our data\n#\nlibrary(RColorBrewer) # Colour Palettes\nlibrary(wesanderson) # Colour Palettes\n#library(gameofthrones) # You all know this!\n#\nlibrary(paletteer) # Colour Palettes\nlibrary(colorspace) # Colour Palettes\n#\nlibrary(patchwork) # arranges plots on Row-Col \nlibrary(ggthemes)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#iconify-noto-v1-package-setting-up-r-packages",
    "href": "content/labs/r-labs/graphics/colors.html#iconify-noto-v1-package-setting-up-r-packages",
    "title": "Lab 05: Colors with Penguins",
    "section": "",
    "text": "knitr::opts_chunk$set(error = TRUE, comment = NA, warning = FALSE, errors = FALSE, message = FALSE, tidy = FALSE, cache = FALSE, fig.path= \"03-figs/\")\n\nlibrary(tidyverse) # Manage data\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.0\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(scales) # Create special ( % or $ ) scales\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n#\nlibrary(palmerpenguins) # source of our data\n#\nlibrary(RColorBrewer) # Colour Palettes\nlibrary(wesanderson) # Colour Palettes\n#library(gameofthrones) # You all know this!\n#\nlibrary(paletteer) # Colour Palettes\nlibrary(colorspace) # Colour Palettes\n#\nlibrary(patchwork) # arranges plots on Row-Col \nlibrary(ggthemes)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "href": "content/labs/r-labs/graphics/colors.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n2  Introduction",
    "text": "2  Introduction\nThis Quarto document is part of my Workshop in R. The material is based on A Layered Grammar of Graphics by Hadley Wickham. The course is meant for First Year students pursuing a Degree in Art and Design. The intent is to build Skill in coding in R, and also appreciate R as a way to metaphorically visualize information of various kinds, using predominantly geometric figures and structures.\nAll Quarto files combine code, text, web-images, and figures developed using code. Everything is text; code chunks are enclosed in fences (```)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#goals",
    "href": "content/labs/r-labs/graphics/colors.html#goals",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n3 Goals",
    "text": "3 Goals\n\n(Re)Understand different kinds of data variables\nAppreciate how they can be identified based on the Interrogative Pronouns they answer to\nUnderstand how each kind of variable lends itself to a specific choice of colour scale in the data visualization."
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#pedagogical-note",
    "href": "content/labs/r-labs/graphics/colors.html#pedagogical-note",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n4 Pedagogical Note",
    "text": "4 Pedagogical Note\nThe method followed will be based on PRIMM:\n\n\nPREDICT Inspect the code and guess at what the code might do, write predictions\n\n\nRUN the code provided and check what happens\n\nINFER what the parameters of the code do and write comments to explain. What bells and whistles can you see?\n\nMODIFY the parameters code provided to understand the options available. Write comments to show what you have aimed for and achieved.\n\nMAKE : take an idea/concept of your own, and graph it.\n\nIn the following, there is some boiler plate code demonstrating the use of colour palettes in R. There are places where YOUR TURN is mention; copy and play with the boiler plate code to see what happens !"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#data",
    "href": "content/labs/r-labs/graphics/colors.html#data",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n5 Data",
    "text": "5 Data\nWe will use the penguins dataset built into the palmerpenguins package. Your should try other datasets too!\nHere is a glimpse of the data:\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nNote that the unit of observation here is one-row-per-penguin.\nVariables you need for this lab:"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#colour-vs-fill-aesthetic",
    "href": "content/labs/r-labs/graphics/colors.html#colour-vs-fill-aesthetic",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n6 Colour vs fill aesthetic",
    "text": "6 Colour vs fill aesthetic\nFill and colour scales in ggplot2 can use the same palettes. Some shapes such as lines only accept the colour aesthetic, while others, such as polygons, accept both colour and fill aesthetics. In the latter case, the colour refers to the border of the shape, and the fill to the interior.\n\n## A look at all 25 symbols\ndf &lt;- data.frame(x = 1:5,\n                 y = rep(rev(seq(0, 24, by = 5)), each = 5),\n                 z = 1:25)\ns &lt;- ggplot(df, aes(x = x, y = y)) +\n  geom_text(aes(label = z, y = y - 1)) +\n  theme_void()\ns + geom_point(aes(shape = z), size = 4) + scale_shape_identity()\n\n\n\n\nAll symbols have a foreground colour, so if we add color = \"navy\", they all are affected.\n\ns + geom_point(aes(shape = z), size = 4, colour = \"blue\")  + scale_shape_identity()\n\n\n\n\nWhile all symbols have a foreground colour, symbols 21-25 also take a background colour (fill). So if we add fill = \"orchid\", only the last row of symbols are affected.\n\ns + geom_point(aes(shape = z), size = 4, colour = \"blue\", fill = \"orchid\")  + scale_shape_identity()"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#discrete-vs-continuous-variables",
    "href": "content/labs/r-labs/graphics/colors.html#discrete-vs-continuous-variables",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n7 Discrete vs continuous variables",
    "text": "7 Discrete vs continuous variables\nWHAT IS THE DIFFERENCE BETWEEN CATEGORICAL, ORDINAL AND INTERVAL VARIABLES?\nIn order to use color with your data, most importantly, you need to know if you’re dealing with discrete or continuous variables."
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#some-colour-palette-packages-in-r",
    "href": "content/labs/r-labs/graphics/colors.html#some-colour-palette-packages-in-r",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n8 Some Colour Palette Packages in R",
    "text": "8 Some Colour Palette Packages in R\nWe have the following example packages that offer palettes in R:\n\nRColorBrewer\nwesanderson\npaletteer\ncolorspace\n\nSee Appendix for a detailed graphical analysis of these palette packages."
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#colour-palette-types",
    "href": "content/labs/r-labs/graphics/colors.html#colour-palette-types",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n9 Colour Palette Types",
    "text": "9 Colour Palette Types\nThese palettes can be:\n\nSequential (type = “seq”) palettes are suited to ordered data that progress from low to high. Lightness steps dominate the look of these schemes, with light colors for low data values to dark colors for high data values. (for numerical data, that are ordered)\n\n\nDiverging (type = “div”) palettes put equal emphasis on mid-range critical values and extremes at both ends of the data range. The critical class or break in the middle of the legend is emphasized with light colors and low and high extremes are emphasized with dark colors that have contrasting hues.(for numerical data that can be positive or negative, often representing deviations from some norm or baseline)\n\n\nQualitative (type = “qual”) palettes do not imply magnitude differences between legend classes, and hues are used to create the primary visual differences between classes. Qualitative schemes are best suited to representing nominal or categorical data. (for qualitative unordered data)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#create-a-simple-set-of-scatter-plots",
    "href": "content/labs/r-labs/graphics/colors.html#create-a-simple-set-of-scatter-plots",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n10 Create a simple set of scatter plots",
    "text": "10 Create a simple set of scatter plots\nWe will create simple base plots in ggplot and see how we may alter the colour scales using palettes.\n\nnames(penguins)\n\n[1] \"species\"           \"island\"            \"bill_length_mm\"   \n[4] \"bill_depth_mm\"     \"flipper_length_mm\" \"body_mass_g\"      \n[7] \"sex\"               \"year\"             \n\n\n\np1 &lt;- penguins %&gt;% \n  drop_na() %&gt;% \n  # pipe data into ggplot\n  # after removing data rows that have missing ( NA ) values\n  ggplot(aes(y = body_mass_g, x = flipper_length_mm, \n           color = species # COLOUR = DISCRETE/QUAL VARIABLE\n           )) +\n           geom_point() + \n           labs(title = \"Default Colours in ggplot\", \n                subtitle = \"P1: DISCRETE/QUAL Colour Palette\")\n\n\np2 &lt;- \npenguins %&gt;% \n  drop_na() %&gt;% \n  # pipe the data into ggplot, \n  # after removing data rows that have missing ( NA ) values\n  ggplot(aes(y = body_mass_g, x = flipper_length_mm, \n           color = bill_length_mm # COLOUR = CONT/QUANT VARIABLE\n           )) +\n           geom_point() + \n           labs(title = \"Default Colours in ggplot\", \n                subtitle = \"P2: CONTINUOUS/QUANT Colour Palette\")\n\np1\n\n\n\np2\n\n\n\n\nNote that these use the default colours in R."
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#colours-for-discrete-qual-variables",
    "href": "content/labs/r-labs/graphics/colors.html#colours-for-discrete-qual-variables",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n11 Colours for Discrete (QUAL) Variables",
    "text": "11 Colours for Discrete (QUAL) Variables\nThe commands below are used to fill colours based on Qualitative Variables:\n\nscale_colour/fill_discrete\n\nscale_colour/fill_brewer # RColorBrewer\n….\n\nNow to use these!"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#plotting-colours-based-on-discrete-variables",
    "href": "content/labs/r-labs/graphics/colors.html#plotting-colours-based-on-discrete-variables",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n12 Plotting Colours based on Discrete Variables",
    "text": "12 Plotting Colours based on Discrete Variables"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#discrete-n-colour-palettes-from-rcolorbrewer",
    "href": "content/labs/r-labs/graphics/colors.html#discrete-n-colour-palettes-from-rcolorbrewer",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n13 Discrete n-Colour palettes from RColorBrewer\n",
    "text": "13 Discrete n-Colour palettes from RColorBrewer\n\n\nRColorBrewer::brewer.pal.info\n\n\n\n  \n\n\nRColorBrewer::display.brewer.all()\n\n\n\n\n\np1 +\n  # default palette = \"Blues\"\n  scale_colour_brewer() +\n  labs(title = \"Brewer Palette = Blues\")\n\n\n\np1 +\n  scale_color_brewer(palette = \"Spectral\") +\n  labs(title = \"Brewer Palette = Spectral\")"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#discrete-colour-scales-using-wesanderson-palettes",
    "href": "content/labs/r-labs/graphics/colors.html#discrete-colour-scales-using-wesanderson-palettes",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n14 Discrete Colour scales using wesanderson palettes",
    "text": "14 Discrete Colour scales using wesanderson palettes\n\nwesanderson::wes_palettes %&gt;% names()\n\n [1] \"BottleRocket1\"  \"BottleRocket2\"  \"Rushmore1\"      \"Rushmore\"      \n [5] \"Royal1\"         \"Royal2\"         \"Zissou1\"        \"Darjeeling1\"   \n [9] \"Darjeeling2\"    \"Chevalier1\"     \"FantasticFox1\"  \"Moonrise1\"     \n[13] \"Moonrise2\"      \"Moonrise3\"      \"Cavalcanti1\"    \"GrandBudapest1\"\n[17] \"GrandBudapest2\" \"IsleofDogs1\"    \"IsleofDogs2\"   \n\n\n\np1 +\n  scale_colour_discrete(type = wes_palette(name = \"GrandBudapest1\",\n                                           n = 3)) +\n  labs(title = \"Wes Anderson Palette: GrandBudapest\")\n\n\n\n# We can also specify colour codes ourselves with scale_x_discrete.\n# Use argument \"values\" instead of \"type\"\nmanual_colours &lt;- c(\"#afc4b8\", \"#f1a4b2\", \"#ffb1e1\") \nmanual_colours\n\n[1] \"#afc4b8\" \"#f1a4b2\" \"#ffb1e1\"\n\np1 +\n  scale_colour_manual(values =  manual_colours) +\n  labs(title = \"Manual Colours\")"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#discrete-n-colour-palettes-from-rcolorbrewer-1",
    "href": "content/labs/r-labs/graphics/colors.html#discrete-n-colour-palettes-from-rcolorbrewer-1",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n15 Discrete n-Colour palettes from RColorBrewer\n",
    "text": "15 Discrete n-Colour palettes from RColorBrewer\n\n\n# scale_x_brewer() for DISCRETE data\np1 +\n  scale_colour_brewer(palette = \"Spectral\") +\n  \n  labs(title = \"RColorBrewer Palette = Spectral\")"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#discrete-colour-scales-using-paletteer-palettes",
    "href": "content/labs/r-labs/graphics/colors.html#discrete-colour-scales-using-paletteer-palettes",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n16 Discrete Colour scales using paletteer palettes",
    "text": "16 Discrete Colour scales using paletteer palettes\n\npalettes_d_names\n\n\n\n  \n\n\npalettes_dynamic_names\n\n\n\n  \n\n\npaletteer_d(\"dutchmasters::pearl_earring\")\n\n&lt;colors&gt;\n#A65141FF #E7CDC2FF #80A0C7FF #394165FF #FCF9F0FF #B1934AFF #DCA258FF #100F14FF #8B9DAFFF #EEDA9DFF #E8DCCFFF \n\npaletteer_dynamic(\"ggthemes_ptol::qualitative\", n = 3)\n\n&lt;colors&gt;\n#4477AAFF #DDCC77FF #CC6677FF \n\np1 +\n  scale_colour_paletteer_d(\"ggthemes_ptol::qualitative\", \n                           dynamic = TRUE) +\n  \n  labs(title = \"ggThemes Palette: Qualitative\", \n          subtitle = \"\")\n\n\n\n# I like Vermeer's \"Girl with the Pearl Earring\"!\np1 +\n  scale_colour_paletteer_d(\"dutchmasters::pearl_earring\",\n                           dynamic = FALSE) +\n  \n  labs(title = \"Palettes from `paletteer`\", \n          subtitle = \" Palette from Vermeer: Girl with Pearl Earring\")"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#colours-for-continuous-quant-variables",
    "href": "content/labs/r-labs/graphics/colors.html#colours-for-continuous-quant-variables",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n17 Colours for Continuous (QUANT) Variables",
    "text": "17 Colours for Continuous (QUANT) Variables\nThe commands below are used to fill colours based on Quantitative Variables:\n\n\nscale_colour/fill_gradient (Two colour gradient)\n\nscale_colour/fill_gradient2 (Three colour gradient)\n\nscale_colour/fill_gradientn (Specify Palette, from other packages also, like wesanderson )\n\nscale_colour/fill_distiller (Palettes from RColorBrewer)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#plotting-colours-based-on-continuous-variables",
    "href": "content/labs/r-labs/graphics/colors.html#plotting-colours-based-on-continuous-variables",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n18 Plotting Colours based on Continuous Variables",
    "text": "18 Plotting Colours based on Continuous Variables"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#continuous-two-colour-gradients",
    "href": "content/labs/r-labs/graphics/colors.html#continuous-two-colour-gradients",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n19 Continuous Two Colour Gradients",
    "text": "19 Continuous Two Colour Gradients\nCreates a pallete containing continuous shades between two colours:\n\np2 +\n    scale_color_gradient(\n      low = \"yellow\", # Play with this in the chunk below\n      high = \"purple\") + # Play with this in the chnk below\n  \n  labs(title = \"Two Colour Gradients\",\n          subtitle = \"P2: Continuous 2-Colour Pallete\")"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#continuous-three-colour-gradients",
    "href": "content/labs/r-labs/graphics/colors.html#continuous-three-colour-gradients",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n20 Continuous Three Colour Gradients",
    "text": "20 Continuous Three Colour Gradients\nSometimes we want a palette this way: a midpoint colour, and colours for the two extremes of a continuous variable:\n\ncolour_midpoint &lt;- mean(penguins$bill_length_mm, \n                         na.rm = TRUE) # remove missing values\n# Struggled all morning on 22 Aug 2020 to get at this ;-D\n\n# Play with the function: 0/mean/median/mode/max/min\n\np2 +\n  scale_colour_gradient2(\n  low = \"brown\", # Play with this in the chunk below\n  mid = \"white\", # Play with this in the chunk below\n  high = \"purple\", # Play with this in the chunk below\n  midpoint = colour_midpoint, # see above\n  space = \"Lab\", # don't mess with this!\n  na.value = \"grey50\")  +\n  labs(title = \"Three colour continuous gradient\", \n          subtitle = \"Mid Colour mapped to midpoint of data variable\",\n          caption = \"Colours inspired by my favourite cocker spaniel, Lord Chestnut\") # Play with these"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#continuous-n-colour-gradients---grdevices-package",
    "href": "content/labs/r-labs/graphics/colors.html#continuous-n-colour-gradients---grdevices-package",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n21 Continuous n-Colour Gradients - grDevices package",
    "text": "21 Continuous n-Colour Gradients - grDevices package\n\n# grDevices Palettes\np2 +\n  scale_colour_gradientn(\n    colours = terrain.colors(10)) +\n  # Try these:\n  # heat.colors() / topo.colors() / cm.colors() / rainbow()\n  \n  labs(title = \"N-colour continuous gradients\", \n          subtitle = \"Palettes from grDevices\",\n          caption = \"Palette: terrain.colors\")"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#continuous-n-colour-gradients---wesanderson-palettes",
    "href": "content/labs/r-labs/graphics/colors.html#continuous-n-colour-gradients---wesanderson-palettes",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n22 Continuous n-Colour Gradients - wesanderson Palettes",
    "text": "22 Continuous n-Colour Gradients - wesanderson Palettes\n\nwes_palettes\n\n$BottleRocket1\n[1] \"#A42820\" \"#5F5647\" \"#9B110E\" \"#3F5151\" \"#4E2A1E\" \"#550307\" \"#0C1707\"\n\n$BottleRocket2\n[1] \"#FAD510\" \"#CB2314\" \"#273046\" \"#354823\" \"#1E1E1E\"\n\n$Rushmore1\n[1] \"#E1BD6D\" \"#EABE94\" \"#0B775E\" \"#35274A\" \"#F2300F\"\n\n$Rushmore\n[1] \"#E1BD6D\" \"#EABE94\" \"#0B775E\" \"#35274A\" \"#F2300F\"\n\n$Royal1\n[1] \"#899DA4\" \"#C93312\" \"#FAEFD1\" \"#DC863B\"\n\n$Royal2\n[1] \"#9A8822\" \"#F5CDB4\" \"#F8AFA8\" \"#FDDDA0\" \"#74A089\"\n\n$Zissou1\n[1] \"#3B9AB2\" \"#78B7C5\" \"#EBCC2A\" \"#E1AF00\" \"#F21A00\"\n\n$Darjeeling1\n[1] \"#FF0000\" \"#00A08A\" \"#F2AD00\" \"#F98400\" \"#5BBCD6\"\n\n$Darjeeling2\n[1] \"#ECCBAE\" \"#046C9A\" \"#D69C4E\" \"#ABDDDE\" \"#000000\"\n\n$Chevalier1\n[1] \"#446455\" \"#FDD262\" \"#D3DDDC\" \"#C7B19C\"\n\n$FantasticFox1\n[1] \"#DD8D29\" \"#E2D200\" \"#46ACC8\" \"#E58601\" \"#B40F20\"\n\n$Moonrise1\n[1] \"#F3DF6C\" \"#CEAB07\" \"#D5D5D3\" \"#24281A\"\n\n$Moonrise2\n[1] \"#798E87\" \"#C27D38\" \"#CCC591\" \"#29211F\"\n\n$Moonrise3\n[1] \"#85D4E3\" \"#F4B5BD\" \"#9C964A\" \"#CDC08C\" \"#FAD77B\"\n\n$Cavalcanti1\n[1] \"#D8B70A\" \"#02401B\" \"#A2A475\" \"#81A88D\" \"#972D15\"\n\n$GrandBudapest1\n[1] \"#F1BB7B\" \"#FD6467\" \"#5B1A18\" \"#D67236\"\n\n$GrandBudapest2\n[1] \"#E6A0C4\" \"#C6CDF7\" \"#D8A499\" \"#7294D4\"\n\n$IsleofDogs1\n[1] \"#9986A5\" \"#79402E\" \"#CCBA72\" \"#0F0D0E\" \"#D9D0D3\" \"#8D8680\"\n\n$IsleofDogs2\n[1] \"#EAD3BF\" \"#AA9486\" \"#B6854D\" \"#39312F\" \"#1C1718\"\n\nnames(wes_palettes)\n\n [1] \"BottleRocket1\"  \"BottleRocket2\"  \"Rushmore1\"      \"Rushmore\"      \n [5] \"Royal1\"         \"Royal2\"         \"Zissou1\"        \"Darjeeling1\"   \n [9] \"Darjeeling2\"    \"Chevalier1\"     \"FantasticFox1\"  \"Moonrise1\"     \n[13] \"Moonrise2\"      \"Moonrise3\"      \"Cavalcanti1\"    \"GrandBudapest1\"\n[17] \"GrandBudapest2\" \"IsleofDogs1\"    \"IsleofDogs2\"   \n\n\n\np2 +\n    scale_colour_gradientn(\n      colors = wes_palette(name = \"GrandBudapest1\", \n                           n = 4), # Keep an eye on \"n\".\n      na.value = \"grey\") +\n  # Try these:\n  # \"BottleRocket1\"  \"BottleRocket2\"  \"Rushmore1\"\n  # \"Rushmore\"       \"Royal1\"         \"Royal2\"\n  # \"Zissou1\"        \"Darjeeling1\"    \"Darjeeling2\"   \n  # \"Chevalier1\"     \"FantasticFox1\"  \"Moonrise1\"     \n  # \"Moonrise2\"      \"Moonrise3\"      \"Cavalcanti1\"   \n  # \"GrandBudapest1\" \"GrandBudapest2\" \"IsleofDogs1\"   \n  # \"IsleofDogs2\"   \n  # Keep an eye on \"n\".\n  \n  labs(title = \"N-colour continuous gradients\", \n       subtitle = \"Palettes from wesanderson\",\n       caption = \"Palette: GrandBudapest1\") # Change this caption based on palette choice"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#continuous-n-colour-palettes-from-rcolorbrewer",
    "href": "content/labs/r-labs/graphics/colors.html#continuous-n-colour-palettes-from-rcolorbrewer",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n23 Continuous n-Colour palettes from RColorBrewer\n",
    "text": "23 Continuous n-Colour palettes from RColorBrewer\n\nRecall Palette types\n\n\nseq for continuous data mapped to colour\n\nqual for categorical data mapped to colour ( discrete)\n\ndiv continuous data mapped to colour, that has pos and neg extremes from a middle value\n\n\nbrewer.pal.info\n\n\n\n  \n\n\n\n\n# scale_color_distiller() and scale_fill_distiller() \n# are used to apply the ColorBrewer colour scales \n# to continuous data.\n\np2 +\n  scale_colour_distiller(\n    palette = \"YlGnBu\") + # Play with this palette\n  \n  labs(title = \"RColorBrewer Palette\")"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#continuous-colour-scales-using-paletteer-palettes",
    "href": "content/labs/r-labs/graphics/colors.html#continuous-colour-scales-using-paletteer-palettes",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n24 Continuous Colour scales using paletteer palettes",
    "text": "24 Continuous Colour scales using paletteer palettes\nThis palette seems to have everything accessible in a simple way! NOTE: In order to access some palettes in paletteer, you may be asked to install other packages. E.g. harrypotter or scico. These need not be brought into your session using library() but are accessed directly by paletteer which is very convenient!!\n\n# What continuous palettes are there in paletteer?\npaletteer::palettes_c_names\n\n\n\n  \n\n\n\nOK, one of the Games of Thrones Palettes, and Harry Potter!\n\np2 +\n  scale_colour_paletteer_c(\"gameofthrones::jon_snow\") +\n  labs(title = \"Using Paletteer\",\n       subtitle = \"Continuous Palette-Game of Thrones: Jon Snow\",\n       caption = \"Oh you awful Srishti people...\") +\n  \n  # Harry Potter Gryffindor Palette.\n  # Will ask for `harrypotter` package to be installed. Say yes!\n  p2 +\n  scale_colour_paletteer_c(\"harrypotter::gryffindor\") +\n  labs(title = \"Using Paletteer\",\n       subtitle = \"Continuous Palette-Harry Potter:Gryffindor\")\n\nError in gen_fun(name = palette[2], n = n): could not find function \"gen_fun\""
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html",
    "href": "content/labs/r-labs/graphics/wizardy.html",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "",
    "text": "This Quarto document is part of my Workshop in R. The material is based on A Layered Grammar of Graphics by Hadley Wickham, and more specifically on the tutorials by Cedric Scherer. The course is meant for First Year students pursuing a Degree in Art and Design. The intent is to build Skill in coding in R, and also appreciate R as a way to metaphorically visualize information of various kinds, using predominantly geometric figures and structures.\nAll Quarto files combine code, text, web-images, and figures developed using code. Everything is text; code chunks are enclosed in fences (```)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "href": "content/labs/r-labs/graphics/wizardy.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "",
    "text": "This Quarto document is part of my Workshop in R. The material is based on A Layered Grammar of Graphics by Hadley Wickham, and more specifically on the tutorials by Cedric Scherer. The course is meant for First Year students pursuing a Degree in Art and Design. The intent is to build Skill in coding in R, and also appreciate R as a way to metaphorically visualize information of various kinds, using predominantly geometric figures and structures.\nAll Quarto files combine code, text, web-images, and figures developed using code. Everything is text; code chunks are enclosed in fences (```)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#goals",
    "href": "content/labs/r-labs/graphics/wizardy.html#goals",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "\n2 Goals",
    "text": "2 Goals\n\n(Re)Understand different kinds of data variables\nAppreciate how they can be identified based on the Interrogative Pronouns they answer to\nUnderstand how each kind of variable lends itself to a specific choice of colour scale in the data visualization."
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#pedagogical-note",
    "href": "content/labs/r-labs/graphics/wizardy.html#pedagogical-note",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "\n3 Pedagogical Note",
    "text": "3 Pedagogical Note\nThe method followed will be based on PRIMM:\n\n\nPREDICT Inspect the code and guess at what the code might do, write predictions\n\n\nRUN the code provided and check what happens\n\nINFER what the parameters of the code do and write comments to explain. What bells and whistles can you see?\n\nMODIFY the parameters code provided to understand the options available. Write comments to show what you have aimed for and achieved.\n\nMAKE : take an idea/concept of your own, and graph it.\n\nIn the following, there is some boiler plate code demonstrating the use of colour palettes in R. There are places where YOUR TURN is mention; copy and play with the boiler plate code to see what happens !"
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#iconify-noto-v1-package-setting-up-r-packages",
    "href": "content/labs/r-labs/graphics/wizardy.html#iconify-noto-v1-package-setting-up-r-packages",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "\n4  Setting up R Packages",
    "text": "4  Setting up R Packages\nLet’s load up a few packages that we need to start:\n\nlibrary(tidyverse)   ## data science package collection (incl. the ggplot2 package)\nlibrary(systemfonts) ## use custom fonts (need to be installed on your OS)  \nlibrary(scico)       ## scico color palettes(http://www.fabiocrameri.ch/colourmaps.php) in R \nlibrary(ggtext)      ## add improved text rendering to ggplot2\nlibrary(ggforce)     ## add missing functionality to ggplot2\nlibrary(ggdist)      ## add uncertainity visualizations to ggplot2\nlibrary(magick)      ## load images into R\nlibrary(patchwork)   ## combine outputs from ggplot2\nlibrary(kableExtra)  ## Produces attractive tables\nlibrary(palmerpenguins)\n\nlibrary(showtext)   ## add google fonts to plots\n\nWe will want to add a few new fonts to our graphs. The best way (currently) is to use the showtext package ( which we loaded above) to bring into our work fonts from Google. To view and select the fonts you might want to work with, spend some time looking over:\n\nGoogle Webfonts Helper App\nGoogle Fonts\n\n\nfont_add_google(\"Gochi Hand\", \"gochi\")\nfont_add_google(\"Schoolbell\", \"bell\")\nfont_add_google(\"Galada\", \"galada\")\nfont_add_google(\"Schoolbell\", \"bell\")\nfont_add_google(\"Roboto\", \"roboto\")\nfont_add_google(\"Noto Sans\", \"noto\")\nfont_add_google(\"Uchen\", \"uchen\")\nfont_add_google(\"Ibarra Real Nova\", \"ibarra\")\nfont_add_google(\"Open Sans\", \"open\")\nfont_add_google(\"Anton\", \"anton\")\nfont_add_google(\"Tangerine\", \"tangerine\")\n\nshowtext_auto() # set the google fonts as default\n\nWe will work with a familiar dataset, so that we can concentrate on the chart aesthetics, without having to spend time getting used to the data: the penguins dataset again, from the palmerpenguins package."
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#data",
    "href": "content/labs/r-labs/graphics/wizardy.html#data",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "\n5 Data",
    "text": "5 Data\nAlways start your work with a table of the data:\n\npenguins &lt;- penguins %&gt;% drop_na() # remove data containing missing data\n\n## Create a nicely formatted table\n## uses `kableExtra` package\n## \npenguins %&gt;% \n  kableExtra::kbl() %&gt;%\n  kableExtra::kable_paper(full_width = TRUE) %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"condensed\", \"responsive\")) %&gt;%\n  kableExtra::scroll_box(width = \"700px\", height = \"500px\")\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n2007\n\n\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n2007\n\n\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nfemale\n2007\n\n\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n2007\n\n\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmale\n2007\n\n\nAdelie\nTorgersen\n38.9\n17.8\n181\n3625\nfemale\n2007\n\n\nAdelie\nTorgersen\n39.2\n19.6\n195\n4675\nmale\n2007\n\n\nAdelie\nTorgersen\n41.1\n17.6\n182\n3200\nfemale\n2007\n\n\nAdelie\nTorgersen\n38.6\n21.2\n191\n3800\nmale\n2007\n\n\nAdelie\nTorgersen\n34.6\n21.1\n198\n4400\nmale\n2007\n\n\nAdelie\nTorgersen\n36.6\n17.8\n185\n3700\nfemale\n2007\n\n\nAdelie\nTorgersen\n38.7\n19.0\n195\n3450\nfemale\n2007\n\n\nAdelie\nTorgersen\n42.5\n20.7\n197\n4500\nmale\n2007\n\n\nAdelie\nTorgersen\n34.4\n18.4\n184\n3325\nfemale\n2007\n\n\nAdelie\nTorgersen\n46.0\n21.5\n194\n4200\nmale\n2007\n\n\nAdelie\nBiscoe\n37.8\n18.3\n174\n3400\nfemale\n2007\n\n\nAdelie\nBiscoe\n37.7\n18.7\n180\n3600\nmale\n2007\n\n\nAdelie\nBiscoe\n35.9\n19.2\n189\n3800\nfemale\n2007\n\n\nAdelie\nBiscoe\n38.2\n18.1\n185\n3950\nmale\n2007\n\n\nAdelie\nBiscoe\n38.8\n17.2\n180\n3800\nmale\n2007\n\n\nAdelie\nBiscoe\n35.3\n18.9\n187\n3800\nfemale\n2007\n\n\nAdelie\nBiscoe\n40.6\n18.6\n183\n3550\nmale\n2007\n\n\nAdelie\nBiscoe\n40.5\n17.9\n187\n3200\nfemale\n2007\n\n\nAdelie\nBiscoe\n37.9\n18.6\n172\n3150\nfemale\n2007\n\n\nAdelie\nBiscoe\n40.5\n18.9\n180\n3950\nmale\n2007\n\n\nAdelie\nDream\n39.5\n16.7\n178\n3250\nfemale\n2007\n\n\nAdelie\nDream\n37.2\n18.1\n178\n3900\nmale\n2007\n\n\nAdelie\nDream\n39.5\n17.8\n188\n3300\nfemale\n2007\n\n\nAdelie\nDream\n40.9\n18.9\n184\n3900\nmale\n2007\n\n\nAdelie\nDream\n36.4\n17.0\n195\n3325\nfemale\n2007\n\n\nAdelie\nDream\n39.2\n21.1\n196\n4150\nmale\n2007\n\n\nAdelie\nDream\n38.8\n20.0\n190\n3950\nmale\n2007\n\n\nAdelie\nDream\n42.2\n18.5\n180\n3550\nfemale\n2007\n\n\nAdelie\nDream\n37.6\n19.3\n181\n3300\nfemale\n2007\n\n\nAdelie\nDream\n39.8\n19.1\n184\n4650\nmale\n2007\n\n\nAdelie\nDream\n36.5\n18.0\n182\n3150\nfemale\n2007\n\n\nAdelie\nDream\n40.8\n18.4\n195\n3900\nmale\n2007\n\n\nAdelie\nDream\n36.0\n18.5\n186\n3100\nfemale\n2007\n\n\nAdelie\nDream\n44.1\n19.7\n196\n4400\nmale\n2007\n\n\nAdelie\nDream\n37.0\n16.9\n185\n3000\nfemale\n2007\n\n\nAdelie\nDream\n39.6\n18.8\n190\n4600\nmale\n2007\n\n\nAdelie\nDream\n41.1\n19.0\n182\n3425\nmale\n2007\n\n\nAdelie\nDream\n36.0\n17.9\n190\n3450\nfemale\n2007\n\n\nAdelie\nDream\n42.3\n21.2\n191\n4150\nmale\n2007\n\n\nAdelie\nBiscoe\n39.6\n17.7\n186\n3500\nfemale\n2008\n\n\nAdelie\nBiscoe\n40.1\n18.9\n188\n4300\nmale\n2008\n\n\nAdelie\nBiscoe\n35.0\n17.9\n190\n3450\nfemale\n2008\n\n\nAdelie\nBiscoe\n42.0\n19.5\n200\n4050\nmale\n2008\n\n\nAdelie\nBiscoe\n34.5\n18.1\n187\n2900\nfemale\n2008\n\n\nAdelie\nBiscoe\n41.4\n18.6\n191\n3700\nmale\n2008\n\n\nAdelie\nBiscoe\n39.0\n17.5\n186\n3550\nfemale\n2008\n\n\nAdelie\nBiscoe\n40.6\n18.8\n193\n3800\nmale\n2008\n\n\nAdelie\nBiscoe\n36.5\n16.6\n181\n2850\nfemale\n2008\n\n\nAdelie\nBiscoe\n37.6\n19.1\n194\n3750\nmale\n2008\n\n\nAdelie\nBiscoe\n35.7\n16.9\n185\n3150\nfemale\n2008\n\n\nAdelie\nBiscoe\n41.3\n21.1\n195\n4400\nmale\n2008\n\n\nAdelie\nBiscoe\n37.6\n17.0\n185\n3600\nfemale\n2008\n\n\nAdelie\nBiscoe\n41.1\n18.2\n192\n4050\nmale\n2008\n\n\nAdelie\nBiscoe\n36.4\n17.1\n184\n2850\nfemale\n2008\n\n\nAdelie\nBiscoe\n41.6\n18.0\n192\n3950\nmale\n2008\n\n\nAdelie\nBiscoe\n35.5\n16.2\n195\n3350\nfemale\n2008\n\n\nAdelie\nBiscoe\n41.1\n19.1\n188\n4100\nmale\n2008\n\n\nAdelie\nTorgersen\n35.9\n16.6\n190\n3050\nfemale\n2008\n\n\nAdelie\nTorgersen\n41.8\n19.4\n198\n4450\nmale\n2008\n\n\nAdelie\nTorgersen\n33.5\n19.0\n190\n3600\nfemale\n2008\n\n\nAdelie\nTorgersen\n39.7\n18.4\n190\n3900\nmale\n2008\n\n\nAdelie\nTorgersen\n39.6\n17.2\n196\n3550\nfemale\n2008\n\n\nAdelie\nTorgersen\n45.8\n18.9\n197\n4150\nmale\n2008\n\n\nAdelie\nTorgersen\n35.5\n17.5\n190\n3700\nfemale\n2008\n\n\nAdelie\nTorgersen\n42.8\n18.5\n195\n4250\nmale\n2008\n\n\nAdelie\nTorgersen\n40.9\n16.8\n191\n3700\nfemale\n2008\n\n\nAdelie\nTorgersen\n37.2\n19.4\n184\n3900\nmale\n2008\n\n\nAdelie\nTorgersen\n36.2\n16.1\n187\n3550\nfemale\n2008\n\n\nAdelie\nTorgersen\n42.1\n19.1\n195\n4000\nmale\n2008\n\n\nAdelie\nTorgersen\n34.6\n17.2\n189\n3200\nfemale\n2008\n\n\nAdelie\nTorgersen\n42.9\n17.6\n196\n4700\nmale\n2008\n\n\nAdelie\nTorgersen\n36.7\n18.8\n187\n3800\nfemale\n2008\n\n\nAdelie\nTorgersen\n35.1\n19.4\n193\n4200\nmale\n2008\n\n\nAdelie\nDream\n37.3\n17.8\n191\n3350\nfemale\n2008\n\n\nAdelie\nDream\n41.3\n20.3\n194\n3550\nmale\n2008\n\n\nAdelie\nDream\n36.3\n19.5\n190\n3800\nmale\n2008\n\n\nAdelie\nDream\n36.9\n18.6\n189\n3500\nfemale\n2008\n\n\nAdelie\nDream\n38.3\n19.2\n189\n3950\nmale\n2008\n\n\nAdelie\nDream\n38.9\n18.8\n190\n3600\nfemale\n2008\n\n\nAdelie\nDream\n35.7\n18.0\n202\n3550\nfemale\n2008\n\n\nAdelie\nDream\n41.1\n18.1\n205\n4300\nmale\n2008\n\n\nAdelie\nDream\n34.0\n17.1\n185\n3400\nfemale\n2008\n\n\nAdelie\nDream\n39.6\n18.1\n186\n4450\nmale\n2008\n\n\nAdelie\nDream\n36.2\n17.3\n187\n3300\nfemale\n2008\n\n\nAdelie\nDream\n40.8\n18.9\n208\n4300\nmale\n2008\n\n\nAdelie\nDream\n38.1\n18.6\n190\n3700\nfemale\n2008\n\n\nAdelie\nDream\n40.3\n18.5\n196\n4350\nmale\n2008\n\n\nAdelie\nDream\n33.1\n16.1\n178\n2900\nfemale\n2008\n\n\nAdelie\nDream\n43.2\n18.5\n192\n4100\nmale\n2008\n\n\nAdelie\nBiscoe\n35.0\n17.9\n192\n3725\nfemale\n2009\n\n\nAdelie\nBiscoe\n41.0\n20.0\n203\n4725\nmale\n2009\n\n\nAdelie\nBiscoe\n37.7\n16.0\n183\n3075\nfemale\n2009\n\n\nAdelie\nBiscoe\n37.8\n20.0\n190\n4250\nmale\n2009\n\n\nAdelie\nBiscoe\n37.9\n18.6\n193\n2925\nfemale\n2009\n\n\nAdelie\nBiscoe\n39.7\n18.9\n184\n3550\nmale\n2009\n\n\nAdelie\nBiscoe\n38.6\n17.2\n199\n3750\nfemale\n2009\n\n\nAdelie\nBiscoe\n38.2\n20.0\n190\n3900\nmale\n2009\n\n\nAdelie\nBiscoe\n38.1\n17.0\n181\n3175\nfemale\n2009\n\n\nAdelie\nBiscoe\n43.2\n19.0\n197\n4775\nmale\n2009\n\n\nAdelie\nBiscoe\n38.1\n16.5\n198\n3825\nfemale\n2009\n\n\nAdelie\nBiscoe\n45.6\n20.3\n191\n4600\nmale\n2009\n\n\nAdelie\nBiscoe\n39.7\n17.7\n193\n3200\nfemale\n2009\n\n\nAdelie\nBiscoe\n42.2\n19.5\n197\n4275\nmale\n2009\n\n\nAdelie\nBiscoe\n39.6\n20.7\n191\n3900\nfemale\n2009\n\n\nAdelie\nBiscoe\n42.7\n18.3\n196\n4075\nmale\n2009\n\n\nAdelie\nTorgersen\n38.6\n17.0\n188\n2900\nfemale\n2009\n\n\nAdelie\nTorgersen\n37.3\n20.5\n199\n3775\nmale\n2009\n\n\nAdelie\nTorgersen\n35.7\n17.0\n189\n3350\nfemale\n2009\n\n\nAdelie\nTorgersen\n41.1\n18.6\n189\n3325\nmale\n2009\n\n\nAdelie\nTorgersen\n36.2\n17.2\n187\n3150\nfemale\n2009\n\n\nAdelie\nTorgersen\n37.7\n19.8\n198\n3500\nmale\n2009\n\n\nAdelie\nTorgersen\n40.2\n17.0\n176\n3450\nfemale\n2009\n\n\nAdelie\nTorgersen\n41.4\n18.5\n202\n3875\nmale\n2009\n\n\nAdelie\nTorgersen\n35.2\n15.9\n186\n3050\nfemale\n2009\n\n\nAdelie\nTorgersen\n40.6\n19.0\n199\n4000\nmale\n2009\n\n\nAdelie\nTorgersen\n38.8\n17.6\n191\n3275\nfemale\n2009\n\n\nAdelie\nTorgersen\n41.5\n18.3\n195\n4300\nmale\n2009\n\n\nAdelie\nTorgersen\n39.0\n17.1\n191\n3050\nfemale\n2009\n\n\nAdelie\nTorgersen\n44.1\n18.0\n210\n4000\nmale\n2009\n\n\nAdelie\nTorgersen\n38.5\n17.9\n190\n3325\nfemale\n2009\n\n\nAdelie\nTorgersen\n43.1\n19.2\n197\n3500\nmale\n2009\n\n\nAdelie\nDream\n36.8\n18.5\n193\n3500\nfemale\n2009\n\n\nAdelie\nDream\n37.5\n18.5\n199\n4475\nmale\n2009\n\n\nAdelie\nDream\n38.1\n17.6\n187\n3425\nfemale\n2009\n\n\nAdelie\nDream\n41.1\n17.5\n190\n3900\nmale\n2009\n\n\nAdelie\nDream\n35.6\n17.5\n191\n3175\nfemale\n2009\n\n\nAdelie\nDream\n40.2\n20.1\n200\n3975\nmale\n2009\n\n\nAdelie\nDream\n37.0\n16.5\n185\n3400\nfemale\n2009\n\n\nAdelie\nDream\n39.7\n17.9\n193\n4250\nmale\n2009\n\n\nAdelie\nDream\n40.2\n17.1\n193\n3400\nfemale\n2009\n\n\nAdelie\nDream\n40.6\n17.2\n187\n3475\nmale\n2009\n\n\nAdelie\nDream\n32.1\n15.5\n188\n3050\nfemale\n2009\n\n\nAdelie\nDream\n40.7\n17.0\n190\n3725\nmale\n2009\n\n\nAdelie\nDream\n37.3\n16.8\n192\n3000\nfemale\n2009\n\n\nAdelie\nDream\n39.0\n18.7\n185\n3650\nmale\n2009\n\n\nAdelie\nDream\n39.2\n18.6\n190\n4250\nmale\n2009\n\n\nAdelie\nDream\n36.6\n18.4\n184\n3475\nfemale\n2009\n\n\nAdelie\nDream\n36.0\n17.8\n195\n3450\nfemale\n2009\n\n\nAdelie\nDream\n37.8\n18.1\n193\n3750\nmale\n2009\n\n\nAdelie\nDream\n36.0\n17.1\n187\n3700\nfemale\n2009\n\n\nAdelie\nDream\n41.5\n18.5\n201\n4000\nmale\n2009\n\n\nGentoo\nBiscoe\n46.1\n13.2\n211\n4500\nfemale\n2007\n\n\nGentoo\nBiscoe\n50.0\n16.3\n230\n5700\nmale\n2007\n\n\nGentoo\nBiscoe\n48.7\n14.1\n210\n4450\nfemale\n2007\n\n\nGentoo\nBiscoe\n50.0\n15.2\n218\n5700\nmale\n2007\n\n\nGentoo\nBiscoe\n47.6\n14.5\n215\n5400\nmale\n2007\n\n\nGentoo\nBiscoe\n46.5\n13.5\n210\n4550\nfemale\n2007\n\n\nGentoo\nBiscoe\n45.4\n14.6\n211\n4800\nfemale\n2007\n\n\nGentoo\nBiscoe\n46.7\n15.3\n219\n5200\nmale\n2007\n\n\nGentoo\nBiscoe\n43.3\n13.4\n209\n4400\nfemale\n2007\n\n\nGentoo\nBiscoe\n46.8\n15.4\n215\n5150\nmale\n2007\n\n\nGentoo\nBiscoe\n40.9\n13.7\n214\n4650\nfemale\n2007\n\n\nGentoo\nBiscoe\n49.0\n16.1\n216\n5550\nmale\n2007\n\n\nGentoo\nBiscoe\n45.5\n13.7\n214\n4650\nfemale\n2007\n\n\nGentoo\nBiscoe\n48.4\n14.6\n213\n5850\nmale\n2007\n\n\nGentoo\nBiscoe\n45.8\n14.6\n210\n4200\nfemale\n2007\n\n\nGentoo\nBiscoe\n49.3\n15.7\n217\n5850\nmale\n2007\n\n\nGentoo\nBiscoe\n42.0\n13.5\n210\n4150\nfemale\n2007\n\n\nGentoo\nBiscoe\n49.2\n15.2\n221\n6300\nmale\n2007\n\n\nGentoo\nBiscoe\n46.2\n14.5\n209\n4800\nfemale\n2007\n\n\nGentoo\nBiscoe\n48.7\n15.1\n222\n5350\nmale\n2007\n\n\nGentoo\nBiscoe\n50.2\n14.3\n218\n5700\nmale\n2007\n\n\nGentoo\nBiscoe\n45.1\n14.5\n215\n5000\nfemale\n2007\n\n\nGentoo\nBiscoe\n46.5\n14.5\n213\n4400\nfemale\n2007\n\n\nGentoo\nBiscoe\n46.3\n15.8\n215\n5050\nmale\n2007\n\n\nGentoo\nBiscoe\n42.9\n13.1\n215\n5000\nfemale\n2007\n\n\nGentoo\nBiscoe\n46.1\n15.1\n215\n5100\nmale\n2007\n\n\nGentoo\nBiscoe\n47.8\n15.0\n215\n5650\nmale\n2007\n\n\nGentoo\nBiscoe\n48.2\n14.3\n210\n4600\nfemale\n2007\n\n\nGentoo\nBiscoe\n50.0\n15.3\n220\n5550\nmale\n2007\n\n\nGentoo\nBiscoe\n47.3\n15.3\n222\n5250\nmale\n2007\n\n\nGentoo\nBiscoe\n42.8\n14.2\n209\n4700\nfemale\n2007\n\n\nGentoo\nBiscoe\n45.1\n14.5\n207\n5050\nfemale\n2007\n\n\nGentoo\nBiscoe\n59.6\n17.0\n230\n6050\nmale\n2007\n\n\nGentoo\nBiscoe\n49.1\n14.8\n220\n5150\nfemale\n2008\n\n\nGentoo\nBiscoe\n48.4\n16.3\n220\n5400\nmale\n2008\n\n\nGentoo\nBiscoe\n42.6\n13.7\n213\n4950\nfemale\n2008\n\n\nGentoo\nBiscoe\n44.4\n17.3\n219\n5250\nmale\n2008\n\n\nGentoo\nBiscoe\n44.0\n13.6\n208\n4350\nfemale\n2008\n\n\nGentoo\nBiscoe\n48.7\n15.7\n208\n5350\nmale\n2008\n\n\nGentoo\nBiscoe\n42.7\n13.7\n208\n3950\nfemale\n2008\n\n\nGentoo\nBiscoe\n49.6\n16.0\n225\n5700\nmale\n2008\n\n\nGentoo\nBiscoe\n45.3\n13.7\n210\n4300\nfemale\n2008\n\n\nGentoo\nBiscoe\n49.6\n15.0\n216\n4750\nmale\n2008\n\n\nGentoo\nBiscoe\n50.5\n15.9\n222\n5550\nmale\n2008\n\n\nGentoo\nBiscoe\n43.6\n13.9\n217\n4900\nfemale\n2008\n\n\nGentoo\nBiscoe\n45.5\n13.9\n210\n4200\nfemale\n2008\n\n\nGentoo\nBiscoe\n50.5\n15.9\n225\n5400\nmale\n2008\n\n\nGentoo\nBiscoe\n44.9\n13.3\n213\n5100\nfemale\n2008\n\n\nGentoo\nBiscoe\n45.2\n15.8\n215\n5300\nmale\n2008\n\n\nGentoo\nBiscoe\n46.6\n14.2\n210\n4850\nfemale\n2008\n\n\nGentoo\nBiscoe\n48.5\n14.1\n220\n5300\nmale\n2008\n\n\nGentoo\nBiscoe\n45.1\n14.4\n210\n4400\nfemale\n2008\n\n\nGentoo\nBiscoe\n50.1\n15.0\n225\n5000\nmale\n2008\n\n\nGentoo\nBiscoe\n46.5\n14.4\n217\n4900\nfemale\n2008\n\n\nGentoo\nBiscoe\n45.0\n15.4\n220\n5050\nmale\n2008\n\n\nGentoo\nBiscoe\n43.8\n13.9\n208\n4300\nfemale\n2008\n\n\nGentoo\nBiscoe\n45.5\n15.0\n220\n5000\nmale\n2008\n\n\nGentoo\nBiscoe\n43.2\n14.5\n208\n4450\nfemale\n2008\n\n\nGentoo\nBiscoe\n50.4\n15.3\n224\n5550\nmale\n2008\n\n\nGentoo\nBiscoe\n45.3\n13.8\n208\n4200\nfemale\n2008\n\n\nGentoo\nBiscoe\n46.2\n14.9\n221\n5300\nmale\n2008\n\n\nGentoo\nBiscoe\n45.7\n13.9\n214\n4400\nfemale\n2008\n\n\nGentoo\nBiscoe\n54.3\n15.7\n231\n5650\nmale\n2008\n\n\nGentoo\nBiscoe\n45.8\n14.2\n219\n4700\nfemale\n2008\n\n\nGentoo\nBiscoe\n49.8\n16.8\n230\n5700\nmale\n2008\n\n\nGentoo\nBiscoe\n49.5\n16.2\n229\n5800\nmale\n2008\n\n\nGentoo\nBiscoe\n43.5\n14.2\n220\n4700\nfemale\n2008\n\n\nGentoo\nBiscoe\n50.7\n15.0\n223\n5550\nmale\n2008\n\n\nGentoo\nBiscoe\n47.7\n15.0\n216\n4750\nfemale\n2008\n\n\nGentoo\nBiscoe\n46.4\n15.6\n221\n5000\nmale\n2008\n\n\nGentoo\nBiscoe\n48.2\n15.6\n221\n5100\nmale\n2008\n\n\nGentoo\nBiscoe\n46.5\n14.8\n217\n5200\nfemale\n2008\n\n\nGentoo\nBiscoe\n46.4\n15.0\n216\n4700\nfemale\n2008\n\n\nGentoo\nBiscoe\n48.6\n16.0\n230\n5800\nmale\n2008\n\n\nGentoo\nBiscoe\n47.5\n14.2\n209\n4600\nfemale\n2008\n\n\nGentoo\nBiscoe\n51.1\n16.3\n220\n6000\nmale\n2008\n\n\nGentoo\nBiscoe\n45.2\n13.8\n215\n4750\nfemale\n2008\n\n\nGentoo\nBiscoe\n45.2\n16.4\n223\n5950\nmale\n2008\n\n\nGentoo\nBiscoe\n49.1\n14.5\n212\n4625\nfemale\n2009\n\n\nGentoo\nBiscoe\n52.5\n15.6\n221\n5450\nmale\n2009\n\n\nGentoo\nBiscoe\n47.4\n14.6\n212\n4725\nfemale\n2009\n\n\nGentoo\nBiscoe\n50.0\n15.9\n224\n5350\nmale\n2009\n\n\nGentoo\nBiscoe\n44.9\n13.8\n212\n4750\nfemale\n2009\n\n\nGentoo\nBiscoe\n50.8\n17.3\n228\n5600\nmale\n2009\n\n\nGentoo\nBiscoe\n43.4\n14.4\n218\n4600\nfemale\n2009\n\n\nGentoo\nBiscoe\n51.3\n14.2\n218\n5300\nmale\n2009\n\n\nGentoo\nBiscoe\n47.5\n14.0\n212\n4875\nfemale\n2009\n\n\nGentoo\nBiscoe\n52.1\n17.0\n230\n5550\nmale\n2009\n\n\nGentoo\nBiscoe\n47.5\n15.0\n218\n4950\nfemale\n2009\n\n\nGentoo\nBiscoe\n52.2\n17.1\n228\n5400\nmale\n2009\n\n\nGentoo\nBiscoe\n45.5\n14.5\n212\n4750\nfemale\n2009\n\n\nGentoo\nBiscoe\n49.5\n16.1\n224\n5650\nmale\n2009\n\n\nGentoo\nBiscoe\n44.5\n14.7\n214\n4850\nfemale\n2009\n\n\nGentoo\nBiscoe\n50.8\n15.7\n226\n5200\nmale\n2009\n\n\nGentoo\nBiscoe\n49.4\n15.8\n216\n4925\nmale\n2009\n\n\nGentoo\nBiscoe\n46.9\n14.6\n222\n4875\nfemale\n2009\n\n\nGentoo\nBiscoe\n48.4\n14.4\n203\n4625\nfemale\n2009\n\n\nGentoo\nBiscoe\n51.1\n16.5\n225\n5250\nmale\n2009\n\n\nGentoo\nBiscoe\n48.5\n15.0\n219\n4850\nfemale\n2009\n\n\nGentoo\nBiscoe\n55.9\n17.0\n228\n5600\nmale\n2009\n\n\nGentoo\nBiscoe\n47.2\n15.5\n215\n4975\nfemale\n2009\n\n\nGentoo\nBiscoe\n49.1\n15.0\n228\n5500\nmale\n2009\n\n\nGentoo\nBiscoe\n46.8\n16.1\n215\n5500\nmale\n2009\n\n\nGentoo\nBiscoe\n41.7\n14.7\n210\n4700\nfemale\n2009\n\n\nGentoo\nBiscoe\n53.4\n15.8\n219\n5500\nmale\n2009\n\n\nGentoo\nBiscoe\n43.3\n14.0\n208\n4575\nfemale\n2009\n\n\nGentoo\nBiscoe\n48.1\n15.1\n209\n5500\nmale\n2009\n\n\nGentoo\nBiscoe\n50.5\n15.2\n216\n5000\nfemale\n2009\n\n\nGentoo\nBiscoe\n49.8\n15.9\n229\n5950\nmale\n2009\n\n\nGentoo\nBiscoe\n43.5\n15.2\n213\n4650\nfemale\n2009\n\n\nGentoo\nBiscoe\n51.5\n16.3\n230\n5500\nmale\n2009\n\n\nGentoo\nBiscoe\n46.2\n14.1\n217\n4375\nfemale\n2009\n\n\nGentoo\nBiscoe\n55.1\n16.0\n230\n5850\nmale\n2009\n\n\nGentoo\nBiscoe\n48.8\n16.2\n222\n6000\nmale\n2009\n\n\nGentoo\nBiscoe\n47.2\n13.7\n214\n4925\nfemale\n2009\n\n\nGentoo\nBiscoe\n46.8\n14.3\n215\n4850\nfemale\n2009\n\n\nGentoo\nBiscoe\n50.4\n15.7\n222\n5750\nmale\n2009\n\n\nGentoo\nBiscoe\n45.2\n14.8\n212\n5200\nfemale\n2009\n\n\nGentoo\nBiscoe\n49.9\n16.1\n213\n5400\nmale\n2009\n\n\nChinstrap\nDream\n46.5\n17.9\n192\n3500\nfemale\n2007\n\n\nChinstrap\nDream\n50.0\n19.5\n196\n3900\nmale\n2007\n\n\nChinstrap\nDream\n51.3\n19.2\n193\n3650\nmale\n2007\n\n\nChinstrap\nDream\n45.4\n18.7\n188\n3525\nfemale\n2007\n\n\nChinstrap\nDream\n52.7\n19.8\n197\n3725\nmale\n2007\n\n\nChinstrap\nDream\n45.2\n17.8\n198\n3950\nfemale\n2007\n\n\nChinstrap\nDream\n46.1\n18.2\n178\n3250\nfemale\n2007\n\n\nChinstrap\nDream\n51.3\n18.2\n197\n3750\nmale\n2007\n\n\nChinstrap\nDream\n46.0\n18.9\n195\n4150\nfemale\n2007\n\n\nChinstrap\nDream\n51.3\n19.9\n198\n3700\nmale\n2007\n\n\nChinstrap\nDream\n46.6\n17.8\n193\n3800\nfemale\n2007\n\n\nChinstrap\nDream\n51.7\n20.3\n194\n3775\nmale\n2007\n\n\nChinstrap\nDream\n47.0\n17.3\n185\n3700\nfemale\n2007\n\n\nChinstrap\nDream\n52.0\n18.1\n201\n4050\nmale\n2007\n\n\nChinstrap\nDream\n45.9\n17.1\n190\n3575\nfemale\n2007\n\n\nChinstrap\nDream\n50.5\n19.6\n201\n4050\nmale\n2007\n\n\nChinstrap\nDream\n50.3\n20.0\n197\n3300\nmale\n2007\n\n\nChinstrap\nDream\n58.0\n17.8\n181\n3700\nfemale\n2007\n\n\nChinstrap\nDream\n46.4\n18.6\n190\n3450\nfemale\n2007\n\n\nChinstrap\nDream\n49.2\n18.2\n195\n4400\nmale\n2007\n\n\nChinstrap\nDream\n42.4\n17.3\n181\n3600\nfemale\n2007\n\n\nChinstrap\nDream\n48.5\n17.5\n191\n3400\nmale\n2007\n\n\nChinstrap\nDream\n43.2\n16.6\n187\n2900\nfemale\n2007\n\n\nChinstrap\nDream\n50.6\n19.4\n193\n3800\nmale\n2007\n\n\nChinstrap\nDream\n46.7\n17.9\n195\n3300\nfemale\n2007\n\n\nChinstrap\nDream\n52.0\n19.0\n197\n4150\nmale\n2007\n\n\nChinstrap\nDream\n50.5\n18.4\n200\n3400\nfemale\n2008\n\n\nChinstrap\nDream\n49.5\n19.0\n200\n3800\nmale\n2008\n\n\nChinstrap\nDream\n46.4\n17.8\n191\n3700\nfemale\n2008\n\n\nChinstrap\nDream\n52.8\n20.0\n205\n4550\nmale\n2008\n\n\nChinstrap\nDream\n40.9\n16.6\n187\n3200\nfemale\n2008\n\n\nChinstrap\nDream\n54.2\n20.8\n201\n4300\nmale\n2008\n\n\nChinstrap\nDream\n42.5\n16.7\n187\n3350\nfemale\n2008\n\n\nChinstrap\nDream\n51.0\n18.8\n203\n4100\nmale\n2008\n\n\nChinstrap\nDream\n49.7\n18.6\n195\n3600\nmale\n2008\n\n\nChinstrap\nDream\n47.5\n16.8\n199\n3900\nfemale\n2008\n\n\nChinstrap\nDream\n47.6\n18.3\n195\n3850\nfemale\n2008\n\n\nChinstrap\nDream\n52.0\n20.7\n210\n4800\nmale\n2008\n\n\nChinstrap\nDream\n46.9\n16.6\n192\n2700\nfemale\n2008\n\n\nChinstrap\nDream\n53.5\n19.9\n205\n4500\nmale\n2008\n\n\nChinstrap\nDream\n49.0\n19.5\n210\n3950\nmale\n2008\n\n\nChinstrap\nDream\n46.2\n17.5\n187\n3650\nfemale\n2008\n\n\nChinstrap\nDream\n50.9\n19.1\n196\n3550\nmale\n2008\n\n\nChinstrap\nDream\n45.5\n17.0\n196\n3500\nfemale\n2008\n\n\nChinstrap\nDream\n50.9\n17.9\n196\n3675\nfemale\n2009\n\n\nChinstrap\nDream\n50.8\n18.5\n201\n4450\nmale\n2009\n\n\nChinstrap\nDream\n50.1\n17.9\n190\n3400\nfemale\n2009\n\n\nChinstrap\nDream\n49.0\n19.6\n212\n4300\nmale\n2009\n\n\nChinstrap\nDream\n51.5\n18.7\n187\n3250\nmale\n2009\n\n\nChinstrap\nDream\n49.8\n17.3\n198\n3675\nfemale\n2009\n\n\nChinstrap\nDream\n48.1\n16.4\n199\n3325\nfemale\n2009\n\n\nChinstrap\nDream\n51.4\n19.0\n201\n3950\nmale\n2009\n\n\nChinstrap\nDream\n45.7\n17.3\n193\n3600\nfemale\n2009\n\n\nChinstrap\nDream\n50.7\n19.7\n203\n4050\nmale\n2009\n\n\nChinstrap\nDream\n42.5\n17.3\n187\n3350\nfemale\n2009\n\n\nChinstrap\nDream\n52.2\n18.8\n197\n3450\nmale\n2009\n\n\nChinstrap\nDream\n45.2\n16.6\n191\n3250\nfemale\n2009\n\n\nChinstrap\nDream\n49.3\n19.9\n203\n4050\nmale\n2009\n\n\nChinstrap\nDream\n50.2\n18.8\n202\n3800\nmale\n2009\n\n\nChinstrap\nDream\n45.6\n19.4\n194\n3525\nfemale\n2009\n\n\nChinstrap\nDream\n51.9\n19.5\n206\n3950\nmale\n2009\n\n\nChinstrap\nDream\n46.8\n16.5\n189\n3650\nfemale\n2009\n\n\nChinstrap\nDream\n45.7\n17.0\n195\n3650\nfemale\n2009\n\n\nChinstrap\nDream\n55.8\n19.8\n207\n4000\nmale\n2009\n\n\nChinstrap\nDream\n43.5\n18.1\n202\n3400\nfemale\n2009\n\n\nChinstrap\nDream\n49.6\n18.2\n193\n3775\nmale\n2009\n\n\nChinstrap\nDream\n50.8\n19.0\n210\n4100\nmale\n2009\n\n\nChinstrap\nDream\n50.2\n18.7\n198\n3775\nfemale\n2009"
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#basic-plot",
    "href": "content/labs/r-labs/graphics/wizardy.html#basic-plot",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "\n6 Basic Plot",
    "text": "6 Basic Plot\nA basic scatter plot, which we will progressively dress up:\n\n## simple plot: data + mappings + geometry\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(alpha = .6, size = 3.5)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#customized-plot",
    "href": "content/labs/r-labs/graphics/wizardy.html#customized-plot",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "\n7 Customized Plot",
    "text": "7 Customized Plot\nLet us set some ggplot theme aspects now!! Here is a handy picture showing ( most of ) the theme-able aspects of a ggplot plot.\nFor more info, type ?theme in your console.\n\n\nggplot Theme Elements\n\n\n## change global theme settings (for all following plots)\ntheme_set(theme_minimal(base_size = 12, base_family = \"open\"))\n\n## modify plot elements globally (for all following plots)\ntheme_update(\n  axis.ticks = element_line(color = \"grey92\"),\n  axis.ticks.length = unit(.5, \"lines\"),\n  panel.grid.minor = element_blank(),\n  legend.title = element_text(size = 12),\n  legend.text = element_text(color = \"grey30\"),\n  plot.title = element_text(size = 18, face = \"bold\"),\n  plot.subtitle = element_text(size = 12, color = \"grey30\"),\n  plot.caption = element_text(size = 9, margin = margin(t = 15))\n)\n\nSince we know what the basic plot looks like, let’s add titles, labels and colours. We will also set limits and scales:\n\nggplot(penguins, aes(x = bill_length_mm, \n                     y = bill_depth_mm)) +\n  \n  geom_point(aes(color = body_mass_g), alpha = .6, \n             size = 3.5) + \n  \n  ## custom axes scaling\n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  \n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5), \n                     limits = c(12.5, 22.5)) +\n  \n  ## custom colors from the scico package\n  scico::scale_color_scico(palette = \"bamako\", direction = -1) +\n  \n  ## custom labels\n  labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (Pygoscelis)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) PLoS ONE',\n    x = 'Bill Length (mm)', \n    y = 'Bill Depth (mm)',\n    color = 'Body mass (g)'\n  )"
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#using-ggtext",
    "href": "content/labs/r-labs/graphics/wizardy.html#using-ggtext",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "\n8 Using {ggtext}\n",
    "text": "8 Using {ggtext}\n\nFrom Claus Wilke’s website (www.wilkelab.org/ggtext)\n\nThe ggtext package provides simple Markdown and HTML rendering for ggplot2. Under the hood, the package uses the gridtext package for the actual rendering, and consequently it is limited to the feature set provided by gridtext.\nSupport is provided for Markdown both in theme elements (plot titles, subtitles, captions, axis labels, legends, etc.) and in geoms (similar to geom_text()). In both cases, there are two alternatives, one for creating simple text labels and one for creating text boxes with word wrapping.\n\nNOTE: on some machines, the ggtext package may not work as expected. In this case, please do as follows, using your Console:\n\nremove gridtext: remove.packages(gridtext).\nInstall development version of gridtext: remotes::install_github(\"wilkelab/gridtext\")\n\n\n\n8.1 element_markdown()\n\nWe can use our familiar markdown syntax right inside the titles and captions of the plot. element_markdown() is a theming command made available by the ggtext package.\nelement_markdown() → formatted text elements, e.g. titles, caption, axis text, strip text\n\n## assign plot to `gt` - we can add new things to this plot later\n## (wrapped in parenthesis so it is assigned and plotted in one step)\n\n(gt &lt;- ggplot(penguins, aes(x = bill_length_mm, \n                            y = bill_depth_mm)) +\n    \n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5) +\n    \n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n    \n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5), \n                     limits = c(12.5, 22.5)) +\n    \n  scico::scale_color_scico(palette = \"bamako\", direction = -1) +\n   \n# New code starts here: Two Step Procedure with ggtext\n# 1. Markdown formatting of labels and title, using asterisks\n# To create italics and bold text in titles\n    \n    \n  labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) *PLoS ONE*',\n    \n    x = '**Bill Length** (mm)', \n    y = '**Bill Depth** (mm)',\n    \n    color = 'Body mass (g)'\n  ) +\n   \n  ## 2. Add theme related commands from ggtext\n  ## render respective text elements\n    \n  theme(\n    plot.title = ggtext::element_markdown(),\n    plot.caption = ggtext::element_markdown(),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown()\n  )\n)\n\n\n\n\n\n8.2 element_markdown() in combination with HTML\nThis allows us to change fonts in titles, labels, and captions:\n\n## use HTML syntax to change text color\ngt_mar &lt;- gt +\n  labs(title = 'Bill Dimensions of Brush-Tailed Penguins &lt;i style=\"color:#28A87D;\"&gt;Pygoscelis&lt;/i&gt;') +\n  theme(plot.margin = margin(t = 25))\n\n\n## use HTML syntax to change font and text size\ngt_mar +\n  labs(title = 'Bill Dimensions of Brush-Tailed Penguins &lt;b style=\"font-size:32pt;font-family:tangerine;\"&gt;Pygoscelis&lt;/b&gt;')\n\n\n\n\n\n8.3 Adding images to ggplot\nSave an image from the web in the same folder as your RMarkdown. Use html tags to include it, say as part of your plot title, as shown below.\n\n## use HTML syntax to add images to text elements\ngt_mar + \n  labs(title = 'Bill Dimensions of Brush-Tailed Penguins &nbsp;&nbsp;&nbsp; &lt;img src=\"images/culmen_depth.png\"‚ width=\"480\"/&gt;')\n\n\n\n\n\n8.4 Annotations with geom_richtext() and geom_textbox()\n\nFurther ggplot annotations can be achieved using geom_richtext() and geom_textbox().\ngeom_richtext() also allows formatted text labels with 360° rotation. One needs to pass a dataframe to geom_richtext() giving the location, colour, rotation etc of the label annotation.\n\ngt_rich &lt;- ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(aes(color = species), alpha = .6, size = 3.5) + \n  \n  ## add text annotations for each species\n  ggtext::geom_richtext(\n    data = tibble(\n      \n      # Three rich text labels, so three sets of locations x and y, and angle of rotation\n      x = c(34, 56, 54), y = c(20, 18.5, 14.5),\n            angle = c(12, 20, 335),\n      species = c(\"Adélie\", \"Chinstrap\", \"Gentoo\"),\n      lab = c(\"&lt;b style='font-family:anton;font-size:24pt;'&gt;Adélie&lt;/b&gt;&lt;br&gt;&lt;i style='color:darkgrey;'&gt;P. adéliae&lt;/i&gt;\", \n              \"&lt;b style='font-family:anton;font-size:24pt;'&gt;Chinstrap&lt;/b&gt;&lt;br&gt;&lt;i style='color:darkgrey;'&gt;P. antarctica&lt;/i&gt;\", \n              \"&lt;b style='font-family:anton;font-size:24pt;'&gt;Gentoo&lt;/b&gt;&lt;br&gt;&lt;i style='color:darkgrey;'&gt;P. papua&lt;/i&gt;\"),\n\n    ),\n    \n    \n    # Now pass these data variables as aesthetics\n    aes(x, y, label = lab, color = species, angle = angle), \n    \n    size = 4, fill = NA, label.color = NA,\n    lineheight = .3\n  ) +\n  \n  \n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5), \n                     limits = c(12.5, 22.5)) +\n  \n  rcartocolor::scale_color_carto_d(palette = \"Bold\", \n                                   guide = \"none\") +\n  \n  labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) *PLoS ONE*',\n    x = '**Bill Length** (mm)', \n    y = '**Bill Depth** (mm)',\n    color = 'Body mass (g)'\n  ) +\n  \n  # Use theme and element_markdown() to format axes and titles as usual\n  theme(\n    plot.title = ggtext::element_markdown(),\n    plot.caption = ggtext::element_markdown(),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown(),\n    plot.margin = margin(25, 6, 15, 6)\n  )\n  \ngt_rich\n\n\n\n\n\n8.5 Formatted Text boxes on ggplots\nelement_textbox() and element_textbox_simple() → formatted text boxes with word wrapping\n\ngt_box &lt;- ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(aes(color = species), alpha = .6, size = 3.5) +\n  \n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  \n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5),\n                     limits = c(12.5, 22.5)) +\n  \n  rcartocolor::scale_color_carto_d(palette = \"Bold\", guide = \"none\") +\n  \n  ## add text annotations for each species\n  ## Creating a tibble for the labels!\n  ggtext::geom_richtext(\n    data = tibble(\n      # Three rich text labels\n      # So three sets of locations x and y, and angle of rotation\n      x = c(34, 56, 54),\n      y = c(20, 18.5, 14.5),\n      angle = c(12, 20, 335),\n      species = c(\"Adélie\", \"Chinstrap\", \"Gentoo\"),\n      notes = c(\n        \"&lt;b style='font-family:anton;font-size:24pt;'&gt;Adélie&lt;/b&gt;&lt;br&gt;&lt;i style='color:darkgrey;'&gt;P. adéliae&lt;/i&gt;\",\n        \"&lt;b style='font-family:anton;font-size:24pt;'&gt;Chinstrap&lt;/b&gt;&lt;br&gt;&lt;i style='color:darkgrey;'&gt;P. antarctica&lt;/i&gt;\",\n        \"&lt;b style='font-family:anton;font-size:24pt;'&gt;Gentoo&lt;/b&gt;&lt;br&gt;&lt;i style='color:darkgrey;'&gt;P. papua&lt;/i&gt;\"\n      )\n      ),\n    \n    \n    # Now pass these data variables as aesthetics\n    aes(\n      x,\n      y,\n      label = notes,\n      color = species,\n      angle = angle\n    ),\n    \n    size = 4,\n    fill = NA,\n    label.color = NA,\n    lineheight = .3\n  ) +\n  \n  \n# Now for the Plot Titles and Labels, as before  \n  labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) *PLoS ONE*',\n    x = '**Bill Length** (mm)',\n    y = '**Bill Depth** (mm)',\n    color = 'Body mass (g)'\n  ) +\n  \n# Add the ggtext theme related commands\n  theme(\n    ## turn title into filled textbox\n    plot.title = ggtext::element_textbox_simple(\n      color = \"white\",\n      fill = \"#28A78D\",\n      size = 32,\n      padding = margin(8, 4, 8, 4),\n      margin = margin(b = 5),\n      lineheight = .9\n    ),\n    ## add round outline to caption\n    plot.caption = ggtext::element_textbox_simple(\n      width = NULL,\n      linetype = 1,\n      padding = margin(4, 8, 4, 8),\n      margin = margin(t = 15),\n      r = grid::unit(8, \"pt\")\n    ),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown(),\n    plot.margin = margin(25, 6, 15, 6)\n  )\n\ngt_box\n\n\n\n\ngeom_textbox() → formatted text boxes with word wrapping\n\ngt_box +\n  ## add textbox with long paragraphs\n  ggtext::geom_textbox(\n    data = tibble(x = 34, y = 13.7, label = \"&lt;span style='font-size:12pt;font-family:anton;'&gt;Lorem Ipsum Dolor Sit Amet&lt;/span&gt;&lt;br&gt;&lt;br&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\"),\n    aes(x, y, label = label),\n    size = 2.2, family = \"sans\",\n    fill = \"cornsilk\", box.color = \"cornsilk3\",\n    width = unit(11, \"lines\")\n  ) +\n  coord_cartesian(clip = \"off\") # ensure no clipping of labels near the edge"
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#using-ggforce",
    "href": "content/labs/r-labs/graphics/wizardy.html#using-ggforce",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "\n9 Using {ggforce}\n",
    "text": "9 Using {ggforce}\n\nFrom Thomas Lin Pedersen’s website → www.ggforce.data-imaginist.com\n\nggforce is a package aimed at providing missing functionality to ggplot2 through the extension system introduced with ggplot2 v2.0.0. Broadly speaking ggplot2 has been aimed primarily at explorative data visualization in order to investigate the data at hand, and less at providing utilities for composing custom plots a la D3.js. ggforce is mainly an attempt to address these “shortcoming” (design choices might be a better description). The goal is to provide a repository of geoms, stats, etc. that are as well documented and implemented as the official ones found in ggplot2.\n\nWe will start with the basic plot, with the ggtext related work done up to now:\n\n## use ggtext rendering for the following plots\ntheme_update(\n  plot.title = ggtext::element_markdown(),\n  plot.caption = ggtext::element_markdown(),\n  axis.title.x = ggtext::element_markdown(),\n  axis.title.y = ggtext::element_markdown()\n)\n\n\n## plot that we will annotate with ggforce afterwards\n(gf &lt;- ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) + \n  scico::scale_color_scico(palette = \"bamako\", direction = -1) +\n  coord_cartesian(xlim = c(25, 65), ylim = c(10, 25)) +\n  rcartocolor::scale_fill_carto_d(palette = \"Bold\") +\n  labs(\n    title = \"Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)\",\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE*\",\n    x = \"**Bill Length** (mm)\", \n    y = \"**Bill Depth** (mm)\",\n    color = \"Body mass (g)\",\n    fill = \"Species\"\n  ))\n\n\n\n\n\n## ellipsoids for all groups\n(gf +\n  ggforce::geom_mark_ellipse(\n    aes(fill = species, label = species), \n    alpha = .15, show.legend = FALSE\n  ) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5)\n)\n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` in the `default_aes` field and elsewhere instead.\n\n\n\n\n\n\n## ellipsoids for specific subset\n(gf +\n  ggforce::geom_mark_ellipse(\n    aes(fill = species, label = species, filter = species == 'Gentoo'), \n    alpha = 0, show.legend = FALSE\n  ) +\n geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5)\n#   coord_cartesian(xlim = c(25, 65), ylim = c(10, 25))\n)\n\n\n\n\n\n## circles\n(gf +\n  ggforce::geom_mark_circle(\n    aes(fill = species, label = species, filter = species == 'Gentoo'), \n    alpha = 0, show.legend = FALSE\n  ) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5)\n)\n\n\n\n\n\n## rectangles\n(gf +\n  ggforce::geom_mark_rect(\n    aes(fill = species, label = species, filter = species == 'Gentoo'), \n    alpha = 0, show.legend = FALSE\n  ) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5) \n)\n\n\n\n\n\nlibrary(concaveman)\n## hull\n(gf +\n  ggforce::geom_mark_hull(\n    aes(fill = species, label = species, filter = species == 'Gentoo'), \n    alpha = 0, show.legend = FALSE\n  ) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5)\n)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#ggplot-tricks",
    "href": "content/labs/r-labs/graphics/wizardy.html#ggplot-tricks",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "\n10 ggplot tricks",
    "text": "10 ggplot tricks\n\n(gg0 &lt;- \n  ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n    ggforce::geom_mark_ellipse(\n      aes(fill = species, label = species), \n      alpha = 0, show.legend = FALSE\n    ) +\n    geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5) + \n    scale_x_continuous(breaks = seq(25, 65, by = 5), limits = c(25, 65)) +\n    scale_y_continuous(breaks = seq(12, 24, by = 2), limits = c(12, 24)) +\n    scico::scale_color_scico(palette = \"bamako\", direction = -1) +\n    labs(\n      title = \"Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)\",\n      subtitle = 'A scatter plot of bill depth versus bill length.',\n      caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE*\",\n      x = \"Bill Length (mm)\", \n      y = \"Bill Depth (mm)\",\n      color = \"Body mass (g)\"\n    )\n)\n\n\n\n\n\n10.1 Left-Aligned Title\n\n(gg1 &lt;- gg0 + theme(plot.title.position = \"plot\"))\n\n\n\n\n\n10.2 Right-Aligned Caption\n\n(gg1b &lt;- gg1 +  theme(plot.caption.position = \"panel\"))\n\n\n\n\n\n10.3 Legend Design\n\n(gg2 &lt;- gg1b + theme(legend.position = \"top\"))\n\n\n\nggsave(\"06a_legend_position.pdf\", width = 9, height = 8, device = cairo_pdf)\n\n(gg2b &lt;- gg2 + \n  guides(color = guide_colorbar(title.position = \"top\", \n                                title.hjust = .5, \n                                barwidth = unit(20, \"lines\"), \n                                barheight = unit(.5, \"lines\"))))\n\n\n\n\n\n10.4 Add Images\n\n## read PNG file from web\npng &lt;- magick::image_read(\"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/man/figures/culmen_depth.png\")\n## turn image into `rasterGrob`\nimg &lt;- grid::rasterGrob(png, interpolate = TRUE)\n\ngg3 &lt;- gg2b +\n  annotation_custom(img, ymin = 18.5, ymax = 30.5, xmin = 55, xmax = 65.5) +\n    labs(caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE* &bull; Illustration: Allison Horst\")\ngg3\n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` in the `default_aes` field and elsewhere instead."
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#using-patchwork",
    "href": "content/labs/r-labs/graphics/wizardy.html#using-patchwork",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "\n11 Using {patchwork}\n",
    "text": "11 Using {patchwork}\n\n\nThe goal of patchwork is to make it ridiculously simple to combine separate ggplots into the same graphic. As such it tries to solve the same problem as gridExtra::grid.arrange() and cowplot::plot_grid but using an API that incites exploration and iteration, and scales to arbitrily complex layouts.\n\n→ https://patchwork.data-imaginist.com/\nLet us make two plots and combine them into a single patchwork plot.\n\n## calculate bill ratio\npenguins_stats &lt;- penguins %&gt;% \n  mutate(bill_ratio = bill_length_mm / bill_depth_mm) %&gt;% \n  filter(!is.na(bill_ratio))\n\n## create a second chart\ngg4 &lt;- ggplot(penguins_stats, aes(y = bill_ratio, x = species, fill = species, color = species)) + geom_violin() + \n  labs(\n    y = \"Bill ratio\",\n    x = \"Species\",\n    subtitle = \"\",\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE* &bull; Illustration: Allison Horst\"\n  ) +\n  theme(\n    panel.grid.major.x = element_line(size = .35),\n    panel.grid.major.y = element_blank(),\n    axis.text.y = element_text(size = 13),\n    axis.ticks.length = unit(0, \"lines\"),\n    plot.title.position = 'plot',\n    plot.subtitle = element_text(margin = margin(t = 5, b = 10)),\n    plot.margin = margin(10, 25, 10, 25)\n  )\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\nNow to combine both plots into one using simple operators:\n\nFor the special case of putting plots besides each other or on top of each other patchwork provides 2 shortcut operators. | will place plots next to each other while / will place them on top of each other.\n\nFirst we stack up the graphs side by side:\n\n## combine both plots\ngg3 | (gg4 + labs(title = \"Bill Ratios of Brush-Tailed Penguins\",\n                  subtitle = \"Violin Plots of Bill Ration versus species\"))\n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` in the `default_aes` field and elsewhere instead.\n\n\n\n\n\nWe can place them in one column:\n\ngg3 / (gg3 + labs(title = \"Bill Ratios of Brush-Tailed Penguins\",\n                  subtitle = \"Violin Plots of Bill Ration versus species\")) +\n  plot_layout(heights = c(0.4, 0.4))"
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#resources",
    "href": "content/labs/r-labs/graphics/wizardy.html#resources",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "\n12 Resources",
    "text": "12 Resources\n\n\nIntro to R (one of many good online tutorials)\n“R for Data Science” book (open-access)\nggplot2 Book (open-access)\nR Graph Gallery\nSlides of Cedric Scherer’s talk\nExtensive ggplot2 tutorial\n“Evolution of a ggplot” blog post by Cedric Scherer\n\n#TidyTuesday project (#TidyTuesday on Twitter)\n\n#TidyTuesday Contributions by Cedric Scherer incl. all codes\n\nR4DS learning community (huge Slack community for people learning R incl. a mentoring program)\n\nIllustrations by Allison Horst (more general about data and stats + R-related)\nR Packages:\n\nggplot2\nggtext\nggforce\nggdist\nggraph\nggstream\nggbump\ngggibous\nwaffle\ngeofacet\ncartogram\npatchwork\nsf"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html",
    "href": "content/labs/r-labs/maps/gram-maps.html",
    "title": "The Grammar of Maps",
    "section": "",
    "text": "This RMarkdown document is part of my Workshop Course in R. The intent is to build Skill in coding in R, and also appreciate R as a way to metaphorically visualize information of various kinds, using predominantly geometric figures and structures.\nAll RMarkdown/Quarto files combine code, text, web-images, and figures developed using code. Everything is text; code chunks are enclosed in fences (```)"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#set-up",
    "href": "content/labs/r-labs/maps/gram-maps.html#set-up",
    "title": "The Grammar of Maps",
    "section": "\n3.1 Set Up",
    "text": "3.1 Set Up\nThe setup code chunk below brings into our coding session R packages that provide specific computational abilities and also datasets which we can use.\nTo reiterate: Packages and datasets are not the same thing !! Packages are (small) collections of programs. Datasets are just….information."
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#using-open-street-map-osm",
    "href": "content/labs/r-labs/maps/gram-maps.html#using-open-street-map-osm",
    "title": "The Grammar of Maps",
    "section": "\n7.1 Using Open Street Map (OSM)",
    "text": "7.1 Using Open Street Map (OSM)\n\nOpenStreetMap (OSM) provides maps of the world mostly created by volunteers. They are completely free to browse and use, with attribution to © OpenStreetMap contributors and adherence to the ODbL license required, and are used by many public and private organisations. OSM data can be downloaded in vector format and used for our own purposes. In this tutorial, we will obtain data from OSM using a query. A query is a request for data from a database. Simple queries can be performed more easily using the osmdata library for R, which automatically constructs the query and imports the data in a convenient format."
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#osm-feature-key-value-pairs",
    "href": "content/labs/r-labs/maps/gram-maps.html#osm-feature-key-value-pairs",
    "title": "The Grammar of Maps",
    "section": "\n7.2 OSM feature key-value pairs",
    "text": "7.2 OSM feature key-value pairs\nOpen Street Map features have attributes in key-value pairs. We can use them to download the specific data we need. These features can easily be explored in the web browser, by using the ‘Query features’ button on OpenStreetMap (OSM):\n\nHead off to OSM Street Map to try this out and to get an intuitive understanding of what OSM key-value pairs are, for different types of map features. Look for places of interest to you (features) and see what key-value pairs attach to those features.\nNOTE: key-value pairs are also referred to as tags.\nUseful key-value pairs / tags include:\n\n\n\n\n\n\nKEY\nVALUEs\n\n\n\nbuilding\nyes (all), house residential, apartments\n\n\nhighway\nresidential, service, track, unclassified, footway, path\n\n\namenity\nparking, parking_space, bench; place_of_worship; restaurant, cafe, fast_food; school, waste_basket, fuel, bank, toilets…\n\n\nshop\nconvenience, supermarket, clothes, hairdresser, car-repair…\n\n\nname\nactual name of the place e.g. Main_Street, McDonald’s, Pizza Hut, Subway\n\n\n\nwaterway\n\n\n\nnatural\n\n\n\nboundary\n\n\n\n\nFor more information see:\nSee OSM Tags for a nice visual description of popular key-value pairs that we can use. See what the highway tag looks like tag : highway"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#downloading-data-from-open-street-map",
    "href": "content/labs/r-labs/maps/gram-maps.html#downloading-data-from-open-street-map",
    "title": "The Grammar of Maps",
    "section": "\n7.3 Downloading Data from Open Street Map",
    "text": "7.3 Downloading Data from Open Street Map\nNow we know the map features we are interested in. We also know what key-value pairs will be used to get this info from OSM. We will get our map data from OSM and then save it avoid repeated downloads. So, please copy/paste and run the following commands in your console.\nDo not run these commands too many times. Re-run this ONLY if you have changed your BOUNDING BOX.\n\n# Eval is set to false here\n# This code is for reference\n# Run these commands ONCE in your Console\n\n# Get all buildings within my bbox\ndat_buildings &lt;-extract_osm_objects (key = \"building\", \n                                     bbox = bbox_2)\n\n# Get all residential roads within my bbox\ndat_roads &lt;- extract_osm_objects (key = 'highway', \n                                     value = c(\"residential\"),\n                                     bbox = bbox_2)\n\n# Get all parks within my bbox\ndat_parks &lt;- extract_osm_objects (key = 'park', \n                                  bbox = bbox_2)\n\n# Get all green areas within my bbox\ndat_greenery &lt;- extract_osm_objects (key = 'landuse', \n                                  value = 'grass', \n                                  bbox = bbox_2)\n\ndat_trees &lt;- extract_osm_objects (key = 'natural', \n                                  value = 'tree', \n                                  bbox = bbox_2)\n\n\n7.3.1 Let us save this data, so we don’t need to download all this again!\nWe will store the downloaded data as .gpkg files on our local hard drives to use when we run this file again later. We will name our stored files as buildings, parks, roads, and trees, each with the .gpkg file extension, e.g. trees.gpkg.\nCheck your local project folder for these files after executing these commands.\n\n# Eval is set to false here\n# This code is for reference\n# Run these commands ONCE in your Console\n\nst_write(dat_buildings, \n         dsn = \"buildings.gpkg\", \n         append = FALSE, \n         quiet = FALSE)\n\nst_write(dat_parks, dsn = \"parks.gpkg\", \n         append = FALSE, quiet = FALSE)\n\nst_write(dat_greenery, dsn = \"greenery.gpkg\", \n         append = FALSE,quiet = FALSE)\n\nst_write(dat_trees, dsn = \"trees.gpkg\", \n         append = FALSE,quiet = FALSE)\n\nst_write(dat_roads, dsn = \"roads.gpkg\", \n         append = FALSE, quiet = FALSE)\n\nAlways work from here to avoid repeated downloads from OSM. Start from the top ONLY if you intend to map new locations and need to modify your Bounding Box.\n\n7.3.2 Reading Back the saved Data\n\nbuildings &lt;- st_read(\"./buildings.gpkg\")\n\nReading layer `buildings' from data source \n  `C:\\Users\\Arvind\\Documents\\R work\\MyWebsites\\my-quarto-website\\content\\labs\\r-labs\\maps\\buildings.gpkg' \n  using driver `GPKG'\nSimple feature collection with 34766 features and 89 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 77.56221 ymin: 12.90906 xmax: 77.60373 ymax: 12.9497\nGeodetic CRS:  WGS 84\n\nparks &lt;- st_read(\"./parks.gpkg\")\n\nReading layer `parks' from data source \n  `C:\\Users\\Arvind\\Documents\\R work\\MyWebsites\\my-quarto-website\\content\\labs\\r-labs\\maps\\parks.gpkg' \n  using driver `GPKG'\nSimple feature collection with 62 features and 25 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 77.56196 ymin: 12.90692 xmax: 77.60389 ymax: 12.95397\nGeodetic CRS:  WGS 84\n\ngreenery &lt;- st_read(\"./greenery.gpkg\")\n\nReading layer `greenery' from data source \n  `C:\\Users\\Arvind\\Documents\\R work\\MyWebsites\\my-quarto-website\\content\\labs\\r-labs\\maps\\greenery.gpkg' \n  using driver `GPKG'\nSimple feature collection with 2 features and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 77.56776 ymin: 12.91751 xmax: 77.57392 ymax: 12.94811\nGeodetic CRS:  WGS 84\n\ntrees &lt;- st_read(\"./trees.gpkg\")\n\nReading layer `trees' from data source \n  `C:\\Users\\Arvind\\Documents\\R work\\MyWebsites\\my-quarto-website\\content\\labs\\r-labs\\maps\\trees.gpkg' \n  using driver `GPKG'\nSimple feature collection with 153 features and 9 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 77.56566 ymin: 12.90806 xmax: 77.60096 ymax: 12.94914\nGeodetic CRS:  WGS 84\n\nroads &lt;- st_read(\"./roads.gpkg\")\n\nReading layer `roads' from data source \n  `C:\\Users\\Arvind\\Documents\\R work\\MyWebsites\\my-quarto-website\\content\\labs\\r-labs\\maps\\roads.gpkg' \n  using driver `GPKG'\nSimple feature collection with 2242 features and 28 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 77.55895 ymin: 12.90635 xmax: 77.60603 ymax: 12.95636\nGeodetic CRS:  WGS 84\n\n\n\n7.3.3 Let’s look at the data\nHow many rows? ( Rows -&gt; Features ) What kind of geom column?\n\n# How many buildings?\nnrow(buildings)\n\n[1] 34766\n\nbuildings$geom\n\nGeometry set for 34766 features \nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 77.56221 ymin: 12.90906 xmax: 77.60373 ymax: 12.9497\nGeodetic CRS:  WGS 84\nFirst 5 geometries:\n\n\nPOLYGON ((77.58405 12.93005, 77.5845 12.93005, ...\n\n\nPOLYGON ((77.57568 12.9199, 77.57592 12.9199, 7...\n\n\nPOLYGON ((77.59592 12.94016, 77.59676 12.94022,...\n\n\nPOLYGON ((77.5937 12.94011, 77.59458 12.94015, ...\n\n\nPOLYGON ((77.59321 12.94042, 77.59321 12.94035,...\n\nclass(buildings$geom)\n\n[1] \"sfc_POLYGON\" \"sfc\"        \n\n\nSo the buildings dataset has 34766 buildings and their geometry is naturally a POLYGON type of geometry column.\n\n7.3.4 Your Turn 1\nDo this check for all the other spatial data, in the code chunk below. What kind of geom column does each dataset have?\n\n\n\n\n7.3.5 What Other Kinds of Data could we get from OSM?\nosm_structures returns a data.frame of OSM structure types, associated key-value pairs and unique suffixes which may be appended to data structures/filenames for storage purposes, and suggested colours.\n\nosmplotr::osm_structures()\n\n\n\n  \n\n\n\nWe can use these key-value pairs to download different types of map data."
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#ggplot-and-geom_sf",
    "href": "content/labs/r-labs/maps/gram-maps.html#ggplot-and-geom_sf",
    "title": "The Grammar of Maps",
    "section": "\n8.1 ggplot and geom_sf()",
    "text": "8.1 ggplot and geom_sf()\nFirst we will plot with ggplot and geom_sf() : recall that our data is stored in 5 files: buildings, parks, roads, trees, and greenery.\n\nggplot() +\n  geom_sf(data = buildings, colour = \"orange\") +    # POLYGONS\n  geom_sf(data = roads, col = \"gray20\") +           # LINES\n  geom_sf(data = parks, col = \"darkseagreen1\") +    # POLYGONS\n  geom_sf(data = greenery, col = \"darkseagreen\") +  # POLYGONS\n  geom_sf(data = trees, col = \"green\")              # POINTS\n\n\n\n\nNote how geom_sf is capable of handling any geometry in the sfc column !!\n\ngeom_sf() is an unusual geom because it will draw different geometric objects depending on what simple features are present in the data: you can get points, lines, or polygons.\n\nSo there, we have our first map!"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#map-using-tmap-package",
    "href": "content/labs/r-labs/maps/gram-maps.html#map-using-tmap-package",
    "title": "The Grammar of Maps",
    "section": "\n8.2 Map using tmap package",
    "text": "8.2 Map using tmap package\nWe can also create a map using a package called tmap. Here we also have the option of making the map interactive. tmap plots are made with code in “groups”: each group starts with a tm_shape() command.\n\n# Group-1\ntm_shape(buildings) +\n  tm_fill(col = \"mediumblue\") +\n\n#Group-2\ntm_shape(roads) +\n  tm_lines(col = \"gold\") +\n\n#Group-3  \ntm_shape(greenery) +\n  tm_polygons(col = \"limegreen\") +\n  \n#Group-4  \ntm_shape(parks) +\n  tm_polygons(col = \"limegreen\") +\n\n#Group-5  \ntm_shape(trees) +\n  tm_dots(col = \"green\")\n\n\n\n\nHow do we make this map interactive? One more line of code !! Add this line in your console and then run the above chunk again\ntmap_mode(\"view\")"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#using-data-from-rnaturalearth",
    "href": "content/labs/r-labs/maps/gram-maps.html#using-data-from-rnaturalearth",
    "title": "The Grammar of Maps",
    "section": "\n9.1 Using data from rnaturalearth\n",
    "text": "9.1 Using data from rnaturalearth\n\nThe rnaturalearth package allows us to download shapes of countries. We can use it to get borders and also internal state/district boundaries.\n\nindia &lt;- \n  ne_states(country =  \"india\", \n            returnclass = \"sf\") # gives a ready sf dataframe !\n\nindia_neighbours &lt;- \n  ne_states(country = (c(\"sri lanka\", \"pakistan\",\n                         \"afghanistan\", \"nepal\",\"bangladesh\", \"bhutan\")\n                       ),\n            returnclass = \"sf\")\n\nLet’s look at the attribute variable columns to colour our graph and to shape our symbols:\n\nnames(india)\n\n [1] \"featurecla\" \"scalerank\"  \"adm1_code\"  \"diss_me\"    \"iso_3166_2\"\n [6] \"wikipedia\"  \"iso_a2\"     \"adm0_sr\"    \"name\"       \"name_alt\"  \n[11] \"name_local\" \"type\"       \"type_en\"    \"code_local\" \"code_hasc\" \n[16] \"note\"       \"hasc_maybe\" \"region\"     \"region_cod\" \"provnum_ne\"\n[21] \"gadm_level\" \"check_me\"   \"datarank\"   \"abbrev\"     \"postal\"    \n[26] \"area_sqkm\"  \"sameascity\" \"labelrank\"  \"name_len\"   \"mapcolor9\" \n[31] \"mapcolor13\" \"fips\"       \"fips_alt\"   \"woe_id\"     \"woe_label\" \n[36] \"woe_name\"   \"latitude\"   \"longitude\"  \"sov_a3\"     \"adm0_a3\"   \n[41] \"adm0_label\" \"admin\"      \"geonunit\"   \"gu_a3\"      \"gn_id\"     \n[46] \"gn_name\"    \"gns_id\"     \"gns_name\"   \"gn_level\"   \"gn_region\" \n[51] \"gn_a1_code\" \"region_sub\" \"sub_code\"   \"gns_level\"  \"gns_lang\"  \n[56] \"gns_adm1\"   \"gns_region\" \"min_label\"  \"max_label\"  \"min_zoom\"  \n[61] \"wikidataid\" \"name_ar\"    \"name_bn\"    \"name_de\"    \"name_en\"   \n[66] \"name_es\"    \"name_fr\"    \"name_el\"    \"name_hi\"    \"name_hu\"   \n[71] \"name_id\"    \"name_it\"    \"name_ja\"    \"name_ko\"    \"name_nl\"   \n[76] \"name_pl\"    \"name_pt\"    \"name_ru\"    \"name_sv\"    \"name_tr\"   \n[81] \"name_vi\"    \"name_zh\"    \"ne_id\"      \"geometry\"  \n\nnames(india_neighbours)\n\n [1] \"featurecla\" \"scalerank\"  \"adm1_code\"  \"diss_me\"    \"iso_3166_2\"\n [6] \"wikipedia\"  \"iso_a2\"     \"adm0_sr\"    \"name\"       \"name_alt\"  \n[11] \"name_local\" \"type\"       \"type_en\"    \"code_local\" \"code_hasc\" \n[16] \"note\"       \"hasc_maybe\" \"region\"     \"region_cod\" \"provnum_ne\"\n[21] \"gadm_level\" \"check_me\"   \"datarank\"   \"abbrev\"     \"postal\"    \n[26] \"area_sqkm\"  \"sameascity\" \"labelrank\"  \"name_len\"   \"mapcolor9\" \n[31] \"mapcolor13\" \"fips\"       \"fips_alt\"   \"woe_id\"     \"woe_label\" \n[36] \"woe_name\"   \"latitude\"   \"longitude\"  \"sov_a3\"     \"adm0_a3\"   \n[41] \"adm0_label\" \"admin\"      \"geonunit\"   \"gu_a3\"      \"gn_id\"     \n[46] \"gn_name\"    \"gns_id\"     \"gns_name\"   \"gn_level\"   \"gn_region\" \n[51] \"gn_a1_code\" \"region_sub\" \"sub_code\"   \"gns_level\"  \"gns_lang\"  \n[56] \"gns_adm1\"   \"gns_region\" \"min_label\"  \"max_label\"  \"min_zoom\"  \n[61] \"wikidataid\" \"name_ar\"    \"name_bn\"    \"name_de\"    \"name_en\"   \n[66] \"name_es\"    \"name_fr\"    \"name_el\"    \"name_hi\"    \"name_hu\"   \n[71] \"name_id\"    \"name_it\"    \"name_ja\"    \"name_ko\"    \"name_nl\"   \n[76] \"name_pl\"    \"name_pt\"    \"name_ru\"    \"name_sv\"    \"name_tr\"   \n[81] \"name_vi\"    \"name_zh\"    \"ne_id\"      \"geometry\"  \n\n# Look only at attributes\nindia %&gt;% st_drop_geometry() %&gt;% head()\n\n\n\n  \n\n\nindia_neighbours %&gt;% st_drop_geometry() %&gt;% head()\n\n\n\n  \n\n\n\nIn the india data frame:\n- Column iso_a2 contains the country name.\n- Column name contains the name of the state\nIn the india_neighbours data frame:\n- Column gu_a3 contains the country abbreviation\n- Column name contains the name of the state\n- Column iso_3166_2 contains the abbreviation of the state within each neighbouring country.\n\n9.1.1 Map 1\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\n# Plot India\n  tm_shape(india) +\n  tm_polygons(\"name\", # Colour by States in India\n              legend.show = FALSE) +\n  \n# Plot Neighbours\n  tm_shape(india_neighbours) +\n  tm_fill(col = \"gu_a3\") +  # Colour by Country Name\n  \n# Plot the cities in India alone\n  tm_shape(metro %&gt;% dplyr::filter(iso_a3 == \"IND\")) +\n    \n  tm_dots(size = \"pop2020\",legend.size.show = FALSE) +\n    # size by population in 2020\n    \n  tm_layout(legend.show = FALSE) +\n  tm_credits(\"Geographical Boundaries are not accurate\",\n             size = 0.5,\n             position = \"right\") +\n  tm_compass(position = c(\"right\", \"top\")) +\n  tm_scale_bar(position = \"left\") +\n  tmap_style(style = \"classic\") \n\nold-style crs object detected; please recreate object with a recent sf::st_crs()\n\n\ntmap style set to \"classic\"\n\n\nother available styles are: \"white\", \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"watercolor\" \n\n\nCredits not supported in view mode.\n\n\nCompass not supported in view mode.\n\n\nWarning: Number of levels of the variable \"name\" is 35, which is larger than\nmax.categories (which is 30), so levels are combined. Set\ntmap_options(max.categories = 35) in the layer function to show all levels.\n\n\n\n\n\n\n#Try other map styles\n#cobalt #gray #white #watercolor #beaver #classic #watercolor #albatross #bw #col_blind"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#your-turn-2",
    "href": "content/labs/r-labs/maps/gram-maps.html#your-turn-2",
    "title": "The Grammar of Maps",
    "section": "\n9.2 Your Turn 2",
    "text": "9.2 Your Turn 2\nCan you try to download a map area of your home town and plot it as we have above?"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#adding-my-favourite-restaurants-to-the-map",
    "href": "content/labs/r-labs/maps/gram-maps.html#adding-my-favourite-restaurants-to-the-map",
    "title": "The Grammar of Maps",
    "section": "\n9.3 Adding my favourite Restaurants to the map",
    "text": "9.3 Adding my favourite Restaurants to the map\nIs it time to order on Swiggy…\nLet us adding interesting places to our map: say based on your favourite restaurants etc. We need restaurant data: lat/long + name + maybe type of restaurant. This can be manually created ( like all of OSMdata ) or if it is already there we can download using key-value pairs in our OSM data query.\nRestaurants can be downloaded using key= \"amenity\", value = \"restaurant\". Since we want JUST their location, and not the restaurant BUILDINGs, we say return_type = \"points\".\nThere are also other tags to explore!Searching for McDonalds for instance…( key = \"name\", value = \"McDonalds\")\n\n# Again, run these commands in your Console\ndat_R &lt;- extract_osm_objects(bbox = bbox_2, \n                             key = \"amenity\", \n                             value = \"restaurant\", \n                             return_type = \"point\") #&lt;&lt;\n\n# Save the data for future use\nwrite_sf(dat_R, dsn = \"restaurants.gpkg\",append = FALSE, quiet = FALSE)\n\nNote the return_type parameter: we want the location and not the building in which the restaurant is!!"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#reading-the-saved-restaurant-data",
    "href": "content/labs/r-labs/maps/gram-maps.html#reading-the-saved-restaurant-data",
    "title": "The Grammar of Maps",
    "section": "\n9.4 Reading the saved Restaurant Data",
    "text": "9.4 Reading the saved Restaurant Data\n\nrestaurants &lt;- st_read(\"./restaurants.gpkg\")\n\nReading layer `restaurants' from data source \n  `C:\\Users\\Arvind\\Documents\\R work\\MyWebsites\\my-quarto-website\\content\\labs\\r-labs\\maps\\restaurants.gpkg' \n  using driver `GPKG'\nSimple feature collection with 203 features and 33 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 77.56373 ymin: 12.9105 xmax: 77.60104 ymax: 12.94917\nGeodetic CRS:  WGS 84\n\n\nHow many restaurants have we got?\n\nrestaurants %&gt;% nrow()\n\n[1] 203\n\n\nSo the restaurants dataset has 203 restaurants and their geometry is naturally a POINT type of geometry column.\nThese are the names of columns in the Restaurant Data: Note the cuisine column.\n\nnames(restaurants)\n\n [1] \"osm_id\"             \"name\"               \"addr.city\"         \n [4] \"addr.housename\"     \"addr.housenumber\"   \"addr.postcode\"     \n [7] \"addr.street\"        \"alt_name\"           \"amenity\"           \n[10] \"building\"           \"capacity\"           \"cuisine\"           \n[13] \"delivery\"           \"description\"        \"diet.vegetarian\"   \n[16] \"email\"              \"food\"               \"internet_access\"   \n[19] \"level\"              \"name.en\"            \"name.kn\"           \n[22] \"note\"               \"opening_hours\"      \"operator\"          \n[25] \"phone\"              \"smoking\"            \"source\"            \n[28] \"takeaway\"           \"toilets.wheelchair\" \"website\"           \n[31] \"wheelchair\"         \"wikidata\"           \"wikipedia\"         \n[34] \"geom\"              \n\n\nSo let us plot the restaurants as POINTs using the restaurants data we have downloaded. The cuisine attribute looks interesting; let us colour the POINT based on the cuisine offered at that restaurant.\nSo Let’s look therefore at the cuisine column!\n\n# ( I want pizza...)\nrestaurants$cuisine %&gt;% unique()\n\n [1] NA                                       \n [2] \"indian\"                                 \n [3] \"italian\"                                \n [4] \"regional\"                               \n [5] \"pizza\"                                  \n [6] \"ice_cream\"                              \n [7] \"chinese\"                                \n [8] \"South_Indian\"                           \n [9] \"Multi-cuisne\"                           \n[10] \"South_India\"                            \n[11] \"chicken;regional\"                       \n[12] \"arab\"                                   \n[13] \"indian;seafood;fine_dining\"             \n[14] \"fast_food\"                              \n[15] \"kebab;grill\"                            \n[16] \"chicken\"                                \n[17] \"chinese;sandwich;tea;indian;coffee_shop\"\n[18] \"indian,_japanese\"                       \n[19] \"indian;regional\"                        \n\n\nBig mess…many NAs, some double entries, separated by commas and semicolons….\n\n\nThe cuisine attribute:\n\n\nNote: The cuisine variable has more than one entry for a given restaurant. We use tidyr::separate() to make multiple columns out of the cuisine column and retain the first one only. Since the entries are badly entered using both “;” and “,” we need to do this twice ;-() Bad Data entry!!\n\n\nLet’s get one cuisine entry per restaurant, and drop off the ones that do not mention a cuisine at all:\n\nrestaurants &lt;- restaurants %&gt;% \n  drop_na(cuisine) %&gt;% # Knock off nondescript restaurants\n  \n  # Some have more than one classification ;-()\n  # Separated by semicolon or comma, so....\n  separate_wider_delim(cols = cuisine, \n                       names = c(\"cuisine\", NA, NA), \n                       delim = \";\", \n                       too_few = \"align_start\",\n                       too_many = \"drop\") %&gt;% \n  separate_wider_delim(cols = cuisine, \n                       names = c(\"cuisine\", NA, NA), \n                       delim = \",\",\n                       too_few = \"align_start\",\n                       too_many = \"drop\")\n\n# Finally good food?\nrestaurants$cuisine\n\n  [1] \"indian\"       \"italian\"      \"indian\"       \"indian\"       \"regional\"    \n  [6] \"indian\"       \"pizza\"        \"regional\"     \"ice_cream\"    \"ice_cream\"   \n [11] \"indian\"       \"chinese\"      \"chinese\"      \"indian\"       \"italian\"     \n [16] \"regional\"     \"indian\"       \"indian\"       \"italian\"      \"regional\"    \n [21] \"indian\"       \"chinese\"      \"indian\"       \"indian\"       \"indian\"      \n [26] \"indian\"       \"indian\"       \"ice_cream\"    \"pizza\"        \"South_Indian\"\n [31] \"regional\"     \"regional\"     \"Multi-cuisne\" \"South_India\"  \"indian\"      \n [36] \"indian\"       \"chicken\"      \"arab\"         \"indian\"       \"regional\"    \n [41] \"regional\"     \"regional\"     \"regional\"     \"regional\"     \"indian\"      \n [46] \"indian\"       \"indian\"       \"indian\"       \"regional\"     \"regional\"    \n [51] \"italian\"      \"regional\"     \"regional\"     \"regional\"     \"regional\"    \n [56] \"regional\"     \"regional\"     \"regional\"     \"regional\"     \"regional\"    \n [61] \"regional\"     \"regional\"     \"regional\"     \"fast_food\"    \"indian\"      \n [66] \"regional\"     \"italian\"      \"regional\"     \"regional\"     \"regional\"    \n [71] \"regional\"     \"regional\"     \"regional\"     \"italian\"      \"fast_food\"   \n [76] \"regional\"     \"fast_food\"    \"regional\"     \"chinese\"      \"regional\"    \n [81] \"regional\"     \"regional\"     \"regional\"     \"regional\"     \"regional\"    \n [86] \"regional\"     \"regional\"     \"regional\"     \"regional\"     \"regional\"    \n [91] \"regional\"     \"regional\"     \"regional\"     \"regional\"     \"regional\"    \n [96] \"regional\"     \"regional\"     \"regional\"     \"regional\"     \"kebab\"       \n[101] \"chicken\"      \"chinese\"      \"indian\"       \"italian\"      \"indian\"      \n[106] \"indian\"       \"indian\"      \n\n\nLooks clean! Each entry is only ONE and not multiple any more. Now let’s plot the Restaurants as POINTs:\n\n# http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf\n# \nggplot() +\n  geom_sf(data = buildings, colour = \"burlywood1\") +\n  geom_sf(data = roads, colour = \"gray80\") +\n  geom_sf(\n    data = restaurants %&gt;% drop_na(cuisine),\n    aes(fill = cuisine),\n    colour = \"black\",\n    shape = 21,\n    size = 3\n  ) +\n  theme(legend.position = \"right\") +\n  labs(title = \"Restaurants in South Central Bangalore\",\n       caption = \"Based on osmdata\")\n\n\n\n\nWe could have done a (much!) better job, by combining cuisines into simpler and fewer categories, ( South_India and South_Indian ), but that is for another day!!\nBy now we know that we can use geom_sf() multiple number of times with different datasets to create layered maps in R."
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#globejs-usage",
    "href": "content/labs/r-labs/maps/gram-maps.html#globejs-usage",
    "title": "The Grammar of Maps",
    "section": "\n11.1 globejs usage",
    "text": "11.1 globejs usage\nThe globejs command from the package threejs allows one to plot points, arcs and images on a globe in 3D. The globe can be rotated and and zoomed. Great Circles and historical routes are a good idea for this perhaps.\nRefer to this page for more ideas http://bwlewis.github.io/rthreejs/globejs.html\nWe will generate some random locations and plot them on the 3D globe.\n\n# Random Lats and Longs\nlat &lt;- rpois(10, 60) + rnorm(10, 80)\nlong &lt;- rpois(10, 60) + rnorm(10, 10)\n\n# Random \"Spike\" heights for each location. Population? Tourists? GDP?\nvalue &lt;- rpois(10, lambda = 80)\n \nglobejs(lat = lat, long = long)\n\n\n\n\n\nAs seen, “spikes” are created at the random lat-lon locations. We can control the height/width/colour of the spikes, as well as the initial view of the globe itself: zoom, location and so on\n\nglobejs(\n  lat = lat,\n  long = long,\n  \n  # random heights of the Spikes (!!) at lat-long combo\n  value = value,\n  color = \"red\",\n  # Zoom factor, default is 35\n  fov = 50\n)\n\n\n\n\n\n\nglobejs(\n  lat = lat,\n  long = long,\n  value = value,\n  color = \"red\",\n  pointsize = 4, # width of the columns\n  # Zoom position\n  fov = 35,\n  # initial position of the globe\n  rotationlat = 0.6, #  in RADIANS !!! Good Heavens!!\n  rotationlong = 0.2 #  in RADIANS !!! Good Heavens!!\n)\n\n\n\n\n\n\nglobejs(\n  lat = lat,\n  long = long,\n  value = value,\n  color = \"red\",\n  pointsize = 4,\n  fov = 35,\n  rotationlat = 0.6,\n  rotationlong = 0.2,\n  lightcolor = \"#aaeeff\",\n  emissive = \"#0000ee\",\n  bodycolor = \"#ffffff\",\n  bg = \"grey\"\n)"
  },
  {
    "objectID": "content/labs/r-labs/maps/leaflet.html",
    "href": "content/labs/r-labs/maps/leaflet.html",
    "title": "Playing with Leaflet",
    "section": "",
    "text": "This Tutorial works through the ideas at Leaflet\n\nLeaflet is a JavaScript library for creating dynamic maps that support panning and zooming along with various annotations like markers, polygons, and popups.\n\nIn this tutorial we will work only with vector data. In a second part, we will work with raster data in leaflet.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.0\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(leaflet)\nlibrary(maps)\n\n\nAttaching package: 'maps'\n\nThe following object is masked from 'package:purrr':\n\n    map\n\nlibrary(sp)\nlibrary(sf)\n\nLinking to GEOS 3.9.3, GDAL 3.5.2, PROJ 8.2.1; sf_use_s2() is TRUE\n\n# Data\nlibrary(osmdata) # Import OSM Vector Data into R\n\nData (c) OpenStreetMap contributors, ODbL 1.0. https://www.openstreetmap.org/copyright\n\nlibrary(osmplotr) # Creating maps with OSM data in R\n\nData (c) OpenStreetMap contributors, ODbL 1.0. http://www.openstreetmap.org/copyright\n\n# library(OpenStreetMap) # Raster Data"
  },
  {
    "objectID": "content/labs/r-labs/maps/leaflet.html#add-shapes-to-a-map",
    "href": "content/labs/r-labs/maps/leaflet.html#add-shapes-to-a-map",
    "title": "Playing with Leaflet",
    "section": "\n2.1 Add Shapes to a Map",
    "text": "2.1 Add Shapes to a Map\nleaflet offers several commands to add points, markers, icons, lines, polylines and polygons to a map. Let us examine a few of these.\n\n2.1.1 Add Markers with popups\n\nm %&gt;% addMarkers(lng = 77.580643, lat = 12.972442, \n                 popup = \"The birthplace of Rvind\")\n\n\n\n\n# Click on the Marker for the popup to appear\n\nThis uses the default pin shape as the Marker.\n\n2.1.2 Adding Popups to a Map\nPopups are small boxes containing arbitrary HTML, that point to a specific point on the map. Use the addPopups() function to add standalone popup to the map.\n\nm %&gt;%\n  addPopups(\n    lng = 77.580643,\n    lat = 12.972442,\n    popup = paste(\n      \"The birthplace of Rvind\",\n      \"&lt;br&gt;\",\n      \"Website: https://the-foundation-series.netlify.app\",\n      \"&lt;br&gt;\"\n    ),\n    \n    # Ensuring we cannot close the popup, else we will not be able to find where it is, since there is no Marker\n    options = popupOptions(closeButton = FALSE)\n  )\n\n\n\n\n\nPopups are usually added to icons, Markers and other shapes can show up when these are clicked.\n\n2.1.3 Adding Labels to a Map\nLabels are messages attached to all shapes, using the argument label wherever it is available.\nLabels are static, and Popups are usually visible on mouse click. Hence a Marker can have both a label and a popup. For example, the function addPopup() offers only a popup argument, whereas the function addMarkers() offers both a popup and a label argument.\nIt is also possible to create labels standalone using addLabelOnlyMarkers() where we can show only text and no Markers.\n\nm %&gt;%\n  addMarkers(\n    lng = 77.580643,\n    lat = 12.972442,\n    \n    # Here is the Label defn.\n    label = \"The birthplace of Rvind\",\n    labelOptions = labelOptions(noHide = TRUE, # Label always visible\n                                textOnly = F, \n                                textsize = 20),\n    \n    # And here is the popup defn.\n    popup = \"This is the Popup Text\"\n  )\n\n\n\n\n\n\n2.1.4 Adding Circles and CircleMarkers on a Map\nWe can add shapes on to a map to depict areas or locations of interest. NOTE: the radius argument works differently in addCircles() and addCircleMarkers().\n\n# Some Cities in the US and their location\nmd_cities &lt;- tibble(\n  name = c(\"Baltimore\",\"Frederick\",\"Rockville\",\"Gaithersburg\",\"Bowie\",\"Hagerstown\",\"Annapolis\",\"College Park\",\"Salisbury\",\"Laurel\"),\n  pop = c(619493,66169,62334,61045,55232,39890,38880,30587,30484,25346),\n  lat = c(39.2920592,39.4143921,39.0840,39.1434,39.0068,39.6418,38.9784,38.9897,38.3607,39.0993), \n  lng = c(-76.6077852,-77.4204875,-77.1528,-77.2014,-76.7791,-77.7200,-76.4922,-76.9378,-75.5994,-76.8483)\n)\n\n\nmd_cities %&gt;%\n  leaflet() %&gt;%\n  addTiles() %&gt;%\n  \n  # CircleMarkers, in blue\n  # radius scales the Marker. Units are in Pixels!!\n  # Here, radius is made proportional to `pop` number\n  addCircleMarkers(radius = ~ pop/1000, # Pixels!!\n                   color = \"blue\",\n                   stroke = FALSE, # no border for the Markers\n                   opacity = 0.8) %&gt;% \n  \n  \n  # Circles, in red\n  addCircles(\n    radius = 5000, # Meters !!!\n    stroke = TRUE,\n    color = \"yellow\", # Stroke Colour\n    weight = 3, # Stroke Weight\n    fill = TRUE,\n    fillColor = \"red\",\n\n  )\n\nAssuming \"lng\" and \"lat\" are longitude and latitude, respectively\nAssuming \"lng\" and \"lat\" are longitude and latitude, respectively\n\n\n\n\n\n\nThe shapes need not be of fixed size or colour; their attributes can be made to correspond to other attribute variables in the geospatial data, as we did with radius in the addCircleMarkers() function above.\n\n2.1.5 Adding Rectangles to a Map\n\n## Adding Rectangles\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  setView(lng = 77.580643, lat = 12.972442, zoom = 6) %&gt;% \n  addRectangles(lat1 = 10.3858, lng1 = 75.0595, \n                lat2 = 12.8890, lng2 = 77.9625)\n\n\n\n\n\n\n2.1.6 Add Polygons to a Map\n\n## Adding Polygons\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  setView(lng = 77.580643, lat = 12.972442, zoom = 6) %&gt;% \n  \n  # arbitrary vector data for lat and lng\n   addPolygons(lng = c(73.5, 75.9, 76.1, 77.23, 79.8),\n               lat =c(10.12, 11.04, 11.87, 12.04, 10.7))\n\n\n\n\n\n\n2.1.7 Add PolyLines to a Map\nThis can be useful say for manually marking a route on a map, with waypoints.\n\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  setView(lng = 77.580643, lat = 12.972442, zoom = 6) %&gt;% \n  \n  # arbitrary vector data for lat and lng\n  # If start and end points are the same, it looks like Polygon\n  # Without the fill\n   addPolylines(lng = c(73.5, 75.9, 76.1, 77.23, 79.8),\n               lat =c(10.12, 11.04, 11.87, 12.04, 10.7)) %&gt;% \n  \n  # Add Waypoint Icons\n  addMarkers(lng = c(73.5, 75.9, 76.1, 77.23, 79.8),\n               lat =c(10.12, 11.04, 11.87, 12.04, 10.7))\n\n\n\n\n\nAs seen, we have created Markers, Labels, Polygons, and PolyLines using fixed.i.e. literal text and numbers. In the following we will also see how external geospatial data columns can be used instead of these literals.\nNOTE: The mapedit package https://r-spatial.org//r/2017/01/30/mapedit_intro.html can also be used to interactively add shapes onto a map and save as an geo-spatial object."
  },
  {
    "objectID": "content/labs/r-labs/maps/leaflet.html#point-data-sources-for-leaflet",
    "href": "content/labs/r-labs/maps/leaflet.html#point-data-sources-for-leaflet",
    "title": "Playing with Leaflet",
    "section": "\n3.1 Point Data Sources for leaflet\n",
    "text": "3.1 Point Data Sources for leaflet\n\nPoint data for markers can come from a variety of sources:\n\n\nSpatialPoints or SpatialPointsDataFrame objects (from the sp package)\n\n\nPOINT, sfc_POINT, and sf objects (from the sf package); only X and Y dimensions will be considered\n\nTwo-column numeric matrices (first column is longitude, second is latitude)\n\n\nData frame/tibble with latitude and longitude columns. You can explicitly tell the marker function which columns contain the coordinate data (e.g. addMarkers(lng = ~Longitude, lat = ~Latitude)), or let the function look for columns named lat/latitude and lon/lng/long/longitude (case insensitive).\n\nSimply provide numeric vectors as lng and lat arguments, which we have covered already in the preceding sections.\n\nNote that MULTIPOINT objects from sf are not supported at this time.\nWe will not consider the use of sp related data structures for plotting POINTs in leaflet since sp is being phased out in favour of the more modern package sf.\n\n3.1.1 Points using simple Data Frames\nLet us read in the data set from data.world that gives us POINT locations of all airports in India in a data frame / tibble. The dataset is available at https://query.data.world/s/ahtyvnm2ybylf65syp4rsb5tulxe6a. You can either download it, save a copy, and read it in as usual, or use the URL itself to read it in from the web. In the latter case, you will need the package data.world and also need to register your credentials for that page with RStudio. The (simple!) instructions are available here at data.world.\n\n#library(devtools)\n#devtools::install_github(\"datadotworld/data.world-r\", build_vignettes = TRUE)\n\nlibrary(data.world)\n\nLoading required package: dwapi\n\n\n\nAttaching package: 'dwapi'\n\n\nThe following object is masked from 'package:dplyr':\n\n    sql\n\nindia_airports &lt;-\n  read_csv(\"https://query.data.world/s/ahtyvnm2ybylf65syp4rsb5tulxe6a\") %&gt;% \n  slice(-1) %&gt;% # Drop the first row which contains labels\n  dplyr::mutate(\n    id = as.integer(id),\n    latitude_deg = as.numeric(latitude_deg),\n    longitude_deg = as.numeric(longitude_deg),\n    elevation_ft = as.integer(elevation_ft)\n  ) %&gt;% \n  rename(\"lon\" = longitude_deg, \"lat\" = latitude_deg) %&gt;% \n  # Remove four locations which seem to be in the African Atlantic\n  filter(!id %in% c(330834, 330867, 325010, 331083))\n\nRows: 345 Columns: 20\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (20): id, ident, type, name, latitude_deg, longitude_deg, elevation_ft, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nindia_airports %&gt;% head()\n\n\n\n  \n\n\n\nLet us plot this in leaflet, using an ESRI National Geographic style map instead of the OSM Base Map. We will also place small circle markers for each airport.\n\nleaflet(data = india_airports) %&gt;% \n  setView(lat = 18, lng = 77, zoom = 4) %&gt;% \n  \n  # Add NatGeo style base map\n  addProviderTiles(providers$Esri.NatGeoWorldMap) %&gt;% # ESRI Basemap\n  \n  # Add Markers for each airport\n  addCircleMarkers(lng = ~lon, lat = ~lat,\n                   # Optional, variables stated for clarity\n                   # leaflet can automatically detect lon-lat columns\n                   # if they are appropriately named in the data\n                   # longitude/lon/lng\n                   # latitude/lat\n                   radius = 2, # Pixels\n                   color = \"red\",\n                   opacity = 1)\n\n\n\n\n\nWe can also change the icon for each airport. Let us try one of theseveral icon families that we can use with leaflet : glyphicons, ionicons, and fontawesome icons.\n\n# Define popup message for each airport\n# Based on data in india_airports\npopup &lt;- paste(\n  \"&lt;strong&gt;\",\n  india_airports$name,\n  \"&lt;/strong&gt;&lt;br&gt;\",\n  india_airports$iata_code,\n  \"&lt;br&gt;\",\n  india_airports$municipality,\n  \"&lt;br&gt;\",\n  \"Elevation(feet)\",\n  india_airports$elevation_ft,\n  \"&lt;br&gt;\",\n  india_airports$wikipedia_link,\n  \"&lt;br&gt;\"\n)\n\niata_icon &lt;- makeIcon(\n  \"iata-logo-transp.png\", # Downloaded from www.iata.org\n  iconWidth = 24,\n  iconHeight = 24,\n  iconAnchorX = 0,\n  iconAnchorY = 0\n)\n\n# Create the Leaflet map\nleaflet(data = india_airports) %&gt;%\n  setView(lat = 18, lng = 77, zoom = 4) %&gt;%\n  addProviderTiles(providers$Esri.NatGeoWorldMap) %&gt;%\n  addMarkers(\n    icon = iata_icon,\n    popup = popup\n  )\n\nAssuming \"lon\" and \"lat\" are longitude and latitude, respectively\n\n\n\n\n\n\nThere are other icons we can use to mark the POINTs. leaflet allows the use of ionicons, glyphicons, and FontAwesomeIcons\nIt is possible to create a list of icons, so that different Markers can have different icons. Let us try to map the MNCs in the ITPL area of Bangalore: we use the ideas in Using Leaflet Markers @JLA-Data.net\n\n# Make a dataframe of addresses of Companies we wan to plot in ITPL\ncompanies_itpl &lt;-\n  data.frame(\n    ticker = c(\n      \"MBRDI\",\n      \"DTICI\",\n      \"IBM\",\n      \"Exxon\",\n      \"Mindtree\",\n      \"FIS Global\",\n      \"Sasken\",\n      \"LTI\"),\n    lat = c(\n      12.986178620989264,\n      12.984160906190121,\n      12.983659088566357,\n      12.985112265986636,\n      12.983794997606187,\n      12.980658616215155,\n      12.982080447350246,\n      12.981338168875348),\n    lon = c(\n      77.7270652183105,\n      77.72808445774321,\n      77.73103488768001,\n      77.72935046040699,\n      77.7227844126931,\n      77.72685064158782,\n      77.72545589289041,\n      77.72287024338216)\n  ) %&gt;% sf::st_as_sf(coords = c(\"lon\", \"lat\"), crs = 4326)\n \n# Vanilla leaflet map\nleaflet(companies_itpl) %&gt;% \n  addTiles() %&gt;% \n  addMarkers()\n\n\n\n\n\nLet us make a list of logos of the Companies and use them as markers!\n\n# a named list of rescaled icons with links to images\nfavicons &lt;- iconList(\n  \"MBRDI\" = makeIcon(\n    iconUrl = \"https://www.mercedes-benz.com/etc/designs/brandhub/frontend/static-assets/header/logo.svg\", \n    iconWidth = 25,\n    iconHeight = 25\n  ),\n  \"DTICI\" = makeIcon(\n    iconUrl = \"https://media-exp1.licdn.com/dms/image/C4D0BAQGzOep26lC03w/company-logo_200_200/0/1638298367374?e=2147483647&v=beta&t=mPyF4gvNhNFvd-tedbqNzJofq4q9qcw6A9z9jQeLAwc\",\n    iconWidth = 45,\n    iconHeight = 45\n  ),\n  \"IBM\" = makeIcon(\n    iconUrl = \"https://www.ibm.com/favicon.ico\",\n    iconWidth = 25,\n    iconHeight = 25\n  ),\n  \"Exxon\" = makeIcon(\n    iconUrl = \"https://corporate.exxonmobil.com/-/media/Global/Icons/logos/ExxonMobilLogoColor2x.png\",\n    iconWidth = 45,\n    iconHeight = 25\n  ),\n  \"Mindtree\" = makeIcon(\n    iconUrl = \"https://www.mindtree.com/themes/custom/mindtree_theme/mindtree-lnt-logo-png.png\",\n    iconWidth = 75,\n    iconHeight = 25\n  ),\n  \"FIS Global\" = makeIcon(\n    iconUrl = \"https://1000logos.net/wp-content/uploads/2021/09/FIS-Logo-768x432.png\",\n    iconWidth = 25,\n    iconHeight = 25\n  ),\n  \"Sasken\" = makeIcon(\n    iconUrl = \"https://www.sasken.com/sites/all/themes/sasken_website/logo.png\",\n    iconWidth = 35,\n    iconHeight = 35,\n  ),\n  \"LTI\" = makeIcon(\n    iconUrl = \"https://www.lntinfotech.com/wp-content/uploads/2021/09/LTI-logo.svg\",\n    iconWidth = 25,\n    iconHeight = 25\n  )\n)\n\n\n# Create the Leaflet map\n\nleaflet(companies_itpl) %&gt;% \n  addMarkers(icon = ~ favicons[ticker], # lookup based on ticker\n             label = ~ companies_itpl$ticker,\n             labelOptions = labelOptions(noHide = F,offset = c(15,-25))) %&gt;%\n  addProviderTiles(\"CartoDB.Positron\")\n\n\n\n\n\n\n3.1.2 Points using sf objects\nWe will use data from an sf data object. This differs from the earlier situation where we had a simple data frame with lon and lat columns. In sf, the lon and lat info is embedded in the geometry column of the sf data frame.\nThe tmap package has a data set of all World metro cities, titled metro. We will plot these on the map and also scale the markers in proportion to one of the feature attributes, pop2030. The popup will be the name of the metro city. We will also use the CartoDB.Positron base map.\nNote that the metro data set has a POINT geometry, as needed!\n\ndata(metro, package = \"tmap\")\nmetro\n\n\n\n  \n\n\nleaflet(data = metro) %&gt;% \n  setView(lat = 18, lng = 77, zoom = 4) %&gt;% \n  \n  # Add CartoDB.Positron\n  addProviderTiles(providers$CartoDB.Positron) %&gt;% # CartoDB Basemap\n  \n  # Add Markers for each airport\n  addCircleMarkers(radius = ~ sqrt(pop2030)/350,\n                   color = \"red\",\n                   popup = paste(\"Name: \", metro$name, \"&lt;br&gt;\",\n                                  \"Population 2030: \", metro$pop2030))\n\n\n\n\n\nWe can also try downloading an sf data frame with POINT geometry from say OSM data&lt;https://osm. Let us get hold of restaurants data in Malleswaram, Bangalore from OSM data:\n\nbbox&lt;- osmdata::getbb(\"Malleswaram, Bengaluru\")\nbbox\n\n       min      max\nx 77.55033 77.59033\ny 12.98274 13.02274\n\nlocations &lt;- osmplotr::extract_osm_objects(\n  bbox = bbox,\n  key = \"amenity\",\n  value = \"restaurant\",\n  return_type = \"point\") \n\nIssuing query to Overpass API ...\n\n\nAnnounced endpoint: none\n\n\nQuery complete!\n\n\nconverting OSM data to sf format\n\nlocations &lt;- locations %&gt;% \n  dplyr::filter(cuisine == \"indian\")\nlocations %&gt;% head()\n\n\n\n  \n\n\n# Fontawesome icons seem to work in `leaflet` only up to FontAwesome V4.7.0.\n# The Fontawesome V4.7.0 Cheatsheet is here: &lt;https://fontawesome.com/v4/cheatsheet/&gt;\n\n\nleaflet(data = locations, options = leafletOptions(minZoom = 12)) %&gt;% \n  \n  addProviderTiles(providers$CartoDB.Voyager) %&gt;% \n  \n  # Regular `leaflet` code\n  addAwesomeMarkers(icon = awesomeIcons(icon = \"fa-coffee\", \n                                        library = \"fa\",\n                                        markerColor = \"blue\",\n                                        iconColor = \"black\",\n                                        iconRotate = TRUE),\n                     popup = paste(\"Name: \", locations$name,\"&lt;br&gt;\",\n                           \"Food: \", locations$cuisine)) \n\n\n\n\n\nFontawesome Workaround\nFor more later versions of Fontawesome, here below is a workaround from https://github.com/rstudio/leaflet/issues/691. Despite this some fontawesome icons simply do not seem to show up. ;-()\n\nlibrary(fontawesome)\ncoffee &lt;- makeAwesomeIcon(\n  text = fa(\"mug-hot\"), # mug-hot was introduced in fa version 5\n  iconColor = \"black\",\n  markerColor = \"blue\",\n  library = \"fa\"\n)\n\n\nleaflet(data = locations) %&gt;% \n  addProviderTiles(providers$CartoDB.Voyager) %&gt;% \n  \n  # Workaround code\n\n  addAwesomeMarkers(icon = coffee,\n             popup = paste(\"Name: \", locations$name,\"&lt;br&gt;\",\n                           \"Food: \", locations$cuisine, \"&lt;br&gt;\"))\n\n\n\n\n\nNote that leaflet automatically detects the lon/lat columns from within the POINT geometry column of the sf data frame.\n\n3.1.3 Points using Two-Column Matrices\nWe can now quickly try providing lon and lat info in a two column matrix.This can be useful to plot a bunch of points recorded on a mobile phone app.\n\nmysore5 &lt;- matrix(c(runif(5, 76.652985-0.01, 76.652985+0.01),\n                 runif(5, 12.311827-0.01, 12.311827+0.01)),\n                 nrow = 5)\nmysore5\n\n         [,1]     [,2]\n[1,] 76.65387 12.31681\n[2,] 76.66260 12.31176\n[3,] 76.65750 12.31252\n[4,] 76.65581 12.30970\n[5,] 76.64777 12.31510\n\nleaflet(data = mysore5) %&gt;% \n  addProviderTiles(providers$OpenStreetMap) %&gt;% \n  \n# Pick an icon from &lt;https://www.w3schools.com/bootstrap/bootstrap_ref_comp_glyphs.asp&gt;\n  addAwesomeMarkers(icon = awesomeIcons(\n  icon = 'music',\n  iconColor = 'black',\n  library = 'glyphicon'),\n  popup = \"Carnatic Music !!\")"
  },
  {
    "objectID": "content/labs/r-labs/maps/leaflet.html#polygons-lines-and-polylines-data-sources-for-leaflet",
    "href": "content/labs/r-labs/maps/leaflet.html#polygons-lines-and-polylines-data-sources-for-leaflet",
    "title": "Playing with Leaflet",
    "section": "\n3.2 Polygons, Lines, and Polylines Data Sources for leaflet\n",
    "text": "3.2 Polygons, Lines, and Polylines Data Sources for leaflet\n\nWe have seen how to get POINT data into leaflet.\nLine and polygon data can come from a variety of sources:\n\n\nSpatialPolygons, SpatialPolygonsDataFrame, Polygons, and Polygon objects (from the sp package)\n\n\nSpatialLines, SpatialLinesDataFrame, Lines, and Line objects (from the sp package)\n\n\nMULTIPOLYGON, POLYGON, MULTILINESTRING, and LINESTRING objects (from the sf package)\n\n\nmap objects (from the maps package’s map() function); use map(fill = TRUE) for polygons, FALSE for polylines\n\nTwo-column numeric matrix; the first column is longitude and the second is latitude. Polygons are separated by rows of (NA, NA). It is not possible to represent multi-polygons nor polygons with holes using this method; use SpatialPolygons instead.\n\nWe will concentrate on using sf data into leaflet. We may explore maps() objects at a later date.\n\n3.2.1 Polygons/MultiPolygons and LineString/MultiLineString using sf data frames\nLet us download College buildings, parks, and the cycling lanes in Amsterdam, Netherlands, and plot these in leaflet.\n\nbbox &lt;- osmdata::getbb(\"Amsterdam, Netherlands\")\nbbox\n\n        min       max\nx  4.728756  5.079162\ny 52.278174 52.431064\n\ncolleges &lt;- osmplotr::extract_osm_objects(bbox = bbox,\n                                           key = \"amenity\",\n                                           value = \"college\",\n                                           return_type = \"polygon\" )\n\nIssuing query to Overpass API ...\n\n\nAnnounced endpoint: none\n\n\nQuery complete!\n\n\nconverting OSM data to sf format\n\nparks &lt;- osmplotr::extract_osm_objects(bbox = bbox,\n                                           key = \"park\",\n                                           return_type = \"polygon\" )\n\nIssuing query to Overpass API ...\n\n\nAnnounced endpoint: none\n\n\nQuery complete!\n\n\nconverting OSM data to sf format\n\nroads &lt;- osmplotr::extract_osm_objects(bbox = bbox,\n                                       key = \"highway\",\n                                       value = \"primary\",\n                                       return_type = \"line\")\n\nIssuing query to Overpass API ...\n\n\nAnnounced endpoint: none\n\n\nQuery complete!\n\n\nconverting OSM data to sf format\n\ncyclelanes &lt;-\n  osmplotr::extract_osm_objects(bbox,\n                                key = \"cycleway\",\n                                value =  \"lane\",\n                                return_type = \"line\")\n\nIssuing query to Overpass API ...\n\n\nAnnounced endpoint: none\n\n\nQuery complete!\n\n\nconverting OSM data to sf format\n\n\nWe have 17 colleges in our data and 370 parks in our data.\n\nleaflet() %&gt;% \n  addTiles() %&gt;% \n  addPolygons(data = colleges, popup = ~colleges$name) %&gt;% \n  addPolygons(data = parks, color = \"green\", popup = parks$name) %&gt;% \n  addPolylines(data = roads, color = \"red\") %&gt;% \n  addPolylines(data = cyclelanes, color = \"purple\")"
  },
  {
    "objectID": "content/labs/r-labs/maps/leaflet.html#chapter-3-using-raster-data-in-leaflet",
    "href": "content/labs/r-labs/maps/leaflet.html#chapter-3-using-raster-data-in-leaflet",
    "title": "Playing with Leaflet",
    "section": "\n3.3 Chapter 3: Using Raster Data in leaflet\n",
    "text": "3.3 Chapter 3: Using Raster Data in leaflet\n\nSo far all the geospatial data we have plotted in leaflet has been vector data. We will now explore how to plot raster data using leaflet. Raster data are used to depict continuous variables across space, such as vegitation, salinity, forest cover etc. Satellite imagery is frequently available as raster data.\n\n3.3.1 Importing Raster Data [Work in Progress!]\nRaster data can be imported into R in many ways:\n\nusing the maptiles package\n\nusing the OpenStreetMap package\n\n\nlibrary(terra)\n\nterra 1.7.18\n\n\n\nAttaching package: 'terra'\n\n\nThe following object is masked from 'package:data.world':\n\n    query\n\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\nlibrary(maptiles)\n#library(OpenStreetMap) # causes RStudio to crash..."
  },
  {
    "objectID": "content/labs/r-labs/maps/leaflet.html#adding-legendswork-in-progress",
    "href": "content/labs/r-labs/maps/leaflet.html#adding-legendswork-in-progress",
    "title": "Playing with Leaflet",
    "section": "\n4.1 Adding Legends[Work in Progress!]",
    "text": "4.1 Adding Legends[Work in Progress!]\n\n## Generate some random lat lon data around Bangalore\ndf &lt;- data.frame(lat = runif(20, min = 11.97, max = 13.07),\n                 lng = runif(20, min = 77.48, max = 77.68),\n                 col = sample(c(\"red\", \"blue\", \"green\"), 20, \n                              replace = TRUE),\n                 stringsAsFactors = FALSE)\n\ndf %&gt;%\n  leaflet() %&gt;%\n  addTiles() %&gt;%\n  addCircleMarkers(color = df$col) %&gt;%\n  addLegend(values = df$col, labels = LETTERS[1:3], colors = c(\"blue\", \"red\", \"green\"))\n\nAssuming \"lng\" and \"lat\" are longitude and latitude, respectively"
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html",
    "href": "content/labs/r-labs/networks/index.html",
    "title": "The Grammar of Networks",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE, warning = FALSE)\n\n########################################\n# For General Data Manipulation\nlibrary(tidyverse)\n\n########################################\n# Network Analysis Library (Handle data and Viz)\nlibrary(igraph)\nlibrary(netrankr)\n\n########################################\n# For Network \"Manipulation\"\nlibrary(tidygraph)\n\n# For Network Visualization\nlibrary(ggraph)\nlibrary(graphlayouts)\nlibrary(visNetwork)\n\n# For \"Network\" Datasets\nlibrary(igraphdata)"
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#iconify-noto-v1-package-setting-up-r-packages",
    "href": "content/labs/r-labs/networks/index.html#iconify-noto-v1-package-setting-up-r-packages",
    "title": "The Grammar of Networks",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE, warning = FALSE)\n\n########################################\n# For General Data Manipulation\nlibrary(tidyverse)\n\n########################################\n# Network Analysis Library (Handle data and Viz)\nlibrary(igraph)\nlibrary(netrankr)\n\n########################################\n# For Network \"Manipulation\"\nlibrary(tidygraph)\n\n# For Network Visualization\nlibrary(ggraph)\nlibrary(graphlayouts)\nlibrary(visNetwork)\n\n# For \"Network\" Datasets\nlibrary(igraphdata)"
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "href": "content/labs/r-labs/networks/index.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "title": "The Grammar of Networks",
    "section": "\n2  Introduction",
    "text": "2  Introduction\nThis Quarto document is part of my workshop course on R . The material is based on A Layered Grammar of Graphics by Hadley Wickham. The intent of this Course is to build Skill in coding in R, and also appreciate R as a way to metaphorically visualize information of various kinds, using predominantly geometric figures and structures.\nAll Quarto document files combine code, text, web-images, and figures developed using code. Everything is text; code chunks are enclosed in fences (```)"
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#goals",
    "href": "content/labs/r-labs/networks/index.html#goals",
    "title": "The Grammar of Networks",
    "section": "\n3 Goals",
    "text": "3 Goals\nAt the end of this Lab session, we should:\n\nknow the types and structures of network data and be able to work with them\nunderstand the basics of modern network packages in R\nbe able to create network visualizations using tidygraph, ggraph( static visualizations ) and visNetwork (interactive visualizations)\nsee directions for how the network metaphor applies in a variety of domains (e.g. biology/ecology, ideas/influence, technology, transportation, to name a few)"
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#pedagogical-note",
    "href": "content/labs/r-labs/networks/index.html#pedagogical-note",
    "title": "The Grammar of Networks",
    "section": "\n4 Pedagogical Note",
    "text": "4 Pedagogical Note\nThe method followed will be based on PRIMM:\n\n\nPREDICT Inspect the code and guess at what the code might do, write predictions\n\n\nRUN the code provided and check what happens\n\nINFER what the parameters of the code do and write comments to explain. What bells and whistles can you see?\n\nMODIFY the parameters code provided to understand the options available. Write comments to show what you have aimed for and achieved.\n\nMAKE : take an idea/concept of your own, and graph it."
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#graph-metaphors",
    "href": "content/labs/r-labs/networks/index.html#graph-metaphors",
    "title": "The Grammar of Networks",
    "section": "\n5 Graph Metaphors",
    "text": "5 Graph Metaphors\nNetwork graphs are characterized by two key terms: nodes and edges\n\n\nNodes : Entities\n\nMetaphors: Individual People? Things? Ideas? Places? to be connected in the network.\nSynonyms: vertices. Nodes have IDs.\n\n\n\nEdges: Connections\n\nMetaphors: Interactions? Relationships? Influence? Letters sent and received? Dependence? between the entities.\nSynonyms: links, ties.\n\n\n\nIn R, we create network representations using node and edge information. One way in which these could be organized are:\n\n\nNode list: a data frame with a single column listing the node IDs found in the edge list. You can also add attribute columns to the data frame such as the names of the nodes or grouping variables. ( Type? Class? Family? Country? Subject? Race? )\n\n\nNode Table\n\n\n\n\n\n\n\nID\nNode Name\nAttribute? Qualities?Categories? Family? Country?Planet?\n\n\n1\nNed\nNursery School Teacher\n\n\n2\nJaguar Paw\nMain Character, Apocalypto\n\n\n3\nJohn Snow\nEpidemiologist\n\n\n\n\n\nEdge list: data frame containing two columns: source node and destination node of an edge. Source and Destination have node IDs.\n\nWeighted network graph: An edge list can also contain additional columns describing attributes of the edges such as a magnitude aspect for an edge. If the edges have a magnitude attribute the graph is considered weighted.\n\n\nEdges Table\n\nFrom\nTo\nRelationship\nWeightage\n\n\n\n1\n3\nFinancial Dealings\n6\n\n\n2\n1\nHistory Lessons\n2\n\n\n2\n3\nVaccination\n15\n\n\n\n\n\nLayout: A geometric arrangement of nodes and edges.\n\nMetaphors: Location? Spacing? Distance? Coordinates? Colour? Shape? Size? Provides visual insight due to the arrangement.\n\n\n\nLayout Algorithms : Method to arranges nodes and edges with the aim of optimizing some metric .\n\nMetaphors: Nodes are masses and edges are springs. The Layout algorithm minimizes the stretching and compressing of all springs.(BTW, are the Spring Constants K the same for all springs?…)\n\n\nDirected and undirected network graph: If the distinction between source and target is meaningful, the network is directed. If the distinction is not meaningful, the network is undirected. Directed edges represent an ordering of nodes, like a relationship extending from one node to another, where switching the direction would change the structure of the network. Undirected edges are simply links between nodes where order does not matter.\n\nExamples:\n - The World Wide Web is an example of a directed network because\n hyperlinks connect one Web page to another, but not necessarily \n the other way around.\n\n - Co-authorship networks represent examples of un-directed networks,\nwhere nodes are authors and they are connected by an edge if they\nhave written a publication together\n\n - When people send e-mail to each other, the distinction between the\nsender (source) and the recipient (target) is clearly meaningful,\ntherefore the network is directed.\n\n\nConnected and Disconnected graphs: If there is some path from any node to any other node, the Networks is said to be Connected. Else, Disconnected."
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#predictruninfer--1",
    "href": "content/labs/r-labs/networks/index.html#predictruninfer--1",
    "title": "The Grammar of Networks",
    "section": "\n6 Predict/Run/Infer -1",
    "text": "6 Predict/Run/Infer -1\n\n6.1 Using tidygraph and ggraph\n\ntidygraph and ggraph are modern R packages for network data. Graph Data setup and manipulation is done in tidygraph and graph visualization with ggraph.\n\n\ntidygraph Data -&gt; “Network Object” in R.\n\nggraph Network Object -&gt; Plots using a chosen layout/algo.\n\nBoth leverage the power of igraph, which is the Big Daddy of all network packages. We will be using the Grey’s Anatomy dataset in our first foray into networks.\n\n6.2 Step1. Read the data\nDownload these two datasets into your current project-&gt; data folder.\n Grey’s Anatomy Nodes \n Grey’s Anatomy Nodes \n\ngrey_nodes &lt;- read_csv(\"../../../materials/data/networks/grey_nodes.csv\")\ngrey_edges &lt;- read_csv(\"../../../materials/data/networks/grey_edges.csv\")\n\ngrey_nodes\n\n\n\n  \n\n\ngrey_edges\n\n\n\n  \n\n\n\n\n\nQuestions and Inferences #1:\n\n\nLook at the console output thumbnail. What does for example name = col_character mean? What attributes (i.e. extra information) are seen for Nodes and Edges? Understand the data in both nodes and edges as shown in the second and third thumbnails. Write some comments and inferences here.\n\n\n\n6.3 Step 2.Create a network object using tidygraph:\nKey function:\n\n\ntbl_graph(): (aka “tibble graph”). Key arguments: nodes, edges and directed. Note this is a very versatile command and can take many input forms, such as data structures that result from other packages. Type ?tbl_graph in the Console and see the Usage section.\n\n\nga &lt;- tbl_graph(nodes = grey_nodes, \n                edges = grey_edges, \n                directed = FALSE)\nga\n\n# A tbl_graph: 54 nodes and 57 edges\n#\n# An undirected simple graph with 4 components\n#\n# A tibble: 54 × 7\n  name               sex   race  birthyear position  season sign  \n  &lt;chr&gt;              &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt; \n1 Addison Montgomery F     White      1967 Attending      1 Libra \n2 Adele Webber       F     Black      1949 Non-Staff      2 Leo   \n3 Teddy Altman       F     White      1969 Attending      6 Pisces\n4 Amelia Shepherd    F     White      1981 Attending      7 Libra \n5 Arizona Robbins    F     White      1976 Attending      5 Leo   \n6 Rebecca Pope       F     White      1975 Non-Staff      3 Gemini\n# ℹ 48 more rows\n#\n# A tibble: 57 × 4\n   from    to weight type    \n  &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt;   \n1     5    47      2 friends \n2    21    47      4 benefits\n3     5    46      1 friends \n# ℹ 54 more rows\n\n\n\n\nQuestions and Inferences #2:\n\n\nQuestions and Inferences: What information does the graph object contain? What attributes do the nodes have? What about the edges?\n\n\n\n6.4 Step 3. Plot using ggraph\n\n3a. Quick Plot: autograph() This is to check quickly is the data is imported properly and to decide upon going on to a more elaborate plotting.\n\nautograph(ga)\n\n\n\n\n\n\nQuestions and Inferences #3:\n\n\nQuestions and Inferences: Describe this graph, in simple words here. Try to use some of the new domain words we have just acquired: nodes/edges, connected/disconnected, directed/undirected.\n\n\n3b. More elaborate plot\nKey functions:\n\n\nggraph(layout = \"......\"): Create classic node-edge diagrams; i.e. Sets up the graph. Rather like ggplot for networks!\n\nTwo kinds of geom: one set for nodes, and another for edges\n\ngeom_node_point(aes(.....)): Draws node as “points”. Alternatives are circle / arc_bar / tile / voronoi. Remember the geoms that we have seen before in Grammar of Graphics!\ngeom_edge_link(aes(.....)): Draws edges as “links”. Alternatives are arc / bend / elbow / hive / loop / parallel / diagonal / point / span /tile.\ngeom_node_text(aes(label = ......), repel = TRUE): Adds text labels (non-overlapping). Alternatives are label /...\nlabs(title = \"....\", subtitle = \"....\", caption = \"....\"): Change main titles, axis labels and legend titles. We know this from our work with ggplot.\n\n\n# Write Comments next to each line \n# About what that line does for the overall graph\n\nggraph(graph = ga,  layout = \"kk\") +\n  #\n  \n  geom_edge_link(width = 2, color = \"pink\") +\n  #\n  \n  geom_node_point(\n    shape = 21,\n    size = 8,\n    fill = \"blue\",\n    color = \"green\",\n    stroke = 2\n  ) +\n  #\n  \n  labs(title = \"Whoo Hoo! My first silly Grey's Anatomy graph in R!\",\n       subtitle = \"Why did I ever get in this course...\",\n       caption = \"Bro, they are doing **cool** things in the other\n       classes...\") \n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` in the `default_aes` field and elsewhere instead.\n\n\n\n\n\n\n\nQuestions and Inferences #3:\n\n\nQuestions and Inferences: What parameters have been changed here, compared to the earlier graph? Where do you see these changes in the code above?\n\n\nLet us Play with this graph and see if we can make some small changes. Colour? Fill? Width? Size? Stroke? Labs? Of course!\n\n# Change the parameters in each of the commands here to new ones\n# Use fixed values for colours or sizes...etc. \n\nggraph(graph = ga,  layout = \"kk\") + \n  geom_edge_link(width = 2) + \n  geom_node_point(shape = 21, size = 8, \n                  fill = \"blue\", \n                  color = \"green\", \n                  stroke = 2) +\n  labs(title = \"Whoo Hoo! My next silly Grey's Anatomy graph in R!\",\n       subtitle = \"Why did I ever get in this course...\",\n       caption = \"Bro, they are doing cool things in the other \n       classes...\") \n\n\n\n\n\n\nQuestions and Inferences #4:\n\n\nQuestions and Inferences: What did the shape parameter achieve? What are the possibilities with shape? How about including alpha?\n\n\n3c. Aesthetic Mapping from Node and Edge attribute columns\nUp to now, we have assigned specific numbers to geometric aesthetics such as shape and size. Now we are ready ( maybe ?) change the meaning and significance of the entire graph and each element within it, and use aesthetics / metaphoric mappings to achieve new meanings or insights. Let us try using aes() inside each geom to map a variable to a geometric aspect.\nDon’t try to use more than 2 aesthetic mappings simultaneously!!\nThe node elements we can tweak are:\n\nTypes of Nodes: geom_node_****()\n\nNode Parameters: inside geom_node_****(aes(...............))\n-aes(alpha  = node-variable) : opacity; a value between 0 and 1\n-aes(shape  = node-variable) : node shape\n-aes(colour = node-variable) : node colour\n-aes(fill   = node-variable) : fill colour for node\n-aes(size   = node-variable) : size of node\n\nThe edge elements we can tweak are:\n\nType of Edges” geom_edge_****()\n\nEdge Parameters: inside geom_edge_****(aes(...............))\n-aes(colour = edge-variable) : colour of the edge\n-aes(width  = edge-variable) : width of the edge\n-aes(label  = some_variable) : labels for the edge\n\nType ?geom_node_point and ?geom-edge_link in your Console for more information.\n\nggraph(graph = ga, layout = \"fr\") +\n  geom_edge_link0(aes(width = weight)) + # change variable here\n  \n  geom_node_point(aes(color = race), size = 6) + # change variable here\n\n\n  labs(title = \"Whoo Hoo! Yet another Grey's Anatomy graph in R!\") \n\n\n\n\n\n\nQuestions and Inferences #5:\n\n\nQuestions and Inferences: Describe some of the changes here. What types of edges worked? Which variables were you able to use for nodes and edges and how? What did not work with either of the two?"
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#predictreuseinfer-2",
    "href": "content/labs/r-labs/networks/index.html#predictreuseinfer-2",
    "title": "The Grammar of Networks",
    "section": "\n7 Predict/Reuse/Infer-2",
    "text": "7 Predict/Reuse/Infer-2\n\n# Arc diagram\nggraph(ga, layout = \"linear\") +\n  geom_edge_arc(aes(width = weight), alpha = 0.8) +\n  scale_edge_width(range = c(0.2, 2)) +\n  geom_node_point(size = 2, colour = \"red\") +\n  labs(edge_width = \"Weight\") \n\n\n\n\n\n\nQuestions and Inferences #6:\n\n\nQuestions and Inferences: How does this graph look “metaphorically” different? Do you see a difference in the relationships between people here? Why?\n\n\n\n# Coord diagram, circular\nggraph(ga, layout = \"linear\", circular = TRUE) + \n  geom_edge_arc(aes(width = weight), alpha = 0.8) + \n  scale_edge_width(range = c(0.2, 2)) +\n  geom_node_point(size = 4,colour = \"red\") + \n  geom_node_text(aes(label = name),repel = TRUE, size = 3,\n                 max.overlaps = 20) +\n  labs(edge_width = \"Weight\") +\n  theme_graph() \n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\n\nQuestions and Inferences #7:\n\n\nQuestions and Inferences: How does this graph look “metaphorically” different? Do you see a difference in the relationships between people here? Why?"
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#hierarchical-layouts",
    "href": "content/labs/r-labs/networks/index.html#hierarchical-layouts",
    "title": "The Grammar of Networks",
    "section": "\n8 Hierarchical layouts",
    "text": "8 Hierarchical layouts\nThese provide for some alternative metaphorical views of networks. Note that not all layouts are possible for all datasets!!\n\nset_graph_style()\n\n# This dataset contains the graph that describes the class \n# hierarchy for the Flare visualization library.\n# Type ?flare in your Console\nhead(flare$vertices)\n\n\n\n  \n\n\nhead(flare$edges)\n\n\n\n  \n\n\n# flare class hierarchy\ngraph = tbl_graph(edges = flare$edges, nodes = flare$vertices)\n\n# dendrogram\nggraph(graph, layout = \"dendrogram\") + \n  geom_edge_diagonal() + \n  labs(title = \"Dendrogram\")\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n# circular dendrogram\nggraph(graph, layout = \"dendrogram\", circular = TRUE) + \n  geom_edge_diagonal() + \n  geom_node_point(aes(filter = leaf)) + \n  coord_fixed()+ \n  labs(title = \"Circular Dendrogram\")\n\n\n\n# rectangular tree map\nggraph(graph, layout = \"treemap\", weight = size) + \n  geom_node_tile(aes(fill = depth), size = 0.25) + \n  labs(title = \"Rectangular Tree Map\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n# circular tree map\nggraph(graph, layout = \"circlepack\", weight = size) + \n  geom_node_circle(aes(fill = depth), size = 0.25, n = 50) + \n  coord_fixed() + \n  labs(title = \"Circular Tree Map\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n# icicle\nggraph(graph, layout = \"partition\") + \n  geom_node_tile(aes(y = -y, fill = depth))\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n# sunburst (circular icicle)\nggraph(graph, layout = \"partition\", circular = TRUE) +\n  geom_node_arc_bar(aes(fill = depth)) +\n  coord_fixed() + \n  labs(title = \"Circular Icicle\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\n\nQuestions and Inferences #8:\n\n\nQuestions and Inferences: How do graphs look “metaphorically” different? Do they reveal different aspects of the group? How?"
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#faceting",
    "href": "content/labs/r-labs/networks/index.html#faceting",
    "title": "The Grammar of Networks",
    "section": "\n9 Faceting",
    "text": "9 Faceting\nFaceting allows to create sub-plots according to the values of a qualitative attribute on nodes or edges.\n\n# setting theme_graph \nset_graph_style()\n\n\n# facet edges by type\nggraph(ga,layout = \"linear\", circular = TRUE) + \n  geom_edge_link(aes(color = type)) + \n  geom_node_point() +\n  facet_edges(~ type) +\n  theme(aspect.ratio = 1)\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n# facet nodes by sex\nggraph(ga,layout = \"linear\", circular = TRUE) + \n  geom_edge_link() + \n  geom_node_point() +\n  facet_nodes(~race) +\n  theme(aspect.ratio = 1)\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n# facet both nodes and edges\nggraph(ga,layout = \"linear\", circular = TRUE) + \n  geom_edge_link(aes(color = type)) + \n  geom_node_point() +\n  facet_graph(type ~ race) + \n  th_foreground(border = TRUE) +\n  theme(aspect.ratio = 1, legend.position = \"top\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\n\nQuestions and Inferences #9:\n\n\nQuestions and Inferences: Does splitting up the main graph into subnetworks give you more insight? Describe some of these."
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#network-analysis-with-tidygraph",
    "href": "content/labs/r-labs/networks/index.html#network-analysis-with-tidygraph",
    "title": "The Grammar of Networks",
    "section": "\n10 Network analysis with tidygraph",
    "text": "10 Network analysis with tidygraph\nThe data frame graph representation can be easily augmented with metrics or statistics computed on the graph. Remember how we computed counts with the penguin dataset in Grammar of Graphics.\nBefore computing a metric on nodes or edges use the activate() function to activate either node or edge data frames. Use dplyr verbs (filter, arrange, mutate) to achieve your computation in the proper way.\n\n10.1 Network Centrality\nCentrality is a an “ill-defined” metric of node and edge importance in a network. It is therefore calculated in many ways. Type ?centrality in your Console.\n\n\nStandards\n\nLet’s add a few columns to the nodes and edges based on network centrality measures:\n\nga %&gt;% \n  activate(nodes) %&gt;% \n  \n  # Node with  the most connections?\n  mutate(degree = centrality_degree(mode = c(\"in\"))) %&gt;% \n  filter(degree &gt; 0) %&gt;% \n  \n  activate(edges) %&gt;% \n  # \"Busiest\" edge?\n  mutate(betweenness = centrality_edge_betweenness())\n\n# A tbl_graph: 54 nodes and 57 edges\n#\n# An undirected simple graph with 4 components\n#\n# A tibble: 57 × 5\n   from    to weight type     betweenness\n  &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;\n1     5    47      2 friends         20.3\n2    21    47      4 benefits        44.7\n3     5    46      1 friends         39  \n4     5    41      1 friends         66.3\n5    18    41      6 friends         39  \n6    21    41     12 benefits        91.5\n# ℹ 51 more rows\n#\n# A tibble: 54 × 8\n  name               sex   race  birthyear position  season sign   degree\n  &lt;chr&gt;              &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1 Addison Montgomery F     White      1967 Attending      1 Libra       3\n2 Adele Webber       F     Black      1949 Non-Staff      2 Leo         1\n3 Teddy Altman       F     White      1969 Attending      6 Pisces      4\n# ℹ 51 more rows\n\n\nPackages tidygraph and ggraph can be pipe-lined to perform analysis and visualization tasks in one go.\n\n# setting theme_graph \nset_graph_style()\n\nga %&gt;% \n  activate(nodes) %&gt;% \n  \n  # Who has the most connections?\n  mutate(degree = centrality_degree()) %&gt;% \n  \n  activate(edges) %&gt;% \n  # Who is the go-through person?\n  mutate(betweenness = centrality_edge_betweenness()) %&gt;%\n  \n  # Now to continue with plotting\n  ggraph(layout = \"nicely\") +\n  geom_edge_link(aes(alpha = betweenness)) +\n  geom_node_point(aes(size = degree, colour = degree)) + \n  \n  # discrete colour legend\n  scale_color_gradient(guide = \"legend\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n# or even less typing\n  ggraph(ga,layout = \"nicely\") +\n  geom_edge_link(aes(alpha = centrality_edge_betweenness())) +\n  geom_node_point(aes(colour = centrality_degree(), \n                      size = centrality_degree())) + \n  scale_color_gradient(guide = \"legend\",\n                       low = \"green\",\n                       high = \"red\") \n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\n\nQuestions and Inferences #10:\n\n\nQuestions and Inferences: How do the Centrality Measures show up in the graph? Would you “agree” with the way we have done it? Try to modify the aesthetics by copy-pasting this chunk below and see how you can make an alternative representation.\n\n\n\n10.2 Analysis and visualizing Network Communities\nWho is close to whom? Which are the groups you can see?\n\n# setting theme_graph \nset_graph_style()\n\n\n# visualize communities of nodes\nga %&gt;% \n  activate(nodes) %&gt;%\n  mutate(community = as.factor(group_louvain())) %&gt;% \n  ggraph(layout = \"graphopt\") + \n  geom_edge_link() + \n  geom_node_point(aes(color = community), size = 5) \n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\n\nQuestions and Inferences #11:\n\n\nQuestions and Inferences: Is the Community depiction clear? How would you do it, with which aesthetic? Copy Paste this chunk below and try.\n\n\n\n10.3 Interactive Graphs with visNetwork\n\nExploring the VisNetwork package. Make graphs wiggle and shake using tidy commands! The package implements interactivity using the physical metaphor of weights and springs we discussed earlier.\nThe visNetwork() function uses a nodes list and edges list to create an interactive graph. The nodes list must include an “id” column, and the edge list must have “from” and “to” columns. The function also plots the labels for the nodes, using the names of the cities from the “label” column in the node list.\n\nlibrary(visNetwork)\n\n# Prepare the data for plotting by visNetwork\ngrey_nodes\n\n\n\n  \n\n\ngrey_edges\n\n\n\n  \n\n\n# Relabel greys anatomy nodes and edges for VisNetwork\ngrey_nodes_vis &lt;- grey_nodes %&gt;% \n  rowid_to_column(var = \"id\") %&gt;% \n  rename(\"label\" = name) %&gt;% \n  mutate(sex = case_when(sex == \"F\" ~ \"Female\",\n                         sex == \"M\" ~ \"Male\")) %&gt;% \n  replace_na(., list(sex = \"Transgender?\")) %&gt;% \n  rename(\"group\" = sex)\ngrey_nodes_vis\n\n\n\n  \n\n\ngrey_edges_vis &lt;- grey_edges %&gt;% \n  select(from, to) %&gt;% \n  left_join(., grey_nodes_vis, \n            by = c(\"from\" = \"label\")) %&gt;% \n  left_join(., grey_nodes_vis, \n            by = c(\"to\" = \"label\")) %&gt;%\n  select(\"from\"= id.x, \"to\" = id.y)\ngrey_edges_vis\n\n\n\n  \n\n\n\nUsing fontawesome icons\n\ngrey_nodes_vis %&gt;%\n\n  visNetwork(nodes = ., edges = grey_edges_vis) %&gt;% \n  visNodes(font = list(size = 40)) %&gt;% \n  \n  # Colour and icons for each of the gender-groups\n  visGroups(groupname = \"Female\", shape = \"icon\", \n            icon = list(code = \"f182\", size = 75, color = \"tomato\"),\n            shadow = list(enabled = TRUE)) %&gt;% \n  \n  visGroups(groupname = \"Male\", shape = \"icon\", \n            icon = list(code = \"f183\", size = 75, color = \"slateblue\"), \n            shadow = list(enabled = TRUE)) %&gt;% \n  \n  visGroups(groupname = \"Transgender?\", shape = \"icon\", \n            icon = list(code = \"f22c\", size = 75, color = \"fuchsia\"), \n            shadow = list(enabled = TRUE)) %&gt;% \n  \n  #visLegend() %&gt;%\n  #Add the fontawesome icons!!\n  addFontAwesome() %&gt;% \n  \n  # Add Interaction Controls\n  visInteraction(navigationButtons = TRUE,\n                 hover = TRUE,\n                 selectConnectedEdges = TRUE,\n                 hoverConnectedEdges = TRUE,\n                 zoomView = TRUE)\n\n\n\n\n\nThere is another family of icons available in visNetwork, called ionicons. Let’s see how they look:\n\ngrey_nodes_vis %&gt;%\n\n  visNetwork(nodes = ., edges = grey_edges_vis,) %&gt;%\n  visLayout(randomSeed = 12345) %&gt;%\n  visNodes(font = list(size = 50)) %&gt;%\n  visEdges(color = \"green\") %&gt;%\n  visGroups(\n    groupname = \"Female\",\n    shape = \"icon\",\n    icon = list(\n      face = 'Ionicons',\n      code = \"f25d\",\n      color = \"fuchsia\",\n      size = 125\n    )\n  ) %&gt;%\n  \n  visGroups(\n    groupname = \"Male\",\n    shape = \"icon\",\n    icon = list(\n      face = 'Ionicons',\n      code = \"f202\",\n      color = \"green\",\n      size = 125\n    )\n  ) %&gt;%\n  \n  visGroups(\n    groupname = \"Transgender?\",\n    shape = \"icon\",\n    icon = list(\n      face = 'Ionicons',\n      code = \"f233\",\n      color = \"dodgerblue\",\n      size = 125\n    )\n  ) %&gt;%\n  visLegend() %&gt;%\n  addIonicons() %&gt;%\n  visInteraction(\n    navigationButtons = TRUE,\n    hover = TRUE,\n    selectConnectedEdges = TRUE,\n    hoverConnectedEdges = TRUE,\n    zoomView = TRUE\n  )\n\n\n\n\n\nSome idea of interactivity and controls with visNetwork:\n Star Wars Nodes \n Star Wars Edges \n\n# let's look again at the data\nstarwars_nodes &lt;- read_csv(\"../../../materials/data/networks/star-wars-network-nodes.csv\")\n\nRows: 22 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): name\ndbl (1): id\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nstarwars_edges &lt;- read_csv(\"../../../materials/data/networks/star-wars-network-edges.csv\")\n\nRows: 60 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): source, target\ndbl (1): weight\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n# We need to rename starwars nodes dataframe and edge dataframe columns for visNetwork\nstarwars_nodes_vis &lt;- \n  starwars_nodes %&gt;% \n  rename(\"label\" = name)\n\n# Convert from and to columns to **node ids**\nstarwars_edges_vis &lt;- \n  starwars_edges %&gt;% \n  \n  # Matching Source &lt;- Source Node id (\"id.x\")\n  left_join(., starwars_nodes_vis, by = c(\"source\" = \"label\")) %&gt;% \n  \n  # Matching Target &lt;- Target Node id (\"id.y\")\n  left_join(., starwars_nodes_vis, by = c(\"target\" = \"label\")) %&gt;% \n  \n# Select \"id.x\" and \"id.y\" ONLY\n# Rename them as \"from\" and \"to\"\n# keep \"weight\" column for aesthetics of edges\n  select(\"from\" = id.x, \"to\" = id.y, \"value\" = weight)\n\n# Check everything once\nstarwars_nodes_vis\n\n\n\n  \n\n\nstarwars_edges_vis\n\n\n\n  \n\n\n\nOk, let’s make things move and shake!!\n\nvisNetwork(nodes = starwars_nodes_vis,\n           edges = starwars_edges_vis) %&gt;% \n  visNodes(font = list(size = 30), shape = \"icon\", \n           icon = list(code = \"f1e3\", size = 75)) %&gt;% \n  addFontAwesome() %&gt;% \n  visEdges(color = \"red\")\n\n\n\n\n\n\nvisNetwork(nodes = starwars_nodes_vis,\n           edges = starwars_edges_vis) %&gt;% \n  visNodes(font = list(size = 30)) %&gt;% \n  visEdges(color = \"red\")"
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#your-assignments",
    "href": "content/labs/r-labs/networks/index.html#your-assignments",
    "title": "The Grammar of Networks",
    "section": "\n11 Your Assignments:",
    "text": "11 Your Assignments:\n\n11.1 Make-1 : With a ready made dataset\nStep 0. Fire up a New Project! Always!\nStep 1. Fire up a new Quarto document. Fill in the YAML header.\nStep 2. Take any one of the “Make1-Datasets” datasets described below.\nStep 3. Document contents:\n\nIntroduce / Inspect in R your data and describe\nIntroduce your Purpose\nCreate graph objects\nTry different layouts\nWrite comments in the code\nWrite narrative in text with sections, bold ,italic etc.\n\nStep 4. Knit before you submit. Submit only your renderable .qmd file.\n\n11.2 Make1 - Datasets:\n\nAirline Data:\n\n Airlines Nodes \n Airlines Edges \n\nStart with this bit of code in your second chunk, after set up\n\n\n\nairline_nodes &lt;-\n  read_csv(\"./mydatafolder/AIRLINES-NODES.csv\") %&gt;% \n  mutate(Id = Id + 1)\n\nairline_edges &lt;-\n  read_csv(\"./mydatafolder/AIRLINES-EDGES.csv\") %&gt;%\n  mutate(Source = Source + 1, Target = Target + 1)\n\n\nThe Famous Zachary Karate Club dataset\n\n\n\nStart with pulling this data into your Rmarkdown:\ndata(“karate”,package= “igraphdata”) karate\n\nTry ?karate in the console\nNote that this is not a set of nodes, nor edges, but already a graph-object!\nSo no need to create a graph object using tbl_graph.\nYou will need to just go ahead and plot using ggraph.\n\n\nGame of Thrones:\n\n GoT Networks \n\nStart with pulling this data into your Rmarkdown:\n\n\nGoT &lt;- read_rds(\"../../../materials/data/networks/GoT.RDS\")\n\n\nNote that this is a list of 7 graphs from Game of Thrones.\nSelect one using GoT[[index]] where index = 1…7 and then plot directly.\nTry to access the nodes and edges and modify them using any attribute data\n\n\nAny other graph dataset from igraphdata (type ?igraphdata in console)\n\n\nAsk me for help if you need any\n\n11.3 Make-2: Literary Network with TV Show / Book / Story / Play\nThis is in groups. Groups of 4. To be announced\nYou need to create a Network Graph for your favourite Book, play, TV serial or Show. (E.g. Friends, BBT, or LB or HIMYM, B99, TGP, JTV…or Hamlet, Little Women , Pride and Prejudice, or LoTR)\nStep 1. Go to: Literary Networks for instructions.\nStep 2. Make your data using the instructions.\n\nIn the nodes excel, use id and names as your columns. Any other details in other columns to the right.\nIn your edges excel, use from and to are your first columns. Entries in these columns can be names or ids but be consistent and don’t mix.\n\nStep 3. Decide on 3 answers that you to seek and plan to make graphs for.\nStep 4. Create graph objects. Say 3 visualizations.\nStep 5. Write comments/answers in the code and narrative text. Add pictures from the web using Markdown syntax.\nStep 6. Write Reflection ( ok, a short one!) inside your Quarto document. Make sure it renders !!\nStep 7. Group Submission: Submit the render-able .qmd file AND the data. Quarto Markdown with joint authorship. Each person submits on their Assignments. All get the same grade on this one.\nAsk me for clarifications on what to do after you have read the Instructions in your group."
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#read-more",
    "href": "content/labs/r-labs/networks/index.html#read-more",
    "title": "The Grammar of Networks",
    "section": "\n12 Read more",
    "text": "12 Read more\n\nThomas Lin Pedersen - 1 giraffe, 2 giraffe,GO!\nIgraph: Network Analysis and Visualization. https://CRAN.R-project.org/package=igraph.\nPedersen, Thomas Lin. 2017a. Ggraph: An Implementation of Grammar of Graphics for Graphs and Networks. https://CRAN.R-project.org/package=ggraph.\n———. 2017b. Tidygraph: A Tidy Api for Graph Manipulation. https://CRAN.R-project.org/package=tidygraph.\nTyner, Sam, François Briatte, and Heike Hofmann. 2017. “Network Visualization with ggplot2.” The R Journal 9 (1): 27–59. https://journal.r-project.org/archive/2017/RJ-2017-023/index.html.\nNetwork Datasets https://icon.colorado.edu/#!/networks\nYunran Chen, Introduction to Network Analysis Using R"
  },
  {
    "objectID": "content/labs/r-labs/r-labs-listing.html",
    "href": "content/labs/r-labs/r-labs-listing.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nCritical Result Callback Monitor\n\n\n\n\n\n\n\n\n\n\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to the dplyr package\n\n\n\n\n\n\n\n\n\n\n\n\nJul 6, 2021\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\nLab 01 - Introduce Yourself\n\n\nInstalling and Getting Started with R!\n\n\nPart of my R for Artists and Designers course using the idea of metaphors in written language.\n\n\n\n\n\n\nJul 6, 2021\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\nLab 01 - Introduce Yourself\n\n\nInstalling and Getting Started with R!\n\n\nPart of my R Workshop course using the idea of metaphors in written language.\n\n\n\n\n\n\nJul 6, 2021\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\nLab 01 - Introduce Yourself\n\n\nInstalling and Getting Started with R!\n\n\nPart of my R for Artists and Designers course using the idea of metaphors in written language.\n\n\n\n\n\n\nJul 6, 2021\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\nLab 01 - Introduce Yourself\n\n\nInstalling and Getting Started with R!\n\n\n\n\n\n\n\n\n\nJul 6, 2021\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\nLab 05: Colors with Penguins\n\n\nPalettes from Famous Paintings, GoT, Harry Potter, and Wes Anderson\n\n\n\n\n\n\n\n\n\nJul 10, 2021\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\nLab 05: What use is a Book without any Pictures?\n\n\n\n\n\n\n\n\n\n\n\n\nJul 6, 2021\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\nLab 06 - The Grammar of Graphics\n\n\nCreating Graphs and Charts using ggplot\n\n\nPart of my R for Artists and Designers course using the idea of metaphors in written language.\n\n\n\n\n\n\nJul 6, 2021\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\nLab 06a: Fonts, Themes, and other Wizardy in ggplot\n\n\nFonts, Themes, and other Wizardy in ggplot\n\n\n\n\n\n\n\n\n\nJul 25, 2021\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\nLab-02: Pronouns and Data\n\n\n\n\n\nPart of my R Workshop using the idea of metaphors in written language.\n\n\n\n\n\n\nJul 6, 2021\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\nPlaying with Leaflet\n\n\n\n\n\n\n\n\n\n\n\n\nMay 13, 2017\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\nPlaying with Leaflet\n\n\n\n\n\n\n\n\n\n\n\n\nMay 13, 2017\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\nThe Grammar of Maps\n\n\nWhere is the Secret Garden?\n\n\nPart of my Workshop course on R\n\n\n\n\n\n\nApr 22, 2021\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\nThe Grammar of Networks\n\n\nVisualizing and Manipulating Network data in R\n\n\nVisualizing and Manipulating Network data in R\n\n\n\n\n\n\nJun 21, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\nggplotly: various examples\n\n\n\n\n\n\n\n\n\n\n\n\nCarson Sievert\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "content/labs/r-labs/tidy/moma.html",
    "href": "content/labs/r-labs/tidy/moma.html",
    "title": "Lab 05: What use is a Book without any Pictures?",
    "section": "",
    "text": "This Quarto document is part of my workshop on R. The material is based on A Layered Grammar of Graphics by Hadley Wickham. The course is meant for First Year students pursuing a Degree in Art and Design.\nThe intent is to build Skill in coding in R, and also appreciate R as a way to metaphorically visualize information of various kinds, using predominantly geometric figures and structures.\nAll Quarto document files combine code, text, web-images, and figures developed using code. Everything is text; code chunks are enclosed in fences (```)"
  },
  {
    "objectID": "content/labs/r-labs/tidy/moma.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "href": "content/labs/r-labs/tidy/moma.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "title": "Lab 05: What use is a Book without any Pictures?",
    "section": "",
    "text": "This Quarto document is part of my workshop on R. The material is based on A Layered Grammar of Graphics by Hadley Wickham. The course is meant for First Year students pursuing a Degree in Art and Design.\nThe intent is to build Skill in coding in R, and also appreciate R as a way to metaphorically visualize information of various kinds, using predominantly geometric figures and structures.\nAll Quarto document files combine code, text, web-images, and figures developed using code. Everything is text; code chunks are enclosed in fences (```)"
  },
  {
    "objectID": "content/labs/r-labs/tidy/moma.html#setting-up-the-packages",
    "href": "content/labs/r-labs/tidy/moma.html#setting-up-the-packages",
    "title": "Lab 05: What use is a Book without any Pictures?",
    "section": "\n2 Setting up the Packages",
    "text": "2 Setting up the Packages\n\nlibrary(tidyverse)\nlibrary(mosaic)"
  },
  {
    "objectID": "content/labs/r-labs/tidy/moma.html#goals-for-this-lab",
    "href": "content/labs/r-labs/tidy/moma.html#goals-for-this-lab",
    "title": "Lab 05: What use is a Book without any Pictures?",
    "section": "\n3 Goals for this Lab",
    "text": "3 Goals for this Lab\n\nUnderstand the idea of “tidy” data\nUsing “tidy data” and the “tidyverse” way of programming in R allows to translate our thoughts readily into code.\nUnderstand dplyr VERB functions to get to know and manipulate a dataset"
  },
  {
    "objectID": "content/labs/r-labs/tidy/moma.html#pedagogical-note",
    "href": "content/labs/r-labs/tidy/moma.html#pedagogical-note",
    "title": "Lab 05: What use is a Book without any Pictures?",
    "section": "\n4 Pedagogical Note",
    "text": "4 Pedagogical Note\nThe method followed will be based on PRIMM:\n\n\nPREDICT Inspect the code and guess at what the code might do, write predictions\n\n\nRUN the code provided and check what happens\n\nINFER what the parameters of the code do and write comments to explain. What bells and whistles can you see?\n\nMODIFY the parameters code provided to understand the options available. Write comments to show what you have aimed for and achieved.\n\nMAKE : take an idea/concept of your own, and graph it. # Inspiration + data\n\nWe’ll use data from the Museum of Modern Art (MoMA)\n\nPublicly available on GitHub\n\nAs analyzed by fivethirtyeight.com\n\nAnd by others"
  },
  {
    "objectID": "content/labs/r-labs/tidy/moma.html#read-in-the-data",
    "href": "content/labs/r-labs/tidy/moma.html#read-in-the-data",
    "title": "Lab 05: What use is a Book without any Pictures?",
    "section": "\n5 Read in the data",
    "text": "5 Read in the data\nThis dataset has been created by Alison Hill(@apreshill on Twitter). Download the dataset, save it into your data folder and then import it into R:\n  artworks-cleaned.csv\n\n\n\ncategorical variables:  \n               name     class levels    n missing\n1             title character   2015 2253       0\n2            artist character    989 2253       0\n3        artist_bio character    858 2252       1\n4     artist_gender character      2 2243      10\n5  circumference_cm   logical      0    0    2253\n6       diameter_cm   logical      0    0    2253\n7         length_cm   logical      0    0    2253\n8    seat_height_cm   logical      0    0    2253\n9          purchase   logical      2 2253       0\n10             gift   logical      2 2253       0\n11         exchange   logical      2 2253       0\n12   classification character      1 2253       0\n13       department character      5 2253       0\n                                    distribution\n1  Untitled (4.5%), I Got Up... (1.2%) ...      \n2  Pablo Picasso (2.4%) ...                     \n3    (Spanish, 1881–1973) (2.4%) ...            \n4  Male (88.8%), Female (11.2%)                 \n5   (%)                                         \n6   (%)                                         \n7   (%)                                         \n8   (%)                                         \n9  FALSE (91.2%), TRUE (8.8%)                   \n10 TRUE (51.8%), FALSE (48.2%)                  \n11 FALSE (93.6%), TRUE (6.4%)                   \n12 Painting (100%)                              \n13 Painting & Sculpture (97.4%) ...             \n\nquantitative variables:  \n                name   class    min         Q1 median          Q3      max\n1  artist_birth_year numeric 1839.0 1890.00000 1913.0 1933.000000 1987.000\n2  artist_death_year numeric 1890.0 1956.00000 1976.0 1996.000000 2018.000\n3        num_artists numeric    1.0    1.00000    1.0    1.000000   10.000\n4   n_female_artists numeric    0.0    0.00000    0.0    0.000000    2.000\n5     n_male_artists numeric    0.0    1.00000    1.0    1.000000    9.000\n6      year_acquired numeric 1930.0 1957.00000 1975.0 1996.000000 2017.000\n7       year_created numeric 1872.0 1933.00000 1956.0 1972.000000 2017.000\n8           depth_cm numeric    0.0    0.00000    0.0    6.762503  647.700\n9          height_cm numeric    7.0   61.27760  106.6  182.200000 1011.000\n10          width_cm numeric    4.1   60.96012   99.0  170.000000 4663.449\n           mean          sd    n missing\n1  1911.8624833  27.9805659 2247       6\n2  1974.8152709  25.5436102 1624     629\n3     1.0093250   0.2116175 2252       1\n4     0.1136263   0.3188231 2253       0\n5     0.8952508   0.3717841 2253       0\n6  1975.5494652  23.3948403 2244       9\n7  1953.5284698  27.9329429 2248       5\n8    10.3302084  44.0511526  298    1955\n9   123.8797591  79.6186437 2253       0\n10  131.9462530 152.6158431 2253       0"
  },
  {
    "objectID": "content/labs/r-labs/tidy/moma.html#getting-to-know-your-data-make-up-some-questions",
    "href": "content/labs/r-labs/tidy/moma.html#getting-to-know-your-data-make-up-some-questions",
    "title": "Lab 05: What use is a Book without any Pictures?",
    "section": "\n6 Getting to Know your data: Make up some Questions",
    "text": "6 Getting to Know your data: Make up some Questions\n\n\n\n\n\n\nChallenge #1\n\n\n\nTry to answer all of these questions using dplyr. Answers are below but try them on your own first!\n\nHow many paintings (rows) are in moma?\nHow many variables (columns) are in moma?\nWhat is the first painting acquired by MoMA? Which year? Which artist? What title?\nWhat is the oldest painting in the collection? Which year? Which artist? What title?\nHow many distinct artists are there?\nWhich artist has the most paintings in the collection? How many paintings are by this artist?\nHow many paintings by male vs female artists?\nHow many paintings acquired by year, and by gender of artist, over time?\n\nIf you want more:\n\nHow many artists of each gender are there?\nIn what year were the most paintings acquired? Created?\nIn what year was the first painting by a (solo) female artist acquired? When was that painting created? Which artist? What title?"
  },
  {
    "objectID": "content/labs/r-labs/tidy/moma.html#how-many-paintings-how-many-variables",
    "href": "content/labs/r-labs/tidy/moma.html#how-many-paintings-how-many-variables",
    "title": "Lab 05: What use is a Book without any Pictures?",
    "section": "\n7 How many paintings? How many Variables?",
    "text": "7 How many paintings? How many Variables?\n\nHow many rows/observations are in moma?\n\nHow many variables are in moma?\n\n\n\n\n\n\nTip\n\n\n\nHint: These questions can be answered using the dplyr function glimpse.\n\n\n\n\n\nlibrary(dplyr)\nmoma\n\n\n\n  \n\n\nglimpse(moma)\n\nRows: 2,253\nColumns: 23\n$ title             &lt;chr&gt; \"Rope and People, I\", \"Fire in the Evening\", \"Portra…\n$ artist            &lt;chr&gt; \"Joan Miró\", \"Paul Klee\", \"Paul Klee\", \"Pablo Picass…\n$ artist_bio        &lt;chr&gt; \"(Spanish, 1893–1983)\", \"(German, born Switzerland. …\n$ artist_birth_year &lt;dbl&gt; 1893, 1879, 1879, 1881, 1880, 1879, 1943, 1880, 1839…\n$ artist_death_year &lt;dbl&gt; 1983, 1940, 1940, 1973, 1946, 1953, 1977, 1950, 1906…\n$ num_artists       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ n_female_artists  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ n_male_artists    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ artist_gender     &lt;chr&gt; \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"Mal…\n$ year_acquired     &lt;dbl&gt; 1936, 1970, 1966, 1955, 1939, 1968, 1997, 1931, 1934…\n$ year_created      &lt;dbl&gt; 1935, 1929, 1927, 1919, 1925, 1919, 1970, 1929, 1885…\n$ circumference_cm  &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ depth_cm          &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ diameter_cm       &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ height_cm         &lt;dbl&gt; 104.8, 33.8, 60.3, 215.9, 50.8, 129.2, 200.0, 54.6, …\n$ length_cm         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ width_cm          &lt;dbl&gt; 74.6, 33.3, 36.8, 78.7, 54.0, 89.9, 200.0, 38.1, 96.…\n$ seat_height_cm    &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ purchase          &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n$ gift              &lt;lgl&gt; TRUE, FALSE, FALSE, TRUE, TRUE, FALSE, TRUE, TRUE, F…\n$ exchange          &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALS…\n$ classification    &lt;chr&gt; \"Painting\", \"Painting\", \"Painting\", \"Painting\", \"Pai…\n$ department        &lt;chr&gt; \"Painting & Sculpture\", \"Painting & Sculpture\", \"Pai…"
  },
  {
    "objectID": "content/labs/r-labs/tidy/moma.html#what-is-the-first-painting-acquired",
    "href": "content/labs/r-labs/tidy/moma.html#what-is-the-first-painting-acquired",
    "title": "Lab 05: What use is a Book without any Pictures?",
    "section": "\n8 What is the first painting acquired?",
    "text": "8 What is the first painting acquired?\n\nWhat is the first painting acquired by MoMA (since they started tracking)?\nWhat year was it acquired?\nWhich artist?\nWhat title?\n\n\n\n\n\n\n\nTip\n\n\n\nHint: These questions can be answered by combining two dplyr functions: select and arrange.\n\n\n\nmoma %&gt;% \n  select(artist, title, year_acquired) %&gt;% \n  arrange(year_acquired)"
  },
  {
    "objectID": "content/labs/r-labs/tidy/moma.html#what-is-the-oldest-painting-in-the-moma-collection",
    "href": "content/labs/r-labs/tidy/moma.html#what-is-the-oldest-painting-in-the-moma-collection",
    "title": "Lab 05: What use is a Book without any Pictures?",
    "section": "\n9 What is the oldest painting in the MoMA collection?",
    "text": "9 What is the oldest painting in the MoMA collection?\n\nWhat is the oldest painting in the MoMA collection historically (since they started tracking)?\nWhat year was it created?\nWhich artist?\nWhat title?\n\n\n\n\n\n\n\nTip\n\n\n\nHint: These questions can be answered by combining two dplyr functions: select and arrange.\n\n\n\nmoma %&gt;% \n  select(artist, title, year_created) %&gt;% \n  arrange(year_created)\n\n\n\n  \n\n\n\n\noldest &lt;- moma %&gt;% \n  select(artist, title, year_created) %&gt;% \n  arrange(year_created) %&gt;% \n  slice(1)\noldest\n\n\n\n  \n\n\n\nTo do inline comments, I could say that the oldest painting is Landscape at Daybreak, painted by Odilon Redon in 1872."
  },
  {
    "objectID": "content/labs/r-labs/tidy/moma.html#how-many-artists",
    "href": "content/labs/r-labs/tidy/moma.html#how-many-artists",
    "title": "Lab 05: What use is a Book without any Pictures?",
    "section": "\n10 How many artists?",
    "text": "10 How many artists?\n\nHow many distinct artists are there?\n\n\n\n\n\n\n\nTip\n\n\n\nHint: Try dplyr::distinct.\n\n\n\nmoma %&gt;% \n  distinct(artist)\n\n\n\n  \n\n\n\nYou could add a tally() too to get just the number of rows. You can also then use pull() to get that single number out of the tibble:\n\nnum_artists &lt;- moma %&gt;% \n  distinct(artist) %&gt;% \n  tally() %&gt;% \n  pull()\nnum_artists\n\n[1] 989\n\n\nThen I can refer to this number in inline comments like: there are 989 total."
  },
  {
    "objectID": "content/labs/r-labs/tidy/moma.html#which-artist-has-the-most-paintings",
    "href": "content/labs/r-labs/tidy/moma.html#which-artist-has-the-most-paintings",
    "title": "Lab 05: What use is a Book without any Pictures?",
    "section": "\n11 Which artist has the most paintings?",
    "text": "11 Which artist has the most paintings?\n\nWhich artist has the most paintings ever owned by moma?\nHow many paintings in the MoMA collection by that artist?\n\n\n\n\n\n\n\nTip\n\n\n\nHint: Try dplyr::count. Use ?count to figure out how to sort the output.\n\n\n\nmoma %&gt;% \n  count(artist, sort = TRUE)\n\n\n\n  \n\n\n\nIn the ?count documentation, it says: “count and tally are designed so that you can call them repeatedly, each time rolling up a level of detail.” Try running count() again (leave parentheses empty) on your last code chunk. ( before the slice())\n\nmoma %&gt;% \n  count(artist, sort = TRUE) %&gt;% \n  count()"
  },
  {
    "objectID": "content/labs/r-labs/tidy/moma.html#how-many-paintings-by-male-vs-female-artists",
    "href": "content/labs/r-labs/tidy/moma.html#how-many-paintings-by-male-vs-female-artists",
    "title": "Lab 05: What use is a Book without any Pictures?",
    "section": "\n12 How many paintings by male vs female artists?",
    "text": "12 How many paintings by male vs female artists?\n\nmoma %&gt;% \n  count(artist_gender)\n\n\n\n  \n\n\n\nNow together we’ll count the number of artists by gender. You’ll need to give count two variable names in the parentheses: artist_gender and artist.\n\nmoma %&gt;% \n  count(artist_gender, artist, sort = TRUE) \n\n\n\n  \n\n\n\nThis output is not super helpful as we already know that Pablo Picasso has 55 paintings in the MoMA collection. But how can we find out which female artist has the most paintings? We have a few options. Let’s first add a filter for females.\n\nmoma %&gt;% \n  count(artist_gender, artist, sort = TRUE) %&gt;% \n  filter(artist_gender == \"Female\")\n\n\n\n  \n\n\n\nAnother option is to use another dplyr function called top_n(). Use ?top_n to see how it works. How it won’t work in this context:\n\nmoma %&gt;% \n  count(artist_gender, artist, sort = TRUE) %&gt;% \n  slice_max(n = 2, order_by = n)\n\n\n\n  \n\n\n\nHow it will work better is following a group_by(artist_gender):\n\nmoma %&gt;% \n  count(artist_gender, artist, sort = TRUE) %&gt;% \n  group_by(artist_gender) %&gt;% \n  slice_max(n = 1, order_by = n)\n\n\n\n  \n\n\n\nNow we can see that Sherrie Levine has 12 paintings. This is a pretty far cry from the 55 paintings by Pablo Picasso."
  },
  {
    "objectID": "content/labs/r-labs/tidy/moma.html#how-many-artists-of-each-gender-are-there",
    "href": "content/labs/r-labs/tidy/moma.html#how-many-artists-of-each-gender-are-there",
    "title": "Lab 05: What use is a Book without any Pictures?",
    "section": "\n13 How many artists of each gender are there?",
    "text": "13 How many artists of each gender are there?\nThis is a harder question to answer than you think! This is because the level of observation in our current moma dataset is unique paintings. We have multiple paintings done by the same artists though, so counting just the number of unique paintings is different than counting the number of unique artists.\nRemember how count can be used back-to-back to roll up a level of detail? Try running count(artist_gender) again on your last code chunk.\n\nmoma %&gt;% \n  count(artist_gender, artist) %&gt;% \n  count(artist_gender)\n\n\n\n  \n\n\n\nThis output takes the previous table (made with count(artist_gender, artist)), and essentially ignores the n column. So we no longer care about how many paintings each individual artist created. Instead, we want to count the rows in this new table where each row is a unique artist. By counting by artist_gender in the last line, we are grouping by levels of that variable (so Female/Male/NA) and nn is the number of unique artists for each gender category recorded."
  },
  {
    "objectID": "content/labs/r-labs/tidy/moma.html#when-were-the-most-paintings-in-the-collection-acquired",
    "href": "content/labs/r-labs/tidy/moma.html#when-were-the-most-paintings-in-the-collection-acquired",
    "title": "Lab 05: What use is a Book without any Pictures?",
    "section": "\n14 When were the most paintings in the collection acquired?",
    "text": "14 When were the most paintings in the collection acquired?\n\nmoma %&gt;% \n  count(year_acquired, sort = TRUE)"
  },
  {
    "objectID": "content/labs/r-labs/tidy/moma.html#when-were-the-most-paintings-in-the-collection-created",
    "href": "content/labs/r-labs/tidy/moma.html#when-were-the-most-paintings-in-the-collection-created",
    "title": "Lab 05: What use is a Book without any Pictures?",
    "section": "\n15 When were the most paintings in the collection created?",
    "text": "15 When were the most paintings in the collection created?\nWhich variable should we count?\n\nmoma %&gt;% \n  count(year_created, sort = TRUE)"
  },
  {
    "objectID": "content/labs/r-labs/tidy/moma.html#what-about-the-first-painting-by-a-solo-female-artist",
    "href": "content/labs/r-labs/tidy/moma.html#what-about-the-first-painting-by-a-solo-female-artist",
    "title": "Lab 05: What use is a Book without any Pictures?",
    "section": "\n16 What about the first painting by a solo female artist?",
    "text": "16 What about the first painting by a solo female artist?\n\n\n\n\n\n\nTip\n\n\n\nHint: Try combining three dplyr functions: filter, select, and arrange.\n\n\nWhen was the first painting by a solo female artist acquired?\n\nmoma %&gt;% \n  filter(num_artists == 1 & n_female_artists == 1) %&gt;% \n  select(title, artist, year_acquired, year_created) %&gt;% \n  arrange(year_acquired)\n\n\n\n  \n\n\n\nWhat is the oldest painting by a solo female artist, and when was it created?\n\nmoma %&gt;% \n  filter(num_artists == 1 & n_female_artists == 1) %&gt;% \n  select(title, artist, year_acquired, year_created) %&gt;% \n  arrange(year_created)\n\n\n\n  \n\n\n\n\n# or, because artist_gender is missing when num_artists &gt; 1\nmoma %&gt;% \n  filter(artist_gender == \"Female\") %&gt;% \n  select(title, artist, year_acquired, year_created) %&gt;% \n  arrange(year_acquired)"
  },
  {
    "objectID": "content/labs/r-labs/tidy/moma.html#how-many-paintings-acquired-by-year-and-by-gender-of-artist-over-time",
    "href": "content/labs/r-labs/tidy/moma.html#how-many-paintings-acquired-by-year-and-by-gender-of-artist-over-time",
    "title": "Lab 05: What use is a Book without any Pictures?",
    "section": "\n17 How many paintings acquired by year, and by gender of artist, over time?",
    "text": "17 How many paintings acquired by year, and by gender of artist, over time?\n\nmoma %&gt;% group_by(year_created, artist_gender) %&gt;% count()"
  },
  {
    "objectID": "content/posts/01-intro/history.html",
    "href": "content/posts/01-intro/history.html",
    "title": "Boston Terrier",
    "section": "",
    "text": "The Boston Terrier is a breed of dog originating in the United States of America. This “American Gentleman” was accepted in 1893 by the American Kennel Club as a non-sporting breed.1 Color and markings are important when distinguishing this breed from the AKC standard. They should be either black, brindle or seal with white markings.2 Boston Terriers are small and compact with a short tail and erect ears. The AKC says they are highly intelligent and very easily trained.3 They are friendly and can be stubborn at times. The average life span of a Boston Terrier is around 11 to 13 years.4 The American Kennel Club ranked the Boston Terrier as the 21st most popular breed in 2019.5"
  },
  {
    "objectID": "content/posts/01-intro/history.html#history",
    "href": "content/posts/01-intro/history.html#history",
    "title": "Boston Terrier",
    "section": "History",
    "text": "History\n\n\n\n\n\n\nTerrier Seated\n\n\n\n\nThe Boston terrier breed originated around 1875, when Robert C. Hooper of Boston purchased from Edward Burnett a dog named Judge (known later as Hooper’s Judge), which was of a bull and terrier type lineage. Hooper’s Judge is directly related to the original bull and terrier breeds of the 19th and early 20th centuries. The American Kennel Club cites Hooper’s Judge as the ancestor of all true modern Boston Terriers. Judge weighed about 32 pounds (15 kg).\n\n\n\n\nJudge was bred to Burnett’s Gyp (or Kate). Gyp was a white bulldog-type female, owned by Edward Burnett, of Southboro, Massachusetts. She weighed about 20 pounds (9.1 kg), was stocky and strong and had the typical blocky head now shown in Bostons. From this foundation of the breed, subsequent breeders refined the breed into its modern day presentation. Bred down in size from fighting dogs of the bull and terrier types, the Boston Terrier originally weighed up to 44 pounds (20 kg) (Old Boston Bulldogs).\n\n\n\n\n\n\n\nA young male Boston Terrier with a Brown brindle coat\n\nThe breed was first shown in Boston in 1870. By 1889 the breed had become sufficiently popular in Boston that fanciers formed the American Bull Terrier Club, the breed’s nickname, “roundheads”. Shortly after, at the suggestion of James Watson (a noted writer and authority), the club changed its name to the Boston Terrier Club and in 1893 it was admitted to membership in the American Kennel Club, thus making it the first US breed to be recognized. It is one of a small number of breeds to have originated in the United States. The Boston Terrier was the first non-sporting dog breed in the US.\nIn the early years, the color and markings were not very important to the breed’s standard. By the 20th century the breed’s distinctive markings and color were written into the standard, becoming an essential feature. The Boston Terrier has lost most of its aggressive nature, preferring the company of humans, although some males will still challenge other dogs if they feel their territory is being invaded. Boston University has used Rhett the Boston Terrier as their mascot since 1922. Wofford College in Spartanburg, SC has had a live Boston Terrier mascot named Blitz since 2003 that attends home football games. The Boston Terrier has also been the official state dog of Massachusetts since 1979."
  },
  {
    "objectID": "content/posts/01-intro/history.html#description",
    "href": "content/posts/01-intro/history.html#description",
    "title": "Boston Terrier",
    "section": "Description",
    "text": "Description\nThe Boston Terrier is a compactly built, well-proportioned dog. It has a square-looking head with erect ears and a slightly arched neck. The muzzle is short and generally wrinkle-free, with an even or a slightly undershot bite. The chest is broad and the tail is short. According to international breed standards, the dog should weigh no more than 25 pounds (11 kg). Boston Terriers usually stand up to 15–17 inches (380–430 mm) at the withers.\nThe American Kennel Club divides the breed into three classes: under 15 pounds, 15 pounds and under 20 pounds, 20 pounds and not exceeding 25 pounds.\n\nCoat and color\nThe Boston Terrier is characteristically marked with white in proportion to either black, brindle, seal (color of a wet seal, a very dark brown that looks black except in the bright sun), or a combination of the three. Any other color is not accepted as a Boston Terrier by the American Kennel Club, as they are usually obtained by crossbreeding with other breeds and the dog loses its characteristic “tuxedo” appearance.6 Any Boston Terrier from AKC parentage regardless of the color, or if it is a splash or has a blue eye or weak ears, can be and are registered by the AKC and participate in any AKC sporting events.\n\n\n\n\nA female Boston Terrier with a black coat\n\nAccording to the American Kennel Club, the Boston Terrier’s markings are broken down into two categories: Required, which consists of a white chest, white muzzle band, and a white band between the eyes; and Desired, which includes the Required markings plus a white collar, white on the forelegs, forelegs, up to the hocks on the rear legs. For conformation showing, symmetrical markings are preferred. Due to the Boston Terrier’s markings resembling formal wear, in addition to its refined and pleasant personality, the breed is commonly referred to as “the American Gentleman.”\n\n\n\n\nAn adult male Boston Terrier with a black coat"
  },
  {
    "objectID": "content/posts/01-intro/history.html#temperament",
    "href": "content/posts/01-intro/history.html#temperament",
    "title": "Boston Terrier",
    "section": "Temperament",
    "text": "Temperament\nBoston Terrier is a gentle breed that typically has a strong, happy-go-lucky, and friendly personality with a merry sense of humor. Boston Terriers are generally eager to please their owner and can be easily trained. They can be very protective of their owners, which may result in aggressive and territorial behavior toward other pets and strangers. The breed requires only a minimal amount of grooming.\nWhile originally bred for fighting as well as hunting rats in garment factories, they were later bred for companionship. They are not considered terriers by the American Kennel Club, however, but are part of the non-sporting group.\nBoth females and males are generally quiet and bark only when necessary, Their usually sensible attitude toward barking makes them excellent choices for apartment dwellers. They enjoy being around people, get along well with children, the elderly, other canines, and non-canine pets, if properly socialized."
  },
  {
    "objectID": "content/posts/01-intro/history.html#grooming",
    "href": "content/posts/01-intro/history.html#grooming",
    "title": "Boston Terrier",
    "section": "Grooming",
    "text": "Grooming\nWith a short, shiny, smooth coat, Boston Terriers require little grooming. Bostons produce light shedding, and weekly brushing of their fine coat is effective at removing loose hair. Brushing promotes the health of the coat because it distributes skin oils, and it also encourages new hair growth. Occasional bathing is suitable for the breed.7\nThe nails of Boston Terriers require regular trimming. Overgrown nails not only have the potential to inflict pain on the breed, but they can also make walking difficult or tear off after getting snagged on something.\nSimilarly to nail trimming, tooth brushing should also be done regularly to promote good oral health. The risk of the breed developing oral pain, gum infection, or bad breath can be decreased with regular tooth brushing that removes plaque buildup and other bacteria. In addition, poor dental hygiene can lead to tooth root abscesses that can lead to damage around the tissue and eventually lead to the loss of teeth."
  },
  {
    "objectID": "content/posts/01-intro/history.html#footnotes",
    "href": "content/posts/01-intro/history.html#footnotes",
    "title": "Boston Terrier",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMeade, Scottee (2000). The Boston Terrier: An Owner’s Guide to a Happy Healthy Pet. Howell Book House. ISBN 1-58245-159-1.↩︎\n“Boston Terrier Dog Breed Information”. Akc.org. Retrieved 11 December 2017.↩︎\n“Boston Terrier - American Kennel Club”. Akc.org.↩︎\n“The Boston Terrier Club Of America”. Bostonterrierclubofamerica.org.↩︎\n“Most Popular Dog Breeds - Full Ranking List”. Akc.org. Retrieved 11 December 2017.↩︎\n“Boston Terrier Dog Breed Information”. Akc.org. Retrieved 11 December 2017.↩︎\n“Get to Know the Boston Terrier”, ‘The American Kennel Club’, retrieved 19 May 2014↩︎"
  },
  {
    "objectID": "content/posts/02-authoring/callout-boxes.html",
    "href": "content/posts/02-authoring/callout-boxes.html",
    "title": "Callout Boxes",
    "section": "",
    "text": ":::{.callout-note}\nNote that there are five types of callouts, including:\n`note`, `warning`, `important`, `tip`, and `caution`.\n:::\n\n:::{.callout-tip}\n## Tip With Caption\n\nThis is an example of a callout with a caption.\n:::\n\n:::{.callout-caution collapse=\"true\"}\n## Expand To Learn About Collapse\n\nThis is an example of a 'folded' caution callout that can be expanded by the user. You can use `collapse=\"true\"` to collapse it by default or `collapse=\"false\"` to make a collapsible callout that is expanded by default.\n:::"
  },
  {
    "objectID": "content/posts/02-authoring/callout-boxes.html#callout-markdown-syntax",
    "href": "content/posts/02-authoring/callout-boxes.html#callout-markdown-syntax",
    "title": "Callout Boxes",
    "section": "",
    "text": ":::{.callout-note}\nNote that there are five types of callouts, including:\n`note`, `warning`, `important`, `tip`, and `caution`.\n:::\n\n:::{.callout-tip}\n## Tip With Caption\n\nThis is an example of a callout with a caption.\n:::\n\n:::{.callout-caution collapse=\"true\"}\n## Expand To Learn About Collapse\n\nThis is an example of a 'folded' caution callout that can be expanded by the user. You can use `collapse=\"true\"` to collapse it by default or `collapse=\"false\"` to make a collapsible callout that is expanded by default.\n:::"
  },
  {
    "objectID": "content/posts/03-computation/inline-code.html",
    "href": "content/posts/03-computation/inline-code.html",
    "title": "Inline Code",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   1.0.0 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(palmerpenguins)\nThe dataset contains 344 penguin size measurements from Adelie, Gentoo, Chinstrap species across Torgersen, Biscoe, Dream islands.."
  },
  {
    "objectID": "content/posts/03-computation/inline-code.html#what-about-formatting",
    "href": "content/posts/03-computation/inline-code.html#what-about-formatting",
    "title": "Inline Code",
    "section": "What about formatting?",
    "text": "What about formatting?\n\npen_summary &lt;- penguins |&gt; \n  group_by(species) |&gt; \n  summarize(avg_mass = mean(body_mass_g, na.rm = TRUE))\n\nThe average body mass by species is 3700.6622517, 3733.0882353, 5076.0162602.\nWe can do better!\n\nbody_mass &lt;- scales::label_number(big.mark = \",\", accuracy = 0.1, suffix = \"g\")(pull(pen_summary, avg_mass))\n\nbody_mass\n\n[1] \"3,700.7g\" \"3,733.1g\" \"5,076.0g\"\n\n\nThe average body mass by species is 3,700.7g, 3,733.1g, 5,076.0g.\nWe can still do better!\n\nmass_reporter &lt;- glue::glue_collapse(body_mass, sep = \", \", last = \", and \")\n\nThe average body mass by species is 3,700.7g, 3,733.1g, and 5,076.0g."
  },
  {
    "objectID": "content/posts/03-computation/inline-code.html#reporting-with-lists",
    "href": "content/posts/03-computation/inline-code.html#reporting-with-lists",
    "title": "Inline Code",
    "section": "Reporting with lists",
    "text": "Reporting with lists\nCredit to TJ Mahr\n\nknitted &lt;- list(\n  when = format(Sys.Date()),\n  with = system(\"quarto --version\", intern = TRUE)\n)\n\nReported prepared on 2023-01-05 with quarto version 1.2.280."
  },
  {
    "objectID": "content/posts/04-static/bootswatch-themed.html",
    "href": "content/posts/04-static/bootswatch-themed.html",
    "title": "Bootswatch Themed QMD",
    "section": "",
    "text": "Plots\nThis is the world’s most amazing plot. Everyone loves penguins.\nPenguins are fancy.\nThere are 344 in the dataset of interest.\n\nCodepenguins %&gt;% \n  ggplot(aes(x = bill_length_mm, y = body_mass_g, color = species)) +\n  geom_point()\n\n\n\n\nYou can also include tables. Tables are super cool. I love tables and I love penguins. I am subject to oversight by the penguin overlords. The eternals are friends with the penguins.\n\nCodepenguins %&gt;% \n  na.omit() %&gt;% \n  group_by(species, sex) %&gt;% \n  rename(\n    body_mass = body_mass_g, bill_length = bill_length_mm, \n    bill_depth = bill_depth_mm\n    ) %&gt;% \n  summarise(\n    n = n(),\n    across(\n      .cols = c(body_mass, bill_length, bill_depth),\n      .fns = list(mean = mean, sd = sd)\n    ),\n    .groups = \"drop\"\n  ) %&gt;% \n  gt(rowname_col = \"sex\") %&gt;% \n  cols_label(\n    n = \"N\", body_mass_mean = \"Mean\", body_mass_sd = \"SD\", \n    bill_length_mean= \"Mean\", bill_length_sd = \"SD\",\n    bill_depth_mean = \"Mean\", bill_depth_sd = \"SD\"\n    ) %&gt;% \n  gt::tab_spanner(\n    label = \"Body Mass (g)\",\n    columns = 4:5\n  ) %&gt;% \n  gt::tab_spanner(\n    label = \"Bill Length (mm)\",\n    columns = 6:7\n  ) %&gt;% \n  gt::tab_spanner(\n    label = \"Bill Depth (mm)\",\n    columns = 8:9\n  ) %&gt;% \n  fmt_number(\n    columns = c(where(is.numeric), -n)\n  )\n\n\n\n\n\n\n\n\nspecies\nN\nBody Mass (g)\nBill Length (mm)\nBill Depth (mm)\n\n\nMean\nSD\nMean\nSD\nMean\nSD\n\n\n\n\nfemale\nAdelie\n73\n3,368.84\n269.38\n37.26\n2.03\n17.62\n0.94\n\n\nmale\nAdelie\n73\n4,043.49\n346.81\n40.39\n2.28\n19.07\n1.02\n\n\nfemale\nChinstrap\n34\n3,527.21\n285.33\n46.57\n3.11\n17.59\n0.78\n\n\nmale\nChinstrap\n34\n3,938.97\n362.14\n51.09\n1.56\n19.25\n0.76\n\n\nfemale\nGentoo\n58\n4,679.74\n281.58\n45.56\n2.05\n14.24\n0.54\n\n\nmale\nGentoo\n61\n5,484.84\n313.16\n49.47\n2.72\n15.72\n0.74\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/posts/04-static/penguin-report.html",
    "href": "content/posts/04-static/penguin-report.html",
    "title": "Penguins Distilled",
    "section": "",
    "text": "Diagram of penguin head with indication of bill length and bill depth.\n\nLiterate Programming\nPer Donald Knuth\n\nA programming paradigm introduced by Donald Knuth in which a computer program is given an explanation of its logic in a natural language, such as English, interspersed with snippets of macros and traditional source code, from which compilable source code can be generated.\n\nInitial explore\nWe can do a quick exploration of the data with skimr::skim(). This will report the counts of various variables, along with some basic descriptive statistics. The skimr package is fantastic for quickly getting a sense of your datasets.\nAhead of skimr there are 344 penguins in this dataset, and the unique species are Adelie, Gentoo, Chinstrap.\nPer the rOpenSci skimr docs:\n\nskimr provides a frictionless approach to summary statistics which conforms to the principle of least surprise, displaying summary statistics the user can skim quickly to understand their data. It handles different data types and returns a skim_df object which can be included in a pipeline or displayed nicely for the human reader.\n\n\npenguins %&gt;% \n  group_by(species) %&gt;% \n  skimr::skim() %&gt;% \n  select(-contains(\"numeric.p\"))\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n344\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nspecies\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nspecies\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nisland\nAdelie\n0\n1.00\nFALSE\n3\nDre: 56, Tor: 52, Bis: 44\n\n\nisland\nChinstrap\n0\n1.00\nFALSE\n1\nDre: 68, Bis: 0, Tor: 0\n\n\nisland\nGentoo\n0\n1.00\nFALSE\n1\nBis: 124, Dre: 0, Tor: 0\n\n\nsex\nAdelie\n6\n0.96\nFALSE\n2\nfem: 73, mal: 73\n\n\nsex\nChinstrap\n0\n1.00\nFALSE\n2\nfem: 34, mal: 34\n\n\nsex\nGentoo\n5\n0.96\nFALSE\n2\nmal: 61, fem: 58\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nspecies\nn_missing\ncomplete_rate\nmean\nsd\nhist\n\n\n\nbill_length_mm\nAdelie\n1\n0.99\n38.79\n2.66\n▁▆▇▆▁\n\n\nbill_length_mm\nChinstrap\n0\n1.00\n48.83\n3.34\n▂▇▇▅▁\n\n\nbill_length_mm\nGentoo\n1\n0.99\n47.50\n3.08\n▃▇▆▁▁\n\n\nbill_depth_mm\nAdelie\n1\n0.99\n18.35\n1.22\n▂▆▇▃▁\n\n\nbill_depth_mm\nChinstrap\n0\n1.00\n18.42\n1.14\n▅▇▇▆▂\n\n\nbill_depth_mm\nGentoo\n1\n0.99\n14.98\n0.98\n▅▇▇▆▂\n\n\nflipper_length_mm\nAdelie\n1\n0.99\n189.95\n6.54\n▁▆▇▅▁\n\n\nflipper_length_mm\nChinstrap\n0\n1.00\n195.82\n7.13\n▁▅▇▅▂\n\n\nflipper_length_mm\nGentoo\n1\n0.99\n217.19\n6.48\n▂▇▇▆▃\n\n\nbody_mass_g\nAdelie\n1\n0.99\n3700.66\n458.57\n▅▇▇▃▂\n\n\nbody_mass_g\nChinstrap\n0\n1.00\n3733.09\n384.34\n▁▅▇▃▁\n\n\nbody_mass_g\nGentoo\n1\n0.99\n5076.02\n504.12\n▃▇▇▇▂\n\n\nyear\nAdelie\n0\n1.00\n2008.01\n0.82\n▇▁▇▁▇\n\n\nyear\nChinstrap\n0\n1.00\n2007.97\n0.86\n▇▁▆▁▇\n\n\nyear\nGentoo\n0\n1.00\n2008.08\n0.79\n▆▁▇▁▇\n\n\n\n\n\nSpecific statistics\nWe can also explore specific statistics\nThe penguins split by species show a specific relationship between weight and flipper length, where the Adelie female penguins are the lighest and have the shortest flippers.\n\npenguins %&gt;% \n  group_by(species, sex) %&gt;% \n  summarize(\n    n = n(), \n    weight = mean(body_mass_g, na.rm = TRUE),\n    flipper_length = mean(flipper_length_mm, na.rm = TRUE)\n    ) %&gt;% \n  arrange(desc(weight))\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 5\n# Groups:   species [3]\n  species   sex        n weight flipper_length\n  &lt;fct&gt;     &lt;fct&gt;  &lt;int&gt;  &lt;dbl&gt;          &lt;dbl&gt;\n1 Gentoo    male      61  5485.           222.\n2 Gentoo    female    58  4680.           213.\n3 Gentoo    &lt;NA&gt;       5  4588.           216.\n4 Adelie    male      73  4043.           192.\n5 Chinstrap male      34  3939.           200.\n6 Adelie    &lt;NA&gt;       6  3540            186.\n7 Chinstrap female    34  3527.           192.\n8 Adelie    female    73  3369.           188.\n\n\nLooks like the Adelie are the lightest penguin. I want to see their distribution along with the overall distribution.\n\npenguins %&gt;% \n  filter(is.na(sex))\n\n# A tibble: 11 × 8\n   species island    bill_length_mm bill_depth_mm flipper_…¹ body_…² sex    year\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;      &lt;int&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt;\n 1 Adelie  Torgersen           NA            NA           NA      NA &lt;NA&gt;   2007\n 2 Adelie  Torgersen           34.1          18.1        193    3475 &lt;NA&gt;   2007\n 3 Adelie  Torgersen           42            20.2        190    4250 &lt;NA&gt;   2007\n 4 Adelie  Torgersen           37.8          17.1        186    3300 &lt;NA&gt;   2007\n 5 Adelie  Torgersen           37.8          17.3        180    3700 &lt;NA&gt;   2007\n 6 Adelie  Dream               37.5          18.9        179    2975 &lt;NA&gt;   2007\n 7 Gentoo  Biscoe              44.5          14.3        216    4100 &lt;NA&gt;   2007\n 8 Gentoo  Biscoe              46.2          14.4        214    4650 &lt;NA&gt;   2008\n 9 Gentoo  Biscoe              47.3          13.8        216    4725 &lt;NA&gt;   2009\n10 Gentoo  Biscoe              44.5          15.7        217    4875 &lt;NA&gt;   2009\n11 Gentoo  Biscoe              NA            NA           NA      NA &lt;NA&gt;   2009\n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n\n\n\nsmaller &lt;- palmerpenguins::penguins %&gt;% \n  filter(species == \"Adelie\", \n         !is.na(body_mass_g))\n\nCleanup the data\nIf you noticed above, there was some NA or missing data. We can remove those rows for now.\n\npenguins_clean &lt;- penguins %&gt;% \n  na.omit() %&gt;% \n  mutate(species_num = as.numeric(species))\n\nPlot Section\nLet’s move on to some plots, for the overall distributions and for just the Adelie penguins. The overall distribution of the data by species shows some overlap in body weight for Adelie/Chinstrap, but more of a separation for the Gentoo penguins.\n\npenguins %&gt;% \n  ggplot(aes(body_mass_g, fill = species)) + \n  geom_density(color = \"white\", alpha = 0.5) +\n  scale_fill_manual(values = c(\"darkorange\",\"purple\",\"cyan4\")) +\n  labs(x = \"Penguin Bins\")\n\n\n\n\nWhen we compare just within the Adelie penguins, we can see more of a specific separation of male vs female. However, there is still a decent amount of overlapping data.\n\npenguin_plot &lt;- smaller %&gt;% \n  filter(!is.na(sex)) %&gt;% \n  ggplot(aes(body_mass_g, fill = sex)) + \n  geom_density(color = \"white\", alpha = 0.5) +\n    scale_fill_manual(values = c(\"darkorange\",\"purple\")) +\n  labs(x = \"Penguin Bins\")\n\npenguin_plot\n\n\n\n\nLastly we can fit a basic linear model comparing body weight in grams to the flipper length of the penguins by specific species. There is a strong linear relationship, although it’s a bit difficult to distinguish between Chinstrap and Adelie penguins.\n\npenguin_size_plot &lt;- penguins_clean %&gt;% \n  ggplot(aes(x = body_mass_g, y = flipper_length_mm, color = species)) + \n  scale_color_manual(values = c(\"darkorange\",\"purple\",\"cyan4\")) +\n  geom_point(size = 2, alpha = 0.5) +\n  labs(x = \"Mass (g)\", y = \"Flipper Length (mm)\") +\n  geom_smooth(aes(group = \"none\"), method = \"lm\")\n\npenguin_size_plot\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nWe can save the overall distribution and the linear model plot.\n\nggsave(\"penguin-dist.png\", penguin_plot, \n  dpi = \"retina\", height = 8, width = 8)\n\nggsave(\"penguin-smooth.png\", penguin_size_plot, \n  dpi = \"retina\", height = 8, width = 8)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nModeling section\nMoving on to some basic modeling we can see if what kind of relationships we observe in the data. Note that I’m really not following any plan, just indicating how you can fit some different models all at once with dplyr + broom.\n\nmodel_inputs &lt;- tibble(\n  model_form = c(\n    list(flipper_length_mm ~ body_mass_g),\n    list(species_num ~ bill_length_mm + body_mass_g + sex),\n    list(flipper_length_mm ~ bill_length_mm + species)\n    ),\n  data = list(penguins_clean)\n) \n\nmodel_metrics &lt;- model_inputs %&gt;% \n  rowwise(model_form, data) %&gt;% \n  summarize(lm = list(lm(model_form, data = data)), .groups = \"drop\") %&gt;% \n  rowwise(model_form, lm, data) %&gt;% \n  summarise(broom::glance(lm), .groups = \"drop\")\n\nWrap up\nWe can then take the model outcomes and throw them into a quick gt table.\n\nmodel_metrics %&gt;% \n  select(model_form, r.squared:p.value) %&gt;% \n  mutate(model_form = as.character(model_form)) %&gt;% \n  gt::gt() %&gt;% \n  gt::fmt_number(r.squared:statistic) %&gt;% \n  gt::fmt_scientific(p.value) %&gt;% \n  gt::cols_width(\n    model_form ~ px(150)\n  )\n\n\n\n\n\n\nmodel_form\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\n\n\n\nflipper_length_mm ~ body_mass_g\n0.76\n0.76\n6.85\n1,060.30\n3.13 × 10−105\n\n\n\nspecies_num ~ bill_length_mm + body_mass_g + sex\n0.84\n0.84\n0.36\n583.59\n2.45 × 10−131\n\n\n\nflipper_length_mm ~ bill_length_mm + species\n0.83\n0.83\n5.83\n529.22\n1.66 × 10−125\n\n\n\n\n\n\n\nOverall, this was a quick overview of the beauty of literate programming. We have R code that is self-documenting, as we capture our thoughts and the outputs in a single document. We know at some level that the code works since it “logs” the outputs at various stages and could still output to additional log files. To render it has to run successfully in a linear fashion, and it is human readable as code, via the visual editor or even in version control like Git!\n\n\n\n Back to top"
  },
  {
    "objectID": "content/posts/05-presentations/revealjs-penguins.html#section",
    "href": "content/posts/05-presentations/revealjs-penguins.html#section",
    "title": "Penguin Report Presentation",
    "section": "",
    "text": "Diagram of penguin head with indication of bill length and bill depth."
  },
  {
    "objectID": "content/posts/05-presentations/revealjs-penguins.html#literate-programming",
    "href": "content/posts/05-presentations/revealjs-penguins.html#literate-programming",
    "title": "Penguin Report Presentation",
    "section": "Literate Programming",
    "text": "Literate Programming\nPer Donald Knuth\n\nA programming paradigm introduced by Donald Knuth in which a computer program is given an explanation of its logic in a natural language, such as English, interspersed with snippets of macros and traditional source code, from which compilable source code can be generated."
  },
  {
    "objectID": "content/posts/05-presentations/revealjs-penguins.html#specific-statistics",
    "href": "content/posts/05-presentations/revealjs-penguins.html#specific-statistics",
    "title": "Penguin Report Presentation",
    "section": "Specific statistics",
    "text": "Specific statistics\n\npenguins %&gt;% \n  group_by(species, sex) %&gt;% \n  summarize(\n    n = n(), \n    weight = mean(body_mass_g, na.rm = TRUE),\n    flipper_length = mean(flipper_length_mm, na.rm = TRUE)\n    ) %&gt;% \n  arrange(desc(weight))\n\n# A tibble: 8 × 5\n# Groups:   species [3]\n  species   sex        n weight flipper_length\n  &lt;fct&gt;     &lt;fct&gt;  &lt;int&gt;  &lt;dbl&gt;          &lt;dbl&gt;\n1 Gentoo    male      61  5485.           222.\n2 Gentoo    female    58  4680.           213.\n3 Gentoo    &lt;NA&gt;       5  4588.           216.\n4 Adelie    male      73  4043.           192.\n5 Chinstrap male      34  3939.           200.\n6 Adelie    &lt;NA&gt;       6  3540            186.\n7 Chinstrap female    34  3527.           192.\n8 Adelie    female    73  3369.           188."
  },
  {
    "objectID": "content/posts/05-presentations/revealjs-penguins.html#cleanup-the-data",
    "href": "content/posts/05-presentations/revealjs-penguins.html#cleanup-the-data",
    "title": "Penguin Report Presentation",
    "section": "Cleanup the data",
    "text": "Cleanup the data\nIf you noticed above, there was some NA or missing data. We can remove those rows for now.\n\npenguins_clean &lt;- penguins %&gt;% \n  na.omit() %&gt;% \n  mutate(species_num = as.numeric(species))"
  },
  {
    "objectID": "content/posts/05-presentations/revealjs-penguins.html#plot-section",
    "href": "content/posts/05-presentations/revealjs-penguins.html#plot-section",
    "title": "Penguin Report Presentation",
    "section": "Plot Section",
    "text": "Plot Section\nLet’s move on to some plots, for the overall distributions and for just the Adelie penguins. The overall distribution of the data by species shows some overlap in body weight for Adelie/Chinstrap, but more of a separation for the Gentoo penguins."
  },
  {
    "objectID": "content/posts/05-presentations/revealjs-penguins.html#distribution",
    "href": "content/posts/05-presentations/revealjs-penguins.html#distribution",
    "title": "Penguin Report Presentation",
    "section": "Distribution",
    "text": "Distribution\n\npenguins %&gt;% \n  ggplot(aes(body_mass_g, fill = species)) + \n  geom_density(color = \"white\", alpha = 0.5) +\n  scale_fill_manual(values = c(\"darkorange\",\"purple\",\"cyan4\")) +\n  labs(x = \"Penguin Bins\")"
  },
  {
    "objectID": "content/posts/05-presentations/revealjs-penguins.html#lm-scatter-plot",
    "href": "content/posts/05-presentations/revealjs-penguins.html#lm-scatter-plot",
    "title": "Penguin Report Presentation",
    "section": "LM + Scatter Plot",
    "text": "LM + Scatter Plot\n\npenguin_size_plot &lt;- penguins_clean %&gt;% \n  ggplot(aes(x = body_mass_g, y = flipper_length_mm, color = species)) + \n  scale_color_manual(values = c(\"darkorange\",\"purple\",\"cyan4\")) +\n  geom_point(size = 2, alpha = 0.5) +\n  labs(x = \"Mass (g)\", y = \"Flipper Length (mm)\") +\n  geom_smooth(aes(group = \"none\"), method = \"lm\")\n\npenguin_size_plot"
  },
  {
    "objectID": "content/posts/05-presentations/revealjs-penguins.html#modeling-section",
    "href": "content/posts/05-presentations/revealjs-penguins.html#modeling-section",
    "title": "Penguin Report Presentation",
    "section": "Modeling section",
    "text": "Modeling section\nMoving on to some basic modeling we can see if what kind of relationships we observe in the data. Note that I’m really not following any plan, just indicating how you can fit some different models all at once with dplyr + broom."
  },
  {
    "objectID": "content/posts/05-presentations/revealjs-penguins.html#section-1",
    "href": "content/posts/05-presentations/revealjs-penguins.html#section-1",
    "title": "Penguin Report Presentation",
    "section": "",
    "text": "model_inputs &lt;- tibble(\n  model_form = c(\n    list(flipper_length_mm ~ body_mass_g),\n    list(species_num ~ bill_length_mm + body_mass_g + sex),\n    list(flipper_length_mm ~ bill_length_mm + species)\n    ),\n  data = list(penguins_clean)\n) \n\nmodel_metrics &lt;- model_inputs %&gt;% \n  rowwise(model_form, data) %&gt;% \n  summarize(lm = list(lm(model_form, data = data)), .groups = \"drop\") %&gt;% \n  rowwise(model_form, lm, data) %&gt;% \n  summarise(broom::glance(lm), .groups = \"drop\")"
  },
  {
    "objectID": "content/posts/05-presentations/revealjs-penguins.html#section-2",
    "href": "content/posts/05-presentations/revealjs-penguins.html#section-2",
    "title": "Penguin Report Presentation",
    "section": "",
    "text": "model_metrics %&gt;% \n  select(model_form, r.squared:p.value) %&gt;% \n  mutate(model_form = as.character(model_form)) %&gt;% \n  gt::gt() %&gt;% \n  gt::fmt_number(r.squared:statistic) %&gt;% \n  gt::fmt_scientific(p.value) %&gt;% \n  gt::cols_width(\n    model_form ~ px(150)\n  )\n\n\n\n\n\n\n\n\nmodel_form\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\n\n\n\n\nflipper_length_mm ~ body_mass_g\n0.76\n0.76\n6.85\n1,060.30\n3.13 × 10−105\n\n\nspecies_num ~ bill_length_mm + body_mass_g + sex\n0.84\n0.84\n0.36\n583.59\n2.45 × 10−131\n\n\nflipper_length_mm ~ bill_length_mm + species\n0.83\n0.83\n5.83\n529.22\n1.66 × 10−125"
  },
  {
    "objectID": "content/posts/07-visuals/plot-layout.html",
    "href": "content/posts/07-visuals/plot-layout.html",
    "title": "Plot Layout",
    "section": "",
    "text": "library(ggplot2)\nlibrary(dplyr)"
  },
  {
    "objectID": "content/posts/07-visuals/plot-layout.html#figure-layout-ncol",
    "href": "content/posts/07-visuals/plot-layout.html#figure-layout-ncol",
    "title": "Plot Layout",
    "section": "Figure layout, ncol",
    "text": "Figure layout, ncol\n\n```{r}\n#| layout-ncol: 2\n#| fig-cap:\n#|   - \"Speed and Stopping Distances of Cars\"\n#|   - \"Engine displacement and fuel efficiency in Cars\"\n\ncars %&gt;% \n  ggplot(aes(x = speed, y = dist)) +\n  geom_point()\n\nmtcars %&gt;% \n  ggplot(aes(x = disp, y = mpg)) +\n  geom_point()\n```\n\n\n\n\n\nSpeed and Stopping Distances of Cars\n\n\n\n\n\nEngine displacement and fuel efficiency in Cars"
  },
  {
    "objectID": "content/posts/07-visuals/plot-layout.html#subcaptions",
    "href": "content/posts/07-visuals/plot-layout.html#subcaptions",
    "title": "Plot Layout",
    "section": "Subcaptions:",
    "text": "Subcaptions:\n\n```{r}\n#| label: fig-charts\n#| fig-cap: Charts\n#| fig-subcap:\n#|   - \"Cars\"\n#|   - \"mtcars\"\n#| layout-ncol: 2\n\ncars %&gt;% \n  ggplot(aes(x = speed, y = dist)) +\n  geom_point()\n\nmtcars %&gt;% \n  ggplot(aes(x = disp, y = mpg)) +\n  geom_point()\n```\n\n\n\n\n\n(a) Cars\n\n\n\n\n\n(b) mtcars\n\n\n\nFigure 1: Charts"
  },
  {
    "objectID": "content/posts/07-visuals/plot-layout.html#figure-layout-custom",
    "href": "content/posts/07-visuals/plot-layout.html#figure-layout-custom",
    "title": "Plot Layout",
    "section": "Figure layout, custom",
    "text": "Figure layout, custom\n\n```{r}\n#| layout: [[45,-10, 45], [50, 50]]\n#| fig-height: 5\n#| fig-align: center\n#| message: false\n#| warning: false\n#| dpi: 300\n\ncars %&gt;% \n  ggplot(aes(x = speed, y = dist)) +\n  geom_smooth() +\n  theme(text = element_text(size = 20))\n\ncars %&gt;% \n  ggplot(aes(x = speed, y = dist)) +\n  geom_point()+\n  theme(text = element_text(size = 20))\n\nmtcars %&gt;% \n  ggplot(aes(x = disp, y = mpg)) +\n  geom_point()+\n  theme(text = element_text(size = 20))\n\nmtcars %&gt;% \n  ggplot(aes(x = cyl, y = mpg, group = cyl, color = factor(cyl))) +\n  geom_boxplot() +\n  geom_jitter() +\n  theme(legend.position = \"none\")+\n  theme(text = element_text(size = 20))\n```"
  },
  {
    "objectID": "content/posts/07-visuals/stat-html.html",
    "href": "content/posts/07-visuals/stat-html.html",
    "title": "gtsummary + R Markdown",
    "section": "",
    "text": "library(gtsummary)\nlibrary(tidyverse)\nlibrary(survival)"
  },
  {
    "objectID": "content/posts/07-visuals/stat-html.html#gtsummary-tables",
    "href": "content/posts/07-visuals/stat-html.html#gtsummary-tables",
    "title": "gtsummary + R Markdown",
    "section": "gtsummary tables",
    "text": "gtsummary tables\nTables created with {gtsummary} can be integrated into R markdown documents. The {gtsummary} package was written to be a companion to the {gt} package from RStudio, and {gtsummary} tables are printed using {gt} when possible. Currently, {gt} supports HTML output, with LaTeX and RTF planned for the future.\n\npatient_characteristics &lt;-\n  trial %&gt;%\n  select(trt, age, grade, response) %&gt;%\n  tbl_summary(by = trt)  \npatient_characteristics\n\n\n\n\n\n\nCharacteristic\n\nDrug A, N = 981\n\n\nDrug B, N = 1021\n\n\n\n\nAge\n46 (37, 59)\n48 (39, 56)\n\n\n    Unknown\n7\n4\n\n\nGrade\n\n\n\n\n    I\n35 (36%)\n33 (32%)\n\n\n    II\n32 (33%)\n36 (35%)\n\n\n    III\n31 (32%)\n33 (32%)\n\n\nTumor Response\n28 (29%)\n33 (34%)\n\n\n    Unknown\n3\n4\n\n\n\n\n1 Median (IQR); n (%)\n\n\n\n\n\nWith HTML output, you can include complex tables with footnotes, indentation, and spanning table headers.\n\n# Side-by-side Regression Models\n# logistic regression model\nt1 &lt;-\n  glm(response ~ trt + grade + age, trial, family = binomial) %&gt;%\n  tbl_regression(exponentiate = TRUE)\n# time to death Cox model\nt2 &lt;-\n  coxph(Surv(ttdeath, death) ~ trt + grade + age, trial) %&gt;%\n  tbl_regression(exponentiate = TRUE)\n\n# printing merged table\ntbl_merge(\n  tbls = list(t1, t2),\n  tab_spanner = c(\"**Tumor Response**\", \"**Time to Death**\")\n)\n\n\n\n\n\n\n\nCharacteristic\nTumor Response\nTime to Death\n\n\n\nOR1\n\n\n95% CI1\n\np-value\n\nHR1\n\n\n95% CI1\n\np-value\n\n\n\n\nChemotherapy Treatment\n\n\n\n\n\n\n\n\n    Drug A\n—\n—\n\n—\n—\n\n\n\n    Drug B\n1.13\n0.60, 2.13\n0.7\n1.30\n0.88, 1.92\n0.2\n\n\nGrade\n\n\n\n\n\n\n\n\n    I\n—\n—\n\n—\n—\n\n\n\n    II\n0.85\n0.39, 1.85\n0.7\n1.21\n0.73, 1.99\n0.5\n\n\n    III\n1.01\n0.47, 2.15\n&gt;0.9\n1.79\n1.12, 2.86\n0.014\n\n\nAge\n1.02\n1.00, 1.04\n0.10\n1.01\n0.99, 1.02\n0.3\n\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval, HR = Hazard Ratio"
  },
  {
    "objectID": "content/posts/07-visuals/stat-html.html#inline-reporting",
    "href": "content/posts/07-visuals/stat-html.html#inline-reporting",
    "title": "gtsummary + R Markdown",
    "section": "inline reporting",
    "text": "inline reporting\nAny number/statistic from a {gtsummary} table can be reported inline in a R markdown document using the inline_text() function. See example below:\n\nAmong patients who received Drug A, 31 (32%) had grade III tumors.\n\nFor detailed examples using functions from {gtsummary}, visit the {gtsummary} website."
  },
  {
    "objectID": "content/posts/08-knitr/lightbox-extension.html",
    "href": "content/posts/08-knitr/lightbox-extension.html",
    "title": "Example Lightbox Document",
    "section": "",
    "text": "Here is a simple image with a description. This also overrides the description position and places it to the left of the image.\n\n\n\nBeach in Chilmark"
  },
  {
    "objectID": "content/posts/08-knitr/lightbox-extension.html#chilmark",
    "href": "content/posts/08-knitr/lightbox-extension.html#chilmark",
    "title": "Example Lightbox Document",
    "section": "",
    "text": "Here is a simple image with a description. This also overrides the description position and places it to the left of the image.\n\n\n\nBeach in Chilmark"
  },
  {
    "objectID": "content/posts/08-knitr/lightbox-extension.html#elsewhere",
    "href": "content/posts/08-knitr/lightbox-extension.html#elsewhere",
    "title": "Example Lightbox Document",
    "section": "Elsewhere",
    "text": "Elsewhere\nThe below demonstrates placing more than one image in a gallery. Note the usage of the layout-ncol which arranges the images on the page side by date. Adding the group attribute to the markdown images places the images in a gallery grouped together based upon the group name provided.\n\n\n\n\n\nAquinnah\n\n\n\n\n\nOak Bluffs\n\n\n\n\n\n\n\nVineyard lighthouse"
  },
  {
    "objectID": "content/posts/08-knitr/lightbox-extension.html#credits",
    "href": "content/posts/08-knitr/lightbox-extension.html#credits",
    "title": "Example Lightbox Document",
    "section": "Credits",
    "text": "Credits\nThe images in this example were used under the Unsplash license, view originals below:\n\nChilmark Beach\nAquinnah\nGingerbread House\nEdgartown Light\nEdgartown Sailboat"
  },
  {
    "objectID": "content/posts/09-using-lordicons/index.html",
    "href": "content/posts/09-using-lordicons/index.html",
    "title": "Using Lordicons, Fontawesome Icons,Academicons, and Iconify Icons",
    "section": "",
    "text": "This is just a compilation of the example files from the Quarto website, so that I can have ready-made code to copy and paste."
  },
  {
    "objectID": "content/posts/09-using-lordicons/index.html#introduction",
    "href": "content/posts/09-using-lordicons/index.html#introduction",
    "title": "Using Lordicons, Fontawesome Icons,Academicons, and Iconify Icons",
    "section": "",
    "text": "This is just a compilation of the example files from the Quarto website, so that I can have ready-made code to copy and paste."
  },
  {
    "objectID": "content/posts/09-using-lordicons/index.html#installation",
    "href": "content/posts/09-using-lordicons/index.html#installation",
    "title": "Using Lordicons, Fontawesome Icons,Academicons, and Iconify Icons",
    "section": "Installation",
    "text": "Installation\nType these in your Terminal:\n\nIconify: quarto install extension mcanouil/quarto-iconify\nFontAwesome: quarto install extension quarto-ext/fontawesome\nLordicons: quarto install extension jmgirard/lordicon\nAcademicons: quarto install extension schochastics/academicons\n\nThese extensions allows you to use a variety of icons in your Quarto HTML documents."
  },
  {
    "objectID": "content/posts/09-using-lordicons/index.html#using-lordicon-shortcodes",
    "href": "content/posts/09-using-lordicons/index.html#using-lordicon-shortcodes",
    "title": "Using Lordicons, Fontawesome Icons,Academicons, and Iconify Icons",
    "section": "Using Lordicon Shortcodes",
    "text": "Using Lordicon Shortcodes\nThe {{&lt; li &gt;}} shortcode renders an icon (specified by its code) after downloading it the lordicon CDN. The {{&lt; lif &gt;}} shortcode renders an icon (specified by its filepath) from a local .json file. Both shortcodes support the same arguments for customization, described below.\n\n\n\n\n\n\n\n\nPseudocode\nExample Code\nRendered\n\n\n\n\n{{&lt; li code &gt;}}\n{{&lt; li wlpxtupd &gt;}}\n\n\n\n{{&lt; lif file &gt;}}\n{{&lt; lif church.json &gt;}}\n\n\n\n\n\nTriggers\ntrigger controls the icon’s animation type. When using the loop or loop-on-hover triggers, you can also set an optional delay (in ms) between loops.\n\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n\n{{&lt; li wxnxiano &gt;}}\n\n\n\n{{&lt; li wxnxiano trigger=click &gt;}}\n\n\n\n{{&lt; li wxnxiano trigger=hover &gt;}}\n\n\n\n{{&lt; li wxnxiano trigger=loop &gt;}}\n\n\n\n{{&lt; li wxnxiano trigger=loop delay=1000 &gt;}}\n\n\n\n{{&lt; li wxnxiano trigger=loop-on-hover &gt;}}\n\n\n\n{{&lt; li wxnxiano trigger=loop-on-hover delay=1000 &gt;}}\n\n\n\n{{&lt; li wxnxiano trigger=morph &gt;}}\n\n\n\n{{&lt; li wxnxiano trigger=boomerang &gt;}}\n\n\n\n\n\n\nSpeed\nspeed controls how quickly the icon’s animation plays.\n\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n\n{{&lt; li lupuorrc trigger=loop speed=0.5 &gt;}}\n\n\n\n{{&lt; li lupuorrc trigger=loop speed=1.0 &gt;}}\n\n\n\n{{&lt; li lupuorrc trigger=loop speed=2.0 &gt;}}\n\n\n\n\n\n\nColors\ncolors controls the icon’s coloring. Outline icons typically have just a primary and secondary color, but flat and lineal icons can have many more. Each color should be given in rank:color format (where ranks are primary, secondary, tertiary, etc.) and multiple colors should be separated by commas. Colors can be given in HTML color names or hexcodes.\n\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n\n{{&lt; li lupuorrc &gt;}}\n\n\n\n{{&lt; li lupuorrc colors=primary:gold &gt;}}\n\n\n\n{{&lt; li lupuorrc colors=primary:gray,secondary:orange &gt;}}\n\n\n\n{{&lt; li lupuorrc colors=primary:#4030e8,secondary:#ee66aa &gt;}}\n\n\n\n\n\n\nStroke\nstroke controls how thick the lines in an icon are.\n\n\n\nShortcode\nIcon\n\n\n\n\n{{&lt; li lupuorrc stroke=50 &gt;}}\n\n\n\n{{&lt; li lupuorrc stroke=100 &gt;}}\n\n\n\n{{&lt; li lupuorrc stroke=150 &gt;}}\n\n\n\n\n\n\nScale\nscale controls how large or zoomed in the icon is.\n\n\n\nShortcode\nIcon\n\n\n\n\n{{&lt; li lupuorrc scale=25 &gt;}}\n\n\n\n{{&lt; li lupuorrc scale=50 &gt;}}\n\n\n\n{{&lt; li lupuorrc scale=100 &gt;}}\n\n\n\n\n\n\nAxis X\nx controls the horizontal position of the center of the icon.\n\n\n\nShortcode\nIcon\n\n\n\n\n{{&lt; li lupuorrc x=25 &gt;}}\n\n\n\n{{&lt; li lupuorrc x=50 &gt;}}\n\n\n\n{{&lt; li lupuorrc x=100 &gt;}}\n\n\n\n\n\n\nAxis Y\ny controls the vertical position of the center of the icon.\n\n\n\nShortcode\nIcon\n\n\n\n\n{{&lt; li lupuorrc y=25 &gt;}}\n\n\n\n{{&lt; li lupuorrc y=50 &gt;}}\n\n\n\n{{&lt; li lupuorrc y=100 &gt;}}"
  },
  {
    "objectID": "content/posts/09-using-lordicons/index.html#using-academicons-shortcodes",
    "href": "content/posts/09-using-lordicons/index.html#using-academicons-shortcodes",
    "title": "Using Lordicons, Fontawesome Icons,Academicons, and Iconify Icons",
    "section": "Using Academicons Shortcodes",
    "text": "Using Academicons Shortcodes\nThis extension allows you to use academicons in your Quarto HTML documents. It provides an {{&lt; ai &gt;}} shortcode:\n\nMandatory &lt;icon-name&gt;:\n{{&lt; ai &lt;icon-name&gt; &gt;}}\nOptional &lt;size=...&gt;:\n{{&lt; ai &lt;icon-name&gt; &lt;size=...&gt; &gt;}}\n\nFor example:\n\n\n\nShortcode\nIcon\n\n\n\n\n{{&lt; ai arxiv &gt;}}\n\n\n\n{{&lt; ai google-scholar &gt;}}\n\n\n\n{{&lt; ai open-access &gt;}}\n\n\n\n{{&lt; ai open-access size=5x &gt;}}"
  },
  {
    "objectID": "content/posts/09-using-lordicons/index.html#using-fontawesome-icons",
    "href": "content/posts/09-using-lordicons/index.html#using-fontawesome-icons",
    "title": "Using Lordicons, Fontawesome Icons,Academicons, and Iconify Icons",
    "section": "Using Fontawesome Icons",
    "text": "Using Fontawesome Icons\nThis extension allows you to use font-awesome icons in your Quarto HTML and PDF documents. It provides an {{&lt; fa &gt;}} shortcode:\n\nMandatory &lt;icon-name&gt;:\n{{&lt; fa &lt;icon-name&gt; &gt;}}\nOptional &lt;group&gt;, &lt;size=...&gt;, and &lt;title=...&gt;:\n{{&lt; fa &lt;group&gt; &lt;icon-name&gt; &lt;size=...&gt; &lt;title=...&gt; &gt;}}\n\nFor example:\n\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n\n{{&lt; fa thumbs-up &gt;}}\n\n\n\n{{&lt; fa folder &gt;}}\n\n\n\n{{&lt; fa chess-pawn &gt;}}\n\n\n\n{{&lt; fa brands bluetooth &gt;}}\n\n\n\n{{&lt; fa brands twitter size=2xl &gt;}} (HTML only)\n\n\n\n{{&lt; fa brands github size=5x &gt;}} (HTML only)\n\n\n\n{{&lt; fa battery-half size=Huge &gt;}}\n\n\n\n{{&lt; fa envelope title=\"An envelope\" &gt;}}"
  },
  {
    "objectID": "content/posts/09-using-lordicons/index.html#using-iconify-shortcodes",
    "href": "content/posts/09-using-lordicons/index.html#using-iconify-shortcodes",
    "title": "Using Lordicons, Fontawesome Icons,Academicons, and Iconify Icons",
    "section": "Using Iconify Shortcodes",
    "text": "Using Iconify Shortcodes\nThis extension allows you to use Iconify icons in your Quarto HTML documents. It provides an {{&lt; iconify &gt;}} shortcode:\n\nMandatory &lt;icon-name&gt;:\n{{&lt; iconify &lt;icon-name&gt; &gt;}}\nOptional &lt;set&gt; (default is fluent-emoji) &lt;size=...&gt;, &lt;width=...&gt;, &lt;height=...&gt;, &lt;flip=...&gt;, and &lt;rotate=...&gt;:\n{{&lt; iconify &lt;set&gt; &lt;icon-name&gt; &lt;size=...&gt; &lt;width=...&gt; &lt;height=...&gt; &lt;flip=...&gt; &lt;rotate=...&gt; &gt;}}\nIf &lt;size=...&gt; is defined, &lt;width=...&gt; and &lt;height=...&gt; are not used.\nSee https://docs.iconify.design/iconify-icon/ for more details.\n\nFor example:\n\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n\n{{&lt; iconify exploding-head &gt;}}\n\n\n\n{{&lt; iconify exploding-head size=2xl &gt;}}\n\n\n\n{{&lt; iconify exploding-head size=5x rotate=180deg &gt;}}\n\n\n\n{{&lt; iconify exploding-head size=Huge &gt;}}\n\n\n\n{{&lt; iconify fluent-emoji-high-contrast 1st-place-medal &gt;}}\n\n\n\n{{&lt; iconify twemoji 1st-place-medal &gt;}}\n\n\n\n{{&lt; iconify line-md loading-alt-loop &gt;}}\n\n\n\n{{&lt; iconify fa6-brands apple width=50px height=10px rotate=90deg flip=vertical &gt;}}"
  },
  {
    "objectID": "content/posts/12-tufte-style-article/index.html",
    "href": "content/posts/12-tufte-style-article/index.html",
    "title": "A Tufte Handout Example",
    "section": "",
    "text": "The Tufte handout style is a style that Edward Tufte uses in his books and handouts. Tufte’s style is known for its extensive use of sidenotes, tight integration of graphics with text, and well-set typography. This style has been implemented in LaTeX and HTML/CSS1, respectively. We have ported both implementations into the tufte package. If you want LaTeX/PDF output, you may use the tufte_handout format for handouts, and tufte_book for books. For HTML output, use tufte_html. These formats can be either specified in the YAML metadata at the beginning of an R Markdown document (see an example below), or passed to the rmarkdown::render() function. See Allaire et al. (2023) for more information about rmarkdown.\n---\ntitle: \"An Example Using the Tufte Style\"\nauthor: \"John Smith\"\noutput:\n  tufte::tufte_handout: default\n  tufte::tufte_html: default\n---\nThere are two goals of this package:\n\nTo produce both PDF and HTML output with similar styles from the same R Markdown document;\nTo provide simple syntax to write elements of the Tufte style such as side notes and margin figures, e.g. when you want a margin figure, all you need to do is the chunk option fig.margin = TRUE, and we will take care of the details for you, so you never need to think about \\begin{marginfigure} \\end{marginfigure} or &lt;span class=\"marginfigure\"&gt; &lt;/span&gt;; the LaTeX and HTML code under the hood may be complicated, but you never need to learn or write such code.\n\nIf you have any feature requests or find bugs in tufte, please do not hesitate to file them to https://github.com/rstudio/tufte/issues. For general questions, you may ask them on StackOverflow: https://stackoverflow.com/tags/rmarkdown."
  },
  {
    "objectID": "content/posts/12-tufte-style-article/index.html#margin-figures",
    "href": "content/posts/12-tufte-style-article/index.html#margin-figures",
    "title": "A Tufte Handout Example",
    "section": "Margin Figures",
    "text": "Margin Figures\nImages and graphics play an integral role in Tufte’s work. To place figures in the margin you can use the knitr chunk option fig.margin = TRUE. For example:\n\nlibrary(ggplot2)\nmtcars2 &lt;- mtcars\nmtcars2$am &lt;- factor(\n  mtcars$am, labels = c('automatic', 'manual')\n)\nggplot(mtcars2, aes(hp, mpg, color = am)) +\n  geom_point() + geom_smooth() +\n  theme(legend.position = 'bottom')\n\n\n\n\nFigure 1: MPG vs horsepower, colored by transmission.\n\n\nNote the use of the fig.cap chunk option to provide a figure caption. You can adjust the proportions of figures using the fig.width and fig.height chunk options. These are specified in inches, and will be automatically scaled down to fit within the handout margin."
  },
  {
    "objectID": "content/posts/12-tufte-style-article/index.html#arbitrary-margin-content",
    "href": "content/posts/12-tufte-style-article/index.html#arbitrary-margin-content",
    "title": "A Tufte Handout Example",
    "section": "Arbitrary Margin Content",
    "text": "Arbitrary Margin Content\nIn fact, you can include anything in the margin using the knitr engine named marginfigure. Unlike R code chunks ```{r}, you write a chunk starting with ```{marginfigure} instead, then put the content in the chunk. See an example on the right about the first fundamental theorem of calculus.\n\nWe know from _the first fundamental theorem of calculus_ that for $x$ in $[a, b]$:\n$$\\frac{d}{dx}\\left( \\int_{a}^{x} f(u)\\,du\\right)=f(x).$$\n\nFor the sake of portability between LaTeX and HTML, you should keep the margin content as simple as possible (syntax-wise) in the marginefigure blocks. You may use simple Markdown syntax like **bold** and _italic_ text, but please refrain from using footnotes, citations, or block-level elements (e.g. blockquotes and lists) there.\nNote: if you set echo = FALSE in your global chunk options, you will have to add echo = TRUE to the chunk to display a margin figure, for example ```{marginfigure, echo = TRUE}."
  },
  {
    "objectID": "content/posts/12-tufte-style-article/index.html#full-width-figures",
    "href": "content/posts/12-tufte-style-article/index.html#full-width-figures",
    "title": "A Tufte Handout Example",
    "section": "Full Width Figures",
    "text": "Full Width Figures\nYou can arrange for figures to span across the entire page by using the chunk option fig.fullwidth = TRUE.\n\nggplot(diamonds, aes(carat, price)) + geom_smooth() +\n  facet_grid(~ cut)\n\n\n\nFigure 2: A full width figure.\n\n\n\nOther chunk options related to figures can still be used, such as fig.width, fig.cap, out.width, and so on. For full width figures, usually fig.width is large and fig.height is small. In the above example, the plot size is \\(10 \\times 2\\)."
  },
  {
    "objectID": "content/posts/12-tufte-style-article/index.html#arbitrary-full-width-content",
    "href": "content/posts/12-tufte-style-article/index.html#arbitrary-full-width-content",
    "title": "A Tufte Handout Example",
    "section": "Arbitrary Full Width Content",
    "text": "Arbitrary Full Width Content\nAny content can span to the full width of the page. This feature requires Pandoc 2.0 or above. All you need is to put your content in a fenced Div with the class fullwidth, e.g.,\n::: {.fullwidth}\nAny _full width_ content here.\n:::\nBelow is an example:\n\nR is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under the terms of the GNU General Public License versions 2 or 3. For more information about these matters see https://www.gnu.org/licenses/."
  },
  {
    "objectID": "content/posts/12-tufte-style-article/index.html#main-column-figures",
    "href": "content/posts/12-tufte-style-article/index.html#main-column-figures",
    "title": "A Tufte Handout Example",
    "section": "Main Column Figures",
    "text": "Main Column Figures\nBesides margin and full width figures, you can of course also include figures constrained to the main column. This is the default type of figures in the LaTeX/HTML output.\n\nggplot(diamonds, aes(cut, price)) + geom_boxplot()\n\n\n\nFigure 3: A figure in the main column."
  },
  {
    "objectID": "content/posts/12-tufte-style-article/index.html#footnotes",
    "href": "content/posts/12-tufte-style-article/index.html#footnotes",
    "title": "A Tufte Handout Example",
    "section": "Footnotes",
    "text": "Footnotes\n\nSee Github repositories tufte-latex and tufte-css↩︎\nBeautiful Evidence↩︎\nNote you should not assume tufte has been attached to your R session. You should either library(tufte) in your R Markdown document before you call newthought(), or use tufte::newthought().↩︎\nThis is a sidenote that was entered using a footnote.↩︎\nThe actual Envisioned CSS was not used in the tufte package. We only changed the fonts, background color, and text color based on the default Tufte style.↩︎"
  },
  {
    "objectID": "content/posts/15-js/Using_sketch.html",
    "href": "content/posts/15-js/Using_sketch.html",
    "title": "Using sketch",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE)\nknitr::knit_engines$set(sketch = sketch::eng_sketch)\nlibrary(tidyverse)\nlibrary(sketch)\n\n#devtools::install_github(\"seankross/p5\")\nlibrary(p5)"
  },
  {
    "objectID": "content/posts/15-js/Using_sketch.html#introduction",
    "href": "content/posts/15-js/Using_sketch.html#introduction",
    "title": "Using sketch",
    "section": "Introduction",
    "text": "Introduction\nTrying to replicate this: https://kcf-jackson.github.io/sketch-website/docs/\n\nprint(\"'sketch' has its own knitr engine from version 1.0.5!\")\n\n\n\n\nsketch::insert_sketch(\n  file = \"./Using_sketch/main.R\", id = \"sketch_1\",\n  width = 500, height = 400\n)\n\n\n\n\n\nsketch::insert_sketch(\n  file = \"./Using_sketch/dots.R\", id = \"sketch_2\", deparsers = default_2_deparsers(),\n    width = 800, height = 600\n  )\n\n\n\n\n\nsketch::insert_sketch(\n  file = \"./Using_sketch/animated_dots.R\", id = \"sketch_2\", \n  deparsers = default_2_deparsers(),\n    width =800, height = 600\n  )\n\n\n\n\n\np5::p5() |&gt;\n  createCanvas(800, 600) |&gt;\n  background(\"#F4F8FC\") |&gt;\n  fill(\"yellow\") |&gt;\n  ellipse(~mouseX, ~mouseY, 30, 30)\n\n\n\n\n\n\nstripes &lt;- tibble(\n  x = rep(0, 7),\n  y = cumsum(c(0, rep(30, 6))),\n  w = rep(300, 7),\n  h = rep(15, 7)\n)\nstripes\n\n\n\n  \n\n\nstripes %&gt;%\n  p5() %&gt;%\n  createCanvas(300, 200) %&gt;%\n  fill(\"#FF0000\") %&gt;%\n  noStroke() %&gt;%\n  rect()\n\n\n\n\n\n\n#! load_script(src = \"https://cdnjs.cloudflare.com/ajax/libs/p5.js/0.9.0/p5.js\")\nsetup &lt;- function() {\n    createCanvas(400, 300)\n}\n\ndraw &lt;- function() {\n    background(0, 0, 33)    # RGB colors\n\n    for (i in 1:3) {\n        dia &lt;- sin(frameCount * 0.025) * 30 * i\n        fill(255, 70 * i, 0)       # RGB colors\n        circle(100 * i, 150, dia)   # (x, y, diameter)    \n    }\n}"
  },
  {
    "objectID": "content/posts/project-1/index.html",
    "href": "content/posts/project-1/index.html",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\nThis is inline code plus a small code chunk.\n\nCodelibrary(tidyverse)\n\nggplot(mpg) +\n  geom_jitter(aes(cty, hwy), size = 4, alpha = 0.5) \n\n\n\n\n\n\n\n\n\n\nTransforming OLS estimates\nMaximizing likelihood\n\n\n\n\nCodepreds_lm %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  theme_minimal(base_size = 12) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\nCodeglm.mod &lt;- glm(sex ~ body_mass_g + bill_length_mm + species, family = binomial, data = dat)\n\npreds &lt;- dat %&gt;% \n  mutate(\n    prob.fit = glm.mod$fitted.values,\n    prediction = if_else(prob.fit &gt; 0.5, 'male', 'female'),\n    correct = if_else(sex == prediction, 'correct', 'incorrect')\n  )\n\n\npreds %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  theme_minimal(base_size = 10) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\\[\n\\int_0^1 f(x) \\ dx\n\\]\n\n\n\n\nCodegeom_density(\n  mapping = NULL,\n  data = NULL,\n  stat = \"density\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  outline.type = \"upper\"\n)\n\n\n\nCodestat_density(\n  mapping = NULL,\n  data = NULL,\n  geom = \"area\",\n  position = \"stack\",\n  ...,\n  bw = \"nrd0\",\n  adjust = 1,\n  kernel = \"gaussian\",\n  n = 512,\n  trim = FALSE,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\n\n\n\n\n\nCodeggplot(data = gapminder::gapminder, mapping = aes(x = lifeExp, fill = continent)) +\n  stat_density(position = \"identity\", alpha = 0.5)\n\n\n\n\nBla bla bla. This is a caption in the margin. Super cool isn’t it?"
  },
  {
    "objectID": "content/posts/project-1/index.html#merriweather",
    "href": "content/posts/project-1/index.html#merriweather",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\nThis is inline code plus a small code chunk.\n\nCodelibrary(tidyverse)\n\nggplot(mpg) +\n  geom_jitter(aes(cty, hwy), size = 4, alpha = 0.5) \n\n\n\n\n\n\n\n\n\n\nTransforming OLS estimates\nMaximizing likelihood\n\n\n\n\nCodepreds_lm %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  theme_minimal(base_size = 12) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\nCodeglm.mod &lt;- glm(sex ~ body_mass_g + bill_length_mm + species, family = binomial, data = dat)\n\npreds &lt;- dat %&gt;% \n  mutate(\n    prob.fit = glm.mod$fitted.values,\n    prediction = if_else(prob.fit &gt; 0.5, 'male', 'female'),\n    correct = if_else(sex == prediction, 'correct', 'incorrect')\n  )\n\n\npreds %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  theme_minimal(base_size = 10) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\\[\n\\int_0^1 f(x) \\ dx\n\\]"
  },
  {
    "objectID": "content/posts/project-1/index.html#columns",
    "href": "content/posts/project-1/index.html#columns",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "Codegeom_density(\n  mapping = NULL,\n  data = NULL,\n  stat = \"density\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  outline.type = \"upper\"\n)\n\n\n\nCodestat_density(\n  mapping = NULL,\n  data = NULL,\n  geom = \"area\",\n  position = \"stack\",\n  ...,\n  bw = \"nrd0\",\n  adjust = 1,\n  kernel = \"gaussian\",\n  n = 512,\n  trim = FALSE,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)"
  },
  {
    "objectID": "content/posts/project-1/index.html#margin-captions-and-figures",
    "href": "content/posts/project-1/index.html#margin-captions-and-figures",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "Codeggplot(data = gapminder::gapminder, mapping = aes(x = lifeExp, fill = continent)) +\n  stat_density(position = \"identity\", alpha = 0.5)\n\n\n\n\nBla bla bla. This is a caption in the margin. Super cool isn’t it?"
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "",
    "text": "This is an attempt to rework , using tidygraph and ggraph, much of Network Visualization with R Polnet 2018 Workshop Tutorial, Washington, DC by Prof. Katherine (Katya) Ognyanova.\nThe aim is to get a working acquaintance with both these packages and also to appreciate some of the concepts in Networks. My code is by no means intended to be elegant; it merely works and there are surely many improvements that people may think of!\nI have attempted to write code for the Sections 2:5.\nI have retained Prof. Ognyanova’s text in all places."
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#introduction",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#introduction",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "",
    "text": "This is an attempt to rework , using tidygraph and ggraph, much of Network Visualization with R Polnet 2018 Workshop Tutorial, Washington, DC by Prof. Katherine (Katya) Ognyanova.\nThe aim is to get a working acquaintance with both these packages and also to appreciate some of the concepts in Networks. My code is by no means intended to be elegant; it merely works and there are surely many improvements that people may think of!\nI have attempted to write code for the Sections 2:5.\nI have retained Prof. Ognyanova’s text in all places."
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#contents",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#contents",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "CONTENTS",
    "text": "CONTENTS\n\nWorking with colors in R plots\nReading in the network data\nNetwork plots in ‘igraph’\nPlotting two-mode networks\nPlotting multiplex networks\nQuick example using ‘network’\nSimple plot animations in R\nInteractive JavaScript networks\nInteractive and dynamic networks with ndtv-d3\nPlotting networks on a geographic map"
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#dataset-1-edgelist--",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#dataset-1-edgelist--",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "——-~~ DATASET 1: edgelist ~~——-",
    "text": "——-~~ DATASET 1: edgelist ~~——-\n\n# Read in the data:\nnodes &lt;- read.csv(\"./Dataset1-Media-Example-NODES.csv\", header = T, as.is = T)\nlinks &lt;- read.csv(\"./Dataset1-Media-Example-EDGES.csv\", header = T, as.is = T)\n\n\n# Examine the data:\nhead(nodes)\n\n   id               media media.type type.label audience.size\n1 s01            NY Times          1  Newspaper            20\n2 s02     Washington Post          1  Newspaper            25\n3 s03 Wall Street Journal          1  Newspaper            30\n4 s04           USA Today          1  Newspaper            32\n5 s05            LA Times          1  Newspaper            20\n6 s06       New York Post          1  Newspaper            50\n\nhead(links)\n\n  from  to      type weight\n1  s01 s02 hyperlink     22\n2  s01 s03 hyperlink     22\n3  s01 s04 hyperlink     21\n4  s01 s15   mention     20\n5  s02 s01 hyperlink     23\n6  s02 s03 hyperlink     21\n\n\nConverting the data to an igraph object:\nThe graph_from_data_frame() function takes two data frames: ‘d’ and ‘vertices’. - ‘d’ describes the edges of the network - it should start with two columns containing the source and target node IDs for each network tie. - ‘vertices’ should start with a column of node IDs. It can be omitted. - Any additional columns in either data frame are interpreted as attributes.\nNOTE: ID columns need not be numbers or integers!!\n\nnet &lt;- graph_from_data_frame(d = links, vertices = nodes, directed = T)\n\n# Examine the resulting object:\nclass(net)\n\n[1] \"igraph\"\n\nnet\n\nIGRAPH 8d2aa7a DNW- 17 49 -- \n+ attr: name (v/c), media (v/c), media.type (v/n), type.label (v/c),\n| audience.size (v/n), type (e/c), weight (e/n)\n+ edges from 8d2aa7a (vertex names):\n [1] s01-&gt;s02 s01-&gt;s03 s01-&gt;s04 s01-&gt;s15 s02-&gt;s01 s02-&gt;s03 s02-&gt;s09 s02-&gt;s10\n [9] s03-&gt;s01 s03-&gt;s04 s03-&gt;s05 s03-&gt;s08 s03-&gt;s10 s03-&gt;s11 s03-&gt;s12 s04-&gt;s03\n[17] s04-&gt;s06 s04-&gt;s11 s04-&gt;s12 s04-&gt;s17 s05-&gt;s01 s05-&gt;s02 s05-&gt;s09 s05-&gt;s15\n[25] s06-&gt;s06 s06-&gt;s16 s06-&gt;s17 s07-&gt;s03 s07-&gt;s08 s07-&gt;s10 s07-&gt;s14 s08-&gt;s03\n[33] s08-&gt;s07 s08-&gt;s09 s09-&gt;s10 s10-&gt;s03 s12-&gt;s06 s12-&gt;s13 s12-&gt;s14 s13-&gt;s12\n[41] s13-&gt;s17 s14-&gt;s11 s14-&gt;s13 s15-&gt;s01 s15-&gt;s04 s15-&gt;s06 s16-&gt;s06 s16-&gt;s17\n[49] s17-&gt;s04\n\n\nThe description of an igraph object starts with four letters: -D or U, for a directed or undirected graph -N for a named graph (where nodes have a name attribute) -W for a weighted graph (where edges have a weight attribute) -B for a bipartite (two-mode) graph (where nodes have a type attribute) The two numbers that follow (17 49) refer to the number of nodes and edges in the graph. The description also lists node & edge attributes.\nWe can access the nodes, edges, and their attributes:\n\nE(net)\n\n+ 49/49 edges from 8d2aa7a (vertex names):\n [1] s01-&gt;s02 s01-&gt;s03 s01-&gt;s04 s01-&gt;s15 s02-&gt;s01 s02-&gt;s03 s02-&gt;s09 s02-&gt;s10\n [9] s03-&gt;s01 s03-&gt;s04 s03-&gt;s05 s03-&gt;s08 s03-&gt;s10 s03-&gt;s11 s03-&gt;s12 s04-&gt;s03\n[17] s04-&gt;s06 s04-&gt;s11 s04-&gt;s12 s04-&gt;s17 s05-&gt;s01 s05-&gt;s02 s05-&gt;s09 s05-&gt;s15\n[25] s06-&gt;s06 s06-&gt;s16 s06-&gt;s17 s07-&gt;s03 s07-&gt;s08 s07-&gt;s10 s07-&gt;s14 s08-&gt;s03\n[33] s08-&gt;s07 s08-&gt;s09 s09-&gt;s10 s10-&gt;s03 s12-&gt;s06 s12-&gt;s13 s12-&gt;s14 s13-&gt;s12\n[41] s13-&gt;s17 s14-&gt;s11 s14-&gt;s13 s15-&gt;s01 s15-&gt;s04 s15-&gt;s06 s16-&gt;s06 s16-&gt;s17\n[49] s17-&gt;s04\n\nV(net)\n\n+ 17/17 vertices, named, from 8d2aa7a:\n [1] s01 s02 s03 s04 s05 s06 s07 s08 s09 s10 s11 s12 s13 s14 s15 s16 s17\n\nE(net)$type\n\n [1] \"hyperlink\" \"hyperlink\" \"hyperlink\" \"mention\"   \"hyperlink\" \"hyperlink\"\n [7] \"hyperlink\" \"hyperlink\" \"hyperlink\" \"hyperlink\" \"hyperlink\" \"hyperlink\"\n[13] \"mention\"   \"hyperlink\" \"hyperlink\" \"hyperlink\" \"mention\"   \"mention\"  \n[19] \"hyperlink\" \"mention\"   \"mention\"   \"hyperlink\" \"hyperlink\" \"mention\"  \n[25] \"hyperlink\" \"hyperlink\" \"mention\"   \"mention\"   \"mention\"   \"hyperlink\"\n[31] \"mention\"   \"hyperlink\" \"mention\"   \"mention\"   \"mention\"   \"hyperlink\"\n[37] \"mention\"   \"hyperlink\" \"mention\"   \"hyperlink\" \"mention\"   \"mention\"  \n[43] \"mention\"   \"hyperlink\" \"hyperlink\" \"hyperlink\" \"hyperlink\" \"mention\"  \n[49] \"hyperlink\"\n\nV(net)$media\n\n [1] \"NY Times\"            \"Washington Post\"     \"Wall Street Journal\"\n [4] \"USA Today\"           \"LA Times\"            \"New York Post\"      \n [7] \"CNN\"                 \"MSNBC\"               \"FOX News\"           \n[10] \"ABC\"                 \"BBC\"                 \"Yahoo News\"         \n[13] \"Google News\"         \"Reuters.com\"         \"NYTimes.com\"        \n[16] \"WashingtonPost.com\"  \"AOL.com\"            \n\n\n\n# Using tidygraph\ntbl_graph(nodes, links, directed = TRUE) %&gt;% \n  activate(edges) %&gt;% \n  select(type)\n\n# A tbl_graph: 17 nodes and 49 edges\n#\n# A directed multigraph with 1 component\n#\n# Edge Data: 49 × 3 (active)\n   from    to type     \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;    \n1     1     2 hyperlink\n2     1     3 hyperlink\n3     1     4 hyperlink\n4     1    15 mention  \n5     2     1 hyperlink\n6     2     3 hyperlink\n# … with 43 more rows\n#\n# Node Data: 17 × 5\n  id    media               media.type type.label audience.size\n  &lt;chr&gt; &lt;chr&gt;                    &lt;int&gt; &lt;chr&gt;              &lt;int&gt;\n1 s01   NY Times                     1 Newspaper             20\n2 s02   Washington Post              1 Newspaper             25\n3 s03   Wall Street Journal          1 Newspaper             30\n# … with 14 more rows\n\ntbl_graph(nodes, links, directed = TRUE) %&gt;% \n  activate(nodes) %&gt;% \n  select(media)\n\n# A tbl_graph: 17 nodes and 49 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 17 × 1 (active)\n  media              \n  &lt;chr&gt;              \n1 NY Times           \n2 Washington Post    \n3 Wall Street Journal\n4 USA Today          \n5 LA Times           \n6 New York Post      \n# … with 11 more rows\n#\n# Edge Data: 49 × 4\n   from    to type      weight\n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;int&gt;\n1     1     2 hyperlink     22\n2     1     3 hyperlink     22\n3     1     4 hyperlink     21\n# … with 46 more rows\n\n\nOr find specific nodes and edges by attribute:(that returns objects of type vertex sequence / edge sequence)\n\nV(net)[media == \"BBC\"]\n\n+ 1/17 vertex, named, from 8d2aa7a:\n[1] s11\n\nE(net)[type == \"mention\"]\n\n+ 20/49 edges from 8d2aa7a (vertex names):\n [1] s01-&gt;s15 s03-&gt;s10 s04-&gt;s06 s04-&gt;s11 s04-&gt;s17 s05-&gt;s01 s05-&gt;s15 s06-&gt;s17\n [9] s07-&gt;s03 s07-&gt;s08 s07-&gt;s14 s08-&gt;s07 s08-&gt;s09 s09-&gt;s10 s12-&gt;s06 s12-&gt;s14\n[17] s13-&gt;s17 s14-&gt;s11 s14-&gt;s13 s16-&gt;s17\n\n\n\n#Using tidygraph\ntbl_graph(nodes, links, directed = TRUE) %&gt;% \n  activate(nodes) %&gt;% \n  filter(media == \"BBC\")\n\n# A tbl_graph: 1 nodes and 0 edges\n#\n# A rooted tree\n#\n# Node Data: 1 × 5 (active)\n  id    media media.type type.label audience.size\n  &lt;chr&gt; &lt;chr&gt;      &lt;int&gt; &lt;chr&gt;              &lt;int&gt;\n1 s11   BBC            2 TV                    34\n#\n# Edge Data: 0 × 4\n# … with 4 variables: from &lt;int&gt;, to &lt;int&gt;, type &lt;chr&gt;, weight &lt;int&gt;\n\ntbl_graph(nodes, links, directed = TRUE) %&gt;% \n  activate(edges) %&gt;% \n  filter(type == \"mention\")\n\n# A tbl_graph: 17 nodes and 20 edges\n#\n# A directed simple graph with 3 components\n#\n# Edge Data: 20 × 4 (active)\n   from    to type    weight\n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;int&gt;\n1     1    15 mention     20\n2     3    10 mention      2\n3     4     6 mention      1\n4     4    11 mention     22\n5     4    17 mention      2\n6     5     1 mention      1\n# … with 14 more rows\n#\n# Node Data: 17 × 5\n  id    media               media.type type.label audience.size\n  &lt;chr&gt; &lt;chr&gt;                    &lt;int&gt; &lt;chr&gt;              &lt;int&gt;\n1 s01   NY Times                     1 Newspaper             20\n2 s02   Washington Post              1 Newspaper             25\n3 s03   Wall Street Journal          1 Newspaper             30\n# … with 14 more rows\n\n\nIf you need them, you can extract an edge list or a matrix back from the igraph networks.\n\nas_edgelist(net, names = T)\n\n      [,1]  [,2] \n [1,] \"s01\" \"s02\"\n [2,] \"s01\" \"s03\"\n [3,] \"s01\" \"s04\"\n [4,] \"s01\" \"s15\"\n [5,] \"s02\" \"s01\"\n [6,] \"s02\" \"s03\"\n [7,] \"s02\" \"s09\"\n [8,] \"s02\" \"s10\"\n [9,] \"s03\" \"s01\"\n[10,] \"s03\" \"s04\"\n[11,] \"s03\" \"s05\"\n[12,] \"s03\" \"s08\"\n[13,] \"s03\" \"s10\"\n[14,] \"s03\" \"s11\"\n[15,] \"s03\" \"s12\"\n[16,] \"s04\" \"s03\"\n[17,] \"s04\" \"s06\"\n[18,] \"s04\" \"s11\"\n[19,] \"s04\" \"s12\"\n[20,] \"s04\" \"s17\"\n[21,] \"s05\" \"s01\"\n[22,] \"s05\" \"s02\"\n[23,] \"s05\" \"s09\"\n[24,] \"s05\" \"s15\"\n[25,] \"s06\" \"s06\"\n[26,] \"s06\" \"s16\"\n[27,] \"s06\" \"s17\"\n[28,] \"s07\" \"s03\"\n[29,] \"s07\" \"s08\"\n[30,] \"s07\" \"s10\"\n[31,] \"s07\" \"s14\"\n[32,] \"s08\" \"s03\"\n[33,] \"s08\" \"s07\"\n[34,] \"s08\" \"s09\"\n[35,] \"s09\" \"s10\"\n[36,] \"s10\" \"s03\"\n[37,] \"s12\" \"s06\"\n[38,] \"s12\" \"s13\"\n[39,] \"s12\" \"s14\"\n[40,] \"s13\" \"s12\"\n[41,] \"s13\" \"s17\"\n[42,] \"s14\" \"s11\"\n[43,] \"s14\" \"s13\"\n[44,] \"s15\" \"s01\"\n[45,] \"s15\" \"s04\"\n[46,] \"s15\" \"s06\"\n[47,] \"s16\" \"s06\"\n[48,] \"s16\" \"s17\"\n[49,] \"s17\" \"s04\"\n\nas_adjacency_matrix(net, attr = \"weight\")\n\n17 x 17 sparse Matrix of class \"dgCMatrix\"\n\n\n  [[ suppressing 17 column names 's01', 's02', 's03' ... ]]\n\n\n                                                     \ns01  . 22 22 21 .  .  .  .  .  .  .  .  .  . 20  .  .\ns02 23  . 21  . .  .  .  .  1  5  .  .  .  .  .  .  .\ns03 21  .  . 22 1  .  .  4  .  2  1  1  .  .  .  .  .\ns04  .  . 23  . .  1  .  .  .  . 22  3  .  .  .  .  2\ns05  1 21  .  . .  .  .  .  2  .  .  .  .  . 21  .  .\ns06  .  .  .  . .  1  .  .  .  .  .  .  .  .  . 21 21\ns07  .  .  1  . .  .  . 22  . 21  .  .  .  4  .  .  .\ns08  .  .  2  . .  . 21  . 23  .  .  .  .  .  .  .  .\ns09  .  .  .  . .  .  .  .  . 21  .  .  .  .  .  .  .\ns10  .  .  2  . .  .  .  .  .  .  .  .  .  .  .  .  .\ns11  .  .  .  . .  .  .  .  .  .  .  .  .  .  .  .  .\ns12  .  .  .  . .  2  .  .  .  .  .  . 22 22  .  .  .\ns13  .  .  .  . .  .  .  .  .  .  . 21  .  .  .  .  1\ns14  .  .  .  . .  .  .  .  .  .  1  . 21  .  .  .  .\ns15 22  .  .  1 .  4  .  .  .  .  .  .  .  .  .  .  .\ns16  .  .  .  . . 23  .  .  .  .  .  .  .  .  .  . 21\ns17  .  .  .  4 .  .  .  .  .  .  .  .  .  .  .  .  .\n\n# Using tidygraph\n# No direct command seems available ...\n\n\n# Or data frames describing nodes and edges:\nigraph::as_data_frame(x = net, what = \"edges\")\n\n   from  to      type weight\n1   s01 s02 hyperlink     22\n2   s01 s03 hyperlink     22\n3   s01 s04 hyperlink     21\n4   s01 s15   mention     20\n5   s02 s01 hyperlink     23\n6   s02 s03 hyperlink     21\n7   s02 s09 hyperlink      1\n8   s02 s10 hyperlink      5\n9   s03 s01 hyperlink     21\n10  s03 s04 hyperlink     22\n11  s03 s05 hyperlink      1\n12  s03 s08 hyperlink      4\n13  s03 s10   mention      2\n14  s03 s11 hyperlink      1\n15  s03 s12 hyperlink      1\n16  s04 s03 hyperlink     23\n17  s04 s06   mention      1\n18  s04 s11   mention     22\n19  s04 s12 hyperlink      3\n20  s04 s17   mention      2\n21  s05 s01   mention      1\n22  s05 s02 hyperlink     21\n23  s05 s09 hyperlink      2\n24  s05 s15   mention     21\n25  s06 s06 hyperlink      1\n26  s06 s16 hyperlink     21\n27  s06 s17   mention     21\n28  s07 s03   mention      1\n29  s07 s08   mention     22\n30  s07 s10 hyperlink     21\n31  s07 s14   mention      4\n32  s08 s03 hyperlink      2\n33  s08 s07   mention     21\n34  s08 s09   mention     23\n35  s09 s10   mention     21\n36  s10 s03 hyperlink      2\n37  s12 s06   mention      2\n38  s12 s13 hyperlink     22\n39  s12 s14   mention     22\n40  s13 s12 hyperlink     21\n41  s13 s17   mention      1\n42  s14 s11   mention      1\n43  s14 s13   mention     21\n44  s15 s01 hyperlink     22\n45  s15 s04 hyperlink      1\n46  s15 s06 hyperlink      4\n47  s16 s06 hyperlink     23\n48  s16 s17   mention     21\n49  s17 s04 hyperlink      4\n\nigraph::as_data_frame(x = net, what = \"vertices\")\n\n    name               media media.type type.label audience.size\ns01  s01            NY Times          1  Newspaper            20\ns02  s02     Washington Post          1  Newspaper            25\ns03  s03 Wall Street Journal          1  Newspaper            30\ns04  s04           USA Today          1  Newspaper            32\ns05  s05            LA Times          1  Newspaper            20\ns06  s06       New York Post          1  Newspaper            50\ns07  s07                 CNN          2         TV            56\ns08  s08               MSNBC          2         TV            34\ns09  s09            FOX News          2         TV            60\ns10  s10                 ABC          2         TV            23\ns11  s11                 BBC          2         TV            34\ns12  s12          Yahoo News          3     Online            33\ns13  s13         Google News          3     Online            23\ns14  s14         Reuters.com          3     Online            12\ns15  s15         NYTimes.com          3     Online            24\ns16  s16  WashingtonPost.com          3     Online            28\ns17  s17             AOL.com          3     Online            33\n\n#Using tidygraph\ntbl_graph(nodes, links, directed = TRUE) %&gt;% \n  activate(nodes) %&gt;% \n  as_tibble()\n\n# A tibble: 17 × 5\n   id    media               media.type type.label audience.size\n   &lt;chr&gt; &lt;chr&gt;                    &lt;int&gt; &lt;chr&gt;              &lt;int&gt;\n 1 s01   NY Times                     1 Newspaper             20\n 2 s02   Washington Post              1 Newspaper             25\n 3 s03   Wall Street Journal          1 Newspaper             30\n 4 s04   USA Today                    1 Newspaper             32\n 5 s05   LA Times                     1 Newspaper             20\n 6 s06   New York Post                1 Newspaper             50\n 7 s07   CNN                          2 TV                    56\n 8 s08   MSNBC                        2 TV                    34\n 9 s09   FOX News                     2 TV                    60\n10 s10   ABC                          2 TV                    23\n11 s11   BBC                          2 TV                    34\n12 s12   Yahoo News                   3 Online                33\n13 s13   Google News                  3 Online                23\n14 s14   Reuters.com                  3 Online                12\n15 s15   NYTimes.com                  3 Online                24\n16 s16   WashingtonPost.com           3 Online                28\n17 s17   AOL.com                      3 Online                33\n\ntbl_graph(nodes, links, directed = TRUE)%&gt;% \n  activate(edges) %&gt;% \n  as_tibble()\n\n# A tibble: 49 × 4\n    from    to type      weight\n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;int&gt;\n 1     1     2 hyperlink     22\n 2     1     3 hyperlink     22\n 3     1     4 hyperlink     21\n 4     1    15 mention       20\n 5     2     1 hyperlink     23\n 6     2     3 hyperlink     21\n 7     2     9 hyperlink      1\n 8     2    10 hyperlink      5\n 9     3     1 hyperlink     21\n10     3     4 hyperlink     22\n# … with 39 more rows\n\n\n\n# You can also access the network matrix directly:\nnet[1,]\n\ns01 s02 s03 s04 s05 s06 s07 s08 s09 s10 s11 s12 s13 s14 s15 s16 s17 \n  0  22  22  21   0   0   0   0   0   0   0   0   0   0  20   0   0 \n\nnet[5,7]\n\n[1] 0\n\n# Using tidygraph\n# Does not seem possible, even with `as.matrix()`.\n# Returns tibbles only as in the code chunk above\n\n\n# First attempt to plot the graph:\nplot(net) # not pretty!\n\n\n\n# Removing loops from the graph:\nnet &lt;-\n  igraph::simplify(net, remove.multiple = F, remove.loops = T)\n\n# Let's and reduce the arrow size and remove the labels:\nplot(net, edge.arrow.size = .4, vertex.label = NA)\n\n\n\n\n\n# Using tidygraph\ntbl_graph(nodes, links, directed = TRUE) %&gt;%\n  ggraph(., layout = \"graphopt\") +\n  geom_edge_link(\n    color = \"grey\",\n    end_cap = circle(0.2, \"cm\"),\n    start_cap = circle(0.2, \"cm\"),\n    # clears an area near the node\n    \n    arrow = arrow(\n      type = \"closed\",\n      ends = \"last\",\n      length = unit(3, \"mm\")\n    )\n  ) +\n  geom_node_point(size = 6, shape = 21, fill = \"orange\") +\n  geom_node_text(aes(label = id))\n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` in the `default_aes` field and elsewhere instead.\n\n\n\n\n# Removing loops from the graph:\n# From the docs:\n# convert() is a shorthand for performing both `morph` and `crystallise` along with extracting a single tbl_graph (defaults to the first). For morphs w(h)ere you know they only create a single graph, and you want to keep it, this is an easy way.\n#\ntbl_graph(nodes, links, directed = TRUE) %&gt;%\n\n  convert(to_simple) %&gt;%\n  ggraph(., layout = \"graphopt\") +\n  geom_edge_link(\n    color = \"grey\",\n    end_cap = circle(0.2, \"cm\"),\n    start_cap = circle(0.2, \"cm\"),\n \n    arrow = arrow(\n      type = \"closed\",\n      ends = \"last\",\n      length = unit(3, \"mm\")\n    )\n  ) +\n  geom_node_point(size = 6, shape = 21, fill = \"orange\")"
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#dataset-2-matrix",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#dataset-2-matrix",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "——-~~ DATASET 2: matrix ——–",
    "text": "——-~~ DATASET 2: matrix ——–\n\n# Read in the data:\nnodes2 &lt;- read.csv(\"./Dataset2-Media-User-Example-NODES.csv\", header = T, as.is = T)\nlinks2 &lt;- read.csv(\"./Dataset2-Media-User-Example-EDGES.csv\", header = T, row.names = 1)\n\n# Examine the data:\nhead(nodes2)\n\n   id   media media.type media.name audience.size\n1 s01     NYT          1  Newspaper            20\n2 s02    WaPo          1  Newspaper            25\n3 s03     WSJ          1  Newspaper            30\n4 s04    USAT          1  Newspaper            32\n5 s05 LATimes          1  Newspaper            20\n6 s06     CNN          2         TV            56\n\nhead(links2)\n\n    U01 U02 U03 U04 U05 U06 U07 U08 U09 U10 U11 U12 U13 U14 U15 U16 U17 U18 U19\ns01   1   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\ns02   0   0   0   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\ns03   0   0   0   0   0   1   1   1   1   0   0   0   0   0   0   0   0   0   0\ns04   0   0   0   0   0   0   0   0   1   1   1   0   0   0   0   0   0   0   0\ns05   0   0   0   0   0   0   0   0   0   0   1   1   1   0   0   0   0   0   0\ns06   0   0   0   0   0   0   0   0   0   0   0   0   1   1   0   0   1   0   0\n    U20\ns01   0\ns02   1\ns03   0\ns04   0\ns05   0\ns06   0\n\n# links2 is a matrix for a two-mode network:\nlinks2 &lt;- as.matrix(links2)\ndim(links2)\n\n[1] 10 20\n\ndim(nodes2)\n\n[1] 30  5\n\n\nNote: What is a two-mode network? A network that as a node$type variable and can be a bipartite or a k-partite network as a result.\n\n# Create an igraph network object from the two-mode matrix:\nnet2 &lt;- igraph::graph_from_incidence_matrix(links2)\n\n# To transform a one-mode network matrix into an igraph object,\n# we would use graph_from_adjacency_matrix()\n\n# A built-in vertex attribute 'type' shows which mode vertices belong to.\ntable(V(net2)$type)\n\n\nFALSE  TRUE \n   10    20 \n\n# Basic igraph plot\nplot(net2,vertex.label = NA)\n\n\n\n\n\n# using tidygraph\n# For all objects that are not node and edge data_frames\n# tidygraph uses `as_tbl_graph()`\n# \ngraph &lt;- as_tbl_graph(links2)\ngraph %&gt;% activate(nodes) %&gt;% as_tibble()\n\n# A tibble: 30 × 2\n   type  name \n   &lt;lgl&gt; &lt;chr&gt;\n 1 FALSE s01  \n 2 FALSE s02  \n 3 FALSE s03  \n 4 FALSE s04  \n 5 FALSE s05  \n 6 FALSE s06  \n 7 FALSE s07  \n 8 FALSE s08  \n 9 FALSE s09  \n10 FALSE s10  \n# … with 20 more rows\n\ngraph %&gt;% activate(edges) %&gt;% as_tibble()\n\n# A tibble: 31 × 3\n    from    to weight\n   &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;\n 1     1    11      1\n 2     1    12      1\n 3     1    13      1\n 4     2    14      1\n 5     2    15      1\n 6     2    30      1\n 7     3    16      1\n 8     3    17      1\n 9     3    18      1\n10     3    19      1\n# … with 21 more rows\n\ngraph %&gt;% \n  ggraph(., layout = \"graphopt\") + \n  geom_edge_link(color = \"grey\") + \n  geom_node_point(fill = \"orange\", \n                  shape = 21, size = 6, \n                  color = \"black\")\n\n\n\n\n\n# Examine the resulting object:\nclass(net2)\n\n[1] \"igraph\"\n\nnet2\n\nIGRAPH 9008ce1 UN-B 30 31 -- \n+ attr: type (v/l), name (v/c)\n+ edges from 9008ce1 (vertex names):\n [1] s01--U01 s01--U02 s01--U03 s02--U04 s02--U05 s02--U20 s03--U06 s03--U07\n [9] s03--U08 s03--U09 s04--U09 s04--U10 s04--U11 s05--U11 s05--U12 s05--U13\n[17] s06--U13 s06--U14 s06--U17 s07--U14 s07--U15 s07--U16 s08--U16 s08--U17\n[25] s08--U18 s08--U19 s09--U06 s09--U19 s09--U20 s10--U01 s10--U11\n\n\nNote: The remaining attributes for the nodes ( in data frame nodes2) are not (yet) a part of the graph, either with igraph or with tidygraph."
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#plot-parameters-in-igraph",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#plot-parameters-in-igraph",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "——~~ Plot parameters in igraph ——–",
    "text": "——~~ Plot parameters in igraph ——–\nCheck out the node options (starting with ‘vertex.’) and the edge options (starting with ‘edge.’).\n\n?igraph.plotting\n\nstarting httpd help server ... done\n\n\nWe can set the node & edge options in two ways - one is to specify them in the plot() function, as we are doing below.\n\nPlot with curved edges (edge.curved = .1) and reduce arrow size:\n\n\nplot(net, edge.arrow.size = .4, edge.curved = .1)\n\n\n\n# Using tidygraph\ngraph &lt;- tbl_graph(nodes, links, directed = TRUE)\ngraph\n\n# A tbl_graph: 17 nodes and 49 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 17 × 5 (active)\n  id    media               media.type type.label audience.size\n  &lt;chr&gt; &lt;chr&gt;                    &lt;int&gt; &lt;chr&gt;              &lt;int&gt;\n1 s01   NY Times                     1 Newspaper             20\n2 s02   Washington Post              1 Newspaper             25\n3 s03   Wall Street Journal          1 Newspaper             30\n4 s04   USA Today                    1 Newspaper             32\n5 s05   LA Times                     1 Newspaper             20\n6 s06   New York Post                1 Newspaper             50\n# … with 11 more rows\n#\n# Edge Data: 49 × 4\n   from    to type      weight\n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;int&gt;\n1     1     2 hyperlink     22\n2     1     3 hyperlink     22\n3     1     4 hyperlink     21\n# … with 46 more rows\n\ngraph %&gt;% ggraph(., layout = \"graphopt\") +\n  geom_edge_arc(\n    color = \"grey\",\n    strength = 0.1,\n    end_cap = circle(.2, \"cm\"),\n\n    arrow = arrow(\n      type = \"closed\",\n      ends = \"both\",\n      length = unit(3, \"mm\")\n    )\n  ) +\n  geom_node_point(\n    fill = \"orange\",\n    shape = 21,\n    size = 8,\n    color = \"black\"\n  ) +\n  geom_node_text(aes(label = id))\n\n\n\n\n\nSet node color to orange and the border color to hex 555555\nReplace the vertex label with the node names stored in “media”\n\n\nplot(\n  net,\n  edge.arrow.size = .2,\n  edge.curved = 0,\n  vertex.color = \"orange\",\n  vertex.frame.color = \"#555555\",\n  vertex.label = V(net)$media,\n  vertex.label.color = \"black\",\n  vertex.label.cex = .7\n)\n\n\n\n# Using tidygraph\n#graph &lt;- tbl_graph(nodes, links, directed = TRUE)\n#graph\ngraph %&gt;%\n  ggraph(., layout = \"gem\") +\n  geom_edge_link(\n    color = \"grey\",\n    end_cap = circle(.3, \"cm\"),\n    \n    arrow = arrow(\n      type = \"closed\",\n      ends = \"both\",\n      length = unit(1, \"mm\")\n    )\n  ) +\n  geom_node_point(\n    fill = \"orange\",\n    shape = 21,\n    size = 6,\n    color = \"#555555\"\n  ) +\n  geom_node_text(aes(label = media))\n\n\n\n\nThe second way to set attributes is to add them to the igraph object.\n\nGenerate colors based on media type:\n\n\ncolrs &lt;- c(\"gray50\", \"tomato\", \"gold\")\nV(net)$color &lt;- colrs[V(net)$media.type]\nplot(net)\n\n\n\n\n\nCompute node degrees (#links) and use that to set node size:\n\n\ndeg &lt;- igraph::degree(net, mode = \"all\")\nV(net)$size &lt;- deg*3\n# Alternatively, we can set node size based on audience size:\nV(net)$size &lt;- V(net)$audience.size*0.7\nV(net)$size\n\n [1] 14.0 17.5 21.0 22.4 14.0 35.0 39.2 23.8 42.0 16.1 23.8 23.1 16.1  8.4 16.8\n[16] 19.6 23.1\n\n# The labels are currently node IDs.\n# Setting them to NA will render no labels:\nV(net)$label.color &lt;- \"black\"\nV(net)$label &lt;- NA\n\n# Set edge width based on weight:\nE(net)$width &lt;- E(net)$weight/6\n\n#change arrow size and edge color:\nE(net)$arrow.size &lt;- .2\nE(net)$edge.color &lt;- \"gray80\"\n\n# We can even set the network layout:\ngraph_attr(net, \"layout\") &lt;- layout_with_lgl\nplot(net)\n\n\n\n\n\n# Using tidygraph\n# graph &lt;- tbl_graph(nodes, links, directed = TRUE)\n# graph\ngraph %&gt;%\n  activate(nodes) %&gt;%\n  mutate(size = centrality_degree()) %&gt;%\n  ggraph(., layout = \"lgl\") +\n  geom_edge_link(\n    aes(width = weight),\n    color = \"grey80\",\n    end_cap = circle(.2, \"cm\"),\n    arrow = arrow(\n      type = \"closed\",\n      ends = \"last\",\n      length = unit(1, \"mm\")\n    )\n  ) +\n  geom_node_point(aes(fill = type.label, size = size),\n                  shape = 21,\n                  color = \"black\") +\n  scale_fill_manual(name = \"Media Type\",\n                    values = c(\"grey50\", \"gold\", \"tomato\")) +\n  scale_edge_width(range = c(0.2, 1.5), guide = \"none\") +\n  scale_size_continuous(\"Degree\", range =  c(2, 16)) +\n  \n  guides(fill = guide_legend(title = \"Media Type\",\n                             override.aes = list(pch = 21, size = 4)))\n\n\n\n\nWe can also override the attributes explicitly in the plot:\n\nplot(net, edge.color = \"orange\", vertex.color = \"gray50\")\n\n\n\n\nWe can also add a legend explaining the meaning of the colors we used:\n\nplot(net)\nlegend(x = -2.1, y = -1.1, \n       c(\"Newspaper\",\"Television\", \"Online News\"), \n       pch = 21,col = \"#777777\", \n       pt.bg = colrs, pt.cex = 2.5, bty = \"n\", ncol = 1)\n\n\n\n# legends are automatic with the tidygraph + ggraph flow\n\nSometimes, especially with semantic networks, we may be interested in plotting only the labels of the nodes:\n\nplot(net, vertex.shape = \"none\", vertex.label = V(net)$media,\n     vertex.label.font = 2, vertex.label.color = \"gray40\",\n     vertex.label.cex = .7, edge.color = \"gray85\")\n\n\n\n#using tidygraph\n\nggraph(net, layout = \"gem\") +\n  geom_edge_link(color = \"grey80\", width = 2,\n                 end_cap = circle(0.5,\"cm\"), \n                 start_cap = circle(0.5, \"cm\")) +\n    geom_node_text(aes(label = media))\n\n\n\n\nLet’s color the edges of the graph based on their source node color. We’ll get the starting node for each edge with ends().\nNote: Edge attribute is being set by start node.\n\nedge.start &lt;- ends(net, es = E(net), names = F)[,1]\nedge.col &lt;- V(net)$color[edge.start] # How simple this is !!!\n# The three colors are recycled \n# \nplot(net, edge.color = edge.col, edge.curved = .4)\n\n\n\n\nNOTE: The source node colour has been set using the media.type, which is a node attribute. Node attributes are not typically accessible to edges. So we need to build a combo data frame using dplyr, so that edges can use this node attribute. ( There may be other ways…)\n\n# Using tidygraph\n# Make a \"combo\" data frame of nodes *and* edges with left_join()\n# Join by `from` so that type.label is based on from = edge.start\n\nlinks %&gt;%\n  left_join(., nodes, by = c(\"from\" = \"id\")) %&gt;%\n  tbl_graph(edges = ., nodes = nodes) %&gt;%\n  \n  mutate(size = centrality_degree()) %&gt;%\n  \n  ggraph(., layout = \"lgl\") +\n  geom_edge_arc(aes(color = type.label,\n                    width = weight),\n                strength = 0.3)  +\n  geom_node_point(aes(fill = type.label,\n                      # type.label is now available as edge attribute\n                      size = size),\n                  shape = 21,\n                  color = \"black\") +\n  scale_fill_manual(\n    name = \"Media Type\",\n    values = c(\"grey50\", \"gold\", \"tomato\"),\n    guide = \"legend\"\n  ) +\n  scale_edge_color_manual(name = \"Source Type\",\n                          values = c(\"grey80\", \"gold\", \"tomato\")) +\n  scale_edge_width(range = c(0.2, 1.5), guide = \"none\") +\n  scale_size_continuous(\"Degree\", range =  c(2, 16)) +\n  # not \"limits\"!\n  guides(fill = guide_legend(override.aes = list(pch = 21,\n                                                 size = 4)))"
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#network-layouts-in-igraph",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#network-layouts-in-igraph",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "——-~~ Network Layouts in ‘igraph’ ——–",
    "text": "——-~~ Network Layouts in ‘igraph’ ——–\nNetwork layouts are algorithms that return coordinates for each node in a network.\nLet’s generate a slightly larger 100-node graph using a preferential attachment model (Barabasi-Albert).\n\nnet.bg &lt;- sample_pa(n =  100, power =  1.2)\nV(net.bg)$size &lt;- 8\nV(net.bg)$frame.color &lt;- \"white\"\nV(net.bg)$color &lt;- \"orange\"\nV(net.bg)$label &lt;- \"\"\nE(net.bg)$arrow.mode &lt;- 0\nplot(net.bg)\n\n\n\n# Using tidygraph\ngraph &lt;- play_barabasi_albert(n = 100, power = 1.2)\ngraph %&gt;%\n  ggraph(., layout = \"graphopt\") +\n  geom_edge_link(color = \"grey\") +\n  geom_node_point(color = \"orange\", size = 4) +\n  theme_graph()\n\n\n\n\nNow let’s plot this network using the layouts available in igraph. You can set the layout in the plot function:\n\nplot(net.bg, layout = layout_randomly)\n\n\n\n\nOr calculate the vertex coordinates in advance:\n\nl &lt;- layout_in_circle(net.bg)\nplot(net.bg, layout = l)\n\n\n\n# Using tidygraph\n# graph &lt;- play_barabasi_albert(n = 100, power = 1.2)\ngraph %&gt;% ggraph(., layout = \"circle\") +\n  geom_edge_link(color = \"grey\") +\n  geom_node_point(color = \"orange\", size = 2) +\n  theme_graph() +\n  theme(aspect.ratio = 1)\n\n\n\n\nl is simply a matrix of x,y coordinates (N x 2) for the N nodes in the graph. You can generate your own:\n\nl\n\n                [,1]          [,2]\n  [1,]  1.000000e+00  0.000000e+00\n  [2,]  9.980267e-01  6.279052e-02\n  [3,]  9.921147e-01  1.253332e-01\n  [4,]  9.822873e-01  1.873813e-01\n  [5,]  9.685832e-01  2.486899e-01\n  [6,]  9.510565e-01  3.090170e-01\n  [7,]  9.297765e-01  3.681246e-01\n  [8,]  9.048271e-01  4.257793e-01\n  [9,]  8.763067e-01  4.817537e-01\n [10,]  8.443279e-01  5.358268e-01\n [11,]  8.090170e-01  5.877853e-01\n [12,]  7.705132e-01  6.374240e-01\n [13,]  7.289686e-01  6.845471e-01\n [14,]  6.845471e-01  7.289686e-01\n [15,]  6.374240e-01  7.705132e-01\n [16,]  5.877853e-01  8.090170e-01\n [17,]  5.358268e-01  8.443279e-01\n [18,]  4.817537e-01  8.763067e-01\n [19,]  4.257793e-01  9.048271e-01\n [20,]  3.681246e-01  9.297765e-01\n [21,]  3.090170e-01  9.510565e-01\n [22,]  2.486899e-01  9.685832e-01\n [23,]  1.873813e-01  9.822873e-01\n [24,]  1.253332e-01  9.921147e-01\n [25,]  6.279052e-02  9.980267e-01\n [26,] -1.608143e-16  1.000000e+00\n [27,] -6.279052e-02  9.980267e-01\n [28,] -1.253332e-01  9.921147e-01\n [29,] -1.873813e-01  9.822873e-01\n [30,] -2.486899e-01  9.685832e-01\n [31,] -3.090170e-01  9.510565e-01\n [32,] -3.681246e-01  9.297765e-01\n [33,] -4.257793e-01  9.048271e-01\n [34,] -4.817537e-01  8.763067e-01\n [35,] -5.358268e-01  8.443279e-01\n [36,] -5.877853e-01  8.090170e-01\n [37,] -6.374240e-01  7.705132e-01\n [38,] -6.845471e-01  7.289686e-01\n [39,] -7.289686e-01  6.845471e-01\n [40,] -7.705132e-01  6.374240e-01\n [41,] -8.090170e-01  5.877853e-01\n [42,] -8.443279e-01  5.358268e-01\n [43,] -8.763067e-01  4.817537e-01\n [44,] -9.048271e-01  4.257793e-01\n [45,] -9.297765e-01  3.681246e-01\n [46,] -9.510565e-01  3.090170e-01\n [47,] -9.685832e-01  2.486899e-01\n [48,] -9.822873e-01  1.873813e-01\n [49,] -9.921147e-01  1.253332e-01\n [50,] -9.980267e-01  6.279052e-02\n [51,] -1.000000e+00 -3.216286e-16\n [52,] -9.980267e-01 -6.279052e-02\n [53,] -9.921147e-01 -1.253332e-01\n [54,] -9.822873e-01 -1.873813e-01\n [55,] -9.685832e-01 -2.486899e-01\n [56,] -9.510565e-01 -3.090170e-01\n [57,] -9.297765e-01 -3.681246e-01\n [58,] -9.048271e-01 -4.257793e-01\n [59,] -8.763067e-01 -4.817537e-01\n [60,] -8.443279e-01 -5.358268e-01\n [61,] -8.090170e-01 -5.877853e-01\n [62,] -7.705132e-01 -6.374240e-01\n [63,] -7.289686e-01 -6.845471e-01\n [64,] -6.845471e-01 -7.289686e-01\n [65,] -6.374240e-01 -7.705132e-01\n [66,] -5.877853e-01 -8.090170e-01\n [67,] -5.358268e-01 -8.443279e-01\n [68,] -4.817537e-01 -8.763067e-01\n [69,] -4.257793e-01 -9.048271e-01\n [70,] -3.681246e-01 -9.297765e-01\n [71,] -3.090170e-01 -9.510565e-01\n [72,] -2.486899e-01 -9.685832e-01\n [73,] -1.873813e-01 -9.822873e-01\n [74,] -1.253332e-01 -9.921147e-01\n [75,] -6.279052e-02 -9.980267e-01\n [76,] -1.836910e-16 -1.000000e+00\n [77,]  6.279052e-02 -9.980267e-01\n [78,]  1.253332e-01 -9.921147e-01\n [79,]  1.873813e-01 -9.822873e-01\n [80,]  2.486899e-01 -9.685832e-01\n [81,]  3.090170e-01 -9.510565e-01\n [82,]  3.681246e-01 -9.297765e-01\n [83,]  4.257793e-01 -9.048271e-01\n [84,]  4.817537e-01 -8.763067e-01\n [85,]  5.358268e-01 -8.443279e-01\n [86,]  5.877853e-01 -8.090170e-01\n [87,]  6.374240e-01 -7.705132e-01\n [88,]  6.845471e-01 -7.289686e-01\n [89,]  7.289686e-01 -6.845471e-01\n [90,]  7.705132e-01 -6.374240e-01\n [91,]  8.090170e-01 -5.877853e-01\n [92,]  8.443279e-01 -5.358268e-01\n [93,]  8.763067e-01 -4.817537e-01\n [94,]  9.048271e-01 -4.257793e-01\n [95,]  9.297765e-01 -3.681246e-01\n [96,]  9.510565e-01 -3.090170e-01\n [97,]  9.685832e-01 -2.486899e-01\n [98,]  9.822873e-01 -1.873813e-01\n [99,]  9.921147e-01 -1.253332e-01\n[100,]  9.980267e-01 -6.279052e-02\n\nl &lt;- cbind(1:vcount(net.bg), c(1, vcount(net.bg):2))\nplot(net.bg, layout = l)\n\n\n\n# Using tidygraph\n# graph &lt;- play_barabasi_albert(n = 100, power = 1.2)\ngraph %&gt;% ggraph(., layout = l) +\n  geom_edge_link(color = \"grey\") +\n  geom_node_point(color = \"orange\", size = 2)+\n  theme_graph()\n\n\n\n\nThis layout is just an example and not very helpful - thankfully igraph has a number of built-in layouts, including:\n\nRandomly placed vertices\n\n\nl &lt;- layout_randomly(net.bg)\nplot(net.bg, layout = l)\n\n\n\n# Using tidygraph\n# graph &lt;- play_barabasi_albert(n = 100, power = 1.2)\ngraph %&gt;% ggraph(., layout = layout_randomly(.)) + \n  geom_edge_link0(colour = \"grey\") + \n  geom_node_point(colour = \"orange\", size = 4)\n\n\n\n\n\nCircle layout\n\n\nl &lt;- layout_in_circle(net.bg)\nplot(net.bg, layout = l)\n\n\n\n# Using tidygraph\n# graph &lt;- play_barabasi_albert(n = 100, power = 1.2)\ngraph %&gt;% ggraph(., layout = layout_in_circle(.)) + \n  geom_edge_link0(colour = \"grey\") + \n  geom_node_point(colour = \"orange\") +\n  theme(aspect.ratio = 1)\n\n\n\n\n\n3D sphere layout\n\n\nl &lt;- layout_on_sphere(net.bg)\nplot(net.bg, layout = l)\n\n\n\n# Using tidygraph\n# graph &lt;- play_barabasi_albert(n = 100, power = 1.2)\ngraph %&gt;% ggraph(., layout = layout_on_sphere(.)) + \n  geom_edge_link0(colour = \"grey\") + \n  geom_node_point(colour = \"orange\")\n\n\n\n\n\nThe Fruchterman-Reingold force-directed algorithm: Nice but slow, most often used in graphs smaller than ~1000 vertices.\n\n\nl &lt;- layout_with_fr(net.bg)\nplot(net.bg, layout = l)\n\n\n\n# Using tidygraph\n# graph &lt;- play_barabasi_albert(n = 100, power = 1.2)\ngraph %&gt;% ggraph(., layout = layout_with_fr(.)) + \n  geom_edge_link0(colour = \"grey\") + \n  geom_node_point(colour = \"orange\")\n\n\n\n\nYou will also notice that the F-R layout is not deterministic - different runs will result in slightly different configurations. Saving the layout in l allows us to get the exact same result multiple times.\n\npar(mfrow = c(2,2), mar = c(1,1,1,1))\nplot(net.bg, layout = layout_with_fr)\nplot(net.bg, layout = layout_with_fr)\nplot(net.bg, layout = l)\nplot(net.bg, layout = l)\n\n\n\n\nBy default, the coordinates of the plots are rescaled to the [-1,1] interval for both x and y. You can change that with the parameter rescale = FALSE and rescale your plot manually by multiplying the coordinates by a scalar. You can use norm_coords to normalize the plot with the boundaries you want. This way you can create more compact or spread out layout versions.\n\n#Get the layout coordinates:\nl &lt;- layout_with_fr(net.bg)\n# Normalize them so that they are in the -1, 1 interval:\nl &lt;- norm_coords(l, ymin = -1, ymax = 1, xmin = -1, xmax = 1)\n\npar(mfrow = c(2,2), mar = c(0,0,0,0))\nplot(net.bg, rescale = F, layout = l*0.4)\nplot(net.bg, rescale = F, layout = l*0.8)\nplot(net.bg, rescale = F, layout = l*1.2)\nplot(net.bg, rescale = F, layout = l*1.6)\n\n\n\n# Using tidygraph\n# Can't do this with tidygraph ( multiplying layout * scalar ), it seems\n\nAnother popular force-directed algorithm that produces nice results for connected graphs is Kamada Kawai. Like Fruchterman Reingold, it attempts to minimize the energy in a spring system.\n\nl &lt;- layout_with_kk(net.bg)\nplot(net.bg, layout = l)\n\n\n\n# Using tidygraph\n# graph &lt;- play_barabasi_albert(n = 100, power = 1.2)\ngraph %&gt;% ggraph(., layout = layout_with_kk(.)) + \n  geom_edge_link0(colour = \"grey\") + \n  geom_node_point(colour = \"orange\", size = 4)\n\n\n\n\nThe MDS (multidimensional scaling) algorithm tries to place nodes based on some measure of similarity or distance between them. More similar/less distant nodes are placed closer to each other. By default, the measure used is based on the shortest paths between nodes in the network. That can be changed with the dist parameter.\n\nplot(net.bg, layout = layout_with_mds)\n\n\n\n# Using tidygraph\n# graph &lt;- play_barabasi_albert(n = 100, power = 1.2)\ngraph %&gt;% ggraph(., layout = layout_with_mds(.)) + \n  geom_edge_link0(colour = \"grey\") + \n  geom_node_point(colour = \"orange\", size = 4)\n\n\n\n\nThe LGL algorithm is for large connected graphs. Here you can specify a root- the node that will be placed in the middle of the layout.\n\nplot(net.bg, layout = layout_with_lgl)\n\n\n\n# Using tidygraph\n# graph &lt;- play_barabasi_albert(n = 100, power = 1.2)\ngraph %&gt;% ggraph(., layout = layout_with_lgl(.)) + \n  geom_edge_link0(colour = \"grey\") + \n  geom_node_point(colour = \"orange\", size = 4)\n\n\n\n\nBy default, igraph uses a layout called layout_nicely which selects an appropriate layout algorithm based on the properties of the graph. Check out all available layouts in igraph:\n\n?igraph::layout_\n\n\nlayouts &lt;- grep(\"^layout_\", ls(\"package:igraph\"), value = TRUE)[-1]\n\n# Remove layouts that do not apply to our graph.\nlayouts &lt;- layouts[!grepl(\"bipartite|merge|norm|sugiyama|tree\", layouts)]\n\npar(mfrow = c(3,3), mar = c(1,1,1,1))\n\nfor (layout in layouts) {\n  print(layout)\n  l &lt;- do.call(layout, list(net))\n  plot(net, edge.arrow.mode = 0, layout = l, main = layout) }\n\n[1] \"layout_as_star\"\n\n\n[1] \"layout_components\"\n\n\n[1] \"layout_in_circle\"\n\n\n[1] \"layout_nicely\"\n\n\n[1] \"layout_on_grid\"\n\n\n[1] \"layout_on_sphere\"\n\n\n[1] \"layout_randomly\"\n\n\n[1] \"layout_with_dh\"\n\n\n[1] \"layout_with_drl\"\n\n\n\n\n\n[1] \"layout_with_fr\"\n\n\n[1] \"layout_with_gem\"\n\n\n[1] \"layout_with_graphopt\"\n\n\n[1] \"layout_with_kk\"\n\n\n[1] \"layout_with_lgl\"\n\n\n[1] \"layout_with_mds\""
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#highlighting-specific-nodes-or-links",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#highlighting-specific-nodes-or-links",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "——-~~ Highlighting specific nodes or links ——–",
    "text": "——-~~ Highlighting specific nodes or links ——–\nSometimes we want to focus the visualization on a particular node or a group of nodes. Let’s represent distance from the NYT:\n\n\ndistances() calculates shortest path from vertices in ‘v’ to ones in ‘to’.\n\n\ndist.from.NYT &lt;- distances(net, \n                           v = V(net)[media == \"NY Times\"], \n                           to = V(net), \n                           weights = NA)\n\n#Set colors to plot the distances:\noranges &lt;- colorRampPalette(c(\"dark red\", \"gold\"))\ncol &lt;- oranges(max(dist.from.NYT)+1)\ncol &lt;- col[dist.from.NYT+1]\n\n# Let's have same coordinates for Nodes in both graph renderings\n# Then we can verify that the distance calculations are the same for both renderings\ncoords &lt;- igraph::layout_nicely(net)\nplot(net, vertex.label = dist.from.NYT,\n     vertex.color = col, vertex.label.color = \"black\",\n     layout = coords)\n\n\n\n\n\n# Using tidygraph\ngraph &lt;- tbl_graph(nodes, links, directed = TRUE)\ngraph\n\n# A tbl_graph: 17 nodes and 49 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 17 × 5 (active)\n  id    media               media.type type.label audience.size\n  &lt;chr&gt; &lt;chr&gt;                    &lt;int&gt; &lt;chr&gt;              &lt;int&gt;\n1 s01   NY Times                     1 Newspaper             20\n2 s02   Washington Post              1 Newspaper             25\n3 s03   Wall Street Journal          1 Newspaper             30\n4 s04   USA Today                    1 Newspaper             32\n5 s05   LA Times                     1 Newspaper             20\n6 s06   New York Post                1 Newspaper             50\n# … with 11 more rows\n#\n# Edge Data: 49 × 4\n   from    to type      weight\n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;int&gt;\n1     1     2 hyperlink     22\n2     1     3 hyperlink     22\n3     1     4 hyperlink     21\n# … with 46 more rows\n\n# Set up NY Times as root node first\n# V(net)[media == \"NY Times\"] cannot be used since it returns an `igraph.vs` ( i.e. a list ) object. \n# We need an integer node id.\nroot_nyt &lt;- graph %&gt;%\n  activate(nodes) %&gt;%\n  as_tibble() %&gt;%\n  rowid_to_column(var = \"node_id\") %&gt;%\n  filter(media == \"NY Times\") %&gt;%\n  select(node_id) %&gt;% as_vector()\nroot_nyt\n\nnode_id \n      1 \n\ngraph %&gt;%\n  activate(nodes) %&gt;%\n  mutate(size = centrality_degree()) %&gt;%\n  \n  # new stuff:\n  # breadth first search for all distances from the root node\n  mutate(order = bfs_dist(root = root_nyt)) %&gt;%\n  \n  ggraph(., layout = coords) + # same layout\n  geom_edge_link(\n    aes(width = weight),\n    color = \"grey80\",\n    end_cap = circle(.2, \"cm\"),\n    arrow = arrow(\n      type = \"closed\",\n      ends = \"last\",\n      length = unit(1, \"mm\")\n    )\n  ) +\n  geom_node_point(aes(fill = order,\n                      size = size),\n                  shape = 21,\n                  color = \"black\") +\n  \n  geom_node_text(aes(label = order)) +\n  \n  scale_fill_gradient(\n    name = \"Distance from NY Times\",\n    low = \"dark red\",\n    high = \"gold\",\n    guide = \"legend\"\n  ) +\n  scale_edge_width(range = c(0.2, 1.5), guide = \"none\") +\n  scale_size_continuous(\"Degree\", range =  c(2, 16)) +\n  guides(fill = guide_legend(override.aes = list(pch = 21,\n                                                 size = 4)))\n\n\n\n\nOr, a bit more readable:\n\nplot(net, vertex.color = col, \n     vertex.label = dist.from.NYT, edge.arrow.size = .6,\n     vertex.label.color = \"white\", \n     vertex.size = V(net)$size*1.6, \n     edge.width = 2,\n     layout = norm_coords(layout_with_lgl(net))*1.4, rescale = F)"
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#path-highlighting",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#path-highlighting",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "Path Highlighting",
    "text": "Path Highlighting\nWe can also highlight paths between the nodes in the network.\n\nSay here between MSNBC and the New York Post\n\n\nnews.path &lt;- shortest_paths(net,\n                            from  =  V(net)[media == \"MSNBC\"],\n                            to   =  V(net)[media == \"New York Post\"],\n                            output  =  \"both\")  #both path nodes and edges\nnews.path.distance &lt;- distances(net,\n                                V(net)[media == \"MSNBC\"],\n                                V(net)[media == \"New York Post\"] )\nnews.path\n\n$vpath\n$vpath[[1]]\n+ 4/17 vertices, named, from 8e65729:\n[1] s08 s03 s12 s06\n\n\n$epath\n$epath[[1]]\n+ 3/48 edges from 8e65729 (vertex names):\n[1] s08-&gt;s03 s03-&gt;s12 s12-&gt;s06\n\n\n$predecessors\nNULL\n\n$inbound_edges\nNULL\n\nnews.path.distance\n\n    s06\ns08   5\n\n#Generate edge color variable to plot the path:\necol &lt;- rep(\"gray80\", ecount(net))\necol[unlist(news.path$epath)] &lt;- \"orange\"\n\n#Generate edge width variable to plot the path:\new &lt;- rep(2, ecount(net))\new[unlist(news.path$epath)] &lt;- 4\n\n#Generate node color variable to plot the path:\nvcol &lt;- rep(\"gray40\", vcount(net))\nvcol[unlist(news.path$vpath)] &lt;- \"gold\"\n\nplot(net, vertex.color = vcol, \n     edge.color = ecol,\n     edge.width = ew, \n     edge.arrow.mode = 0,\n     ## added lines\n     vertex.label = V(net)$media,\n     vertex.label.font = 2, \n     vertex.label.color = \"gray40\",\n     vertex.label.cex = .7,\n     layout = coords * 1.5)\n\n\n\n\n\n# Using tidygraph\n# We need to use:\n# to_shortest_path(graph, from, to, mode = \"out\", weights = NULL)\n# Let's set up `to` and `from` nodes\n#\n# V(net)[media == \"NY Times\"] cannot be used since it returns an `igraph.vs` ( i.e. a list ) object.\n# We need integer node ids for `from` and `to` in `to_shortest_path`\n\nmsnbc &lt;- graph %&gt;%\n  activate(nodes) %&gt;%\n  as_tibble() %&gt;%\n  rowid_to_column(var = \"node_id\") %&gt;%\n  filter(media == \"MSNBC\") %&gt;%\n  select(node_id) %&gt;% as_vector()\nmsnbc\n\nnode_id \n      8 \n\nnypost &lt;- graph %&gt;%\n  activate(nodes) %&gt;%\n  as_tibble() %&gt;%\n  rowid_to_column(var = \"node_id\") %&gt;%\n  filter(media == \"New York Post\") %&gt;%\n  select(node_id) %&gt;% as_vector()\nnypost\n\nnode_id \n      6 \n\n# Let's create a fresh graph object using morph\n# However we want to merge it back with the original `graph`\n# to get an overlay plot\n#\n# # Can do this to obtain a separate graph\n# convert(to_shortest_path,from = msnbc,to = nypost)\n# However we want to merge it back with the original `graph`\n# to get an overlay plot\nmsnbc_nyp &lt;-\n  graph %&gt;%\n  # first mark all nodes and edges as *not* on the shortest path\n  activate(nodes) %&gt;%\n  mutate(shortest_path_node = FALSE) %&gt;%\n  activate(edges) %&gt;%\n  mutate(shortest_path_edge = FALSE) %&gt;%\n  \n  # Find shortest path between MSNBC and NY Post\n  morph(to_shortest_path, from = msnbc, to = nypost) %&gt;%\n  \n  # Now to mark the shortest_path nodes as TRUE\n  activate(nodes) %&gt;%\n  mutate(shortest_path_node = TRUE) %&gt;%\n  \n  # Now to mark the shortest_path edges as TRUE\n  activate(edges) %&gt;%\n  mutate(shortest_path_edge = TRUE) %&gt;%\n  #\n  # Merge back into main graph; Still saving it as a `msnbc_nyp`\n  unmorph()\nmsnbc_nyp\n\n# A tbl_graph: 17 nodes and 49 edges\n#\n# A directed multigraph with 1 component\n#\n# Edge Data: 49 × 5 (active)\n   from    to type      weight shortest_path_edge\n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;int&gt; &lt;lgl&gt;             \n1     1     2 hyperlink     22 FALSE             \n2     1     3 hyperlink     22 FALSE             \n3     1     4 hyperlink     21 FALSE             \n4     1    15 mention       20 FALSE             \n5     2     1 hyperlink     23 FALSE             \n6     2     3 hyperlink     21 FALSE             \n# … with 43 more rows\n#\n# Node Data: 17 × 6\n  id    media               media.type type.label audience.size shortest_path_n…\n  &lt;chr&gt; &lt;chr&gt;                    &lt;int&gt; &lt;chr&gt;              &lt;int&gt; &lt;lgl&gt;           \n1 s01   NY Times                     1 Newspaper             20 FALSE           \n2 s02   Washington Post              1 Newspaper             25 FALSE           \n3 s03   Wall Street Journal          1 Newspaper             30 TRUE            \n# … with 14 more rows\n\nmsnbc_nyp %&gt;%\n  activate(nodes) %&gt;%\n  mutate(size = centrality_degree()) %&gt;%\n  ggraph(layout = coords) +\n  #geom_edge_link0(colour = \"grey\") +\n  geom_edge_link0(aes(colour = shortest_path_edge,\n                      width = shortest_path_edge)) +\n  \n  geom_node_point(aes(size = size,\n                      fill = shortest_path_node), shape = 21) +\n  geom_node_text(aes(label = media)) +\n  \n  scale_size_continuous(\"Degree\", range =  c(2, 16)) +\n  scale_fill_manual(\"Shortest Path\",\n                    values = c(\"grey\", \"gold\")) +\n  \n  scale_edge_width_manual(values = c(1, 4)) +\n  \n  scale_edge_colour_manual(values = c(\"grey\", \"orange\")) +\n  guides(\n    fill = guide_legend(override.aes = list(pch = 21,\n                                            size = 6)),\n    edge_colour = \"none\",\n    edge_width = \"none\"\n  )\n\n\n\n\n\nHighlight the edges going into or out of a vertex, for instance the WSJ. For a single node, use incident(), for multiple nodes use incident_edges()\n\n\n\ninc.edges &lt;-\n  incident(net, V(net)[media == \"Wall Street Journal\"], mode = \"all\")\n\n#Set colors to plot the selected edges.\necol &lt;- rep(\"gray80\", ecount(net))\necol[inc.edges] &lt;- \"orange\"\nvcol &lt;- rep(\"grey40\", vcount(net))\nvcol[V(net)$media == \"Wall Street Journal\"] &lt;- \"gold\"\nplot(\n  net,\n  vertex.color = vcol,\n  edge.color = ecol,\n  edge.width = 2,\n  layout = coords\n)\n\n\n\n\n\n# Using tidygraph\nwsj &lt;- graph %&gt;% \n  activate(nodes) %&gt;% \n  as_tibble() %&gt;% \n  rowid_to_column(var = \"node_id\") %&gt;% \n  filter(media == \"Wall Street Journal\") %&gt;% \n  select(node_id) %&gt;% as_vector()\n\ngraph %&gt;% \n  activate(nodes) %&gt;% \n  mutate(wsj_adjacent = node_is_adjacent(to = wsj, mode = \"all\", \n                                         include_to = TRUE),\n         size = centrality_degree()) %&gt;% \n  mutate(WSJ = if_else(media == \"Wall Street Journal\", TRUE, FALSE)) %&gt;% \n  activate(edges) %&gt;% \n  mutate(wsj_links = edge_is_incident(wsj)) %&gt;% \n  \n  ggraph(., layout = coords) +\n  geom_edge_link0(aes(colour = wsj_links), width = 2) + \n  \n  geom_node_point(aes(fill = WSJ, \n                      size = size),shape = 21) +\n  \n  geom_node_text(aes(label = media), repel = TRUE) + \n  \n  scale_fill_manual(\"WSJ Neighbours\", \n                      values = c(\"grey\", \"gold\"), \n                      guide = guide_legend(override.aes = \n                                             list(pch = 21, \n                                                  size = 5))) + \n  scale_edge_colour_manual(\"WSJ Links\", \n                      values = c(\"grey\", \"orange\")) + \n  scale_size(\"Degree\", range = c( 2, 16)) +\n  ggtitle(label = \"Highlighting WSJ Neighbours and Links\") +\n  guides(shape = \"none\", fill = \"none\" #, colour = \"none\"\n  )"
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#highlight-neighbours",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#highlight-neighbours",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "Highlight Neighbours",
    "text": "Highlight Neighbours\nOr we can highlight the immediate neighbors of a vertex, say WSJ. The neighbors function finds all nodes one step out from the focal actor. To find the neighbors for multiple nodes, use adjacent_vertices(). To find node neighborhoods going more than one step out, use function ego() with parameter order set to the number of steps out to go from the focal node(s).\n\nneigh.nodes &lt;- neighbors(net, V(net)[media == \"Wall Street Journal\"], mode = \"out\")\n\n# Set colors to plot the neighbors:\nvcol[neigh.nodes] &lt;- \"#ff9d00\"\nplot(net, vertex.color = vcol)\n\n\n\n\n\n# Using tidygraph\nwsj &lt;- graph %&gt;% \n  activate(nodes) %&gt;% \n  as_tibble() %&gt;% \n  rowid_to_column(var = \"node_id\") %&gt;% \n  filter(media == \"Wall Street Journal\") %&gt;% \n  select(node_id) %&gt;% as_vector()\n\ngraph %&gt;% \n  activate(nodes) %&gt;% \n  mutate(wsj_adjacent = node_is_adjacent(to = wsj, mode = \"all\", \n  # remove WSJ from the list!\n  # highlight only the neighbours\n  \n                                         include_to = FALSE),\n         size = centrality_degree()) %&gt;% \n  mutate(WSJ = if_else(media == \"Wall Street Journal\", TRUE, FALSE)) %&gt;% \n  activate(edges) %&gt;% \n  mutate(wsj_links = edge_is_incident(wsj)) %&gt;% \n  \n  ggraph(., layout = coords) +\n  geom_edge_link0(aes(colour = wsj_links), width = 2) + \n  \n  geom_node_point(aes(fill = wsj_adjacent, \n                      size = size),shape = 21) +\n  \n  geom_node_text(aes(label = media), repel = TRUE) + \n  \n  scale_fill_manual(\"WSJ Neighbours\", \n                      values = c(\"grey\", \"gold\"), \n                      guide = guide_legend(override.aes = \n                                             list(pch = 21, \n                                                  size = 5))) + \n  scale_edge_colour_manual(\"WSJ Links\", \n                      values = c(\"grey\", \"orange\")) + \n  scale_size(\"Degree\", range = c( 2, 16)) +\n  ggtitle(label = \"Highlighting WSJ Neighbours and Links\") +\n  guides(shape = \"none\", fill = \"none\" #, colour = \"none\"\n  )\n\n\n\n\nAnother way to draw attention to a group of nodes: (This is generally not recommended since, depending on layout, nodes that are not ‘marked’ can accidentally get placed on top of the mark)\n\nplot(net, mark.groups = c(1,4,5,8), mark.col = \"#C5E5E7\", mark.border = NA)\n\n\n\n# Mark multiple groups:\nplot(net, mark.groups = list(c(1,4,5,8), c(15:17)),\n          mark.col = c(\"#C5E5E7\",\"#ECD89A\"), mark.border = NA)"
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#interactive-plotting-with-tkplot",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#interactive-plotting-with-tkplot",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "——-~~ Interactive plotting with ‘tkplot’ ——–",
    "text": "——-~~ Interactive plotting with ‘tkplot’ ——–\nR and igraph offer interactive plotting capabilities (mostly helpful for small networks)\n\ntkid &lt;- tkplot(net) #tkid is the id of the tkplot\n\nl &lt;- tkplot.getcoords(tkid) # grab the coordinates from tkplot\nplot(net, layout = l)"
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#other-ways-to-represent-a-network",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#other-ways-to-represent-a-network",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "——-~~ Other ways to represent a network ——–",
    "text": "——-~~ Other ways to represent a network ——–\nOne reminder that there are other ways to represent a network:\n\nHeatmap of the network matrix:\n\n\nnetm &lt;- as_adjacency_matrix(net, attr = \"weight\", sparse = F)\ncolnames(netm) &lt;- V(net)$media\nrownames(netm) &lt;- V(net)$media\n\npalf &lt;- colorRampPalette(c(\"gold\", \"dark orange\"))\n\n# The Rowv & Colv parameters turn dendrograms on and off\nheatmap(netm[,17:1], Rowv  =  NA, Colv  =  NA, col  =  palf(20),\n        scale = \"none\", margins = c(10,10) )\n\n\n\n\n\nDegree distribution\n\n\ndeg.dist &lt;- degree_distribution(net, cumulative = T, mode = \"all\")\n# degree is available in `sna` too\nplot(x = 0:max(igraph::degree(net)), y = 1-deg.dist, pch = 19, cex = 1.4, col = \"orange\", xlab = \"Degree\", ylab = \"Cumulative Frequency\")\n\n\n\n# Using Tidygraph\n# https://stackoverflow.com/questions/18356860/cumulative-histogram-with-ggplot2\ngraph %&gt;% \n  activate(nodes) %&gt;% \n  mutate(degree = centrality_degree(mode = \"all\")) %&gt;% \n  as_tibble() %&gt;% \n  ggplot(aes(x = degree, y = stat(count))) +\n  # geom_histogram(aes(y = cumsum(..count..)), binwidth = 1) + \n  stat_bin(aes(y = cumsum(..count..)),\n                binwidth = 1,# Ta-Da !!\n                geom =\"point\",color =\"orange\", size = 5)\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead."
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#plotting-two-mode-networks",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#plotting-two-mode-networks",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "4. Plotting two-mode networks",
    "text": "4. Plotting two-mode networks\n\nhead(nodes2)\n\n   id   media media.type media.name audience.size\n1 s01     NYT          1  Newspaper            20\n2 s02    WaPo          1  Newspaper            25\n3 s03     WSJ          1  Newspaper            30\n4 s04    USAT          1  Newspaper            32\n5 s05 LATimes          1  Newspaper            20\n6 s06     CNN          2         TV            56\n\nhead(links2)\n\n    U01 U02 U03 U04 U05 U06 U07 U08 U09 U10 U11 U12 U13 U14 U15 U16 U17 U18 U19\ns01   1   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\ns02   0   0   0   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\ns03   0   0   0   0   0   1   1   1   1   0   0   0   0   0   0   0   0   0   0\ns04   0   0   0   0   0   0   0   0   1   1   1   0   0   0   0   0   0   0   0\ns05   0   0   0   0   0   0   0   0   0   0   1   1   1   0   0   0   0   0   0\ns06   0   0   0   0   0   0   0   0   0   0   0   0   1   1   0   0   1   0   0\n    U20\ns01   0\ns02   1\ns03   0\ns04   0\ns05   0\ns06   0\n\nnet2\n\nIGRAPH 9008ce1 UN-B 30 31 -- \n+ attr: type (v/l), name (v/c)\n+ edges from 9008ce1 (vertex names):\n [1] s01--U01 s01--U02 s01--U03 s02--U04 s02--U05 s02--U20 s03--U06 s03--U07\n [9] s03--U08 s03--U09 s04--U09 s04--U10 s04--U11 s05--U11 s05--U12 s05--U13\n[17] s06--U13 s06--U14 s06--U17 s07--U14 s07--U15 s07--U16 s08--U16 s08--U17\n[25] s08--U18 s08--U19 s09--U06 s09--U19 s09--U20 s10--U01 s10--U11\n\nplot(net2)\n\n\n\n\nThis time we will make nodes look different based on their type. Media outlets are blue squares, audience nodes are orange circles:\n\nV(net2)$color &lt;- c(\"steel blue\", \"orange\")[V(net2)$type+1]\nV(net2)$shape &lt;- c(\"square\", \"circle\")[V(net2)$type+1]\n\n# Media outlets will have name labels, audience members will not:\nV(net2)$label &lt;- \"\"\nV(net2)$label[V(net2)$type == F] &lt;- nodes2$media[V(net2)$type == F]\nV(net2)$label.cex = .6\nV(net2)$label.font = 2\n\nplot(net2, vertex.label.color = \"white\", vertex.size = (2-V(net2)$type)*8)\n\n\n\n\n\n# Using tidygraph\nas_tbl_graph(x = links2, directed = TRUE) %&gt;%\n  activate(nodes) %&gt;%\n  left_join(nodes2, by = c(\"name\" = \"id\")) %&gt;%\n  ggraph(layout = \"nicely\") +\n  geom_edge_link0() +\n  geom_node_point(aes(shape = type, fill = type, size = type)) +\n  geom_node_text(aes(label = if_else(type, \"\", media)), colour = \"white\", size = 3) +\n  \n  scale_shape_manual(\n    \"Type\",\n    values = c(22, 21),\n    labels = c(\"Media\", \"Persons\"),\n    guide = guide_legend(override.aes = list(size = 6))\n  ) +\n  \n  scale_fill_manual(\n    \"Type\",\n    values = c(\"dodgerblue\", \"orange\"),\n    labels = c(\"Media\", \"Persons\")\n  ) +\n  \n  scale_size_manual(values = c(10, 4), guide = \"none\") \n\n\n\n\nigraph has a built-in bipartite layout, though it’s not the most helpful:\n\nplot(net2, vertex.label = NA, vertex.size = 7, layout = layout_as_bipartite)\n\n\n\n# using tidygraph\nas_tbl_graph(x = links2, directed = TRUE) %&gt;%\n  activate(nodes) %&gt;%\n  left_join(nodes2, by = c(\"name\" = \"id\")) %&gt;%\n  ggraph(., layout = \"igraph\", algorithm = \"bipartite\") +\n  geom_edge_link0() +\n  geom_node_point(aes(shape = type, fill = type, size = type)) +\n  geom_node_text(aes(label = if_else(type, \"\", media)), colour = \"white\", size = 3) +\n  \n  scale_shape_manual(\n    \"Type\",\n    values = c(22, 21),\n    labels = c(\"Media\", \"Persons\"),\n    guide = guide_legend(override.aes = list(size = 6))\n  ) +\n  \n  scale_fill_manual(\n    \"Type\",\n    values = c(\"dodgerblue\", \"orange\"),\n    labels = c(\"Media\", \"Persons\")\n  ) +\n  \n  scale_size_manual(values = c(10, 4), guide = \"none\") \n\n\n\n\n\nUsing text as nodes:\n\n\npar(mar = c(0,0,0,0))\nplot(net2, vertex.shape = \"none\", vertex.label = nodes2$media,\n     vertex.label.color = V(net2)$color, vertex.label.font = 2,\n     vertex.label.cex = .95, edge.color = \"gray70\",  edge.width = 2)\n\n\n\n# Using tidygraph\nas_tbl_graph(x = links2, directed = TRUE) %&gt;%\n  activate(nodes) %&gt;%\n  left_join(nodes2, by = c(\"name\" = \"id\")) %&gt;%\n  ggraph(layout = \"nicely\") +\n  geom_edge_link(end_cap = circle(.4,\"cm\"), \n                 start_cap = circle(0.4, \"cm\")) +\n  # geom_node&gt;point(aes(shape = type, fill = type, size = type)) +\n  geom_node_text(aes(label= media, colour = type), size = 4) +\n  \n  scale_shape_manual(\n    \"Type\",\n    values = c(22, 21),\n    labels = c(\"Media\", \"Persons\"),\n    guide = guide_legend(override.aes = list(size = 4))\n  ) +\n  \n  scale_fill_manual(\n    \"Type\",\n    values = c(\"dodgerblue\", \"orange\"),\n    labels = c(\"Media\", \"Persons\")\n  ) +\n  \n  scale_size_manual(values = c(10, 4), guide = \"none\") \n\n\n\n\n\nUsing images as nodes You will need the ‘png’ package to do this:\n\n\n# install.packages(\"png\")\nlibrary(\"png\")\n\nimg.1 &lt;- readPNG(\"./images/news.png\")\nimg.2 &lt;- readPNG(\"./images/user.png\")\n\nV(net2)$raster &lt;- list(img.1, img.2)[V(net2)$type+1]\n\npar(mar = c(3,3,3,3))\n\nplot(net2, vertex.shape = \"raster\", vertex.label = NA,\n     vertex.size = 16, vertex.size2 = 16, edge.width = 2)\n\n\n# By the way, you can also add any image you want to any plot. For example, many #network graphs could be improved by a photo of a puppy carrying a basket full of kittens.\nimg.3 &lt;- readPNG(\"./images/puppy.png\")\nrasterImage(img.3,  xleft = -1.7, xright = 0, ybottom = -1.2, ytop = 0)\n\n\n\n# The numbers after the image are coordinates for the plot.\n# The limits of your plotting area are given in par()$usr\n\n\n# Using ~~tidygraph~~ visNetwork\n# See this cheatsheet:\n# system.file(\"fontAwesome/Font_Awesome_Cheatsheet.pdf\", package = \"visNetwork\")\nlibrary(visNetwork)\n\nas_tbl_graph(x = links2, directed = TRUE) %&gt;%\n  activate(nodes) %&gt;%\n  left_join(nodes2, by = c(\"name\" = \"id\")) %&gt;% \n  \n  # visNetwork needs a \"group\" variable for grouping...\n  mutate(group = as.character(type)) %&gt;% \n  visIgraph(.) %&gt;% \n  visGroups(groupname = \"FALSE\",shape = \"icon\", \n            icon = list(code = \"f26c\", size = 75, color = \"orange\")) %&gt;% \n  visGroups(groupname = \"TRUE\",shape = \"icon\", \n            icon = list(code = \"f007\", size = 75)) %&gt;% \n  addFontAwesome()\n\n\n\n\n\nWe can also generate and plot bipartite projections for the two-mode network : (co-memberships are easy to calculate by multiplying the network matrix by its transposed matrix, or using igraph’s bipartite.projection function)\n\nnet2.bp &lt;- bipartite.projection(net2)\n\n#We can calculate the projections manually as well:\nas_incidence_matrix(net2)  %*% t(as_incidence_matrix(net2))\n\n    s01 s02 s03 s04 s05 s06 s07 s08 s09 s10\ns01   3   0   0   0   0   0   0   0   0   1\ns02   0   3   0   0   0   0   0   0   1   0\ns03   0   0   4   1   0   0   0   0   1   0\ns04   0   0   1   3   1   0   0   0   0   1\ns05   0   0   0   1   3   1   0   0   0   1\ns06   0   0   0   0   1   3   1   1   0   0\ns07   0   0   0   0   0   1   3   1   0   0\ns08   0   0   0   0   0   1   1   4   1   0\ns09   0   1   1   0   0   0   0   1   3   0\ns10   1   0   0   1   1   0   0   0   0   2\n\nt(as_incidence_matrix(net2)) %*%   as_incidence_matrix(net2)\n\nWarning in rm(list = cmd, envir = .tkplot.env): object 'tkp.1' not found\n\n\n    U01 U02 U03 U04 U05 U06 U07 U08 U09 U10 U11 U12 U13 U14 U15 U16 U17 U18 U19\nU01   2   1   1   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0\nU02   1   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\nU03   1   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\nU04   0   0   0   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\nU05   0   0   0   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\nU06   0   0   0   0   0   2   1   1   1   0   0   0   0   0   0   0   0   0   1\nU07   0   0   0   0   0   1   1   1   1   0   0   0   0   0   0   0   0   0   0\nU08   0   0   0   0   0   1   1   1   1   0   0   0   0   0   0   0   0   0   0\nU09   0   0   0   0   0   1   1   1   2   1   1   0   0   0   0   0   0   0   0\nU10   0   0   0   0   0   0   0   0   1   1   1   0   0   0   0   0   0   0   0\nU11   1   0   0   0   0   0   0   0   1   1   3   1   1   0   0   0   0   0   0\nU12   0   0   0   0   0   0   0   0   0   0   1   1   1   0   0   0   0   0   0\nU13   0   0   0   0   0   0   0   0   0   0   1   1   2   1   0   0   1   0   0\nU14   0   0   0   0   0   0   0   0   0   0   0   0   1   2   1   1   1   0   0\nU15   0   0   0   0   0   0   0   0   0   0   0   0   0   1   1   1   0   0   0\nU16   0   0   0   0   0   0   0   0   0   0   0   0   0   1   1   2   1   1   1\nU17   0   0   0   0   0   0   0   0   0   0   0   0   1   1   0   1   2   1   1\nU18   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   1   1   1\nU19   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   1   1   1   2\nU20   0   0   0   1   1   1   0   0   0   0   0   0   0   0   0   0   0   0   1\n    U20\nU01   0\nU02   0\nU03   0\nU04   1\nU05   1\nU06   1\nU07   0\nU08   0\nU09   0\nU10   0\nU11   0\nU12   0\nU13   0\nU14   0\nU15   0\nU16   0\nU17   0\nU18   0\nU19   1\nU20   2\n\npar(mfrow = c(1, 2))\n\nplot(\n  net2.bp$proj1,\n  vertex.label.color = \"black\",\n  vertex.label.dist = 2,\n  vertex.label = nodes2$media[!is.na(nodes2$media.type)]\n)\n\nplot(\n  net2.bp$proj2,\n  vertex.label.color = \"black\",\n  vertex.label.dist = 2,\n  vertex.label = nodes2$media[is.na(nodes2$media.type)]\n)\n\n\n\n\n\n# Using tidygraph\n# Calculate projections and add attributes/labels\nproj1 &lt;-\n  as_incidence_matrix(net2)  %*% t(as_incidence_matrix(net2)) %&gt;%\n  as_tbl_graph() %&gt;%\n  activate(nodes) %&gt;%\n  left_join(., nodes2, by = c(\"name\" = \"id\"))\n\nWarning in (is.null(rownames(x)) && is.null(colnames(x))) || colnames(x) == :\n'length(x) = 10 &gt; 1' in coercion to 'logical(1)'\n\nproj2 &lt;-\n  t(as_incidence_matrix(net2)) %*%   as_incidence_matrix(net2) %&gt;% as_tbl_graph() %&gt;%\n  activate(nodes) %&gt;%\n  left_join(., nodes2, by = c(\"name\" = \"id\"))\n\nWarning in (is.null(rownames(x)) && is.null(colnames(x))) || colnames(x) == :\n'length(x) = 20 &gt; 1' in coercion to 'logical(1)'\n\np1 &lt;- proj1 %&gt;%\n  ggraph(layout = \"graphopt\") +\n  geom_edge_link0() +\n  geom_node_point(size = 6, colour = \"orange\") +\n  geom_node_text(aes(label = media), repel = TRUE)\n\np2 &lt;- proj2 %&gt;%\n  ggraph(layout = \"graphopt\") +\n  geom_edge_link0() +\n  geom_node_point(\n    aes(colour = media.type),\n    size = 6,\n    shape  = 15,\n    colour = \"dodgerblue\"\n  ) +\n  geom_node_text(aes(label = media), repel = TRUE)\n\np1 + p2"
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#plotting-multiplex-networks",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#plotting-multiplex-networks",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "5. Plotting multiplex networks",
    "text": "5. Plotting multiplex networks\nIn some cases, the networks we want to plot are multigraphs: they can have multiple edges connecting the same two nodes. A related concept, multiplex networks, contain multiple types of ties – e.g. friendship, romantic, and work relationships between individuals.\nIn our example network, we also have two tie types: hyperlinks and mentions. One thing we can do is plot each type of tie separately:\n\nE(net)$width &lt;- 2\nplot(\n  net,\n  edge.color = c(\"dark red\", \"slategrey\")[(E(net)$type == \"hyperlink\") +\n                                            1],\n  vertex.color = \"gray40\",\n  layout = layout_in_circle,\n  edge.curved = .3\n)\n\n\n\n# Another way to delete edges using the minus operator:\nnet.m &lt;- net - E(net)[E(net)$type == \"hyperlink\"]\nnet.h &lt;- net - E(net)[E(net)$type == \"mention\"]\n\n#Plot the two links separately:\npar(mfrow = c(1, 2))\n\nplot(net.h,\n     vertex.color = \"orange\",\n     layout = layout_with_fr,\n     main = \"Tie: Hyperlink\")\nplot(net.m,\n     vertex.color = \"lightsteelblue2\",\n     layout = layout_with_fr,\n     main = \"Tie: Mention\")\n\n\n\n\n\nMake sure the nodes stay in the same place in both plots:\n\n\npar(mfrow = c(1, 2), mar = c(1, 1, 4, 1))\n\nl &lt;- layout_with_fr(net)\nplot(net.h,\n     vertex.color = \"orange\",\n     layout = l,\n     main = \"Tie: Hyperlink\")\nplot(net.m,\n     vertex.color = \"lightsteelblue2\",\n     layout = l,\n     main = \"Tie: Mention\")\n\n\n\n\n\n#Using tidygraph\n\nlayout &lt;- layout_in_circle(net)\np1 &lt;- tbl_graph(nodes, links, directed = TRUE) %&gt;% \n  activate(nodes) %&gt;% \n  mutate(size = centrality_degree()) %&gt;% \n  activate(edges) %&gt;% \n  filter(type == \"hyperlink\") %&gt;% \n  \n  # reusing the earlier computed layout\n  ggraph(layout = layout) +\n  geom_edge_arc(strength = 0.05) +\n  geom_node_point(aes(size = size), shape = 21, \n                  fill = \"orange\") +\n  scale_size(range = c(2, 12)) + \n  labs(title = \"Tie: Hyperlink\") + \n  theme(aspect.ratio = 1,,\n        legend.position = \"bottom\")\n\np2 &lt;- tbl_graph(nodes, links, directed = TRUE) %&gt;% \n  activate(nodes) %&gt;% \n  mutate(size = centrality_degree()) %&gt;% \n  activate(edges) %&gt;% \n  filter(type == \"mention\") %&gt;% \n   # reusing the earlier computed layout\n  ggraph(layout = layout) +\n  geom_edge_arc(strength = 0.05) +\n  geom_node_point(aes(size = size), shape = 21, \n                  fill = \"lightsteelblue2\") +\n  scale_size(range = c(2, 12)) + \n  labs(title = \"Tie: Mention\") + \n  theme(aspect.ratio = 1, legend.position = \"bottom\")\n\nwrap_plots(p1, p2,guides = \"collect\") & \n  # note this \"pipe\" for patchwork!\n  theme(legend.position = \"none\")\n\n\n\n\nIn our example network, we don’t have node dyads connected by multiple types of connections (we never have both a ‘hyperlink’ and a ‘mention’ tie between the same two news outlets) – however that could happen.\nNote: See the edges between s03 and s10…these are in opposite directions. So no dyads.\n\nlayout &lt;- layout_in_circle(net)\ntbl_graph(nodes, links, directed = TRUE) %&gt;%  \n  activate(nodes) %&gt;% \n  mutate(size = centrality_degree()) %&gt;% \n\n  # reusing the earlier computed layout\n  ggraph(layout = layout) +\n  geom_edge_arc(strength = 0.05, aes(colour = type)) +\n  geom_node_point(aes(size = size), shape = 21, \n                  fill = \"orange\") +\n  geom_node_text(aes(label = id), repel = TRUE) +\n  scale_size(range = c(2, 12)) + \n  labs(title = \"Tie: Hyperlink\") + \n  theme(aspect.ratio = 1,,\n        legend.position = \"bottom\")\n\n\n\n\nOne challenge in visualizing multiplex networks is that multiple edges between the same two nodes may get plotted on top of each other in a way that makes them impossible to distinguish. For example, let us generate a simple multiplex network with two nodes and three ties between them:\n\nmultigtr &lt;- graph(edges = c(1, 2, 1, 2, 1, 2), n = 2)\n\nl &lt;- layout_with_kk(multigtr)\n\n# Let's just plot the graph:\nplot(\n  multigtr,\n  vertex.color = \"lightsteelblue\",\n  vertex.frame.color = \"white\",\n  vertex.size = 40,\n  vertex.shape = \"circle\",\n  vertex.label = NA,\n  edge.color = c(\"gold\", \"tomato\", \"yellowgreen\"),\n  edge.width = 10,\n  edge.arrow.size = 5,\n  edge.curved = 0.1,\n  layout = l\n)\n\n\n\n# Using tidygraph\nmultigtr %&gt;% \n  as_tbl_graph() %&gt;% \n  activate(edges) %&gt;% \n  mutate(edge_col = c(\"gold\", \"tomato\", \"yellowgreen\")) %&gt;% \nggraph(., layout = l) +\n  geom_edge_arc(strength = 0.1, aes(colour = edge_col)) + \n  geom_node_point(size = 4, colour = \"lightsteelblue\") +\n  theme(legend.position = \"none\")\n\n\n\n\nBecause all edges in the graph have the same curvature, they are drawn over each other so that we only see the last one. What we can do is assign each edge a different curvature. One useful function in ‘igraph’ called curve_multiple() can help us here. For a graph G, curve.multiple(G) will generate a curvature for each edge that maximizes visibility.\n\nplot(\n  multigtr,\n  vertex.color = \"lightsteelblue\",\n  vertex.frame.color = \"white\",\n  vertex.size = 40,\n  vertex.shape = \"circle\",\n  vertex.label = NA,\n  edge.color = c(\"gold\", \"tomato\", \"yellowgreen\"),\n  edge.width = 10,\n  edge.arrow.size = 5,\n  edge.curved = curve_multiple(multigtr),\n  layout = l\n)\n\n\n\n\n\nmultigtr %&gt;% \n  as_tbl_graph() %&gt;% \n  activate(edges) %&gt;% \n  mutate(edge_col = c(\"gold\", \"tomato\", \"yellowgreen\")) %&gt;% \nggraph(., layout = l) +\n  geom_edge_fan(strength = 0.1, aes(colour = edge_col),width = 2) + \n  geom_node_point(size = 4, colour = \"lightsteelblue\") +\n  theme(legend.position = \"none\")\n\n\n\n\nAnd that is the end of this reoworked tutorial! Hope you enjoyed it and found it useful!!"
  },
  {
    "objectID": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html",
    "href": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html",
    "title": "The TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad",
    "section": "",
    "text": "Here we go with another of my TRIZ Chronicles. The first edition analysing the famous movie Lawrence of Arabia is here.\nThe recent series of ads by Spotify have been very well received, by the ad industry as well. There have been several of these ads, each with a solid little story and a great punch line. There is one popular one that I have selected here, featuring a Mother shopping for veggies, while her son watches (and “listens”) to her bargaining with the shopkeeper. Here I take just that highly effective ad and interpret it from a TRIZ viewpoint.\nThis piece stems from my teaching a course on Creative Thinking and Problem Solving based on TRIZ, titled Play and Invent, over the past 8 years or more at the Srishti Manipal Institute of Art, Design, and Technology, Bangalore, India. (https://srishtimanipalinstitute.in)."
  },
  {
    "objectID": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#introduction",
    "href": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#introduction",
    "title": "The TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad",
    "section": "",
    "text": "Here we go with another of my TRIZ Chronicles. The first edition analysing the famous movie Lawrence of Arabia is here.\nThe recent series of ads by Spotify have been very well received, by the ad industry as well. There have been several of these ads, each with a solid little story and a great punch line. There is one popular one that I have selected here, featuring a Mother shopping for veggies, while her son watches (and “listens”) to her bargaining with the shopkeeper. Here I take just that highly effective ad and interpret it from a TRIZ viewpoint.\nThis piece stems from my teaching a course on Creative Thinking and Problem Solving based on TRIZ, titled Play and Invent, over the past 8 years or more at the Srishti Manipal Institute of Art, Design, and Technology, Bangalore, India. (https://srishtimanipalinstitute.in)."
  },
  {
    "objectID": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#the-famous-spotify-ad",
    "href": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#the-famous-spotify-ad",
    "title": "The TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad",
    "section": "The Famous Spotify Ad",
    "text": "The Famous Spotify Ad\nLet us watch the Spotify ad first, before analyzing it!"
  },
  {
    "objectID": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#the-young-mans-problem",
    "href": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#the-young-mans-problem",
    "title": "The TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad",
    "section": "The Young Man’s Problem",
    "text": "The Young Man’s Problem\nIn order to make a story out of this, I want make a Protagonist out the young man in the ad. It is he who has the problem and he who is going to apply TRIZ to solve it. I discuss the source of his Problem and give an analysis of the Problem from a (classical) TRIZ perspective, including the formulation of the Contradiction, Identification of Causes, the statement of the Ideal Final Result, and finally using the TRIZ Contradiction Matrix to find Inventive Principles, that lead to the solution, which of course, is meant to unerringly include Spotify !\nFirst a philosophical digression:—\nSeveral authors have taken a Game View of life. James P Carse’s famous book titled Finite and Infinite Games speaks of Play, Types of Games, Rules, Winning and our own aims in the Game itself. A similar articulation is, in my opinion, that of Mihaly Csikszentlmihalyi in his concept of Flow, shown here below:\n\n\nFrom Sketchplanations\n\nWhen the Game presents very little Challenge, we are bored. When the Game demands extreme skills the challenge is too much for us and we experience anxiety. When the Challenge presented is just barely matched by our Skill, we are in the zone of Flow, or what I call Play.\nA good metaphoric image for this experience is as follows:— that we live in a space where the Floor of Boredom is always rising and would crush us against our Ceiling of Anxiety. One Way to deal with this is to develop more Skills and push the Ceiling away, effectively moving into the zone of Flow. Another Way of looking at this is what Carse suggests: When Play is no longer possible, change the Game.\nSo what does all this have to do with getting veggies?\nThe ad is, in my opinion, all about Boredom, and how to avoid it. And not offend anybody. The Young Man (hereinafter, “YM”) simply has to accompany his Mom, and be there while she gets the veggies. I will exaggerate his irritation and his boredom at the risk of offending young people likely to read this, and say that he would rather not be there but he does not want to hurt Mom.\nWe are now ready for the TRIZ based Analysis of this Problem!"
  },
  {
    "objectID": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#a-triz-analysis-of-a-visit-to-the-subzi-mandi",
    "href": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#a-triz-analysis-of-a-visit-to-the-subzi-mandi",
    "title": "The TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad",
    "section": "A TRIZ Analysis of a Visit to the Subzi Mandi",
    "text": "A TRIZ Analysis of a Visit to the Subzi Mandi\nFor a TRIZ workflow, we proceed as before:\nFirst, using the method described in Open Source TRIZ, we identify knobs or parameters within the situation and see how turning these could lead to identifying a Cause for a Problem in the form of a Contradiction.\nHere below is a quick Ishikawa Diagram to help us identify the Parameters of this Problem:\n\n\n\n\n\n\n\n\nTurning the knobs/parameters in the Ishikawa Diagram, it seems that if the YM goes to the market with Mom, he would most likely get bored, but would please Mom. If he doesn’t go, then he chills at home, but Mom is going to justifiably furious. Herein lies the Contradiction, which we can now specify as an Administrative Contradiction(AC) in plain English:\n\n\nAC: The YM wants to chill at home but Mom wants him to take her veggie shopping. He has to put up with the Waste of Time, and being bored, and Stress at being away from friends.\n\n\nNext, based on this Contradiction and the inspection of the Ishikawa Diagram above, we are now ready to define a TRIZ Ideal Final Result:\n\n\nIFR: The YM must go to the Market and not be bored.\n\n\nNote again the impossible sounding way of expressing the IFR! One needs practice, like the Queen in Alice in Wonderland, who could think of Six Impossible Things before Breakfast ! Also note there could be other ways of specifying the IFR. See below, section Alternative Ideas for IFR.\nLet us take the AC and convert it into a Technical Contradiction(TC), keeping this IFR in mind. We will look at the 48 TRIZ Parameters in the TRIZ Contradiction Matrix (PDF) and see which Parameter we want to improve, while not worsening another. Here is what we can obtain. We will analyze each Contradiction both ways1:\n\n- TC 1: Improve Loss of Time (26) and not worsen Effect of External Harmful Factors (30)\n- TC 2: Improve Increase Productivity (44) and not worsen Stress (19)\n\nHere we choose these Parameters based on our IFR that while going to the Market may be unavoidable, Boredom need not ensue. Parameters chosen from the TRIZ Matrix can be thought of as metaphors for the knobs that lie within our AC. Going from the AC to the TC is an act of making metaphors. We could easily have chosen the Parameter Noise(29) as the “metaphoric thing” to avoid, but the current IFR doesn’t quite support that. There is here a considerable flexibility and possibility for imaginative interpretations of the AC, but using the language of TRIZ.\nWe could also formulate a Physical Contradiction(PC)2:\n\n\nPC: The YM must be in the market and not be in the market at the same time.\n\n\nwhich is aimed squarely at one of the Assumptions in the Problem, that the YM simply has to go. Again, if the IFR is formulated differently we could obtain a very different set of AC and PC. See below, section Alternative Ideas for IFR.\nIn a future post, we will deal with using the PC and the TRIZ Separation Principles to solve Problems."
  },
  {
    "objectID": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#solving-the-technical-contradiction",
    "href": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#solving-the-technical-contradiction",
    "title": "The TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad",
    "section": "Solving the Technical Contradiction",
    "text": "Solving the Technical Contradiction\nLet us take the both the TC-s into the Contradiction Matrix and arrive at the list of TRIZ Inventive Principles. Here is the Matrix solution for TC-1 in the figure below:\n\n\n\n\n\n\n\n\n\nThe two squares for the TC1 have been circled in red, solving TC-1.\nThe Inventive Principles are:(TC1, TC2, both ways)\n\n1(Segmentation)\n35(Parameter Change)\n21(Skipping)\n18(Mechanical Vibration) (!!)\n2(Taking Out/Separation)\n10(Prior Action)\n\n36(Phase Transitions)\nand with TC2:\n\n3(Local Quality)\n14(Spheroidality/Curvature)\n9(Preliminary Anti-Action)\n37(Thermal Expansion)\n40(Composite Materials)\n25(Self Service)\n24(Intermediary)\n\nThat is a considerable list for us to try to use!! Let us apply some these Inventive Principles! Viewing these Inventive Principles as we Generalized Solutions we try to map these back into the Problem at hand:\n\n\n35(Parameter Change): Which Parameter to change? Location? No. Sound? Change the “Bargaining Talk” into what? Sweet Musical Lyrics!!🎵🤣\n\n18(Mechanical Vibration) : What, make noise of your own? Yes! Play Music !!🔉 🤣\n\n14(Spheroidality): Wear “spherical” headphones!!🎧! Create a “sound sphere”! This is a long shot!!\n\n3(Local Quality): also indicates the creation of a “local” cocoon around the YM, but needs to be combined with 18(Mechanical Vibration) to truly arrive at the musical solution!\n\nOne could make decent interpretations of 2(Taking Out/Separation), and 24(Intermediary), but we are already there! The rest are perhaps (at least to me!) not very evocative, unless 37(Thermal Expansion) means “throw a temper tantrum at Mom”? Never! So there you have it! The Cinderella song played on Spotify becomes not just a noise canceller but actually seems to substitute the very conversation between Mom and the vendor. And the YM has successfully attained Flow ! And the IFR too, since with the music in his head, he is effectively “in the marketplace and not in the marketplace at the same time!\nAnd I attained Flow in writing this!!"
  },
  {
    "objectID": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#alternative-ideas-for-ifr",
    "href": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#alternative-ideas-for-ifr",
    "title": "The TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad",
    "section": "Alternative Ideas for IFR",
    "text": "Alternative Ideas for IFR\nWe note in passing that there is more than one way of formulating the Ideal Final Result. Here are two more examples:\n\n\nIFR2: The veggies should arrive without (the YM) going to the Market\nIFR3: Food should be prepared without having to go buy veggies.\n\n\nClearly these are at least as good as the one we have chosen, sounding nicely “impossible” in their own right! The point is that in the analysis of the Problem, we do need to ask Who has the Problem, as we did, and the IFR needs to stem from there. These alternative IFRs could well be the Voice of (another) Customer.\nIf there is any interesting situation that could be analyzed with TRIZ, please send me a DM! Thanks !"
  },
  {
    "objectID": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#references",
    "href": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#references",
    "title": "The TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad",
    "section": "References",
    "text": "References\n\nJames P Carse, Finite and Infinite Games, Free Press, 1986. ISBN: 0-02-905980-1\nMihaly Csikszentmihalyi, Creativity, Flow, and the Psychology of Discovery and Invention. Harper Perennial; Reprint edition (August 6, 2013)\nJack Hipple, The Ideal Result and How to Achieve It. Springer; 2012th edition (June 26, 2012)\nValery Souchkov, Defining Contradictions. http://www.xtriz.com/Training/TRIZ_DefineContradiction_Tutorial.pdf"
  },
  {
    "objectID": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#footnotes",
    "href": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#footnotes",
    "title": "The TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad",
    "section": "Footnotes",
    "text": "Footnotes\n\nThe Contradiction Matrix is not quite symmetric, so stating the Contradiction both ways allows us to access a slightly larger set of Inventive Principles from two cells of the Matrix.↩︎\nArriving at Physical Contradictions is not always easy! If we can, then there are a very crisp set of TRIZ Separation Principles that we can apply to solve the Problem.↩︎"
  },
  {
    "objectID": "content/projects/2023-11-03-Owind/index.html",
    "href": "content/projects/2023-11-03-Owind/index.html",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the O-Wind Turbine",
    "section": "",
    "text": "Here we go with another of my TRIZ Chronicles ! The earlier editions are here: Lawrence of Arabia, Spotify, and the Great Bubble Barrier.\nThis is another piece stems from my teaching a course on Creative Thinking and Problem Solving based on TRIZ, titled Play and Invent, over the past 8 years or more at the Srishti Manipal Institute of Art, Design, and Technology and now at DSU School of Commerce & Management Studies, both in Bangalore, INDIA."
  },
  {
    "objectID": "content/projects/2023-11-03-Owind/index.html#a-triz-analysis-of-the-dyson-o-wind-generator",
    "href": "content/projects/2023-11-03-Owind/index.html#a-triz-analysis-of-the-dyson-o-wind-generator",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the O-Wind Turbine",
    "section": "A TRIZ Analysis of the Dyson O-Wind Generator",
    "text": "A TRIZ Analysis of the Dyson O-Wind Generator\nFor a TRIZ workflow, we proceed as before:\n\nFirst, using the method described in Open Source TRIZ, we identify knobs or parameters within the situation\nWe see how turning these could lead to identifying a Statement / Cause for a Problem in the form of a Contradiction.\nRe-word the plain English Contradiction into TRIZ Parameters and look it up in the Contradiction Matrix. Obtain the Inventive Principles.\nApply these Inventive Principles into your Problem and solve it.\n\nIn the video itself, we heard about how electrical power consumption centers are the urban areas and these are far away from the generation sites. This leads to capital costs in HT Transmission equipment; we go to HT transmission to reduce losses on the way. This is already a Contradiction, which we might solve using Segmentation to arrive at Local Generation of Power. Local generation is a good idea to reduce these costs. This leads easily to Solar Panels on rooftops for example. Again while this may be cheaper than the electrical distribution system, it still uses a fair bit of capex and space and is centralized per building. Can we take Segmentation even further and think of a hyper-local household-based power generation unit, using the Wind?\nWhat would be the problems with using Wind based power generation around the home? Here below is a quick Ishikawa Diagram to help us identify the Parameters of this Problem:\n\n\n\n\n\nLooking at this Diagram, with the aspects identified, we could pair them off and see how they affect one another. In doing so, we could make up several problem. Let us state at some of our Problems: I have marked some of these with question marks since I am using imagination here and not direct primary research or information to formulate these. Note that some these may sound naive, but that is exactly way to start!\n\nI would like to have access my generator, but it needs to be not too close to the walls for it to harness the wind.\nHow to tap the power from the generator? What if the connection wires get twisted?\nDo I need a conventional Commutator? Won’t that be heavy?\nWhat voltage and current will I get? Will it be compatible with my 230V AC mains?\n\nAs you can see, many different problems and contradictions await our attention. Let us cut to the chase and state perhaps the most interesting problem (to me!) that the inventors have solved as demonstrated in the video above. We will state this as an Administrative Contradiction(AC) in plain English:\n\n\n\n\n\n\nAdministrative Contradiction\n\n\n\nAC: Winds help to generate power by making something rotate, but winds can change direction and slow down the existing rotation.\n\n\nWhat would an IFR be in this situation? How “unreasonable” can we be? Let us try:\n\n\n\n\n\n\nIdeal Final Result\n\n\n\nTorque must be in one direction only (irrespective of wind direction)\n\n\n\n\n\n\n\n\nUnidirectional Assumption\n\n\n\nI have made a strong assumption here about the the unidirectional movement: the main intent is for the rotating generator to be able to harness winds from any direction to establish or continue rotation in one direction (CW or CCW). Alternating current power generation is in principle immune to direction of rotation.\n\n\nLet us take our AC and convert it into a Technical Contradiction(TC), keeping this IFR in mind. We will look at the 48 TRIZ Parameters in the TRIZ Contradiction Matrix(PDF) and see which Parameter we want to improve, while not worsening another. Here is what we can obtain. We will analyze the Contradiction both ways1:\n\n\n\n\n\n\nTechnical Contradictions\n\n\n\n\n\nTC 1: Improve (15)Force/Torque while not worsening (3)Angle/Length of Moving Object\n\n\nTC 2: Improve (3)Angle/Length of Moving Object while not worsening (15)Force/Torque\n\n\n\n\nAgain we have chosen the TRIZ Parameters based on our IFR. Other metaphoric TRIZ Parameters that may suggest themselves are 12(Duration of Action on a Moving Object), 14(Speed), and (40)Harmful Effects Acting on the System.\nIs there a Physical Contradiction(PC)2 possible here?\n\n\n\n\n\n\nPhysical Contradiction\n\n\n\nThe Rotor must yield and not yield to the Wind at the same time. In other words, the rotor must be “porous and non-porous”3 to the wind at the same time.\n\n\nLet us now apply the TCs to the Contradiction Matrix and obtain the TRIZ Inventive Principles."
  },
  {
    "objectID": "content/projects/2023-11-03-Owind/index.html#solving-the-technical-contradiction",
    "href": "content/projects/2023-11-03-Owind/index.html#solving-the-technical-contradiction",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the O-Wind Turbine",
    "section": "Solving the Technical Contradiction",
    "text": "Solving the Technical Contradiction\nLet us take the both the TC-s into the Contradiction Matrix and arrive at the list of TRIZ Inventive Principles. Here is what the Matrix suggests:\nFor TC-1:\n\n17(Another Dimension) !!\n4( Asymmetry)\n14(Curvature) !!!\n\n10(Preliminary Action)\nand with TC-2:\n\n3(Local Quality)\n9(Preliminary Anti-Action)\n35(Parameter Change)\n\nHmm…based on the PC, we may have expected a Separation in Space solution, suggested by Curvature, Another Dimension and Asymmetry. Viewing these Inventive Principles as we Generalized Solutions, we try to map these back into the Problem at hand. In keeping with the metaphoric/analogic way of thinking that TRIZ embodies, I deliberately use many visual hints here from math, physics, geography, and biology.\n\n(14)Curvature: Hmm…nothing new here, or is there? Of course the rotor has to be curved and kind of sphere-like….\n17(Another Dimension): A near-spherical thing has really only one dimension..the radius. And that points in all directions / dimensions! Should there be changes in radius then? Should the radius change create bumps ( positive change ) or depressions ( negative change?) Should the bump be like a welt, and the depression like a groove? How can a bump or a depression itself be curved, as 14(Curvature) suggests?\n4(Asymmetry): The bumps or depressions…..they have to be asymmetric? So….not like longitudes and nor latitudes, but may be like those great circles.\n\n3(Local Quality): OK, the bumps or depressions are already “local”….can we go further? Here is where I stretch and go hyper-local: Should there be structures on or inside them, like flaps or fins or vanes? How can these be asymmetric, then? By acting like miniature flaps or trapdoors, that yield / fall flat when pushed in one direction and stand up / resist when pushed in the other direction…somewhat like a dog or cat’s fur? Then push and pull work differently…\n\n\n\n\n\n\nFrom Flaps to ….Funnels!\n\n\n\nMaking these flaps movable as the above paragraph seems to suggest would probably not be a good idea, from an engineering standpoint. But once we have the image of wind + flaps / fins / vanes and differences in pressure or movement, the Bernoulli Principle and Venturi effect suggest themselves immediately!! So what could this vane-fin-fur-flap thingy be then? Oh good heavens, a funnel !!!\n\n\n{HappyApple, Public domain, via Wikimedia Commons}\nSo each of those bumps are segmented into funnel-like structures that cause differences in air pressure when the wind blow. These differences are unidirectional and create movement/rotation! And because the bumps are curved along the surface of the sphere, and they are not parallel to one another (asymmetry), at least some of the internal funnels will always be “in the wind” 4, and capable of creating rotation using Bernoulli/Venturi effect!\n\n9(Preliminary Anti-Action): What do we wish to guard against? Counter acting wind forces. Well, the funnel structures work only with wind blowing into the broad opening and so we are fine!\n\nSo finally we could just imagine a spherical object, mounted on a spindle, with spiral arc-like bumps at different places on the surfaces. Within the arc-like bumps are funnel-like structures that create differentials in pressure when subject to the wind, and that creates rotation. Since the funnels are asymmetric by nature, our final rotation is unidirectional. Whew! ( Yes, that “whew” is also very suggestive here 😃!)"
  },
  {
    "objectID": "content/projects/2023-11-03-Owind/index.html#using-triz-separation-principles",
    "href": "content/projects/2023-11-03-Owind/index.html#using-triz-separation-principles",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the O-Wind Turbine",
    "section": "Using TRIZ Separation Principles",
    "text": "Using TRIZ Separation Principles\nAs Hipple explains, there is frequently an underlying physical parameter, such as length, breadth, weight, or energy, or speed for example that lies at the root of our Technical Contradiction.\nOur IFR states that we want the rotor to yield one way and to not yield when pushed the other way so it needs to be both hard and soft at the same time. This is a Physical Contradiction! In this case we can easily see and application of Separation in Space and also Separation on Condition. However I think in this case, it would not be easy to arrive at the Solution using just these.\nThat’s a wrap! In the next episode of the #TRIZ Chronicles, I wish to step even further out of my area of expertise and dabble in HR! I think looking at some of the institution-building ideas in Ricardo Semler’s book, Maverick would be a good idea!"
  },
  {
    "objectID": "content/projects/2023-11-03-Owind/index.html#references",
    "href": "content/projects/2023-11-03-Owind/index.html#references",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the O-Wind Turbine",
    "section": "References",
    "text": "References\n\nJack Hipple, The Ideal Result and How to Achieve It. Springer; 2012th edition (June 26, 2012)\nValery Souchkov, Defining Contradictions. http://www.xtriz.com/Training/TRIZ_DefineContradiction_Tutorial.pdf\n\nOpen Source TRIZ: Making Contradictions. https://www.youtube.com/watch?v=cah0OhCH55k"
  },
  {
    "objectID": "content/projects/2023-11-03-Owind/index.html#footnotes",
    "href": "content/projects/2023-11-03-Owind/index.html#footnotes",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the O-Wind Turbine",
    "section": "Footnotes",
    "text": "Footnotes\n\nThe Contradiction Matrix is not quite symmetric, so stating the Contradiction both ways allows us to access a slightly larger set of Inventive Principles from two cells of the Matrix.↩︎\nArriving at Physical Contradictions is not always easy! If we can, then there are a very crisp set of TRIZ Separation Principles that we can apply to solve the Problem.↩︎\nSo the Rotor must have…holes? How do holes “work in one direction only”? We will see…↩︎\nMathematically, the Wind direction vector will be (nearly) normal to the aperture of some funnel.↩︎"
  },
  {
    "objectID": "content/projects/listing.html",
    "href": "content/projects/listing.html",
    "title": "The Foundation Series",
    "section": "",
    "text": "A Design of Experiments Class\n\n\nA Design of Experiments Class\n\n\n\nArvind Venkatadri\n\n\nJan 31, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nA TRIZ Analysis of Lawrence of Arabia\n\n\nThe Attack on Aqaba: A TRIZ Analysis\n\n\n\nArvind Venkatadri\n\n\nDec 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nA Tidygraph version of a Popular Network Science Tutorial\n\n\n\n\n\n\nArvind Venkatadri\n\n\nJun 16, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResources for Order and Chaos\n\n\n\n\n\n\nArvind Venkatadri\n\n\nDec 12, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nThe TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad\n\n\nPunjabi Pop and Getting the Veggies\n\n\n\nArvind Venkatadri\n\n\nJan 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nThe TRIZ Chronicles: TRIZ Analysis of the Banff Wildlife Crossings\n\n\nIs there a solution to wildlife roadkill?\n\n\n\nArvind Venkatadri\n\n\nJul 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nThe TRIZ Chronicles: TRIZ Analysis of the O-Wind Turbine\n\n\nIs there a cheap and effective way to generate power using the Wind, right in your home?\n\n\n\nArvind Venkatadri\n\n\nMar 1, 2023\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#image-right",
    "href": "content/slides/projects-slides/portfolio/index.html#image-right",
    "title": "Introduction to Networks in R",
    "section": ".image-right",
    "text": ".image-right\n\n\nWe can use the .image-right and .image-left classes to insert images in the background\nThese images will be placed behind most other content"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#image-left",
    "href": "content/slides/projects-slides/portfolio/index.html#image-left",
    "title": "Introduction to Networks in R",
    "section": ".image-left",
    "text": ".image-left\n\n\n\n\nIt is therefore recommened that you use multiple columns to only have text on the background area"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#section",
    "href": "content/slides/projects-slides/portfolio/index.html#section",
    "title": "Introduction to Networks in R",
    "section": "",
    "text": "background images"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#setting-background-colors",
    "href": "content/slides/projects-slides/portfolio/index.html#setting-background-colors",
    "title": "Introduction to Networks in R",
    "section": "Setting background colors",
    "text": "Setting background colors\nyou can set your background as you normally would"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#video-slide-title",
    "href": "content/slides/projects-slides/portfolio/index.html#video-slide-title",
    "title": "Introduction to Networks in R",
    "section": "Video Slide Title",
    "text": "Video Slide Title\nThis slides’s background video will play in a loop with audio muted."
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#slide-title",
    "href": "content/slides/projects-slides/portfolio/index.html#slide-title",
    "title": "Introduction to Networks in R",
    "section": "Slide Title",
    "text": "Slide Title"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#further-modifying-theme",
    "href": "content/slides/projects-slides/portfolio/index.html#further-modifying-theme",
    "title": "Introduction to Networks in R",
    "section": "Further Modifying theme",
    "text": "Further Modifying theme\nIf you want to modify theme, you can specify the .scss my modifying the yaml to look like this\nformat: \n  letterbox-revealjs:\n    theme: [default, style.scss]"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#modifying-letterbox-background",
    "href": "content/slides/projects-slides/portfolio/index.html#modifying-letterbox-background",
    "title": "Introduction to Networks in R",
    "section": "Modifying letterbox background",
    "text": "Modifying letterbox background\nThe background colors can be with with, where #444444 represents the new background color and #222222 represents the color of the shadow\n.quarto-light {\n  background-color: #444444;\n}\n\n.quarto-dark {\n  background-color: #444444;\n}\n\n.slides {\n  box-shadow: #222222 0px 0px 30px 0px;\n}"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#quarto",
    "href": "content/slides/projects-slides/portfolio/index.html#quarto",
    "title": "Introduction to Networks in R",
    "section": "Quarto",
    "text": "Quarto\nQuarto enables you to weave together content and executable code into a finished presentation. To learn more about Quarto presentations see https://quarto.org/docs/presentations/."
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#bullets",
    "href": "content/slides/projects-slides/portfolio/index.html#bullets",
    "title": "Introduction to Networks in R",
    "section": "Bullets",
    "text": "Bullets\nWhen you click the Render button a document will be generated that includes:\n\nContent authored with markdown\nOutput from executable code"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#code",
    "href": "content/slides/projects-slides/portfolio/index.html#code",
    "title": "Introduction to Networks in R",
    "section": "Code",
    "text": "Code\nWhen you click the Render button a presentation will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\n[1] 2"
  },
  {
    "objectID": "content/slides/r-slides/graphics/index.html#image-right",
    "href": "content/slides/r-slides/graphics/index.html#image-right",
    "title": "The Grammar of Graphics in R",
    "section": ".image-right",
    "text": ".image-right\n\n\nWe can use the .image-right and .image-left classes to insert images in the background\nThese images will be placed behind most other content"
  },
  {
    "objectID": "content/slides/r-slides/graphics/index.html#image-left",
    "href": "content/slides/r-slides/graphics/index.html#image-left",
    "title": "The Grammar of Graphics in R",
    "section": ".image-left",
    "text": ".image-left\n\n\n\n\nIt is therefore recommened that you use multiple columns to only have text on the background area"
  },
  {
    "objectID": "content/slides/r-slides/graphics/index.html#section",
    "href": "content/slides/r-slides/graphics/index.html#section",
    "title": "The Grammar of Graphics in R",
    "section": "",
    "text": "background images"
  },
  {
    "objectID": "content/slides/r-slides/graphics/index.html#setting-background-colors",
    "href": "content/slides/r-slides/graphics/index.html#setting-background-colors",
    "title": "The Grammar of Graphics in R",
    "section": "Setting background colors",
    "text": "Setting background colors\nyou can set your background as you normally would"
  },
  {
    "objectID": "content/slides/r-slides/graphics/index.html#video-slide-title",
    "href": "content/slides/r-slides/graphics/index.html#video-slide-title",
    "title": "The Grammar of Graphics in R",
    "section": "Video Slide Title",
    "text": "Video Slide Title\nThis slides’s background video will play in a loop with audio muted."
  },
  {
    "objectID": "content/slides/r-slides/graphics/index.html#slide-title",
    "href": "content/slides/r-slides/graphics/index.html#slide-title",
    "title": "The Grammar of Graphics in R",
    "section": "Slide Title",
    "text": "Slide Title"
  },
  {
    "objectID": "content/slides/r-slides/graphics/index.html#further-modifying-theme",
    "href": "content/slides/r-slides/graphics/index.html#further-modifying-theme",
    "title": "The Grammar of Graphics in R",
    "section": "Further Modifying theme",
    "text": "Further Modifying theme\nIf you want to modify theme, you can specify the .scss my modifying the yaml to look like this\nformat: \n  letterbox-revealjs:\n    theme: [default, style.scss]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/index.html#modifying-letterbox-background",
    "href": "content/slides/r-slides/graphics/index.html#modifying-letterbox-background",
    "title": "The Grammar of Graphics in R",
    "section": "Modifying letterbox background",
    "text": "Modifying letterbox background\nThe background colors can be with with, where #444444 represents the new background color and #222222 represents the color of the shadow\n.quarto-light {\n  background-color: #444444;\n}\n\n.quarto-dark {\n  background-color: #444444;\n}\n\n.slides {\n  box-shadow: #222222 0px 0px 30px 0px;\n}"
  },
  {
    "objectID": "content/slides/r-slides/graphics/index.html#quarto",
    "href": "content/slides/r-slides/graphics/index.html#quarto",
    "title": "The Grammar of Graphics in R",
    "section": "Quarto",
    "text": "Quarto\nQuarto enables you to weave together content and executable code into a finished presentation. To learn more about Quarto presentations see https://quarto.org/docs/presentations/."
  },
  {
    "objectID": "content/slides/r-slides/graphics/index.html#bullets",
    "href": "content/slides/r-slides/graphics/index.html#bullets",
    "title": "The Grammar of Graphics in R",
    "section": "Bullets",
    "text": "Bullets\nWhen you click the Render button a document will be generated that includes:\n\nContent authored with markdown\nOutput from executable code"
  },
  {
    "objectID": "content/slides/r-slides/graphics/index.html#code",
    "href": "content/slides/r-slides/graphics/index.html#code",
    "title": "The Grammar of Graphics in R",
    "section": "Code",
    "text": "Code\nWhen you click the Render button a presentation will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\n[1] 2"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#how-does-one-read-shakespeare",
    "href": "content/slides/r-slides/graphics/metaphors.html#how-does-one-read-shakespeare",
    "title": "Metaphors with Graphics",
    "section": "How does one read Shakespeare?",
    "text": "How does one read Shakespeare?\n\nshakespeareTo code or not to code, that is the question…"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#what-is-a-grammar-of-graphics",
    "href": "content/slides/r-slides/graphics/metaphors.html#what-is-a-grammar-of-graphics",
    "title": "Metaphors with Graphics",
    "section": "What is a Grammar of Graphics?",
    "text": "What is a Grammar of Graphics?\n\nCode looks and reads like English.\n\nHas verbs, nouns, some adjectives….\n\n–\n\nDescribes Information/ideas/concepts from any source domain.\n\n–\n\nGEOMETRY as the target domain : What comes out of R is predominantly “geometry”"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#how-do-we-express-visuals-in-words",
    "href": "content/slides/r-slides/graphics/metaphors.html#how-do-we-express-visuals-in-words",
    "title": "Metaphors with Graphics",
    "section": "How do we express visuals in words?",
    "text": "How do we express visuals in words?\n.font120[ - Data to be visualized]\n\n\n\n\n\n\n.font120[ - .hlb[Geom]etric objects that appear on the plot]\n\n\n\n\n.font120[ - .hlb[Aes]thetic mappings from data to visual component]\n\n\n\n.font120[ - .hlb[Stat]istics transform data on the way to visualization]\n\n\n\n\n\n\n.font120[ - .hlb[Coord]inates organize location of geometric objects]\n\n\n\n\n.font120[ - .hlb[Scale]s define the range of values for aesthetics]\n\n\n\n.font120[ - .hlb[Facet]s group into subplots]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#the-essence-of-ggplot",
    "href": "content/slides/r-slides/graphics/metaphors.html#the-essence-of-ggplot",
    "title": "Metaphors with Graphics",
    "section": "The Essence of ggplot",
    "text": "The Essence of ggplot\nall ggplot2\n\naes(x = , y = ) (aesthetics)\naes(x = , y = , color = ) (add color)\naes(x = , y = , size = ) (add size)\n+ facet_wrap(~ ) (facetting)\n+ scale_ ( add a scale)"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#gg-is-for-grammar-of-graphics",
    "href": "content/slides/r-slides/graphics/metaphors.html#gg-is-for-grammar-of-graphics",
    "title": "Metaphors with Graphics",
    "section": "gg is for Grammar of Graphics",
    "text": "gg is for Grammar of Graphics\n.left-column[ ### Data ### Aesthetics ### Geoms\n+ geom_*()\n]\n.right-column[\n\n\n\n\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#the-five-named-graphs",
    "href": "content/slides/r-slides/graphics/metaphors.html#the-five-named-graphs",
    "title": "Metaphors with Graphics",
    "section": "The Five-Named Graphs",
    "text": "The Five-Named Graphs\n\nScatterplot: geom_point()\nLine graph: geom_line()\nHistogram: geom_histogram()\nBoxplot: geom_boxplot()\nBar graph: geom_bar() or geom_col (see Lab 02)"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#chunk-penguins",
    "href": "content/slides/r-slides/graphics/metaphors.html#chunk-penguins",
    "title": "Metaphors with Graphics",
    "section": "Chunk : penguins",
    "text": "Chunk : penguins\n\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_l…¹ body_…² sex    year\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;       &lt;int&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7         181    3750 male   2007\n2 Adelie  Torgersen           39.5          17.4         186    3800 fema…  2007\n3 Adelie  Torgersen           40.3          18           195    3250 fema…  2007\n4 Adelie  Torgersen           NA            NA            NA      NA &lt;NA&gt;   2007\n5 Adelie  Torgersen           36.7          19.3         193    3450 fema…  2007\n6 Adelie  Torgersen           39.3          20.6         190    3650 male   2007\n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n\n\nWe see the first few rows of the dataset penguins. We see that there are a few NA data observations too. Let us remove them for now."
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#chunk-mapping",
    "href": "content/slides/r-slides/graphics/metaphors.html#chunk-mapping",
    "title": "Metaphors with Graphics",
    "section": "Chunk: Mapping",
    "text": "Chunk: Mapping\n.pull-left[\n]\n.pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#chunk-mapping-1",
    "href": "content/slides/r-slides/graphics/metaphors.html#chunk-mapping-1",
    "title": "Metaphors with Graphics",
    "section": "Chunk: Mapping",
    "text": "Chunk: Mapping\n.pull-left[\n]\n.pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#chunk-mapping-2",
    "href": "content/slides/r-slides/graphics/metaphors.html#chunk-mapping-2",
    "title": "Metaphors with Graphics",
    "section": "Chunk: Mapping",
    "text": "Chunk: Mapping\n.pull-left[\n]\n.pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#chunk-mapping-3",
    "href": "content/slides/r-slides/graphics/metaphors.html#chunk-mapping-3",
    "title": "Metaphors with Graphics",
    "section": "Chunk: Mapping",
    "text": "Chunk: Mapping\n.pull-left[\n]\n.pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#chunk-geom_point_position_colour",
    "href": "content/slides/r-slides/graphics/metaphors.html#chunk-geom_point_position_colour",
    "title": "Metaphors with Graphics",
    "section": "Chunk: Geom_Point_Position_Colour",
    "text": "Chunk: Geom_Point_Position_Colour\n.pull-left[\n] .pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#chunk-geom_point_position_colour-1",
    "href": "content/slides/r-slides/graphics/metaphors.html#chunk-geom_point_position_colour-1",
    "title": "Metaphors with Graphics",
    "section": "Chunk: Geom_Point_Position_Colour",
    "text": "Chunk: Geom_Point_Position_Colour\n.pull-left[\nWe can leave out the “mapping” word and just use aes .\nWhy is there no plot?\n🤔 💭\nRight !! We have not used a geom command yet!! ] .pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#chunk-geom_point_position_colour-2",
    "href": "content/slides/r-slides/graphics/metaphors.html#chunk-geom_point_position_colour-2",
    "title": "Metaphors with Graphics",
    "section": "Chunk: Geom_Point_Position_Colour",
    "text": "Chunk: Geom_Point_Position_Colour\n.pull-left[\nNote that the points are located by position coordinates on both x and y axis, and coloured by the island variable.\n] .pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#chunk-geom_point_position_colour-3",
    "href": "content/slides/r-slides/graphics/metaphors.html#chunk-geom_point_position_colour-3",
    "title": "Metaphors with Graphics",
    "section": "Chunk: Geom_Point_Position_Colour",
    "text": "Chunk: Geom_Point_Position_Colour\n.pull-left[\nNote that the points are located by position coordinates on both x and y axis, and coloured by the island variable.\nAnd we’ve fixed size = 4!\n] .pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#alpha",
    "href": "content/slides/r-slides/graphics/metaphors.html#alpha",
    "title": "Metaphors with Graphics",
    "section": "Alpha",
    "text": "Alpha\n.pull-left[\nAre the points all overlapping? Can we see them better? ]\n.pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#alpha-1",
    "href": "content/slides/r-slides/graphics/metaphors.html#alpha-1",
    "title": "Metaphors with Graphics",
    "section": "Alpha",
    "text": "Alpha\n.pull-left[\nAre the points all overlapping? Can we see them better?\n]\n.pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#chunk-box-plot",
    "href": "content/slides/r-slides/graphics/metaphors.html#chunk-box-plot",
    "title": "Metaphors with Graphics",
    "section": "Chunk: Box Plot",
    "text": "Chunk: Box Plot\n.pull-left[\n]\n.pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#chunk-box-plot-1",
    "href": "content/slides/r-slides/graphics/metaphors.html#chunk-box-plot-1",
    "title": "Metaphors with Graphics",
    "section": "Chunk: Box Plot",
    "text": "Chunk: Box Plot\n.pull-left[\n]\n.pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#chunk-geom_bar_1",
    "href": "content/slides/r-slides/graphics/metaphors.html#chunk-geom_bar_1",
    "title": "Metaphors with Graphics",
    "section": "Chunk: Geom_Bar_1",
    "text": "Chunk: Geom_Bar_1\n.pull-left[ ::: {.cell hash=‘metaphors_cache/revealjs/3a_4be4390b2bb8a0bb273e1c4174c47445’}\n::: ] .pull-right[ ::: {.cell hash=‘metaphors_cache/revealjs/unnamed-chunk-11_c54ec299e4510a718f20ea0f35e457fa’} ::: {.cell-output-display}  ::: :::]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#chunk-geom_bar_1-1",
    "href": "content/slides/r-slides/graphics/metaphors.html#chunk-geom_bar_1-1",
    "title": "Metaphors with Graphics",
    "section": "Chunk: Geom_Bar_1",
    "text": "Chunk: Geom_Bar_1\n.pull-left[\n] .pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#chunk-geom_bar_1-2",
    "href": "content/slides/r-slides/graphics/metaphors.html#chunk-geom_bar_1-2",
    "title": "Metaphors with Graphics",
    "section": "Chunk: Geom_Bar_1",
    "text": "Chunk: Geom_Bar_1\n.pull-left[\nThe bars are plotted with positions on the x-axis, defined by the species variable, and heights mapped to the y-axis.\nHow did the graph “know” the heights of the bars?\ngeom_bar has an internal count statistic computation. Many geom_s have internal computation that are accessible to programmers.\n] .pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#geom_bar_position_stack_and_dodge",
    "href": "content/slides/r-slides/graphics/metaphors.html#geom_bar_position_stack_and_dodge",
    "title": "Metaphors with Graphics",
    "section": "Geom_Bar_Position_Stack_and_Dodge",
    "text": "Geom_Bar_Position_Stack_and_Dodge\n.pull-left[ When using more than a pair of variables with a bar chart, we have a few more position options:\n] .pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#geom_bar_position_stack_and_dodge-1",
    "href": "content/slides/r-slides/graphics/metaphors.html#geom_bar_position_stack_and_dodge-1",
    "title": "Metaphors with Graphics",
    "section": "Geom_Bar_Position_Stack_and_Dodge",
    "text": "Geom_Bar_Position_Stack_and_Dodge\n.pull-left[ When using more than a pair of variables with a bar chart, we have a few more position options:\nThe bars are coloured by the island variable and are stacked in position. ] .pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#geom_bar_position_stack_and_dodge-2",
    "href": "content/slides/r-slides/graphics/metaphors.html#geom_bar_position_stack_and_dodge-2",
    "title": "Metaphors with Graphics",
    "section": "Geom_Bar_Position_Stack_and_Dodge",
    "text": "Geom_Bar_Position_Stack_and_Dodge\n.pull-left[ And here we use the dodge option: ::: {.cell hash=‘metaphors_cache/revealjs/5c_305f0626b040002d5728ac2d1abf747a’}\n::: ]\n.pull-right[ ::: {.cell hash=‘metaphors_cache/revealjs/unnamed-chunk-16_b1b9949a06c2c12e5ed535f2455b2d6c’} ::: {.cell-output-display}  ::: :::]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#facetting",
    "href": "content/slides/r-slides/graphics/metaphors.html#facetting",
    "title": "Metaphors with Graphics",
    "section": "Facetting",
    "text": "Facetting\n.pull-left[\n] .pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#facetting-1",
    "href": "content/slides/r-slides/graphics/metaphors.html#facetting-1",
    "title": "Metaphors with Graphics",
    "section": "Facetting",
    "text": "Facetting\n.pull-left[\n]\n.pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#facetting-2",
    "href": "content/slides/r-slides/graphics/metaphors.html#facetting-2",
    "title": "Metaphors with Graphics",
    "section": "Facetting",
    "text": "Facetting\n.pull-left[\n]\n.pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#facetting-3",
    "href": "content/slides/r-slides/graphics/metaphors.html#facetting-3",
    "title": "Metaphors with Graphics",
    "section": "Facetting",
    "text": "Facetting\n.pull-left[\nThe graph has split into small multiples, based on the number of islands. ]\n.pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#still-more-facetting",
    "href": "content/slides/r-slides/graphics/metaphors.html#still-more-facetting",
    "title": "Metaphors with Graphics",
    "section": "Still more Facetting",
    "text": "Still more Facetting\n.pull-left[ ::: {.cell hash=‘metaphors_cache/revealjs/6e_11fffde8991ec6023b002517c71fb782’}\n:::\nWhat if we have even more “factor” variables? We have island and species…can we split further?\n]\n.pull-right[ ::: {.cell hash=‘metaphors_cache/revealjs/unnamed-chunk-21_8baf2a1f3d326823f4808f0ec3a1c274’} ::: {.cell-output-display}  ::: :::]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#still-more-facetting-1",
    "href": "content/slides/r-slides/graphics/metaphors.html#still-more-facetting-1",
    "title": "Metaphors with Graphics",
    "section": "Still more Facetting",
    "text": "Still more Facetting\n.pull-left[\nThe graph has split into multiples, based on the number of islands and the number of species.\n] .pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#and-shall-we-look-briefly-at-colour",
    "href": "content/slides/r-slides/graphics/metaphors.html#and-shall-we-look-briefly-at-colour",
    "title": "Metaphors with Graphics",
    "section": "And shall we look briefly at colour?",
    "text": "And shall we look briefly at colour?"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#finallycolour-just-a-bit",
    "href": "content/slides/r-slides/graphics/metaphors.html#finallycolour-just-a-bit",
    "title": "Metaphors with Graphics",
    "section": "Finally…Colour !! ( Just a bit )",
    "text": "Finally…Colour !! ( Just a bit )\n.pull-left[\n] .pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#finallycolour-just-a-bit-1",
    "href": "content/slides/r-slides/graphics/metaphors.html#finallycolour-just-a-bit-1",
    "title": "Metaphors with Graphics",
    "section": "Finally…Colour !! ( Just a bit )",
    "text": "Finally…Colour !! ( Just a bit )\n.pull-left[ ::: {.cell hash=‘metaphors_cache/revealjs/8b_af117602443139046988dd623fbb1cb2’}\n::: We are using the RColorBrewer package here. Type RColorBrewer::display.brewer.all() in your Console and see what palettes are available.\n] .pull-right[ ::: {.cell hash=‘metaphors_cache/revealjs/unnamed-chunk-24_43588c1909bff1ebea8c039025075182’} ::: {.cell-output-display}  ::: :::]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#chunk-colour-just-a-bit",
    "href": "content/slides/r-slides/graphics/metaphors.html#chunk-colour-just-a-bit",
    "title": "Metaphors with Graphics",
    "section": "Chunk: Colour !! ( Just a bit )",
    "text": "Chunk: Colour !! ( Just a bit )\n.pull-left[\n] .pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#chunk-colour-just-a-bit-1",
    "href": "content/slides/r-slides/graphics/metaphors.html#chunk-colour-just-a-bit-1",
    "title": "Metaphors with Graphics",
    "section": "Chunk: Colour !! ( Just a bit )",
    "text": "Chunk: Colour !! ( Just a bit )\n.pull-left[\n] .pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#chunk-colour-just-a-bit-2",
    "href": "content/slides/r-slides/graphics/metaphors.html#chunk-colour-just-a-bit-2",
    "title": "Metaphors with Graphics",
    "section": "Chunk: Colour !! ( Just a bit )",
    "text": "Chunk: Colour !! ( Just a bit )\n.pull-left[\n] .pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#conclusion",
    "href": "content/slides/r-slides/graphics/metaphors.html#conclusion",
    "title": "Metaphors with Graphics",
    "section": "Conclusion",
    "text": "Conclusion\n\nggplot takes a dataframe/tibble as the data argument\nThe aes-thetic arguments can be x, y, colour, shape, alpha for example…\nThe geom_*() commands specify the kind of plot, from a geometric perspective\nTogether, the ggplot package offers a Grammar of near-English commands which allow us to plot data in various ways."
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#references",
    "href": "content/slides/r-slides/graphics/metaphors.html#references",
    "title": "Metaphors with Graphics",
    "section": "References",
    "text": "References\n\nWickham, Hadley. (2010) “A Layered Grammar of Graphics”. Journal of Computational and Graphical Statistics, 19(1).\nWilkinson, Leland. (2005). The Grammar of Graphics. (UChicago authentication required)"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#image-right",
    "href": "content/slides/r-slides/networks/index.html#image-right",
    "title": "Introduction to Networks in R",
    "section": ".image-right",
    "text": ".image-right\n\n\nWe can use the .image-right and .image-left classes to insert images in the background\nThese images will be placed behind most other content"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#image-left",
    "href": "content/slides/r-slides/networks/index.html#image-left",
    "title": "Introduction to Networks in R",
    "section": ".image-left",
    "text": ".image-left\n\n\n\n\nIt is therefore recommened that you use multiple columns to only have text on the background area"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#section",
    "href": "content/slides/r-slides/networks/index.html#section",
    "title": "Introduction to Networks in R",
    "section": "",
    "text": "background images"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#setting-background-colors",
    "href": "content/slides/r-slides/networks/index.html#setting-background-colors",
    "title": "Introduction to Networks in R",
    "section": "Setting background colors",
    "text": "Setting background colors\nyou can set your background as you normally would"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#video-slide-title",
    "href": "content/slides/r-slides/networks/index.html#video-slide-title",
    "title": "Introduction to Networks in R",
    "section": "Video Slide Title",
    "text": "Video Slide Title\nThis slides’s background video will play in a loop with audio muted."
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#slide-title",
    "href": "content/slides/r-slides/networks/index.html#slide-title",
    "title": "Introduction to Networks in R",
    "section": "Slide Title",
    "text": "Slide Title"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#further-modifying-theme",
    "href": "content/slides/r-slides/networks/index.html#further-modifying-theme",
    "title": "Introduction to Networks in R",
    "section": "Further Modifying theme",
    "text": "Further Modifying theme\nIf you want to modify theme, you can specify the .scss my modifying the yaml to look like this\nformat: \n  letterbox-revealjs:\n    theme: [default, style.scss]"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#modifying-letterbox-background",
    "href": "content/slides/r-slides/networks/index.html#modifying-letterbox-background",
    "title": "Introduction to Networks in R",
    "section": "Modifying letterbox background",
    "text": "Modifying letterbox background\nThe background colors can be with with, where #444444 represents the new background color and #222222 represents the color of the shadow\n.quarto-light {\n  background-color: #444444;\n}\n\n.quarto-dark {\n  background-color: #444444;\n}\n\n.slides {\n  box-shadow: #222222 0px 0px 30px 0px;\n}"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#quarto",
    "href": "content/slides/r-slides/networks/index.html#quarto",
    "title": "Introduction to Networks in R",
    "section": "Quarto",
    "text": "Quarto\nQuarto enables you to weave together content and executable code into a finished presentation. To learn more about Quarto presentations see https://quarto.org/docs/presentations/."
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#bullets",
    "href": "content/slides/r-slides/networks/index.html#bullets",
    "title": "Introduction to Networks in R",
    "section": "Bullets",
    "text": "Bullets\nWhen you click the Render button a document will be generated that includes:\n\nContent authored with markdown\nOutput from executable code"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#code",
    "href": "content/slides/r-slides/networks/index.html#code",
    "title": "Introduction to Networks in R",
    "section": "Code",
    "text": "Code\nWhen you click the Render button a presentation will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\n[1] 2"
  },
  {
    "objectID": "content/slides/r-slides/r-slides-listing.html",
    "href": "content/slides/r-slides/r-slides-listing.html",
    "title": "The Foundation Series",
    "section": "",
    "text": "Introduction to Networks in R\n\n\nUsing tidygraph and visNetwork\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nMIT License\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nMIT License\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nMetaphors with Graphics\n\n\nFrom Code to Geometry\n\n\n\n\n\n\n\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\nPlaying with Leaflet\n\n\n\n\n\n\n\n\n\n\n\n\nMay 13, 2017\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\nThe Grammar of Graphics in R\n\n\nUsing the tidyverse\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nThe Nature of Data\n\n\nHow does Human Experience link with Data?\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nWorking in R\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "content/slides/r-slides/working-in-R/index.html#introduction",
    "href": "content/slides/r-slides/working-in-R/index.html#introduction",
    "title": "Working in R",
    "section": "Introduction",
    "text": "Introduction"
  },
  {
    "objectID": "content/slides/r-slides/working-in-R/index.html#data-structures",
    "href": "content/slides/r-slides/working-in-R/index.html#data-structures",
    "title": "Working in R",
    "section": "Data Structures",
    "text": "Data Structures"
  },
  {
    "objectID": "content/work-related/fsp-discussions/index.html",
    "href": "content/work-related/fsp-discussions/index.html",
    "title": "FSP Discussions 2021",
    "section": "",
    "text": "Slides and Tutorials\n.nbsp;.nbsp;"
  },
  {
    "objectID": "content/work-related/fsp-discussions/index.html#introduction",
    "href": "content/work-related/fsp-discussions/index.html#introduction",
    "title": "FSP Discussions 2021",
    "section": "Introduction",
    "text": "Introduction"
  },
  {
    "objectID": "content/work-related/fsp-discussions/index.html#this-is-a-summary-of-the-group-discussion-among",
    "href": "content/work-related/fsp-discussions/index.html#this-is-a-summary-of-the-group-discussion-among",
    "title": "FSP Discussions 2021",
    "section": "This is a summary of the group discussion among:",
    "text": "This is a summary of the group discussion among:\n1. Sadhvi Jawa\n2. Minashshi Singh\n3. Vidhu Gandhi\n4. Yash Bhandari\n5. Arvind Venkatadri"
  },
  {
    "objectID": "content/work-related/fsp-portfolio/index.html",
    "href": "content/work-related/fsp-portfolio/index.html",
    "title": "Teaching in this Pandemic Year 2020-2021",
    "section": "",
    "text": "This is a short Portfolio of Teaching Initiatives and Student Outcomes during this pandemic year, 2020-2021, from Arvind Venkatadri.\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Metaphorics",
    "section": "",
    "text": "Hi, I’m Arvind Venkatadri.\nI’m an Adjunct Professor at DSU, Bangalore, INDIA, with a passion for R, Data Visualization, Complexity Science, and Creative Thinking and Problem Solving with TRIZ. On this website, I share my course materials and methods. I also blog about TRIZ and Data Science on occasion.\nTo get started, you can check out my courses. You can find me on Twitter, or GitHub, and on LinkedIn! Feel free to reach out to me via mail too!\n\n\n\n\n\n Back to top"
  }
]