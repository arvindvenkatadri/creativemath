[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Applied Metaphorics",
    "section": "",
    "text": "Hi, I‚Äôm Arvind Venkatadri.\nI‚Äôm an Adjunct Professor at DSU, Bangalore, INDIA, with a passion for R, data visualization, and Creative Thinking and Problem Solving with TRIZ. On this blog, I share and teach what I learn.\nTo get started, you can check out my courses. You can find me on Twitter or GitHub and YouTube. Feel free to reach out to me via mail !"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/listing.html",
    "href": "content/courses/Basics-of-Modeling/listing.html",
    "title": "Basics of Modelling",
    "section": "",
    "text": "Order By\nDefault\n\n          Date - Oldest\n        \n\n          Date - Newest\n        \n\n          Title\n        \n\n          Author\n        \n\n    \n      \n      \n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\nReading Time\n\n\n\n\n\n\n\n\nNov 9, 2021\n\n\nüß≠ Basics of Statistical Modeling\n\n\nArvind Venkatadri\n\n\n6 min\n\n\n\n\n\n¬†\n\n\n\nNov 25, 2022\n\n\nLinear Model: Correlation Test\n\n\nArvind Venkatadri\n\n\n13 min\n\n\n\n\n\n\n\nInvalid Date\n\n\n‚àù Simulation Tests\n\n\nArvind Venkatadri\n\n\n5 min\n\n\n\n\n\n\n\nNov 25, 2022\n\n\nLinear Models: Comparing Proportions\n\n\nArvind Venkatadri\n\n\n26 min\n\n\n\n\n\n¬†\n\n\n\nNov 25, 2022\n\n\nSamples, Populations, Statistics and Inference\n\n\nArvind Venkatadri\n\n\n13 min\n\n\n\n\n\n\n\nNov 26, 2022\n\n\nLinear Models: Comparing Means\n\n\nArvind Venkatadri\n\n\n13 min\n\n\n\n\n\n¬†\n\n\n\nNov 10, 2022\n\n\nüÉè Permutation Test for Two Means\n\n\nArvind Venkatadri\n\n\n10 min\n\n\n\n\n\n¬†\n\n\n\nNov 28, 2022\n\n\nBootstrap\n\n\nArvind Venkatadri\n\n\n0 min\n\n\n\n\n\n¬†\n\n\n\nNov 10, 2022\n\n\nüÉè Permutation Test for Two Proportions\n\n\nArvind Venkatadri\n\n\n5 min\n\n\n\n\n\n\n\nNov 27, 2022\n\n\nBasics of Simulation Tests\n\n\nArvind Venkatadri\n\n\n5 min\n\n\n\n\n\n¬†\n\n\n\nNov 28, 2022\n\n\nPermutation Tests\n\n\nArvind Venkatadri\n\n\n0 min\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/10-Intro/index.html#introduction",
    "href": "content/courses/Basics-of-Modeling/Modules/10-Intro/index.html#introduction",
    "title": "üß≠ Basics of Statistical Modeling",
    "section": "Introduction",
    "text": "Introduction\nIn this set of modules we will explore Data, understand what types of data variables there are, and the kinds of statistical tests and visualizations we can create with them."
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/10-Intro/index.html#what-is-a-statistical-model",
    "href": "content/courses/Basics-of-Modeling/Modules/10-Intro/index.html#what-is-a-statistical-model",
    "title": "üß≠ Basics of Statistical Modeling",
    "section": "What is a Statistical Model?",
    "text": "What is a Statistical Model?\n\n‚ÄúModeling‚Äù is a process of asking questions. ‚ÄúStatistical‚Äù refers in part to data ‚Äì the statistical models you will construct will be rooted in data. But it refers also to a distinctively modern idea: that you can measure what you don‚Äôt know and that doing so contributes to your understanding.\nThe conclusions you reach from data depend on the specific questions you ask.\nThe word ‚Äúmodeling‚Äù highlights that your goals, your beliefs, and your current state of knowledge all influence your analysis of data.\nSimilarly, in statistical modeling, you examine your data to see whether they are consistent with the hypotheses that frame your understanding of the system under study."
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/10-Intro/index.html#uses-and-types-of-statistical-models",
    "href": "content/courses/Basics-of-Modeling/Modules/10-Intro/index.html#uses-and-types-of-statistical-models",
    "title": "üß≠ Basics of Statistical Modeling",
    "section": "Uses and Types of Statistical Models",
    "text": "Uses and Types of Statistical Models\nThere are three main uses for statistical models. They are closely related, but distinct enough to be worth enumerating.\n\nDescription. Sometimes you want to describe the range or typical values of a quantity. For example, what‚Äôs a ‚Äúnormal‚Äù white blood cell count? Sometimes you want to describe the relationship between things. Example: What‚Äôs the relationship between the price of gasoline and consumption by automobiles?\nClassification or prediction. You often have information about some observable traits, qualities, or attributes of a system you observe and want to draw conclusions about other things that you can‚Äôt directly observe. For instance, you know a patient‚Äôs white blood-cell count and other laboratory measurements and want to diagnose the patient‚Äôs illness.\nAnticipating the consequences of interventions. Here, you intend to do something: you are not merely an observer but an active participant in the system. For example, people involved in setting or debating public policy have to deal with questions like these: To what extent will increasing the tax on gasoline reduce consumption? To what extent will paying teachers more increase student performance?\n\nThe appropriate form of a model depends on the purpose. For example, a model that diagnoses a patient as ill based on an observation of a high number of white blood cells can be sensible and useful. But that same model could give absurd predictions about intervention: Do you really think that lowering the white blood cell count by bleeding a patient will make the patient better?\nTo anticipate correctly the effects of an intervention you need to get the direction of cause and effect correct in your models. But for a model used for classification or prediction, it may be unnecessary to represent causation correctly. Instead, other issues, e.g., the reliability of data, can be the most important. One of the thorniest issues in statistical modeling ‚Äì with tremendous consequences for science, medicine, government, and commerce ‚Äì is how you can legitimately draw conclusions about interventions from models based on data collected without performing these interventions.\nThe Intent of Modelling\nFrom Daniel T. Kaplan‚Äôs book:\n\nStatistics is about variation. Describing and interpreting variation is a major goal of statistics.\nYou can create empirical, mathematical descriptions not only of a single trait or variable but also of the relationships between two or more traits. (Empirical means based on measurements, data, observations.)\n\nModels let you split variation into components: ‚Äúexplained‚Äù versus ‚Äúunexplained.‚Äù How to measure the size of these components and how to compare them to one another is a central aspect of statistical methodology. Indeed, this provides a definition of statistics:\nStatistics is the explanation of variation in the context of what remains unexplained.\n\nBy collecting data in ways that require care but are quite feasible, you can estimate how reliable your descriptions are, e.g., whether it‚Äôs plausible that you should see similar relationships if you collected new data. This notion of reliability is very narrow and there are some issues that depend critically on the context in which the data were collected and the correctness of assumptions that you make about how the world works.\nRelationships between pairs of traits can be studied in isolation only in special circumstances. In general, to get valid results it is necessary to study entire systems of traits simultaneously. Failure to do so can easily lead to conclusions that are grossly misleading.\nDescriptions of relationships are often subjective ‚Äì they depend on choices that you, the modeler, make. These choices are generally rooted in your own beliefs about how the world works, or the theories accepted as plausible within some community of inquiry.\nIf data are collected properly, you can get an indication of whether the data are consistent or inconsistent with your subjective beliefs or ‚Äì and this is important ‚Äì whether you don‚Äôt have enough data to tell either way.\nModels can be used to check out the sensitivity of your conclusions to different beliefs. People who disagree in their views of how the world works often may not be able to reconcile their differences based on data, but they will be able to decide objectively whether their own or the other party‚Äôs beliefs are reasonable given the data.\nNotwithstanding everything said above about the strong link between your prior, subjective beliefs and the conclusions you draw from data, by collecting data in a certain context ‚Äì experiments ‚Äì you can dramatically simplify the interpretation of the results. It‚Äôs actually possible to remove the dependence on identified subjective beliefs by intervening in the system under study experimentally.\nTypes of Models\nLet us look at the famous dataset pertaining to Francis Galton‚Äôs work on the heights of children and the heights of their parents. We can create 4 kinds of models based on the variables in that dataset.\nOur method in this set of modules is to take the modern view that all these models can be viewed from a standpoint of the Linear Model, also called Linear Regression $ y = _1 x + _0 $ . For example, it is relatively straightforward to imagine Plot B (Quant vs Quant ) as an example of a Linear Model. We will try to work up to the intuition that this model can be used to understand all the models in the Figure."
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/10-Intro/index.html#degrees-of-freedom",
    "href": "content/courses/Basics-of-Modeling/Modules/10-Intro/index.html#degrees-of-freedom",
    "title": "üß≠ Basics of Statistical Modeling",
    "section": "Degrees of Freedom",
    "text": "Degrees of Freedom"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/10-Intro/index.html#effect-size",
    "href": "content/courses/Basics-of-Modeling/Modules/10-Intro/index.html#effect-size",
    "title": "üß≠ Basics of Statistical Modeling",
    "section": "Effect Size",
    "text": "Effect Size\nAn effect size tells how the output of a model changes when a simple change is made to the input.\nEffect sizes always involve two variables: a response variable and a single explanatory variable. Effect size is always about a model. The model might have one explanatory variable or many explanatory variables. Each explanatory variable will have its own effect size, so a model with multiple explanatory variables will have multiple effect sizes."
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/20-LM-Correlations/index.html",
    "href": "content/courses/Basics-of-Modeling/Modules/20-LM-Correlations/index.html",
    "title": "Linear Model: Correlation Test",
    "section": "",
    "text": "R Tutorial¬†¬†\n  Orange Tutorial\n\n  Radiant Tutorial¬†\n  Datasets"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/20-LM-Correlations/index.html#the-linear-model",
    "href": "content/courses/Basics-of-Modeling/Modules/20-LM-Correlations/index.html#the-linear-model",
    "title": "Linear Model: Correlation Test",
    "section": "\n0.2 The Linear Model",
    "text": "0.2 The Linear Model\nThe premise here is that many common statistical tests are special cases of the linear model. A linear model estimates the relationship between dependent variable or ‚Äúresponse‚Äù variable (\\(y\\)) and an explanatory variable or ‚Äúpredictor‚Äù (\\(x\\)). It is assumed that the relationship is linear. \\(\\beta_0\\) is the intercept and \\(\\beta_1\\) is the slope of the linear fit, that predicts the value of y based the value of x.\n\\[\ny = \\beta_0 + \\beta_1 *x\n\\]"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/20-LM-Correlations/index.html#the-correlation-test",
    "href": "content/courses/Basics-of-Modeling/Modules/20-LM-Correlations/index.html#the-correlation-test",
    "title": "Linear Model: Correlation Test",
    "section": "\n0.3 The Correlation Test",
    "text": "0.3 The Correlation Test\nTBD: Some introductory text here on Correlation itself)\nOne of the basic Questions we would have of our data is: Does some variable depend upon another in some way? Does \\(y\\) depend upon \\(x\\)?\nA Correlation Test is designed to answer exactly this question.\nLet us now see how a Correlation Test can be re-formulated as a Linear Model + Hypothesis Test.\n\n0.3.1 Some Toy Data\nMost examples in this exposition are based on three ‚Äúimaginary‚Äù samples, \\(x, y1, y2\\). Each is normally distributed and made up of 50 observations. The means and the sds are, respectively:\n\n\n\n\n  \n\n\n\n\n\n\nLet us look at our toy data in three ways:\n\nAll three variables:\n\n\n\n\n\n  \n\n\n\n\nVariables stacked and labelled (Note: group is now a Qual variable !!)\n\n\n\n\n\n  \n\n\n\n\nSame as 2, but only for the dependent y variables:"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/20-LM-Correlations/index.html#pearson-correlation",
    "href": "content/courses/Basics-of-Modeling/Modules/20-LM-Correlations/index.html#pearson-correlation",
    "title": "Linear Model: Correlation Test",
    "section": "\n1.1 Pearson Correlation",
    "text": "1.1 Pearson Correlation\n\n1.1.1 Model\nThe model for Pearson Correlation tests is exactly the Linear Model:\n\\[\n\\begin{aligned}\ny = \\beta_0 + \\beta_1 \\times x\\\\\n\\\\\nH_0: Null\\ Hypothesis\\ => \\beta_1 = 0\\\\\\\nH_a: Alternate\\ Hypothesis\\ => \\beta_1 \\ne 0\\\\\n\\end{aligned}\n\\]\n\n\n\n\n  \n\n\n\nUsing the linear model method we get:\n\n\n\n\n  \n\n\n\nWhy are \\(r\\) and \\(\\beta_1\\) different, though the p-value is suspiciously the same!?\nDid we miss a factor of \\(\\frac{-0.463}{-0.231} = 2\\) somewhere‚Ä¶??\nLet us scale the variables to within {-1, +1} : (subtract the mean and divide by sd) and re-do the Linear Model with scaled versions \\(x\\) and \\(y\\):\n\n\n\n\n  \n\n\n\nSo we conclude:\n\nWhen both x and y have the same standard deviation, the slope from the linear model and the Pearson correlation are the same. Here, since x has twice the sd of y, the ratio of slope = -0.4635533 to r = -0.2317767 is 0.5.\nThere is this relationship between the slope in the linear model and Pearson correlation:\n\n\\[\nSlope\\ \\beta_1 = \\frac{sd_y}{sd_x} * r\n\\]\nThe slope is usually much more interpretable and informative than the correlation coefficient.\n\nHence a linear model using scale() for both variables will show slope = r.\n\nSlope_Scaled: -0.2317767 = Correlation: -0.2317767\n\nFinally, the p-value for Pearson Correlation and that for the slope in the linear model is the same (\\(0.1053\\)). Which means we cannot reject the NULL hypothesis of ‚Äúno relationship‚Äù.\n\n1.1.2 Example\nTBD"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/20-LM-Correlations/index.html#spearman-correlation",
    "href": "content/courses/Basics-of-Modeling/Modules/20-LM-Correlations/index.html#spearman-correlation",
    "title": "Linear Model: Correlation Test",
    "section": "\n1.2 Spearman Correlation",
    "text": "1.2 Spearman Correlation\n\n1.2.1 Model\nIn some cases the LINE assumptions may not hold.\nNonlinear relationships, non-normally distributed data ( with large outliers ) and working with ordinal rather than continuous data: these situations necessitate the use of Spearman‚Äôs ranked correlation scores. (Ranked, not sign-ranked.).\nSee the example below: We choose to look at the gpa_study_hours dataset. It has two numeric columns gpa and study_hours:\n\n\nRows: 193\nColumns: 2\n$ gpa         <dbl> 4.000, 3.800, 3.930, 3.400, 3.200, 3.520, 3.680, 3.400, 3.‚Ä¶\n$ study_hours <dbl> 10, 25, 45, 10, 4, 10, 24, 40, 10, 10, 30, 7, 15, 60, 10, ‚Ä¶\n\n\nWe can plot this:\n\n\n\n\n\n\n\n\nHmm‚Ä¶not normally distributed, and there is a sort of increasing relationship, however is it linear? And there is some evidence of heteroscedasticity, so the LINE assumptions are clearly in violation. Pearson correlation would not be the best idea here.\nLet us quickly try it anyway, using a Linear Model for the scaled gpa and study_hours variables, from where we get:\n\n\n\n\n  \n\n\n\nThe correlation estimate is \\(0.133\\); the p-value is \\(0.065\\) (and the confidence interval includes \\(0\\)).\nHence we fail to reject the NULL hypothesis that study_hours and gpa have no relationship. But can this be right?\nShould we use another test, that does not need the LINE assumptions?"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/20-LM-Correlations/index.html#signed-rank-values",
    "href": "content/courses/Basics-of-Modeling/Modules/20-LM-Correlations/index.html#signed-rank-values",
    "title": "Linear Model: Correlation Test",
    "section": "\n1.3 ‚ÄúSigned Rank‚Äù Values",
    "text": "1.3 ‚ÄúSigned Rank‚Äù Values\nMost statistical tests use the actual values of the data variables. However, in some non-parametric statistical tests, the data are used in rank-transformed sense/order. (In some cases the signed-rank of the data values is used instead of the data itself.)\nSigned Rank is calculated as follows:\n\nTake the absolute value of each observation in a sample\nPlace the ranks in order of (absolute magnitude). The smallest number has rank = 1 and so on. This gives is ranked data.\nGive each of the ranks the sign of the original observation ( + or -). This gives us signed ranked data."
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/20-LM-Correlations/index.html#plotting-original-and-signed-rank-data",
    "href": "content/courses/Basics-of-Modeling/Modules/20-LM-Correlations/index.html#plotting-original-and-signed-rank-data",
    "title": "Linear Model: Correlation Test",
    "section": "\n1.4 Plotting Original and Signed Rank Data",
    "text": "1.4 Plotting Original and Signed Rank Data\nLet us see how this might work by comparing data and its signed-rank version‚Ä¶A quick set of plots:\n\n\n\n\n\n\n\n\nSo the means of the ranks three separate variables seem to be in the same order as the means of the data variables themselves.\nHow about associations between data? Do ranks reflect well what the data might?\n\n\n\n\n\n\n\n\nThe slopes are almost identical, \\(0.25\\) for both original data and ranked data for \\(y1\\sim x\\). So maybe ranked and even sign_ranked data could work, and if it can work despite LINE assumptions not being satisfied, that would be nice!"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/20-LM-Correlations/index.html#how-does-sign-rank-data-work",
    "href": "content/courses/Basics-of-Modeling/Modules/20-LM-Correlations/index.html#how-does-sign-rank-data-work",
    "title": "Linear Model: Correlation Test",
    "section": "\n1.5 How does Sign-Rank data work?",
    "text": "1.5 How does Sign-Rank data work?\nTBD: need to add some explanation here.\nSpearman correlation = Pearson correlation using the rank of the data observations. Let‚Äôs check how this holds for a our x and y1 data:\nSo the Linear Model for the Ranked Data would be:\n\\[\n\\begin{aligned}\ny = \\beta_0 + \\beta_1 \\times rank(x)\\\\\n\\\\\nH_0: Null\\ Hypothesis\\ => \\beta_1 = 0\\\\\\\nH_a: Alternate\\ Hypothesis\\ => \\beta_1 \\ne 0\\\\\n\\end{aligned}\n\\]\n\n1.5.1 Code\n\n\n\n\n  \n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n  \n\n\n\nNotes:\n\nWhen ranks are used, the slope of the linear model (\\(\\beta_1\\)) has the same value as the Spearman correlation coefficient ( \\(\\rho\\) ).\nNote that the slope from the linear model now has an intuitive interpretation: the number of ranks y changes for each change in rank of x. ( Ranks are ‚Äúindependent‚Äù of sd )\n\n1.5.2 Example\nWe examine the cars93 data, where the numeric variables of interest are weight and price.\n\n\n\n\n\n\n\n\nLet us try a Spearman Correlation score for these variables, since the data are not linearly related and the variance of price also is not constant over weight\n\n\n\n\n  \n\n\n\n\nCall:\nlm(formula = rank(price) ~ rank(weight), data = cars93)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-20.0676  -3.0135   0.7815   3.6926  20.4099 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   3.22074    2.05894   1.564    0.124    \nrank(weight)  0.88288    0.06514  13.554   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.46 on 52 degrees of freedom\nMultiple R-squared:  0.7794,    Adjusted R-squared:  0.7751 \nF-statistic: 183.7 on 1 and 52 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\n\n\nWe see that using ranks of the price variable, we obtain a Spearman‚Äôs \\(\\rho = 0.882\\) with a p-value that is very small. Hence we are able to reject the NULL hypothesis and state that there is a relationship between these two variables. The linear relationship is evaluated as a correlation of 0.882."
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#a-workflow-in-radiant",
    "href": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#a-workflow-in-radiant",
    "title": "Linear Models: Comparing Proportions",
    "section": "\n0.2 A Workflow in Radiant",
    "text": "0.2 A Workflow in Radiant"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#a-workflow-in-r",
    "href": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#a-workflow-in-r",
    "title": "Linear Models: Comparing Proportions",
    "section": "\n0.3 A Workflow in R",
    "text": "0.3 A Workflow in R"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#test",
    "href": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#test",
    "title": "Linear Models: Comparing Proportions",
    "section": "\n2.1 Test",
    "text": "2.1 Test\n\n2.1.1 Model\nExplanation, formula etc.\n\n2.1.2 Code\nWith Toy Data; Graphs\n\n2.1.3 Example\nWith another ‚Äúreal world‚Äù data set; Graphs"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#sample-values",
    "href": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#sample-values",
    "title": "Linear Models: Comparing Proportions",
    "section": "\n3.1 Sample Values",
    "text": "3.1 Sample Values\nMost examples in this exposition are based on three ‚Äúimaginary‚Äù samples, \\(x, y, y2\\). Each is normally distributed and made up of 50 observations.\nWe start by creating a function that will allow us to produce samples of a given size (N) with a specified mean (mu) and standard deviation (sd): Note: this gives a matrix of numbers, as opposed to a vector using rnorm by itself. The data are created both in vector form and tibble form for flexibility of use in diverse packages and formulae in what follows.\n\nShow the Codernorm_fixed  <- function(N, mu = 0, sd = 1) {\n  scale(rnorm(N))* sd + mu\n}\n\n\nWe create three variables: x ( explanatory) and y, y2 ( dependent ).\n\nShow the Codeset.seed(40) # for replication\n\n# Data as vectors ( for t.tests etc)\nx <- rnorm_fixed(50, mu = 0.0, sd = 1) #explanatory\ny <- rnorm_fixed(50, mu = 0.3, sd = 2) # dependent #1\ny2 <- rnorm_fixed(50, mu = 0.5, sd = 1.5) # dependent #2\n\n# Make a tibble with all variables\nmydata_wide <- tibble(x = x, y = y, y2 = y2)\n\n# Long form data\nmydata_long <- \n  mydata_wide %>%\n  pivot_longer(., cols = c(x,y,y2), \n               names_to = \"group\", \n               values_to = \"value\")\n\n# Long form data with only dependent variables\nmydata_long_y <- \n  mydata_wide %>% \n  select(-x) %>% \n  pivot_longer(., cols = c(y,y2), \n               names_to = \"group\", \n               values_to = \"value\")\nmydata_wide\n\n\n\n  \n\n\nShow the Codemydata_long\n\n\n\n  \n\n\nShow the Codemydata_long_y"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#signed-rank-values",
    "href": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#signed-rank-values",
    "title": "Linear Models: Comparing Proportions",
    "section": "\n3.2 ‚ÄúSigned Rank‚Äù Values",
    "text": "3.2 ‚ÄúSigned Rank‚Äù Values\nMost statistical tests use the actual values of the data variables. However, in some non-parametric statistical tests, the data are used in rank-transformed sense/order. In some cases the signed-rank of the data values is used instead of the data itself.\nSigned Rank is calculated as follows:\n1. Take the absolute value of each observation in a sample\n2. Place the ranks in order of (absolute magnitude). The smallest number has rank = 1 and so on.\n3. Give each of the ranks the sign of the original observation ( + or - )\n\nShow the Codesigned_rank <- function(x) {sign(x) * rank(abs(x))}"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#plotting-original-and-signed-rank-data",
    "href": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#plotting-original-and-signed-rank-data",
    "title": "Linear Models: Comparing Proportions",
    "section": "\n3.3 Plotting Original and Signed Rank Data",
    "text": "3.3 Plotting Original and Signed Rank Data\nA quick plot:\n\nShow the Codep1 <- ggplot(mydata_long,aes(x = group, y = value)) +\n  geom_jitter(width = 0.02, height = 0,aes(colour = group), size = 4) +\n  geom_segment(data = mydata_wide, aes(y = 0, yend = 0, \n                                       x = .75, \n                                       xend = 1.25 )) + \n  geom_text(aes(x = 1, y = 0.5, label = \"0\")) +\n  geom_segment(data = mydata_wide, aes(y = 0.3, yend = 0.3, \n                                       x = 1.75 , \n                                       xend = 2.25 )) + \n  geom_text(aes(x = 2, y = 0.6, label = \"0.3\")) +\n  geom_segment(data = mydata_wide, aes(y = 0.5, yend = 0.5, \n                                       x = 2.75, \n                                       xend = 3.25 )) + \n  geom_text(aes(x = 3, y = 0.8, label = \"0.5\")) +\n  labs(title = \"Original Data\") +\n  ylab(\"Response Variable\")\n\np2 <- mydata_long %>% \n  group_by(group) %>% \n  mutate( s_value = signed_rank(value)) %>% \n  ggplot(., aes(x = group, y = s_value)) + \n  geom_jitter(width = 0.02, height = 0,aes(colour = group), size = 4) + \n  stat_summary(fun = \"mean\", geom = \"point\", colour = \"red\", \n               size = 8) + \n  labs(title = \"Signed Rank of Data\") +\n  ylab(\"Signed Rank of Response Variable\")\n\npatchwork::wrap_plots(p1,p2, nrow = 1, guides = \"collect\")"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#how-does-sign-rank-data-work",
    "href": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#how-does-sign-rank-data-work",
    "title": "Linear Models: Comparing Proportions",
    "section": "\n3.4 How does Sign-Rank data work?",
    "text": "3.4 How does Sign-Rank data work?\nTBD: need to add some explanation here."
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#linear-models-as-hypothesis-tests",
    "href": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#linear-models-as-hypothesis-tests",
    "title": "Linear Models: Comparing Proportions",
    "section": "\n4.1 Linear Models as Hypothesis Tests",
    "text": "4.1 Linear Models as Hypothesis Tests\nUsing linear models is based on the idea of Testing of Hypotheses. The Hypothesis Testing method typically defines a NULL Hypothesis where the statements read as ‚Äúthere is no relationship‚Äù between the variables at hand, explanatory and responses. The Alternative Hypothesis typically states that there is a relationship between the variables.\nAccordingly, in fitting a linear model, we follow the process as follows:\n\nMake the following hypotheses: \\[\ny = \\beta_0 + \\beta_1 *x \\\\\nNULL\\ Hypothesis\\ H_0 => x\\ and\\ y\\ are\\ unrelated.\\ (\\beta_1 = 0)\n\\] \\[\ny = \\beta_0 + \\beta_1 *x \\\\\nAlternate\\ Hypothesis\\ H_1 => x\\ and\\ y\\ are\\ linearly\\ related\\ (\\beta_1 \\ne 0)\n\\]\n\nWe ‚Äúassume‚Äù that \\(H_0\\) is true.\nWe calculate \\(\\beta_1\\).\nWe then find probability p that [\\(\\beta_1 = Estimated\\ Value\\)] when the NULL Hypothesis is assumed TRUE. This is the p-value. If that probability is p>=0.05, we say we ‚Äúcannot reject‚Äù \\(H_0\\) and there is unlikely to be significant linear relationship.\n\nHowever, if p<= 0.05 can we reject the NULL hypothesis, and say that there could be a significant linear relationship,because \\(\\beta_1 = Estimated\\ Value\\) by mere chance under \\(H_0\\) is very small.\nPheeew !!"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#linear-models-in-r",
    "href": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#linear-models-in-r",
    "title": "Linear Models: Comparing Proportions",
    "section": "\n4.2 Linear Models in R",
    "text": "4.2 Linear Models in R\nlm() is the function to create linear models in R. In R we are lazy and write :\n\\[\ny \\sim 1 + x\\\\\nwhich\\ reads\\ like\\\\\ny = 1*number + x* another\\ number\n\\]\nNote: there are very many ways in which linear models can be coded in R. See Vito Ricci on CRAN.\n\nShow the Code# using lm()\nlm(y ~ 1 + x, data = mydata_wide) %>% \n  summary() %>%   \n  print(digits = 5)\n\n\nCall:\nlm(formula = y ~ 1 + x, data = mydata_wide)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3.93980 -1.09967  0.12548  1.27904  3.88372 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)  0.30000    0.27799  1.0792   0.2859\nx           -0.46355    0.28081 -1.6507   0.1053\n\nResidual standard error: 1.9657 on 48 degrees of freedom\nMultiple R-squared:  0.05372,   Adjusted R-squared:  0.034006 \nF-statistic:  2.725 on 1 and 48 DF,  p-value: 0.10532\n\n\nSince the p-value is >=0.05, we fail to reject the NULL Hypothesis that there is no relationship between x and y."
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#assumptions-in-linear-models",
    "href": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#assumptions-in-linear-models",
    "title": "Linear Models: Comparing Proportions",
    "section": "\n4.3 Assumptions in Linear Models",
    "text": "4.3 Assumptions in Linear Models\n\n\nL: \\(\\color{blue}{linear}\\) relationship\n\nI: Errors are independent (across observations)\n\nN: y is \\(\\color{red}{normally}\\) distributed at each ‚Äúlevel‚Äù of\n\n\n\n\n\nE: equal variance at all levels of x. No heteroscedasticity. \n\n\nLet us now see which standard statistical tests can be re-formulated as Linear Models."
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#pearson-correlation",
    "href": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#pearson-correlation",
    "title": "Linear Models: Comparing Proportions",
    "section": "\n5.1 Pearson Correlation",
    "text": "5.1 Pearson Correlation\n\n5.1.1 Model\nThe model for Pearson Correlation tests is exactly the Linear Model:\n\\[\ny = \\beta_0 + \\beta_1 * x\n\\\\\nH_0: \\beta_1 = 0\n\\]\nSee the Code section for further insights into the relationship between the Correlation Score and the Slope of the Linear Model.\n\n5.1.2 Code\n\nShow the Code# Pearson (built-in test)\ncor <- cor.test(y,x,method = \"pearson\") %>% \n  broom::tidy() %>% select(estimate, p.value)\n\n# Linear Model\nlin <- lm(y ~ 1 + x, data = mydata_wide) %>% \n  broom::tidy() %>% select(estimate, p.value)\n\n# Scaled linear model\nlin_scl <- lm(scale(y) ~ 1 + scale(x), data = mydata_wide) %>% \n  broom::tidy() %>% select(estimate, p.value)\n\nprint(cor)\n\n# A tibble: 1 √ó 2\n  estimate p.value\n     <dbl>   <dbl>\n1   -0.232   0.105\n\nShow the Codeprint(lin)\n\n# A tibble: 2 √ó 2\n  estimate p.value\n     <dbl>   <dbl>\n1    0.3     0.286\n2   -0.464   0.105\n\nShow the Codeprint(lin_scl)\n\n# A tibble: 2 √ó 2\n   estimate p.value\n      <dbl>   <dbl>\n1 -9.06e-17   1    \n2 -2.32e- 1   0.105\n\nShow the Code# All together\nrbind(cor, lin, lin_scl) %>% print()\n\n# A tibble: 5 √ó 2\n   estimate p.value\n      <dbl>   <dbl>\n1 -2.32e- 1   0.105\n2  3   e- 1   0.286\n3 -4.64e- 1   0.105\n4 -9.06e-17   1    \n5 -2.32e- 1   0.105\n\n\nNotes: 1. The p-value for Pearson Correlation and that for the slope in the linear model is the same ( 0.1053 ). Which means we cannot reject the NULL hypothesis of ‚Äúno relationship‚Äù.\n\nHere is the relationship between the slope and correlation:\n\n\\[\nSlope\\ \\beta_1 = \\frac{sd_y}{sd_x} * r\n\\]\nWhen both x and y have the same standard deviation, the slope and correlation are the same. Here, since x has twice the sd of y, the ratio of slope = -0.4635533 to r = -0.2317767 is 0.5. Hence a linear model using scale() for both variables will show slope = r.\nSlope_Scaled: -0.2317767 = Correlation: -0.2317767\n\n5.1.3 Example\nWe choose to look at the gpa_study_hours dataset. It has two numeric columns gpa and study_hours:\n\nShow the Codeglimpse(gpa_study_hours)\n\nRows: 193\nColumns: 2\n$ gpa         <dbl> 4.000, 3.800, 3.930, 3.400, 3.200, 3.520, 3.680, 3.400, 3.‚Ä¶\n$ study_hours <dbl> 10, 25, 45, 10, 4, 10, 24, 40, 10, 10, 30, 7, 15, 60, 10, ‚Ä¶\n\n\n\nShow the Code# Checks for Normal/Symmetric distributions\np1 <- ggplot(gpa_study_hours) + geom_histogram(aes(gpa))\np2 <- ggplot(gpa_study_hours) + geom_histogram(aes(study_hours))\np3 <- ggplot(gpa_study_hours) + geom_point(aes(gpa, study_hours))\n(p1 + p2) / p3\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nHmm‚Ä¶not normally distributed, and the relationship is also not linear, and there is some evidence of heterscedasticity, so Pearson correlation would not be the best idea here.\n\nShow the Code# Pearson Correlation as Linear Model\nlm(gpa ~ study_hours, data = gpa_study_hours) %>% summary()\n\n\nCall:\nlm(formula = gpa ~ study_hours, data = gpa_study_hours)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.95130 -0.19456  0.03879  0.21708  0.73872 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 3.527997   0.037424  94.272   <2e-16 ***\nstudy_hours 0.003328   0.001794   1.855   0.0652 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2837 on 191 degrees of freedom\nMultiple R-squared:  0.01769,   Adjusted R-squared:  0.01255 \nF-statistic:  3.44 on 1 and 191 DF,  p-value: 0.06517\n\nShow the Code# Other ways using other packages\nmosaic::cor_test(gpa ~ study_hours, data = gpa_study_hours)\n\n\n    Pearson's product-moment correlation\n\ndata:  gpa and study_hours\nt = 1.8548, df = 191, p-value = 0.06517\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.008383868  0.269196552\nsample estimates:\n      cor \n0.1330138 \n\nShow the CodestatsExpressions::corr_test(data = gpa_study_hours, \n                            x = study_hours, \n                            y = gpa)\n\n\n\n  \n\n\n\nThe correlation estimate is \\(0.133\\); the p-value is 0.065 and the confidence interval includes 0. Hence we fail to reject the NULL hypothesis that study_hours and gpa have no relationship.\nWe can use a later package ggstaplot to plot this:\n\nShow the Codeggstatsplot::ggscatterstats(data = gpa_study_hours, \n                            x = study_hours, \n                            y = gpa,\n                            type = \"robust\",\n                            marginal = TRUE,\n                            title = \"GPA vs Study Hours\")\n\nRegistered S3 method overwritten by 'ggside':\n  method from   \n  +.gg   ggplot2\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#spearman-correlation",
    "href": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#spearman-correlation",
    "title": "Linear Models: Comparing Proportions",
    "section": "\n5.2 Spearman Correlation",
    "text": "5.2 Spearman Correlation\n\n5.2.1 Model\nIn some cases the LINE assumptions may not hold. Nonlinear relationships, non-normally distributed data ( with large outliers ) and working with ordinal rather than continuous data: these situations necessitate the use of Spearman‚Äôs ranked correlation scores. (Ranked, not sign-ranked.)\n\\[\nrank(y) = \\beta_0 + \\beta_1 * rank(x) \\\\\nH_0: \\beta_1 = 0\n\\]\nSpearman correlation = Pearson correlation using the rank of the data observations. Let‚Äôs check how this holds for a our x and y data:\n\nShow the Code# Plot the data\np1 <- ggplot(mydata_wide, aes(x, y)) + \n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  ggtitle(\" Pearson Correlation\\n and Linear Models\")\n\n# Plot ranked data\np2 <- mydata_wide %>% \n  mutate(x_rank = rank(x),\n         y_rank = rank(y)) %>%\n  ggplot(.,aes(x_rank, y_rank)) + \n  geom_point(shape = 15, size = 2) +\n  geom_smooth(method = \"lm\") + \n  ggtitle(\" Spearman Ranked Correlation\\n and Linear Models\")\n\npatchwork::wrap_plots(p1,p2, nrow = 1, guides = \"collect\")\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nSlopes are almost identical, ~ 0.25.\n\n5.2.2 Code\n\nShow the Code# Spearman\ncor1 <- cor.test(y,x, method = \"spearman\") %>% \n  broom::tidy() %>% select(estimate, p.value)\n\n# Pearson using ranks\ncor2 <- cor.test(rank(y), rank(x), method = \"pearson\") %>% \nbroom::tidy() %>% select(estimate, p.value)\n\n# Linear Models using rank\ncor3 <- lm(rank(y) ~ 1 + rank(x),data = mydata_wide) %>% \n  broom::tidy() %>% select(estimate, p.value)\n\nrbind(cor1, cor2, cor3) %>% print()\n\n# A tibble: 4 √ó 2\n  estimate  p.value\n     <dbl>    <dbl>\n1   -0.227 1.13e- 1\n2   -0.227 1.14e- 1\n3   31.3   9.11e-10\n4   -0.227 1.14e- 1\n\n\nNotes:\n\nWhen ranks are used, the slope of the linear model (\\(\\beta_1\\)) has the same value as the correlation coefficient ( \\(\\rho\\) ).\nNote that the slope from the linear model now has an intuitive interpretation: the number of ranks y changes for each change in rank of x. ( Ranks are ‚Äúindependent‚Äù of sd )\n\n5.2.3 Example\nWe examine the cars93 data, where the numeric variables of interest are weight and price.\n\nShow the Codecars93 %>% \n  ggplot(aes(weight, price)) + \n  geom_point() + geom_smooth(method = \"lm\", se = FALSE, lty = 2) + \n  labs(title = \"Car Weight and Car Price have a nonlinear relationship\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nLet us try a Spearman Correlation score for these variables, since the data are not linearly related and the variance of price also is not constant over weight\n\nShow the Codecor.test(cars93$price, cars93$weight, method = \"spearman\") %>% broom::tidy()\n\nWarning in cor.test.default(x, y, ...): Cannot compute exact p-value with ties\n\n\n\n\n  \n\n\nShow the Code# Using linear Model\nlm(rank(price) ~ rank(weight), data = cars93) %>% summary()\n\n\nCall:\nlm(formula = rank(price) ~ rank(weight), data = cars93)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-20.0676  -3.0135   0.7815   3.6926  20.4099 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   3.22074    2.05894   1.564    0.124    \nrank(weight)  0.88288    0.06514  13.554   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.46 on 52 degrees of freedom\nMultiple R-squared:  0.7794,    Adjusted R-squared:  0.7751 \nF-statistic: 183.7 on 1 and 52 DF,  p-value: < 2.2e-16\n\nShow the Code# Stats Plot\nggstatsplot::ggscatterstats(data = cars93, x = weight, \n                            y = price,\n                            type = \"nonparametric\",\n                            title = \"Cars93: Weight vs Price\",\n                            subtitle = \"Spearman Correlation\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nWe see that using ranks of the price variable, we obtain a Spearman‚Äôs \\(\\rho = 0.882\\) with a p-value that is very small. Hence we are able to reject the NULL hypothesis and state that there is a relationship between these two variables. The linear relationship is evaluated as a correlation of 0.882."
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#the-students-t-test-with-one-sample",
    "href": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#the-students-t-test-with-one-sample",
    "title": "Linear Models: Comparing Proportions",
    "section": "\n6.1 The Student‚Äôs t-test with one sample",
    "text": "6.1 The Student‚Äôs t-test with one sample\n\n6.1.1 Model\nA single number predicts y:\n\\[\ny = \\beta_0 + \\beta_1*x \\\\\n\\\\and\\ further \\ actually\\\\\ny = \\beta_0\n\\]\nand the second term vanishes, since ‚Äúthere is no x‚Äù: all the x-values are made equal to zero in the linear model !! The NULL Hypothesis therefore is:\n\\[\n\\ H_0: \\beta_0 = 0\n\\]\nThis NULL Hypothesis makes sense, because in the accompanying linear model all values of the explanatory variable x are zero, and therefore the NULL Hypothesis for the model should be that y also should be zero mean. Note that if we want the NULL hypothesis to be that the mean is other than zero, we can use the lm(...., mu = some_number, ..) parameter in the command.\n\n6.1.2 Code\nIf we compare the t.test with the appropriate lm model:\n\nShow the Code# t-test\nt1 <- t.test(y, mu = 0, alternative = \"two.sided\")\nprint(t1)\n\n\n    One Sample t-test\n\ndata:  y\nt = 1.0607, df = 49, p-value = 0.294\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.2683937  0.8683937\nsample estimates:\nmean of x \n      0.3 \n\nShow the Code# linear model\nlm1 <- lm(y ~ 1, data = mydata_wide)\nlm1 %>% summary()\n\n\nCall:\nlm(formula = y ~ 1, data = mydata_wide)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5554 -1.4845 -0.0392  1.5559  4.5119 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   0.3000     0.2828   1.061    0.294\n\nResidual standard error: 2 on 49 degrees of freedom\n\nShow the Codelm1 %>% confint()\n\n                 2.5 %    97.5 %\n(Intercept) -0.2683937 0.8683937\n\n\nThe confidence intervals for both the t.test and the lm model are identical.\nt-test confidence intervals: -0.2683937, 0.8683937\nlinear model confidence intervals: -0.2683937, 0.8683937\nSo even though y has a mean of 0.3, the confidence intervals straddle zero, and hence we cannot reject the NULL hypothesis that the true population, of which y is a sample, could have mean=0.\n\n6.1.3 Example\n\nShow the Codeexam_grades"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#wilcoxons-signed-rank-test",
    "href": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#wilcoxons-signed-rank-test",
    "title": "Linear Models: Comparing Proportions",
    "section": "\n6.2 Wilcoxon‚Äôs Signed-Rank Test",
    "text": "6.2 Wilcoxon‚Äôs Signed-Rank Test\nSince we are dealing with the mean, the sign of the rank becomes important to use, in the case of a non-parametric single mean test.\n\n6.2.1 Model\n\\[\nsigned\\_rank(y) = \\beta_0 \\\\\nH_0: \\beta_0 = 0\n\\]\n\n6.2.2 Code\n\nShow the Code# Standard Wilcoxon Signed_Rank Test\nw1 <- wilcox.test(y)\nw1\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  y\nV = 754, p-value = 0.2628\nalternative hypothesis: true location is not equal to 0\n\nShow the Code# Wilcoxon test with lm\nw2 <- lm(signed_rank(y) ~ 1 , data = mydata_wide)\nw2\n\n\nCall:\nlm(formula = signed_rank(y) ~ 1, data = mydata_wide)\n\nCoefficients:\n(Intercept)  \n       4.66  \n\nShow the Code# t-test with signed_rank data\nw3 <- t.test(signed_rank(y))\nw3\n\n\n    One Sample t-test\n\ndata:  signed_rank(y)\nt = 1.1277, df = 49, p-value = 0.265\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -3.644491 12.964491\nsample estimates:\nmean of x \n     4.66 \n\n\nWe can plot the y data both original and ranked to see where the mean lies in each case. The approximation to the true $_0¬†( is good when the number of observations N is >=50. Lindoloev has a simulation on this.. We can also plot the model using lm for both the original data and the sign-ranked data.\n\n6.2.3 Example\n\n6.2.4 Plots for both t-test and Wilcoxon test\n\nShow the Codep1 <- ggplot(mydata_wide, aes( x = 0, y = y)) +\n  geom_point(alpha = 0.4) +\n  geom_segment(aes(y = t1$estimate, \n                   yend = t1$estimate, \n                   x = -0.2, xend = 0.2)) + \n  labs(title = \"Student's\\n t-Test\")\n\n# t-test using linear model\np2 <- ggplot(mydata_wide, aes( x = 0, y = y)) +\n  geom_point(alpha = 0.4) +\n  geom_segment(aes(y = lm(y ~ 1)$coefficient, \n                   yend = lm(y ~ 1)$coefficient, \n                   x = -0.2, xend = 0.2)) + \n  labs(title = \"Student's\\n t-Test \\n using lm\")\n\n# Wilcoxon test, using signed-ranks of data\np3 <- ggplot(mydata_wide, aes( x = 0, y = signed_rank(y))) +\n  geom_point(alpha = 0.4) +\n  geom_segment(aes(y = mean(signed_rank(y)), yend = mean(signed_rank(y)), x = -0.2, xend = 0.2)) + \n  labs(title = \"Wilcoxon \\nSigned-Rank\\n Test\")\n\n# Wilcoxon test, using signed-ranks of data, and lm\np4 <- ggplot(mydata_wide, aes( x = 0, y = signed_rank(y))) +\n  geom_point(alpha = 0.4) +\n  geom_segment(aes(y = lm(signed_rank(y) ~1)$coefficient, \n                   yend = lm(signed_rank(y) ~1)$coefficient, \n                   x = -0.2, xend = 0.2)) + \n  labs(title = \"Wilcoxon \\n Signed-Rank \\n Test with lm\")\n\n\npatchwork::wrap_plots(p1,p2,p3,p4, nrow = 1, guides = \"collect\")"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#paired-sample-t-test",
    "href": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#paired-sample-t-test",
    "title": "Linear Models: Comparing Proportions",
    "section": "\n6.3 Paired Sample t-test",
    "text": "6.3 Paired Sample t-test\nWe use this when we have two samples and the observations from one sample can be ‚Äúpaired‚Äù with observations in the other sample. Controlled studies for interventions/measures, such as before/after kind of data, comparisons between two interventions on the same set of subjects, and two measurements made on the same subjects using different methods etc.\n\n6.3.1 Model\n\\[\ny_2 - y_1 = \\beta_0 \\\\\nH_0 : \\beta_0 = 0\n\\]\nThe NULL Hypothesis is that there is no difference, either way, between the two samples. Again, in the linear model, we assume as before that ‚Äúthe explanatory x variable has been equated to zero.\nWe therefore set two.sided and mu = 0 in the t.test.\n\n6.3.2 Code\n\nShow the Code# Using paired t-test\nt2 <- t.test(y2, y, paired = TRUE, mu = 0, \n             alternative = \"two.sided\")\nt2\n\n\n    Paired t-test\n\ndata:  y2 and y\nt = 0.54264, df = 49, p-value = 0.5898\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -0.54067  0.94067\nsample estimates:\nmean difference \n            0.2 \n\nShow the Code# linear model\nlm2 <- lm(y2-y ~ 1, data = mydata_wide)\nlm2 %>% summary()\n\n\nCall:\nlm(formula = y2 - y ~ 1, data = mydata_wide)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.7027 -2.1872 -0.1367  1.3835  5.7262 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   0.2000     0.3686   0.543     0.59\n\nResidual standard error: 2.606 on 49 degrees of freedom\n\nShow the Codelm2 %>% confint()\n\n               2.5 %  97.5 %\n(Intercept) -0.54067 0.94067\n\n\nBoth tests report the difference to be 0.2. However the p-value in both tests is about 0.6, so the result is not statistically significant.\n\n6.3.3 Example"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#wilcoxon-paired-test",
    "href": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#wilcoxon-paired-test",
    "title": "Linear Models: Comparing Proportions",
    "section": "\n6.4 Wilcoxon Paired Test",
    "text": "6.4 Wilcoxon Paired Test\nWhen the original data is not normally distributed or has outliers etc,. we use a nonparametric Wilcoxon paired test. The difference between the paired and unpaired Wilcoxon test is that the test is run on the signed-ranks of the pairwise differences y2- y.\n\n6.4.1 Model\n$$ signed_rank(y2 - y1) = _0 \\\nH_0: _0 = 0\n$$\n\n6.4.2 Code\n\nShow the Code# Paired Wilcoxon Test\nw4 <- wilcox.test(y, y2, paired = TRUE)\nw4\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  y and y2\nV = 608, p-value = 0.7795\nalternative hypothesis: true location shift is not equal to 0\n\nShow the Code# Linear Model\nlm4 <- lm(signed_rank(y2-y) ~ 1 , data = mydata_wide)\nlm4 %>% summary()\n\n\nCall:\nlm(formula = signed_rank(y2 - y) ~ 1, data = mydata_wide)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-48.18 -27.93   0.82  22.57  48.82 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)    1.180      4.182   0.282    0.779\n\nResidual standard error: 29.57 on 49 degrees of freedom\n\nShow the Code# t-test with Signed Rank\nt4 <- t.test(signed_rank(y2-y), mu = 0 , alternative = \"two.sided\")\nt4\n\n\n    One Sample t-test\n\ndata:  signed_rank(y2 - y)\nt = 0.28214, df = 49, p-value = 0.779\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -7.224733  9.584733\nsample estimates:\nmean of x \n     1.18 \n\n\nHere too, the p-values reported by the three tests are p = 0.779 so the difference reported is not significant.\n\n6.4.3 Example"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#dummy-group-variable-concept",
    "href": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#dummy-group-variable-concept",
    "title": "Linear Models: Comparing Proportions",
    "section": "\n7.1 Dummy Group Variable Concept",
    "text": "7.1 Dummy Group Variable Concept\nAn important construct here is the dummy variable. When there is more than one group in the data, a dummy categorical variable is set up, whose entries specify the group ID. The group IDs are still numerical ( as with factors, remember ). The dummy variable is plotted on the x-axis. The consecutive IDs in the dummy variable x are separated by 1. Hence the between-groups difference in the stat measures computed on y are numerically equivalent to the slope in the linear model. Thus the dummy variable allows us to **mathematically* use the linear model, as presented in the equations above.\nDummy variables become even more useful when the explanatory variable ( ‚Äúx-planatory‚Äù ) is already categorical, as with ANOVA and friends.\nWe can visualize this as follows:\n\nShow the Codemydata_wide_new <- \n  tibble(y1 = rnorm(50, mean = 0, sd = 0.5),\n         y2 = rnorm(50, mean = 1.2, sd = 0.5)) %>%\n  pivot_longer(cols = c(y1, y2),\n               names_to = \"variable\",  \n               values_to = \"values\") %>% \n  cbind(group = rep(0:1, 50))\n\nmydata_wide_new %>% \n  ggplot(aes(x = group, y = values)) + \n  geom_point() + \n  stat_summary(fun = \"mean\", colour = \"red\", size = 4, geom =\"point\") +\n  stat_summary(fun = \"mean\", geom= \"line\", colour = \"blue\", lty = 2) +\n  xlab(\"Dummy Variable to show groups\") +\n  ylab(\"y1 and y2, on the same scale\") +\n  scale_x_discrete(name = \"Dummy Variable x_i  [0,1]\",\n                limits = c(0,1)) +\n  annotate(\"text\", x = 0, y = 1.5, label = \"Difference in means \\n equals slope in linear model\") \n\nWarning: Continuous limits supplied to discrete scale.\n‚Ñπ Did you mean `limits = factor(...)` or `scale_*_continuous()`?\n\n\n\n\n\n\n\nShow the Code# Need to use `glue` here to add more annotations\n# Math annotation on graphs"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#model-7",
    "href": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#model-7",
    "title": "Linear Models: Comparing Proportions",
    "section": "\n7.2 Model",
    "text": "7.2 Model\n\\[\ny_i = \\beta_0 + \\beta_1 * x_i \\\\\nwhere\\\\\nx_i= \\left\\{\\begin{matrix}\n1\\ when\\ x\\ \\in\\ Group 1\\\\\n0\\ when\\ x\\ \\in\\ Group2\n\\end{matrix}\\right.\n\\]"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#independent-t-test",
    "href": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#independent-t-test",
    "title": "Linear Models: Comparing Proportions",
    "section": "\n7.3 Independent t-test",
    "text": "7.3 Independent t-test\n\n7.3.1 Model\nThe assumptions here are: - both data sets are normally distributed. Small samples may be assumned to be t-distributed - variances are the same - no outliers - observations across data sets are independent (obviously)\n\\[\ny_i = \\beta_0 + \\beta_1 * x_i \\\\\nwhere \\\\\nx_i= \\left\\{\\begin{matrix}\n1\\ when\\ x\\ \\in\\ Group 1\\\\\n0\\ when\\ x\\ \\in\\ Group2\n\\end{matrix}\\right.\n\\\\\nH_0 : \\beta_1 = 0\n\\]\nThe t.test computes a statistic as follows:\n\\[\nt \\ \\ = \\mod(\\bar{x_1}\\ - \\bar{x_2}) / std.error(\\bar{x_1}\\ - \\bar{x_2}) \\\\\n= \\mod(\\bar{x_1} - \\bar{x_2})\\ / \\sqrt{s_x^2 /n_x + s_y^2/n_y}  \\\\\nand\\\\\ndf = n_1 + n_2 - 2\\ \\ \\ \\ ( degrees\\ of\\ freedom)\n\\]\nThe t-test uses an approximation to the sampling distribution of the difference in sample means based on the Central Limit Theorem, which ensures that for sufficiently large samples, the sampling distribution will be very close to Normal. The mean of the sampling distribution will be the difference in (underlying) population means, and the variance of the sampling distribution will be the standard error of the difference in sample means.\nFor the t-statistic, note that the numerator of the formula is the difference between means. The denominator is a measurement of experimental error in the two groups combined. The wider the difference between means, the more confident you are in the data. The more experimental error you have, the less confident you are in the data. Thus the higher the value of t, the greater the confidence that there is a difference.\nThe t-statistic has a t-distribution. It is compared to a critical value of t, for a given probability value ( 0.05 usually). If the calculated t exceeds the critical value, we can assert that the NULL Hypothesis can be rejected and there could be a significant difference in means. \n\n7.3.2 Code\n\nShow the Code# Independent t-test\nt5 <- t.test(y2, y, var.equal = TRUE)\nt5 %>% tidy() \n\n\n\n  \n\n\nShow the Code# Welch test when variances are not equal\nt6 <- t.test(y2,y, var.equal = FALSE)\nt6 %>% tidy()\n\n\n\n  \n\n\nShow the Code# Linear Model with Dummy variable\n# lm(value ~ 1 + group, data = mydata_long_y) # also works\nlm6 <- lm(value ~ 1 + I(group == \"y2\"), data = mydata_long_y)\nlm6 %>% tidy()\n\n\n\n  \n\n\n\nWe get the same estimates for means of y and y2 ( 0.3 and 0.5 respectively).\n\n7.3.3 Example"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#welchs-t-test",
    "href": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#welchs-t-test",
    "title": "Linear Models: Comparing Proportions",
    "section": "\n7.4 Welch‚Äôs t-test",
    "text": "7.4 Welch‚Äôs t-test\n\n7.4.1 Model\nWelch‚Äôs test we have already explored as a variant of the t.test for two means, with variances unequal.\nWelch‚Äôs test is also stated as a Generalized Linear Model using, not lm but the gls command from the nlme package. This is explained on StackExchange but we need not digress now.\nWhen the variances are unequal, there is a difference in the t-statistic that is computed only when the group **sizes* are different ( Denominator of t-statistic ).\nFrom StackExchange:\\[\nt_w =\\ \\frac {\\bar{x_1}-\\bar{x_2}}{\\sqrt{{s_1^2/n_1}{s_2^2/n_2}}}\n\\\\\\\\\n=\\ \\frac{\\bar{x_1}-\\bar{x_2}} {{\\sqrt{\\frac{s_1^2 + s_2^2}{n}}}}\n\\\\\\\\\n=\\ \\frac{\\bar{x_1}-\\bar{x_2}} {{\\sqrt{\\frac{s_1^2 + s_2^2} {2} * (\\frac{2}{n})}}}\n\\\\\\\\\n=\\ t_s\n\\]\nSo under these conditions the t-statistic for the Welch t-test is the same as that for the standard t-test.\n\n7.4.2 Code\n\nShow the Codet7 <- t.test(y, y2, var.equal = FALSE)\n\n\n\n7.4.3 Example"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#mann-whitney-u-test",
    "href": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#mann-whitney-u-test",
    "title": "Linear Models: Comparing Proportions",
    "section": "\n7.5 Mann-Whitney U Test",
    "text": "7.5 Mann-Whitney U Test\n(Wilcoxon Independent Sample test)\nAs before, when sample groups are not normally distributed, and when variances are different, and they are outliers, a nonparametric rank-sum test is preferred. This is the same as the Wilcoxon Test for independent variables, and is called the Mann-Whitney Test.\n\n7.5.1 Model\nAs before:\n\\[\nrank(y_i) = \\beta_0 + \\beta_1 * rank(x_i) \\\\\nwhere \\\\\nx_i= \\left\\{\\begin{matrix}\n1\\ when\\ x\\ \\in\\ Group 1\\\\\n0\\ when\\ x\\ \\in\\ Group2\n\\end{matrix}\\right.\n\\\\\nH_0 : \\beta_1 = 0\n\\]\n\n7.5.2 Code\n\nShow the Code# As Wilcoxon Test\nw5 <- wilcox.test(y2,y, paired = FALSE)\nw5 %>% tidy()\n\n\n\n  \n\n\nShow the Code# As Linear model\nlm5 <- lm(rank(value) ~ 1 + I(group == \"y2\"), \n          data = mydata_long_y)\nlm5 %>% tidy()\n\n\n\n  \n\n\n\nNot clear how to explain this. Need to dig more into Mann-Whitney.\n\n7.5.3 Example"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#model-11",
    "href": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#model-11",
    "title": "Linear Models: Comparing Proportions",
    "section": "\n8.1 Model",
    "text": "8.1 Model\nANOVAs are linear models with (only) categorical predictors so they simply extend everything we did above, relying heavily on dummy coding."
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#one-way-anova",
    "href": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#one-way-anova",
    "title": "Linear Models: Comparing Proportions",
    "section": "\n8.2 One-way ANOVA",
    "text": "8.2 One-way ANOVA\n\n8.2.1 Model\nOne mean for each group predicts y.\n\\[\ny=\\beta_0 + \\beta_1*x_1 + \\beta_2*x_2...+\\beta_n*x_n\\\\\nH0:y=Œ≤0\n\\]\nwhere \\(x_i\\) are indicators (x=0 or x=1) where at most one \\(x_i=1\\) while all others are \\(x_i=0\\).\nNotice how this is just ‚Äúmore of the same‚Äù of what we already did in other models above. When there are only two groups, this model is \\(y=Œ≤0+Œ≤1‚àóx\\), i.e.¬†the independent t-test. If there is only one group, it is \\(y=Œ≤0\\), i.e.¬†the one-sample t-test. This makes one-way ANOVA a multiple regression model.\nThis is easy to see in the visualization below - just cover up a few groups and see that it matches the other visualizations above. Let‚Äôs visualize this using toy data:\n\nShow the CodeN = 15\nD_anova1 = data.frame(\n  y = c(\n    rnorm_fixed(N, 0.5, 0.3),\n    rnorm_fixed(N, 0, 0.3),\n    rnorm_fixed(N, 1, 0.3),\n    rnorm_fixed(N, 0.8, 0.3)\n  ),\n  x = rep(0:3, each = 15)\n)\nymeans = aggregate(y~x, D_anova1, mean)$y\nP_anova1 = ggplot(D_anova1, aes(x=x, y=y)) + \n  stat_summary(fun.y=mean, geom = \"errorbar\", aes(ymax = ..y.., ymin = ..y.., color='intercepts'), lwd=2) + \n  \n  geom_segment(x = -10, xend = 100, \n               y = 0.5, yend = 0.5, \n               lwd = 2, aes(color = 'beta_0')) +\n  geom_segment(x = 0, xend = 1, \n               y = ymeans[1], yend = ymeans[2], \n               lwd = 2, aes(color = 'betas')) +\n  geom_segment(x = 1, xend = 2, \n               y = ymeans[1], yend = ymeans[3], \n               lwd = 2, aes(color = 'betas')) +\n  geom_segment(x = 2, xend = 3, \n               y = ymeans[1], yend = ymeans[4], \n               lwd = 2, aes(color = 'betas')) +\n  \n  scale_color_manual(name = NULL, \n                     values = c(\"blue\", \"red\", \"darkblue\"),\n                     labels=c(bquote(beta[0]*\" (group 1 mean)\"),\n                              bquote(beta[1]*\", \"*beta[2]*\", \n                                     etc. (slopes/differences to \"*beta[0]*\")\"),\n      bquote(beta[0]*\"+\"*beta[1]*\", \"*beta[0]*\"+\"*beta[2]*\", etc. (group 2, 3, ... means)\")\n    )\n  )\n\nWarning: The `fun.y` argument of `stat_summary()` is deprecated as of ggplot2 3.3.0.\n‚Ñπ Please use the `fun` argument instead.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\nShow the CodeP_anova1\n\nWarning: The dot-dot notation (`..y..`) was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `after_stat(y)` instead.\n\n\n\n\n\n\n\n\n\n8.2.2 Code\n\n8.2.3 Example"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#two-way-anova",
    "href": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#two-way-anova",
    "title": "Linear Models: Comparing Proportions",
    "section": "\n8.3 Two-way ANOVA",
    "text": "8.3 Two-way ANOVA\n\n8.3.1 Model\n\n8.3.2 Code\n\n8.3.3 Example"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#ancova",
    "href": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#ancova",
    "title": "Linear Models: Comparing Proportions",
    "section": "\n8.4 ANCOVA",
    "text": "8.4 ANCOVA\n\n8.4.1 Model\n\n8.4.2 Code\n\n8.4.3 Example"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#model-15",
    "href": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#model-15",
    "title": "Linear Models: Comparing Proportions",
    "section": "\n9.1 Model",
    "text": "9.1 Model"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#discrete-variables",
    "href": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#discrete-variables",
    "title": "Linear Models: Comparing Proportions",
    "section": "\n9.2 Discrete Variables",
    "text": "9.2 Discrete Variables\n\n9.2.1 Model\n\n9.2.2 Code\n\n9.2.3 Example"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#continuous-variables",
    "href": "content/courses/Basics-of-Modeling/Modules/30-LM-Comparing-Proportions/index.html#continuous-variables",
    "title": "Linear Models: Comparing Proportions",
    "section": "\n9.3 Continuous Variables",
    "text": "9.3 Continuous Variables\n\n9.3.1 Model\n\n9.3.2 Code\n\n9.3.3 Example"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/40-LM-Comparing-Means/index.html",
    "href": "content/courses/Basics-of-Modeling/Modules/40-LM-Comparing-Means/index.html",
    "title": "Linear Models: Comparing Means",
    "section": "",
    "text": "R Tutorial¬†¬†\n  Orange Tutorial\n\n  Radiant Tutorial¬†\n  Datasets"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/40-LM-Comparing-Means/index.html#linear-models-as-hypothesis-tests",
    "href": "content/courses/Basics-of-Modeling/Modules/40-LM-Comparing-Means/index.html#linear-models-as-hypothesis-tests",
    "title": "Linear Models: Comparing Means",
    "section": "\n3.1 Linear Models as Hypothesis Tests",
    "text": "3.1 Linear Models as Hypothesis Tests\nUsing linear models is based on the idea of Testing of Hypotheses. The Hypothesis Testing method typically defines a NULL Hypothesis where the statements read as ‚Äúthere is no relationship‚Äù between the variables at hand, explanatory and responses. The Alternative Hypothesis typically states that there is a relationship between the variables.\nAccordingly, in fitting a linear model, we follow the process as follows:\n\nMake the following hypotheses: \\[\ny = \\beta_0 + \\beta_1 *x \\\\\nNULL\\ Hypothesis\\ H_0 => x\\ and\\ y\\ are\\ unrelated.\\ (\\beta_1 = 0)\n\\] \\[\ny = \\beta_0 + \\beta_1 *x \\\\\nAlternate\\ Hypothesis\\ H_1 => x\\ and\\ y\\ are\\ linearly\\ related\\ (\\beta_1 \\ne 0)\n\\]\n\nWe ‚Äúassume‚Äù that \\(H_0\\) is true.\nWe calculate \\(\\beta_1\\).\nWe then find probability p that [\\(\\beta_1 = Estimated\\ Value\\)] when the NULL Hypothesis is assumed TRUE. This is the p-value. If that probability is p>=0.05, we say we ‚Äúcannot reject‚Äù \\(H_0\\) and there is unlikely to be significant linear relationship.\n\nHowever, if p<= 0.05 can we reject the NULL hypothesis, and say that there could be a significant linear relationship,because \\(\\beta_1 = Estimated\\ Value\\) by mere chance under \\(H_0\\) is very small."
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/40-LM-Comparing-Means/index.html#assumptions-in-linear-models",
    "href": "content/courses/Basics-of-Modeling/Modules/40-LM-Comparing-Means/index.html#assumptions-in-linear-models",
    "title": "Linear Models: Comparing Means",
    "section": "\n3.2 Assumptions in Linear Models",
    "text": "3.2 Assumptions in Linear Models\n\n\nL: \\(\\color{blue}{linear}\\) relationship\n\nI: Errors are independent (across observations)\n\nN: y is \\(\\color{red}{normally}\\) distributed at each ‚Äúlevel‚Äù of\n\n\n\n\n\nE: equal variance at all levels of x. No heteroscedasticity. \n\n\nLet us now see which standard statistical tests can be re-formulated as Linear Models."
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/40-LM-Comparing-Means/index.html#sample-values",
    "href": "content/courses/Basics-of-Modeling/Modules/40-LM-Comparing-Means/index.html#sample-values",
    "title": "Linear Models: Comparing Means",
    "section": "\n4.1 Sample Values",
    "text": "4.1 Sample Values\nMost examples in this exposition are based on three ‚Äúimaginary‚Äù samples, \\(x, y, y2\\). Each is normally distributed and made up of 50 observations. The means are (\\(mu = c(0,0.3,0.5)\\)), and the sds (\\(sd = c(1,2,1.5)\\)) are\n\n\n\n\n  \n\n\n\n\n\n\nLet us look at our toy data in three ways:"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/40-LM-Comparing-Means/index.html#signed-rank-values",
    "href": "content/courses/Basics-of-Modeling/Modules/40-LM-Comparing-Means/index.html#signed-rank-values",
    "title": "Linear Models: Comparing Means",
    "section": "\n4.2 ‚ÄúSigned Rank‚Äù Values",
    "text": "4.2 ‚ÄúSigned Rank‚Äù Values\nMost statistical tests use the actual values of the data variables. However, in some non-parametric statistical tests, the data are used in rank-transformed sense/order. In some cases the signed-rank of the data values is used instead of the data itself.\nSigned Rank is calculated as follows:\n1. Take the absolute value of each observation in a sample\n2. Place the ranks in order of (absolute magnitude). The smallest number has rank = 1 and so on.\n3. Give each of the ranks the sign of the original observation ( + or - )"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/40-LM-Comparing-Means/index.html#plotting-original-and-signed-rank-data",
    "href": "content/courses/Basics-of-Modeling/Modules/40-LM-Comparing-Means/index.html#plotting-original-and-signed-rank-data",
    "title": "Linear Models: Comparing Means",
    "section": "\n4.3 Plotting Original and Signed Rank Data",
    "text": "4.3 Plotting Original and Signed Rank Data\nA quick plot:"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/40-LM-Comparing-Means/index.html#how-does-sign-rank-data-work",
    "href": "content/courses/Basics-of-Modeling/Modules/40-LM-Comparing-Means/index.html#how-does-sign-rank-data-work",
    "title": "Linear Models: Comparing Means",
    "section": "\n4.4 How does Sign-Rank data work?",
    "text": "4.4 How does Sign-Rank data work?\nTBD: need to add some explanation here."
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/40-LM-Comparing-Means/index.html#pearson-correlation",
    "href": "content/courses/Basics-of-Modeling/Modules/40-LM-Comparing-Means/index.html#pearson-correlation",
    "title": "Linear Models: Comparing Means",
    "section": "\n5.1 Pearson Correlation",
    "text": "5.1 Pearson Correlation\n\n5.1.1 Model\nThe model for Pearson Correlation tests is exactly the Linear Model:\n\\[\ny = \\beta_0 + \\beta_1 * x\n\\\\\nH_0: \\beta_1 = 0\n\\]\nSee the Code section for further insights into the relationship between the Correlation Score and the Slope of the Linear Model.\n\n5.1.2 Code\n\n\n# A tibble: 1 √ó 2\n  estimate p.value\n     <dbl>   <dbl>\n1   -0.232   0.105\n\n\n# A tibble: 2 √ó 2\n  estimate p.value\n     <dbl>   <dbl>\n1    0.3     0.286\n2   -0.464   0.105\n\n\n# A tibble: 2 √ó 2\n   estimate p.value\n      <dbl>   <dbl>\n1 -9.06e-17   1    \n2 -2.32e- 1   0.105\n\n\n# A tibble: 5 √ó 2\n   estimate p.value\n      <dbl>   <dbl>\n1 -2.32e- 1   0.105\n2  3   e- 1   0.286\n3 -4.64e- 1   0.105\n4 -9.06e-17   1    \n5 -2.32e- 1   0.105\n\n\nNotes: 1. The p-value for Pearson Correlation and that for the slope in the linear model is the same ( 0.1053 ). Which means we cannot reject the NULL hypothesis of ‚Äúno relationship‚Äù.\n\nHere is the relationship between the slope and correlation:\n\n\\[\nSlope\\ \\beta_1 = \\frac{sd_y}{sd_x} * r\n\\]\nWhen both x and y have the same standard deviation, the slope and correlation are the same. Here, since x has twice the sd of y, the ratio of slope = -0.4635533 to r = -0.2317767 is 0.5. Hence a linear model using scale() for both variables will show slope = r.\nSlope_Scaled: -0.2317767 = Correlation: -0.2317767\n\n5.1.3 Example\nWe choose to look at the gpa_study_hours dataset. It has two numeric columns gpa and study_hours:\n\n\nRows: 193\nColumns: 2\n$ gpa         <dbl> 4.000, 3.800, 3.930, 3.400, 3.200, 3.520, 3.680, 3.400, 3.‚Ä¶\n$ study_hours <dbl> 10, 25, 45, 10, 4, 10, 24, 40, 10, 10, 30, 7, 15, 60, 10, ‚Ä¶\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nHmm‚Ä¶not normally distributed, and the relationship is also not linear, and there is some evidence of heterscedasticity, so Pearson correlation would not be the best idea here.\n\n\n\nCall:\nlm(formula = gpa ~ study_hours, data = gpa_study_hours)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.95130 -0.19456  0.03879  0.21708  0.73872 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 3.527997   0.037424  94.272   <2e-16 ***\nstudy_hours 0.003328   0.001794   1.855   0.0652 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2837 on 191 degrees of freedom\nMultiple R-squared:  0.01769,   Adjusted R-squared:  0.01255 \nF-statistic:  3.44 on 1 and 191 DF,  p-value: 0.06517\n\n\n\n    Pearson's product-moment correlation\n\ndata:  gpa and study_hours\nt = 1.8548, df = 191, p-value = 0.06517\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.008383868  0.269196552\nsample estimates:\n      cor \n0.1330138 \n\n\n\n\n  \n\n\n\nThe correlation estimate is \\(0.133\\); the p-value is 0.065 and the confidence interval includes 0. Hence we fail to reject the NULL hypothesis that study_hours and gpa have no relationship.\nWe can use a later package ggstaplot to plot this:\n\n\nRegistered S3 method overwritten by 'ggside':\n  method from   \n  +.gg   ggplot2\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/40-LM-Comparing-Means/index.html#spearman-correlation",
    "href": "content/courses/Basics-of-Modeling/Modules/40-LM-Comparing-Means/index.html#spearman-correlation",
    "title": "Linear Models: Comparing Means",
    "section": "\n5.2 Spearman Correlation",
    "text": "5.2 Spearman Correlation\n\n5.2.1 Model\nIn some cases the LINE assumptions may not hold. Nonlinear relationships, non-normally distributed data ( with large outliers ) and working with ordinal rather than continuous data: these situations necessitate the use of Spearman‚Äôs ranked correlation scores. (Ranked, not sign-ranked.)\n\\[\nrank(y) = \\beta_0 + \\beta_1 * rank(x) \\\\\nH_0: \\beta_1 = 0\n\\]\nSpearman correlation = Pearson correlation using the rank of the data observations. Let‚Äôs check how this holds for a our x and y data:\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nSlopes are almost identical, ~ 0.25.\n\n5.2.2 Code\n\n\n# A tibble: 4 √ó 2\n  estimate  p.value\n     <dbl>    <dbl>\n1   -0.227 1.13e- 1\n2   -0.227 1.14e- 1\n3   31.3   9.11e-10\n4   -0.227 1.14e- 1\n\n\nNotes:\n\nWhen ranks are used, the slope of the linear model (\\(\\beta_1\\)) has the same value as the correlation coefficient ( \\(\\rho\\) ).\nNote that the slope from the linear model now has an intuitive interpretation: the number of ranks y changes for each change in rank of x. ( Ranks are ‚Äúindependent‚Äù of sd )\n\n5.2.3 Example\nWe examine the cars93 data, where the numeric variables of interest are weight and price.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nLet us try a Spearman Correlation score for these variables, since the data are not linearly related and the variance of price also is not constant over weight\n\n\nWarning in cor.test.default(x, y, ...): Cannot compute exact p-value with ties\n\n\n\n\n  \n\n\n\n\nCall:\nlm(formula = rank(price) ~ rank(weight), data = cars93)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-20.0676  -3.0135   0.7815   3.6926  20.4099 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   3.22074    2.05894   1.564    0.124    \nrank(weight)  0.88288    0.06514  13.554   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.46 on 52 degrees of freedom\nMultiple R-squared:  0.7794,    Adjusted R-squared:  0.7751 \nF-statistic: 183.7 on 1 and 52 DF,  p-value: < 2.2e-16\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nWe see that using ranks of the price variable, we obtain a Spearman‚Äôs \\(\\rho = 0.882\\) with a p-value that is very small. Hence we are able to reject the NULL hypothesis and state that there is a relationship between these two variables. The linear relationship is evaluated as a correlation of 0.882."
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/40-LM-Comparing-Means/index.html#conclusion",
    "href": "content/courses/Basics-of-Modeling/Modules/40-LM-Comparing-Means/index.html#conclusion",
    "title": "Linear Models: Comparing Means",
    "section": "\n5.3 Conclusion",
    "text": "5.3 Conclusion\nHopefully, interpreting Statistical Tests in terms of the Linear Model has benefits of improved intuitive understanding."
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/40-LM-Comparing-Means/index.html#iconify-orange-circle-a-workflow-in-orange",
    "href": "content/courses/Basics-of-Modeling/Modules/40-LM-Comparing-Means/index.html#iconify-orange-circle-a-workflow-in-orange",
    "title": "Linear Models: Comparing Means",
    "section": "\n5.4  A Workflow in Orange",
    "text": "5.4  A Workflow in Orange\n  Orange Tutorial\nDownload the Orange Workflow file by clicking the icon above, save the file, and open it in Orange."
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/40-LM-Comparing-Means/index.html#fa-person-rays-a-workflow-in-radiant",
    "href": "content/courses/Basics-of-Modeling/Modules/40-LM-Comparing-Means/index.html#fa-person-rays-a-workflow-in-radiant",
    "title": "Linear Models: Comparing Means",
    "section": "\n5.5  A Workflow in Radiant",
    "text": "5.5  A Workflow in Radiant\n  Radiant Tutorial¬†¬†\nDownload the Radiant Workflow statefile by clicking the icon above, an upload/open it in Radiant. You need to start radiant from the Add-Inmenu in RStudio."
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/40-LM-Comparing-Means/index.html#fa-brands-r-project-a-workflow-in-r",
    "href": "content/courses/Basics-of-Modeling/Modules/40-LM-Comparing-Means/index.html#fa-brands-r-project-a-workflow-in-r",
    "title": "Linear Models: Comparing Means",
    "section": "\n5.6  A Workflow in R",
    "text": "5.6  A Workflow in R\n R Tutorial¬†¬†\nDownload the Quarto tutorial file by clicking the icon above and open it in RStudio or https://rstudio.cloud ."
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/50-Sampling-and-Probability/files/Chap04SamplingDist_Exer_d.html",
    "href": "content/courses/Basics-of-Modeling/Modules/50-Sampling-and-Probability/files/Chap04SamplingDist_Exer_d.html",
    "title": "Chapter 4 Sampling Distribution-Exercises",
    "section": "",
    "text": "Exercise 4\n\npop <- c(3, 5, 6, 6, 8, 11, 13, 15, 19, 20)\nN <- 10^4\nXbar <- numeric(N)\n\nfor (i in 1:N)\n{\n samp <- sample(pop, 4, replace = TRUE)\n Xbar[i] <- mean(samp)\n}\n\nggplot() + geom_histogram(aes(Xbar), bins = 10)\n\n\n\nmean(Xbar < 11)\n\n[1] 0.5556\n\n\nExercise 6\n\nRecidivism <- read.csv(\"../../../../../materials/data/resampling/Recidivism.csv\")\n N <- 10^4\n phat <- numeric(N)\n n <- 25\n\n for (i in 1:N)\n {\n  samp <- sample(Recidivism$Recid, n)\n  phat[i] <- mean(samp == \"Yes\")\n }\n\n#c)  change n <- 250\n\nExercise 19 X1,X2,..X10 ~ N(20, 8^2), Y1, Y2,..Y15 ~ N(16,7^2) W = X + Y\n\n W <- numeric(1000)\n set.seed(0)\n    for (i in 1:1000)\n    {\n       x <- rnorm(10, 20, 8)  #draw 10 from N(20, 8^2)\n       y <- rnorm(15, 16, 7)  #draw 15 from N(16, 7^2)\n       W[i] <- mean(x) + mean(y) #save sum of means\n    }\n\n ggplot() + geom_histogram(aes(W), bins = 12)\n\n\n\n mean(W < 40)\n\n[1] 0.9\n\n\nExercise 22\n\nX <- runif(1000, 40, 60)\nY <- runif(1000, 45, 80)\n\ntotal <- X + Y\n\nggplot() + geom_histogram(aes(total), bins = 12)\n\n\n\n\nExercise 33 Finite population simulation\n\nN <- 400 # population size\nn <- 5 # sample size\n\nfinpop <- rexp(N, 1/10) # Create a finite pop. of size N=400 from\n# Exp(1/10)\nggplot() + geom_histogram(aes(finpop), bins = 12) # distribution of your finite pop.\n\n\n\nmean(finpop) # mean (mu) of your pop.\n\n[1] 10.52696\n\nsd(finpop) # stdev (sigma) of your pop.\n\n[1] 10.98211\n\nsd(finpop)/sqrt(n) # theoretical standard error of sampling\n\n[1] 4.911349\n\n# dist. of mean(x), with replacement\nsd(finpop)/sqrt(n) * sqrt((N-n)/(N-1)) # without replacement\n\n[1] 4.886669\n\nXbar <- numeric(1000)\nfor (i in 1:1000)\n{\nx <- sample(finpop, n) # Random sample of size n (w/o replacement)\nXbar[i] <- mean(x) # Find mean of sample, store in my.means\n}\n\nggplot() + geom_histogram(aes(Xbar), bins = 12)\n\n\n\ndf <- data.frame(Xbar)\nggplot(df, aes(sample=Xbar)) + stat_qq() + stat_qq_line()\n\n\n\nmean(Xbar)\n\n[1] 10.75019\n\nsd(Xbar) # estimated standard error of sampling\n\n[1] 5.102043\n\n             # distribution\n\nExercise 34\n\nW <- numeric(1000)\nfor (i in 1:1000)\n{\nx <- rnorm(20, 25, 7)\nW[i] <- var(x)\n}\nmean(W)\n\n[1] 49.46759\n\nvar(W)\n\n[1] 247.7439\n\nggplot() + geom_histogram(aes(W), bins = 10)\n\n\n\ndf <- data.frame(W)\nggplot(df, aes(sample = W)) + stat_qq() + stat_qq_line()"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/50-Sampling-and-Probability/files/Chap4-sampling.html",
    "href": "content/courses/Basics-of-Modeling/Modules/50-Sampling-and-Probability/files/Chap4-sampling.html",
    "title": "Chapter 4 Sampling Distributions",
    "section": "",
    "text": "###Example 4.2: Sampling distribution from Exp(1/15)\n\nXbar <- numeric(1000)\n#set.seed(300)\nfor (i in 1:1000)\n{\n  x <- rexp(100, rate = 1/15)\n  Xbar[i] <- mean(x)\n}\n\nggplot() + geom_histogram(aes(Xbar), bins = 15) + xlab(\"means\")\n\n\n\ndf <- data.frame(Xbar)\nggplot(df, aes(sample=Xbar)) + stat_qq() + stat_qq_line()\n\n\n\nmean(Xbar)\n\n[1] 15.07407\n\nsd(Xbar)\n\n[1] 1.508802\n\n\n###Example 4.3: Sampling Dist from Unif[0,1]\n\nmaxY <- numeric(1000)\n#set.seed(100)\nfor (i in 1:1000)\n {\n   y <- runif(12)        #draw random sample of size 12\n   maxY[i] <- max(y)     #find max, save in position i\n }\n\nggplot() + geom_histogram(aes(maxY), binwidth=.05, center=.975) + xlab(\"maximums\") \n\n\n\n\nTo create a histogram with a density curve imposed, we will need to create a data frame that holds the ‚ÄòmaxY‚Äô variable. We also create a function for the density curve \\(f(x)=12x^{11}\\).\n\ndf <- data.frame(maxY)\nmyfun <- function(x){12*x^{11}}\n\nggplot(df) + geom_histogram(aes(maxY, y = stat(density)), binwidth=.05, center=.975) +xlab(\"maximums\") + stat_function(fun = myfun)\n\nWarning: `stat(density)` was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `after_stat(density)` instead.\n\n\n\n\n\n###Example 4.6 Sum of Poisson random variables\n\nX <- rpois(10^4, 5)   #Draw 10^4 values from Pois(5)\nY <- rpois(10^4, 12)   #Draw 10^4 values from Pois(12)\nW <- X + Y\n\ndf1 <- data.frame(W)\ndf2 <- data.frame(x=2:35, y = dpois(2:35,17))\nggplot(df1, aes(W)) + geom_histogram(aes(y=stat(density)), bins=12) + geom_line(data=df2, aes(x=x, y=y), colour = \"red\")\n\n\n\nmean(W)\n\n[1] 17.0557\n\nvar(W)\n\n[1] 17.11891\n\n\n###Example 4.7 Sampling distribution simulation Sample of size 30 from gamma r=5, lambda=2\n\n#set.seed(10)\nXbar <- numeric(1000)\nfor (i in 1:1000)\n  {\n    x <- rgamma(30, shape = 5, rate = 2)\n    Xbar[i] <- mean(x)\n  }\n\nggplot() + geom_histogram(aes(Xbar), bins=15) + labs(title = \"Distribution of means\")\n\n\n\nggplot() + stat_qq(aes(sample = Xbar)) \n\n\n\n#If you want a line, then\ndf <- data.frame(Xbar)\nggplot(df, aes(sample = Xbar)) + stat_qq() + stat_qq_line()\n\n\n\nmean(Xbar)\n\n[1] 2.497688\n\nsd(Xbar)\n\n[1] 0.2116128\n\nsum(Xbar > 3)/1000\n\n[1] 0.013\n\n#alternatively\nmean(Xbar > 3)\n\n[1] 0.013\n\n\n###Example 4.11 R Note\n\ndbinom(25, 120, .3)\n\n[1] 0.006807598\n\npbinom(25, 120, .3)\n\n[1] 0.0159137"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/50-Sampling-and-Probability/files/sampling.html",
    "href": "content/courses/Basics-of-Modeling/Modules/50-Sampling-and-Probability/files/sampling.html",
    "title": "Sampling",
    "section": "",
    "text": "Continuing to treat the NHANES dataset as a population, We will try to replicate the process of sampling and CLT for another variable in the NHANES variable, AlcoholYear."
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/50-Sampling-and-Probability/index.html#what-is-a-population",
    "href": "content/courses/Basics-of-Modeling/Modules/50-Sampling-and-Probability/index.html#what-is-a-population",
    "title": "Samples, Populations, Statistics and Inference",
    "section": "\n2 What is a Population?",
    "text": "2 What is a Population?\nA population is a collection of individuals or observations we are interested in. This is also commonly denoted as a study population. We mathematically denote the population‚Äôs size using upper-case N.\nA population parameter is some numerical summary about the population that is unknown but you wish you knew. For example, when this quantity is a mean like the average height of all Bangaloreans, the population parameter of interest is the population mean.\nA census is an exhaustive enumeration or counting of all N individuals in the population. We do this in order to compute the population parameter‚Äôs value exactly. Of note is that as the number N of individuals in our population increases, conducting a census gets more expensive (in terms of time, energy, and money)."
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/50-Sampling-and-Probability/index.html#what-is-a-sample",
    "href": "content/courses/Basics-of-Modeling/Modules/50-Sampling-and-Probability/index.html#what-is-a-sample",
    "title": "Samples, Populations, Statistics and Inference",
    "section": "\n3 What is a Sample?",
    "text": "3 What is a Sample?\nSampling is the act of collecting a sample from the population, which we generally do when we can‚Äôt perform a census. We mathematically denote the sample size using lower case n, as opposed to upper case N which denotes the population‚Äôs size. Typically the sample size n is much smaller than the population size N. Thus sampling is a much cheaper alternative than performing a census.\nA sample statistic, also known as a point estimate, is a summary statistic like a mean or standard deviation that is computed from a sample."
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/50-Sampling-and-Probability/index.html#why-do-we-sample",
    "href": "content/courses/Basics-of-Modeling/Modules/50-Sampling-and-Probability/index.html#why-do-we-sample",
    "title": "Samples, Populations, Statistics and Inference",
    "section": "\n4 Why do we sample?",
    "text": "4 Why do we sample?\nBecause we cannot conduct a census ( not always ) ‚Äî and sometimes we won‚Äôt even know how big the population is ‚Äî we take samples. And we still want to do useful work for/with the population, after estimating its parameters, an act of generalizing from sample to population. So the question is, can we estimate useful parameters of the population, using just samples? Can point estimates serve as useful guides to population parameters?\nThis act of generalizing from sample to population is at the heart of statistical inference.\nNOTE: there is an alliterative mnemonic here: Samples have Statistics; Populations have Parameters."
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/50-Sampling-and-Probability/index.html#sampling",
    "href": "content/courses/Basics-of-Modeling/Modules/50-Sampling-and-Probability/index.html#sampling",
    "title": "Samples, Populations, Statistics and Inference",
    "section": "\n5 Sampling",
    "text": "5 Sampling\nWe will first execute some samples from a known dataset. We load up the NHANES dataset and inspect it.\n\nShow the Codedata(\"NHANES\")\nmosaic::inspect(NHANES)\n\n\ncategorical variables:  \n               name  class levels     n missing\n1          SurveyYr factor      2 10000       0\n2            Gender factor      2 10000       0\n3         AgeDecade factor      8  9667     333\n4             Race1 factor      5 10000       0\n5             Race3 factor      6  5000    5000\n6         Education factor      5  7221    2779\n7     MaritalStatus factor      6  7231    2769\n8          HHIncome factor     12  9189     811\n9           HomeOwn factor      3  9937      63\n10             Work factor      3  7771    2229\n11 BMICatUnder20yrs factor      4  1274    8726\n12          BMI_WHO factor      4  9603     397\n13         Diabetes factor      2  9858     142\n14        HealthGen factor      5  7539    2461\n15   LittleInterest factor      3  6667    3333\n16        Depressed factor      3  6673    3327\n17     SleepTrouble factor      2  7772    2228\n18       PhysActive factor      2  8326    1674\n19         TVHrsDay factor      7  4859    5141\n20       CompHrsDay factor      7  4863    5137\n21  Alcohol12PlusYr factor      2  6580    3420\n22         SmokeNow factor      2  3211    6789\n23         Smoke100 factor      2  7235    2765\n24        Smoke100n factor      2  7235    2765\n25        Marijuana factor      2  4941    5059\n26     RegularMarij factor      2  4941    5059\n27        HardDrugs factor      2  5765    4235\n28          SexEver factor      2  5767    4233\n29          SameSex factor      2  5768    4232\n30   SexOrientation factor      3  4842    5158\n31      PregnantNow factor      3  1696    8304\n                                    distribution\n1  2009_10 (50%), 2011_12 (50%)                 \n2  female (50.2%), male (49.8%)                 \n3   40-49 (14.5%),  0-9 (14.4%) ...             \n4  White (63.7%), Black (12%) ...               \n5  White (62.7%), Black (11.8%) ...             \n6  Some College (31.4%) ...                     \n7  Married (54.6%), NeverMarried (19.1%) ...    \n8  more 99999 (24.2%) ...                       \n9  Own (64.7%), Rent (33.1%) ...                \n10 Working (59.4%), NotWorking (36.6%) ...      \n11 NormWeight (63.2%), Obese (17.3%) ...        \n12 18.5_to_24.9 (30.3%) ...                     \n13 No (92.3%), Yes (7.7%)                       \n14 Good (39.2%), Vgood (33.3%) ...              \n15 None (76.5%), Several (16.9%) ...            \n16 None (78.6%), Several (15.1%) ...            \n17 No (74.6%), Yes (25.4%)                      \n18 Yes (55.8%), No (44.2%)                      \n19 2_hr (26.2%), 1_hr (18.2%) ...               \n20 0_to_1_hr (29%), 0_hrs (22.1%) ...           \n21 Yes (79.2%), No (20.8%)                      \n22 No (54.3%), Yes (45.7%)                      \n23 No (55.6%), Yes (44.4%)                      \n24 Non-Smoker (55.6%), Smoker (44.4%)           \n25 Yes (58.5%), No (41.5%)                      \n26 No (72.4%), Yes (27.6%)                      \n27 No (81.5%), Yes (18.5%)                      \n28 Yes (96.1%), No (3.9%)                       \n29 No (92.8%), Yes (7.2%)                       \n30 Heterosexual (95.8%), Bisexual (2.5%) ...    \n31 No (92.7%), Yes (4.2%) ...                   \n\nquantitative variables:  \n              name   class      min        Q1    median        Q3        max\n1               ID integer 51624.00 56904.500 62159.500 67039.000  71915.000\n2              Age integer     0.00    17.000    36.000    54.000     80.000\n3        AgeMonths integer     0.00   199.000   418.000   624.000    959.000\n4      HHIncomeMid integer  2500.00 30000.000 50000.000 87500.000 100000.000\n5          Poverty numeric     0.00     1.240     2.700     4.710      5.000\n6        HomeRooms integer     1.00     5.000     6.000     8.000     13.000\n7           Weight numeric     2.80    56.100    72.700    88.900    230.700\n8           Length numeric    47.10    75.700    87.000    96.100    112.200\n9         HeadCirc numeric    34.20    39.575    41.450    42.925     45.400\n10          Height numeric    83.60   156.800   166.000   174.500    200.400\n11             BMI numeric    12.88    21.580    25.980    30.890     81.250\n12           Pulse integer    40.00    64.000    72.000    82.000    136.000\n13        BPSysAve integer    76.00   106.000   116.000   127.000    226.000\n14        BPDiaAve integer     0.00    61.000    69.000    76.000    116.000\n15          BPSys1 integer    72.00   106.000   116.000   128.000    232.000\n16          BPDia1 integer     0.00    62.000    70.000    76.000    118.000\n17          BPSys2 integer    76.00   106.000   116.000   128.000    226.000\n18          BPDia2 integer     0.00    60.000    68.000    76.000    118.000\n19          BPSys3 integer    76.00   106.000   116.000   126.000    226.000\n20          BPDia3 integer     0.00    60.000    68.000    76.000    116.000\n21    Testosterone numeric     0.25    17.700    43.820   362.410   1795.600\n22      DirectChol numeric     0.39     1.090     1.290     1.580      4.030\n23         TotChol numeric     1.53     4.110     4.780     5.530     13.650\n24       UrineVol1 integer     0.00    50.000    94.000   164.000    510.000\n25      UrineFlow1 numeric     0.00     0.403     0.699     1.221     17.167\n26       UrineVol2 integer     0.00    52.000    95.000   171.750    409.000\n27      UrineFlow2 numeric     0.00     0.475     0.760     1.513     13.692\n28     DiabetesAge integer     1.00    40.000    50.000    58.000     80.000\n29 DaysPhysHlthBad integer     0.00     0.000     0.000     3.000     30.000\n30 DaysMentHlthBad integer     0.00     0.000     0.000     4.000     30.000\n31    nPregnancies integer     1.00     2.000     3.000     4.000     32.000\n32         nBabies integer     0.00     2.000     2.000     3.000     12.000\n33      Age1stBaby integer    14.00    19.000    22.000    26.000     39.000\n34   SleepHrsNight integer     2.00     6.000     7.000     8.000     12.000\n35  PhysActiveDays integer     1.00     2.000     3.000     5.000      7.000\n36   TVHrsDayChild integer     0.00     1.000     2.000     3.000      6.000\n37 CompHrsDayChild integer     0.00     0.000     1.000     6.000      6.000\n38      AlcoholDay integer     1.00     1.000     2.000     3.000     82.000\n39     AlcoholYear integer     0.00     3.000    24.000   104.000    364.000\n40        SmokeAge integer     6.00    15.000    17.000    19.000     72.000\n41   AgeFirstMarij integer     1.00    15.000    16.000    19.000     48.000\n42     AgeRegMarij integer     5.00    15.000    17.000    19.000     52.000\n43          SexAge integer     9.00    15.000    17.000    19.000     50.000\n44 SexNumPartnLife integer     0.00     2.000     5.000    12.000   2000.000\n45  SexNumPartYear integer     0.00     1.000     1.000     1.000     69.000\n           mean           sd     n missing\n1  6.194464e+04 5.871167e+03 10000       0\n2  3.674210e+01 2.239757e+01 10000       0\n3  4.201239e+02 2.590431e+02  4962    5038\n4  5.720617e+04 3.302028e+04  9189     811\n5  2.801844e+00 1.677909e+00  9274     726\n6  6.248918e+00 2.277538e+00  9931      69\n7  7.098180e+01 2.912536e+01  9922      78\n8  8.501602e+01 1.370503e+01   543    9457\n9  4.118068e+01 2.311483e+00    88    9912\n10 1.618778e+02 2.018657e+01  9647     353\n11 2.666014e+01 7.376579e+00  9634     366\n12 7.355973e+01 1.215542e+01  8563    1437\n13 1.181550e+02 1.724817e+01  8551    1449\n14 6.748006e+01 1.435480e+01  8551    1449\n15 1.190902e+02 1.749636e+01  8237    1763\n16 6.827826e+01 1.378078e+01  8237    1763\n17 1.184758e+02 1.749133e+01  8353    1647\n18 6.766455e+01 1.441978e+01  8353    1647\n19 1.179292e+02 1.717719e+01  8365    1635\n20 6.729874e+01 1.495839e+01  8365    1635\n21 1.978980e+02 2.265045e+02  4126    5874\n22 1.364865e+00 3.992581e-01  8474    1526\n23 4.879220e+00 1.075583e+00  8474    1526\n24 1.185161e+02 9.033648e+01  9013     987\n25 9.792946e-01 9.495143e-01  8397    1603\n26 1.196759e+02 9.016005e+01  1478    8522\n27 1.149372e+00 1.072948e+00  1476    8524\n28 4.842289e+01 1.568050e+01   629    9371\n29 3.334838e+00 7.400700e+00  7532    2468\n30 4.126493e+00 7.832971e+00  7534    2466\n31 3.026882e+00 1.795341e+00  2604    7396\n32 2.456954e+00 1.315227e+00  2416    7584\n33 2.264968e+01 4.772509e+00  1884    8116\n34 6.927531e+00 1.346729e+00  7755    2245\n35 3.743513e+00 1.836358e+00  4663    5337\n36 1.938744e+00 1.434431e+00   653    9347\n37 2.197550e+00 2.516667e+00   653    9347\n38 2.914123e+00 3.182672e+00  4914    5086\n39 7.510165e+01 1.030337e+02  5922    4078\n40 1.782662e+01 5.326660e+00  3080    6920\n41 1.702283e+01 3.895010e+00  2891    7109\n42 1.769107e+01 4.806103e+00  1366    8634\n43 1.742870e+01 3.716551e+00  5540    4460\n44 1.508507e+01 5.784643e+01  5725    4275\n45 1.342330e+00 2.782688e+00  4928    5072\n\n\nLet us create a NHANES dataset without duplicated IDs and only adults:\n\nShow the CodeNHANES <-\n  NHANES %>%\n  distinct(ID, .keep_all = TRUE) \n\n#create a dataset of only adults\nNHANES_adult <- \n  NHANES %>%\n  filter( \n    Age >= 18\n  ) %>%\n  drop_na(Height)\n\nglimpse(NHANES_adult)\n\nRows: 4,790\nColumns: 76\n$ ID               <int> 51624, 51630, 51647, 51654, 51656, 51657, 51666, 5166‚Ä¶\n$ SurveyYr         <fct> 2009_10, 2009_10, 2009_10, 2009_10, 2009_10, 2009_10,‚Ä¶\n$ Gender           <fct> male, female, female, male, male, male, female, male,‚Ä¶\n$ Age              <int> 34, 49, 45, 66, 58, 54, 58, 50, 33, 60, 56, 57, 54, 3‚Ä¶\n$ AgeDecade        <fct>  30-39,  40-49,  40-49,  60-69,  50-59,  50-59,  50-5‚Ä¶\n$ AgeMonths        <int> 409, 596, 541, 795, 707, 654, 700, 603, 404, 721, 677‚Ä¶\n$ Race1            <fct> White, White, White, White, White, White, Mexican, Wh‚Ä¶\n$ Race3            <fct> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ Education        <fct> High School, Some College, College Grad, Some College‚Ä¶\n$ MaritalStatus    <fct> Married, LivePartner, Married, Married, Divorced, Mar‚Ä¶\n$ HHIncome         <fct> 25000-34999, 35000-44999, 75000-99999, 25000-34999, m‚Ä¶\n$ HHIncomeMid      <int> 30000, 40000, 87500, 30000, 100000, 70000, 87500, 175‚Ä¶\n$ Poverty          <dbl> 1.36, 1.91, 5.00, 2.20, 5.00, 2.20, 2.03, 1.24, 1.27,‚Ä¶\n$ HomeRooms        <int> 6, 5, 6, 5, 10, 6, 10, 4, 11, 5, 10, 9, 3, 6, 6, 10, ‚Ä¶\n$ HomeOwn          <fct> Own, Rent, Own, Own, Rent, Rent, Rent, Rent, Own, Own‚Ä¶\n$ Work             <fct> NotWorking, NotWorking, Working, NotWorking, Working,‚Ä¶\n$ Weight           <dbl> 87.4, 86.7, 75.7, 68.0, 78.4, 74.7, 57.5, 84.1, 93.8,‚Ä¶\n$ Length           <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ HeadCirc         <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ Height           <dbl> 164.7, 168.4, 166.7, 169.5, 181.9, 169.4, 148.1, 177.‚Ä¶\n$ BMI              <dbl> 32.22, 30.57, 27.24, 23.67, 23.69, 26.03, 26.22, 26.6‚Ä¶\n$ BMICatUnder20yrs <fct> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ BMI_WHO          <fct> 30.0_plus, 30.0_plus, 25.0_to_29.9, 18.5_to_24.9, 18.‚Ä¶\n$ Pulse            <int> 70, 86, 62, 60, 62, 76, 94, 74, 96, 84, 64, 70, 64, 6‚Ä¶\n$ BPSysAve         <int> 113, 112, 118, 111, 104, 134, 127, 142, 128, 152, 95,‚Ä¶\n$ BPDiaAve         <int> 85, 75, 64, 63, 74, 85, 83, 68, 74, 100, 69, 89, 41, ‚Ä¶\n$ BPSys1           <int> 114, 118, 106, 124, 108, 136, NA, 138, 126, 154, 94, ‚Ä¶\n$ BPDia1           <int> 88, 82, 62, 64, 76, 86, NA, 66, 80, 98, 74, 82, 48, 8‚Ä¶\n$ BPSys2           <int> 114, 108, 118, 108, 104, 132, 134, 142, 128, 150, 94,‚Ä¶\n$ BPDia2           <int> 88, 74, 68, 62, 72, 88, 82, 74, 74, 98, 70, 88, 42, 8‚Ä¶\n$ BPSys3           <int> 112, 116, 118, 114, 104, 136, 120, 142, NA, 154, 96, ‚Ä¶\n$ BPDia3           <int> 82, 76, 60, 64, 76, 82, 84, 62, NA, 102, 68, 90, 40, ‚Ä¶\n$ Testosterone     <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ DirectChol       <dbl> 1.29, 1.16, 2.12, 0.67, 0.96, 1.16, 1.14, 1.06, 0.91,‚Ä¶\n$ TotChol          <dbl> 3.49, 6.70, 5.82, 4.99, 4.24, 6.41, 4.78, 5.22, 5.59,‚Ä¶\n$ UrineVol1        <int> 352, 77, 106, 113, 163, 215, 29, 64, 155, 238, 26, 13‚Ä¶\n$ UrineFlow1       <dbl> NA, 0.094, 1.116, 0.489, NA, 0.903, 0.299, 0.190, 0.5‚Ä¶\n$ UrineVol2        <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 86, NA, NA, N‚Ä¶\n$ UrineFlow2       <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 0.43, NA, NA,‚Ä¶\n$ Diabetes         <fct> No, No, No, No, No, No, No, No, No, No, No, No, No, N‚Ä¶\n$ DiabetesAge      <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ HealthGen        <fct> Good, Good, Vgood, Vgood, Vgood, Fair, NA, Good, Fair‚Ä¶\n$ DaysPhysHlthBad  <int> 0, 0, 0, 10, 0, 4, NA, 0, 3, 7, 3, 0, 0, 3, 0, 2, 0, ‚Ä¶\n$ DaysMentHlthBad  <int> 15, 10, 3, 0, 0, 0, NA, 0, 7, 0, 0, 0, 0, 4, 0, 30, 0‚Ä¶\n$ LittleInterest   <fct> Most, Several, None, None, None, None, NA, None, Seve‚Ä¶\n$ Depressed        <fct> Several, Several, None, None, None, None, NA, None, N‚Ä¶\n$ nPregnancies     <int> NA, 2, 1, NA, NA, NA, NA, NA, NA, NA, 4, 2, NA, NA, N‚Ä¶\n$ nBabies          <int> NA, 2, NA, NA, NA, NA, NA, NA, NA, NA, 3, 2, NA, NA, ‚Ä¶\n$ Age1stBaby       <int> NA, 27, NA, NA, NA, NA, NA, NA, NA, NA, 26, 32, NA, N‚Ä¶\n$ SleepHrsNight    <int> 4, 8, 8, 7, 5, 4, 5, 7, 6, 6, 7, 8, 6, 5, 6, 4, 5, 7,‚Ä¶\n$ SleepTrouble     <fct> Yes, Yes, No, No, No, Yes, No, No, No, Yes, No, No, Y‚Ä¶\n$ PhysActive       <fct> No, No, Yes, Yes, Yes, Yes, Yes, Yes, No, No, Yes, Ye‚Ä¶\n$ PhysActiveDays   <int> NA, NA, 5, 7, 5, 1, 2, 7, NA, NA, 7, 3, 3, NA, 2, NA,‚Ä¶\n$ TVHrsDay         <fct> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ CompHrsDay       <fct> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ TVHrsDayChild    <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ CompHrsDayChild  <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ Alcohol12PlusYr  <fct> Yes, Yes, Yes, Yes, Yes, Yes, NA, No, Yes, Yes, Yes, ‚Ä¶\n$ AlcoholDay       <int> NA, 2, 3, 1, 2, 6, NA, NA, 3, 6, 1, 1, 2, NA, 12, NA,‚Ä¶\n$ AlcoholYear      <int> 0, 20, 52, 100, 104, 364, NA, 0, 104, 36, 12, 312, 15‚Ä¶\n$ SmokeNow         <fct> No, Yes, NA, No, NA, NA, Yes, NA, No, No, NA, No, NA,‚Ä¶\n$ Smoke100         <fct> Yes, Yes, No, Yes, No, No, Yes, No, Yes, Yes, No, Yes‚Ä¶\n$ Smoke100n        <fct> Smoker, Smoker, Non-Smoker, Smoker, Non-Smoker, Non-S‚Ä¶\n$ SmokeAge         <int> 18, 38, NA, 13, NA, NA, 17, NA, NA, 16, NA, 18, NA, N‚Ä¶\n$ Marijuana        <fct> Yes, Yes, Yes, NA, Yes, Yes, NA, No, No, NA, No, Yes,‚Ä¶\n$ AgeFirstMarij    <int> 17, 18, 13, NA, 19, 15, NA, NA, NA, NA, NA, 18, NA, N‚Ä¶\n$ RegularMarij     <fct> No, No, No, NA, Yes, Yes, NA, No, No, NA, No, No, No,‚Ä¶\n$ AgeRegMarij      <int> NA, NA, NA, NA, 20, 15, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ HardDrugs        <fct> Yes, Yes, No, No, Yes, Yes, NA, No, No, No, No, No, N‚Ä¶\n$ SexEver          <fct> Yes, Yes, Yes, Yes, Yes, Yes, NA, Yes, Yes, Yes, Yes,‚Ä¶\n$ SexAge           <int> 16, 12, 13, 17, 22, 12, NA, NA, 27, 20, 20, 18, 14, 2‚Ä¶\n$ SexNumPartnLife  <int> 8, 10, 20, 15, 7, 100, NA, 9, 1, 1, 2, 5, 20, 1, 20, ‚Ä¶\n$ SexNumPartYear   <int> 1, 1, 0, NA, 1, 1, NA, 1, 1, NA, 1, 1, 2, 1, 3, 1, NA‚Ä¶\n$ SameSex          <fct> No, Yes, Yes, No, No, No, NA, No, No, No, No, No, No,‚Ä¶\n$ SexOrientation   <fct> Heterosexual, Heterosexual, Bisexual, NA, Heterosexua‚Ä¶\n$ PregnantNow      <fct> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n\n\nFor now, we will treat this dataset as our Population. So each variable in the dataset is a population for that particular quantity/category, with appropriate population parameters such as means, sd-s, and proportions. Let us calculate the population parameters for the Height data:\n\nShow the Codepop_mean_height <- mean(~ Height, data = NHANES_adult)\npop_sd_height <- sd(~ Height, data = NHANES_adult)\n\npop_mean_height\n\n[1] 168.3497\n\nShow the Codepop_sd_height\n\n[1] 10.15705\n\n\nNow, we will sample ONCE from the NHANES Height variable. Let us take a sample of sample size 50. We will compare sample statistics with population parameters on the basis of this ONE sample of 50:\n\nShow the Codesample_height <- sample(NHANES_adult, size = 50) %>% select(Height)\nsample_height\n\n\n\n\n\nSingle-Sample Mean and Population Mean\n\nShow the Codesample_mean_height <- mean(~ Height, data = sample_height)\nsample_mean_height\n\n[1] 165.866\n\nShow the Code# Plotting the histogram of this sample\nsample_height %>% gf_histogram(~ Height) %>% \n  gf_vline(xintercept = sample_mean_height, color = \"red\") %>% \n  gf_vline(xintercept = pop_mean_height, colour = \"blue\") %>% \n  gf_text(1 ~ (pop_mean_height + 5), label = \"Population Mean Height\", color = \"blue\") %>% \n  gf_text(2 ~ (sample_mean_height-5), label = \"Sample Mean Height\", color = \"red\")\n\n\n\nSingle-Sample Mean and Population Mean\n\n\n\n\nOK, so the sample_mean_height is not too far from the pop_mean_height. Is this always true? Let us check: we will create 500 samples each of size 50. And calculate their mean as the sample statistic, giving us a dataframe containing 5000 sample means. We will then compare if these 500 means are close to the pop_mean_height:\n\nShow the Codesample_height_500 <- do(500) * {\n  sample(NHANES_adult, size = 50) %>%\n    select(Height) %>%\n    summarise(\n      sample_mean_500 = mean(Height),\n      sample_min_500 = min(Height),\n      sample_max_500 = max(Height))\n}\n\nhead(sample_height_500)\n\n\n\n\n\nMultiple Sample-Means and Population Mean\n\nShow the Codedim(sample_height_500)\n\n[1] 500   5\n\nShow the Codesample_height_500 %>%\n  gf_point(.index ~ sample_mean_500, color = \"red\") %>%\n  gf_segment(\n    .index + .index ~ sample_min_500 + sample_max_500,\n    color = \"red\",\n    size = 0.3,\n    alpha = 0.3,\n    ylab = \"Sample Index (1-500)\",\n    xlab = \"Sample Means\"\n  ) %>%\n  gf_vline(xintercept = ~ pop_mean_height, color = \"blue\") %>%\n  gf_label(-15 ~ pop_mean_height, label = \"Population Mean\", color = \"blue\")\n\n\n\nMultiple Sample-Means and Population Mean\n\n\n\n\nThe sample_means (red dots), are themselves random because the samples are random, of course. It appears that they are generally in the vicinity of the pop_mean (blue line).\n\n5.1 Distribution of Sample-Means\nSince the sample-means are themselves random variables, let‚Äôs plot the distribution of these 5000 sample-means themselves, called a a distribution of sample-means.\n\nNOTE: this a distribution of sample-means will itself have a mean and standard deviation. Do not get confused ;-D\n\nWe will also plot the position of the population mean pop_mean_height parameter, the means of the Height variable.\n\nShow the Codesample_height_500 %>% gf_dhistogram(~ sample_mean_500) %>% \n  gf_vline(xintercept = pop_mean_height, color = \"blue\") %>% \n   gf_label(0.01 ~ pop_mean_height, label = \"Population Mean\", color = \"blue\")\n\n\n\nSampling Mean Distribution\n\n\n\n\nHow does this distribution of sample-means compare with that of the overall distribution of the population?\n\nShow the Codesample_height_500 %>% gf_dhistogram(~ sample_mean_500) %>% \n  gf_vline(xintercept = pop_mean_height, color = \"blue\") %>% \n   gf_label(0.01 ~ pop_mean_height, label = \"Population Mean\", color = \"blue\") %>% \n\n  ## Add the population histogram\n  gf_histogram(~ Height, data = NHANES_adult, alpha = 0.2, fill = \"blue\", bins = 50) %>% \n  gf_label(0.025 ~ (pop_mean_height + 20), label = \"Population Distribution\", color = \"blue\")\n\n\n\nSampling Means and Population Distributions\n\n\n\n\n\n5.2 Central limit theorem\nWe see in the Figure above that\n\nthe distribution of sample-means is centered around mean = pop_mean.\nThat the standard deviation of the distribution of sample means is less than that of the original population. But exactly what is it?\nAnd what is the kind of distribution?\n\nOne more experiment.\nNow let‚Äôs repeatedly sample Height and compute the sample mean, and look at the resulting histograms and Q-Q plots. ( Q-Q plots check whether a certain distribution is close to being normal or not.)\nWe will use sample sizes of c(16, 32, 64, 128) and generate 1000 samples each time, take the means and plot these 1000 means:\n\nShow the Codeset.seed(12345)\n\n\nsamples_height_16 <- do(1000,) * mean(resample(NHANES_adult$Height, size = 16))\nsamples_height_32 <- do(1000) * mean(resample(NHANES_adult$Height, size = 32))\nsamples_height_64 <- do(1000) * mean(resample(NHANES_adult$Height, size = 64))\nsamples_height_128 <- do(1000) * mean(resample(NHANES_adult$Height, size = 128))\n\n# Quick Check\nhead(samples_height_16)\n\n\n\n  \n\n\nShow the Code### do(1000,) * mean(resample(NHANES_adult$Height, size = 16)) produces a data frame with a variable named mean.\n###\n\n\nNow let‚Äôs create separate Q-Q plots for the different sample sizes.\n\n\n\n\n\nLet us plot their individual histograms to compare them:\n\n\n\n\n\nAnd if we overlay the histograms:\n\n\n\n\n\nThis shows that the results become more normally distributed (i.e. following the straight line) as the samples get larger. Hence we learn that:\n\nthe sample-means are normally distributed around the population mean. This is because when we sample from the population, many values will be close to the population mean, and values far away from the mean will be increasingly scarce.\n\n\nShow the Codemean(~ mean, data  = samples_height_16)\n\n[1] 168.306\n\nShow the Codemean(~ mean, data  = samples_height_32)\n\n[1] 168.4349\n\nShow the Codemean(~ mean, data  = samples_height_64)\n\n[1] 168.3184\n\nShow the Codemean(~ mean, data  = samples_height_128)\n\n[1] 168.366\n\nShow the Codepop_mean_height\n\n[1] 168.3497\n\n\n\nthe sample-means become ‚Äúmore normally distributed‚Äù with sample length, as shown by the (small but definite) improvements in the Q-Q plots with sample-size.\nthe sample-mean distributions narrow with sample length.\n\nThis is regardless of the distribution of the population itself. ( The Height variable seems to be normally distributed at population level. We will try other non-normal population variables as an exercise). This is the ** Central Limit Theorem (CLT) **.\nAs we saw above, the standard deviations of the sample-mean distributions reduce with sample size. In fact their SDs are defined by:\nsd = pop_sd/sqrt(sample_size) where sample-size here is one of c(16,32,64,128)\n\nShow the Codesd(~ mean, data  = samples_height_16)\n\n[1] 2.578355\n\nShow the Codesd(~ mean, data  = samples_height_32)\n\n[1] 1.834979\n\nShow the Codesd(~ mean, data  = samples_height_64)\n\n[1] 1.280014\n\nShow the Codesd(~ mean, data  = samples_height_128)\n\n[1] 0.9096318\n\n\nThe standard deviation of the sample-mean distribution is called the Standard Error. This statistic derived from the sample, will help us infer our population parameters with a precise estimate of the uncertainty involved.\n\\[\nStandard\\ Error\\ \\pmb {se} = \\frac{population\\ sd}{\\sqrt[]{sample\\ size}}\\\\\\\n\\pmb {se} = \\frac{\\sigma}{\\sqrt[]{n}}\n\\]\nIn our sampling experiments, the Standard Errors evaluate to:\n\nShow the Codepop_sd_height <- sd(~ Height, data = NHANES_adult)\n\npop_sd_height/sqrt(16)\n\n[1] 2.539262\n\nShow the Codepop_sd_height/sqrt(32)\n\n[1] 1.795529\n\nShow the Codepop_sd_height/sqrt(64)\n\n[1] 1.269631\n\nShow the Codepop_sd_height/sqrt(128)\n\n[1] 0.8977646\n\n\nAs seen, these are identical to the Standard Deviations of the individual sample-mean distributions."
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/50-Sampling-and-Probability/index.html#confidence-intervals",
    "href": "content/courses/Basics-of-Modeling/Modules/50-Sampling-and-Probability/index.html#confidence-intervals",
    "title": "Samples, Populations, Statistics and Inference",
    "section": "\n6 Confidence intervals",
    "text": "6 Confidence intervals\nWhen we work with samples, we want to be able to speak with a certain degree of confidence about the population mean, based on the evaluation of one sample mean,not a whole large number of them. Give that sample-means are normally distributed around the population means, we can say that \\(68\\%\\) of all possible sample-mean lie within $ \\textpm 1 SE$ of the population mean; and further that \\(95%\\) of of all possible sample-mean lie within $ \\textpm 1.5* SE$ of the population mean.\nThese two intervals [sample-mean +/- SE] and [sample-mean +/- 1.5SE] are called the confidence intervals for the population mean, at levels 68% and 95% respectively.\nThus if we want to estimate a population mean: - we take one sample from the population\n- we calculate the sample-mean - we calculate the sample-sd - We calculate the Standard Error as \\(\\frac{sample-sd}{\\sqrt[]{n}}\\) - We calculate 95% confidence intervals for the population mean based on the formula above."
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/50-Sampling-and-Probability/index.html#references",
    "href": "content/courses/Basics-of-Modeling/Modules/50-Sampling-and-Probability/index.html#references",
    "title": "Samples, Populations, Statistics and Inference",
    "section": "\n7 References",
    "text": "7 References\n\nDiez, David M & Barr, Christopher D & √áetinkaya-Rundel, Mine, OpenIntro Statistics. https://www.openintro.org/book/os/\nStats Test Wizard. https://www.socscistatistics.com/tests/what_stats_test_wizard.aspx\nDiez, David M & Barr, Christopher D & √áetinkaya-Rundel, Mine: OpenIntro Statistics. Available online https://www.openintro.org/book/os/\nM√•ns Thulin, Modern Statistics with R: From wrangling and exploring data to inference and predictive modelling http://www.modernstatisticswithr.com/\nJonas Kristoffer Lindel√∏v, Common statistical tests are linear models (or: how to teach stats) https://lindeloev.github.io/tests-as-linear/\nCheatSheet https://lindeloev.github.io/tests-as-linear/linear_tests_cheat_sheet.pdf\nCommon statistical tests are linear models: a work through by Steve Doogue https://steverxd.github.io/Stat_tests/\nJeffrey Walker ‚ÄúElements of Statistical Modeling for Experimental Biology‚Äù. https://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/65-Basic-Simulation-Test/files/simulation.html",
    "href": "content/courses/Basics-of-Modeling/Modules/65-Basic-Simulation-Test/files/simulation.html",
    "title": "simulation",
    "section": "",
    "text": "In this module we will use simulation to solve several problems in Business Decision Making."
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/65-Basic-Simulation-Test/index.html",
    "href": "content/courses/Basics-of-Modeling/Modules/65-Basic-Simulation-Test/index.html",
    "title": "Basics of Simulation Tests",
    "section": "",
    "text": "R Tutorial¬†\n  Orange Tutorial\n Radiant Tutorial\n Datasets\n\n\n\n\n\n\n\n\n\n\nThere is a famous story about a lady who claimed that tea with milk tasted different depending on whether the milk was added to the tea or the tea added to the milk. The story is famous because of the setting in which she made this claim. She was attending a party in Cambridge, England, in the 1920s. Also in attendance were a number of university dons and their wives. The scientists in attendance scoffed at the woman and her claim. What, after all, could be the difference?\nAll the scientists but one, that is. Rather than simply dismiss the woman‚Äôs claim, he proposed that they decide how one should test the claim. The tenor of the conversation changed at this suggestion, and the scientists began to discuss how the claim should be tested. Within a few minutes cups of tea with milk had been prepared and presented to the woman for tasting.\nAt this point, you may be wondering who the innovative scientist was and what the results of the experiment were. The scientist was R. A. Fisher, who first described this situation as a pedagogical example in his 1925 book on statistical methodology[^1]. Fisher developed statistical methods that are among the most important and widely used methods to this day, and most of his applications were biological.\n\n\nLet‚Äôs try an experiment. I‚Äôll flip 10 coins. You guess which are heads and which are tails, and we‚Äôll see how you do. Please write down a sequence of ‚ÄúH‚Äù or ‚ÄúT‚Äù. Comparing with your classmates, we will undoubtedly see that some of you did better and others worse.\nWhat would be your impression of one of you got 9 guesses correct? Is that SKILL or is that something else? What would be your immediate reaction and next move?\n\nBack to the Lady who drank Tea !!\n\nLet‚Äôs suppose we decide to test the lady with ten cups of tea. We‚Äôll flip a coin to decide which way to prepare the cups. If we flip a head, we will pour the milk in first; if tails, we put the tea in first. Then we present the ten cups to the lady and have her state which ones she thinks were prepared each way.\nIt is easy to give her a score (9 out of 10, or 7 out of 10, or whatever it happens to be). It is trickier to figure out what to do with her score. Even if she is just guessing and has no idea, she could get lucky and get quite a few correct ‚Äì maybe even all 10. But how likely is that?\nNow let‚Äôs suppose the lady gets 9 out of 10 correct. That‚Äôs not perfect, but it is better than we would expect for someone who was just guessing. On the other hand, it is not impossible to get 9 out of 10 just by guessing.\nSo here is Fisher‚Äôs great idea: Let‚Äôs figure out how hard it is to get 9 out of 10 by guessing. If it‚Äôs not so hard to do, then perhaps that‚Äôs just what happened ( that she was guessing ), so we won‚Äôt be too impressed with the lady‚Äôs tea tasting ability. On the other hand, if it is really unusual to get 9 out of 10 correct by guessing, then we will have some evidence that she must be able to tell something ( and has an unusual Skill).\nBut how do we figure out how unusual it is to get 9 out of 10 just by guessing? Let‚Äôs just flip a bunch of coins and keep track. If the lady is just guessing, she might as well be flipping a coin.\nSo here‚Äôs the plan. We‚Äôll flip 10 coins. We‚Äôll call the heads correct guesses and the tails incorrect guesses.\n\n\n\nheads\n   0    1    2    3    4    5    6    7    8    9   10 \n   8   84  421 1164 2010 2458 2110 1190  432  110   13 \n\n\n\n\n\nSo what do we conclude? It is possible that the lady could get 9 or 10 correct just by guessing, but it is not very likely (it only happened in about 3% of our simulations). So one of two things must be true:\n‚Ä¢ The lady got unusually ‚Äúlucky‚Äù, or\n‚Ä¢ The lady is not just guessing\n\nFirst we realize something is surprising, and that we have a question or doubt. This is based on something we see, or measure, a test statistic. In our story, it is the score of \\(10/10\\) that the Lady was able to achieve about how the Tea was made.\nWe then assume the Lady is guessing and somehow by chance able to guess correctly. This would be our‚Ä¶.NULL Hypothesis. This is our (conservative) belief about the Real World.\nWe then randomly generate many Parallel Counterfactual Worlds, where we repeat the experiment many many times, each time calculating the test statistic, under the assumption of the NULL Hypothesis is TRUE.\nWe see how often our Parallel Worlds can mimic or exceed Real World measurement of the the test statistic by comparison. If this is common (i.e.¬†probability is high) we say we cannot reject the NULL Hypothesis (and the Lady is lucky). If the occurrence is rare, as in our case, we say we have reason to reject the NULL Hypothesis and reason to believe an underlying pattern (and Lady‚Äôs ability is beyond Question !)\nThis is the essence of the Simulation Method in statistical modelling. Take one more look at the picture from Allen Downey‚Äôs blog, below:\n\n\n[^1]. R.A. Fisher. Statistical Methods for Research Workers. Oliver & Boyd, 1925\n\nLaura Chihara, Tim Hesterberg, Mathematical Statistics with Resampling and R, Wiley, 2019.\nhttps://timesofindia.indiatimes.com/sports/cricket/icc-mens-t20-world-cup/in-numbers-virat-kohli-and-his-strange-luck-with-the-coin-toss/articleshow/87538443.cms\nD. Salsburg. The Lady Tasting Tea: How statistics revolutionized science in the twentieth century. W.H. Freeman, New York, 2001"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/67-Permutation-Tests/files/permutation.html",
    "href": "content/courses/Basics-of-Modeling/Modules/67-Permutation-Tests/files/permutation.html",
    "title": "Permutation Tests",
    "section": "",
    "text": "A student conducted a study of hot wings and beer consumption at a Bar. She asked patrons at the bar to record their consumption of hot wings and beer over the course of several hours. She wanted to know if people who ate more hot wings would then drink more beer. In addition, she investigated whether or not gender had an impact on hot wings or beer consumption.\n\nShow the CodeBeerwings <- read.csv(\"../../../../../materials/data/resampling/Beerwings.csv\")\ninspect(Beerwings)\n\n\ncategorical variables:  \n    name     class levels  n missing\n1 Gender character      2 30       0\n                                   distribution\n1 F (50%), M (50%)                             \n\nquantitative variables:  \n      name   class min    Q1 median    Q3 max     mean        sd  n missing\n1       ID integer   1  8.25   15.5 22.75  30 15.50000  8.803408 30       0\n2 Hotwings integer   4  8.00   12.5 15.50  21 11.93333  4.784554 30       0\n3     Beer integer   0 24.00   30.0 36.00  48 26.20000 11.842064 30       0\n\n\nLet us calculate the observed difference in Hotwings consumption between Males and Females ( Gender)\n\nShow the Codemean(Hotwings ~ Gender, data = Beerwings)\n\n        F         M \n 9.333333 14.533333 \n\nShow the Codeobs_diff_wings <- mosaic::diffmean(data = Beerwings, Hotwings ~ Gender)\nobs_diff_wings \n\ndiffmean \n     5.2 \n\n\n\nShow the Codegf_boxplot(data = Beerwings, Hotwings ~ Gender, title = \"Hotwings Consumption by Gender\")\n\n\n\n\nThe observed difference in mean consumption of Hotwings between Males and Females is 5.2. Could this have occurred by chance? Here is our formulation of the Hypotheses:\n\\[\nNULL\\ Hypothesis\\ H_0 => No\\¬†difference\\ between\\ means\\ across\\ groups\\\\\nAlternative\\ Hypothesis\\\nH_a =>Significant\\ difference\\ between\\ the\\ means\\\n\\]\nSo we perform a Permutation Test to check:\n\nShow the Codenull_dist_wings <- do(1000) * diffmean(Hotwings ~ shuffle(Gender), data = Beerwings)\nnull_dist_wings %>% head()\n\n\n\n  \n\n\nShow the Codegf_histogram(data = null_dist_wings, ~ diffmean) %>% \n  gf_vline(xintercept = obs_diff_wings, colour = \"red\")\n\n\n\nShow the Codeprop1(~ diffmean >= obs_diff_wings, data = null_dist_wings)\n\n  prop_TRUE \n0.002997003 \n\n\nThe \\(\\color{red}{red\\ line}\\) shows the actual measured mean difference in Hot Wings consumption. The probability that our Permutation distribution is able to equal or exceed that number is \\(0.001998002\\) and we have to reject the Null Hypothesis that the means are identical.\n\n\nShow the Codeverizon <- read.csv(\"../../../../../materials/data/resampling/Verizon.csv\")\ninspect(verizon)\n\n\ncategorical variables:  \n   name     class levels    n missing\n1 Group character      2 1687       0\n                                   distribution\n1 ILEC (98.6%), CLEC (1.4%)                    \n\nquantitative variables:  \n  name   class min   Q1 median   Q3   max     mean       sd    n missing\n1 Time numeric   0 0.75   3.63 7.35 191.6 8.522009 14.78848 1687       0\n\n\n\nShow the Codemean(Time ~ Group, data = verizon)\n\n     CLEC      ILEC \n16.509130  8.411611 \n\nShow the Codeobs_diff_verizon <- diffmean(Time ~ Group, data = verizon)\nobs_diff_verizon\n\ndiffmean \n-8.09752 \n\n\n\nShow the Codenull_dist_verizon <- do(1000) * diffmean(Time ~ shuffle(Group), data = verizon)\ngf_histogram(data = null_dist_verizon, ~ diffmean) %>% \n  gf_vline(xintercept = obs_diff_wings, colour = \"red\")\n\n\n\nShow the Codeprop1(~ diffmean >= obs_diff_wings, data = null_dist_verizon)\n\n prop_TRUE \n0.01298701 \n\n\n\nDo criminals released after a jail term commit crimes again?\n\nShow the Coderecidivism <- read.csv(\"../../../../../materials/data/resampling/Recidivism.csv\")\ninspect(recidivism)\n\n\ncategorical variables:  \n     name     class levels     n missing\n1  Gender character      2 17019       3\n2     Age character      5 17019       3\n3   Age25 character      2 17019       3\n4 Offense character      2 17022       0\n5   Recid character      2 17022       0\n6    Type character      3 17022       0\n                                   distribution\n1 M (87.7%), F (12.3%)                         \n2 25-34 (36.6%), 35-44 (23.7%) ...             \n3 Over 25 (81.9%), Under 25 (18.1%)            \n4 Felony (80.6%), Misdemeanor (19.4%)          \n5 No (68.4%), Yes (31.6%)                      \n6 No Recidivism (68.4%), New (20.2%) ...       \n\nquantitative variables:  \n  name   class min  Q1 median  Q3  max     mean       sd    n missing\n1 Days integer   0 241    418 687 1095 473.3275 283.1393 5386   11636\n\n\nThere are some missing values in the variable  Age25. The  complete.cases command gives the row numbers where values are not missing. We create a new data frame omitting the rows where there is a missing value in the  ‚ÄòAge25‚Äô  variable.\n\nShow the Coderecidivism_na <- recidivism %>% tidyr::drop_na(Age25)\n\n\nAlso, the variable Recid is a factor variable coded ‚ÄúYes‚Äù or ‚ÄúNo‚Äù. We convert it to a numeric variable of 1‚Äôs and 0‚Äôs.\n\nShow the Coderecidivism_na <- recidivism_na %>% mutate(Recid2 = ifelse(Recid==\"Yes\", 1, 0))\n\nobs_diff_recid <- diffmean( Recid2 ~ Age25, data = recidivism_na)\nobs_diff_recid\n\n  diffmean \n0.05919913 \n\nShow the Codenull_dist_recid <- do(1000) * diffmean( Recid2 ~ shuffle(Age25), data = recidivism_na)\n\ngf_histogram( ~ diffmean, data = null_dist_recid) %>% \n  gf_vline(xintercept = obs_diff_recid, colour = \"red\")\n\n\n\n\n\n\nShow the CodeDiving2017 <- read.csv(\"../../../../../materials/data/resampling/Diving2017.csv\")\nhead(Diving2017)\n\n\n\n  \n\n\nShow the Codeinspect(Diving2017)\n\n\ncategorical variables:  \n     name     class levels  n missing\n1    Name character     12 12       0\n2 Country character      8 12       0\n                                   distribution\n1  SI Yajie (8.3%) ...                         \n2 Canada (16.7%), China (16.7%) ...            \n\nquantitative variables:  \n       name   class    min       Q1  median      Q3   max    mean       sd  n\n1 Semifinal numeric 313.70 322.2000 325.625 356.575 382.8 338.500 22.94946 12\n2     Final numeric 283.35 318.5875 358.925 387.150 397.5 350.475 40.02204 12\n  missing\n1       0\n2       0\n\n\nThe data is made up of paired observations per swimmer. So we need to take the difference between the two swim records for each swimmer and then shuffle the differences to either polarity. Another way to look at this is to shuffle the records between Semifinal and Final on a per Swimmer basis.\n\nShow the CodeDiving2017\n\n\n\n  \n\n\nShow the CodeDiving2017 %>% diffmean(data = ., Final ~ Semifinal, only.2 = FALSE)\n\n  318.7-313.7  320.55-318.7 322.75-320.55  325.5-322.75  325.75-325.5 \n       12.350       -63.050         5.225        85.125      -114.150 \n   346-325.75    355.15-346 360.85-355.15  367.5-360.85   382.8-367.5 \n      102.200       -54.150        28.600        31.950         4.050 \n\nShow the Codeobs_diff_swim <- mean(~ Final - Semifinal, data = Diving2017)\nobs_diff_swim\n\n[1] 11.975\n\n\n\nShow the Codepolarity <- c(rep(1, 6), rep(-1,6))\npolarity\n\n [1]  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1\n\nShow the Codenull_dist_swim <- do(100000) * mean(data = Diving2017, \n                                    ~(Final - Semifinal) * resample(polarity,\n                                                    replace = TRUE))\nnull_dist_swim %>% head()\n\n\n\n  \n\n\nShow the Codegf_histogram(data = null_dist_swim, ~mean) %>% \n  gf_vline(xintercept = obs_diff_swim, colour = \"red\")\n\n\n\n\n\nLaGuardia Airport (LGA) is one of three major airports that serves the New York City metropolitan area. In 2008, over 23 million passengers and over 375 000 planes flew in or out of LGA. United Airlines and America Airlines are two major airlines that schedule services at LGA. The data set FlightDelays contains information on all 4029 departures of these two airlines from LGA during May and June 2009.\n\nShow the CodeflightDelays <- read.csv(\"../../../../../materials/data/resampling/FlightDelays.csv\")\n\ninspect(flightDelays)\n\n\ncategorical variables:  \n         name     class levels    n missing\n1     Carrier character      2 4029       0\n2 Destination character      7 4029       0\n3  DepartTime character      5 4029       0\n4         Day character      7 4029       0\n5       Month character      2 4029       0\n6   Delayed30 character      2 4029       0\n                                   distribution\n1 AA (72.1%), UA (27.9%)                       \n2 ORD (44.3%), DFW (22.8%), MIA (15.1%) ...    \n3 8-Noon (26.1%), Noon-4pm (26%) ...           \n4 Fri (15.8%), Mon (15.6%), Tue (15.6%) ...    \n5 June (50.4%), May (49.6%)                    \n6 No (85.2%), Yes (14.8%)                      \n\nquantitative variables:  \n          name   class min   Q1 median   Q3  max      mean         sd    n\n1           ID integer   1 1008   2015 3022 4029 2015.0000 1163.21645 4029\n2     FlightNo integer  71  371    691  787 2255  827.1035  551.30939 4029\n3 FlightLength integer  68  155    163  228  295  185.3011   41.78783 4029\n4        Delay integer -19   -6     -3    5  693   11.7379   41.63050 4029\n  missing\n1       0\n2       0\n3       0\n4       0\n\n\nThe variables in the flightDelays dataset are:\n\nflightDelay dataset variables\n\n\n\n\n\nVariable\nDescription\n\n\n\nCarrier\nUA=United Airlines, AA=American Airlines\n\n\nFlightNo\nFlight number\n\n\nDestination\nAirport code\n\n\nDepartTime\nScheduled departure time in 4 h intervals\n\n\nDay\nDay of the Week\n\n\nMonth\nMay or June\n\n\nDelay\nMinutes flight delayed (negative indicates early departure)\n\n\nDelayed30\nDeparture delayed more than 30 min? Yes or No\n\n\nFlightLength\nLength of time of flight (minutes)\n\n\n\n\nLet us compute the proportion of times that each carrier‚Äôs flights was delayed more than 20 min. We will conduct a two-sided test to see if the difference in these proportions is statistically significant.\n\n\nShow the Codeprop(data = flightDelays, Delay >= 20 ~ Carrier)\n\nprop_TRUE.AA prop_TRUE.UA \n   0.1713696    0.2226180 \n\nShow the Codeobs_diff_delay <- diffprop(data = flightDelays, Delay >= 20 ~ Carrier)\nobs_diff_delay\n\n  diffprop \n0.05124841 \n\n\nWe see carrier AA has a 17.13% chance of delays>= 20, while UA has 22.26% chance. The difference is 5.12%. Is this statistically significant? We take the Delays for both Carriers and perform a permutation test by shuffle on the carrier variable:\n\nShow the Codenull_dist_delay <- do(10000) * diffprop(data = flightDelays, Delay >= 20 ~ shuffle(Carrier))\nnull_dist_delay %>% head()\n\n\n\n  \n\n\nShow the Codegf_histogram(data = null_dist_delay, ~ diffprop) %>% gf_vline(xintercept = obs_diff_delay, color = \"red\")\n\n\n\n\nIt appears that the difference indelay times is significant. We can compute the p-value based on this test:\n\nShow the Code2* mean(null_dist_delay >= obs_diff_delay)\n\n[1] 0\n\n\nwhich is very small. Hence we reject the null Hypothesis that there is no difference between carriers on delay times.\n\nCompute the variance in the flight delay lengths for each carrier. Conduct a test to see if the variance for United Airlines differs from that of American Airlines.\n\n\nShow the Codevar(data = flightDelays, Delay ~ Carrier)\n\n      AA       UA \n1606.457 2037.525 \n\nShow the Code# There is no readymade function in mosaic called `diffvar`...so...we construct one\nobs_diff_var <- diff(var(data = flightDelays, Delay ~ Carrier))\nobs_diff_var\n\n      UA \n431.0677 \n\n\nThe difference in variances in Delay between the two carriers is \\(-431.0677\\). In our Permutation Test, we shuffle the Carrier variable:\n\nShow the Codeobs_diff_var <- diff(var(data = flightDelays, Delay ~ Carrier))\nnull_dist_var <-\n  do(10000) * diff(var(data = flightDelays, Delay ~ shuffle(Carrier)))\nnull_dist_var %>% head()\n\n\n\n  \n\n\nShow the Code# The null distribution variable is called `UA`\ngf_histogram(data = null_dist_var, ~ UA) %>% gf_vline(xintercept = obs_diff_delay, color = \"red\")\n\n\n\nShow the Code2 * mean(null_dist_var >= obs_diff_var)\n\n[1] 0.2998\n\n\nClearly there is no case for a significant difference in variances!\n\nIs there a difference in the price of groceries sold by the two retailers Target and Walmart? The data set Groceries contains a sample of grocery items and their prices advertised on their respective web sites on one specific day.\n\nInspect the data set, then explain why this is an example of matched pairs data.\nCompute summary statistics of the prices for each store.\nConduct a permutation test to determine whether or not there is a difference in the mean prices.\nCreate a histogram bar-chart of the difference in prices. What is unusual about Quaker Oats Life cereal?\nRedo the hypothesis test without this observation. Do you reach the same conclusion?\n\n\nShow the Codegroceries <- read.csv(\"../../../../../materials/data/resampling/Groceries.csv\") %>% mutate(Product = stringr::str_squish(Product))\nhead(groceries)\n\n\n\n  \n\n\nShow the Codeinspect(groceries)\n\n\ncategorical variables:  \n     name     class levels  n missing\n1 Product character     30 30       0\n2    Size character     24 30       0\n                                   distribution\n1 Annie's Macaroni & Cheese (3.3%) ...         \n2 18oz (10%), 12oz (6.7%) ...                  \n\nquantitative variables:  \n     name   class  min     Q1 median    Q3  max     mean       sd  n missing\n1  Target numeric 0.99 1.8275  2.545 3.140 7.99 2.762333 1.582128 30       0\n2 Walmart numeric 1.00 1.7600  2.340 2.955 6.98 2.705667 1.560211 30       0\n\n\nWe see that the comparison is to be made between two prices for the same product, and hence this is one more example of paired data, as in Case Study #4. Let us plot the prices for the products:\n\nShow the Codegf_col(data = groceries,\n       Target ~ Product,\n       fill = \"#0073C299\",\n       width = 0.5 ) %>% \n  gf_col(data = groceries,\n         -Walmart ~ Product,\n         fill = \"#EFC00099\",\n         ylab = \"Prices\",\n         width = 0.5\n       ) %>% \n  gf_col(data = groceries %>% filter(Product == \"Quaker Oats Life Cereal Original\"), \n         -Walmart ~ Product,\n         fill = \"red\", \n         width = 0.5) %>% \n  gf_theme(theme_classic()) %>%\n  gf_theme(ggplot2::theme(axis.text.x = element_text(\n    size = 8,\n    face = \"bold\",\n    vjust = 0,\n    hjust = 1\n  ))) %>% gf_theme(ggplot2::coord_flip())\n\n\n\n\nWe see that the price difference between Walmart and Target prices is highest for the Product named Quaker Oats Life Cereal Original. Let us check the mean difference in prices:\n\nShow the Codediffmean(data = groceries, Walmart ~ Target, only.2 = FALSE)\n\n   1-0.99    1.22-1 1.42-1.22 1.49-1.42 1.59-1.49 1.62-1.59 1.79-1.62 1.94-1.79 \n-0.580000  0.170000  0.210000 -0.100000  0.190000  0.070000  0.180000  0.160000 \n1.99-1.94 2.12-1.99 2.39-2.12  2.5-2.39  2.59-2.5 2.64-2.59 2.79-2.64 2.82-2.79 \n 0.090000  0.010000  0.200000  0.600000 -0.200000 -0.600000  0.660000  0.040000 \n2.99-2.82 3.19-2.99 3.49-3.19 3.99-3.49 4.79-3.99 7.19-4.79 7.99-7.19 \n 0.220000  1.263333 -1.183333 -0.480000  2.290000  2.190000  0.000000 \n\nShow the Codeobs_diff_price = mean( ~ Walmart - Target, data = groceries)\nobs_diff_price\n\n[1] -0.05666667\n\n\nLet us perform the pair-wise permutation test on prices, by shuffling the two store names:\n\nShow the Codepolarity <- c(rep(1, 15), rep(-1,15))\npolarity\n\n [1]  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n[26] -1 -1 -1 -1 -1\n\nShow the Codenull_dist_price <- do(100000) * mean(data = groceries, \n                                    ~(Walmart-Target) * resample(polarity,\n                                                    replace = TRUE))\nnull_dist_price %>% head()\n\n\n\n  \n\n\nShow the Codegf_histogram(data = null_dist_price, ~mean) %>% \n  gf_vline(xintercept = obs_diff_price, colour = \"red\")\n\n\n\nShow the Code2*(sum(null_dist_price >= obs_diff_price + 1)/(100000+1)) #P-value\n\n[1] 0\n\n\nDoes not seem to be aby significant difference in prices‚Ä¶\nSuppose we knock off the Quaker Cereal data item‚Ä¶\n\nShow the Codewhich(groceries$Product == \"Quaker Oats Life Cereal Original\")\n\n[1] 2\n\nShow the Codegroceries_less <- groceries[-2,]\ngroceries_less\n\n\n\n  \n\n\nShow the Codeobs_diff_price_less = mean( ~ Walmart - Target, data = groceries_less)\nobs_diff_price_less\n\n[1] -0.1558621\n\nShow the Codepolarity_less <- c(rep(1, 15), rep(-1,14)) # Due to resampling this small bias makes no difference\nnull_dist_price_less <- do(100000) * mean(data = groceries_less, \n                                    ~(Walmart-Target) * resample(polarity_less,\n                                                    replace = TRUE))\nnull_dist_price_less %>% head()\n\n\n\n  \n\n\nShow the Codegf_histogram(data = null_dist_price_less, ~mean) %>% \n  gf_vline(xintercept = obs_diff_price_less, colour = \"red\")\n\n\n\nShow the Code1- mean(null_dist_price_less >= obs_diff_price_less) #P-value\n\n[1] 0.01615\n\n\n\nLet us try a dataset with Qualitative / Categorical data. This is a General Social Survey dataset, and we have people with different levels of Education stating their opinion on the Death Penalty. We want to know if these two Categorical variables have a correlation, i.e.¬†can the opinions in favour of the Death Penalty be explained by the Education level?\nSince data is Categorical, we need to take counts in a table, and then implement a chi-square test. In the test, we will permute the Education variable to see if we can see how significant its effect size is.\n\nShow the CodeGSS2002 <- read.csv(\"../../../../../materials/data/resampling/GSS2002.csv\")\ninspect(GSS2002)\n\n\ncategorical variables:  \n            name     class levels    n missing\n1         Region character      7 2765       0\n2         Gender character      2 2765       0\n3           Race character      3 2765       0\n4      Education character      5 2760       5\n5        Marital character      5 2765       0\n6       Religion character     13 2746      19\n7          Happy character      3 1369    1396\n8         Income character     24 1875     890\n9       PolParty character      8 2729      36\n10      Politics character      7 1331    1434\n11     Marijuana character      2  851    1914\n12  DeathPenalty character      2 1308    1457\n13        OwnGun character      3  924    1841\n14        GunLaw character      2  916    1849\n15 SpendMilitary character      3 1324    1441\n16     SpendEduc character      3 1343    1422\n17      SpendEnv character      3 1322    1443\n18      SpendSci character      3 1266    1499\n19        Pres00 character      5 1749    1016\n20      Postlife character      2 1211    1554\n                                    distribution\n1  North Central (24.7%) ...                    \n2  Female (55.6%), Male (44.4%)                 \n3  White (79.1%), Black (14.8%) ...             \n4  HS (53.8%), Bachelors (16.1%) ...            \n5  Married (45.9%), Never Married (25.6%) ...   \n6  Protestant (53.2%), Catholic (24.5%) ...     \n7  Pretty happy (57.3%) ...                     \n8  40000-49999 (9.1%) ...                       \n9  Ind (19.3%), Not Str Dem (18.9%) ...         \n10 Moderate (39.2%), Conservative (15.8%) ...   \n11 Not legal (64%), Legal (36%)                 \n12 Favor (68.7%), Oppose (31.3%)                \n13 No (65.5%), Yes (33.5%) ...                  \n14 Favor (80.5%), Oppose (19.5%)                \n15 About right (46.5%) ...                      \n16 Too little (73.9%) ...                       \n17 Too little (60%) ...                         \n18 About right (49.7%) ...                      \n19 Bush (50.6%), Gore (44.7%) ...               \n20 Yes (80.5%), No (19.5%)                      \n\nquantitative variables:  \n  name   class min  Q1 median   Q3  max mean       sd    n missing\n1   ID integer   1 692   1383 2074 2765 1383 798.3311 2765       0\n\n\nNote how all variables are Categorical !! Education has five levels:\n\nShow the CodeGSS2002 %>% count(Education)\n\n\n\n  \n\n\nShow the CodeGSS2002 %>% count(DeathPenalty)\n\n\n\n  \n\n\n\nLet us drop NA entries in Education and Death Penalty. And set up a table for the chi-square test.\n\nShow the Codegss2002 <- GSS2002 %>% \n  dplyr::select(Education, DeathPenalty) %>% \n  tidyr::drop_na(., c(Education, DeathPenalty))\ndim(gss2002)\n\n[1] 1307    2\n\nShow the Codegss_summary <- gss2002 %>%\n  mutate(\n    Education = factor(\n      Education,\n      levels = c(\"Bachelors\", \"Graduate\", \"Jr Col\", \"HS\", \"Left HS\"),\n      labels = c(\"Bachelors\", \"Graduate\", \"Jr Col\", \"HS\", \"Left HS\")\n    ),\n    DeathPenalty = as.factor(DeathPenalty)\n  ) %>%\n  group_by(Education, DeathPenalty) %>%\n  summarise(count = n()) %>% # This is good for a chisq test\n  \n  # Add two more columns to faciltate mosaic/Marrimekko Plot\n  # \n  mutate(edu_count = sum(count), \n         edu_prop = count / sum(count)) %>%\n  ungroup() \n\ngss_summary\n\n\n\n  \n\n\nShow the Code# We can plot a heatmap-like `mosaic chart` for this table, using `ggplot`:\n# https://stackoverflow.com/questions/19233365/how-to-create-a-marimekko-mosaic-plot-in-ggplot2\n\nggplot(data = gss_summary, aes( x = Education, y = edu_prop)) +\n  geom_bar(aes(width = edu_count, fill = DeathPenalty), stat = \"identity\", position = \"fill\", colour = \"black\") +\n  geom_text(aes(label = scales::percent(edu_prop)), position = position_stack(vjust = 0.5)) +\n\n\n# if labels are desired\n facet_grid(~ Education, scales = \"free_x\", space = \"free_x\") + \n  theme(scale_fill_brewer(palette = \"RdYlGn\")) + \n  # theme(panel.spacing.x = unit(0, \"npc\")) + # if no spacing preferred between bars\n  theme_void() \n\n\n\n\nLet us now perform the base chisq test: We need a table and then the chisq test:\n\nShow the Codegss_table <- tally(DeathPenalty ~ Education, data = gss2002)\ngss_table\n\n            Education\nDeathPenalty Bachelors Graduate  HS Jr Col Left HS\n      Favor        135       64 511     71     117\n      Oppose        71       50 200     16      72\n\nShow the Code# Get the observed chi-square statistic\nobservedChi2 <- mosaic::chisq(tally(DeathPenalty ~ Education, data = gss2002))\nobservedChi2\n\nX.squared \n 23.45093 \n\nShow the Code# Actual chi-square test\nstats::chisq.test(tally(DeathPenalty ~ Education, data = gss2002))\n\n\n    Pearson's Chi-squared test\n\ndata:  tally(DeathPenalty ~ Education, data = gss2002)\nX-squared = 23.451, df = 4, p-value = 0.0001029\n\n\nWe should now repeat the test with permutations on Education:\n\nShow the Codenull_chisq <- do(10000) * chisq.test(tally(DeathPenalty ~ shuffle(Education), data = gss2002))\n\nhead(null_chisq)\n\n\n\n  \n\n\nShow the Codegf_histogram( ~ X.squared, data = null_chisq) %>% \n  gf_vline(xintercept = observedChi2, color = \"red\")\n\n\n\nShow the Codegf_histogram( ~ p.value, data = null_chisq, binwidth = 0.1, center = 0.05)\n\n\n\n\nSo we would conclude that Education has a significant effect on DeathPenalty opinion!"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/67-Permutation-Tests/index.html#datasets",
    "href": "content/courses/Basics-of-Modeling/Modules/67-Permutation-Tests/index.html#datasets",
    "title": "Permutation Tests",
    "section": "Datasets",
    "text": "Datasets"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/67-Permutation-Tests/index.html#workflow-in-orange",
    "href": "content/courses/Basics-of-Modeling/Modules/67-Permutation-Tests/index.html#workflow-in-orange",
    "title": "Permutation Tests",
    "section": "Workflow in Orange",
    "text": "Workflow in Orange"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/67-Permutation-Tests/index.html#workflow-in-radiant",
    "href": "content/courses/Basics-of-Modeling/Modules/67-Permutation-Tests/index.html#workflow-in-radiant",
    "title": "Permutation Tests",
    "section": "Workflow in Radiant",
    "text": "Workflow in Radiant"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/67-Permutation-Tests/index.html#workflow-in-r",
    "href": "content/courses/Basics-of-Modeling/Modules/67-Permutation-Tests/index.html#workflow-in-r",
    "title": "Permutation Tests",
    "section": "Workflow in R",
    "text": "Workflow in R"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/67-Permutation-Tests/index.html#conclusion",
    "href": "content/courses/Basics-of-Modeling/Modules/67-Permutation-Tests/index.html#conclusion",
    "title": "Permutation Tests",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/70-Bootstrap-Tests/files/bootstrap.html",
    "href": "content/courses/Basics-of-Modeling/Modules/70-Bootstrap-Tests/files/bootstrap.html",
    "title": "Bootstrap Case Studies",
    "section": "",
    "text": "Example 5.2\n\nShow the Codemy.sample <- rgamma(16, 1, 1/2)\n\nN <- 10^5\nmy.boot <- numeric(N)\nfor (i in 1:N)\n {\n  x <- sample(my.sample, 16, replace = TRUE)  #draw resample\n  my.boot[i] <- mean(x)                     #compute mean, store in my.boot\n  }\n\nggplot() + geom_histogram(aes(my.boot), bins=15)\n\n\n\nShow the Codemean(my.boot)  #mean\n\n[1] 3.241264\n\nShow the Codesd(my.boot)    #bootstrap SE\n\n[1] 0.7062477\n\n\nExample 5.3\nArsenic in wells in Bangladesh\n\nShow the CodeBangladesh <- read.csv(\"../../../../../materials/data/resampling/Bangladesh.csv\")\n\nggplot(Bangladesh, aes(Arsenic)) + geom_histogram(bins = 15)\n\n\n\nShow the Codeggplot(Bangladesh, aes(sample = Arsenic)) + stat_qq() + stat_qq_line()\n\n\n\nShow the CodeArsenic <- pull(Bangladesh, Arsenic)\n#Alternatively\n#Arsenic <- Bangladesh$Arsenic\n\nn <- length(Arsenic)\nN <- 10^4\n\narsenic.mean <- numeric(N)\n\nfor (i in 1:N)\n{\n   x <- sample(Arsenic, n, replace = TRUE)\n   arsenic.mean[i] <- mean(x)\n}\n\nggplot() + geom_histogram(aes(arsenic.mean), bins = 15) + \n  labs(title=\"Bootstrap distribution of means\") + \n  geom_vline(xintercept = mean(Arsenic), colour = \"blue\")\n\n\n\nShow the Codedf <- data.frame(x = arsenic.mean)\nggplot(df, aes(sample = x)) + stat_qq() + stat_qq_line()\n\n\n\nShow the Codemean(arsenic.mean)                 #bootstrap mean\n\n[1] 125.3367\n\nShow the Codemean(arsenic.mean) - mean(Arsenic) #bias\n\n[1] 0.01678066\n\nShow the Codesd(arsenic.mean)                   #bootstrap SE\n\n[1] 18.16831\n\nShow the Codesum(arsenic.mean > 161.3224)/N\n\n[1] 0.0334\n\nShow the Codesum(arsenic.mean < 89.75262)/N\n\n[1] 0.0161\n\n\nExample 5.4 Skateboard\n\nShow the CodeSkateboard <- read.csv(\"../../../../../materials/data/resampling/Skateboard.csv\")\n\ntestF <- Skateboard %>% filter(Experimenter == \"Female\") %>% pull(Testosterone)\ntestM <- Skateboard %>% filter(Experimenter == \"Male\") %>% pull(Testosterone)\n\nobserved <- mean(testF) - mean(testM)\n\nnf <- length(testF)\nnm <- length(testM)\n\nN <- 10^4\n\nTestMean <- numeric(N)\n\nfor (i in 1:N)\n{\n  sampleF <- sample(testF, nf, replace = TRUE)\n  sampleM <- sample(testM, nm, replace = TRUE)\n  TestMean[i] <- mean(sampleF) - mean(sampleM)\n}\n\ndf <- data.frame(TestMean)\nggplot(df) + geom_histogram(aes(TestMean), bins = 15) + \n  labs(title = \"Bootstrap distribution of difference in means\", xlab = \"means\") +\n  geom_vline(xintercept = observed, colour = \"blue\")\n\n\n\nShow the Codeggplot(df, aes(sample = TestMean))  + stat_qq() + stat_qq_line()\n\n\n\nShow the Codemean(testF) - mean(testM)\n\n[1] 83.0692\n\nShow the Codemean(TestMean)\n\n[1] 83.25584\n\nShow the Codesd(TestMean)\n\n[1] 29.76779\n\nShow the Codequantile(TestMean,c(0.025,0.975))\n\n     2.5%     97.5% \n 24.71615 141.40654 \n\nShow the Codemean(TestMean)- observed  #bias\n\n[1] 0.1866404\n\n\nPermutation test for Skateboard means\n\nShow the CodetestAll <- pull(Skateboard, Testosterone)\n#testAll <- Skateboard$Testosterone\n\nN <- 10^4 - 1  #set number of times to repeat this process\n\n#set.seed(99)\nresult <- numeric(N) # space to save the random differences\nfor(i in 1:N)\n  {\n  index <- sample(71, size = nf, replace = FALSE) #sample of numbers from 1:71\n  result[i] <- mean(testAll[index]) - mean(testAll[-index])\n}\n\n(sum(result >= observed)+1)/(N + 1)  #P-value\n\n[1] 0.008\n\nShow the Codeggplot() + geom_histogram(aes(result), bins = 15) + \n  labs(x = \"xbar1-xbar2\", title=\"Permutation distribution for testosterone levels\") +\n  geom_vline(xintercept = observed, colour = \"blue\")\n\n\n\nShow the Codedf <- data.frame(result)\nggplot(df, aes(sample = result)) + stat_qq() + stat_qq_line()\n\n\n\n\nSection 5.4.1 Matched pairs for Diving data\n\nShow the CodeDiving2017 <- read.csv(\"../../../../../materials/data/resampling/Diving2017.csv\")\nDiff <- Diving2017 %>% mutate(Diff = Final - Semifinal) %>% pull(Diff)\n#alternatively\n#Diff <- Diving2017$Final - Diving2017$Semifinal\nn <- length(Diff)\n\nN <- 10^5\nresult <- numeric(N)\n\nfor (i in 1:N)\n{\n  dive.sample <- sample(Diff, n, replace = TRUE)\n  result[i] <- mean(dive.sample)\n}\n\nggplot() + geom_histogram(aes(result), bins = 15)\n\n\n\nShow the Codequantile(result, c(0.025, 0.975))\n\n     2.5%     97.5% \n-6.745833 30.866667 \n\n\nExample 5.5 Verizon cont. Bootstrap means for the ILEC data and for the CLEC data\nBootstrap difference of means.\n\nShow the CodeVerizon <- read.csv(\"../../../../../materials/data/resampling/Verizon.csv\")\n\nTime.ILEC <- Verizon %>% filter(Group == \"ILEC\") %>% pull(Time)\nTime.CLEC <- Verizon %>% filter(Group == \"CLEC\") %>% pull(Time)\n\nobserved <- mean(Time.ILEC) - mean(Time.CLEC)\n\nn.ILEC <- length(Time.ILEC)\nn.CLEC <- length(Time.CLEC)\n\nN <- 10^4\n\ntime.ILEC.boot <- numeric(N)\ntime.CLEC.boot <- numeric(N)\ntime.diff.mean <- numeric(N)\n\nset.seed(100)\nfor (i in 1:N)\n {\n  ILEC.sample <- sample(Time.ILEC, n.ILEC, replace = TRUE)\n  CLEC.sample <- sample(Time.CLEC, n.CLEC, replace = TRUE)\n  time.ILEC.boot[i] <- mean(ILEC.sample)\n  time.CLEC.boot[i] <- mean(CLEC.sample)\n  time.diff.mean[i] <- mean(ILEC.sample) - mean(CLEC.sample)\n}\n\n#bootstrap for ILEC\nggplot() + geom_histogram(aes(time.ILEC.boot), bins = 15) + \n  labs(title = \"Bootstrap distribution of ILEC means\", x = \"means\") + \n  geom_vline(xintercept = mean(Time.ILEC), colour = \"blue\") + \n  geom_vline(xintercept = mean(time.ILEC.boot), colour = \"red\", lty=2)\n\n\n\nShow the Codesummary(time.ILEC.boot)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  7.036   8.156   8.400   8.406   8.642   9.832 \n\nShow the Codedf <- data.frame(x = time.ILEC.boot)\nggplot(df, aes(sample = x)) + stat_qq() + stat_qq_line()\n\n\n\nShow the Code#bootstrap for CLEC\nggplot() + geom_histogram(aes(time.CLEC.boot), bins = 15) + \n  labs(title = \"Bootstrap distribution of CLEC means\", x = \"means\") + \n  geom_vline(xintercept = mean(Time.CLEC), colour = \"blue\") + \n  geom_vline(xintercept = mean(time.CLEC.boot), colour = \"red\", lty = 2)\n\n\n\nShow the Codedf <- data.frame(x = time.CLEC.boot)\nggplot(df, aes(sample = x)) + stat_qq() + stat_qq_line()\n\n\n\nShow the Code#Different in means\nggplot() + geom_histogram(aes(time.diff.mean), bins = 15) + \n  labs(title = \"Bootstrap distribution of difference in means\", x = \"means\") +\n  geom_vline(xintercept = mean(time.diff.mean), colour = \"blue\") + \n  geom_vline(xintercept = mean(observed), colour = \"red\", lty = 2)\n\n\n\nShow the Codedf <- data.frame(x = time.diff.mean)\nggplot(df, aes(sample = x)) + stat_qq() + stat_qq_line()\n\n\n\nShow the Codemean(time.diff.mean)\n\n[1] -8.096489\n\nShow the Codequantile(time.diff.mean, c(0.025, 0.975))\n\n      2.5%      97.5% \n-16.970068  -1.690859 \n\n\nSection 5.5 Verizon cont.\nBootstrap difference in trimmed means\n\nShow the CodeTime.ILEC <- Verizon %>% filter(Group == \"ILEC\") %>% pull(Time)\nTime.CLEC <- Verizon %>% filter(Group == \"CLEC\") %>% pull(Time)\nn.ILEC <- length(Time.ILEC)\nn.CLEC <- length(Time.CLEC)\n\nN <- 10^4\ntime.diff.trim <- numeric(N)\n\n#set.seed(100)\nfor (i in 1:N)\n{\n  x.ILEC <- sample(Time.ILEC, n.ILEC, replace = TRUE)\n  x.CLEC <- sample(Time.CLEC, n.CLEC, replace = TRUE)\n  time.diff.trim[i] <- mean(x.ILEC, trim = .25) - mean(x.CLEC, trim = .25)\n}\n\nggplot() + geom_histogram(aes(time.diff.trim), bins = 15) + \n  labs(x = \"difference in trimmed means\") + \n  geom_vline(xintercept = mean(time.diff.trim),colour = \"blue\") + \n  geom_vline(xintercept = mean(Time.ILEC, trim = .25) - mean(Time.CLEC, trim = .25), colour = \"red\", lty = 2)\n\n\n\nShow the Codedf <- data.frame(x = time.diff.trim)\nggplot(df, aes(sample = x)) + stat_qq() + stat_qq_line()\n\n\n\nShow the Codemean(time.diff.trim)\n\n[1] -10.32079\n\nShow the Codequantile(time.diff.trim, c(0.025,0.975))\n\n     2.5%     97.5% \n-15.47049  -4.97130 \n\n\nSection 5.5 Other statistics Verizon cont:\nBootstrap of the ratio of means\nTime.ILEC and Time.CLEC created above.\nn.ILEC, n.CLEC created above\n\nShow the CodeN <- 10^4\ntime.ratio.mean <- numeric(N)\n\n#set.seed(100)\nfor (i in 1:N)\n{\n  ILEC.sample <- sample(Time.ILEC, n.ILEC, replace = TRUE)\n  CLEC.sample <- sample(Time.CLEC, n.CLEC, replace = TRUE)\n  time.ratio.mean[i] <- mean(ILEC.sample)/mean(CLEC.sample)\n}\n\nggplot() + geom_histogram(aes(time.ratio.mean), bins = 12) + \n  labs(title = \"bootstrap distribution of ratio of means\", x = \"ratio of means\") +\n  geom_vline(xintercept = mean(time.ratio.mean), colour = \"red\", lty = 2) + \n  geom_vline(xintercept  = mean(Time.ILEC)/mean(Time.CLEC), col = \"blue\")\n\n\n\nShow the Codedf <- data.frame(x = time.ratio.mean)\nggplot(df, aes(sample = x)) + stat_qq() + stat_qq_line()\n\n\n\nShow the Codemean(time.ratio.mean)\n\n[1] 0.5429164\n\nShow the Codesd(time.ratio.mean)\n\n[1] 0.1354238\n\nShow the Codequantile(time.ratio.mean, c(0.025, 0.975))\n\n     2.5%     97.5% \n0.3283862 0.8517156 \n\n\nExample 5.7 Relative risk example\n\nShow the Codehighbp <- rep(c(1,0),c(55,3283))   #high blood pressure\nlowbp <- rep(c(1,0),c(21,2655))    #low blood pressure\n\nN <- 10^4\nboot.rr <- numeric(N)\nhigh.prop <- numeric(N)\nlow.prop <- numeric(N)\n\nfor (i in 1:N)\n{\n   x.high <- sample(highbp,3338, replace = TRUE)\n   x.low  <- sample(lowbp, 2676, replace = TRUE)\n   high.prop[i] <- sum(x.high)/3338\n   low.prop[i]  <- sum(x.low)/2676\n   boot.rr[i] <- high.prop[i]/low.prop[i]\n}\n\nci <- quantile(boot.rr, c(0.025, 0.975))\n\nggplot() + geom_histogram(aes(boot.rr), bins = 15) + \n  labs(title = \"Bootstrap distribution of relative risk\", x = \"relative risk\") +\n  geom_vline(aes(xintercept = mean(boot.rr), colour = \"mean of bootstrap\")) +\n  geom_vline(aes(xintercept = 2.12, colour=\"observed rr\"), lty = 2) + \n  scale_colour_manual(name=\"\", values = c(\"mean of bootstrap\"=\"blue\", \"observed rr\" = \"red\"))\n\n\n\nShow the Codetemp <- ifelse(high.prop < 1.31775*low.prop, 1, 0)\ntemp2 <- ifelse(high.prop > 3.687*low.prop, 1, 0)\ntemp3 <- temp + temp2\n\ndf <- data.frame(y=high.prop, x=low.prop, temp, temp2, temp3)\ndf1 <- df %>% filter(temp == 1)\ndf2 <- df %>% filter (temp2 == 1)\ndf3 <- df %>% filter(temp3 == 0)\n\nggplot(df, aes(x=x, y = y)) + \n  geom_point(data =df1, aes(x= x, y = y), colour = \"green\") + \n  geom_point(data = df2, aes(x = x, y = y), colour = \"green\") + \n  geom_point(data = df3, aes(x = x, y = y), colour = \"red\") + \n  geom_vline(aes(xintercept = mean(low.prop)), colour = \"red\") +\n  geom_hline(yintercept = mean(high.prop), colour = \"red\") + \n  geom_abline(aes(intercept = 0, slope = 2.12, colour = \"observed rr\"), lty = 2, lwd = 1) + \n  geom_abline(aes(intercept = 0, slope = ci[1], colour = \"bootstrap CI\"), lty = 2, lwd = 1) + \n  geom_abline(intercept = 0, slope = ci[2], colour = \"blue\", lty = 2, lwd = 1) +\n  scale_colour_manual(name=\"\", values=c(\"observed rr\"=\"black\", \"bootstrap CI\" = \"blue\")) +\n  labs(x = \"Proportion in low blood pressure group\", y = \"Proportion in high blood pressure group\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead."
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/70-Bootstrap-Tests/index.html",
    "href": "content/courses/Basics-of-Modeling/Modules/70-Bootstrap-Tests/index.html",
    "title": "Bootstrap",
    "section": "",
    "text": "Slides and Tutorials\n.nbsp;.nbsp;\n.nbsp;.nbsp;\n.nbsp;.nbsp;"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/70-Bootstrap-Tests/index.html#introduction",
    "href": "content/courses/Basics-of-Modeling/Modules/70-Bootstrap-Tests/index.html#introduction",
    "title": "Bootstrap",
    "section": "Introduction",
    "text": "Introduction"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/70-Bootstrap-Tests/index.html#datasets",
    "href": "content/courses/Basics-of-Modeling/Modules/70-Bootstrap-Tests/index.html#datasets",
    "title": "Bootstrap",
    "section": "Datasets",
    "text": "Datasets"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/70-Bootstrap-Tests/index.html#workflow-in-orange",
    "href": "content/courses/Basics-of-Modeling/Modules/70-Bootstrap-Tests/index.html#workflow-in-orange",
    "title": "Bootstrap",
    "section": "Workflow in Orange",
    "text": "Workflow in Orange"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/70-Bootstrap-Tests/index.html#workflow-in-radiant",
    "href": "content/courses/Basics-of-Modeling/Modules/70-Bootstrap-Tests/index.html#workflow-in-radiant",
    "title": "Bootstrap",
    "section": "Workflow in Radiant",
    "text": "Workflow in Radiant"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/70-Bootstrap-Tests/index.html#workflow-in-r",
    "href": "content/courses/Basics-of-Modeling/Modules/70-Bootstrap-Tests/index.html#workflow-in-r",
    "title": "Bootstrap",
    "section": "Workflow in R",
    "text": "Workflow in R"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/70-Bootstrap-Tests/index.html#conclusion",
    "href": "content/courses/Basics-of-Modeling/Modules/70-Bootstrap-Tests/index.html#conclusion",
    "title": "Bootstrap",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#nonparametric-tests-workflow-in-radiant",
    "href": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#nonparametric-tests-workflow-in-radiant",
    "title": "Nonparametric Tests",
    "section": "Nonparametric Tests Workflow in Radiant",
    "text": "Nonparametric Tests Workflow in Radiant"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#nonparametric-tests-workflow-in-r",
    "href": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#nonparametric-tests-workflow-in-r",
    "title": "Nonparametric Tests",
    "section": "Nonparametric Tests Workflow in R",
    "text": "Nonparametric Tests Workflow in R"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#test",
    "href": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#test",
    "title": "Nonparametric Tests",
    "section": "Test",
    "text": "Test\nModel\nExplanation, formula etc.\nCode\nWith Toy Data; Graphs\nExample\nWith another ‚Äúreal world‚Äù data set; Graphs"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#sample-values",
    "href": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#sample-values",
    "title": "Nonparametric Tests",
    "section": "Sample Values",
    "text": "Sample Values\nMost examples in this exposition are based on three ‚Äúimaginary‚Äù samples, \\(x, y, y2\\). Each is normally distributed and made up of 50 observations.\nWe start by creating a function that will allow us to produce samples of a given size (N) with a specified mean (mu) and standard deviation (sd): Note: this gives a matrix of numbers, as opposed to a vector using rnorm by itself. The data are created both in vector form and tibble form for flexibility of use in diverse packages and formulae in what follows.\n\nShow the Codernorm_fixed  <- function(N, mu = 0, sd = 1) {\n  scale(rnorm(N))* sd + mu\n}\n\n\nWe create three variables: x ( explanatory) and y, y2 ( dependent ).\n\nShow the Codeset.seed(40) # for replication\n\n# Data as vectors ( for t.tests etc)\nx <- rnorm_fixed(50, mu = 0.0, sd = 1) #explanatory\ny <- rnorm_fixed(50, mu = 0.3, sd = 2) # dependent #1\ny2 <- rnorm_fixed(50, mu = 0.5, sd = 1.5) # dependent #2\n\n# Make a tibble with all variables\nmydata_wide <- tibble(x = x, y = y, y2 = y2)\n\n# Long form data\nmydata_long <- \n  mydata_wide %>%\n  pivot_longer(., cols = c(x,y,y2), \n               names_to = \"group\", \n               values_to = \"value\")\n\n# Long form data with only dependent variables\nmydata_long_y <- \n  mydata_wide %>% \n  select(-x) %>% \n  pivot_longer(., cols = c(y,y2), \n               names_to = \"group\", \n               values_to = \"value\")\nmydata_wide\n\n\n\n  \n\n\nShow the Codemydata_long\n\n\n\n  \n\n\nShow the Codemydata_long_y"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#signed-rank-values",
    "href": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#signed-rank-values",
    "title": "Nonparametric Tests",
    "section": "‚ÄúSigned Rank‚Äù Values",
    "text": "‚ÄúSigned Rank‚Äù Values\nMost statistical tests use the actual values of the data variables. However, in some non-parametric statistical tests, the data are used in rank-transformed sense/order. In some cases the signed-rank of the data values is used instead of the data itself.\nSigned Rank is calculated as follows:\n1. Take the absolute value of each observation in a sample\n2. Place the ranks in order of (absolute magnitude). The smallest number has rank = 1 and so on.\n3. Give each of the ranks the sign of the original observation ( + or - )\n\nShow the Codesigned_rank <- function(x) {sign(x) * rank(abs(x))}"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#plotting-original-and-signed-rank-data",
    "href": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#plotting-original-and-signed-rank-data",
    "title": "Nonparametric Tests",
    "section": "Plotting Original and Signed Rank Data",
    "text": "Plotting Original and Signed Rank Data\nA quick plot:\n\nShow the Codep1 <- ggplot(mydata_long,aes(x = group, y = value)) +\n  geom_jitter(width = 0.02, height = 0,aes(colour = group), size = 4) +\n  geom_segment(data = mydata_wide, aes(y = 0, yend = 0, \n                                       x = .75, \n                                       xend = 1.25 )) + \n  geom_text(aes(x = 1, y = 0.5, label = \"0\")) +\n  geom_segment(data = mydata_wide, aes(y = 0.3, yend = 0.3, \n                                       x = 1.75 , \n                                       xend = 2.25 )) + \n  geom_text(aes(x = 2, y = 0.6, label = \"0.3\")) +\n  geom_segment(data = mydata_wide, aes(y = 0.5, yend = 0.5, \n                                       x = 2.75, \n                                       xend = 3.25 )) + \n  geom_text(aes(x = 3, y = 0.8, label = \"0.5\")) +\n  labs(title = \"Original Data\") +\n  ylab(\"Response Variable\")\n\np2 <- mydata_long %>% \n  group_by(group) %>% \n  mutate( s_value = signed_rank(value)) %>% \n  ggplot(., aes(x = group, y = s_value)) + \n  geom_jitter(width = 0.02, height = 0,aes(colour = group), size = 4) + \n  stat_summary(fun = \"mean\", geom = \"point\", colour = \"red\", \n               size = 8) + \n  labs(title = \"Signed Rank of Data\") +\n  ylab(\"Signed Rank of Response Variable\")\n\npatchwork::wrap_plots(p1,p2, nrow = 1, guides = \"collect\")"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#how-does-sign-rank-data-work",
    "href": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#how-does-sign-rank-data-work",
    "title": "Nonparametric Tests",
    "section": "How does Sign-Rank data work?",
    "text": "How does Sign-Rank data work?\nTBD: need to add some explanation here."
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#linear-models-as-hypothesis-tests",
    "href": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#linear-models-as-hypothesis-tests",
    "title": "Nonparametric Tests",
    "section": "Linear Models as Hypothesis Tests",
    "text": "Linear Models as Hypothesis Tests\nUsing linear models is based on the idea of Testing of Hypotheses. The Hypothesis Testing method typically defines a NULL Hypothesis where the statements read as ‚Äúthere is no relationship‚Äù between the variables at hand, explanatory and responses. The Alternative Hypothesis typically states that there is a relationship between the variables.\nAccordingly, in fitting a linear model, we follow the process as follows:\n\nMake the following hypotheses: \\[\ny = \\beta_0 + \\beta_1 *x \\\\\nNULL\\ Hypothesis\\ H_0 => x\\ and\\ y\\ are\\ unrelated.\\ (\\beta_1 = 0)\n\\] \\[\ny = \\beta_0 + \\beta_1 *x \\\\\nAlternate\\ Hypothesis\\ H_1 => x\\ and\\ y\\ are\\ linearly\\ related\\ (\\beta_1 \\ne 0)\n\\]\n\nWe ‚Äúassume‚Äù that \\(H_0\\) is true.\nWe calculate \\(\\beta_1\\).\nWe then find probability p that [\\(\\beta_1 = Estimated\\ Value\\)] when the NULL Hypothesis is assumed TRUE. This is the p-value. If that probability is p>=0.05, we say we ‚Äúcannot reject‚Äù \\(H_0\\) and there is unlikely to be significant linear relationship.\n\nHowever, if p<= 0.05 can we reject the NULL hypothesis, and say that there could be a significant linear relationship,because \\(\\beta_1 = Estimated\\ Value\\) by mere chance under \\(H_0\\) is very small.\nPheeew !!"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#linear-models-in-r",
    "href": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#linear-models-in-r",
    "title": "Nonparametric Tests",
    "section": "Linear Models in R",
    "text": "Linear Models in R\nlm() is the function to create linear models in R. In R we are lazy and write :\n\\[\ny \\sim 1 + x\\\\\nwhich\\ reads\\ like\\\\\ny = 1*number + x* another\\ number\n\\]\nNote: there are very many ways in which linear models can be coded in R. See Vito Ricci on CRAN.\n\nShow the Code# using lm()\nlm(y ~ 1 + x, data = mydata_wide) %>% \n  summary() %>%   \n  print(digits = 5)\n\n\nCall:\nlm(formula = y ~ 1 + x, data = mydata_wide)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3.93980 -1.09967  0.12548  1.27904  3.88372 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)  0.30000    0.27799  1.0792   0.2859\nx           -0.46355    0.28081 -1.6507   0.1053\n\nResidual standard error: 1.9657 on 48 degrees of freedom\nMultiple R-squared:  0.05372,   Adjusted R-squared:  0.034006 \nF-statistic:  2.725 on 1 and 48 DF,  p-value: 0.10532\n\n\nSince the p-value is >=0.05, we fail to reject the NULL Hypothesis that there is no relationship between x and y."
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#assumptions-in-linear-models",
    "href": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#assumptions-in-linear-models",
    "title": "Nonparametric Tests",
    "section": "Assumptions in Linear Models",
    "text": "Assumptions in Linear Models\n\n\nL: \\(\\color{blue}{linear}\\) relationship\n\nI: Errors are independent (across observations)\n\nN: y is \\(\\color{red}{normally}\\) distributed at each ‚Äúlevel‚Äù of\n\n\n\n\n\nE: equal variance at all levels of x. No heteroscedasticity. \n\n\nLet us now see which standard statistical tests can be re-formulated as Linear Models."
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#pearson-correlation",
    "href": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#pearson-correlation",
    "title": "Nonparametric Tests",
    "section": "Pearson Correlation",
    "text": "Pearson Correlation\nModel\nThe model for Pearson Correlation tests is exactly the Linear Model:\n\\[\ny = \\beta_0 + \\beta_1 * x\n\\\\\nH_0: \\beta_1 = 0\n\\]\nSee the Code section for further insights into the relationship between the Correlation Score and the Slope of the Linear Model.\nCode\n\nShow the Code# Pearson (built-in test)\ncor <- cor.test(y,x,method = \"pearson\") %>% \n  broom::tidy() %>% select(estimate, p.value)\n\n# Linear Model\nlin <- lm(y ~ 1 + x, data = mydata_wide) %>% \n  broom::tidy() %>% select(estimate, p.value)\n\n# Scaled linear model\nlin_scl <- lm(scale(y) ~ 1 + scale(x), data = mydata_wide) %>% \n  broom::tidy() %>% select(estimate, p.value)\n\nprint(cor)\n\n# A tibble: 1 √ó 2\n  estimate p.value\n     <dbl>   <dbl>\n1   -0.232   0.105\n\nShow the Codeprint(lin)\n\n# A tibble: 2 √ó 2\n  estimate p.value\n     <dbl>   <dbl>\n1    0.3     0.286\n2   -0.464   0.105\n\nShow the Codeprint(lin_scl)\n\n# A tibble: 2 √ó 2\n   estimate p.value\n      <dbl>   <dbl>\n1 -9.06e-17   1    \n2 -2.32e- 1   0.105\n\nShow the Code# All together\nrbind(cor, lin, lin_scl) %>% print()\n\n# A tibble: 5 √ó 2\n   estimate p.value\n      <dbl>   <dbl>\n1 -2.32e- 1   0.105\n2  3   e- 1   0.286\n3 -4.64e- 1   0.105\n4 -9.06e-17   1    \n5 -2.32e- 1   0.105\n\n\nNotes: 1. The p-value for Pearson Correlation and that for the slope in the linear model is the same ( 0.1053 ). Which means we cannot reject the NULL hypothesis of ‚Äúno relationship‚Äù.\n\nHere is the relationship between the slope and correlation:\n\n\\[\nSlope\\ \\beta_1 = \\frac{sd_y}{sd_x} * r\n\\]\nWhen both x and y have the same standard deviation, the slope and correlation are the same. Here, since x has twice the sd of y, the ratio of slope = -0.4635533 to r = -0.2317767 is 0.5. Hence a linear model using scale() for both variables will show slope = r.\nSlope_Scaled: -0.2317767 = Correlation: -0.2317767\nExample\nWe choose to look at the gpa_study_hours dataset. It has two numeric columns gpa and study_hours:\n\nShow the Codeglimpse(gpa_study_hours)\n\nRows: 193\nColumns: 2\n$ gpa         <dbl> 4.000, 3.800, 3.930, 3.400, 3.200, 3.520, 3.680, 3.400, 3.‚Ä¶\n$ study_hours <dbl> 10, 25, 45, 10, 4, 10, 24, 40, 10, 10, 30, 7, 15, 60, 10, ‚Ä¶\n\n\n\nShow the Code# Checks for Normal/Symmetric distributions\np1 <- ggplot(gpa_study_hours) + geom_histogram(aes(gpa))\np2 <- ggplot(gpa_study_hours) + geom_histogram(aes(study_hours))\np3 <- ggplot(gpa_study_hours) + geom_point(aes(gpa, study_hours))\n(p1 + p2) / p3\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nHmm‚Ä¶not normally distributed, and the relationship is also not linear, and there is some evidence of heterscedasticity, so Pearson correlation would not be the best idea here.\n\nShow the Code# Pearson Correlation as Linear Model\nlm(gpa ~ study_hours, data = gpa_study_hours) %>% summary()\n\n\nCall:\nlm(formula = gpa ~ study_hours, data = gpa_study_hours)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.95130 -0.19456  0.03879  0.21708  0.73872 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 3.527997   0.037424  94.272   <2e-16 ***\nstudy_hours 0.003328   0.001794   1.855   0.0652 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2837 on 191 degrees of freedom\nMultiple R-squared:  0.01769,   Adjusted R-squared:  0.01255 \nF-statistic:  3.44 on 1 and 191 DF,  p-value: 0.06517\n\nShow the Code# Other ways using other packages\nmosaic::cor_test(gpa ~ study_hours, data = gpa_study_hours)\n\n\n    Pearson's product-moment correlation\n\ndata:  gpa and study_hours\nt = 1.8548, df = 191, p-value = 0.06517\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.008383868  0.269196552\nsample estimates:\n      cor \n0.1330138 \n\nShow the CodestatsExpressions::corr_test(data = gpa_study_hours, \n                            x = study_hours, \n                            y = gpa)\n\n\n\n  \n\n\n\nThe correlation estimate is \\(0.133\\); the p-value is 0.065 and the confidence interval includes 0. Hence we fail to reject the NULL hypothesis that study_hours and gpa have no relationship.\nWe can use a later package ggstaplot to plot this:\n\nShow the Codeggstatsplot::ggscatterstats(data = gpa_study_hours, \n                            x = study_hours, \n                            y = gpa,\n                            type = \"robust\",\n                            marginal = TRUE,\n                            title = \"GPA vs Study Hours\")\n\nRegistered S3 method overwritten by 'ggside':\n  method from   \n  +.gg   ggplot2\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#spearman-correlation",
    "href": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#spearman-correlation",
    "title": "Nonparametric Tests",
    "section": "Spearman Correlation",
    "text": "Spearman Correlation\nModel\nIn some cases the LINE assumptions may not hold. Nonlinear relationships, non-normally distributed data ( with large outliers ) and working with ordinal rather than continuous data: these situations necessitate the use of Spearman‚Äôs ranked correlation scores. (Ranked, not sign-ranked.)\n\\[\nrank(y) = \\beta_0 + \\beta_1 * rank(x) \\\\\nH_0: \\beta_1 = 0\n\\]\nSpearman correlation = Pearson correlation using the rank of the data observations. Let‚Äôs check how this holds for a our x and y data:\n\nShow the Code# Plot the data\np1 <- ggplot(mydata_wide, aes(x, y)) + \n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  ggtitle(\" Pearson Correlation\\n and Linear Models\")\n\n# Plot ranked data\np2 <- mydata_wide %>% \n  mutate(x_rank = rank(x),\n         y_rank = rank(y)) %>%\n  ggplot(.,aes(x_rank, y_rank)) + \n  geom_point(shape = 15, size = 2) +\n  geom_smooth(method = \"lm\") + \n  ggtitle(\" Spearman Ranked Correlation\\n and Linear Models\")\n\npatchwork::wrap_plots(p1,p2, nrow = 1, guides = \"collect\")\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nSlopes are almost identical, ~ 0.25.\nCode\n\nShow the Code# Spearman\ncor1 <- cor.test(y,x, method = \"spearman\") %>% \n  broom::tidy() %>% select(estimate, p.value)\n\n# Pearson using ranks\ncor2 <- cor.test(rank(y), rank(x), method = \"pearson\") %>% \nbroom::tidy() %>% select(estimate, p.value)\n\n# Linear Models using rank\ncor3 <- lm(rank(y) ~ 1 + rank(x),data = mydata_wide) %>% \n  broom::tidy() %>% select(estimate, p.value)\n\nrbind(cor1, cor2, cor3) %>% print()\n\n# A tibble: 4 √ó 2\n  estimate  p.value\n     <dbl>    <dbl>\n1   -0.227 1.13e- 1\n2   -0.227 1.14e- 1\n3   31.3   9.11e-10\n4   -0.227 1.14e- 1\n\n\nNotes:\n\nWhen ranks are used, the slope of the linear model (\\(\\beta_1\\)) has the same value as the correlation coefficient ( \\(\\rho\\) ).\nNote that the slope from the linear model now has an intuitive interpretation: the number of ranks y changes for each change in rank of x. ( Ranks are ‚Äúindependent‚Äù of sd )\nExample\nWe examine the cars93 data, where the numeric variables of interest are weight and price.\n\nShow the Codecars93 %>% \n  ggplot(aes(weight, price)) + \n  geom_point() + geom_smooth(method = \"lm\", se = FALSE, lty = 2) + \n  labs(title = \"Car Weight and Car Price have a nonlinear relationship\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nLet us try a Spearman Correlation score for these variables, since the data are not linearly related and the variance of price also is not constant over weight\n\nShow the Codecor.test(cars93$price, cars93$weight, method = \"spearman\") %>% broom::tidy()\n\nWarning in cor.test.default(x, y, ...): Cannot compute exact p-value with ties\n\n\n\n\n  \n\n\nShow the Code# Using linear Model\nlm(rank(price) ~ rank(weight), data = cars93) %>% summary()\n\n\nCall:\nlm(formula = rank(price) ~ rank(weight), data = cars93)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-20.0676  -3.0135   0.7815   3.6926  20.4099 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   3.22074    2.05894   1.564    0.124    \nrank(weight)  0.88288    0.06514  13.554   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.46 on 52 degrees of freedom\nMultiple R-squared:  0.7794,    Adjusted R-squared:  0.7751 \nF-statistic: 183.7 on 1 and 52 DF,  p-value: < 2.2e-16\n\nShow the Code# Stats Plot\nggstatsplot::ggscatterstats(data = cars93, x = weight, \n                            y = price,\n                            type = \"nonparametric\",\n                            title = \"Cars93: Weight vs Price\",\n                            subtitle = \"Spearman Correlation\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nWe see that using ranks of the price variable, we obtain a Spearman‚Äôs \\(\\rho = 0.882\\) with a p-value that is very small. Hence we are able to reject the NULL hypothesis and state that there is a relationship between these two variables. The linear relationship is evaluated as a correlation of 0.882."
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#the-students-t-test-with-one-sample",
    "href": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#the-students-t-test-with-one-sample",
    "title": "Nonparametric Tests",
    "section": "The Student‚Äôs t-test with one sample",
    "text": "The Student‚Äôs t-test with one sample\nModel\nA single number predicts y:\n\\[\ny = \\beta_0 + \\beta_1*x \\\\\n\\\\and\\ further \\ actually\\\\\ny = \\beta_0\n\\]\nand the second term vanishes, since ‚Äúthere is no x‚Äù: all the x-values are made equal to zero in the linear model !! The NULL Hypothesis therefore is:\n\\[\n\\ H_0: \\beta_0 = 0\n\\]\nThis NULL Hypothesis makes sense, because in the accompanying linear model all values of the explanatory variable x are zero, and therefore the NULL Hypothesis for the model should be that y also should be zero mean. Note that if we want the NULL hypothesis to be that the mean is other than zero, we can use the lm(...., mu = some_number, ..) parameter in the command.\nCode\nIf we compare the t.test with the appropriate lm model:\n\nShow the Code# t-test\nt1 <- t.test(y, mu = 0, alternative = \"two.sided\")\nprint(t1)\n\n\n    One Sample t-test\n\ndata:  y\nt = 1.0607, df = 49, p-value = 0.294\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.2683937  0.8683937\nsample estimates:\nmean of x \n      0.3 \n\nShow the Code# linear model\nlm1 <- lm(y ~ 1, data = mydata_wide)\nlm1 %>% summary()\n\n\nCall:\nlm(formula = y ~ 1, data = mydata_wide)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5554 -1.4845 -0.0392  1.5559  4.5119 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   0.3000     0.2828   1.061    0.294\n\nResidual standard error: 2 on 49 degrees of freedom\n\nShow the Codelm1 %>% confint()\n\n                 2.5 %    97.5 %\n(Intercept) -0.2683937 0.8683937\n\n\nThe confidence intervals for both the t.test and the lm model are identical.\nt-test confidence intervals: -0.2683937, 0.8683937\nlinear model confidence intervals: -0.2683937, 0.8683937\nSo even though y has a mean of 0.3, the confidence intervals straddle zero, and hence we cannot reject the NULL hypothesis that the true population, of which y is a sample, could have mean=0.\nExample\n\nShow the Codeexam_grades"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#wilcoxons-signed-rank-test",
    "href": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#wilcoxons-signed-rank-test",
    "title": "Nonparametric Tests",
    "section": "Wilcoxon‚Äôs Signed-Rank Test",
    "text": "Wilcoxon‚Äôs Signed-Rank Test\nSince we are dealing with the mean, the sign of the rank becomes important to use, in the case of a non-parametric single mean test.\nModel\n\\[\nsigned\\_rank(y) = \\beta_0 \\\\\nH_0: \\beta_0 = 0\n\\]\nCode\n\nShow the Code# Standard Wilcoxon Signed_Rank Test\nw1 <- wilcox.test(y)\nw1\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  y\nV = 754, p-value = 0.2628\nalternative hypothesis: true location is not equal to 0\n\nShow the Code# Wilcoxon test with lm\nw2 <- lm(signed_rank(y) ~ 1 , data = mydata_wide)\nw2\n\n\nCall:\nlm(formula = signed_rank(y) ~ 1, data = mydata_wide)\n\nCoefficients:\n(Intercept)  \n       4.66  \n\nShow the Code# t-test with signed_rank data\nw3 <- t.test(signed_rank(y))\nw3\n\n\n    One Sample t-test\n\ndata:  signed_rank(y)\nt = 1.1277, df = 49, p-value = 0.265\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -3.644491 12.964491\nsample estimates:\nmean of x \n     4.66 \n\n\nWe can plot the y data both original and ranked to see where the mean lies in each case. The approximation to the true $_0¬†( is good when the number of observations N is >=50. Lindoloev has a simulation on this.. We can also plot the model using lm for both the original data and the sign-ranked data.\nExample\nPlots for both t-test and Wilcoxon test\n\nShow the Codep1 <- ggplot(mydata_wide, aes( x = 0, y = y)) +\n  geom_point(alpha = 0.4) +\n  geom_segment(aes(y = t1$estimate, \n                   yend = t1$estimate, \n                   x = -0.2, xend = 0.2)) + \n  labs(title = \"Student's\\n t-Test\")\n\n# t-test using linear model\np2 <- ggplot(mydata_wide, aes( x = 0, y = y)) +\n  geom_point(alpha = 0.4) +\n  geom_segment(aes(y = lm(y ~ 1)$coefficient, \n                   yend = lm(y ~ 1)$coefficient, \n                   x = -0.2, xend = 0.2)) + \n  labs(title = \"Student's\\n t-Test \\n using lm\")\n\n# Wilcoxon test, using signed-ranks of data\np3 <- ggplot(mydata_wide, aes( x = 0, y = signed_rank(y))) +\n  geom_point(alpha = 0.4) +\n  geom_segment(aes(y = mean(signed_rank(y)), yend = mean(signed_rank(y)), x = -0.2, xend = 0.2)) + \n  labs(title = \"Wilcoxon \\nSigned-Rank\\n Test\")\n\n# Wilcoxon test, using signed-ranks of data, and lm\np4 <- ggplot(mydata_wide, aes( x = 0, y = signed_rank(y))) +\n  geom_point(alpha = 0.4) +\n  geom_segment(aes(y = lm(signed_rank(y) ~1)$coefficient, \n                   yend = lm(signed_rank(y) ~1)$coefficient, \n                   x = -0.2, xend = 0.2)) + \n  labs(title = \"Wilcoxon \\n Signed-Rank \\n Test with lm\")\n\n\npatchwork::wrap_plots(p1,p2,p3,p4, nrow = 1, guides = \"collect\")"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#paired-sample-t-test",
    "href": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#paired-sample-t-test",
    "title": "Nonparametric Tests",
    "section": "Paired Sample t-test",
    "text": "Paired Sample t-test\nWe use this when we have two samples and the observations from one sample can be ‚Äúpaired‚Äù with observations in the other sample. Controlled studies for interventions/measures, such as before/after kind of data, comparisons between two interventions on the same set of subjects, and two measurements made on the same subjects using different methods etc.\nModel\n\\[\ny_2 - y_1 = \\beta_0 \\\\\nH_0 : \\beta_0 = 0\n\\]\nThe NULL Hypothesis is that there is no difference, either way, between the two samples. Again, in the linear model, we assume as before that ‚Äúthe explanatory x variable has been equated to zero.\nWe therefore set two.sided and mu = 0 in the t.test.\nCode\n\nShow the Code# Using paired t-test\nt2 <- t.test(y2, y, paired = TRUE, mu = 0, \n             alternative = \"two.sided\")\nt2\n\n\n    Paired t-test\n\ndata:  y2 and y\nt = 0.54264, df = 49, p-value = 0.5898\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -0.54067  0.94067\nsample estimates:\nmean difference \n            0.2 \n\nShow the Code# linear model\nlm2 <- lm(y2-y ~ 1, data = mydata_wide)\nlm2 %>% summary()\n\n\nCall:\nlm(formula = y2 - y ~ 1, data = mydata_wide)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.7027 -2.1872 -0.1367  1.3835  5.7262 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   0.2000     0.3686   0.543     0.59\n\nResidual standard error: 2.606 on 49 degrees of freedom\n\nShow the Codelm2 %>% confint()\n\n               2.5 %  97.5 %\n(Intercept) -0.54067 0.94067\n\n\nBoth tests report the difference to be 0.2. However the p-value in both tests is about 0.6, so the result is not statistically significant.\nExample"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#wilcoxon-paired-test",
    "href": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#wilcoxon-paired-test",
    "title": "Nonparametric Tests",
    "section": "Wilcoxon Paired Test",
    "text": "Wilcoxon Paired Test\nWhen the original data is not normally distributed or has outliers etc,. we use a nonparametric Wilcoxon paired test. The difference between the paired and unpaired Wilcoxon test is that the test is run on the signed-ranks of the pairwise differences y2- y.\nModel\n$$ signed_rank(y2 - y1) = _0 \\\nH_0: _0 = 0\n$$\nCode\n\nShow the Code# Paired Wilcoxon Test\nw4 <- wilcox.test(y, y2, paired = TRUE)\nw4\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  y and y2\nV = 608, p-value = 0.7795\nalternative hypothesis: true location shift is not equal to 0\n\nShow the Code# Linear Model\nlm4 <- lm(signed_rank(y2-y) ~ 1 , data = mydata_wide)\nlm4 %>% summary()\n\n\nCall:\nlm(formula = signed_rank(y2 - y) ~ 1, data = mydata_wide)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-48.18 -27.93   0.82  22.57  48.82 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)    1.180      4.182   0.282    0.779\n\nResidual standard error: 29.57 on 49 degrees of freedom\n\nShow the Code# t-test with Signed Rank\nt4 <- t.test(signed_rank(y2-y), mu = 0 , alternative = \"two.sided\")\nt4\n\n\n    One Sample t-test\n\ndata:  signed_rank(y2 - y)\nt = 0.28214, df = 49, p-value = 0.779\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -7.224733  9.584733\nsample estimates:\nmean of x \n     1.18 \n\n\nHere too, the p-values reported by the three tests are p = 0.779 so the difference reported is not significant.\nExample"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#dummy-group-variable-concept",
    "href": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#dummy-group-variable-concept",
    "title": "Nonparametric Tests",
    "section": "Dummy Group Variable Concept",
    "text": "Dummy Group Variable Concept\nAn important construct here is the dummy variable. When there is more than one group in the data, a dummy categorical variable is set up, whose entries specify the group ID. The group IDs are still numerical ( as with factors, remember ). The dummy variable is plotted on the x-axis. The consecutive IDs in the dummy variable x are separated by 1. Hence the between-groups difference in the stat measures computed on y are numerically equivalent to the slope in the linear model. Thus the dummy variable allows us to **mathematically* use the linear model, as presented in the equations above.\nDummy variables become even more useful when the explanatory variable ( ‚Äúx-planatory‚Äù ) is already categorical, as with ANOVA and friends.\nWe can visualize this as follows:\n\nShow the Codemydata_wide_new <- \n  tibble(y1 = rnorm(50, mean = 0, sd = 0.5),\n         y2 = rnorm(50, mean = 1.2, sd = 0.5)) %>%\n  pivot_longer(cols = c(y1, y2),\n               names_to = \"variable\",  \n               values_to = \"values\") %>% \n  cbind(group = rep(0:1, 50))\n\nmydata_wide_new %>% \n  ggplot(aes(x = group, y = values)) + \n  geom_point() + \n  stat_summary(fun = \"mean\", colour = \"red\", size = 4, geom =\"point\") +\n  stat_summary(fun = \"mean\", geom= \"line\", colour = \"blue\", lty = 2) +\n  xlab(\"Dummy Variable to show groups\") +\n  ylab(\"y1 and y2, on the same scale\") +\n  scale_x_discrete(name = \"Dummy Variable x_i  [0,1]\",\n                limits = c(0,1)) +\n  annotate(\"text\", x = 0, y = 1.5, label = \"Difference in means \\n equals slope in linear model\") \n\nWarning: Continuous limits supplied to discrete scale.\n‚Ñπ Did you mean `limits = factor(...)` or `scale_*_continuous()`?\n\n\n\n\n\n\n\nShow the Code# Need to use `glue` here to add more annotations\n# Math annotation on graphs"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#model-7",
    "href": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#model-7",
    "title": "Nonparametric Tests",
    "section": "Model",
    "text": "Model\n\\[\ny_i = \\beta_0 + \\beta_1 * x_i \\\\\nwhere\\\\\nx_i= \\left\\{\\begin{matrix}\n1\\ when\\ x\\ \\in\\ Group 1\\\\\n0\\ when\\ x\\ \\in\\ Group2\n\\end{matrix}\\right.\n\\]"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#independent-t-test",
    "href": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#independent-t-test",
    "title": "Nonparametric Tests",
    "section": "Independent t-test",
    "text": "Independent t-test\nModel\nThe assumptions here are: - both data sets are normally distributed. Small samples may be assumned to be t-distributed - variances are the same - no outliers - observations across data sets are independent (obviously)\n\\[\ny_i = \\beta_0 + \\beta_1 * x_i \\\\\nwhere \\\\\nx_i= \\left\\{\\begin{matrix}\n1\\ when\\ x\\ \\in\\ Group 1\\\\\n0\\ when\\ x\\ \\in\\ Group2\n\\end{matrix}\\right.\n\\\\\nH_0 : \\beta_1 = 0\n\\]\nThe t.test computes a statistic as follows:\n\\[\nt \\ \\ = \\mod(\\bar{x_1}\\ - \\bar{x_2}) / std.error(\\bar{x_1}\\ - \\bar{x_2}) \\\\\n= \\mod(\\bar{x_1} - \\bar{x_2})\\ / \\sqrt{s_x^2 /n_x + s_y^2/n_y}  \\\\\nand\\\\\ndf = n_1 + n_2 - 2\\ \\ \\ \\ ( degrees\\ of\\ freedom)\n\\]\nThe t-test uses an approximation to the sampling distribution of the difference in sample means based on the Central Limit Theorem, which ensures that for sufficiently large samples, the sampling distribution will be very close to Normal. The mean of the sampling distribution will be the difference in (underlying) population means, and the variance of the sampling distribution will be the standard error of the difference in sample means.\nFor the t-statistic, note that the numerator of the formula is the difference between means. The denominator is a measurement of experimental error in the two groups combined. The wider the difference between means, the more confident you are in the data. The more experimental error you have, the less confident you are in the data. Thus the higher the value of t, the greater the confidence that there is a difference.\nThe t-statistic has a t-distribution. It is compared to a critical value of t, for a given probability value ( 0.05 usually). If the calculated t exceeds the critical value, we can assert that the NULL Hypothesis can be rejected and there could be a significant difference in means. \nCode\n\nShow the Code# Independent t-test\nt5 <- t.test(y2, y, var.equal = TRUE)\nt5 %>% tidy() \n\n\n\n  \n\n\nShow the Code# Welch test when variances are not equal\nt6 <- t.test(y2,y, var.equal = FALSE)\nt6 %>% tidy()\n\n\n\n  \n\n\nShow the Code# Linear Model with Dummy variable\n# lm(value ~ 1 + group, data = mydata_long_y) # also works\nlm6 <- lm(value ~ 1 + I(group == \"y2\"), data = mydata_long_y)\nlm6 %>% tidy()\n\n\n\n  \n\n\n\nWe get the same estimates for means of y and y2 ( 0.3 and 0.5 respectively).\nExample"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#welchs-t-test",
    "href": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#welchs-t-test",
    "title": "Nonparametric Tests",
    "section": "Welch‚Äôs t-test",
    "text": "Welch‚Äôs t-test\nModel\nWelch‚Äôs test we have already explored as a variant of the t.test for two means, with variances unequal.\nWelch‚Äôs test is also stated as a Generalized Linear Model using, not lm but the gls command from the nlme package. This is explained on StackExchange but we need not digress now.\nWhen the variances are unequal, there is a difference in the t-statistic that is computed only when the group **sizes* are different ( Denominator of t-statistic ).\nFrom StackExchange:\\[\nt_w =\\ \\frac {\\bar{x_1}-\\bar{x_2}}{\\sqrt{{s_1^2/n_1}{s_2^2/n_2}}}\n\\\\\\\\\n=\\ \\frac{\\bar{x_1}-\\bar{x_2}} {{\\sqrt{\\frac{s_1^2 + s_2^2}{n}}}}\n\\\\\\\\\n=\\ \\frac{\\bar{x_1}-\\bar{x_2}} {{\\sqrt{\\frac{s_1^2 + s_2^2} {2} * (\\frac{2}{n})}}}\n\\\\\\\\\n=\\ t_s\n\\]\nSo under these conditions the t-statistic for the Welch t-test is the same as that for the standard t-test.\nCode\n\nShow the Codet7 <- t.test(y, y2, var.equal = FALSE)\n\n\nExample"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#mann-whitney-u-test",
    "href": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#mann-whitney-u-test",
    "title": "Nonparametric Tests",
    "section": "Mann-Whitney U Test",
    "text": "Mann-Whitney U Test\n(Wilcoxon Independent Sample test)\nAs before, when sample groups are not normally distributed, and when variances are different, and they are outliers, a nonparametric rank-sum test is preferred. This is the same as the Wilcoxon Test for independent variables, and is called the Mann-Whitney Test.\nModel\nAs before:\n\\[\nrank(y_i) = \\beta_0 + \\beta_1 * rank(x_i) \\\\\nwhere \\\\\nx_i= \\left\\{\\begin{matrix}\n1\\ when\\ x\\ \\in\\ Group 1\\\\\n0\\ when\\ x\\ \\in\\ Group2\n\\end{matrix}\\right.\n\\\\\nH_0 : \\beta_1 = 0\n\\]\nCode\n\nShow the Code# As Wilcoxon Test\nw5 <- wilcox.test(y2,y, paired = FALSE)\nw5 %>% tidy()\n\n\n\n  \n\n\nShow the Code# As Linear model\nlm5 <- lm(rank(value) ~ 1 + I(group == \"y2\"), \n          data = mydata_long_y)\nlm5 %>% tidy()\n\n\n\n  \n\n\n\nNot clear how to explain this. Need to dig more into Mann-Whitney.\nExample"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#model-11",
    "href": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#model-11",
    "title": "Nonparametric Tests",
    "section": "Model",
    "text": "Model\nANOVAs are linear models with (only) categorical predictors so they simply extend everything we did above, relying heavily on dummy coding."
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#one-way-anova",
    "href": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#one-way-anova",
    "title": "Nonparametric Tests",
    "section": "One-way ANOVA",
    "text": "One-way ANOVA\nModel\nOne mean for each group predicts y.\n\\[\ny=\\beta_0 + \\beta_1*x_1 + \\beta_2*x_2...+\\beta_n*x_n\\\\\nH0:y=Œ≤0\n\\]\nwhere \\(x_i\\) are indicators (x=0 or x=1) where at most one \\(x_i=1\\) while all others are \\(x_i=0\\).\nNotice how this is just ‚Äúmore of the same‚Äù of what we already did in other models above. When there are only two groups, this model is \\(y=Œ≤0+Œ≤1‚àóx\\), i.e.¬†the independent t-test. If there is only one group, it is \\(y=Œ≤0\\), i.e.¬†the one-sample t-test. This makes one-way ANOVA a multiple regression model.\nThis is easy to see in the visualization below - just cover up a few groups and see that it matches the other visualizations above. Let‚Äôs visualize this using toy data:\n\nShow the CodeN = 15\nD_anova1 = data.frame(\n  y = c(\n    rnorm_fixed(N, 0.5, 0.3),\n    rnorm_fixed(N, 0, 0.3),\n    rnorm_fixed(N, 1, 0.3),\n    rnorm_fixed(N, 0.8, 0.3)\n  ),\n  x = rep(0:3, each = 15)\n)\nymeans = aggregate(y~x, D_anova1, mean)$y\nP_anova1 = ggplot(D_anova1, aes(x=x, y=y)) + \n  stat_summary(fun.y=mean, geom = \"errorbar\", aes(ymax = ..y.., ymin = ..y.., color='intercepts'), lwd=2) + \n  \n  geom_segment(x = -10, xend = 100, \n               y = 0.5, yend = 0.5, \n               lwd = 2, aes(color = 'beta_0')) +\n  geom_segment(x = 0, xend = 1, \n               y = ymeans[1], yend = ymeans[2], \n               lwd = 2, aes(color = 'betas')) +\n  geom_segment(x = 1, xend = 2, \n               y = ymeans[1], yend = ymeans[3], \n               lwd = 2, aes(color = 'betas')) +\n  geom_segment(x = 2, xend = 3, \n               y = ymeans[1], yend = ymeans[4], \n               lwd = 2, aes(color = 'betas')) +\n  \n  scale_color_manual(name = NULL, \n                     values = c(\"blue\", \"red\", \"darkblue\"),\n                     labels=c(bquote(beta[0]*\" (group 1 mean)\"),\n                              bquote(beta[1]*\", \"*beta[2]*\", \n                                     etc. (slopes/differences to \"*beta[0]*\")\"),\n      bquote(beta[0]*\"+\"*beta[1]*\", \"*beta[0]*\"+\"*beta[2]*\", etc. (group 2, 3, ... means)\")\n    )\n  )\n\nWarning: The `fun.y` argument of `stat_summary()` is deprecated as of ggplot2 3.3.0.\n‚Ñπ Please use the `fun` argument instead.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\nShow the CodeP_anova1\n\nWarning: The dot-dot notation (`..y..`) was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `after_stat(y)` instead.\n\n\n\n\n\n\n\n\nCode\nExample"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#two-way-anova",
    "href": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#two-way-anova",
    "title": "Nonparametric Tests",
    "section": "Two-way ANOVA",
    "text": "Two-way ANOVA\nModel\nCode\nExample"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#ancova",
    "href": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#ancova",
    "title": "Nonparametric Tests",
    "section": "ANCOVA",
    "text": "ANCOVA\nModel\nCode\nExample"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#model-15",
    "href": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#model-15",
    "title": "Nonparametric Tests",
    "section": "Model",
    "text": "Model"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#discrete-variables",
    "href": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#discrete-variables",
    "title": "Nonparametric Tests",
    "section": "Discrete Variables",
    "text": "Discrete Variables\nModel\nCode\nExample"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#continuous-variables",
    "href": "content/courses/Basics-of-Modeling/Modules/80-Nonparametric-Tests/index.html#continuous-variables",
    "title": "Nonparametric Tests",
    "section": "Continuous Variables",
    "text": "Continuous Variables\nModel\nCode\nExample"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/listing.html",
    "href": "content/courses/Descriptive-Analytics/listing.html",
    "title": "Descriptive Analytics",
    "section": "",
    "text": "Order By\nDefault\n\n          Date - Oldest\n        \n\n          Date - Newest\n        \n\n          Title\n        \n\n    \n      \n      \n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\n\n\nNov 12, 2022\n\n\nüêâ Introduction to Orange\n\n\n1 min\n\n\n\n\n\n¬†\n\n\n\nNov 11, 2022\n\n\nüêâ Introduction to Radiant\n\n\n0 min\n\n\n\n\n\n\n\nNov 1, 2021\n\n\nüï∂ Science, Human Experience, Experiments, and Data\n\n\n20 min\n\n\n\n\n\n¬†\n\n\n\nNov 14, 2022\n\n\nüêâ Introduction to R\n\n\n0 min\n\n\n\n\n\n¬†\n\n\n\nNov 15, 2022\n\n\nüìä Distributions\n\n\n6 min\n\n\n\n\n\n¬†\n\n\n\nNov 22, 2022\n\n\nüìé Correlations\n\n\n2 min\n\n\n\n\n\n¬†\n\n\n\nNov 22, 2022\n\n\nüï∏ Evolution and Flow\n\n\n2 min\n\n\n\n\n\n¬†\n\n\n\nNov 25, 2022\n\n\nüçï Parts of a Whole\n\n\n0 min\n\n\n\n\n\n¬†\n\n\n\nInvalid Date\n\n\nüñè Ratings and Rankings\n\n\n0 min\n\n\n\n\n\n¬†\n\n\n\nAug 15, 2022\n\n\nüó∫ Maps\n\n\n0 min\n\n\n\n\n\n¬†\n\n\n\nDec 10, 2022\n\n\nüïî Time Series\n\n\n0 min\n\n\n\n\n\n¬†\n\n\n\nNov 21, 2022\n\n\nüï∏ Networks\n\n\n0 min\n\n\n\n\n\n¬†\n\n\n\nDec 27, 2022\n\n\nüêâ Visualizing Categorical Data\n\n\n0 min\n\n\n\n\n\n¬†\n\n\n\nNov 15, 2022\n\n\nüï∏ Additional Reference Material\n\n\n0 min\n\n\n\n\n\n¬†\n\n\n\nNov 11, 2022\n\n\nüìö Miscellaneous Graphs and Tools\n\n\n0 min\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/05-Intro-to-Orange/index.html",
    "href": "content/courses/Descriptive-Analytics/Modules/05-Intro-to-Orange/index.html",
    "title": "üêâ Introduction to Orange",
    "section": "",
    "text": "Orange is a visual drag-and-drop tool for\n\nData visualization\nMachine Learning\nData mining\n\nand much more. Here is what it looks like:\n\nAll operations are done using a visual menu-driven interface. The visuals can be exported to PNG/SVG/PDF and the entire workflow can be exported into a Report Form and edited for presentation and sharing."
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/05-Intro-to-Orange/index.html#installing-orange",
    "href": "content/courses/Descriptive-Analytics/Modules/05-Intro-to-Orange/index.html#installing-orange",
    "title": "üêâ Introduction to Orange",
    "section": "\n2 Installing Orange",
    "text": "2 Installing Orange\nYou can download and install Orange from here:\nhttps://orangedatamining.com/download/"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/05-Intro-to-Orange/index.html#basic-usage-of-orange",
    "href": "content/courses/Descriptive-Analytics/Modules/05-Intro-to-Orange/index.html#basic-usage-of-orange",
    "title": "üêâ Introduction to Orange",
    "section": "\n3 Basic Usage of Orange",
    "text": "3 Basic Usage of Orange"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/05-Intro-to-Orange/index.html#orange-workflows",
    "href": "content/courses/Descriptive-Analytics/Modules/05-Intro-to-Orange/index.html#orange-workflows",
    "title": "üêâ Introduction to Orange",
    "section": "\n4 Orange Workflows",
    "text": "4 Orange Workflows"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/05-Intro-to-Orange/index.html#widgets-and-channels",
    "href": "content/courses/Descriptive-Analytics/Modules/05-Intro-to-Orange/index.html#widgets-and-channels",
    "title": "üêâ Introduction to Orange",
    "section": "\n5 Widgets and Channels",
    "text": "5 Widgets and Channels"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/05-Intro-to-Orange/index.html#loading-data-into-orange",
    "href": "content/courses/Descriptive-Analytics/Modules/05-Intro-to-Orange/index.html#loading-data-into-orange",
    "title": "üêâ Introduction to Orange",
    "section": "\n6 Loading data into Orange",
    "text": "6 Loading data into Orange\n\n\n\nWe are good to get started with Orange!!"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/05-Intro-to-Orange/index.html#simple-visuals-using-orange",
    "href": "content/courses/Descriptive-Analytics/Modules/05-Intro-to-Orange/index.html#simple-visuals-using-orange",
    "title": "üêâ Introduction to Orange",
    "section": "\n7 Simple Visuals using Orange",
    "text": "7 Simple Visuals using Orange\nLet us create some simple visualizations using Orange.\n\nUse the File Widget to import the iris dataset into your session\nUse the Data Table Widget to look at the data, and note its variable names\nUse the Visualization Widgets ( Scatter Plot, Bar Plot, and Distributions) to look at the properties of the variables, and examine relationships between them."
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/05-Intro-to-Orange/index.html#reference",
    "href": "content/courses/Descriptive-Analytics/Modules/05-Intro-to-Orange/index.html#reference",
    "title": "üêâ Introduction to Orange",
    "section": "\n8 Reference",
    "text": "8 Reference\n\nIntroduction to Data Mining-Working notes for the hands-on course with Orange Data Mining.(Download file)\nOrange Data Mining Widget Catalog: Look here for help and guidance! https://orangedatamining.com/widget-catalog/"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/10-Intro-to-Radiant/index.html",
    "href": "content/courses/Descriptive-Analytics/Modules/10-Intro-to-Radiant/index.html",
    "title": "üêâ Introduction to Radiant",
    "section": "",
    "text": "Radiant is an open-source platform-independent browser-based interface for business analytics in R.\nKey features\n\nExplore: Quickly and easily summarize, visualize, and analyze your data\nCross-platform: It runs in a browser on Windows, Mac, and Linux\nReproducible: Recreate results and share work with others as a state file or an Rmarkdown report\nProgramming: Integrate Radiant‚Äôs analysis functions with your own R-code\nContext: Data and examples focus on business applications\n\nRadiant can be used for a variety of tasks\n\nProbability and Stats\nData visualization\nMachine Learning\nData mining\nReport Generation.\n\nRadiant Workflows can also be exported to R/RStudio easily."
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/10-Intro-to-Radiant/index.html#installing-radiant",
    "href": "content/courses/Descriptive-Analytics/Modules/10-Intro-to-Radiant/index.html#installing-radiant",
    "title": "üêâ Introduction to Radiant",
    "section": "\n2 Installing Radiant",
    "text": "2 Installing Radiant\nYou can download and install Radiant from here:\nhttps://radiant-rstats.github.io/docs/install.html\nNOTE: - this automatically installs R, RStudio, and Radiant on your machine. This is going to be convenient when we start working in R too! - It also installs Latex, which allows us to create crisp PDF reports of our analyses."
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/10-Intro-to-Radiant/index.html#basic-tutorials-with-radiant",
    "href": "content/courses/Descriptive-Analytics/Modules/10-Intro-to-Radiant/index.html#basic-tutorials-with-radiant",
    "title": "üêâ Introduction to Radiant",
    "section": "\n3 Basic Tutorials with Radiant",
    "text": "3 Basic Tutorials with Radiant\nAll the Tutorials are available on Youtube; the links to individual videos are on the page below\nhttps://radiant-rstats.github.io/docs/radiant-tutorial-series.html"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/100-Networks/index.html",
    "href": "content/courses/Descriptive-Analytics/Modules/100-Networks/index.html",
    "title": "üï∏ Networks",
    "section": "",
    "text": "Slides and Tutorials\n.nbsp;.nbsp;\n.nbsp;.nbsp;"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/100-Networks/index.html#what-kind-network-graphs-will-we-make",
    "href": "content/courses/Descriptive-Analytics/Modules/100-Networks/index.html#what-kind-network-graphs-will-we-make",
    "title": "üï∏ Networks",
    "section": "\n1 What kind Network graphs will we make?",
    "text": "1 What kind Network graphs will we make?\nHere is a network map of the characters in Victor Hugo‚Äôs Les Miserables:"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/120-Categorical-Data/index.html",
    "href": "content/courses/Descriptive-Analytics/Modules/120-Categorical-Data/index.html",
    "title": "üêâ Visualizing Categorical Data",
    "section": "",
    "text": "{{% youtube \"7NhNeADL8fA\" %}}"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/120-Categorical-Data/index.html#a-workflow-with-orange",
    "href": "content/courses/Descriptive-Analytics/Modules/120-Categorical-Data/index.html#a-workflow-with-orange",
    "title": "üêâ Visualizing Categorical Data",
    "section": "\n2 A Workflow with Orange",
    "text": "2 A Workflow with Orange"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/120-Categorical-Data/index.html#a-workflow-with-radiant",
    "href": "content/courses/Descriptive-Analytics/Modules/120-Categorical-Data/index.html#a-workflow-with-radiant",
    "title": "üêâ Visualizing Categorical Data",
    "section": "\n3 A Workflow with Radiant",
    "text": "3 A Workflow with Radiant"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/120-Categorical-Data/index.html#a-workflow-with-r",
    "href": "content/courses/Descriptive-Analytics/Modules/120-Categorical-Data/index.html#a-workflow-with-r",
    "title": "üêâ Visualizing Categorical Data",
    "section": "\n4 A Workflow with R",
    "text": "4 A Workflow with R"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/120-Categorical-Data/index.html#conclusion",
    "href": "content/courses/Descriptive-Analytics/Modules/120-Categorical-Data/index.html#conclusion",
    "title": "üêâ Visualizing Categorical Data",
    "section": "\n5 Conclusion",
    "text": "5 Conclusion"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/15-Intro-to-R/index.html",
    "href": "content/courses/Descriptive-Analytics/Modules/15-Intro-to-R/index.html",
    "title": "üêâ Introduction to R",
    "section": "",
    "text": "Slides and Tutorials"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/15-Intro-to-R/index.html#introduction-to-r",
    "href": "content/courses/Descriptive-Analytics/Modules/15-Intro-to-R/index.html#introduction-to-r",
    "title": "üêâ Introduction to R",
    "section": "\n1 Introduction to R",
    "text": "1 Introduction to R\nWe have already installed R and RStudio when we installed Radiant!\nWe will get acquainted with data and its representations in R! We will also form a view of how English Grammar ( pronouns, verbs, adjectives,figures of speech..) get metaphorized into the R World!!"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/15-Intro-to-R/index.html#readings",
    "href": "content/courses/Descriptive-Analytics/Modules/15-Intro-to-R/index.html#readings",
    "title": "üêâ Introduction to R",
    "section": "\n2 Readings",
    "text": "2 Readings\n\nR for Data Science, Workflow: Basics Chapter: http://r4ds.had.co.nz/workflow-basics.html\nModern Dive, Getting Started Chapter: http://moderndive.com/2-getting-started.html\nR & RStudio Basics: https://bookdown.org/chesterismay/rbasics/3-rstudiobasics.html\nRStudio IDE Cheatsheet: https://github.com/rstudio/cheatsheets/blob/master/rstudio-ide.pdf"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/18-The-Nature-of-Data/index.html#fa-envelope-titlean-envelope-where-does-data-come-from",
    "href": "content/courses/Descriptive-Analytics/Modules/18-The-Nature-of-Data/index.html#fa-envelope-titlean-envelope-where-does-data-come-from",
    "title": "üï∂ Science, Human Experience, Experiments, and Data",
    "section": "\n2  Where does Data come from?",
    "text": "2  Where does Data come from?\nWe will need to form a basic understanding of basic scientific enterprise. Let us look at the slides."
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/18-The-Nature-of-Data/index.html#fa-chart-simple-why-visualize",
    "href": "content/courses/Descriptive-Analytics/Modules/18-The-Nature-of-Data/index.html#fa-chart-simple-why-visualize",
    "title": "üï∂ Science, Human Experience, Experiments, and Data",
    "section": "\n3  Why Visualize?",
    "text": "3  Why Visualize?\n\nWe can digest information more easily when it is pictorial\nOur Working Memories are both short-term and limited in capacity. So a picture abstracts the details and presents us with an overall summary, an insight, or a story that is both easy to recall and easy on retention.\n\nData Viz includes shapes that carry strong cultural memories and impressions for us. These cultural memories help us to use data viz in a universal way to appeal to a wide variety of audiences. (Do humans have a gene for geometry?)\nIt helps sift facts and mere statements: for example:\n\n\n\nRape Capital\n\n\n\n\nWhat does Data Reveal?"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/18-The-Nature-of-Data/index.html#iconify-mdi-category-plus-what-are-data-types",
    "href": "content/courses/Descriptive-Analytics/Modules/18-The-Nature-of-Data/index.html#iconify-mdi-category-plus-what-are-data-types",
    "title": "üï∂ Science, Human Experience, Experiments, and Data",
    "section": "\n4  What are Data Types?",
    "text": "4  What are Data Types?\n\n\n\nIn more detail:"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/18-The-Nature-of-Data/index.html#sec-data-types",
    "href": "content/courses/Descriptive-Analytics/Modules/18-The-Nature-of-Data/index.html#sec-data-types",
    "title": "üï∂ Science, Human Experience, Experiments, and Data",
    "section": "\n5  How do we Spot Data Variable Types?",
    "text": "5  How do we Spot Data Variable Types?\nBy asking questions!\n\n\nQuestions and Types of Variables\n\n\n\n\n\n\n\n\nPronoun\nAnswer\nVariable / Scale\nExample\nWhat O p erations ?\n\n\n\nWhat, Who, Where, Whom, Which\nName, Place, Animal, Thing\nQ u a litative or No m inal**\nName\n\n\nCount of\nCases\n\nMode\n\n\n\nHow, What Kind, What Sort\nA Manner / Method / Type or A ttribute from a List, with list items in some ‚Äúorder‚Äù ( e.g good, better, i mproved, best‚Ä¶)\nQ u a litative or Or d inal**\n\nSo c i o economic status,\nE d ucation,\nIncome,\nS a t i sfaction\n\n\n\nMedian\nP e r\n\ncentiles\n\n\n\nHow Many, How much, How Heavy , How few, Seldom, Often, When\n\nQ u antities with Scale\nDi f f e rences are m e a ningful, but not products or ratios\n\nQ u a n titative or I n t e rval**\n\npH\nSAT Score ( 200-800)\nCredit Score ( 300-850)\nYear of Starting College\n\n\n\nMean\nStandard\n\nD eviation\n\n\n\nHow Many, How much, How Heavy , How few, Seldom, Often, When\n\nQ u a ntities, with Scale and a Zero Value.\nD i f ferences and Ratios / Products are m e a ningful. (e.g Weight )\n\nQ u a n titative or Ratio**\n\nWeight, Length, Height,\nT e m p erature, Enzyme A ctivity, Dosage Amount, Reaction Rate,\nC on c e n tration, Pulse Rate, Survival Time\n\n\nC o r relation\nC o e fficient of V ariation\n\n\n\n\n\nAs you go from Qualitative to Quantitative data types in the table, I hope you can detect a movement from fuzzy groups/categories to more and more crystallized numbers.\n\n\nType of Variables\n\n\nEach variable/scale can be subjected to the operations of the previous group. In the words of S.S. Stevens\n\nthe basic operations needed to create each type of scale is cumulative: to an operation listed opposite a particular scale must be added all those operations preceding it."
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/18-The-Nature-of-Data/index.html#sec-data-viz",
    "href": "content/courses/Descriptive-Analytics/Modules/18-The-Nature-of-Data/index.html#sec-data-viz",
    "title": "üï∂ Science, Human Experience, Experiments, and Data",
    "section": "\n6 What Are the Parts of a Data Viz?",
    "text": "6 What Are the Parts of a Data Viz?"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/18-The-Nature-of-Data/index.html#how-to-pick-a-data-viz",
    "href": "content/courses/Descriptive-Analytics/Modules/18-The-Nature-of-Data/index.html#how-to-pick-a-data-viz",
    "title": "üï∂ Science, Human Experience, Experiments, and Data",
    "section": "\n7 How to pick a Data Viz?",
    "text": "7 How to pick a Data Viz?\nMost Data Visualizations use one or more of the following geometric attributes or aesthetics. These geometric aesthetics are used to represent qualitative or quantitative variables from your data.\n\n\nCommon Geometric Aesthetics in Charts\n\n\nWhat does this mean? We can think of simple visualizations as combinations of these aesthetics. Some examples:\n\n\nGeometries , Combinations, and Graphs\n\n\n\n\n\n\nAesthetic #1\nAesthetic #2\nShape\n\n\n\nPosition X = Quant Variable\nPosition Y = Quant Variable\nPoints/Circles with Fixed Size\n\n\nPosition X = Qual Variable\nPosition Y = Count of Qual Variable\nColumns\n\n\nPosition X = Qual Variable\nPosition Y = Qual Variable\nRectangles, with area proportional to joint(X,Y) count\n\n\n\nPosition X = Qual Variable\nPosition Y = Rank-Ordered Quant Variable\nBox + Whisker, Box length proportional to Inter-Quartile Range, whisker-length proportional to upper and lower quartile resp.\n\n\nPosition X = Quant Variable\nPosition Y = Quant Variable + Qual Variable\n\nLine and Area\nColour for Area"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/180-Reference-Material/index.html",
    "href": "content/courses/Descriptive-Analytics/Modules/180-Reference-Material/index.html",
    "title": "üï∏ Additional Reference Material",
    "section": "",
    "text": "https://hdlab.stanford.edu/palladio/\n\nhttps://infogram.com/\n\nhttps://www.visme.co/chart-maker/"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/20-Quant-Data-Distributions/files/distributions.html",
    "href": "content/courses/Descriptive-Analytics/Modules/20-Quant-Data-Distributions/files/distributions.html",
    "title": "Distributions in R",
    "section": "",
    "text": "We will create Distributions for data in R. As always, we will consistently use the Project Mosaic ecosystem of packages in R (mosaic, mosaicData and ggformula).\n\n\n\n\nNote the standard method for all commands from the mosaic package:\ngoal( y ~ x | z, data = mydata, ‚Ä¶)\n\n\nLet us inspect what datasets are available in the package mosaicData. Type data(package = \"mosaicData\") in your Console to see what datasets are available.\nLet us choose the famous Galton dataset:\n\nShow the Codedata(\"Galton\")\ninspect(Galton)\n#> \n#> categorical variables:  \n#>     name  class levels   n missing\n#> 1 family factor    197 898       0\n#> 2    sex factor      2 898       0\n#>                                    distribution\n#> 1 185 (1.7%), 166 (1.2%), 66 (1.2%) ...        \n#> 2 M (51.8%), F (48.2%)                         \n#> \n#> quantitative variables:  \n#>     name   class min Q1 median   Q3  max      mean       sd   n missing\n#> 1 father numeric  62 68   69.0 71.0 78.5 69.232851 2.470256 898       0\n#> 2 mother numeric  58 63   64.0 65.5 70.5 64.084410 2.307025 898       0\n#> 3 height numeric  56 64   66.5 69.7 79.0 66.760690 3.582918 898       0\n#> 4  nkids integer   1  4    6.0  8.0 15.0  6.135857 2.685156 898       0\n\n\nThe data is described as:\n\nA data frame with 898 observations on the following variables.\n\n\nfamily a factor with levels for each family\n\nfather the father‚Äôs height (in inches)\n\nmother the mother‚Äôs height (in inches)\n\nsex the child‚Äôs sex: F or M\n\nheight the child‚Äôs height as an adult (in inches)\n\nnkids the number of adult children in the family, or, at least, the number whose heights Galton recorded.\n\n\nThere is a lot of Description generated by the mosaic::inspect() command ! What can we say about the dataset and its variables? How big is the dataset? How many variables? What types are they, Quant or Qual? If they are Qual, what are the levels? Are they ordered levels? Discuss!\n\nAs Stigler said, summaries are the first thing to look at in data. Let us tabulate some quick stat summaries of the important variables in Galton:\n\nShow the Codefavstats(~ father, data = Galton)\n\n\n\n  \n\n\nShow the Codefavstats(~ height | sex, data = Galton)\n\n\n\n  \n\n\n\nQ.1 How many families in the data for each value of nkids?\n\nShow the Codetally(~ nkids, data = Galton)\n#> nkids\n#>   1   2   3   4   5   6   7   8   9  10  11  15 \n#>  32  40  66 116 140 114 112 128  63  40  32  15\n\n\nQ.2. What is the break-up by sex of the child?\n\nShow the Codetally(~ nkids | sex, data = Galton)\n#>      sex\n#> nkids  F  M\n#>    1  15 17\n#>    2  18 22\n#>    3  31 35\n#>    4  48 68\n#>    5  61 79\n#>    6  57 57\n#>    7  61 51\n#>    8  61 67\n#>    9  32 31\n#>    10 24 16\n#>    11 17 15\n#>    15  8  7\n\n\n\nWhat Questions might we have, that we could answer with a Distribution?\nQ.1 How many families based on the number of children?\n\nShow the Code\n# Convert the tally into a dataframe. See the difference!\nfamily_count <- tally( ~ nkids | sex, data = Galton) %>% \n  as_tibble() %>% \n  \n  # Convert nkids from char to int\n  mutate( nkids = as.integer(nkids))\nfamily_count\n\n\n\n  \n\n\nShow the Codegf_col(n ~ nkids | sex, data = family_count, fill = ~ sex, ylab = \"Number of Families\", xlab = \"Number of Kids / Family\")\n\n\n\n\nQ.2: How are the children‚Äôs heights distributed?\n\nShow the Codegf_histogram(~ height, data = Galton) %>% \n  gf_vline(xintercept = mean(Galton$height))\n\n\n\n\nQ.3: Is there a difference in height distributions between Male and Female children?\n\nShow the Codemeasures <- favstats(~ height | sex, data = Galton)\nmeasures\n\n\n\n  \n\n\nShow the Code\ngf_histogram(~ height | sex, data = Galton) %>% \n  gf_vline(xintercept = ~ mean, data = measures)\n\n\n\n\nQ.4: Are Mothers generally shorter than fathers?\n\nShow the Codegf_density(~ father, data = Galton, fill = \"blue\", alpha = 0.3) %>% \n  gf_density( ~ mother, data = Galton, fill = \"red\", alpha = 0.3, xlab = \"Heights\")\n#> Warning: `stat(density)` was deprecated in ggplot2 3.4.0.\n#> ‚Ñπ Please use `after_stat(density)` instead.\n\n\n\n\nQ.5: Are heights of children different based on the number of kids in the family? For Male and Female children?\n\nShow the Codegf_boxplot(height ~ sex | nkids, data = Galton)\n\n\n\n\nQ.6: Does the mean height of children in a family vary with the number of children in the family? ( family size)\n\nShow the Codemean( height ~ sex | nkids, data = Galton) %>% as_tibble() # not very inspiring!\n\n\n\n  \n\n\nShow the Codeby_sex_nkids <- favstats( height ~ sex + nkids, data = Galton)\nby_sex_nkids # much better!\n\n\n\n  \n\n\n\n\nShow the Codegf_col(mean ~ sex.nkids, data = by_sex_nkids)\n\n\n\n\nHmm‚Ä¶not a very informative plot‚Ä¶\n\nLet us try the NHANES dataset. Try help(NHANES) in your Console.\n\nShow the Codedata(\"NHANES\")\nnames(NHANES)\n#>  [1] \"ID\"               \"SurveyYr\"         \"Gender\"           \"Age\"             \n#>  [5] \"AgeDecade\"        \"AgeMonths\"        \"Race1\"            \"Race3\"           \n#>  [9] \"Education\"        \"MaritalStatus\"    \"HHIncome\"         \"HHIncomeMid\"     \n#> [13] \"Poverty\"          \"HomeRooms\"        \"HomeOwn\"          \"Work\"            \n#> [17] \"Weight\"           \"Length\"           \"HeadCirc\"         \"Height\"          \n#> [21] \"BMI\"              \"BMICatUnder20yrs\" \"BMI_WHO\"          \"Pulse\"           \n#> [25] \"BPSysAve\"         \"BPDiaAve\"         \"BPSys1\"           \"BPDia1\"          \n#> [29] \"BPSys2\"           \"BPDia2\"           \"BPSys3\"           \"BPDia3\"          \n#> [33] \"Testosterone\"     \"DirectChol\"       \"TotChol\"          \"UrineVol1\"       \n#> [37] \"UrineFlow1\"       \"UrineVol2\"        \"UrineFlow2\"       \"Diabetes\"        \n#> [41] \"DiabetesAge\"      \"HealthGen\"        \"DaysPhysHlthBad\"  \"DaysMentHlthBad\" \n#> [45] \"LittleInterest\"   \"Depressed\"        \"nPregnancies\"     \"nBabies\"         \n#> [49] \"Age1stBaby\"       \"SleepHrsNight\"    \"SleepTrouble\"     \"PhysActive\"      \n#> [53] \"PhysActiveDays\"   \"TVHrsDay\"         \"CompHrsDay\"       \"TVHrsDayChild\"   \n#> [57] \"CompHrsDayChild\"  \"Alcohol12PlusYr\"  \"AlcoholDay\"       \"AlcoholYear\"     \n#> [61] \"SmokeNow\"         \"Smoke100\"         \"Smoke100n\"        \"SmokeAge\"        \n#> [65] \"Marijuana\"        \"AgeFirstMarij\"    \"RegularMarij\"     \"AgeRegMarij\"     \n#> [69] \"HardDrugs\"        \"SexEver\"          \"SexAge\"           \"SexNumPartnLife\" \n#> [73] \"SexNumPartYear\"   \"SameSex\"          \"SexOrientation\"   \"PregnantNow\"\n\n\n\n\nShow the Codemosaic::inspect(NHANES)\n#> \n#> categorical variables:  \n#>                name  class levels     n missing\n#> 1          SurveyYr factor      2 10000       0\n#> 2            Gender factor      2 10000       0\n#> 3         AgeDecade factor      8  9667     333\n#> 4             Race1 factor      5 10000       0\n#> 5             Race3 factor      6  5000    5000\n#> 6         Education factor      5  7221    2779\n#> 7     MaritalStatus factor      6  7231    2769\n#> 8          HHIncome factor     12  9189     811\n#> 9           HomeOwn factor      3  9937      63\n#> 10             Work factor      3  7771    2229\n#> 11 BMICatUnder20yrs factor      4  1274    8726\n#> 12          BMI_WHO factor      4  9603     397\n#> 13         Diabetes factor      2  9858     142\n#> 14        HealthGen factor      5  7539    2461\n#> 15   LittleInterest factor      3  6667    3333\n#> 16        Depressed factor      3  6673    3327\n#> 17     SleepTrouble factor      2  7772    2228\n#> 18       PhysActive factor      2  8326    1674\n#> 19         TVHrsDay factor      7  4859    5141\n#> 20       CompHrsDay factor      7  4863    5137\n#> 21  Alcohol12PlusYr factor      2  6580    3420\n#> 22         SmokeNow factor      2  3211    6789\n#> 23         Smoke100 factor      2  7235    2765\n#> 24        Smoke100n factor      2  7235    2765\n#> 25        Marijuana factor      2  4941    5059\n#> 26     RegularMarij factor      2  4941    5059\n#> 27        HardDrugs factor      2  5765    4235\n#> 28          SexEver factor      2  5767    4233\n#> 29          SameSex factor      2  5768    4232\n#> 30   SexOrientation factor      3  4842    5158\n#> 31      PregnantNow factor      3  1696    8304\n#>                                     distribution\n#> 1  2009_10 (50%), 2011_12 (50%)                 \n#> 2  female (50.2%), male (49.8%)                 \n#> 3   40-49 (14.5%),  0-9 (14.4%) ...             \n#> 4  White (63.7%), Black (12%) ...               \n#> 5  White (62.7%), Black (11.8%) ...             \n#> 6  Some College (31.4%) ...                     \n#> 7  Married (54.6%), NeverMarried (19.1%) ...    \n#> 8  more 99999 (24.2%) ...                       \n#> 9  Own (64.7%), Rent (33.1%) ...                \n#> 10 Working (59.4%), NotWorking (36.6%) ...      \n#> 11 NormWeight (63.2%), Obese (17.3%) ...        \n#> 12 18.5_to_24.9 (30.3%) ...                     \n#> 13 No (92.3%), Yes (7.7%)                       \n#> 14 Good (39.2%), Vgood (33.3%) ...              \n#> 15 None (76.5%), Several (16.9%) ...            \n#> 16 None (78.6%), Several (15.1%) ...            \n#> 17 No (74.6%), Yes (25.4%)                      \n#> 18 Yes (55.8%), No (44.2%)                      \n#> 19 2_hr (26.2%), 1_hr (18.2%) ...               \n#> 20 0_to_1_hr (29%), 0_hrs (22.1%) ...           \n#> 21 Yes (79.2%), No (20.8%)                      \n#> 22 No (54.3%), Yes (45.7%)                      \n#> 23 No (55.6%), Yes (44.4%)                      \n#> 24 Non-Smoker (55.6%), Smoker (44.4%)           \n#> 25 Yes (58.5%), No (41.5%)                      \n#> 26 No (72.4%), Yes (27.6%)                      \n#> 27 No (81.5%), Yes (18.5%)                      \n#> 28 Yes (96.1%), No (3.9%)                       \n#> 29 No (92.8%), Yes (7.2%)                       \n#> 30 Heterosexual (95.8%), Bisexual (2.5%) ...    \n#> 31 No (92.7%), Yes (4.2%) ...                   \n#> \n#> quantitative variables:  \n#>               name   class      min        Q1    median        Q3        max\n#> 1               ID integer 51624.00 56904.500 62159.500 67039.000  71915.000\n#> 2              Age integer     0.00    17.000    36.000    54.000     80.000\n#> 3        AgeMonths integer     0.00   199.000   418.000   624.000    959.000\n#> 4      HHIncomeMid integer  2500.00 30000.000 50000.000 87500.000 100000.000\n#> 5          Poverty numeric     0.00     1.240     2.700     4.710      5.000\n#> 6        HomeRooms integer     1.00     5.000     6.000     8.000     13.000\n#> 7           Weight numeric     2.80    56.100    72.700    88.900    230.700\n#> 8           Length numeric    47.10    75.700    87.000    96.100    112.200\n#> 9         HeadCirc numeric    34.20    39.575    41.450    42.925     45.400\n#> 10          Height numeric    83.60   156.800   166.000   174.500    200.400\n#> 11             BMI numeric    12.88    21.580    25.980    30.890     81.250\n#> 12           Pulse integer    40.00    64.000    72.000    82.000    136.000\n#> 13        BPSysAve integer    76.00   106.000   116.000   127.000    226.000\n#> 14        BPDiaAve integer     0.00    61.000    69.000    76.000    116.000\n#> 15          BPSys1 integer    72.00   106.000   116.000   128.000    232.000\n#> 16          BPDia1 integer     0.00    62.000    70.000    76.000    118.000\n#> 17          BPSys2 integer    76.00   106.000   116.000   128.000    226.000\n#> 18          BPDia2 integer     0.00    60.000    68.000    76.000    118.000\n#> 19          BPSys3 integer    76.00   106.000   116.000   126.000    226.000\n#> 20          BPDia3 integer     0.00    60.000    68.000    76.000    116.000\n#> 21    Testosterone numeric     0.25    17.700    43.820   362.410   1795.600\n#> 22      DirectChol numeric     0.39     1.090     1.290     1.580      4.030\n#> 23         TotChol numeric     1.53     4.110     4.780     5.530     13.650\n#> 24       UrineVol1 integer     0.00    50.000    94.000   164.000    510.000\n#> 25      UrineFlow1 numeric     0.00     0.403     0.699     1.221     17.167\n#> 26       UrineVol2 integer     0.00    52.000    95.000   171.750    409.000\n#> 27      UrineFlow2 numeric     0.00     0.475     0.760     1.513     13.692\n#> 28     DiabetesAge integer     1.00    40.000    50.000    58.000     80.000\n#> 29 DaysPhysHlthBad integer     0.00     0.000     0.000     3.000     30.000\n#> 30 DaysMentHlthBad integer     0.00     0.000     0.000     4.000     30.000\n#> 31    nPregnancies integer     1.00     2.000     3.000     4.000     32.000\n#> 32         nBabies integer     0.00     2.000     2.000     3.000     12.000\n#> 33      Age1stBaby integer    14.00    19.000    22.000    26.000     39.000\n#> 34   SleepHrsNight integer     2.00     6.000     7.000     8.000     12.000\n#> 35  PhysActiveDays integer     1.00     2.000     3.000     5.000      7.000\n#> 36   TVHrsDayChild integer     0.00     1.000     2.000     3.000      6.000\n#> 37 CompHrsDayChild integer     0.00     0.000     1.000     6.000      6.000\n#> 38      AlcoholDay integer     1.00     1.000     2.000     3.000     82.000\n#> 39     AlcoholYear integer     0.00     3.000    24.000   104.000    364.000\n#> 40        SmokeAge integer     6.00    15.000    17.000    19.000     72.000\n#> 41   AgeFirstMarij integer     1.00    15.000    16.000    19.000     48.000\n#> 42     AgeRegMarij integer     5.00    15.000    17.000    19.000     52.000\n#> 43          SexAge integer     9.00    15.000    17.000    19.000     50.000\n#> 44 SexNumPartnLife integer     0.00     2.000     5.000    12.000   2000.000\n#> 45  SexNumPartYear integer     0.00     1.000     1.000     1.000     69.000\n#>            mean           sd     n missing\n#> 1  6.194464e+04 5.871167e+03 10000       0\n#> 2  3.674210e+01 2.239757e+01 10000       0\n#> 3  4.201239e+02 2.590431e+02  4962    5038\n#> 4  5.720617e+04 3.302028e+04  9189     811\n#> 5  2.801844e+00 1.677909e+00  9274     726\n#> 6  6.248918e+00 2.277538e+00  9931      69\n#> 7  7.098180e+01 2.912536e+01  9922      78\n#> 8  8.501602e+01 1.370503e+01   543    9457\n#> 9  4.118068e+01 2.311483e+00    88    9912\n#> 10 1.618778e+02 2.018657e+01  9647     353\n#> 11 2.666014e+01 7.376579e+00  9634     366\n#> 12 7.355973e+01 1.215542e+01  8563    1437\n#> 13 1.181550e+02 1.724817e+01  8551    1449\n#> 14 6.748006e+01 1.435480e+01  8551    1449\n#> 15 1.190902e+02 1.749636e+01  8237    1763\n#> 16 6.827826e+01 1.378078e+01  8237    1763\n#> 17 1.184758e+02 1.749133e+01  8353    1647\n#> 18 6.766455e+01 1.441978e+01  8353    1647\n#> 19 1.179292e+02 1.717719e+01  8365    1635\n#> 20 6.729874e+01 1.495839e+01  8365    1635\n#> 21 1.978980e+02 2.265045e+02  4126    5874\n#> 22 1.364865e+00 3.992581e-01  8474    1526\n#> 23 4.879220e+00 1.075583e+00  8474    1526\n#> 24 1.185161e+02 9.033648e+01  9013     987\n#> 25 9.792946e-01 9.495143e-01  8397    1603\n#> 26 1.196759e+02 9.016005e+01  1478    8522\n#> 27 1.149372e+00 1.072948e+00  1476    8524\n#> 28 4.842289e+01 1.568050e+01   629    9371\n#> 29 3.334838e+00 7.400700e+00  7532    2468\n#> 30 4.126493e+00 7.832971e+00  7534    2466\n#> 31 3.026882e+00 1.795341e+00  2604    7396\n#> 32 2.456954e+00 1.315227e+00  2416    7584\n#> 33 2.264968e+01 4.772509e+00  1884    8116\n#> 34 6.927531e+00 1.346729e+00  7755    2245\n#> 35 3.743513e+00 1.836358e+00  4663    5337\n#> 36 1.938744e+00 1.434431e+00   653    9347\n#> 37 2.197550e+00 2.516667e+00   653    9347\n#> 38 2.914123e+00 3.182672e+00  4914    5086\n#> 39 7.510165e+01 1.030337e+02  5922    4078\n#> 40 1.782662e+01 5.326660e+00  3080    6920\n#> 41 1.702283e+01 3.895010e+00  2891    7109\n#> 42 1.769107e+01 4.806103e+00  1366    8634\n#> 43 1.742870e+01 3.716551e+00  5540    4460\n#> 44 1.508507e+01 5.784643e+01  5725    4275\n#> 45 1.342330e+00 2.782688e+00  4928    5072\n\n\nAgain, lots of data from inspect, about the Quant and Qual variables. Spend a little time looking through the output of inspect. Which variables could have been data given by each respondent, and which ones could have been measured data variables? Why do you think so?\nWhy is there so much missing data? Which variable are the most affected by this?\n\nShow the Codetally(Education ~ Work, data = NHANES)\n#>                 Work\n#> Education        Looking NotWorking Working <NA>\n#>   8th Grade           13        249     188    1\n#>   9 - 11th Grade      39        438     411    0\n#>   High School         52        579     886    0\n#>   Some College        88        792    1387    0\n#>   College Grad        72        474    1552    0\n#>   <NA>                47        315     189 2228\n\n\n\nQ.1. What is the distribution of Physical Activity Days, across Gender? Across Education?\n\nShow the Codegf_histogram(data = NHANES, ~ PhysActiveDays | Gender)\n#> Warning: Removed 5337 rows containing non-finite values (`stat_bin()`).\n\n\n\nShow the Codegf_histogram(data = NHANES, ~ PhysActiveDays | Education)\n#> Warning: Removed 5337 rows containing non-finite values (`stat_bin()`).\n\n\n\n\nQ.1. How are people Ages distributed across levels of Education?\n\nShow the Codegf_boxplot(Age ~ Education, data = NHANES)\n\n\n\n\nQ.2. How is Education distributed over Race?\n\nShow the Codegf_histogram()\n#> gf_histogram() uses \n#>     * a formula with shape ~x or y ~ . or y ~ x. \n#>     * geom:  bar \n#>     * stat:  bin \n#>     * position:  stack \n#>     * key attributes:  bins = 25, binwidth, alpha = 0.5, color, fill, group,\n#>                    linetype, size \n#> Note:  y may be stat(density) or stat(count) or stat(ndensity) or stat(ncount), but see gf_dhistogram().\n#> \n#> For more information, try ?gf_histogram\n\n\nQ.3 What is the distribution of people‚Äôs BMI, split by Gender? By Race1?\n\nShow the Codegf_histogram(~ BMI | Gender, data = NHANES)\n#> Warning: Removed 366 rows containing non-finite values (`stat_bin()`).\n\n\n\nShow the Codegf_histogram(~ BMI | Race1 + Race3, data = NHANES)\n#> Warning: Removed 366 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nHere is a dataset from Jeremy Singer-Vine‚Äôs blog, Data Is Plural. This is a list of all books banned in schools across the US.\n\nShow the Codebanned <- readxl::read_xlsx(path = \"data/banned.xlsx\",\n                            sheet = \"Sorted by Author & Title\")\nbanned\n\n\n\n  \n\n\nShow the Code\nnames(banned)\n#>  [1] \"Author\"                    \"Title\"                    \n#>  [3] \"Type of Ban\"               \"Secondary Author(s)\"      \n#>  [5] \"Illustrator(s)\"            \"Translator(s)\"            \n#>  [7] \"State\"                     \"District\"                 \n#>  [9] \"Date of Challenge/Removal\" \"Origin of Challenge\"\n\n\nClearly the variables are all Qualitative, except perhaps for Date of Challenge/Removal, (which in this case has been badly mangled by Excel) So we need to make counts based on the levels* of the Qual variables and plot Bar/Column charts.\nLet us quickly make some Stat Summaries:\n\nShow the Codemosaic::inspect(banned)\n#> \n#> categorical variables:  \n#>                         name     class levels    n missing\n#> 1                     Author character    797 1586       0\n#> 2                      Title character   1145 1586       0\n#> 3                Type of Ban character      4 1586       0\n#> 4        Secondary Author(s) character     61   98    1488\n#> 5             Illustrator(s) character    192  364    1222\n#> 6              Translator(s) character      9   10    1576\n#> 7                      State character     26 1586       0\n#> 8                   District character     86 1586       0\n#> 9  Date of Challenge/Removal character     15 1586       0\n#> 10       Origin of Challenge character      2 1586       0\n#>                                     distribution\n#> 1  Kobabe, Maia (1.9%) ...                      \n#> 2  Gender Queer: A Memoir (1.9%) ...            \n#> 3  Banned Pending Investigation (46.1%) ...     \n#> 4  Cast, Kristin (12.2%) ...                    \n#> 5  Aly, Hatem (4.7%) ...                        \n#> 6  Mlawer, Teresa (20%) ...                     \n#> 7  Texas (45%), Pennsylvania (28.8%) ...        \n#> 8  Central York (27.8%) ...                     \n#> 9  44440 (28.8%), 44531 (28.3%) ...             \n#> 10 Administrator (95.6%) ...\n\n\nLet us try to answer this question: What is the count of banned books by type and by US state?\n\nShow the Codebanned_by_state <- banned %>% group_by(State) %>% summarise(total = n()) %>% ungroup()\nbanned_by_state\n\n\n\n  \n\n\nShow the Code\nbanned %>% group_by(State, `Type of Ban`) %>% summarise(count = n()) %>% ungroup() %>% left_join(., banned_by_state, by = c(\"State\" = \"State\")) %>% \n #  pivot_wider(.,id_cols = State,\n #              names_from = `Type of Ban`,\n #              values_from = count) %>% janitor::clean_names() %>% \n #  replace_na(list(banned_from_libraries_and_classrooms = 0,\n #                  banned_from_libraries = 0,\n #                  banned_pending_investigation = 0,\n #                  banned_from_classrooms = 0)) %>% \n # mutate(total = sum(across(where(is.integer)))) %>%\nggplot(aes(x = reorder(State, total), y = count, fill = `Type of Ban`)) + geom_col() + coord_flip()\n#> `summarise()` has grouped output by 'State'. You can override using the\n#> `.groups` argument."
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/20-Quant-Data-Distributions/index.html",
    "href": "content/courses/Descriptive-Analytics/Modules/20-Quant-Data-Distributions/index.html",
    "title": "üìä Distributions",
    "section": "",
    "text": "R Tutorial¬†¬†\n  Orange Tutorial\n\n  Radiant Tutorial¬†\n  Datasets"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/20-Quant-Data-Distributions/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/Descriptive-Analytics/Modules/20-Quant-Data-Distributions/index.html#what-graphs-will-we-see-today",
    "title": "üìä Distributions",
    "section": "\n2 What graphs will we see today?",
    "text": "2 What graphs will we see today?\nSome of the very basic and commonly used plots for data are:\n\nBar and Column Charts\nHistograms and Frequency Distributions\nBox Plots\n2D Hexbins Plots and 2D Frequency Distributions\nRidge Plots ( Quant + Qual variables)"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/20-Quant-Data-Distributions/index.html#histograms-and-frequency-distributions",
    "href": "content/courses/Descriptive-Analytics/Modules/20-Quant-Data-Distributions/index.html#histograms-and-frequency-distributions",
    "title": "üìä Distributions",
    "section": "\n3 Histograms and Frequency Distributions",
    "text": "3 Histograms and Frequency Distributions\nHistograms are best to show the distribution of raw quantitative data,by displaying the number of values that fall within defined ranges, often called buckets or bins.\nAlthough histograms may look similar to column charts, the two are different. First, histograms show continuous data, and usually you can adjust the bucket ranges to explore frequency patterns. For example, you can shift histogram buckets from 0-1, 1-2, 2-3, etc. to 0-2, 2-4, etc. By contrast, column charts show categorical data, such as the number of apples, bananas, carrots, etc. Second, histograms do not usually show spaces between buckets because these are continuous values, while column charts show spaces to separate each category.\nLet us listen to the late great Hans Rosling from the Gapminder Project, which aims at telling stories of the world with data, to remove systemic biases about poverty, income and gender related issues."
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/20-Quant-Data-Distributions/index.html#examine-the-data",
    "href": "content/courses/Descriptive-Analytics/Modules/20-Quant-Data-Distributions/index.html#examine-the-data",
    "title": "üìä Distributions",
    "section": "\n4 Examine the Data",
    "text": "4 Examine the Data\nLet us look at the popular mpg dataset ( from R ) using mosaic::inspect(). We get two new pieces of output (i.e.¬†two new dataframes), describing the Qual and Quant variables separately:\n\nShow the Codeinspect(mpg)\n\n\ncategorical variables:  \n          name     class levels   n missing\n1 manufacturer character     15 234       0\n2        model character     38 234       0\n3        trans character     10 234       0\n4          drv character      3 234       0\n5           fl character      5 234       0\n6        class character      7 234       0\n                                   distribution\n1 dodge (15.8%), toyota (14.5%) ...            \n2 caravan 2wd (4.7%) ...                       \n3 auto(l4) (35.5%), manual(m5) (24.8%) ...     \n4 f (45.3%), 4 (44%), r (10.7%)                \n5 r (71.8%), p (22.2%), e (3.4%) ...           \n6 suv (26.5%), compact (20.1%) ...             \n\nquantitative variables:  \n   name   class    min     Q1 median     Q3  max        mean       sd   n\n1 displ numeric    1.6    2.4    3.3    4.6    7    3.471795 1.291959 234\n2  year integer 1999.0 1999.0 2003.5 2008.0 2008 2003.500000 4.509646 234\n3   cyl integer    4.0    4.0    6.0    8.0    8    5.888889 1.611534 234\n4   cty integer    9.0   14.0   17.0   19.0   35   16.858974 4.255946 234\n5   hwy integer   12.0   18.0   24.0   27.0   44   23.440171 5.954643 234\n  missing\n1       0\n2       0\n3       0\n4       0\n5       0\n\n\n\n\nWe can save and see the outputs separately:\n\nShow the Codempg_describe <- inspect(mpg)\n\nmpg_describe$categorical\nmpg_describe$quantitative\n\n\nThere is a lot of Description generated by the mosaic::inspect() command ! What can we say about the dataset and its variables? How big is the dataset? How many variables? What types are they, Quant or Qual? If they are Qual, what are the levels? Are they ordered levels? Discuss!"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/20-Quant-Data-Distributions/index.html#some-sample-charts-for-quant-data-distributions",
    "href": "content/courses/Descriptive-Analytics/Modules/20-Quant-Data-Distributions/index.html#some-sample-charts-for-quant-data-distributions",
    "title": "üìä Distributions",
    "section": "\n5 Some Sample Charts for Quant Data Distributions",
    "text": "5 Some Sample Charts for Quant Data Distributions\n\nShow the Code# library(tidyverse)\n# library(patchwork)\n# library(ggridges)\n\nmpg <- mpg %>% mutate(drv= as_factor(drv))\np <- ggplot(mpg, alpha = 0.3) \np1 <- p + geom_histogram(aes(x = hwy, fill = drv)) + labs(title = \"Histogram\")\np2 <- p + geom_density(aes(x = hwy, fill = drv))+ labs(title = \"Frequency Density\")\np3 <- p + geom_boxplot(aes(x = drv, y = hwy, fill = drv))+ labs(title = \"Boxplot\")\np4 <- p + geom_violin(aes(x = drv, y = hwy, fill = drv))+ labs(title = \"Violin\")\n\n(p1+p3)/(p2+p4) + plot_layout(nrow = 3) + plot_annotation(title = \"Single Quant Variable\",subtitle = \"And one Qual Variable too\", tag_levels = 'A') & theme_minimal()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nShow the Codempg <- mpg %>% mutate(drv= as_factor(drv))\n\np5 <- p + geom_density_ridges(aes(x = hwy, y = drv, fill = drv), alpha = 0.3, rel_min_height = 0.005) +\n  scale_y_discrete(expand = c(0.01, 0)) +\n  scale_x_continuous(expand = c(0.01, 0)) +\n  theme_ridges() + labs(title = \"Ridge Plot\")\n\np7 <- p + geom_hex(aes(x = hwy, y = cty, fill = drv))+ labs(title = \"Hex Bin Plot\")\np8 <- p + geom_density_2d(aes(x = hwy, y = cty))+ labs(title = \"2D Density Plot\")\n\n(p7 + p8) / p5 + plot_layout(guides = 'keep') + \n  plot_annotation(title = \"Two Quant Variables\",subtitle = \"And one Qual Variable too\", tag_levels = 'A') & theme_minimal()\n\nPicking joint bandwidth of 1.28"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/20-Quant-Data-Distributions/index.html#iconify-orange-circle-a-workflow-in-orange",
    "href": "content/courses/Descriptive-Analytics/Modules/20-Quant-Data-Distributions/index.html#iconify-orange-circle-a-workflow-in-orange",
    "title": "üìä Distributions",
    "section": "\n6  A Workflow in Orange",
    "text": "6  A Workflow in Orange\nHow does one visualize Distributions in Orange?\n\n  Orange Tutorial\nDownload the Orange Workflow file by clicking the icon above, save the file, and open it in Orange."
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/20-Quant-Data-Distributions/index.html#fa-person-rays-a-workflow-in-radiant",
    "href": "content/courses/Descriptive-Analytics/Modules/20-Quant-Data-Distributions/index.html#fa-person-rays-a-workflow-in-radiant",
    "title": "üìä Distributions",
    "section": "\n7  A Workflow in Radiant",
    "text": "7  A Workflow in Radiant\n  Radiant Tutorial¬†¬†\nDownload the Radiant Workflow statefile by clicking the icon above, an upload/open it in Radiant. You need to start radiant from the Add-Inmenu in RStudio."
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/20-Quant-Data-Distributions/index.html#fa-brands-r-project-a-workflow-in-r",
    "href": "content/courses/Descriptive-Analytics/Modules/20-Quant-Data-Distributions/index.html#fa-brands-r-project-a-workflow-in-r",
    "title": "üìä Distributions",
    "section": "\n8  A Workflow in R",
    "text": "8  A Workflow in R\n R Tutorial¬†¬†\nDownload the Quarto tutorial file by clicking the icon above and open it in RStudio or https://rstudio.cloud ."
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/20-Quant-Data-Distributions/index.html#conclusion",
    "href": "content/courses/Descriptive-Analytics/Modules/20-Quant-Data-Distributions/index.html#conclusion",
    "title": "üìä Distributions",
    "section": "\n9 Conclusion",
    "text": "9 Conclusion\n\nHistograms, Frequency Distributions, and Box Plots are used for Quantitative data variables\nHistograms ‚Äúdwell upon‚Äù counts, ranges, means and standard deviations\n\nFrequency Density plots ‚Äúdwell upon‚Äù probabilities and densities\n\nBox Plots ‚Äúdwell upon‚Äù medians and Quartiles\n\nQualitative data variables can be plotted as counts, using Bar Charts, or using Heat Maps\n2D density plots are used for describing two quant variables*\nRidge Plots are density plots used for describing one Quant and one Qual variable (by inherent splitting)\nWe can split all these plots on the basis of another Qualitative variable.(Ridge Plots are already split)"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/20-Quant-Data-Distributions/index.html#your-turn",
    "href": "content/courses/Descriptive-Analytics/Modules/20-Quant-Data-Distributions/index.html#your-turn",
    "title": "üìä Distributions",
    "section": "\n10 Your Turn",
    "text": "10 Your Turn\n  Datasets\n\nClick on the Dataset Icon above, and unzip that archive. Try to make distribution plots with each of the three tools.\nA dataset from calmcode.io https://calmcode.io/datasets.html\nOld Faithful Data in R ( Find it!)\n\ninspect the dataset in each case and develop a set of Questions, that can be answered by appropriate stat measures, or by using a chart to show the distribution."
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/20-Quant-Data-Distributions/index.html#references",
    "href": "content/courses/Descriptive-Analytics/Modules/20-Quant-Data-Distributions/index.html#references",
    "title": "üìä Distributions",
    "section": "\n11 References",
    "text": "11 References\n\nSee the scrolly animation for a histogram at this website: Exploring Histograms, an essay by Aran Lunzer and Amelia McNamara https://tinlizzie.org/histograms/?s=09\nMinimal R using mosaic. https://cran.r-project.org/web/packages/mosaic/vignettes/MinimalRgg.pdf"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/200-Miscellaneous/index.html",
    "href": "content/courses/Descriptive-Analytics/Modules/200-Miscellaneous/index.html",
    "title": "üìö Miscellaneous Graphs and Tools",
    "section": "",
    "text": "https://rawgraphs.io\n\nhttps://datawrapper.de\n\nhttps://hdlab.stanford.edu/palladio/\n\nhttps://infogram.com/\n\nhttps://www.visme.co/chart-maker/\n\nhttps://flourish.studio/ https://www.figma.com/"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/200-Miscellaneous/index.html#references",
    "href": "content/courses/Descriptive-Analytics/Modules/200-Miscellaneous/index.html#references",
    "title": "üìö Miscellaneous Graphs and Tools",
    "section": "\n2 References",
    "text": "2 References\n\nGetting started with Flourish & Figma to create beautiful custom charts, https://inside.mediahack.co.za/getting-started-with-flourish-figma-to-create-beautiful-custom-charts-34e4efb8fd3d\nFlowing Data Chart Types https://flowingdata.com/chart-types/\nGeeks for Geeks: Chart Types https://www.geeksforgeeks.org/r-charts-and-graphs/\nFinancial Times Visual Vocabulary (Interactive) https://ft-interactive.github.io/visual-vocabulary/\nFinancial Times Visual Vocabulary (PDF) https://github.com/Financial-Times/chart-doctor/blob/main/visual-vocabulary/FT4schools_RGS.pdf\nFinancial Times Data Journalism Visuals https://www.ft.com/visual-and-data-journalism\nSeverino Ribecca and John Schwabish , The Graphic Continuum https://www.severinoribecca.one/portfolio-item/the-graphic-continuum/\nWeb based tools for Dataviz https://policyviz.com/resources/data-viz-tools/\nNightingale Data Visualization Society Blog: How to visualize categorical data: https://nightingaledvs.com/endless-river-an-overview-of-dataviz-for-categorical-data/\nJohn Schwabish‚Äôs policyviz Data Viz catalogue: https://datastudio.google.com/s/quUUlgosF4U"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/200-Miscellaneous/index.html#papers",
    "href": "content/courses/Descriptive-Analytics/Modules/200-Miscellaneous/index.html#papers",
    "title": "üìö Miscellaneous Graphs and Tools",
    "section": "\n3 Papers",
    "text": "3 Papers\n1.Christopher G. Healey Department of Computer Science, North Carolina State University. Perception in Visualization"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/30-Quant-Data-Correlations/files/correlations.html",
    "href": "content/courses/Descriptive-Analytics/Modules/30-Quant-Data-Correlations/files/correlations.html",
    "title": "Correlations in R",
    "section": "",
    "text": "We will create Tables for Correlations, and graphs for Correlations in R. As always, we will consistently use the Project Mosaic ecosystem of packages in R (mosaic, mosaicData and ggformula).\n\n\n\n\nNote the standard method for all commands from the mosaic package:\ngoal( y ~ x | z, data = mydata, ‚Ä¶)\n\n\nLet us inspect what datasets are available in the package mosaicData. Run this command in your Console:\n\nShow the Code# Run in Console\ndata(package = \"mosaicData\")\n\n\nThe popup tab shows a lot of datasets we could use. Let us continue to use the famous Galton dataset and inspect it:\n\nShow the Codedata(\"Galton\")\ninspect(Galton)\n#> \n#> categorical variables:  \n#>     name  class levels   n missing\n#> 1 family factor    197 898       0\n#> 2    sex factor      2 898       0\n#>                                    distribution\n#> 1 185 (1.7%), 166 (1.2%), 66 (1.2%) ...        \n#> 2 M (51.8%), F (48.2%)                         \n#> \n#> quantitative variables:  \n#>     name   class min Q1 median   Q3  max      mean       sd   n missing\n#> 1 father numeric  62 68   69.0 71.0 78.5 69.232851 2.470256 898       0\n#> 2 mother numeric  58 63   64.0 65.5 70.5 64.084410 2.307025 898       0\n#> 3 height numeric  56 64   66.5 69.7 79.0 66.760690 3.582918 898       0\n#> 4  nkids integer   1  4    6.0  8.0 15.0  6.135857 2.685156 898       0\n\n\nThe inspect command already gives us a series of statistical measures of different variables of interest. As discussed previously, we can retain the output of inspect and use it in our reports: (there are ways of dressing up these tables too)\n\nShow the Code\ngalton_describe <- inspect(Galton)\n\ngalton_describe$categorical\n\n\n\n  \n\n\nShow the Codegalton_describe$quantitative\n\n\n\n  \n\n\n\nTry help(\"Galton\") in your Console. The dataset is described as:\n\nA data frame with 898 observations on the following variables.\n- family a factor with levels for each family\n- father the father‚Äôs height (in inches)\n- mother the mother‚Äôs height (in inches)\n- sex the child‚Äôs sex: F or M\n- height the child‚Äôs height as an adult (in inches)\n- nkids the number of adult children in the family, or, at least, the number whose heights Galton recorded.\n\nThere is a lot of Description generated by the mosaic::inspect() command ! What can we say about the dataset and its variables? How big is the dataset? How many variables? What types are they, Quant or Qual? If they are Qual, what are the levels? Are they ordered levels? Discuss!\n\nWhat Questions might we have, that we could answer with a Statistical Measure, or Correlation chart?\nQ.1 Which are the variables that have significant pair-wise correlations? What polarity are these correlations?\n\nShow the Code\n\n# Pulling out the list of Quant variables from NHANES\ngalton_quant <- galton_describe$quantitative\ngalton_quant$name\n#> [1] \"father\" \"mother\" \"height\" \"nkids\"\n\nGGally::ggpairs(\n  Galton,\n  columns = c(\"father\", \"mother\", \"height\", \"nkids\"),\n  diag = list(\"densityDiag\"),\n  title = \"Galton Data Correlations Plot\"\n)\n\n\n\n\nCan we plot a Correlogram for this dataset?\n\nShow the Code#library(corrplot)\n\ngalton_num_var <- Galton %>% select(father, mother, height, nkids)\ngalton_cor <- cor(galton_num_var)\ngalton_correlogram <- galton_cor %>%\n  corrplot(method = \"ellipse\",\n           type = \"lower\",\n           main = \"Correlogram for Galton dataset\")\n\n\n\nShow the Codegalton_correlogram\n#> $corr\n#>             father      mother     height       nkids\n#> father  1.00000000  0.07366461  0.2753548 -0.16002294\n#> mother  0.07366461  1.00000000  0.2016549 -0.02002964\n#> height  0.27535483  0.20165489  1.0000000 -0.12691012\n#> nkids  -0.16002294 -0.02002964 -0.1269101  1.00000000\n#> \n#> $corrPos\n#>     xName  yName x y        corr\n#> 1  father father 1 4  1.00000000\n#> 2  father mother 1 3  0.07366461\n#> 3  father height 1 2  0.27535483\n#> 4  father  nkids 1 1 -0.16002294\n#> 5  mother mother 2 3  1.00000000\n#> 6  mother height 2 2  0.20165489\n#> 7  mother  nkids 2 1 -0.02002964\n#> 8  height height 3 2  1.00000000\n#> 9  height  nkids 3 1 -0.12691012\n#> 10  nkids  nkids 4 1  1.00000000\n#> \n#> $arg\n#> $arg$type\n#> [1] \"lower\"\n\n\nClearly height is positvely correlated to father and mother; interestingly, height is negatively correlated ( slightly) with nkids.\nQ.2: Let us confirm with a test:\n\nShow the Codemosaic::cor_test(height ~ father, data = Galton)\n#> \n#>  Pearson's product-moment correlation\n#> \n#> data:  height and father\n#> t = 8.5737, df = 896, p-value < 2.2e-16\n#> alternative hypothesis: true correlation is not equal to 0\n#> 95 percent confidence interval:\n#>  0.2137851 0.3347455\n#> sample estimates:\n#>       cor \n#> 0.2753548\n\n\n\nShow the Codemosaic::cor_test(height ~ mother, data = Galton)\n#> \n#>  Pearson's product-moment correlation\n#> \n#> data:  height and mother\n#> t = 6.1628, df = 896, p-value = 1.079e-09\n#> alternative hypothesis: true correlation is not equal to 0\n#> 95 percent confidence interval:\n#>  0.1380554 0.2635982\n#> sample estimates:\n#>       cor \n#> 0.2016549\n\n\nQ.3 What does this correlation look when split by sex of Child?\n\nShow the Code\n# For the sons\ncor_test(height ~ father, data = Galton %>% filter(sex == \"M\"))\n#> \n#>  Pearson's product-moment correlation\n#> \n#> data:  height and father\n#> t = 9.1498, df = 463, p-value < 2.2e-16\n#> alternative hypothesis: true correlation is not equal to 0\n#> 95 percent confidence interval:\n#>  0.3114667 0.4656805\n#> sample estimates:\n#>       cor \n#> 0.3913174\ncor_test(height ~ mother, data = Galton %>% filter(sex == \"M\"))\n#> \n#>  Pearson's product-moment correlation\n#> \n#> data:  height and mother\n#> t = 7.628, df = 463, p-value = 1.367e-13\n#> alternative hypothesis: true correlation is not equal to 0\n#> 95 percent confidence interval:\n#>  0.2508178 0.4125305\n#> sample estimates:\n#>       cor \n#> 0.3341309\n\n# For the daughters\ncor_test(height ~ father, data = Galton %>% filter(sex == \"F\"))\n#> \n#>  Pearson's product-moment correlation\n#> \n#> data:  height and father\n#> t = 10.719, df = 431, p-value < 2.2e-16\n#> alternative hypothesis: true correlation is not equal to 0\n#> 95 percent confidence interval:\n#>  0.3809944 0.5300812\n#> sample estimates:\n#>       cor \n#> 0.4587605\ncor_test(height ~ mother, data = Galton %>% filter(sex == \"F\"))\n#> \n#>  Pearson's product-moment correlation\n#> \n#> data:  height and mother\n#> t = 6.8588, df = 431, p-value = 2.421e-11\n#> alternative hypothesis: true correlation is not equal to 0\n#> 95 percent confidence interval:\n#>  0.2261463 0.3962226\n#> sample estimates:\n#>       cor \n#> 0.3136984\n\n\nQ.4. How can we show this correlation in a set of Scatter Plots + Regression Lines? Can we recreate Galton‚Äôs famous diagram?\n\nShow the Code# For the sons\ngf_point(height ~ father, data = Galton %>% filter(sex == \"M\")) %>% gf_smooth(method = \"lm\")\n\n\n\nShow the Codegf_point(height ~ mother, data = Galton %>% filter(sex == \"M\")) %>% gf_smooth(method = \"lm\")\n\n\n\nShow the Code\n# For the daughters\ngf_point(height ~ father, data = Galton %>% filter(sex == \"F\")) %>% gf_smooth(method = \"lm\")\n\n\n\nShow the Codegf_point(height ~ mother, data = Galton %>% filter(sex == \"F\")) %>% gf_smooth(method = \"lm\")\n\n\n\n\nAn approximation to Galton‚Äôs famous plot (see Wikipedia):\n\nShow the Codegf_point(height ~ (father + mother)/2, data = Galton) %>% gf_smooth(method = \"lm\") %>% gf_density_2d(n = 8) %>% gf_abline(slope = 1)\n\n\n\n\nHow would you interpret this plot?\n\nWe will ‚Äúlive code‚Äù this in class!"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/30-Quant-Data-Correlations/index.html",
    "href": "content/courses/Descriptive-Analytics/Modules/30-Quant-Data-Correlations/index.html",
    "title": "üìé Correlations",
    "section": "",
    "text": "Slides and Tutorials\n.nbsp;.nbsp;   .nbsp;.nbsp;   .nbsp;.nbsp;"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/30-Quant-Data-Correlations/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/Descriptive-Analytics/Modules/30-Quant-Data-Correlations/index.html#what-graphs-will-we-see-today",
    "title": "üìé Correlations",
    "section": "\n1 What graphs will we see today?",
    "text": "1 What graphs will we see today?\nSome of the very basic and commonly used plots for data are:\n\nScatter Plot\nContour Plot\nCorrelogram\nHeatmap"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/30-Quant-Data-Correlations/index.html#what-correlation-charts-can-we-plot-with-numerical-quantitative-data",
    "href": "content/courses/Descriptive-Analytics/Modules/30-Quant-Data-Correlations/index.html#what-correlation-charts-can-we-plot-with-numerical-quantitative-data",
    "title": "üìé Correlations",
    "section": "\n2 What Correlation Charts can we plot with Numerical / Quantitative Data?",
    "text": "2 What Correlation Charts can we plot with Numerical / Quantitative Data?"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/30-Quant-Data-Correlations/index.html#a-workflow-in-orange",
    "href": "content/courses/Descriptive-Analytics/Modules/30-Quant-Data-Correlations/index.html#a-workflow-in-orange",
    "title": "üìé Correlations",
    "section": "\n3 A Workflow in Orange",
    "text": "3 A Workflow in Orange\nHow does one calculate and visualize Correlations in Orange?\n\nHere is the help page on Correlations in Orange:\n\nShow the Codeknitr::include_url(\"https://orange3.readthedocs.io/projects/orange-visual-programming/en/latest/widgets/data/correlations.html\")\n\n\n\nDownload the Orange Workflow file by clicking the icon below, and open it in Orange.\n{r, echo=FALSE} download_file( path = ‚Äúfiles/correlations.ows‚Äù, output_extension = ‚Äú.ows‚Äù, output_name = ‚ÄúOrange Workflow‚Äù, button_label = ‚ÄúDownload Orange Workflow‚Äù, button_type = ‚Äúdanger‚Äù, has_icon = TRUE, icon = ‚Äúfa fa-save‚Äù, self_contained = FALSE )"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/30-Quant-Data-Correlations/index.html#a-workflow-in-radiant",
    "href": "content/courses/Descriptive-Analytics/Modules/30-Quant-Data-Correlations/index.html#a-workflow-in-radiant",
    "title": "üìé Correlations",
    "section": "\n4 A Workflow in Radiant",
    "text": "4 A Workflow in Radiant\nDownload the Radiant Workflow statefile by clicking the icon above, and upload/open it in Radiant. You need to start radiant from the Add-In menu in RStudio.\n{r, echo=FALSE} download_file( path = ‚Äúfiles/correlation.rda‚Äù, output_extension = ‚Äú.rda‚Äù, output_name = ‚Äúradiant-correlation‚Äù, button_label = ‚ÄúDownload Radiant Workflow‚Äù, button_type = ‚Äúdanger‚Äù, has_icon = TRUE, icon = ‚Äúfa fa-save‚Äù, self_contained = FALSE )"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/30-Quant-Data-Correlations/index.html#a-workflow-in-r",
    "href": "content/courses/Descriptive-Analytics/Modules/30-Quant-Data-Correlations/index.html#a-workflow-in-r",
    "title": "üìé Correlations",
    "section": "\n5 A Workflow in R",
    "text": "5 A Workflow in R\nDownload the RMarkdown tutorial file by clicking the icon above and open it in RStudio or rstudio.cloud.\n{r, echo=FALSE} download_file( path = ‚Äúfiles/correlations.Rmd‚Äù, output_extension = ‚Äú.Rmd‚Äù, output_name = ‚ÄúR Markdown Workflow‚Äù, button_label = ‚ÄúDownload Orange Workflow‚Äù, button_type = ‚Äúdanger‚Äù, has_icon = TRUE, icon = ‚Äúfa fa-save‚Äù, self_contained = FALSE )"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/30-Quant-Data-Correlations/index.html#conclusion",
    "href": "content/courses/Descriptive-Analytics/Modules/30-Quant-Data-Correlations/index.html#conclusion",
    "title": "üìé Correlations",
    "section": "\n6 Conclusion",
    "text": "6 Conclusion"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/30-Quant-Data-Correlations/index.html#your-turn",
    "href": "content/courses/Descriptive-Analytics/Modules/30-Quant-Data-Correlations/index.html#your-turn",
    "title": "üìé Correlations",
    "section": "\n7 Your Turn",
    "text": "7 Your Turn"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/30-Quant-Data-Correlations/index.html#references",
    "href": "content/courses/Descriptive-Analytics/Modules/30-Quant-Data-Correlations/index.html#references",
    "title": "üìé Correlations",
    "section": "\n8 References",
    "text": "8 References\n\nMinimal R using mosaic. https://cran.r-project.org/web/packages/mosaic/vignettes/MinimalRgg.pdf"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/40-Evolution-and-Flow/index.html",
    "href": "content/courses/Descriptive-Analytics/Modules/40-Evolution-and-Flow/index.html",
    "title": "üï∏ Evolution and Flow",
    "section": "",
    "text": "Slides and Tutorials\n.nbsp;.nbsp;\n.nbsp;.nbsp;\n.nbsp;.nbsp;"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/40-Evolution-and-Flow/index.html#what-time-evolution-charts-can-we-plot",
    "href": "content/courses/Descriptive-Analytics/Modules/40-Evolution-and-Flow/index.html#what-time-evolution-charts-can-we-plot",
    "title": "üï∏ Evolution and Flow",
    "section": "\n1 What Time Evolution Charts can we plot?",
    "text": "1 What Time Evolution Charts can we plot?\nIn these cases, the x-axis is typically time‚Ä¶and we chart the variable of another Quant variable with respect to time, using a line geometry.\nLet is take a healthcare budget dataset from Our World in Data: We will plot graphs for 5 countries (India, China, Brazil, Russia, Canada )"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/40-Evolution-and-Flow/index.html#what-space-evolution-charts-can-we-plot",
    "href": "content/courses/Descriptive-Analytics/Modules/40-Evolution-and-Flow/index.html#what-space-evolution-charts-can-we-plot",
    "title": "üï∏ Evolution and Flow",
    "section": "\n2 What Space Evolution Charts can we plot?",
    "text": "2 What Space Evolution Charts can we plot?\nHere, the space can be any Qual variable, and we can chart another Quant or Qual variable move across levels of the first chosen Qual variable.\nFor instance we can contemplate Enrollment at a University, and show how students move from course to course in a University. Or how customers drift from one category of products or brands to another‚Ä¶.or the movement of cricket players from one IPL Team to another !!\n\nA sankey diagram is a visualization used to depict a flow from one set of values to another. The things being connected are called nodes and the connections are called links. Sankeys are best used when you want to show a many-to-many mapping between two domains or multiple paths through a set of stages.\n\n\nShow the Codeknitr::include_graphics(\"images/sankey.png\")\n\n\n\n\n\nShow the Codelibrary(ggalluvial)\ndata(\"Titanic\")\nTitanic %>% as_tibble() %>% \nggplot(data = .,\n       aes(axis1 = Class, axis2 = Sex, axis3 = Age,\n           y = n)) +\n  geom_alluvium(aes(fill = Survived)) +\n  geom_stratum() +\n  scale_x_discrete(limits = c(\"Class\", \"Sex\", \"Age\"), \n                   expand = c(.2, .05)) +\n  geom_text(stat = \"stratum\", aes(label = after_stat(stratum))) +\n  xlab(\"Demographic\") +\n  theme_minimal() +\n  ggtitle(\"passengers on the maiden voyage of the Titanic\",\n          \"stratified by demographics and survival\")\n\n\n\n\nHere is another example of a Sankey Diagram:This diagram show how energy is converted or transmitted before being consumed or lost: supplies are on the left, and demands are on the right. Data: Department of Energy & Climate Change via Tom Counsell"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/40-Evolution-and-Flow/index.html#datasets",
    "href": "content/courses/Descriptive-Analytics/Modules/40-Evolution-and-Flow/index.html#datasets",
    "title": "üï∏ Evolution and Flow",
    "section": "\n3 Datasets",
    "text": "3 Datasets\nTBD link here to .zip file"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/40-Evolution-and-Flow/index.html#a-workflow-in-orange",
    "href": "content/courses/Descriptive-Analytics/Modules/40-Evolution-and-Flow/index.html#a-workflow-in-orange",
    "title": "üï∏ Evolution and Flow",
    "section": "\n4 A Workflow in Orange",
    "text": "4 A Workflow in Orange\n\n4.1 Time/Line Charts in Orange\nTBD\n\n4.2 Sankey Charts in Orange\nA Sankey Diagram is currently not possible in Orange."
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/40-Evolution-and-Flow/index.html#a-workflow-in-radiant",
    "href": "content/courses/Descriptive-Analytics/Modules/40-Evolution-and-Flow/index.html#a-workflow-in-radiant",
    "title": "üï∏ Evolution and Flow",
    "section": "\n5 A Workflow in Radiant",
    "text": "5 A Workflow in Radiant\n\n5.1 Time/Line Charts in Radiant\n\n5.2 Sankey Charts in Orange\nRadiant does not offer plotting of Sankey diagrams."
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/40-Evolution-and-Flow/index.html#a-workflow-in-r",
    "href": "content/courses/Descriptive-Analytics/Modules/40-Evolution-and-Flow/index.html#a-workflow-in-r",
    "title": "üï∏ Evolution and Flow",
    "section": "\n6 A Workflow in R",
    "text": "6 A Workflow in R"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/40-Evolution-and-Flow/index.html#conclusion",
    "href": "content/courses/Descriptive-Analytics/Modules/40-Evolution-and-Flow/index.html#conclusion",
    "title": "üï∏ Evolution and Flow",
    "section": "\n7 Conclusion",
    "text": "7 Conclusion"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/40-Evolution-and-Flow/index.html#references",
    "href": "content/courses/Descriptive-Analytics/Modules/40-Evolution-and-Flow/index.html#references",
    "title": "üï∏ Evolution and Flow",
    "section": "\n8 References",
    "text": "8 References\n\nGlobal Migration, https://download.gsb.bund.de/BIB/global_flow/ A good example of the use of a Chord Diagram."
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/50-Part-of-a-Whole/files/parts.html",
    "href": "content/courses/Descriptive-Analytics/Modules/50-Part-of-a-Whole/files/parts.html",
    "title": "Part of a Whole in R",
    "section": "",
    "text": "We will create Data Visualizations in R to show Parts ofa Whole. As always, we will consistently use the Project Mosaic ecosystem of packages in R (mosaic, mosaicData and ggformula). Some specialized plots ( e.g.¬†Fan Plots) may require us to load other R Packages. These will be introduced appropriately.\n\n\n\n\nRecall the standard method for all commands from the mosaic package:\ngoal( y ~ x | z, data = mydata, ‚Ä¶)\n\n\nI have downloaded this data from Our World in Data: https://ourworldindata.org/ghg-emissions-by-sector\nLet us inspect import this data and inspect it:\n\nghg <- readxl::read_xlsx(\"data/Global-GHG-Emissions-by-sector-based-on-WRI-2020.xlsx\")\ninspect(ghg)\n#> \n#> categorical variables:  \n#>         name     class levels  n missing\n#> 1 Sub-sector character     29 29       0\n#>                                    distribution\n#> 1 Agricultural Soils (3.4%) ...                \n#> \n#> quantitative variables:  \n#>                                           name   class min  Q1 median  Q3  max\n#> 1 Share of global greenhouse gas emissions (%) numeric 0.1 1.3    1.9 4.1 11.9\n#>       mean       sd  n missing\n#> 1 3.448276 3.370314 29       0\n\nHmm‚Ä¶The names of the columns are bit unwieldy, so let‚Äôs clean them up:\n\nghg <- readxl::read_xlsx(\"data/Global-GHG-Emissions-by-sector-based-on-WRI-2020.xlsx\") %>% \n  janitor::clean_names() %>% \n  dplyr::rename(\"share\" = share_of_global_greenhouse_gas_emissions_percent) %>% \n  mutate(sub_sector = as_factor(sub_sector))\ninspect(ghg)\n#> \n#> categorical variables:  \n#>         name  class levels  n missing\n#> 1 sub_sector factor     29 29       0\n#>                                    distribution\n#> 1 Road (3.4%), Aviation (3.4%) ...             \n#> \n#> quantitative variables:  \n#>    name   class min  Q1 median  Q3  max     mean       sd  n missing\n#> 1 share numeric 0.1 1.3    1.9 4.1 11.9 3.448276 3.370314 29       0\n\nThis is a simple pre-aggregated dataset, with one categorical column and one numerical column, perfectly suited to plotting Pies and Fans. We will use this variable to plot a Pie chart and the much better Fan chart.\n\npie(ghg$share,labels = ghg$sub_sector,main = \"Greenhouse Emissions by Industry\")\n\n\n\n\nClearly this is messy‚Ä¶we will take the top 8 industries by emission and lump the rest into an other category:\n\n\nghg <- ghg %>% mutate(sub_sector = fct_lump(sub_sector, prop = 0.1))\nghg\n#> # A tibble: 29 √ó 2\n#>   sub_sector share\n#>   <fct>      <dbl>\n#> 1 Other       11.9\n#> 2 Other        1.9\n#> 3 Other        0.4\n#> 4 Other        0.3\n#> # ‚Ä¶ with 25 more rows\npie(ghg$share,labels = ghg$sub_sector,main = \"Greenhouse Emissions by Industry\")\n\n\n\n\n\nAs Stigler said, summaries are the first thing to look at in data. Let us tabulate some quick stat summaries of the important variables in Galton:\n\nfavstats(~ height, data = Galton)\n#>  min Q1 median   Q3 max     mean       sd   n missing\n#>   56 64   66.5 69.7  79 66.76069 3.582918 898       0\nfavstats(~ height | sex, data = Galton)\n#>   sex min   Q1 median   Q3  max     mean       sd   n missing\n#> 1   F  56 62.5   64.0 65.5 70.5 64.11016 2.370320 433       0\n#> 2   M  60 67.5   69.2 71.0 79.0 69.22882 2.631594 465       0\n\nQ.1 How many families in the data for each value of nkids?\n\ntally(~ nkids, data = Galton)\n#> nkids\n#>   1   2   3   4   5   6   7   8   9  10  11  15 \n#>  32  40  66 116 140 114 112 128  63  40  32  15\n\nQ.2. What is the break-up by sex of the child?\n\ntally(~ nkids | sex, data = Galton)\n#>      sex\n#> nkids  F  M\n#>    1  15 17\n#>    2  18 22\n#>    3  31 35\n#>    4  48 68\n#>    5  61 79\n#>    6  57 57\n#>    7  61 51\n#>    8  61 67\n#>    9  32 31\n#>    10 24 16\n#>    11 17 15\n#>    15  8  7\n\n\nWhat Questions might we have, that we could answer with a Distribution?\nQ.1 How many families based on the number of children?\n\n\n# Convert the tally into a dataframe. See the difference!\nfamily_count <- tally( ~ nkids | sex, data = Galton) %>% \n  as_tibble() %>% \n  \n  # Convert nkids from char to int\n  mutate( nkids = as.integer(nkids))\nfamily_count\n#> # A tibble: 24 √ó 3\n#>   nkids sex       n\n#>   <int> <chr> <int>\n#> 1     1 F        15\n#> 2     2 F        18\n#> 3     3 F        31\n#> 4     4 F        48\n#> # ‚Ä¶ with 20 more rows\ngf_col(n ~ nkids | sex, data = family_count, fill = ~ sex, ylab = \"Number of Families\", xlab = \"Number of Kids / Family\")\n\n\n\n\nQ.2: How are the children‚Äôs heights distributed?\n\ngf_histogram(~ height, data = Galton) %>% \n  gf_vline(xintercept = mean(Galton$height))\n\n\n\n\nQ.3: Is there a difference in height distributions between Male and Female children?\n\nmeasures <- favstats(~ height | sex, data = Galton)\nmeasures\n#>   sex min   Q1 median   Q3  max     mean       sd   n missing\n#> 1   F  56 62.5   64.0 65.5 70.5 64.11016 2.370320 433       0\n#> 2   M  60 67.5   69.2 71.0 79.0 69.22882 2.631594 465       0\n\ngf_histogram(~ height | sex, data = Galton) %>% \n  gf_vline(xintercept = ~ mean, data = measures)\n\n\n\n\nQ.4: Are Mothers generally shorter than fathers?\n\ngf_density(~ father, data = Galton, fill = \"blue\", alpha = 0.3) %>% \n  gf_density( ~ mother, data = Galton, fill = \"red\", alpha = 0.3, xlab = \"Heights\")\n#> Warning: `stat(density)` was deprecated in ggplot2 3.4.0.\n#> ‚Ñπ Please use `after_stat(density)` instead.\n\n\n\n\nQ.5: Are heights of children different based on the number of kids in the family? For Male and Female children?\n\ngf_boxplot(height ~ sex | nkids, data = Galton)\n\n\n\n\nQ.6: Does the mean height of children in a family vary with the number of children in the family? ( family size)\n\nmean( height ~ sex | nkids, data = Galton) %>% as_tibble() # not very inspiring!\n#> # A tibble: 36 √ó 1\n#>   value\n#>   <dbl>\n#> 1  64.3\n#> 2  68.8\n#> 3  63.9\n#> 4  68.8\n#> # ‚Ä¶ with 32 more rows\nby_sex_nkids <- favstats( height ~ sex + nkids, data = Galton)\nby_sex_nkids # much better!\n#>    sex.nkids  min     Q1 median     Q3  max     mean       sd  n missing\n#> 1        F.1 60.0 62.600  64.00 65.600 69.5 64.32667 2.423535 15       0\n#> 2        M.1 64.5 67.000  69.00 70.000 75.0 68.84706 2.631330 17       0\n#> 3        F.2 57.0 62.250  64.50 65.425 68.0 63.92222 2.753726 18       0\n#> 4        M.2 66.0 67.125  68.75 70.925 73.0 68.83182 2.075798 22       0\n#> 5        F.3 60.0 63.000  64.50 67.100 70.5 64.89677 2.650344 31       0\n#> 6        M.3 64.0 68.000  70.50 71.750 73.0 69.66286 2.589354 35       0\n#> 7        F.4 60.0 63.000  64.50 66.000 69.2 64.45000 2.535703 48       0\n#> 8        M.4 63.0 67.000  69.00 71.000 73.5 69.05441 2.500115 68       0\n#> 9        F.5 60.0 62.500  64.00 65.500 68.0 63.97213 2.004173 61       0\n#> 10       M.5 64.0 68.500  70.00 71.500 78.0 69.94684 2.477428 79       0\n#> 11       F.6 60.5 63.000  64.50 66.000 70.5 64.59298 2.281906 57       0\n#> 12       M.6 62.5 67.700  69.70 72.000 76.5 69.63158 2.798479 57       0\n#> 13       F.7 56.0 62.000  64.00 65.200 69.0 63.65902 2.309933 61       0\n#> 14       M.7 60.0 68.000  70.00 70.700 79.0 69.37255 2.940141 51       0\n#> 15       F.8 57.5 62.000  63.50 66.000 70.0 63.95574 2.547190 61       0\n#> 16       M.8 64.5 67.000  69.00 70.500 74.0 69.02687 2.483923 67       0\n#> 17       F.9 59.0 62.000  64.00 65.000 69.0 63.94375 2.326140 32       0\n#> 18       M.9 63.0 68.000  69.20 70.200 73.0 68.69677 2.702281 31       0\n#> 19      F.10 61.0 63.000  65.00 65.000 66.0 64.18750 1.538015 24       0\n#> 20      M.10 63.0 64.925  67.50 69.250 75.0 67.54375 3.178672 16       0\n#> 21      F.11 62.0 63.000  63.70 66.000 69.2 64.32353 1.964857 17       0\n#> 22      M.11 66.5 68.250  69.00 70.500 73.0 69.41333 1.818110 15       0\n#> 23      F.15 57.0 59.500  61.50 63.250 65.0 61.25000 2.815772  8       0\n#> 24      M.15 65.0 65.600  66.00 66.750 68.0 66.24286 1.014655  7       0\n\n\ngf_col(mean ~ sex.nkids, data = by_sex_nkids)\n\n\n\n\nHmm‚Ä¶not a very informative plot‚Ä¶\n\nLet us try the NHANES dataset. Try help(NHANES) in your Console.\n\ndata(\"NHANES\")\nnames(NHANES)\n#>  [1] \"ID\"               \"SurveyYr\"         \"Gender\"           \"Age\"             \n#>  [5] \"AgeDecade\"        \"AgeMonths\"        \"Race1\"            \"Race3\"           \n#>  [9] \"Education\"        \"MaritalStatus\"    \"HHIncome\"         \"HHIncomeMid\"     \n#> [13] \"Poverty\"          \"HomeRooms\"        \"HomeOwn\"          \"Work\"            \n#> [17] \"Weight\"           \"Length\"           \"HeadCirc\"         \"Height\"          \n#> [21] \"BMI\"              \"BMICatUnder20yrs\" \"BMI_WHO\"          \"Pulse\"           \n#> [25] \"BPSysAve\"         \"BPDiaAve\"         \"BPSys1\"           \"BPDia1\"          \n#> [29] \"BPSys2\"           \"BPDia2\"           \"BPSys3\"           \"BPDia3\"          \n#> [33] \"Testosterone\"     \"DirectChol\"       \"TotChol\"          \"UrineVol1\"       \n#> [37] \"UrineFlow1\"       \"UrineVol2\"        \"UrineFlow2\"       \"Diabetes\"        \n#> [41] \"DiabetesAge\"      \"HealthGen\"        \"DaysPhysHlthBad\"  \"DaysMentHlthBad\" \n#> [45] \"LittleInterest\"   \"Depressed\"        \"nPregnancies\"     \"nBabies\"         \n#> [49] \"Age1stBaby\"       \"SleepHrsNight\"    \"SleepTrouble\"     \"PhysActive\"      \n#> [53] \"PhysActiveDays\"   \"TVHrsDay\"         \"CompHrsDay\"       \"TVHrsDayChild\"   \n#> [57] \"CompHrsDayChild\"  \"Alcohol12PlusYr\"  \"AlcoholDay\"       \"AlcoholYear\"     \n#> [61] \"SmokeNow\"         \"Smoke100\"         \"Smoke100n\"        \"SmokeAge\"        \n#> [65] \"Marijuana\"        \"AgeFirstMarij\"    \"RegularMarij\"     \"AgeRegMarij\"     \n#> [69] \"HardDrugs\"        \"SexEver\"          \"SexAge\"           \"SexNumPartnLife\" \n#> [73] \"SexNumPartYear\"   \"SameSex\"          \"SexOrientation\"   \"PregnantNow\"\n\n\n\nmosaic::inspect(NHANES)\n#> \n#> categorical variables:  \n#>                name  class levels     n missing\n#> 1          SurveyYr factor      2 10000       0\n#> 2            Gender factor      2 10000       0\n#> 3         AgeDecade factor      8  9667     333\n#> 4             Race1 factor      5 10000       0\n#> 5             Race3 factor      6  5000    5000\n#> 6         Education factor      5  7221    2779\n#> 7     MaritalStatus factor      6  7231    2769\n#> 8          HHIncome factor     12  9189     811\n#> 9           HomeOwn factor      3  9937      63\n#> 10             Work factor      3  7771    2229\n#> 11 BMICatUnder20yrs factor      4  1274    8726\n#> 12          BMI_WHO factor      4  9603     397\n#> 13         Diabetes factor      2  9858     142\n#> 14        HealthGen factor      5  7539    2461\n#> 15   LittleInterest factor      3  6667    3333\n#> 16        Depressed factor      3  6673    3327\n#> 17     SleepTrouble factor      2  7772    2228\n#> 18       PhysActive factor      2  8326    1674\n#> 19         TVHrsDay factor      7  4859    5141\n#> 20       CompHrsDay factor      7  4863    5137\n#> 21  Alcohol12PlusYr factor      2  6580    3420\n#> 22         SmokeNow factor      2  3211    6789\n#> 23         Smoke100 factor      2  7235    2765\n#> 24        Smoke100n factor      2  7235    2765\n#> 25        Marijuana factor      2  4941    5059\n#> 26     RegularMarij factor      2  4941    5059\n#> 27        HardDrugs factor      2  5765    4235\n#> 28          SexEver factor      2  5767    4233\n#> 29          SameSex factor      2  5768    4232\n#> 30   SexOrientation factor      3  4842    5158\n#> 31      PregnantNow factor      3  1696    8304\n#>                                     distribution\n#> 1  2009_10 (50%), 2011_12 (50%)                 \n#> 2  female (50.2%), male (49.8%)                 \n#> 3   40-49 (14.5%),  0-9 (14.4%) ...             \n#> 4  White (63.7%), Black (12%) ...               \n#> 5  White (62.7%), Black (11.8%) ...             \n#> 6  Some College (31.4%) ...                     \n#> 7  Married (54.6%), NeverMarried (19.1%) ...    \n#> 8  more 99999 (24.2%) ...                       \n#> 9  Own (64.7%), Rent (33.1%) ...                \n#> 10 Working (59.4%), NotWorking (36.6%) ...      \n#> 11 NormWeight (63.2%), Obese (17.3%) ...        \n#> 12 18.5_to_24.9 (30.3%) ...                     \n#> 13 No (92.3%), Yes (7.7%)                       \n#> 14 Good (39.2%), Vgood (33.3%) ...              \n#> 15 None (76.5%), Several (16.9%) ...            \n#> 16 None (78.6%), Several (15.1%) ...            \n#> 17 No (74.6%), Yes (25.4%)                      \n#> 18 Yes (55.8%), No (44.2%)                      \n#> 19 2_hr (26.2%), 1_hr (18.2%) ...               \n#> 20 0_to_1_hr (29%), 0_hrs (22.1%) ...           \n#> 21 Yes (79.2%), No (20.8%)                      \n#> 22 No (54.3%), Yes (45.7%)                      \n#> 23 No (55.6%), Yes (44.4%)                      \n#> 24 Non-Smoker (55.6%), Smoker (44.4%)           \n#> 25 Yes (58.5%), No (41.5%)                      \n#> 26 No (72.4%), Yes (27.6%)                      \n#> 27 No (81.5%), Yes (18.5%)                      \n#> 28 Yes (96.1%), No (3.9%)                       \n#> 29 No (92.8%), Yes (7.2%)                       \n#> 30 Heterosexual (95.8%), Bisexual (2.5%) ...    \n#> 31 No (92.7%), Yes (4.2%) ...                   \n#> \n#> quantitative variables:  \n#>               name   class      min        Q1    median        Q3        max\n#> 1               ID integer 51624.00 56904.500 62159.500 67039.000  71915.000\n#> 2              Age integer     0.00    17.000    36.000    54.000     80.000\n#> 3        AgeMonths integer     0.00   199.000   418.000   624.000    959.000\n#> 4      HHIncomeMid integer  2500.00 30000.000 50000.000 87500.000 100000.000\n#> 5          Poverty numeric     0.00     1.240     2.700     4.710      5.000\n#> 6        HomeRooms integer     1.00     5.000     6.000     8.000     13.000\n#> 7           Weight numeric     2.80    56.100    72.700    88.900    230.700\n#> 8           Length numeric    47.10    75.700    87.000    96.100    112.200\n#> 9         HeadCirc numeric    34.20    39.575    41.450    42.925     45.400\n#> 10          Height numeric    83.60   156.800   166.000   174.500    200.400\n#> 11             BMI numeric    12.88    21.580    25.980    30.890     81.250\n#> 12           Pulse integer    40.00    64.000    72.000    82.000    136.000\n#> 13        BPSysAve integer    76.00   106.000   116.000   127.000    226.000\n#> 14        BPDiaAve integer     0.00    61.000    69.000    76.000    116.000\n#> 15          BPSys1 integer    72.00   106.000   116.000   128.000    232.000\n#> 16          BPDia1 integer     0.00    62.000    70.000    76.000    118.000\n#> 17          BPSys2 integer    76.00   106.000   116.000   128.000    226.000\n#> 18          BPDia2 integer     0.00    60.000    68.000    76.000    118.000\n#> 19          BPSys3 integer    76.00   106.000   116.000   126.000    226.000\n#> 20          BPDia3 integer     0.00    60.000    68.000    76.000    116.000\n#> 21    Testosterone numeric     0.25    17.700    43.820   362.410   1795.600\n#> 22      DirectChol numeric     0.39     1.090     1.290     1.580      4.030\n#> 23         TotChol numeric     1.53     4.110     4.780     5.530     13.650\n#> 24       UrineVol1 integer     0.00    50.000    94.000   164.000    510.000\n#> 25      UrineFlow1 numeric     0.00     0.403     0.699     1.221     17.167\n#> 26       UrineVol2 integer     0.00    52.000    95.000   171.750    409.000\n#> 27      UrineFlow2 numeric     0.00     0.475     0.760     1.513     13.692\n#> 28     DiabetesAge integer     1.00    40.000    50.000    58.000     80.000\n#> 29 DaysPhysHlthBad integer     0.00     0.000     0.000     3.000     30.000\n#> 30 DaysMentHlthBad integer     0.00     0.000     0.000     4.000     30.000\n#> 31    nPregnancies integer     1.00     2.000     3.000     4.000     32.000\n#> 32         nBabies integer     0.00     2.000     2.000     3.000     12.000\n#> 33      Age1stBaby integer    14.00    19.000    22.000    26.000     39.000\n#> 34   SleepHrsNight integer     2.00     6.000     7.000     8.000     12.000\n#> 35  PhysActiveDays integer     1.00     2.000     3.000     5.000      7.000\n#> 36   TVHrsDayChild integer     0.00     1.000     2.000     3.000      6.000\n#> 37 CompHrsDayChild integer     0.00     0.000     1.000     6.000      6.000\n#> 38      AlcoholDay integer     1.00     1.000     2.000     3.000     82.000\n#> 39     AlcoholYear integer     0.00     3.000    24.000   104.000    364.000\n#> 40        SmokeAge integer     6.00    15.000    17.000    19.000     72.000\n#> 41   AgeFirstMarij integer     1.00    15.000    16.000    19.000     48.000\n#> 42     AgeRegMarij integer     5.00    15.000    17.000    19.000     52.000\n#> 43          SexAge integer     9.00    15.000    17.000    19.000     50.000\n#> 44 SexNumPartnLife integer     0.00     2.000     5.000    12.000   2000.000\n#> 45  SexNumPartYear integer     0.00     1.000     1.000     1.000     69.000\n#>            mean           sd     n missing\n#> 1  6.194464e+04 5.871167e+03 10000       0\n#> 2  3.674210e+01 2.239757e+01 10000       0\n#> 3  4.201239e+02 2.590431e+02  4962    5038\n#> 4  5.720617e+04 3.302028e+04  9189     811\n#> 5  2.801844e+00 1.677909e+00  9274     726\n#> 6  6.248918e+00 2.277538e+00  9931      69\n#> 7  7.098180e+01 2.912536e+01  9922      78\n#> 8  8.501602e+01 1.370503e+01   543    9457\n#> 9  4.118068e+01 2.311483e+00    88    9912\n#> 10 1.618778e+02 2.018657e+01  9647     353\n#> 11 2.666014e+01 7.376579e+00  9634     366\n#> 12 7.355973e+01 1.215542e+01  8563    1437\n#> 13 1.181550e+02 1.724817e+01  8551    1449\n#> 14 6.748006e+01 1.435480e+01  8551    1449\n#> 15 1.190902e+02 1.749636e+01  8237    1763\n#> 16 6.827826e+01 1.378078e+01  8237    1763\n#> 17 1.184758e+02 1.749133e+01  8353    1647\n#> 18 6.766455e+01 1.441978e+01  8353    1647\n#> 19 1.179292e+02 1.717719e+01  8365    1635\n#> 20 6.729874e+01 1.495839e+01  8365    1635\n#> 21 1.978980e+02 2.265045e+02  4126    5874\n#> 22 1.364865e+00 3.992581e-01  8474    1526\n#> 23 4.879220e+00 1.075583e+00  8474    1526\n#> 24 1.185161e+02 9.033648e+01  9013     987\n#> 25 9.792946e-01 9.495143e-01  8397    1603\n#> 26 1.196759e+02 9.016005e+01  1478    8522\n#> 27 1.149372e+00 1.072948e+00  1476    8524\n#> 28 4.842289e+01 1.568050e+01   629    9371\n#> 29 3.334838e+00 7.400700e+00  7532    2468\n#> 30 4.126493e+00 7.832971e+00  7534    2466\n#> 31 3.026882e+00 1.795341e+00  2604    7396\n#> 32 2.456954e+00 1.315227e+00  2416    7584\n#> 33 2.264968e+01 4.772509e+00  1884    8116\n#> 34 6.927531e+00 1.346729e+00  7755    2245\n#> 35 3.743513e+00 1.836358e+00  4663    5337\n#> 36 1.938744e+00 1.434431e+00   653    9347\n#> 37 2.197550e+00 2.516667e+00   653    9347\n#> 38 2.914123e+00 3.182672e+00  4914    5086\n#> 39 7.510165e+01 1.030337e+02  5922    4078\n#> 40 1.782662e+01 5.326660e+00  3080    6920\n#> 41 1.702283e+01 3.895010e+00  2891    7109\n#> 42 1.769107e+01 4.806103e+00  1366    8634\n#> 43 1.742870e+01 3.716551e+00  5540    4460\n#> 44 1.508507e+01 5.784643e+01  5725    4275\n#> 45 1.342330e+00 2.782688e+00  4928    5072\n\nAgain, lots of data from inspect, about the Quant and Qual variables. Spend a little time looking through the output of inspect. Which variables could have been data given by each respondent, and which ones could have been measured data variables? Why do you think so?\nWhy is there so much missing data? Which variable are the most affected by this?\n\ntally(Education ~ Work, data = NHANES)\n#>                 Work\n#> Education        Looking NotWorking Working <NA>\n#>   8th Grade           13        249     188    1\n#>   9 - 11th Grade      39        438     411    0\n#>   High School         52        579     886    0\n#>   Some College        88        792    1387    0\n#>   College Grad        72        474    1552    0\n#>   <NA>                47        315     189 2228\n\n\nQ.1. What is the distribution of Physical Activity Days, across Gender? Across Education?\n\ngf_histogram(data = NHANES, ~ PhysActiveDays | Gender)\n#> Warning: Removed 5337 rows containing non-finite values (`stat_bin()`).\n\n\n\ngf_histogram(data = NHANES, ~ PhysActiveDays | Education)\n#> Warning: Removed 5337 rows containing non-finite values (`stat_bin()`).\n\n\n\n\nQ.1. How are people Ages distributed across levels of Education?\n\ngf_boxplot(Age ~ Education, data = NHANES)\n\n\n\n\nQ.2. How is Education distributed over Race?\n\ngf_histogram()\n#> gf_histogram() uses \n#>     * a formula with shape ~x or y ~ . or y ~ x. \n#>     * geom:  bar \n#>     * stat:  bin \n#>     * position:  stack \n#>     * key attributes:  bins = 25, binwidth, alpha = 0.5, color, fill, group,\n#>                    linetype, size \n#> Note:  y may be stat(density) or stat(count) or stat(ndensity) or stat(ncount), but see gf_dhistogram().\n#> \n#> For more information, try ?gf_histogram\n\nQ.3 What is the distribution of people‚Äôs BMI, split by Gender? By Race1?\n\ngf_histogram(~ BMI | Gender, data = NHANES)\n#> Warning: Removed 366 rows containing non-finite values (`stat_bin()`).\n\n\n\ngf_histogram(~ BMI | Race1 + Race3, data = NHANES)\n#> Warning: Removed 366 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nHere is a dataset from Jeremy Singer-Vine‚Äôs blog, Data Is Plural. This is a list of all books banned in schools across the US.\n\nbanned <- readxl::read_xlsx(path = \"data/banned.xlsx\",\n                            sheet = \"Sorted by Author & Title\")\nbanned\n#> # A tibble: 1,586 √ó 10\n#>   Author     Title Type ‚Ä¶¬π Secon‚Ä¶¬≤ Illus‚Ä¶¬≥ Trans‚Ä¶‚Å¥ State Distr‚Ä¶‚Åµ Date ‚Ä¶‚Å∂ Origi‚Ä¶‚Å∑\n#>   <chr>      <chr> <chr>   <chr>   <chr>   <chr>   <chr> <chr>   <chr>   <chr>  \n#> 1 √Äb√≠k√©-√çy√≠‚Ä¶ Ace ‚Ä¶ Banned‚Ä¶ <NA>    <NA>    <NA>    Flor‚Ä¶ Indian‚Ä¶ 44501   Admini‚Ä¶\n#> 2 Acevedo, ‚Ä¶ Clap‚Ä¶ Banned‚Ä¶ <NA>    <NA>    <NA>    Penn‚Ä¶ Centra‚Ä¶ 44440   Admini‚Ä¶\n#> 3 Acevedo, ‚Ä¶ The ‚Ä¶ Banned‚Ä¶ <NA>    <NA>    <NA>    Flor‚Ä¶ Indian‚Ä¶ 44501   Admini‚Ä¶\n#> 4 Acevedo, ‚Ä¶ The ‚Ä¶ Banned‚Ä¶ <NA>    <NA>    <NA>    New ‚Ä¶ Marlbo‚Ä¶ 44593   Admini‚Ä¶\n#> # ‚Ä¶ with 1,582 more rows, and abbreviated variable names ¬π‚Äã`Type of Ban`,\n#> #   ¬≤‚Äã`Secondary Author(s)`, ¬≥‚Äã`Illustrator(s)`, ‚Å¥‚Äã`Translator(s)`, ‚Åµ‚ÄãDistrict,\n#> #   ‚Å∂‚Äã`Date of Challenge/Removal`, ‚Å∑‚Äã`Origin of Challenge`\n\nnames(banned)\n#>  [1] \"Author\"                    \"Title\"                    \n#>  [3] \"Type of Ban\"               \"Secondary Author(s)\"      \n#>  [5] \"Illustrator(s)\"            \"Translator(s)\"            \n#>  [7] \"State\"                     \"District\"                 \n#>  [9] \"Date of Challenge/Removal\" \"Origin of Challenge\"\n\nClearly the variables are all Qualitative, except perhaps for Date of Challenge/Removal, (which in this case has been badly mangled by Excel) So we need to make counts based on the levels* of the Qual variables and plot Bar/Column charts.\nLet us quickly make some Stat Summaries:\n\nmosaic::inspect(banned)\n#> \n#> categorical variables:  \n#>                         name     class levels    n missing\n#> 1                     Author character    797 1586       0\n#> 2                      Title character   1145 1586       0\n#> 3                Type of Ban character      4 1586       0\n#> 4        Secondary Author(s) character     61   98    1488\n#> 5             Illustrator(s) character    192  364    1222\n#> 6              Translator(s) character      9   10    1576\n#> 7                      State character     26 1586       0\n#> 8                   District character     86 1586       0\n#> 9  Date of Challenge/Removal character     15 1586       0\n#> 10       Origin of Challenge character      2 1586       0\n#>                                     distribution\n#> 1  Kobabe, Maia (1.9%) ...                      \n#> 2  Gender Queer: A Memoir (1.9%) ...            \n#> 3  Banned Pending Investigation (46.1%) ...     \n#> 4  Cast, Kristin (12.2%) ...                    \n#> 5  Aly, Hatem (4.7%) ...                        \n#> 6  Mlawer, Teresa (20%) ...                     \n#> 7  Texas (45%), Pennsylvania (28.8%) ...        \n#> 8  Central York (27.8%) ...                     \n#> 9  44440 (28.8%), 44531 (28.3%) ...             \n#> 10 Administrator (95.6%) ...\n\nLet us try to answer this question: What is the count of banned books by type and by US state?\n\nbanned_by_state <- banned %>% group_by(State) %>% summarise(total = n()) %>% ungroup()\nbanned_by_state\n#> # A tibble: 26 √ó 2\n#>   State    total\n#>   <chr>    <int>\n#> 1 Alaska       1\n#> 2 Arkansas     1\n#> 3 Florida    204\n#> 4 Georgia     13\n#> # ‚Ä¶ with 22 more rows\n\nbanned %>% group_by(State, `Type of Ban`) %>% summarise(count = n()) %>% ungroup() %>% left_join(., banned_by_state, by = c(\"State\" = \"State\")) %>% \n #  pivot_wider(.,id_cols = State,\n #              names_from = `Type of Ban`,\n #              values_from = count) %>% janitor::clean_names() %>% \n #  replace_na(list(banned_from_libraries_and_classrooms = 0,\n #                  banned_from_libraries = 0,\n #                  banned_pending_investigation = 0,\n #                  banned_from_classrooms = 0)) %>% \n # mutate(total = sum(across(where(is.integer)))) %>%\nggplot(aes(x = reorder(State, total), y = count, fill = `Type of Ban`)) + geom_col() + coord_flip()\n#> `summarise()` has grouped output by 'State'. You can override using the\n#> `.groups` argument.\n\n\n\n\n\n\n#Conclusion"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/50-Part-of-a-Whole/index.html",
    "href": "content/courses/Descriptive-Analytics/Modules/50-Part-of-a-Whole/index.html",
    "title": "üçï Parts of a Whole",
    "section": "",
    "text": "Slides and Tutorials\n.nbsp;.nbsp;\n.nbsp;.nbsp;\n.nbsp;.nbsp;"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/50-Part-of-a-Whole/index.html#an-example",
    "href": "content/courses/Descriptive-Analytics/Modules/50-Part-of-a-Whole/index.html#an-example",
    "title": "üçï Parts of a Whole",
    "section": "\n1 An Example",
    "text": "1 An Example\nWhat influences love at first sight? (Or, at least, love in the first four minutes?)\nThis dataset was compiled by Columbia Business School professors Ray Fisman and Sheena Iyengar for their paper Gender Differences in Mate Selection: Evidence From a Speed DatingExperiment.\nData was gathered from participants in experimental speed dating events from 2002-2004. During the events, the attendees would have a four minute ‚Äúfirst date‚Äù with every other participant of the opposite sex. At the end of their four minutes, participants were asked if they would like to see their date again. They were also asked to rate their date on six attributes: Attractiveness, Sincerity, Intelligence, Fun, Ambition, and Shared Interests."
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/50-Part-of-a-Whole/index.html#what-is-the-story-here",
    "href": "content/courses/Descriptive-Analytics/Modules/50-Part-of-a-Whole/index.html#what-is-the-story-here",
    "title": "üçï Parts of a Whole",
    "section": "\n2 What is the Story here?",
    "text": "2 What is the Story here?\n\nDo people prefer to date someone from their own race?\nIs that different for men and women?\nDoes physical attractiveness help to overcome age differences?"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/50-Part-of-a-Whole/index.html#a-workflow-in-orange",
    "href": "content/courses/Descriptive-Analytics/Modules/50-Part-of-a-Whole/index.html#a-workflow-in-orange",
    "title": "üçï Parts of a Whole",
    "section": "\n3 A Workflow in Orange",
    "text": "3 A Workflow in Orange"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/50-Part-of-a-Whole/index.html#a-workflow-in-radiant",
    "href": "content/courses/Descriptive-Analytics/Modules/50-Part-of-a-Whole/index.html#a-workflow-in-radiant",
    "title": "üçï Parts of a Whole",
    "section": "\n4 A Workflow in Radiant",
    "text": "4 A Workflow in Radiant"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/50-Part-of-a-Whole/index.html#a-workflow-in-r",
    "href": "content/courses/Descriptive-Analytics/Modules/50-Part-of-a-Whole/index.html#a-workflow-in-r",
    "title": "üçï Parts of a Whole",
    "section": "\n5 A Workflow in R",
    "text": "5 A Workflow in R"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/60-Ranking/index.html",
    "href": "content/courses/Descriptive-Analytics/Modules/60-Ranking/index.html",
    "title": "üñè Ratings and Rankings",
    "section": "",
    "text": "Slides and Tutorials\n.nbsp;.nbsp;\n.nbsp;.nbsp;\n.nbsp;.nbsp;"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/60-Ranking/index.html#a-workflow-in-orange",
    "href": "content/courses/Descriptive-Analytics/Modules/60-Ranking/index.html#a-workflow-in-orange",
    "title": "üñè Ratings and Rankings",
    "section": "\n1 A Workflow in Orange",
    "text": "1 A Workflow in Orange"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/60-Ranking/index.html#a-workflow-in-radiant",
    "href": "content/courses/Descriptive-Analytics/Modules/60-Ranking/index.html#a-workflow-in-radiant",
    "title": "üñè Ratings and Rankings",
    "section": "\n2 A Workflow in Radiant",
    "text": "2 A Workflow in Radiant"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/60-Ranking/index.html#a-workflow-in-r",
    "href": "content/courses/Descriptive-Analytics/Modules/60-Ranking/index.html#a-workflow-in-r",
    "title": "üñè Ratings and Rankings",
    "section": "\n3 A Workflow in R",
    "text": "3 A Workflow in R"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/70-Space/index.html",
    "href": "content/courses/Descriptive-Analytics/Modules/70-Space/index.html",
    "title": "üó∫ Maps",
    "section": "",
    "text": ".nbsp;.nbsp;\n  .nbsp;.nbsp;\n  .nbsp;.nbsp;\n  .nbsp;.nbsp;"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/70-Space/index.html#what-kind-of-maps-will-we-make",
    "href": "content/courses/Descriptive-Analytics/Modules/70-Space/index.html#what-kind-of-maps-will-we-make",
    "title": "üó∫ Maps",
    "section": "\n1.1 What kind of maps will we make?",
    "text": "1.1 What kind of maps will we make?\n\n1.1.1 Chlorpleth\n\n\n\n1.1.2 Bubble Map"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/70-Space/index.html#a-workflow-in-orange",
    "href": "content/courses/Descriptive-Analytics/Modules/70-Space/index.html#a-workflow-in-orange",
    "title": "üó∫ Maps",
    "section": "\n1.2 A Workflow in Orange",
    "text": "1.2 A Workflow in Orange"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/70-Space/index.html#a-workflow-in-radiant",
    "href": "content/courses/Descriptive-Analytics/Modules/70-Space/index.html#a-workflow-in-radiant",
    "title": "üó∫ Maps",
    "section": "\n1.3 A Workflow in Radiant",
    "text": "1.3 A Workflow in Radiant\nRadiant does not support map-making. We need to do it one of the many many ways available in R directly!"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/70-Space/index.html#a-workflow-in-r",
    "href": "content/courses/Descriptive-Analytics/Modules/70-Space/index.html#a-workflow-in-r",
    "title": "üó∫ Maps",
    "section": "\n1.4 A Workflow in R",
    "text": "1.4 A Workflow in R\n\n1.4.1 1. Animal and Bird Migration\n\nHead off to movebank.org\n\nLook at a few species of interest and choose one.\nDownload the data ( ESRI Shapefile)\nImport that into R using sf_read()\n\nSee how you can plot locations, tracks and colour by species‚Ä¶.based on the data you download.\n\nFor tutorial info: https://movebankworkshopraleighnc.netlify.app/\n\n1.4.2 2. UFO Sightings\nHere is a UFO Sighting dataset, containing location and text descriptions. https://github.com/planetsig/ufo-reports/blob/master/csv-data/ufo-scrubbed-geocoded-time-standardized.csv"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/80-Time/files/timeseries.html",
    "href": "content/courses/Descriptive-Analytics/Modules/80-Time/files/timeseries.html",
    "title": "Analysis and Plotting of Time Series in R",
    "section": "",
    "text": "Any metric that is measured over regular time intervals forms a time series. Analysis of Time Series is commercially important because of industrial need and relevance, especially with respect to Forecasting (Weather data, sports scores, population growth figures, stock prices, demand, sales, supply‚Ä¶). In the graph shown below is the temperature over times in two US cities:\n\n\nA time series can be broken down to its components so as to systematically understand, analyze, model and forecast it. We have to begin by answering fundamental questions such as:\n\nWhat are the types of time series?\n\nHow to decompose it? How to extract a level, a trend, and seasonal components from a time series?\n\nWhat is auto correlation etc.\n\nWhat is a stationary time series?\n\nAnd, how do you plot time series?"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/80-Time/files/timeseries.html#creating-and-plotting-time-series",
    "href": "content/courses/Descriptive-Analytics/Modules/80-Time/files/timeseries.html#creating-and-plotting-time-series",
    "title": "Analysis and Plotting of Time Series in R",
    "section": "\n1.1 Creating and Plotting Time Series",
    "text": "1.1 Creating and Plotting Time Series\nIn this first example, we will use simple ts data first, and then do another with tsibble format, and then a third example with a tibble that we can plot as is and do more after conversion to tsibble format.\n\n1.1.1 ts format data\nThere are a few datasets in base R that are in ts format already.\n\nShow the CodeAirPassengers\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1949 112 118 132 129 121 135 148 148 136 119 104 118\n1950 115 126 141 135 125 149 170 170 158 133 114 140\n1951 145 150 178 163 172 178 199 199 184 162 146 166\n1952 171 180 193 181 183 218 230 242 209 191 172 194\n1953 196 196 236 235 229 243 264 272 237 211 180 201\n1954 204 188 235 227 234 264 302 293 259 229 203 229\n1955 242 233 267 269 270 315 364 347 312 274 237 278\n1956 284 277 317 313 318 374 413 405 355 306 271 306\n1957 315 301 356 348 355 422 465 467 404 347 305 336\n1958 340 318 362 348 363 435 491 505 404 359 310 337\n1959 360 342 406 396 420 472 548 559 463 407 362 405\n1960 417 391 419 461 472 535 622 606 508 461 390 432\n\nShow the Codestr(AirPassengers)\n\n Time-Series [1:144] from 1949 to 1961: 112 118 132 129 121 135 148 148 136 119 ...\n\n\nThis can be easily plotted using base R:\n\nShow the Codeplot(AirPassengers)\n\n\n\n\nOne can see that there is an upward trend and also seasonal variations that also increase over time.\nLet us take data that is ‚Äútime oriented‚Äù but not in ts format: the syntax of ts() is:\nSyntax:  objectName <- ts(data, start, end, frequency)\n\nwhere,\n\n    `data`: represents the data vector\n    `start`: represents the first observation in time series\n    `end`: represents the last observation in time series\n    `frequency`: represents number of observations per unit time. For \n    example 1=annual, 4=quarterly, 12=monthly, etc.\nWe will pick simple numerical vector data variable from trees:\n\nShow the Codetrees\n\n\n\n  \n\n\nShow the Code# Choosing the `height` variable\ntrees_ts <- ts(trees$Height, \n               frequency = 1, # No reason to believe otherwise\n               start = 1980)  # Arbitrarily picked \"1980\" !\nplot(trees_ts)\n\n\n\n\n( Note that this example is just for demonstration: tree heights do not decrease over time!!)"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/80-Time/files/timeseries.html#tsibble-data",
    "href": "content/courses/Descriptive-Analytics/Modules/80-Time/files/timeseries.html#tsibble-data",
    "title": "Analysis and Plotting of Time Series in R",
    "section": "\n1.2 tsibble data",
    "text": "1.2 tsibble data\nThe package tsibbledata contains several ready made tsibble format data. Run data(package = \"tsibbledata\") to find out about these. Let us try PBS which is a dataset containing Monthly Medicare prescription data in Australia.\n\nShow the Codedata(\"PBS\")\n\n\nThis is a large dataset, with 1M observations, for 336 combinations of key variables. Data appears to be monthly. Note that there is more than one quantitative variable, which one would not be able to support in the ts format.\nThere are multiple Quantitative variables ( Scripts and Cost). The Qualitative Variables are described below. (Type help(\"PBS\") in your Console)\n\nThe data is disaggregated using four keys:\n\n\nConcession: Concessional scripts are given to pensioners, unemployed, dependents, and other card holders\nType: Co-payments are made until an individual‚Äôs script expenditure hits a threshold ($290.00 for concession, $1141.80 otherwise). Safety net subsidies are provided to individuals exceeding this amount.\nATC1: Anatomical Therapeutic Chemical index (level 1) ATC2: Anatomical Therapeutic Chemical index (level 2)\n\nLet us simply plot Cost over time:\n\nShow the CodePBS %>% ggplot(aes(x = Month, y = Cost)) + \n  geom_point() + \n  geom_line()\n\n\n\n\nThis basic plot is quite messy. We ought to use dplyr to filter the data using some combination of the Qualitative variables( 336 combinations!). Let us try that now:\n\nShow the CodePBS %>% count(ATC1, ATC2, Concession, Type)\n\n\n\n  \n\n\n\nWe have 336 combinations of Qualitative variables, each containing 204 observations: so let us filter for a few such combinations and plot:\n\nShow the CodePBS %>% dplyr::filter(Concession == \"General\", \n                      ATC1 == \"A\",\n                      ATC2 == \"A10\") %>% \n  ggplot(aes(x = Month, y = Cost, colour = Type)) + \n  geom_line() + \n  geom_point()\n\n\n\n\nAs can be seen, very different time patterns based on the two Types of payment methods. Strongly seasonal for both, with seasonal variation increasing over the years, but there is an upward trend with the Co-payments method of payment."
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/80-Time/files/timeseries.html#tibble-data",
    "href": "content/courses/Descriptive-Analytics/Modules/80-Time/files/timeseries.html#tibble-data",
    "title": "Analysis and Plotting of Time Series in R",
    "section": "\n1.3 tibble data",
    "text": "1.3 tibble data\nLet us read and inspect in the US births data from 2000 to 2014. Download this data by clicking on the icon below, and saving the downloaded file in a sub-folder called data inside your project:\nRead this data in:\n\n\nRows: 5,479\nColumns: 5\n$ year          <dbl> 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 20‚Ä¶\n$ month         <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,‚Ä¶\n$ date_of_month <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1‚Ä¶\n$ day_of_week   <dbl> 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3,‚Ä¶\n$ births        <dbl> 9083, 8006, 11363, 13032, 12558, 12466, 12516, 8934, 794‚Ä¶\n\n\nSo there are several numerical variables for year, month, and day_of_month, day_of_week, and of course the births on a daily basis. We will create a date column with these separate ones above, and then plot the births, say for the month of March, in each year:\n\n\n\n\n  \n\n\n\n\n\n\nHmm‚Ä¶can we try to plot box plots over time (Candle-Stick Plots)? Over month / quarter or year?\n\nShow the Code# Monthly box plots\nbirths_tsibble %>%\n  index_by(month_index = ~ yearmonth(.)) %>% # 180 months over 15 years\n  # No need to summarise, since we want boxplots per year / month\n  ggplot(., aes(y = births, x = date, \n                group =  month_index)) + # plot the groups\n  \n  geom_boxplot(aes(fill = month_index))      # 180 plots!!  \n\n\n\nShow the Code# Quarterly boxplots\nbirths_tsibble %>%\n  index_by(qrtr_index = ~ yearquarter(.)) %>% # 60 quarters over 15 years\n  # No need to summarise, since we want boxplots per year / month\n  ggplot(., aes(y = births, x = date, \n                group = qrtr_index)) +\n  \n  geom_boxplot(aes(fill = qrtr_index))        # 60 plots!!\n\n\n\nShow the Code# Yearwise boxplots\nbirths_tsibble %>% \n  index_by(year_index = ~ lubridate::year(.)) %>% # 15 years, 15 groups\n    # No need to summarise, since we want boxplots per year / month\n\n  ggplot(., aes(y = births, \n                x = date, \n                group = year_index)) + # plot the groups\n  \n  geom_boxplot(aes(fill = year_index)) +           # 15 plots\n  scale_fill_distiller(palette = \"Spectral\")\n\n\n\n\nAlthough the graphs are very busy, they do reveal seasonality trends at different periods."
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/80-Time/files/timeseries.html#seasons-trends-cycles-and-random-changes",
    "href": "content/courses/Descriptive-Analytics/Modules/80-Time/files/timeseries.html#seasons-trends-cycles-and-random-changes",
    "title": "Analysis and Plotting of Time Series in R",
    "section": "\n1.4 Seasons, Trends, Cycles, and Random Changes",
    "text": "1.4 Seasons, Trends, Cycles, and Random Changes\n\nTrend A trend exists when there is a long-term increase or decrease in the data. It does not have to be linear. Sometimes we will refer to a trend as ‚Äúchanging direction‚Äù, when it might go from an increasing trend to a decreasing trend.\n\n\nSeasonal A seasonal pattern occurs when a time series is affected by seasonal factors such as the time of the year or the day of the week. Seasonality is always of a fixed and known period. The monthly sales of drugs ( with the PBD data ) shows seasonality which is induced partly by the change in the cost of the drugs at the end of the calendar year.\n\n\nCyclic A cycle occurs when the data exhibit rises and falls that are not of a fixed frequency. These fluctuations are usually due to economic conditions, and are often related to the ‚Äúbusiness cycle‚Äù. The duration of these fluctuations is usually at least 2 years.\n\nLet us try to find and plot these patterns in Time Series.\n\nShow the Codebirths_STL_yearly <- births_tsibble %>% \n  fabletools::model(STL(births ~ season(period = \"year\")))\n\nfabletools::components(births_STL_yearly)\n\n\n\n  \n\n\nShow the Codefeasts::autoplot(components(births_STL_yearly))\n\n\n\n\nHow about a heatmap? We can cook up a categorical variable based on the number of births (low, fine, high) and use that to create a heatmap:"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/80-Time/files/timeseries.html#case-study--1-walmart-sales-dataset-from-timetk",
    "href": "content/courses/Descriptive-Analytics/Modules/80-Time/files/timeseries.html#case-study--1-walmart-sales-dataset-from-timetk",
    "title": "Analysis and Plotting of Time Series in R",
    "section": "\n1.5 Case Study -1: Walmart Sales Dataset from timetk\n",
    "text": "1.5 Case Study -1: Walmart Sales Dataset from timetk\n\nLet us inspect what datasets are available in the package timetk. Type data(package = \"timetk\") in your Console to see what datasets are available.\nLet us choose the Walmart Sales dataset. See here for more details: Walmart Recruiting - Store Sales Forecasting |Kaggle\n\nShow the Codedata(\"walmart_sales_weekly\")\nwalmart_sales_weekly\n\n\n\n  \n\n\nShow the Codeinspect(walmart_sales_weekly)\n\n\ncategorical variables:  \n       name     class levels    n missing\n1        id    factor   3331 1001       0\n2 IsHoliday   logical      2 1001       0\n3      Type character      1 1001       0\n                                   distribution\n1 1_1 (14.3%), 1_3 (14.3%), 1_8 (14.3%) ...    \n2 FALSE (93%), TRUE (7%)                       \n3 A (100%)                                     \n\nDate variables:  \n  name class      first       last min_diff max_diff    n missing\n1 Date  Date 2010-02-05 2012-10-26   0 days   7 days 1001       0\n\nquantitative variables:  \n           name   class         min          Q1      median          Q3\n1         Store numeric      1.0000      1.0000      1.0000      1.0000\n2          Dept numeric      1.0000      3.0000     13.0000     93.0000\n3  Weekly_Sales numeric   6165.7300  28257.3000  39886.0600  77943.5700\n4          Size numeric 151315.0000 151315.0000 151315.0000 151315.0000\n5   Temperature numeric     35.4000     57.7900     69.6400     80.4900\n6    Fuel_Price numeric      2.5140      2.7590      3.2900      3.5940\n7     MarkDown1 numeric    410.3100   4039.3900   6154.1400  10121.9700\n8     MarkDown2 numeric      0.5000     40.4800    144.8700   1569.0000\n9     MarkDown3 numeric      0.2500      6.0000     25.9650    101.6400\n10    MarkDown4 numeric      8.0000    577.1400   1822.5500   3750.5900\n11    MarkDown5 numeric    554.9200   3127.8800   4325.1900   6222.2500\n12          CPI numeric    210.3374    211.5312    215.4599    220.6369\n13 Unemployment numeric      6.5730      7.3480      7.7870      7.8380\n           max         mean           sd    n missing\n1       1.0000 1.000000e+00 0.000000e+00 1001       0\n2      95.0000 3.585714e+01 3.849159e+01 1001       0\n3  148798.0500 5.464634e+04 3.627627e+04 1001       0\n4  151315.0000 1.513150e+05 0.000000e+00 1001       0\n5      91.6500 6.830678e+01 1.420767e+01 1001       0\n6       3.9070 3.219699e+00 4.260286e-01 1001       0\n7   34577.0600 8.090766e+03 6.550983e+03  357     644\n8   46011.3800 2.941315e+03 7.873661e+03  294     707\n9   55805.5100 1.225400e+03 7.811934e+03  350     651\n10  32403.8700 3.746085e+03 5.948867e+03  357     644\n11  20475.3200 5.018655e+03 3.254071e+03  357     644\n12    223.4443 2.159969e+02 4.337818e+00 1001       0\n13      8.1060 7.610420e+00 3.825958e-01 1001       0\n\nShow the Codeglimpse(walmart_sales_weekly)\n\nRows: 1,001\nColumns: 17\n$ id           <fct> 1_1, 1_1, 1_1, 1_1, 1_1, 1_1, 1_1, 1_1, 1_1, 1_1, 1_1, 1_‚Ä¶\n$ Store        <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ Dept         <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ Date         <date> 2010-02-05, 2010-02-12, 2010-02-19, 2010-02-26, 2010-03-‚Ä¶\n$ Weekly_Sales <dbl> 24924.50, 46039.49, 41595.55, 19403.54, 21827.90, 21043.3‚Ä¶\n$ IsHoliday    <lgl> FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA‚Ä¶\n$ Type         <chr> \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A‚Ä¶\n$ Size         <dbl> 151315, 151315, 151315, 151315, 151315, 151315, 151315, 1‚Ä¶\n$ Temperature  <dbl> 42.31, 38.51, 39.93, 46.63, 46.50, 57.79, 54.58, 51.45, 6‚Ä¶\n$ Fuel_Price   <dbl> 2.572, 2.548, 2.514, 2.561, 2.625, 2.667, 2.720, 2.732, 2‚Ä¶\n$ MarkDown1    <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ MarkDown2    <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ MarkDown3    <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ MarkDown4    <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ MarkDown5    <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ CPI          <dbl> 211.0964, 211.2422, 211.2891, 211.3196, 211.3501, 211.380‚Ä¶\n$ Unemployment <dbl> 8.106, 8.106, 8.106, 8.106, 8.106, 8.106, 8.106, 8.106, 7‚Ä¶\n\nShow the Code# Try this in your Console\n# help(\"walmart_sales_weekly\")\n\n\nThe data is described as:\n\nA tibble: 9,743 x 3\n\n\nid Factor. Unique series identifier (4 total)\n\nStore Numeric. Store ID.\n\nDept Numeric. Department ID.\n\nDate Date. Weekly timestamp.\n\nWeekly_Sales Numeric. Sales for the given department in the given store.\n\nIsHoliday Logical. Whether the week is a ‚Äúspecial‚Äù holiday for the store.\n\nType Character. Type identifier of the store.\n\nSize Numeric. Store square-footage\n\nTemperature Numeric. Average temperature in the region.\n\nFuel_Price Numeric. Cost of fuel in the region.\n\nMarkDown1, MarkDown2, MarkDown3, MarkDown4, MarkDown5 Numeric. Anonymized data related to promotional markdowns that Walmart is running. MarkDown data is only available after Nov 2011, and is not available for all stores all the time. Any missing value is marked with an NA.\n\nCPI Numeric. The consumer price index.\n\nUnemployment Numeric. The unemployment rate in the region.\n\n\nVery cool to know that mosaic::inspect() identifies date variables separately!\nNOTE: 1. This is still a data.frame, with a time-oriented variable of course, and not yet a time-series object. Note that this data frame has the YMD columns repeated for each Dept. 2. The Date column has repeated entries for each Dept! To deal with this repetition, we will always need to split the Weekly_Sales by the Dept column before we plot or analyze.\nSince our sales are weekly, we will convert Date to yearweek format:\n\nShow the Codewalmart_time <- walmart_sales_weekly %>% \n  # mutate(Date = as.Date(Date)) %>% \n  as_tsibble(index = Date, # Time Variable\n             key = Dept \n             \n  #  Identifies unique \"subject\" who are measures\n  #  All other variables such as Weekly_sales become \"measured variable\"\n  #  Each observation should be uniquely identified by index and key\n\n             )\nwalmart_time\n\n\n\n  \n\n\n\n\n1.5.1 Basic Time Series Plots\nThe easiest way is to use autoplot from the feasts package. You may need to specify the actual measured variable, if there is more than one numerical column:\n\nShow the Codeautoplot(walmart_time,.vars = Weekly_Sales)\n\n\n\n\ntimetk gives us interactive plots that may be more evocative than the static plot above. The basic plot function with timetk is plot_time_series. There are arguments for the date variable, the value you want to plot, colours, groupings etc.\nLet us explore this dataset using timetk, using our trusted method of asking Questions:\nQ.1 How are the weekly sales different for each Department?\nThere are\n\n\n\n\nnumber of Departments. So we should be fine plotting them and also facetting with them, as we will see in a bit:\n\nShow the Codewalmart_time %>%   timetk::plot_time_series(Date, Weekly_Sales,\n                   .color_var = Dept, .legend_show = TRUE,\n                   .title = \"Walmart Sales Data by Department\",\n                   .smooth = FALSE)\n\n\n\n\n\nQ.2. What do the sales per Dept look like during the month of December (Christmas time) in 2012? Show the individual Depts as facets.\nWe can of course zoom into the interactive plot above, but if we were to plot it anyway:\n\nShow the Code# Only include rows from  1 to December 31, 2011\n# Data goes only up to Oct 2012\n\nwalmart_time %>% \n  # Each side of the time_formula is specified as the character 'YYYY-MM-DD HH:MM:SS',\n  timetk::filter_by_time(.date_var = Date,\n                         .start_date = \"2011-12-01\",\n                         .end_date = \"2011-12-31\") %>%\n\n  plot_time_series(.date_var = Date, \n                   .value = Weekly_Sales, \n                   .color_var = Dept, \n                   .facet_vars = Dept, \n                   .facet_ncol = 2,\n                   .smooth = FALSE) # Only 4 points per graph\n\n\n\n\n\nClearly the ‚Äúunfortunate‚Äù Dept#13 has seen something of a Christmas drop in sales, as has Dept#38 ! The rest, all is well, it seems‚Ä¶\n\n1.5.2 Too much noise? How about some averaging?\nQ.3 How do we smooth out some of the variations in the time series to be able to understand it better?\nSometimes there is too much noise in the time series observations and we want to take what is called a rolling average. For this we will use the function timetk::slidify to create an averaging function of our choice, and then apply it to the time series using regular dplyr::mutate\n\nShow the Code# Let's take the average of Sales for each month in each Department.\n# Our **function** will be named \"rolling_avg_month\": \n\nrolling_avg_month = slidify(.period = 4, # every 4 weeks\n                            .f = mean, # The funtion to average\n                            .align = \"center\", # Aligned with middle of month\n                            .partial = TRUE) # TO catch any leftover half weeks\nrolling_avg_month\n\nfunction (...) \n{\n    slider_2(..., .slider_fun = slider::pslide, .f = .f, .period = .period, \n        .align = .align, .partial = .partial, .unlist = .unlist)\n}\n<bytecode: 0x0000024cfb3466c8>\n<environment: 0x0000024cfb347030>\n\n\nOK, slidify creates a function! Let‚Äôs apply it to the Walmart Sales time series‚Ä¶\n\nShow the Codewalmart_time %>% \n  # group_by(Dept) %>% \n  mutate(avg_monthly_sales = rolling_avg_month(Weekly_Sales)) %>% \n  # ungroup() %>% \n  timetk::plot_time_series(Date, avg_monthly_sales,.color_var = Dept, .smooth = FALSE)\n\n\n\n\n\nCurves are smoother now. Need to check whether the averaging was done on a per-Dept basis‚Ä¶should we have had a group_by(Dept) before the averaging, and ungroup() before plotting? Try it !!\n\n1.5.3 Decomposing Time Series: Trends, Seasonal Patterns, and Cycles\nEach data point (\\(Y_t\\)) at time \\(t\\) in a Time Series can be expressed as either a sum or a product of 4 components, namely, Seasonality(\\(S_t\\)), Trend(\\(T_t\\)), Cyclic, and Error(\\(e_t\\)) (a.k.a White Noise).\n\nTrend: pattern exists when there is a long-term increase or decrease in the data.\nSeasonal: pattern exists when a series is influenced by seasonal factors (e.g., the quarter of the year, the month, or day of the week).\nCyclic: pattern exists when data exhibit rises and falls that are not of fixed period (duration usually of at least 2 years).\nError or Noise: Random component\n\nDecomposing non-seasonal datas means breaking it up into trend and irregular components.To estimate the trend component of a non-seasonal time series that can be described using an additive model, it is common to use a smoothing method, such as calculating the simple moving average of the time series.\ntimetk has the ability to achieve this: Let us plot the trend, seasonal, cyclic and irregular aspects of Weekly_Sales for Dept 38:\n\nShow the Codewalmart_time %>% \n  filter(Dept == \"38\") %>% \n  timetk::plot_stl_diagnostics(.date_var = Date, .value = Weekly_Sales)\n\nfrequency = 13 observations per 1 quarter\n\n\ntrend = 52 observations per 1 year\n\n\n\n\n\n\nWe can do this for all Dept using fable and fabletools:\n\nShow the Codewalmart_decomposed <- \n  walmart_time %>% \n  \n  # If we want to filter, we do it here\n  # filter(Dept == \"38\") %>% \n  # \n\nfabletools::model(stl = STL(Weekly_Sales))\n\nfabletools::components(walmart_decomposed)\n\n\n\n  \n\n\nShow the Codeautoplot(components((walmart_decomposed)))"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/80-Time/files/timeseries.html#case-study-2-dataset-from-nycflights13",
    "href": "content/courses/Descriptive-Analytics/Modules/80-Time/files/timeseries.html#case-study-2-dataset-from-nycflights13",
    "title": "Analysis and Plotting of Time Series in R",
    "section": "\n1.6 Case Study-2: Dataset from nycflights13\n",
    "text": "1.6 Case Study-2: Dataset from nycflights13\n\nLet us try the flights dataset from the package nycflights13. Try data(package = \"nycflights13\") in your Console.\nWe have the following datasets innycflights13:\nData sets in package nycflights13:\n\n\nairlines Airline names.\n\nairports Airport metadata\n\nflights Flights data\n\nplanes Plane metadata.\n\nweather Hourly weather data\n\nLet us analyze the flights data:\n\nShow the Codedata(\"flights\", package = \"nycflights13\")\nmosaic::inspect(flights)\n\n\ncategorical variables:  \n     name     class levels      n missing\n1 carrier character     16 336776       0\n2 tailnum character   4043 334264    2512\n3  origin character      3 336776       0\n4    dest character    105 336776       0\n                                   distribution\n1 UA (17.4%), B6 (16.2%), EV (16.1%) ...       \n2 N725MQ (0.2%), N722MQ (0.2%) ...             \n3 EWR (35.9%), JFK (33%), LGA (31.1%)          \n4 ORD (5.1%), ATL (5.1%), LAX (4.8%) ...       \n\nquantitative variables:  \n             name   class  min   Q1 median   Q3  max        mean          sd\n1            year integer 2013 2013   2013 2013 2013 2013.000000    0.000000\n2           month integer    1    4      7   10   12    6.548510    3.414457\n3             day integer    1    8     16   23   31   15.710787    8.768607\n4        dep_time integer    1  907   1401 1744 2400 1349.109947  488.281791\n5  sched_dep_time integer  106  906   1359 1729 2359 1344.254840  467.335756\n6       dep_delay numeric  -43   -5     -2   11 1301   12.639070   40.210061\n7        arr_time integer    1 1104   1535 1940 2400 1502.054999  533.264132\n8  sched_arr_time integer    1 1124   1556 1945 2359 1536.380220  497.457142\n9       arr_delay numeric  -86  -17     -5   14 1272    6.895377   44.633292\n10         flight integer    1  553   1496 3465 8500 1971.923620 1632.471938\n11       air_time numeric   20   82    129  192  695  150.686460   93.688305\n12       distance numeric   17  502    872 1389 4983 1039.912604  733.233033\n13           hour numeric    1    9     13   17   23   13.180247    4.661316\n14         minute numeric    0    8     29   44   59   26.230100   19.300846\n        n missing\n1  336776       0\n2  336776       0\n3  336776       0\n4  328521    8255\n5  336776       0\n6  328521    8255\n7  328063    8713\n8  336776       0\n9  327346    9430\n10 336776       0\n11 327346    9430\n12 336776       0\n13 336776       0\n14 336776       0\n\ntime variables:  \n       name   class               first                last min_diff   max_diff\n1 time_hour POSIXct 2013-01-01 05:00:00 2013-12-31 23:00:00   0 secs 25200 secs\n       n missing\n1 336776       0\n\n\nWe have time-related columns; Apart from year, month, day we have time_hour; and time-event numerical data such as arr_delay (arrival delay) and dep_delay (departure delay). We also have categorical data such as carrier, origin, dest, flight and tailnum of the aircraft. It is also a large dataset containing 330K entries. Enough to play with!!\nLet us replace the NAs in arr_delay and dep_delay with zeroes for now, and convert it into a time-series object with tsibble:\n\nShow the Codeflights_delay_ts <- flights %>% \n  \n  mutate(arr_delay = replace_na(arr_delay, 0), \n         dep_delay = replace_na(dep_delay, 0)) %>% \n  \n  select(time_hour, arr_delay, dep_delay, carrier, origin, dest, flight, tailnum) %>% \n  tsibble::as_tsibble(index = time_hour, \n                      # All the remaining identify unique entries\n                      # Along with index\n                      # Many of these variables are common\n                      # Need *all* to make unique entries!\n                      key = c(carrier, origin, dest,flight, tailnum), \n                      validate = TRUE) # Making sure each entry is unique\n\n\nflights_delay_ts\n\n\n\n  \n\n\n\nQ.1. Plot the monthly average arrival delay by carrier\n\nShow the Codemean_arr_delays_by_carrier <- \n  flights_delay_ts %>%\n  group_by(carrier) %>% \n  \n  index_by(month = ~ yearmonth(.)) %>% \n  # index_by uses (year, yearquarter, yearmonth, yearweek, as.Date)\n  # to create a new column to show the time-grouping\n  # year / quarter / month/ week, or day...\n  # which IS different from traditional dplyr\n  \n  summarise(mean_arr_delay = mean(arr_delay, na.rm = TRUE)\n  )\n\nmean_arr_delays_by_carrier\n\n\n\n  \n\n\nShow the Codemean_arr_delays_by_carrier %>%\n  timetk::plot_time_series(\n    .date_var = month,\n    .value = mean_arr_delay,\n    .facet_vars = carrier,\n    .smooth = FALSE,\n    # .smooth_degree = 1,\n    \n    # keep .smooth off since it throws warnings if there are too few points\n    # Like if we do quarterly or even yearly summaries\n    # Use only for smaller values of .smooth_degree (0,1)\n    #\n    .facet_ncol = 4,\n    .title = \"Average Monthly Arrival Delays by Carrier\"\n  )\n\n\n\n\n\nQ.2. Plot a candlestick chart for total flight delays by month for each carrier\n\nShow the Codeflights_delay_ts %>% \n  mutate(total_delay = arr_delay + dep_delay) %>%\n  timetk::plot_time_series_boxplot(\n    .date_var = time_hour,\n    .value = total_delay,\n    .color_var = origin,\n    .facet_vars = origin,\n    .period = \"month\",\n  # same warning again\n    .smooth = FALSE\n  )\n\n\n\n\n\nQ.2. Plot a heatmap chart for total flight delays by origin, aggregated by month\n\nShow the Codeavg_delays_month <- flights_delay_ts %>% \n  group_by(origin) %>% \n  mutate(total_delay = arr_delay + dep_delay) %>% \n  index_by(month = ~ yearmonth(.)) %>% \n  # index_by uses (year, yearquarter, yearmonth, yearweek, as.Date)\n  # to create a new column to show the time-grouping\n  # year / quarter / month/ week, or day...\n  # which IS different from traditional dplyr\n    summarise(mean_monthly_delay = mean(total_delay, na.rm = TRUE)\n  )\n\navg_delays_month \n\n\n\n  \n\n\nShow the Code# three origins 12 months therefore 36 rows\n# Tsibble index_by + summarise also gives us a  month` column \n\n\n\nggformula::gf_tile(origin ~ month, fill = ~ mean_monthly_delay, \n                   color = \"black\", data = avg_delays_month,\n                   title = \"Mean Flight Delays from NY Airports in 2013\") %>% \n  gf_theme(theme_classic()) %>% \n  gf_theme(scale_fill_viridis_c(option = \"A\")) %>% \n  \n# \"magma\" (or \"A\") inferno\" (or \"B\") \"plasma\" (or \"C\") \n# \"viridis\" (or \"D\") \"cividis\" (or \"E\") \n# \"rocket\" (or \"F\") \"mako\" (or \"G\") \"turbo\" (or \"H\")\n\n  gf_theme(scale_x_time(breaks = 1:12, labels = month.abb))"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/80-Time/index.html",
    "href": "content/courses/Descriptive-Analytics/Modules/80-Time/index.html",
    "title": "üïî Time Series",
    "section": "",
    "text": "Slides and Tutorials\n.nbsp;.nbsp;\n.nbsp;.nbsp;\n.nbsp;.nbsp;"
  },
  {
    "objectID": "content/courses/Descriptive-Analytics/Modules/80-Time/index.html#readings",
    "href": "content/courses/Descriptive-Analytics/Modules/80-Time/index.html#readings",
    "title": "üïî Time Series",
    "section": "\n1 Readings",
    "text": "1 Readings\n\nThe Nuclear Threat‚ÄîThe Shadow Peace, part 1\n11 Ways to Visualize Changes Over Time ‚Äì A Guide\nWhat is seasonal adjustment and why is it used?\nThe start-at-zero rule\nKeeping one‚Äôs appetite after touring the sausage factory\nHow Common is Your Birthday? This Visualization Might Surprise You\nThe Fallen of World War II\nVisualizing Statistical Mix Effects and Simpson‚Äôs Paradox\nHow To Fix a Toilet (And Other Things We Couldn‚Äôt Do Without Search)"
  },
  {
    "objectID": "content/courses/Predictive-Analytics/listing.html",
    "href": "content/courses/Predictive-Analytics/listing.html",
    "title": "Predictive Analytics",
    "section": "",
    "text": "Order By\nDefault\n\n          Date - Oldest\n        \n\n          Date - Newest\n        \n\n          Title\n        \n\n          Author\n        \n\n    \n      \n      \n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\nReading Time\n\n\n\n\n\n\n\n\nOct 17, 2022\n\n\nüêâ Intro to Orange\n\n\nArvind Venkatadri\n\n\n1 min\n\n\n\n\n\n¬†\n\n\n\nAug 16, 2022\n\n\nML - Regression\n\n\nArvind Venkatadri\n\n\n3 min\n\n\n\n\n\n\n\nJul 22, 2022\n\n\nML - Classification\n\n\nArvind Venkatadri\n\n\n9 min\n\n\n\n\n\n¬†\n\n\n\nJul 19, 2022\n\n\nML - Clustering\n\n\nArvind Venkatadri\n\n\n1 min\n\n\n\n\n\n¬†\n\n\n\nNov 19, 2022\n\n\nüïî Modelling Time Series\n\n\nArvind Venkatadri\n\n\n7 min\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/courses/Predictive-Analytics/Modules/10-Introduction-to-Orange/index.html",
    "href": "content/courses/Predictive-Analytics/Modules/10-Introduction-to-Orange/index.html",
    "title": "üêâ Intro to Orange",
    "section": "",
    "text": "Orange is a visual drag-and-drop tool for\n\nData visualization\n\nMachine Learning\n\nData mining\n\nand much more. Here is what it looks like:\n\nAll operations are done using a visual menu-driven interface. The visuals can be exported to PNG/SVG/PDF and the entire workflow can be exported into a Report Form and edited for presentation and sharing."
  },
  {
    "objectID": "content/courses/Predictive-Analytics/Modules/10-Introduction-to-Orange/index.html#installing-orange",
    "href": "content/courses/Predictive-Analytics/Modules/10-Introduction-to-Orange/index.html#installing-orange",
    "title": "üêâ Intro to Orange",
    "section": "Installing Orange",
    "text": "Installing Orange\nYou can download and install Orange from here:\nhttps://orangedatamining.com/download/"
  },
  {
    "objectID": "content/courses/Predictive-Analytics/Modules/10-Introduction-to-Orange/index.html#basic-usage-of-orange",
    "href": "content/courses/Predictive-Analytics/Modules/10-Introduction-to-Orange/index.html#basic-usage-of-orange",
    "title": "üêâ Intro to Orange",
    "section": "Basic Usage of Orange",
    "text": "Basic Usage of Orange\n{{% youtube \"HXjnDIgGDuI\" %}}"
  },
  {
    "objectID": "content/courses/Predictive-Analytics/Modules/10-Introduction-to-Orange/index.html#orange-workflows",
    "href": "content/courses/Predictive-Analytics/Modules/10-Introduction-to-Orange/index.html#orange-workflows",
    "title": "üêâ Intro to Orange",
    "section": "Orange Workflows",
    "text": "Orange Workflows\n{{% youtube \"lb-x36xqJ-E\" %}}"
  },
  {
    "objectID": "content/courses/Predictive-Analytics/Modules/10-Introduction-to-Orange/index.html#widgets-and-channels",
    "href": "content/courses/Predictive-Analytics/Modules/10-Introduction-to-Orange/index.html#widgets-and-channels",
    "title": "üêâ Intro to Orange",
    "section": "Widgets and Channels",
    "text": "Widgets and Channels\n{{% youtube \"2xS6QjnG714\" %}}"
  },
  {
    "objectID": "content/courses/Predictive-Analytics/Modules/10-Introduction-to-Orange/index.html#loading-data-into-orange",
    "href": "content/courses/Predictive-Analytics/Modules/10-Introduction-to-Orange/index.html#loading-data-into-orange",
    "title": "üêâ Intro to Orange",
    "section": "Loading data into Orange",
    "text": "Loading data into Orange\n{{% youtube \"MHcGdQeYCMg\" %}} \nWe are good to get started with Orange!!"
  },
  {
    "objectID": "content/courses/Predictive-Analytics/Modules/10-Introduction-to-Orange/index.html#simple-visuals-using-orange",
    "href": "content/courses/Predictive-Analytics/Modules/10-Introduction-to-Orange/index.html#simple-visuals-using-orange",
    "title": "üêâ Intro to Orange",
    "section": "Simple Visuals using Orange",
    "text": "Simple Visuals using Orange\nLet us create some simple visualizations using Orange.\n\nUse the File Widget to import the iris dataset into your session\nUse the Data Table Widget to look at the data, and note its variable names\nUse the Visualization Widgets ( Scatter Plot, Bar Plot, and Distributions) to look at the properties of the variables, and examine relationships between them."
  },
  {
    "objectID": "content/courses/Predictive-Analytics/Modules/10-Introduction-to-Orange/index.html#reference",
    "href": "content/courses/Predictive-Analytics/Modules/10-Introduction-to-Orange/index.html#reference",
    "title": "üêâ Intro to Orange",
    "section": "Reference",
    "text": "Reference\n\nIntroduction to Data Mining-Working notes for the hands-on course with Orange Data Mining. (Download file)"
  },
  {
    "objectID": "content/courses/Predictive-Analytics/Modules/20-Regression/index.html",
    "href": "content/courses/Predictive-Analytics/Modules/20-Regression/index.html",
    "title": "ML - Regression",
    "section": "",
    "text": "Interpolation:\n\nbetween TWO colours, both colours inclusive using a straight line between them\n\nbetween several different colours?\n\nby mixing ‚Äúequal proportions‚Äù of each\n\nProportions based on ‚Äúdistance‚Äù from each colour\n\nOn a ‚Äúplane‚Äù with these points"
  },
  {
    "objectID": "content/courses/Predictive-Analytics/Modules/20-Regression/index.html#workflow-in-orange",
    "href": "content/courses/Predictive-Analytics/Modules/20-Regression/index.html#workflow-in-orange",
    "title": "ML - Regression",
    "section": "Workflow in Orange",
    "text": "Workflow in Orange\nLet us ‚Äúdraw inspiration‚Äù from the picture above, and see if we can replicate it. We will fire up Orange, paint some data and see if we can fit a linear regression ML model to it.\nHere is the Orange file for you to download. Open this file in Orange."
  },
  {
    "objectID": "content/courses/Predictive-Analytics/Modules/20-Regression/index.html#workflow-in-radiant",
    "href": "content/courses/Predictive-Analytics/Modules/20-Regression/index.html#workflow-in-radiant",
    "title": "ML - Regression",
    "section": "Workflow in Radiant",
    "text": "Workflow in Radiant"
  },
  {
    "objectID": "content/courses/Predictive-Analytics/Modules/20-Regression/index.html#workflow-in-r",
    "href": "content/courses/Predictive-Analytics/Modules/20-Regression/index.html#workflow-in-r",
    "title": "ML - Regression",
    "section": "Workflow in R",
    "text": "Workflow in R"
  },
  {
    "objectID": "content/courses/Predictive-Analytics/Modules/20-Regression/index.html#conclusion",
    "href": "content/courses/Predictive-Analytics/Modules/20-Regression/index.html#conclusion",
    "title": "ML - Regression",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "content/courses/Predictive-Analytics/Modules/30-Classification/files/Random-Forests.html",
    "href": "content/courses/Predictive-Analytics/Modules/30-Classification/files/Random-Forests.html",
    "title": "Random Forests",
    "section": "",
    "text": "Machine Learning Basics - Random Forest at Shirin‚Äôs Playground"
  },
  {
    "objectID": "content/courses/Predictive-Analytics/Modules/30-Classification/files/Random-Forests.html#penguin-random-forest-model-withrandomforest",
    "href": "content/courses/Predictive-Analytics/Modules/30-Classification/files/Random-Forests.html#penguin-random-forest-model-withrandomforest",
    "title": "Random Forests",
    "section": "Penguin Random Forest Model withrandomForest\n",
    "text": "Penguin Random Forest Model withrandomForest\n\nUsing the penguins dataset and Random Forest Classification.\n\npenguins\n\n# A tibble: 344 √ó 8\n   species island    bill_length_mm bill_depth_mm flipper_‚Ä¶¬π body_‚Ä¶¬≤ sex    year\n   <fct>   <fct>              <dbl>         <dbl>      <int>   <int> <fct> <int>\n 1 Adelie  Torgersen           39.1          18.7        181    3750 male   2007\n 2 Adelie  Torgersen           39.5          17.4        186    3800 fema‚Ä¶  2007\n 3 Adelie  Torgersen           40.3          18          195    3250 fema‚Ä¶  2007\n 4 Adelie  Torgersen           NA            NA           NA      NA <NA>   2007\n 5 Adelie  Torgersen           36.7          19.3        193    3450 fema‚Ä¶  2007\n 6 Adelie  Torgersen           39.3          20.6        190    3650 male   2007\n 7 Adelie  Torgersen           38.9          17.8        181    3625 fema‚Ä¶  2007\n 8 Adelie  Torgersen           39.2          19.6        195    4675 male   2007\n 9 Adelie  Torgersen           34.1          18.1        193    3475 <NA>   2007\n10 Adelie  Torgersen           42            20.2        190    4250 <NA>   2007\n# ‚Ä¶ with 334 more rows, and abbreviated variable names ¬π‚Äãflipper_length_mm,\n#   ¬≤‚Äãbody_mass_g\n\nsummary(penguins)\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2                                 \n\npenguins %>% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n344\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nspecies\n0\n1.00\nFALSE\n3\nAde: 152, Gen: 124, Chi: 68\n\n\nisland\n0\n1.00\nFALSE\n3\nBis: 168, Dre: 124, Tor: 52\n\n\nsex\n11\n0.97\nFALSE\n2\nmal: 168, fem: 165\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nbill_length_mm\n2\n0.99\n43.92\n5.46\n32.1\n39.23\n44.45\n48.5\n59.6\n‚ñÉ‚ñá‚ñá‚ñÜ‚ñÅ\n\n\nbill_depth_mm\n2\n0.99\n17.15\n1.97\n13.1\n15.60\n17.30\n18.7\n21.5\n‚ñÖ‚ñÖ‚ñá‚ñá‚ñÇ\n\n\nflipper_length_mm\n2\n0.99\n200.92\n14.06\n172.0\n190.00\n197.00\n213.0\n231.0\n‚ñÇ‚ñá‚ñÉ‚ñÖ‚ñÇ\n\n\nbody_mass_g\n2\n0.99\n4201.75\n801.95\n2700.0\n3550.00\n4050.00\n4750.0\n6300.0\n‚ñÉ‚ñá‚ñÜ‚ñÉ‚ñÇ\n\n\nyear\n0\n1.00\n2008.03\n0.82\n2007.0\n2007.00\n2008.00\n2009.0\n2009.0\n‚ñá‚ñÅ‚ñá‚ñÅ‚ñá\n\n\n\n\npenguins <- penguins %>% tidyr::drop_na()\n# Spent one hour trying to find `drop-na()` ( 14 June 2020)\n\n\n# library(corrplot)\ncor <- penguins %>% select(is.numeric) %>% cor() \n\nWarning: Use of bare predicate functions was deprecated in tidyselect 1.1.0.\n‚Ñπ Please use wrap predicates in `where()` instead.\n  # Was:\n  data %>% select(is.numeric)\n\n  # Now:\n  data %>% select(where(is.numeric))\n\ncor %>% corrplot(., method = \"ellipse\", order = \"hclust\",tl.cex = 0.5)\n\n\n\n# try these too:\n# cor %>% corrplot(., method = \"square\", order = \"hclust\",tl.cex = 0.5)\n# cor %>% corrplot(., method = \"color\", order = \"hclust\",tl.cex = 0.5)\n# cor %>% corrplot(., method = \"shade\", order = \"hclust\",tl.cex = 0.5)\n\nNotes: - flipper_length_mm and culmen_depth_mm are negtively correlated at approx (-0.7) - flipper_length_mm and body_mass_g are positively correlated at approx 0.8\nSo we will use steps in the recipe to remove correlated variables.\nPenguin Data Sampling and Recipe\n\n# Data Split\npenguin_split <- initial_split(penguins, prop = 0.6)\npenguin_train <- training(penguin_split)\npenguin_test <- testing(penguin_split)\npenguin_split\n\n<Training/Testing/Total>\n<199/134/333>\n\nhead(penguin_train)\n\n# A tibble: 6 √ó 8\n  species   island bill_length_mm bill_depth_mm flipper_le‚Ä¶¬π body_‚Ä¶¬≤ sex    year\n  <fct>     <fct>           <dbl>         <dbl>        <int>   <int> <fct> <int>\n1 Adelie    Dream            37.3          17.8          191    3350 fema‚Ä¶  2008\n2 Chinstrap Dream            50.2          18.8          202    3800 male   2009\n3 Gentoo    Biscoe           44.5          14.7          214    4850 fema‚Ä¶  2009\n4 Gentoo    Biscoe           44.4          17.3          219    5250 male   2008\n5 Gentoo    Biscoe           45.2          14.8          212    5200 fema‚Ä¶  2009\n6 Chinstrap Dream            48.5          17.5          191    3400 male   2007\n# ‚Ä¶ with abbreviated variable names ¬π‚Äãflipper_length_mm, ¬≤‚Äãbody_mass_g\n\n# Recipe\npenguin_recipe <- penguins %>% \n  recipe(species ~ .) %>% \n  step_normalize(all_numeric()) %>% # Scaling and Centering\n  step_corr(all_numeric()) %>%  # Handling correlated variables\n  prep()\n\n# Baking the data\npenguin_train_baked <-  penguin_train %>% \n  bake(object = penguin_recipe, new_data = .)\n\npenguin_test_baked <-  penguin_test %>% \n  bake(object = penguin_recipe, new_data = .)\n\nhead(penguin_train_baked)\n\n# A tibble: 6 √ó 8\n  island bill_length_mm bill_depth_mm flipper_le‚Ä¶¬π body_‚Ä¶¬≤ sex      year species\n  <fct>           <dbl>         <dbl>        <dbl>   <dbl> <fct>   <dbl> <fct>  \n1 Dream         -1.22          0.323       -0.711   -1.06  fema‚Ä¶ -0.0517 Adelie \n2 Dream          1.14          0.830        0.0737  -0.506 male   1.18   Chinst‚Ä¶\n3 Biscoe         0.0927       -1.25         0.930    0.798 fema‚Ä¶  1.18   Gentoo \n4 Biscoe         0.0745        0.0686       1.29     1.30  male  -0.0517 Gentoo \n5 Biscoe         0.221        -1.20         0.787    1.23  fema‚Ä¶  1.18   Gentoo \n6 Dream          0.824         0.170       -0.711   -1.00  male  -1.28   Chinst‚Ä¶\n# ‚Ä¶ with abbreviated variable names ¬π‚Äãflipper_length_mm, ¬≤‚Äãbody_mass_g\n\n\nPenguin Random Forest Model\n\npenguin_model <- \n  rand_forest(trees = 100) %>% \n  set_engine(\"randomForest\") %>% \n  set_mode(\"classification\")\npenguin_model\n\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  trees = 100\n\nComputational engine: randomForest \n\npenguin_fit <- \n  penguin_model %>% \n  fit(species ~ .,penguin_train_baked)\npenguin_fit\n\nparsnip model object\n\n\nCall:\n randomForest(x = maybe_data_frame(x), y = y, ntree = ~100) \n               Type of random forest: classification\n                     Number of trees: 100\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 2.01%\nConfusion matrix:\n          Adelie Chinstrap Gentoo class.error\nAdelie        93         1      1  0.02105263\nChinstrap      2        33      0  0.05714286\nGentoo         0         0     69  0.00000000\n\n# iris_ranger <- \n#   rand_forest(trees = 100) %>% \n#   set_mode(\"classification\") %>% \n#   set_engine(\"ranger\") %>% \n#   fit(Species ~ ., data = iris_training_baked)\n\nMetrics for the Penguin Random Forest Model\n\n# Predictions\npredict(object = penguin_fit, new_data = penguin_test_baked) %>%  \n  dplyr::bind_cols(penguin_test_baked) %>% \n  glimpse()\n\nRows: 134\nColumns: 9\n$ .pred_class       <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel‚Ä¶\n$ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen, Torgerse‚Ä¶\n$ bill_length_mm    <dbl> -0.8946955, -0.6752636, -0.8581235, -0.9312674, -1.7‚Ä¶\n$ bill_depth_mm     <dbl> 0.77955895, 0.42409105, 1.74440040, 0.32252879, 1.99‚Ä¶\n$ flipper_length_mm <dbl> -1.42460769, -0.42573251, -0.78247365, -1.42460769, ‚Ä¶\n$ body_mass_g       <dbl> -0.567620576, -1.188572125, -0.691810886, -0.7228584‚Ä¶\n$ sex               <fct> male, female, male, female, male, female, female, ma‚Ä¶\n$ year              <dbl> -1.2818130, -1.2818130, -1.2818130, -1.2818130, -1.2‚Ä¶\n$ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel‚Ä¶\n\n# Prediction Accuracy Metrics\npredict(object = penguin_fit, new_data = penguin_test_baked) %>%  \n  dplyr::bind_cols(penguin_test_baked) %>% \n  yardstick::metrics(truth = species, estimate = .pred_class)\n\n# A tibble: 2 √ó 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy multiclass         1\n2 kap      multiclass         1\n\n# Prediction Probabilities\npenguin_fit_probs <- \n  predict(penguin_fit, penguin_test_baked, type = \"prob\") %>%\n  dplyr::bind_cols(penguin_test_baked)\nglimpse(penguin_fit_probs)\n\nRows: 134\nColumns: 11\n$ .pred_Adelie      <dbl> 1.00, 1.00, 0.99, 1.00, 0.97, 1.00, 1.00, 0.51, 1.00‚Ä¶\n$ .pred_Chinstrap   <dbl> 0.00, 0.00, 0.01, 0.00, 0.03, 0.00, 0.00, 0.45, 0.00‚Ä¶\n$ .pred_Gentoo      <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.04, 0.00‚Ä¶\n$ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen, Torgerse‚Ä¶\n$ bill_length_mm    <dbl> -0.8946955, -0.6752636, -0.8581235, -0.9312674, -1.7‚Ä¶\n$ bill_depth_mm     <dbl> 0.77955895, 0.42409105, 1.74440040, 0.32252879, 1.99‚Ä¶\n$ flipper_length_mm <dbl> -1.42460769, -0.42573251, -0.78247365, -1.42460769, ‚Ä¶\n$ body_mass_g       <dbl> -0.567620576, -1.188572125, -0.691810886, -0.7228584‚Ä¶\n$ sex               <fct> male, female, male, female, male, female, female, ma‚Ä¶\n$ year              <dbl> -1.2818130, -1.2818130, -1.2818130, -1.2818130, -1.2‚Ä¶\n$ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel‚Ä¶\n\n# Confusion Matrix\npenguin_fit$fit$confusion %>% tidy()\n\nWarning: 'tidy.numeric' is deprecated.\nSee help(\"Deprecated\")\n\n\n# A tibble: 3 √ó 1\n  x[,\"Adelie\"] [,\"Chinstrap\"] [,\"Gentoo\"] [,\"class.error\"]\n         <dbl>          <dbl>       <dbl>            <dbl>\n1           93              1           1           0.0211\n2            2             33           0           0.0571\n3            0              0          69           0     \n\n# Gain Curves\npenguin_fit_probs %>% \n  yardstick::gain_curve(species, .pred_Adelie:.pred_Gentoo) %>%\n  autoplot()\n\n\n\n# ROC Plot\npenguin_fit_probs%>%\n  roc_curve(species, .pred_Adelie:.pred_Gentoo) %>%\n  autoplot()\n\n\n\n\nUsing broom on the penguin model\n\npenguin_split\n\n<Training/Testing/Total>\n<199/134/333>\n\npenguin_split %>% broom::tidy()\n\n# A tibble: 333 √ó 2\n     Row Data    \n   <int> <chr>   \n 1     2 Analysis\n 2     4 Analysis\n 3     7 Analysis\n 4     8 Analysis\n 5     9 Analysis\n 6    12 Analysis\n 7    13 Analysis\n 8    17 Analysis\n 9    20 Analysis\n10    21 Analysis\n# ‚Ä¶ with 323 more rows\n\npenguin_recipe %>% broom::tidy()\n\n# A tibble: 2 √ó 6\n  number operation type      trained skip  id             \n   <int> <chr>     <chr>     <lgl>   <lgl> <chr>          \n1      1 step      normalize TRUE    FALSE normalize_xH3M2\n2      2 step      corr      TRUE    FALSE corr_lZXmq     \n\n# Following do not work for `random forest models` !! ;-()\n#penguin_model %>% tidy()\n#penguin_fit %>% tidy() \npenguin_model %>% str()\n\nList of 7\n $ args                 :List of 3\n  ..$ mtry : language ~NULL\n  .. ..- attr(*, \".Environment\")=<environment: R_EmptyEnv> \n  ..$ trees: language ~100\n  .. ..- attr(*, \".Environment\")=<environment: R_EmptyEnv> \n  ..$ min_n: language ~NULL\n  .. ..- attr(*, \".Environment\")=<environment: R_EmptyEnv> \n $ eng_args             : Named list()\n  ..- attr(*, \"class\")= chr [1:2] \"quosures\" \"list\"\n $ mode                 : chr \"classification\"\n $ user_specified_mode  : logi TRUE\n $ method               : NULL\n $ engine               : chr \"randomForest\"\n $ user_specified_engine: logi TRUE\n - attr(*, \"class\")= chr [1:2] \"rand_forest\" \"model_spec\"\n\npenguin_test_baked\n\n# A tibble: 134 √ó 8\n   island    bill_length_mm bill_depth_mm flipper‚Ä¶¬π body_m‚Ä¶¬≤ sex    year species\n   <fct>              <dbl>         <dbl>     <dbl>    <dbl> <fct> <dbl> <fct>  \n 1 Torgersen         -0.895         0.780    -1.42  -0.568   male  -1.28 Adelie \n 2 Torgersen         -0.675         0.424    -0.426 -1.19    fema‚Ä¶ -1.28 Adelie \n 3 Torgersen         -0.858         1.74     -0.782 -0.692   male  -1.28 Adelie \n 4 Torgersen         -0.931         0.323    -1.42  -0.723   fema‚Ä¶ -1.28 Adelie \n 5 Torgersen         -1.72          2.00     -0.212  0.240   male  -1.28 Adelie \n 6 Torgersen         -1.35          0.323    -1.14  -0.630   fema‚Ä¶ -1.28 Adelie \n 7 Torgersen         -1.75          0.627    -1.21  -1.10    fema‚Ä¶ -1.28 Adelie \n 8 Torgersen          0.367         2.20     -0.497 -0.00876 male  -1.28 Adelie \n 9 Biscoe            -1.13          0.576    -1.92  -1.00    fema‚Ä¶ -1.28 Adelie \n10 Biscoe            -1.48          1.03     -0.854 -0.506   fema‚Ä¶ -1.28 Adelie \n# ‚Ä¶ with 124 more rows, and abbreviated variable names ¬π‚Äãflipper_length_mm,\n#   ¬≤‚Äãbody_mass_g"
  },
  {
    "objectID": "content/courses/Predictive-Analytics/Modules/30-Classification/files/Random-Forests.html#iris-random-forest-model-with-ranger",
    "href": "content/courses/Predictive-Analytics/Modules/30-Classification/files/Random-Forests.html#iris-random-forest-model-with-ranger",
    "title": "Random Forests",
    "section": "Iris Random Forest Model with ranger\n",
    "text": "Iris Random Forest Model with ranger\n\nUsing the iris dataset and Random Forest Classification. This part uses rsample to split the data and the recipes to prep the data for model making.\n\n#set.seed(100)\niris_split <- rsample::initial_split(iris, prop = 0.6)\niris_split\n\n<Training/Testing/Total>\n<90/60/150>\n\niris_split %>% training() %>% glimpse()\n\nRows: 90\nColumns: 5\n$ Sepal.Length <dbl> 6.3, 4.5, 5.0, 4.8, 4.9, 6.5, 5.9, 6.5, 5.4, 6.3, 5.2, 5.‚Ä¶\n$ Sepal.Width  <dbl> 2.5, 2.3, 3.3, 3.1, 3.6, 3.2, 3.2, 3.0, 3.0, 2.5, 3.5, 3.‚Ä¶\n$ Petal.Length <dbl> 5.0, 1.3, 1.4, 1.6, 1.4, 5.1, 4.8, 5.8, 4.5, 4.9, 1.5, 4.‚Ä¶\n$ Petal.Width  <dbl> 1.9, 0.3, 0.2, 0.2, 0.1, 2.0, 1.8, 2.2, 1.5, 1.5, 0.2, 1.‚Ä¶\n$ Species      <fct> virginica, setosa, setosa, setosa, setosa, virginica, ver‚Ä¶\n\niris_split %>% testing() %>% glimpse()\n\nRows: 60\nColumns: 5\n$ Sepal.Length <dbl> 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.9, 5.4, 5.4, 5.1, 5.‚Ä¶\n$ Sepal.Width  <dbl> 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 3.1, 3.7, 3.9, 3.5, 3.‚Ä¶\n$ Petal.Length <dbl> 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.5, 1.5, 1.3, 1.4, 1.‚Ä¶\n$ Petal.Width  <dbl> 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.1, 0.2, 0.4, 0.3, 0.‚Ä¶\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setosa, s‚Ä¶\n\n\nIris Data Pre-Processing: Creating the Recipe\nThe recipes package provides an interface that specializes in data pre-processing. Within the package, the functions that start, or execute, the data transformations are named after cooking actions. That makes the interface more user-friendly. For example:\n\nrecipe() - Starts a new set of transformations to be applied, similar to the ggplot() command. Its main argument is the model‚Äôs formula.\nprep() - Executes the transformations on top of the data that is supplied (typically, the training data). Each data transformation is a step() function. ( Recall what we did with the caret package: Centering, Scaling, Removing Correlated variables‚Ä¶)\n\nNote that in order to avoid data leakage (e.g: transferring information from the train set into the test set), data should be ‚Äúprepped‚Äù using the train_tbl only. https://towardsdatascience.com/modelling-with-tidymodels-and-parsnip-bae2c01c131c CRAN: The idea is that the preprocessing operations will all be created using the training set and then these steps will be applied to both the training and test set.\n\n# Pre Processing the Training Data\n\niris_recipe <- \n  training(iris_split) %>% # Note: Using TRAINING data !!\n  recipe(Species ~ .)      # Note: Outcomes ~ Predictors !!\n\n# The data contained in the `data` argument need not be the training set; this data is only used to catalog the names of the variables and their types (e.g. numeric, etc.).\n\nQ: How does the recipe ‚Äúfigure‚Äù out which are the outcomes and which are the predictors? A.The recipe command defines Outcomes and Predictors using the formula interface. Not clear how this recipe ‚Äúfigures‚Äù out which are the outcomes and which are the predictors, when we have not yet specified them‚Ä¶\nQ. Why is the recipe not agnostic to data set? Is that a meaningful question? A. The use of the training set in the recipe command is just to declare the variables and specify the roles of the data, nothing else. Roles are open-ended and extensible. From https://cran.r-project.org/web/packages/recipes/vignettes/Simple_Example.html :\n\nThis document demonstrates some basic uses of recipes. First, some definitions are required: - variables are the original (raw) data columns in a data frame or tibble. For example, in a traditional formula Y ~ A + B + A:B, the variables are A, B, and Y. - roles define how variables will be used in the model. Examples are: predictor (independent variables), response, and case weight. This is meant to be open-ended and extensible. - terms are columns in a design matrix such as A, B, and A:B. These can be other derived entities that are grouped, such as a set of principal components or a set of columns, that define a basis function for a variable. These are synonymous with features in machine learning. Variables that have predictor roles would automatically be main effect terms.\n\n\n# Apply the transformation steps\niris_recipe <- iris_recipe %>% \n  step_corr(all_predictors()) %>% \n  step_center(all_predictors(), -all_outcomes()) %>% \n  step_scale(all_predictors(), -all_outcomes()) %>% \n  prep()\n\nThis has created the recipe() and prepped it too. We now need to apply it to our datasets:\n\nTake training data and bake() it to prepare it for modelling.\nDo the same for the testing set.\n\n\niris_training_baked <- \n  iris_split %>% \n  training() %>% \n  bake(iris_recipe,.)\niris_training_baked\n\n# A tibble: 90 √ó 4\n   Sepal.Length Sepal.Width Petal.Width Species   \n          <dbl>       <dbl>       <dbl> <fct>     \n 1      0.503       -1.10         0.859 virginica \n 2     -1.78        -1.53        -1.37  setosa    \n 3     -1.15         0.633       -1.51  setosa    \n 4     -1.40         0.200       -1.51  setosa    \n 5     -1.27         1.28        -1.65  setosa    \n 6      0.756        0.417        0.999 virginica \n 7     -0.00423      0.417        0.720 versicolor\n 8      0.756       -0.0169       1.28  virginica \n 9     -0.638       -0.0169       0.302 versicolor\n10      0.503       -1.10         0.302 versicolor\n# ‚Ä¶ with 80 more rows\n\niris_testing_baked <- \n  iris_split %>% \n  testing() %>% \n  bake(iris_recipe,.)\niris_testing_baked \n\n# A tibble: 60 √ó 4\n   Sepal.Length Sepal.Width Petal.Width Species\n          <dbl>       <dbl>       <dbl> <fct>  \n 1       -1.27      -0.0169       -1.51 setosa \n 2       -1.53       0.417        -1.51 setosa \n 3       -1.65       0.200        -1.51 setosa \n 4       -1.15       1.28         -1.51 setosa \n 5       -0.638      1.93         -1.23 setosa \n 6       -1.65       0.850        -1.37 setosa \n 7       -1.15       0.850        -1.51 setosa \n 8       -1.27       0.200        -1.65 setosa \n 9       -0.638      1.50         -1.51 setosa \n10       -0.638      1.93         -1.23 setosa \n# ‚Ä¶ with 50 more rows\n\n\nIris Model Training using parsnip\n\nDifferent ML packages provide different interfaces (APIs ) to do the same thing (e.g random forests). The tidymodels package provides a consistent interface to invoke a wide variety of packages supporting a wide variety of models.\nThe parsnip package is a successor to caret.\nTo model with parsnip: 1. Pick a model : 2. Set the engine 3. Set the mode (if needed): Classification or Regression\nCheck here for models available in parsnip.\n\nMode: classification and regression in parsnip, each using a variety of models. ( Which Way). This defines the form of the output.\nEngine: The engine is the R package that is invoked by parsnip to execute the model. E.g glm, glmnet,keras.( How ) parsnip provides wrappers for models from these packages.\nModel: is the specific technique used for the modelling task. E.g linear_reg(), logistic_reg(), mars, decision_tree, nearest_neighbour‚Ä¶(What model).\n\nand models have: - hyperparameters: that are numerical or factor variables that tune the model ( Like the alpha beta parameters for Bayesian priors)\nWe can use the random forest model to classify the iris into species. Here Species is the Outcome variable and the rest are predictor variables. The random forest model is provided by the ranger package, to which tidymodels/parsnip provides a simple and consistent interface.\n\nlibrary(ranger)\niris_ranger <- \n  rand_forest(trees = 100) %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"ranger\") %>% \n  fit(Species ~ ., data = iris_training_baked)\n\nranger can generate random forest models for classification, regression, survival( time series, time to event stuff). Extreme Forests are also supported, wherein all points in the dataset are used ( instead of bootstrap samples) along with feature bagging. We can also run the same model using the randomForest package:\n\nlibrary(randomForest,quietly = TRUE)\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:ranger':\n\n    importance\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\niris_rf <- \n  rand_forest(trees = 100) %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"randomForest\") %>% \n  fit(Species ~ ., data = iris_training_baked)\n\nIris Predictions\nThe predict() function run against a parsnip model returns a prediction tibble. By default, the prediction variable is called .pred_class.\n\npredict(object = iris_ranger, new_data = iris_testing_baked) %>%  \n  dplyr::bind_cols(iris_testing_baked) %>% \n  glimpse()\n\nRows: 60\nColumns: 5\n$ .pred_class  <fct> setosa, setosa, setosa, setosa, setosa, setosa, setosa, s‚Ä¶\n$ Sepal.Length <dbl> -1.2718283, -1.5253489, -1.6521092, -1.1450680, -0.638026‚Ä¶\n$ Sepal.Width  <dbl> -0.01685809, 0.41663561, 0.19988876, 1.28362301, 1.933863‚Ä¶\n$ Petal.Width  <dbl> -1.509753, -1.509753, -1.509753, -1.509753, -1.231029, -1‚Ä¶\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setosa, s‚Ä¶\n\n\nIris Classification Model Validation\nWe use metrics() function from the yardstick package to evaluate how good the model is.\n\npredict(iris_ranger, iris_testing_baked) %>%\n  dplyr::bind_cols(iris_testing_baked) %>% \n  yardstick::metrics(truth = Species, estimate = .pred_class)\n\n# A tibble: 2 √ó 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy multiclass     0.983\n2 kap      multiclass     0.974\n\n\nWe can also check the metrics for randomForest model:\n\npredict(iris_rf, iris_testing_baked) %>%\n  dplyr::bind_cols(iris_testing_baked) %>% \n  yardstick::metrics(truth = Species, estimate = .pred_class)\n\n# A tibble: 2 √ó 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy multiclass     0.983\n2 kap      multiclass     0.974\n\n\nIris Per-Classifier Metrics\nWe can use the parameter type = \"prob\" in the predict() function to obtain a probability score on each prediction. TBD: How is this prob calculated? Possible answer: the Random Forest model outputs its answer by majority voting across n trees. Each of the possible answers( i.e.¬†predictions) for a particular test datum gets a share of the vote, that represents its probability. Hence each dataum in the test vector can show a probability for the ‚Äúwinning‚Äù answer. ( Quite possibly we can get the probabilities for all possible outcomes for each test datum)\n\niris_ranger_probs <- \n  predict(iris_ranger, iris_testing_baked, type = \"prob\") %>%\n  dplyr::bind_cols(iris_testing_baked)\nglimpse(iris_ranger_probs)\n\nRows: 60\nColumns: 7\n$ .pred_setosa     <dbl> 0.8063568, 0.9904167, 0.9673690, 0.9822222, 0.9605714‚Ä¶\n$ .pred_versicolor <dbl> 0.150555556, 0.006333333, 0.026523810, 0.013333333, 0‚Ä¶\n$ .pred_virginica  <dbl> 0.043087607, 0.003250000, 0.006107143, 0.004444444, 0‚Ä¶\n$ Sepal.Length     <dbl> -1.2718283, -1.5253489, -1.6521092, -1.1450680, -0.63‚Ä¶\n$ Sepal.Width      <dbl> -0.01685809, 0.41663561, 0.19988876, 1.28362301, 1.93‚Ä¶\n$ Petal.Width      <dbl> -1.509753, -1.509753, -1.509753, -1.509753, -1.231029‚Ä¶\n$ Species          <fct> setosa, setosa, setosa, setosa, setosa, setosa, setos‚Ä¶\n\niris_rf_probs <- \n  predict(iris_rf, iris_testing_baked, type = \"prob\") %>%\n  dplyr::bind_cols(iris_testing_baked)\nglimpse(iris_rf_probs)\n\nRows: 60\nColumns: 7\n$ .pred_setosa     <dbl> 0.80, 0.99, 0.99, 1.00, 0.99, 0.99, 0.99, 0.85, 0.99,‚Ä¶\n$ .pred_versicolor <dbl> 0.15, 0.01, 0.01, 0.00, 0.00, 0.00, 0.00, 0.12, 0.00,‚Ä¶\n$ .pred_virginica  <dbl> 0.05, 0.00, 0.00, 0.00, 0.01, 0.01, 0.01, 0.03, 0.01,‚Ä¶\n$ Sepal.Length     <dbl> -1.2718283, -1.5253489, -1.6521092, -1.1450680, -0.63‚Ä¶\n$ Sepal.Width      <dbl> -0.01685809, 0.41663561, 0.19988876, 1.28362301, 1.93‚Ä¶\n$ Petal.Width      <dbl> -1.509753, -1.509753, -1.509753, -1.509753, -1.231029‚Ä¶\n$ Species          <fct> setosa, setosa, setosa, setosa, setosa, setosa, setos‚Ä¶\n\n# Tabulating the probabilities\nftable(iris_rf_probs$.pred_versicolor)\n\n  0 0.01 0.02 0.04 0.05 0.06 0.07 0.08 0.1 0.12 0.13 0.14 0.15 0.16 0.18 0.5 0.55 0.69 0.7 0.74 0.75 0.78 0.79 0.81 0.83 0.91 0.95 0.96 0.97 0.98  1\n                                                                                                                                                    \n 17    6    2    1    2    1    1    1   2    2    2    2    2    1    1   1    1    2   1    1    1    1    1    1    1    1    1    1    1    1  1\n\nftable(iris_rf_probs$.pred_virginica)\n\n  0 0.01 0.02 0.03 0.04 0.05 0.09 0.12 0.17 0.18 0.19 0.2 0.22 0.23 0.31 0.45 0.78 0.79 0.84 0.85 0.86 0.9 0.91 0.92 0.93 0.94 0.95 0.99  1\n                                                                                                                                           \n  9   13    2    4    2    1    1    1    2    1    1   1    2    1    1    1    1    2    1    1    1   1    1    1    1    1    3    1  2\n\nftable(iris_rf_probs$.pred_setosa)\n\n  0 0.01 0.03 0.08 0.09 0.1 0.27 0.79 0.8 0.85 0.88 0.94 0.95 0.99  1\n                                                                     \n 21    1    2    6    2   1    1    1   1    2    1    1    1   15  4\n\n\n\n### Iris Classifier: Gain and ROC Curves\n\nWe can plot gain and ROC curves for each of these models\n\n::: {.cell}\n\n```{.r .cell-code}\niris_ranger_probs %>% \n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %>% \n  glimpse()\n\nRows: 139\nColumns: 5\n$ .level          <chr> \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"set‚Ä¶\n$ .n              <dbl> 0, 1, 3, 4, 5, 6, 8, 10, 11, 12, 15, 18, 19, 20, 21, 2‚Ä¶\n$ .n_events       <dbl> 0, 1, 3, 4, 5, 6, 8, 10, 11, 12, 15, 18, 19, 20, 21, 2‚Ä¶\n$ .percent_tested <dbl> 0.000000, 1.666667, 5.000000, 6.666667, 8.333333, 10.0‚Ä¶\n$ .percent_found  <dbl> 0.000000, 3.846154, 11.538462, 15.384615, 19.230769, 2‚Ä¶\n\niris_ranger_probs %>% \n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %>% \n  autoplot()\n\n\n\niris_ranger_probs %>% \n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %>% \n  glimpse()\n\nRows: 142\nColumns: 4\n$ .level      <chr> \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\"‚Ä¶\n$ .threshold  <dbl> -Inf, 0.0000000000, 0.0003846154, 0.0012500000, 0.00163461‚Ä¶\n$ specificity <dbl> 0.0000000, 0.0000000, 0.2058824, 0.2647059, 0.3235294, 0.3‚Ä¶\n$ sensitivity <dbl> 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0‚Ä¶\n\niris_ranger_probs %>% \n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %>% \n  autoplot()\n\n\n\n:::\n\niris_rf_probs %>% \n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %>% \n  glimpse()\n\nRows: 78\nColumns: 5\n$ .level          <chr> \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"set‚Ä¶\n$ .n              <dbl> 0, 4, 19, 20, 21, 22, 24, 25, 26, 27, 28, 30, 36, 38, ‚Ä¶\n$ .n_events       <dbl> 0, 4, 19, 20, 21, 22, 24, 25, 26, 26, 26, 26, 26, 26, ‚Ä¶\n$ .percent_tested <dbl> 0.000000, 6.666667, 31.666667, 33.333333, 35.000000, 3‚Ä¶\n$ .percent_found  <dbl> 0.00000, 15.38462, 73.07692, 76.92308, 80.76923, 84.61‚Ä¶\n\niris_rf_probs %>% \n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %>% \n  autoplot()\n\n\n\niris_rf_probs %>% \n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %>% \n  glimpse()\n\nRows: 81\nColumns: 4\n$ .level      <chr> \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\"‚Ä¶\n$ .threshold  <dbl> -Inf, 0.00, 0.01, 0.03, 0.08, 0.09, 0.10, 0.27, 0.79, 0.80‚Ä¶\n$ specificity <dbl> 0.0000000, 0.0000000, 0.6176471, 0.6470588, 0.7058824, 0.8‚Ä¶\n$ sensitivity <dbl> 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0‚Ä¶\n\niris_rf_probs %>% \n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %>% \n  autoplot()\n\n\n\n\nIris Classifier: Metrics\n\npredict(iris_ranger, iris_testing_baked, type = \"prob\") %>% \n  bind_cols(predict(iris_ranger,iris_testing_baked)) %>% \n  bind_cols(select(iris_testing_baked,Species)) %>% \n  glimpse()\n\nRows: 60\nColumns: 5\n$ .pred_setosa     <dbl> 0.8063568, 0.9904167, 0.9673690, 0.9822222, 0.9605714‚Ä¶\n$ .pred_versicolor <dbl> 0.150555556, 0.006333333, 0.026523810, 0.013333333, 0‚Ä¶\n$ .pred_virginica  <dbl> 0.043087607, 0.003250000, 0.006107143, 0.004444444, 0‚Ä¶\n$ .pred_class      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setos‚Ä¶\n$ Species          <fct> setosa, setosa, setosa, setosa, setosa, setosa, setos‚Ä¶\n\npredict(iris_ranger, iris_testing_baked, type = \"prob\") %>% \n  bind_cols(predict(iris_ranger,iris_testing_baked)) %>% \n  bind_cols(select(iris_testing_baked,Species)) %>% \n  yardstick::metrics(data = ., truth = Species, estimate = .pred_class, ... = .pred_setosa:.pred_virginica)\n\n# A tibble: 4 √ó 3\n  .metric     .estimator .estimate\n  <chr>       <chr>          <dbl>\n1 accuracy    multiclass     0.983\n2 kap         multiclass     0.974\n3 mn_log_loss multiclass     0.152\n4 roc_auc     hand_till      0.994\n\n# And for the `randomForest`method\n\npredict(iris_rf, iris_testing_baked, type = \"prob\") %>% \n  bind_cols(predict(iris_ranger,iris_testing_baked)) %>% \n  bind_cols(select(iris_testing_baked,Species)) %>% \n  glimpse()\n\nRows: 60\nColumns: 5\n$ .pred_setosa     <dbl> 0.80, 0.99, 0.99, 1.00, 0.99, 0.99, 0.99, 0.85, 0.99,‚Ä¶\n$ .pred_versicolor <dbl> 0.15, 0.01, 0.01, 0.00, 0.00, 0.00, 0.00, 0.12, 0.00,‚Ä¶\n$ .pred_virginica  <dbl> 0.05, 0.00, 0.00, 0.00, 0.01, 0.01, 0.01, 0.03, 0.01,‚Ä¶\n$ .pred_class      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setos‚Ä¶\n$ Species          <fct> setosa, setosa, setosa, setosa, setosa, setosa, setos‚Ä¶\n\npredict(iris_rf, iris_testing_baked, type = \"prob\") %>% \n  bind_cols(predict(iris_ranger,iris_testing_baked)) %>% \n  bind_cols(select(iris_testing_baked,Species)) %>% \n  yardstick::metrics(data = ., truth = Species, estimate = .pred_class, ... = .pred_setosa:.pred_virginica)\n\n# A tibble: 4 √ó 3\n  .metric     .estimator .estimate\n  <chr>       <chr>          <dbl>\n1 accuracy    multiclass     0.983\n2 kap         multiclass     0.974\n3 mn_log_loss multiclass     0.139\n4 roc_auc     hand_till      0.994"
  },
  {
    "objectID": "content/courses/Predictive-Analytics/Modules/30-Classification/index.html",
    "href": "content/courses/Predictive-Analytics/Modules/30-Classification/index.html",
    "title": "ML - Classification",
    "section": "",
    "text": "Have you played a Childhood Game called 20 Questions? Someone has a ‚Äútarget‚Äù entity in mind ( a person or a thing or a literary character) and the others need to discover that entity by asking 20 questions.\n\nHow does one create questions in the game?\n\nCategories?\nNumbers? How?\nComparisons?\n\n\nWhat sort of answers can you expect for each question?"
  },
  {
    "objectID": "content/courses/Predictive-Analytics/Modules/30-Classification/index.html#twenty-questions-game-as-a-play-with-data",
    "href": "content/courses/Predictive-Analytics/Modules/30-Classification/index.html#twenty-questions-game-as-a-play-with-data",
    "title": "ML - Classification",
    "section": "Twenty Questions Game as a Play with Data‚Ä¶",
    "text": "Twenty Questions Game as a Play with Data‚Ä¶\nAssuming we think of a 20Q Target as say, celebrity singer like Taylor Swift, or a cartoon character like Thomas the Tank Engine, what would an underlying ‚Äúdata structure‚Äù look like? We would ask Questions for instance in the following order to find the target of Taylor Swift:\n- Human?(Yes)\n- Living?(Yes)\n- Male?(No)\n- Celebrity?(Yes)\n- Music?(Yes)\n- USA?(Yes)\nOh‚Ä¶Taylor Swift!!!\nLet us try to construct the ‚Äúdatasets‚Äù underlying this game!\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nOccupation\nSex\nLiving\nNationality\ngenre\npet\n\n\nTaylor Swift\nSinger\nF\nTRUE\nUSA\ncountry/rock\nScottish Fold Cats\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nType\nLiving\nhuman\nNationality\ncolour\nmaterial\n\n\nThomas, the Tank Engine\nCartoon Character\nFALSE\nFALSE\nUK\nblue\nmetal\n\n\n\n\nIt should be fairly clear that the Questions we ask are based on the COLUMNs in the respective 1-row datasets! The TARGET Column in both cases is the name column."
  },
  {
    "objectID": "content/courses/Predictive-Analytics/Modules/30-Classification/index.html#what-is-a-decision-tree",
    "href": "content/courses/Predictive-Analytics/Modules/30-Classification/index.html#what-is-a-decision-tree",
    "title": "ML - Classification",
    "section": "What is a Decision Tree?",
    "text": "What is a Decision Tree?\nCan you imagine how the 20 Questions Game can be shown as a tree?\n\n\n\n\n\n\nEach Question we ask, based on one of the Feature columns, begets a Yes/NO answer and we turn the left or right accordingly. When we arrive at the leaf, we should be in a position to guess the answer !"
  },
  {
    "objectID": "content/courses/Predictive-Analytics/Modules/30-Classification/index.html#twenty-times-20-questions",
    "href": "content/courses/Predictive-Analytics/Modules/30-Classification/index.html#twenty-times-20-questions",
    "title": "ML - Classification",
    "section": "Twenty times 20 Questions !!",
    "text": "Twenty times 20 Questions !!\nWhat if the dataset we had contained many rows, instead of just one row? How would we play the 20Q Game in this situation? Here is a sample of the famous penguins dataset:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\nAdelie\nDream\n37.2\n18.1\n178\n3900\nmale\n2007\n\n\nAdelie\nDream\n40.2\n17.1\n193\n3400\nfemale\n2009\n\n\nAdelie\nBiscoe\n42.0\n19.5\n200\n4050\nmale\n2008\n\n\nGentoo\nBiscoe\n49.6\n15.0\n216\n4750\nmale\n2008\n\n\nChinstrap\nDream\n49.0\n19.6\n212\n4300\nmale\n2009\n\n\nAdelie\nDream\n36.0\n17.9\n190\n3450\nfemale\n2007\n\n\nGentoo\nBiscoe\n45.2\n16.4\n223\n5950\nmale\n2008\n\n\nGentoo\nBiscoe\n48.2\n15.6\n221\n5100\nmale\n2008\n\n\nGentoo\nBiscoe\n51.1\n16.3\n220\n6000\nmale\n2008\n\n\nGentoo\nBiscoe\n44.5\n14.3\n216\n4100\nNA\n2007\n\n\nChinstrap\nDream\n55.8\n19.8\n207\n4000\nmale\n2009\n\n\nAdelie\nBiscoe\n38.2\n20.0\n190\n3900\nmale\n2009\n\n\n\n\n\nAs before, we would need to look at the dataset as containing a TARGET column which we want to predict using several other FEATURE columns. Let us choose species.\nWhen we look at the FEATURE columns, We would need to formulate questions based on entire columns at a time. For instance:\n\n\n‚ÄúIs the bill_length_mm* greater than 45mm?‚Äù considers the entire bill_length_mm* FEATURE column\nIs the sex female? considers the entire sex column\n\nIf the specific FEATURE column is a Numerical (N) variable, the question would use some ‚Äúthresholding‚Äù as shown in the question above, to convert the Numerical Variable into a Categorical variable.\nIf a specific FEATURE column is a Categorical (C) variable, the question would be like a filter operation in Excel.\nEither way, we end up answering with a smaller and smaller subset of rows in the dataset, to which the questions are answered with a Yes. It is as if we played many 20 Questions games in parallel, since there are so many simultaneous ‚Äúanswers‚Äù!\nOnce we exhaust all the FEATURE columns, then what remains is a subset (i.e.¬†rows) of the original dataset and we read off the TARGET column, which should now contain a set of identical entries, e.g. ‚ÄúAdelie‚Äù. Thus we can extend a single-target 20Q game to a multiple-target one using a larger dataset. ( Note how the multiple targets are all the same: ‚ÄúAdelie‚Äù, or ‚ÄúGentoo‚Äù, or ‚ÄúChinstrap‚Äù)\nThis forms the basic intuition for a Machine Learning Algorithm called a Decision Tree.\nDecision Tree in Orange\nLet us visualize this Decision Tree in Orange. Look at the now famous penguins dataset, available here:\nhttps://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\nWe see that there are three species of penguins, that live on three islands. The measurements for each penguin are flipper_length_mm, bill_length_mm, bill_depth_mm, and body_mass_g.\n\nTask 1: Create a few data visualizations for the variables, and pairs of variables from this dataset.\nTask 2: Can you inspect the visualizations and imagine how each of this dataset can be used in a 20 Questions Game, to create a Decision Tree for this dataset as shown below?\n\n\n\nPenguins Decision Tree!\n\n\nWhat did we learn?\n\nThe 20Q Game can be viewed as a ‚ÄúDecision Tree‚Äù of Questions and Answers,\nEach fork in the game is a Question.\nDepending upon whether the current answer is yes or no, we turn in one direction or the other.\nEach of our questions is based on the information available in one or other of the columns!!\nWe arrive at a final ‚Äúanswer‚Äù or ‚Äútarget‚Äù after a particular sequence of yes/no answers. This is the one of the leaf nodes in the Tree.\nThe island and the species columns are categories and are especially suited to being the targets for a 20 Questions Game.\nWe can therefore use an entire column of data as our 20Questions target, rather than just one entity, person.\n\nThis is how we will use this Game as a Model for our first ML algorithm, classification using Decision Trees."
  },
  {
    "objectID": "content/courses/Predictive-Analytics/Modules/30-Classification/index.html#how-do-we-make-predictions-using-our-decision-tree",
    "href": "content/courses/Predictive-Analytics/Modules/30-Classification/index.html#how-do-we-make-predictions-using-our-decision-tree",
    "title": "ML - Classification",
    "section": "How do we Make Predictions using our Decision Tree",
    "text": "How do we Make Predictions using our Decision Tree\nOur aim is to make predictions. Predictions of what? When we are given new unseen data in the same format, we should be able to predict TARGET variable using the same FEATURE columns.\nNOTE: This that is usually a class/category (We CAN also predict a numerical value with a Decision Tree; but we will deal with that later.)\nIn order to make predictions with completely unseen data, we need to first check if the algorithm is working well with known data. The way to do this is to use a large portion of data to design the tree, and then use the tree to predict some aspect of the remaining, but similar, data. Let us split the penguins dataset into two pieces: a training set to design our tree, and a test set to check how it is working.\nDownload this penguin tree file and open it in Orange.\nHow good are the Predictions? What is the Classification Error Rate?"
  },
  {
    "objectID": "content/courses/Predictive-Analytics/Modules/30-Classification/index.html#how-many-trees-do-we-need-enter-the-random-forest",
    "href": "content/courses/Predictive-Analytics/Modules/30-Classification/index.html#how-many-trees-do-we-need-enter-the-random-forest",
    "title": "ML - Classification",
    "section": "How Many Trees do we Need? Enter the Random Forest!",
    "text": "How Many Trees do we Need? Enter the Random Forest!\nCheck all your individual Decision Trees: do they ask the same Questions? Do they fork in the same way? Yes, they all seem to use the same set of parameters to reach the target. So they are capable of being ‚Äúbiased‚Äù and make the same mistakes. So we ask: Does it help to use more than one tree, if all the questions/forks in the Trees are similar?\nNo‚Ä¶we need different Trees to be able to ask different questions, based on different variables or features in the data. That will make the Trees as different as possible and so‚Ä¶unbiased. This is what we also saw when we played 20Q: offbeat questions opened up some avenues for predicting the answer/target.\nA forest of such trees is called the Wild Wood a Random Forest !"
  },
  {
    "objectID": "content/courses/Predictive-Analytics/Modules/30-Classification/index.html#an-introduction-to-random-forests",
    "href": "content/courses/Predictive-Analytics/Modules/30-Classification/index.html#an-introduction-to-random-forests",
    "title": "ML - Classification",
    "section": "An Introduction to Random Forests",
    "text": "An Introduction to Random Forests\nIn the Random Forest method, we do as follows:\n\nSplit the dataset into training and test subsets (70::30 proportion is very common). Keep aside the testing dataset for final testing.\nDecide on a number of trees, say 100-500 in the forest.\nTake the training dataset and repeatedly sample some of the rows in it. Rows can be repeated too; this is called bootstrap sampling.\nGive this sampled training set to each tree. Each tree develops a question from this dataset, in a random fashion, using a randomly chosen variable. E.g. with penguins, if our target is species, then some trees will will use island, some others will use body_mass_g and some others may use bill_length_mm.\nEach tree will ‚Äúgrow its questions‚Äù in a unique way !! Since the questions are possibly based on a different variable at each time, the trees will grow in very different ways.\nStop when the required accuracy has been achieved (the sets contain observations/rows from only one species predominantly)\nWith the test set let each tree vote on which species it has decided upon. Take the majority vote.\n\nPhew!!\nLet‚Äôs get a visual sense of how all this works:\nhttps://waternova.github.io/random-forest-viz/"
  },
  {
    "objectID": "content/courses/Predictive-Analytics/Modules/30-Classification/index.html#random-forest-classification-for-heart-patients",
    "href": "content/courses/Predictive-Analytics/Modules/30-Classification/index.html#random-forest-classification-for-heart-patients",
    "title": "ML - Classification",
    "section": "Random Forest Classification for Heart Patients",
    "text": "Random Forest Classification for Heart Patients\nDo you want to develop an ML model for heart patients? We have a dataset of heart patients at the University of California, Arvind Irvine ML Dataset Repository\nHeart Patient Data. Import into Orange !!\nWhat are the variables?\n\n(age): age in years\n\n(sex): 1 = male; 0 = female\n\n(cp): chest-pain type( 4 types, 1/2/3/4)\n\n(trestbps): resting blood pressure (in mm Hg on admission to the hospital)\n(chol) : serum cholesterol in mg/dl\n(fbs): (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n(restecg): resting electrocardiograph results (0 = normal; 1= ST-T wave abnormality; 3 = LV hypertrophy)\n(thalach): maximum heart rate achieved\n(exang): exercise induced angina (1 = yes; 0 = no) (remember Puneet Rajkumar)\n\n(oldpeak): ST depression induced by exercise relative to rest\n(slope): the slope of the peak exercise ST segment\nValue 1: upsloping\n\nValue 2: flat\n\nValue 3: downsloping\n\n\n\n(ca): number of major vessels (0-3) colored by fluoroscopy\n\n(thal): 3 = normal; 6 = fixed defect; 7 = reversible defect\n\n(num) : the target attribute, diagnosis of heart disease (angiographic disease status)\nValue 0: < 50% diameter narrowing\n\nValue 1: > 50% diameter narrowing\n(in any major vessel: attributes 59 through 68 are vessels)\n\n\n\nWe will create a Random Forest Model for this dataset, and compare with the Desision Tree for the same dataset."
  },
  {
    "objectID": "content/courses/Predictive-Analytics/Modules/30-Classification/index.html#how-good-is-my-random-forest",
    "href": "content/courses/Predictive-Analytics/Modules/30-Classification/index.html#how-good-is-my-random-forest",
    "title": "ML - Classification",
    "section": "How good is my Random Forest?",
    "text": "How good is my Random Forest?\n\nClassification Error\n\nGini Impurity\n\nCross Entropy"
  },
  {
    "objectID": "content/courses/Predictive-Analytics/Modules/30-Classification/index.html#references",
    "href": "content/courses/Predictive-Analytics/Modules/30-Classification/index.html#references",
    "title": "ML - Classification",
    "section": "References",
    "text": "References\n\nhttps://towardsdatascience.com/data-science-made-easy-data-modeling-and-prediction-using-orange-f451f17061fa\nThe beauty of Random Forests: https://orangedatamining.com/blog/2016/12/22/the-beauty-of-random-forest/\nPythagorean Trees for Random Forests: https://orangedatamining.com/blog/2016/07/29/pythagorean-trees-and-forests/\ndata.tree sample applications, Christoph Glur, 2020-07-31. https://cran.r-project.org/web/packages/data.tree/vignettes/applications.html\nhttps://ryjohnson09.netlify.app/post/caret-and-tidymodels/"
  },
  {
    "objectID": "content/courses/Predictive-Analytics/Modules/40-Clustering/index.html",
    "href": "content/courses/Predictive-Analytics/Modules/40-Clustering/index.html",
    "title": "ML - Clustering",
    "section": "",
    "text": "Quoting from http://baoqiang.org/?p=579\n\nThese two are arguably the two commonly used cluster methods. One of the reasons is that they are easy to use and also somehow straightforward. So how do they work?\nk-Nearest-Neighbour: Provide N n-dimension entries with known associated classes for each entry, the number of classes is k, that is, \\[\n\\{\\vec{x_i}, y_i\\} ,\\ \\vec{x_i} \\in\\ {\\Re^{n}}\\ , y_i\\ = \\{c_1,...c_k\\},\ni = 1...N\n\\]\nFor a new entry \\(\\vec{v_j}\\), to which class should it belong? We need use a distance measure to get the k closest entries of the new entry , the final decision is simple majority vote based the closest k neighbors. The distance metric could be euclidean or other similar ones.\n\n\nK-means: Given N n-dimension entries and classify them in k classes. At first, we randomly choose k entries and assign them to k clusters. They are the seed classes. Then we calculate the distance between each entry and each class. Each entry will be assigned into one class in terms of the its distance to each class, i.e., assign the entry to its closest class. After the assignment is complete, we then calculate the centroid of each class based on their new members. After the centroid calculation, we go back to the distance calculation and therefore new round classification. We stop the iteration when there is convergence,i.e,, no new centroid and classification.\nThe two methods are all semi-supervised learning algorithms because they do need we provide the number of clusters prior the clustering."
  },
  {
    "objectID": "content/courses/Predictive-Analytics/Modules/40-Clustering/index.html#workflow-using-orange",
    "href": "content/courses/Predictive-Analytics/Modules/40-Clustering/index.html#workflow-using-orange",
    "title": "ML - Clustering",
    "section": "Workflow using Orange",
    "text": "Workflow using Orange"
  },
  {
    "objectID": "content/courses/Predictive-Analytics/Modules/40-Clustering/index.html#workflow-using-radiant",
    "href": "content/courses/Predictive-Analytics/Modules/40-Clustering/index.html#workflow-using-radiant",
    "title": "ML - Clustering",
    "section": "Workflow using Radiant",
    "text": "Workflow using Radiant"
  },
  {
    "objectID": "content/courses/Predictive-Analytics/Modules/40-Clustering/index.html#workflow-using-r",
    "href": "content/courses/Predictive-Analytics/Modules/40-Clustering/index.html#workflow-using-r",
    "title": "ML - Clustering",
    "section": "Workflow using R",
    "text": "Workflow using R"
  },
  {
    "objectID": "content/courses/Predictive-Analytics/Modules/40-Clustering/index.html#conclusion",
    "href": "content/courses/Predictive-Analytics/Modules/40-Clustering/index.html#conclusion",
    "title": "ML - Clustering",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "content/courses/Predictive-Analytics/Modules/40-Clustering/index.html#references",
    "href": "content/courses/Predictive-Analytics/Modules/40-Clustering/index.html#references",
    "title": "ML - Clustering",
    "section": "References",
    "text": "References\n\nK-means Cluster Analysis. UC Business Analytics R Programming Guide https://uc-r.github.io/kmeans_clustering#optimal\nThean C Lim. Clustering: k-means, k-means ++ and gganimate. https://theanlim.rbind.io/post/clustering-k-means-k-means-and-gganimate/"
  },
  {
    "objectID": "content/courses/Predictive-Analytics/Modules/50-Modelling-Time-Series/index.html",
    "href": "content/courses/Predictive-Analytics/Modules/50-Modelling-Time-Series/index.html",
    "title": "üïî Modelling Time Series",
    "section": "",
    "text": "In this module we will look at modelling of time series. We will start with the simplest of expoential models and go all the way through ARIMA and forecasting with Prophet.\nFirst, some terminology!"
  },
  {
    "objectID": "content/courses/Predictive-Analytics/Modules/50-Modelling-Time-Series/index.html#additive-and-multiplicative-time-series-models",
    "href": "content/courses/Predictive-Analytics/Modules/50-Modelling-Time-Series/index.html#additive-and-multiplicative-time-series-models",
    "title": "üïî Modelling Time Series",
    "section": "Additive and Multiplicative Time Series Models",
    "text": "Additive and Multiplicative Time Series Models\nAdditive Time Series can be represented as:\n\\[\nY_t‚ÄÑ=‚ÄÑS_t‚ÄÖ+‚ÄÖT_t‚ÄÖ+‚ÄÖœµ_t\n\\]\nMultiplicative Time Series can be described as:\n\\[\nY_t‚ÄÑ=‚ÄÑS_t‚ÄÖ√ó‚ÄÖT_t‚ÄÖ√ó‚ÄÖœµ_t\n\\]\nLet us consider a Multiplicative Time Series, pertaining to sales of souvenirs at beaches in Australia: The time series looks like this:\n\n\n\n\n\n\nNote that along with the trend, the amplitude of both seasonal and noise components are also increasing in a multiplicative here !! A multiplicative time series can be converted to additive by taking a log of the time series."
  },
  {
    "objectID": "content/courses/Predictive-Analytics/Modules/50-Modelling-Time-Series/index.html#stationarity",
    "href": "content/courses/Predictive-Analytics/Modules/50-Modelling-Time-Series/index.html#stationarity",
    "title": "üïî Modelling Time Series",
    "section": "Stationarity",
    "text": "Stationarity\nA time series is said to be stationary if it holds the following conditions true:\n\nThe mean value of time-series is constant over time, which implies, the trend component is nullified/constant.\nThe variance does not increase over time.\nSeasonality effect is minimal.\n\nThis means it is devoid of trend or seasonal patterns, which makes it looks like a random white noise irrespective of the observed time interval.( i.e.¬†self-similar and fractal)"
  },
  {
    "objectID": "content/courses/Predictive-Analytics/Modules/50-Modelling-Time-Series/index.html#a-bit-of-forecasting",
    "href": "content/courses/Predictive-Analytics/Modules/50-Modelling-Time-Series/index.html#a-bit-of-forecasting",
    "title": "üïî Modelling Time Series",
    "section": "A Bit of Forecasting?",
    "text": "A Bit of Forecasting?\nWe are always interested in the future. We will do this in three ways:\n\nuse Simple Exponential Smoothing\nuse a package called forecast to fit an ARIMA (Autoregressive Moving Average Integrated Model) model to the data and make predictions for weekly sales;\nAnd do the same using a package called prophet.\n\nForecasting using Exponential Smoothing\nFor example, the file contains total annual rainfall in inches for London, from 1813-1912 (original data from Hipel and McLeod, 1994).\n\n\n\n\n\nThere is a nearly constant value of about 25 around which there are random fluctuations and it seems to be an additive model. How can we make forecasts with this time series?\nA deliberate detour:\nLet‚Äôs see some quick notation to aid understanding: Much of smoothing is based on the high school concept of a straight line, $ y = m * x + c$.\nIn the following, we choose to describe the models with:\n\n\n\\(y\\) : the actual values in the time series\n\n\\(\\hat y\\) : our predictions from whichever model we create\n\n\\(l\\) : a level or mean as forecast;\n\n\\(b\\) : a trend variable; akin to the slope in the straight line equation;\n\n\\(s\\) : seasonal component of the time series. Note that this is a set of values that stretch over one cycle of the time series.\n\nIn Exponential Smoothing and Forecasting, we make three models of increasing complexity:\n\nSimple Exponential Model: Here we deal only with the mean or level aspect of the (decomposed) time series and make predictions with that.\nHolt Model: Here we use the level and the trend from the decomposed time series for predictions\nHolt-Winters Model: Here we use the level, the trend, and the seasonal component from the decomposed time series for predictions.\n\n\n\n\n\n\n\n\n\n\nSimple Smoothing is smoothing based forecasting using just the level ( i.e.¬†mean) of the Time Series to make forecasts.\nDouble exponential smoothing, or Holt Smoothing Model, is just exponential smoothing applied to both level and trend.\nThe idea behind triple exponential smoothing, or the Holt-Winters Smoothing Model, is to apply exponential smoothing to the seasonal components in addition to level and trend.\nWhat does ‚ÄúExponential‚Äù mean?\nAll three models use memory: at each time instant in the Time Series, a set of past values, along with the present sample is used to make a prediction of the relevant parameter ( level / slope / seasonal). These are then added together to make the forecast.\nThe memory in each case controlled by a parameter: alpha for the estimate of the level beta for the slope estimate, and gamma for the seasonal component estimate at the current time point. All these parameters are between 0 and 1. The model takes a weighted average of past values of each parameter. The weights are derived in the form of \\(\\alpha(1-\\alpha)^i\\), where \\(i\\) defines how old the sample is compared to the present one, thus forming a set of weights that decrease exponentially with delay. Values of \\(\\alpha, \\beta. \\gamma\\) that are close to 0 mean that significant weightage is placed on observations in the past.(Memory is ‚Äústronger‚Äù).To express this in mathematical notation we now need three equations: one for level, one for the trend and one to combine the level and trend to get the expected \\(\\hat y\\) .\nTo make forecasts using simple exponential smoothing in R, we can use the HoltWinters() function in R, or the forecast::ets() function from forecasts. This latter function s more powerful.\n\n\nfunction (x, alpha = NULL, beta = NULL, gamma = NULL, seasonal = c(\"additive\", \n    \"multiplicative\"), start.periods = 2, l.start = NULL, b.start = NULL, \n    s.start = NULL, optim.start = c(alpha = 0.3, beta = 0.1, \n        gamma = 0.1), optim.control = list()) \nNULL\n\n\nfunction (y, model = \"ZZZ\", damped = NULL, alpha = NULL, beta = NULL, \n    gamma = NULL, phi = NULL, additive.only = FALSE, lambda = NULL, \n    biasadj = FALSE, lower = c(rep(1e-04, 3), 0.8), upper = c(rep(0.9999, \n        3), 0.98), opt.crit = c(\"lik\", \"amse\", \"mse\", \"sigma\", \n        \"mae\"), nmse = 3, bounds = c(\"both\", \"usual\", \"admissible\"), \n    ic = c(\"aicc\", \"aic\", \"bic\"), restrict = TRUE, allow.multiplicative.trend = FALSE, \n    use.initial.values = FALSE, na.action = c(\"na.contiguous\", \n        \"na.interp\", \"na.fail\"), ...) \nNULL\n\n\nTo use HoltWinters() for simple exponential smoothing, we need to set the parameters beta=FALSE and gamma=FALSE in the HoltWinters() function (the beta and gamma parameters are used for double exponential smoothing, or triple exponential smoothing.\nTo use forecast::ets, we set the model argument to ‚ÄúANN‚Äù, ‚ÄúAAN‚Äù, and ‚ÄúAAA‚Äù respectively for each of the three smoothing models.\nNote: The HoltWinters() function returns a list variable, that contains several named elements.\n\n\n\n\n\n\n\n\nARIMA\nWe can also use past trends and seasonality in the data to make predictions about the future using the forecast package. Here we use an auto ARIMA model to guess at the trend in the time series. Then we use that model to forecast a few periods into the future.\nMathematically an ARIMA model can be shown as follows:\n\n\n\n\n\n\nWe will use the familiar Walmart Sales dataset, and try to predict weekly sales for one of the Departments.\n\n\nTime Series:\nStart = c(2010, 1) \nEnd = c(2012, 52) \nFrequency = 52 \n  [1] 106690.06 111390.36 107952.07 103652.58 112807.75 112048.41 117716.13\n  [8] 113117.35 111466.37 116770.82 126341.84 110204.77 107648.14 125592.28\n [15] 120247.90 120036.99 121902.19 133056.97 131995.00 134118.05 120172.47\n [22] 124821.44 126241.20 121386.73 116256.35 108781.57 131128.96 131288.83\n [29] 124601.48 117929.58 124220.10 125027.49 124372.90 114702.69 113009.41\n [36] 120764.22 123510.99 110052.15 105793.40 110332.92 110209.31 107544.02\n [43] 106015.41 100834.31 111384.36 116521.67 121695.13  93676.95 107317.32\n [50] 109955.90 103724.16  99043.34 114270.08 117548.75 112165.80 107742.95\n [57] 116225.68 120621.32 123405.41 122280.13 112905.09 126746.25 126834.30\n [64] 118632.26 111764.31 120882.84 124953.94 112581.20 119815.67 135260.49\n [71] 136364.46 135197.63 121814.84 128054.88 133213.04 127906.50 121483.11\n [78] 117284.94 138538.47 138567.10 133260.84 122721.92 130446.34 133762.77\n [85] 133939.40 116165.28 115663.78 132805.42 125954.30 116931.34 108018.21\n [92] 114793.92 115047.16 113966.34 112688.97 102798.99 119053.80 120721.07\n [99] 125041.39  93358.91 116427.93 118685.12 113021.23 102202.04 115507.25\n[106] 125038.09 119807.63 110870.94 118406.27 125840.82 132318.50 117030.73\n[113] 127706.00 137958.76 129438.22 123172.79 118589.44 130920.36 131341.85\n[120] 129031.19 127603.00 130573.37 139857.10 140806.36 124594.40 131935.56\n[127] 148798.05 129724.74 126861.49 121030.79 134832.22 137408.20 136264.68\n[134] 118845.34 124741.33 140657.40 128542.73 119121.35 115326.47 127009.22\n[141] 124559.93 123346.24 117375.38 106690.06 111390.36 107952.07 103652.58\n[148] 112807.75 112048.41 117716.13 113117.35 111466.37 116770.82 126341.84\n[155] 110204.77 107648.14\n\n\nSeries: sales_95_ts \nARIMA(0,1,1)(0,1,0)[52] \n\nCoefficients:\n          ma1\n      -0.8842\ns.e.   0.0530\n\nsigma^2 = 29974424:  log likelihood = -1033.02\nAIC=2070.03   AICc=2070.15   BIC=2075.3\n\n\n\n\n\n\n\n\nWe‚Äôre fairly limited in what we can actually tweak when using autoplot(), so instead we can convert the forecast object to a data frame and use ggplot() like normal:\n\n\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n2013.000       116571.1 109554.8 123587.5 105840.6 127301.7\n2013.019       126102.0 119038.7 133165.2 115299.7 136904.3\n2013.038       120871.5 113761.7 127981.4 109998.0 131745.1\n2013.058       111934.8 104778.7 119091.0 100990.5 122879.2\n2013.077       119470.2 112268.0 126672.3 108455.5 130484.9\n2013.096       126904.7 119656.9 134152.5 115820.1 137989.3\n2013.115       133382.4 126089.2 140675.6 122228.3 144536.5\n2013.135       118094.6 110756.3 125433.0 106871.6 129317.7\n2013.154       128769.9 121386.7 136153.1 117478.2 140061.6\n2013.173       139022.7 131594.8 146450.5 127662.8 150382.5\n2013.192       130502.1 123030.0 137974.3 119074.5 141929.8\n2013.212       124236.7 116720.5 131752.9 112741.7 135731.7\n\n\n# A tibble: 324 √ó 7\n   index key      value lo.80 lo.95 hi.80 hi.95\n   <dbl> <chr>    <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 2010  actual 106690.    NA    NA    NA    NA\n 2 2010. actual 111390.    NA    NA    NA    NA\n 3 2010. actual 107952.    NA    NA    NA    NA\n 4 2010. actual 103653.    NA    NA    NA    NA\n 5 2010. actual 112808.    NA    NA    NA    NA\n 6 2010. actual 112048.    NA    NA    NA    NA\n 7 2010. actual 117716.    NA    NA    NA    NA\n 8 2010. actual 113117.    NA    NA    NA    NA\n 9 2010. actual 111466.    NA    NA    NA    NA\n10 2010. actual 116771.    NA    NA    NA    NA\n# ‚Ä¶ with 314 more rows\n\n\n# A tibble: 324 √ó 8\n   index key      value lo.80 lo.95 hi.80 hi.95 actual_date\n   <dbl> <chr>    <dbl> <dbl> <dbl> <dbl> <dbl> <date>     \n 1 2010  actual 106690.    NA    NA    NA    NA 1975-07-04 \n 2 2010. actual 111390.    NA    NA    NA    NA 1975-07-04 \n 3 2010. actual 107952.    NA    NA    NA    NA 1975-07-04 \n 4 2010. actual 103653.    NA    NA    NA    NA 1975-07-04 \n 5 2010. actual 112808.    NA    NA    NA    NA 1975-07-04 \n 6 2010. actual 112048.    NA    NA    NA    NA 1975-07-04 \n 7 2010. actual 117716.    NA    NA    NA    NA 1975-07-04 \n 8 2010. actual 113117.    NA    NA    NA    NA 1975-07-04 \n 9 2010. actual 111466.    NA    NA    NA    NA 1975-07-04 \n10 2010. actual 116771.    NA    NA    NA    NA 1975-07-04 \n# ‚Ä¶ with 314 more rows\n\n\n\n\n\nA Bit of Forecasting?\nWe are always interested in the future. We will do this in three ways:\n\nuse Simple Exponential Smoothing\nuse a package called forecast to fit an ARIMA (Autoregressive Moving Average Integrated Model) model to the data and make predictions for weekly sales;\nAnd do the same using a package called"
  },
  {
    "objectID": "content/courses/Predictive-Analytics/Modules/50-Modelling-Time-Series/index.html#workflow-using-orange",
    "href": "content/courses/Predictive-Analytics/Modules/50-Modelling-Time-Series/index.html#workflow-using-orange",
    "title": "üïî Modelling Time Series",
    "section": "Workflow using Orange",
    "text": "Workflow using Orange"
  },
  {
    "objectID": "content/courses/Predictive-Analytics/Modules/50-Modelling-Time-Series/index.html#workflow-using-radiant",
    "href": "content/courses/Predictive-Analytics/Modules/50-Modelling-Time-Series/index.html#workflow-using-radiant",
    "title": "üïî Modelling Time Series",
    "section": "Workflow using Radiant",
    "text": "Workflow using Radiant"
  },
  {
    "objectID": "content/courses/Predictive-Analytics/Modules/50-Modelling-Time-Series/index.html#workflow-using-r",
    "href": "content/courses/Predictive-Analytics/Modules/50-Modelling-Time-Series/index.html#workflow-using-r",
    "title": "üïî Modelling Time Series",
    "section": "Workflow using R",
    "text": "Workflow using R\n fred_raw.csv"
  },
  {
    "objectID": "content/courses/Predictive-Analytics/Modules/50-Modelling-Time-Series/index.html#conclusion",
    "href": "content/courses/Predictive-Analytics/Modules/50-Modelling-Time-Series/index.html#conclusion",
    "title": "üïî Modelling Time Series",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "content/courses/Predictive-Analytics/Modules/50-Modelling-Time-Series/index.html#references",
    "href": "content/courses/Predictive-Analytics/Modules/50-Modelling-Time-Series/index.html#references",
    "title": "üïî Modelling Time Series",
    "section": "References",
    "text": "References\n1, Shampoo Dataset Brownlee: https://raw.githubusercontent.com/jbrownlee/Datasets/master/shampoo.csv"
  },
  {
    "objectID": "content/courses/Prescriptive-Analytics/listing.html",
    "href": "content/courses/Prescriptive-Analytics/listing.html",
    "title": "Prescriptive Analytics",
    "section": "",
    "text": "Order By\nDefault\n\n          Date - Oldest\n        \n\n          Date - Newest\n        \n\n          Title\n        \n\n          Author\n        \n\n    \n      \n      \n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\nReading Time\n\n\n\n\n\n\n¬†\n\n\n\nNov 10, 2022\n\n\nüìê Intro to Linear Programming\n\n\nArvind Venkatadri\n\n\n1 min\n\n\n\n\n\n¬†\n\n\n\nNov 14, 2022\n\n\nüí≠ The Simplex Method - Intuitively\n\n\nArvind Venkatadri\n\n\n6 min\n\n\n\n\n\n¬†\n\n\n\nNov 20, 2022\n\n\nüìÖ The Simplex Method - In Excel\n\n\nArvind Venkatadri\n\n\n1 min\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/courses/Prescriptive-Analytics/Modules/10-Introduction-to-Linear-Programming/index.html",
    "href": "content/courses/Prescriptive-Analytics/Modules/10-Introduction-to-Linear-Programming/index.html",
    "title": "üìê Intro to Linear Programming",
    "section": "",
    "text": "What is Linear Programming?"
  },
  {
    "objectID": "content/courses/Prescriptive-Analytics/Modules/10-Introduction-to-Linear-Programming/index.html#demonstration-of-level-curve",
    "href": "content/courses/Prescriptive-Analytics/Modules/10-Introduction-to-Linear-Programming/index.html#demonstration-of-level-curve",
    "title": "üìê Intro to Linear Programming",
    "section": "Demonstration of Level Curve",
    "text": "Demonstration of Level Curve"
  },
  {
    "objectID": "content/courses/Prescriptive-Analytics/Modules/10-Introduction-to-Linear-Programming/index.html#linear-programming-solver",
    "href": "content/courses/Prescriptive-Analytics/Modules/10-Introduction-to-Linear-Programming/index.html#linear-programming-solver",
    "title": "üìê Intro to Linear Programming",
    "section": "Linear Programming Solver",
    "text": "Linear Programming Solver"
  },
  {
    "objectID": "content/courses/Prescriptive-Analytics/Modules/10-Introduction-to-Linear-Programming/index.html#linear-programming-in-3d-view",
    "href": "content/courses/Prescriptive-Analytics/Modules/10-Introduction-to-Linear-Programming/index.html#linear-programming-in-3d-view",
    "title": "üìê Intro to Linear Programming",
    "section": "Linear Programming in 3D view",
    "text": "Linear Programming in 3D view"
  },
  {
    "objectID": "content/courses/Prescriptive-Analytics/Modules/10-Introduction-to-Linear-Programming/index.html#linear-programming-interactive",
    "href": "content/courses/Prescriptive-Analytics/Modules/10-Introduction-to-Linear-Programming/index.html#linear-programming-interactive",
    "title": "üìê Intro to Linear Programming",
    "section": "Linear Programming Interactive",
    "text": "Linear Programming Interactive\nLet us say we have a Linear Programming problem with 3 variables: We define the model:\n\\[\nMaximise : 20x_1 + 10x_2 + 15x_3\\\\\nSubject \\ to \\\\\n\\\\\nx_1 + x_2 + x_3 <= 10\\\\\n3x_1 + x_3 <= 24\n\\]\n\n\n\n\n\n\nHere is the interactive LP Polytope:"
  },
  {
    "objectID": "content/courses/Prescriptive-Analytics/Modules/10-Introduction-to-Linear-Programming/index.html#references",
    "href": "content/courses/Prescriptive-Analytics/Modules/10-Introduction-to-Linear-Programming/index.html#references",
    "title": "üìê Intro to Linear Programming",
    "section": "References",
    "text": "References\n\nVirginia Postrel, Operations Everything, Boston Globe, Hune 27, 2004. http://archive.boston.com/news/globe/ideas/articles/2004/06/27/operation_everything?pg=full"
  },
  {
    "objectID": "content/courses/Prescriptive-Analytics/Modules/20-The-Simplex-Method-Intuitive/index.html",
    "href": "content/courses/Prescriptive-Analytics/Modules/20-The-Simplex-Method-Intuitive/index.html",
    "title": "üí≠ The Simplex Method - Intuitively",
    "section": "",
    "text": "To be written up."
  },
  {
    "objectID": "content/courses/Prescriptive-Analytics/Modules/20-The-Simplex-Method-Intuitive/index.html#a-random-walk-with-the-simplex-method",
    "href": "content/courses/Prescriptive-Analytics/Modules/20-The-Simplex-Method-Intuitive/index.html#a-random-walk-with-the-simplex-method",
    "title": "üí≠ The Simplex Method - Intuitively",
    "section": "A Random Walk with the Simplex Method",
    "text": "A Random Walk with the Simplex Method\nLet us try to form a geometric intuition for the Simplex method.\nWe will define an LP problem, and geometrically traverse the steps the Simplex method might take to solve for the optimum solution.\nLet us define a problem:\n\\[\nMaximise\\ 7.75x_1 + 10x_2\\\\\n\\] \\[\nSubject\\ to\\\\\n  \\begin{cases}\n    C1: -3x_1 + 2x_2 &<= 3\\\\\n    C2: 2x_1 + 4x_2 &<= 27\\\\\n    C3: 9x_1 + 10x_2 &<= 90\\\\\n    x_1, x_2 >= 0\n  \\end{cases}\n\\]\nThe Objective function is: \\(7.75x_1 + 10x_2\\)\nThe Constraints are defined by the three inequalities \\(C1::C3\\). In order to plot these, we convert the inequalities to equalities and plot these as lines. Each line splits the \\(x_1:x_2\\) plane into two half-planes. The inequality part is then taken into account by choosing the appropriate half-plane created by the equation. The intersection of all the half-planes defined by the constraints is the Feasibility Region.\n\n\n\nThe Feasibility region for this LP problem is plotted below:\n\n\n\n\n\n\n\n\nThe corner points of the Feasibility Region are:\n\n#   name     x1     x2\n# 1    A  0.000 0.0000\n# 2    B 10.000 0.0000\n# 3    C  5.625 3.9375\n# 4    D  2.625 5.4375\n# 5    E  0.000 1.5000\n\nRecall that:\n\nThe optimum in an LP problem is found on the boundary, at one of the vertices\nAt each of these vertices one or more constraints (\\(C1::C_n\\)) is tight, i.e.¬†there is no slack."
  },
  {
    "objectID": "content/courses/Prescriptive-Analytics/Modules/20-The-Simplex-Method-Intuitive/index.html#procedure",
    "href": "content/courses/Prescriptive-Analytics/Modules/20-The-Simplex-Method-Intuitive/index.html#procedure",
    "title": "üí≠ The Simplex Method - Intuitively",
    "section": "Procedure",
    "text": "Procedure\n\nWe start with an arbitrary point on the edge of the Feasibility Region. \\(A = (0,0)\\) is a common choice. At this point, since all variables are \\(0\\), the objective function is also \\(0\\).\nWe (arbitrarily) decide to move along the boundary of the Feasibility Region, to another FSP. We arbitrarily chose the \\(x_1\\) axis, and set/keep \\(x_2 = 0\\). We now wish to find out the \\(x_1\\) coordinate of the next FSP point. This would be at the intersection of the \\(x_1\\) axis and one of the Constraint lines.\nAll the three Constraint Lines would possibly intersect the \\(x_1\\) axis. We need to choose that intercept point that has the smallest, non-negative \\(x_1\\) intercept value. (Why?)\nSo, which Constraint Line intersects the \\(x_1\\) axis at the smallest value? Is it point B, or point F?\nTo find out, we substitute \\(x_2 = 0\\) in each of the Constraint equations, and solve for the \\(x_1\\):\\[\n\\begin{cases}\nC1: -3x_1 + 2 \\times 0 = 3 \\ => x_1 = \\color{red}{-1}\\\\\nC2: 2x_1 + 4\\times0 = 27 \\ => x_1 = 13.5\\ Point\\ F\\\\\n   {\\mathbf{ \\color{lightgreen}{C3}: 9x_1 + 10\\times0 = 90 \\ => x_1 = 10\\  \\color{lightgreen}{Point\\ B}}}\n\\end{cases}\n\\]\nNegative values for any variable are not permitted. So the smallest value of intercept is \\(x_1 = 10\\) for Constraint \\(C3\\). We therefore move to point \\(B(10,0)\\). At this point the objective function has improved to:\n\n\\[\nObjective = 7.75\\times 10 + 10\\times0 = 77.5\\ at\\ Point\\ B\n\\]\n\nWe now start from Point B, and move to the next nearest point. In identical fashion to Step2, we ‚Äúimagine‚Äù that we move along a new axis defined by:\\[\nIntercept = Point\\¬†B(10,0)\\\\\n\\] \\[\nEquation = Constraint\\¬†C3:¬†9x_1 + 10x_2 = 90\\\\\n\\] We express \\(x_1\\) in terms of \\(x_2\\) with \\(C3\\): \\[\n\\hat C3:¬†x_1 = \\frac{90 - 10x_2}{9}\n\\] As in Step 2, we substitute this equation \\(\\hat C3\\) into the other two constraints, \\(C1\\) and \\(C2\\): \\[\n\\begin{cases}\nC1: -3\\times \\frac{90 - 10x_2}{9} + 2x_2 = 3 \\ => x_2 = 6.18\\ Point\\ K\\\\\n{\\mathbf{ \\color{lightgreen}{C2}: 2\\times \\frac{90 - 10x_2}{9}+ 4x_2 = 27 => x_2 = 3.93\\ \\color{lightgreen}{Point\\ C}}}\n\\end{cases}\n\\] As before we choose the smaller of the two intercepts, so \\(x_2 = 3.93\\). Calculating for \\(x_1\\), we get point \\(C(5.63, 3.93)\\). At this point the objective function has improved to:\n\n\\[\n7.75\\times 5.63 + 10\\times 3.93 = 82.9\\ at\\ Point\\ C\n\\]\n\nWe now proceed along the constraint line \\(C2\\) towards the next point. In identical fashion to Step 2 and 3, we ‚Äúimagine‚Äù that we move along a new axis defined by: \\[\nIntercept = Point\\¬†C(5.63,3.93)\n\\] \\[\nEquation = Constraint\\¬†C2:¬†2x_1 + 4x_2 = 27 \\\\\n\\] Again, We express \\(x_1\\) in terms of \\(x_2\\) with \\(C2\\) this time: \\[\n\\hat C2:¬†x_1 = \\frac{27 - 4x_2}{2}\n\\] As in Step 2 and, we substitute this equation \\(\\hat C2\\) into the other constraint, the only remaining \\(C1\\): \\[\n{\\mathbf C1: -3\\times \\frac{27 - 4x_2}{2} + 2x_2 = 3 \\ => x_2 = 5.44\\ Point\\ D\\\\}\n\\] Calculating for \\(x_1\\), we get point \\(C(2.63, 5.44)\\). At this point the objective function has improved decreased to: \\[\n7.75\\times 2.63 + 10\\times 5.44 = 74.8\\¬†at\\¬†Point\\¬†D\n\\] Since this value for the Objective function is smaller than that at the previous point, our search terminates and we decide that Point \\(C(5.63,3.93)\\) is the optimal point.\nSo the final result is: \\[\n   x_1(max) = 5.63\\\\\n\\] \\[\n   x_2(max) = 3.93\\\\\n\\] \\[\n   Maximum\\¬†Objective\\¬†Function\\¬†Value = 82.9\n\\] The final result is plotted below:"
  },
  {
    "objectID": "content/courses/Prescriptive-Analytics/Modules/20-The-Simplex-Method-Intuitive/index.html#summary",
    "href": "content/courses/Prescriptive-Analytics/Modules/20-The-Simplex-Method-Intuitive/index.html#summary",
    "title": "üí≠ The Simplex Method - Intuitively",
    "section": "Summary",
    "text": "Summary\nThe essence of this ‚Äúintuitive method‚Äù can be captured as follows:\n\nStart from a known simple point on the edge of Feasibility Region, e.g.¬†(0,0), since the two coordinate axes frequently form two edges to the Feasibility Region.\n\nMove along one of the axis to find a first adjacent edge point. This adjacent point corresponds to the ‚Äútightening‚Äù of one or other of the Constraint equations(i.e.¬†slack = 0 for that Constraint)\n\nCalculate the Objective function at that point.\n\nUse this new point as the next starting point and move along the Constraint line from the previous step.\n\nRepeat step 2 and 3, calculating the Objective function each time.\n\nKeep the solution point where the objective function hits a maximum, i.e.¬†when moving to the next point reduces the value of the Objective function."
  },
  {
    "objectID": "content/courses/Prescriptive-Analytics/Modules/30-The-Simplex-Method-Excel/index.html",
    "href": "content/courses/Prescriptive-Analytics/Modules/30-The-Simplex-Method-Excel/index.html",
    "title": "üìÖ The Simplex Method - In Excel",
    "section": "",
    "text": "Let us take the same problem as before:\n\\[\nMaximise\\ 7.75x_1 + 10x_2 \\\\\n\\] \\[\nSubject\\ to \\\\\n  \\begin{cases}\n    C1: -3x_1 + 2x_2 &<= 3 \\\\\n    C2: 2x_1 + 4x_2 &<= 27 \\\\\n    C3: 9x_1 + 10x_2 &<= 90 \\\\\n    x_1, x_2 >= 0\n  \\end{cases}\n\\]"
  },
  {
    "objectID": "content/courses/Prescriptive-Analytics/Modules/30-The-Simplex-Method-Excel/index.html#procedure",
    "href": "content/courses/Prescriptive-Analytics/Modules/30-The-Simplex-Method-Excel/index.html#procedure",
    "title": "üìÖ The Simplex Method - In Excel",
    "section": "Procedure",
    "text": "Procedure\n\nSet up an Excel sheet as shown in the picture below. We enter in the objective function and the constraints in tabular form as shown:\n\n\n\n\n\n\n\n\n\n\nNext we invoke the Solver Add-in: (Data -> Solver):\n\n\n\n\n\n\n\n\n\n\nWe set up the Solver for our problem as follows: Hit the SOLVE button.\n\n\n\n\n\n\n\n\n\n\nChoose to have all the three kinds of Reports from Solver (Answers, Sensitivity, and Limits).\n\n\n\n\n\n\n\n\n\nThis will create three new tabs which give additional information on:\n- How ‚Äúcentered‚Äù the solution is, or is it sensitive to variations of some parameters\n- How much slack do the individual constraints still have, at the end\nWe will discuss this in class!\nThe complete Excel file is here for your reference."
  },
  {
    "objectID": "content/courses/R-for-Artists/listing.html",
    "href": "content/courses/R-for-Artists/listing.html",
    "title": "R for Artists",
    "section": "",
    "text": "Order By\nDefault\n\n          Date - Oldest\n        \n\n          Date - Newest\n        \n\n          Title\n        \n\n    \n      \n      \n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\n\n\nNov 1, 2021\n\n\nüï∂ Science, Human Experience, Experiments, and Data\n\n\n20 min\n\n\n\n\n\n\n\nInvalid Date\n\n\nLab-2: Down the R-abbit Hole‚Ä¶\n\n\n0 min\n\n\n\n\n\n\n\nAug 21, 2022\n\n\nLab-6: These Roses have been Painted !!\n\n\n0 min\n\n\n\n\n\n\n\nJun 6, 2022\n\n\nLab-10: An Invitation from the Queen‚Ä¶to play Croquet\n\n\n0 min\n\n\n\n\n\n\n\nInvalid Date\n\n\nLab-11: The Queen of Hearts, She Made some Tarts\n\n\n0 min\n\n\n\n\n\n\n\nInvalid Date\n\n\nLab-12: Time is a Him!!\n\n\n6 min\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/10-Basics/index.html#fa-envelope-titlean-envelope-where-does-data-come-from",
    "href": "content/courses/R-for-Artists/Modules/10-Basics/index.html#fa-envelope-titlean-envelope-where-does-data-come-from",
    "title": "üï∂ Science, Human Experience, Experiments, and Data",
    "section": "\n Where does Data come from?",
    "text": "Where does Data come from?\nWe will need to form a basic understanding of basic scientific enterprise. Let us look at the slides."
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/10-Basics/index.html#fa-chart-simple-why-visualize",
    "href": "content/courses/R-for-Artists/Modules/10-Basics/index.html#fa-chart-simple-why-visualize",
    "title": "üï∂ Science, Human Experience, Experiments, and Data",
    "section": "\n Why Visualize?",
    "text": "Why Visualize?\n\nWe can digest information more easily when it is pictorial\nOur Working Memories are both short-term and limited in capacity. So a picture abstracts the details and presents us with an overall summary, an insight, or a story that is both easy to recall and easy on retention.\n\nData Viz includes shapes that carry strong cultural memories and impressions for us. These cultural memories help us to use data viz in a universal way to appeal to a wide variety of audiences. (Do humans have a gene for geometry?)\nIt helps sift facts and mere statements: for example:\n\n\n\nRape Capital\n\n\n\n\nWhat does Data Reveal?"
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/10-Basics/index.html#iconify-mdi-category-plus-what-are-data-types",
    "href": "content/courses/R-for-Artists/Modules/10-Basics/index.html#iconify-mdi-category-plus-what-are-data-types",
    "title": "üï∂ Science, Human Experience, Experiments, and Data",
    "section": "\n What are Data Types?",
    "text": "What are Data Types?\n\n\n\nIn more detail:"
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/10-Basics/index.html#sec-data-types",
    "href": "content/courses/R-for-Artists/Modules/10-Basics/index.html#sec-data-types",
    "title": "üï∂ Science, Human Experience, Experiments, and Data",
    "section": "\n How do we Spot Data Variable Types?",
    "text": "How do we Spot Data Variable Types?\nBy asking questions!\n\n\nQuestions and Types of Variables\n\n\n\n\n\n\n\n\nPronoun\nAnswer\nVariable / Scale\nExample\nWhat Operations ?\n\n\n\nWhat, Who, Where, Whom, Which\nName, Place, Animal, Thing\nQualitative or Nominal\n\nName\n\nCount of Cases\nMode\n\n\n\nHow, What Kind, What Sort\nA Manner / Method / Type or Attribute from a List, with list items in some ‚Äúorder‚Äù ( e.g good, better, improved, best‚Ä¶)\nQualitative or Ordinal\n\n\nSo cioeconomic status,\nEducation,\nIncome,\nS atisfaction\n\n\nMedian\nPercentiles\n\n\n\nHow Many, How much, How Heavy , How few, Seldom, Often, When\n\nQuantities with Scale\nDi fferences are meaningful, but not products or ratios\n\nQ uantitative or I nterval**\n\npH\nSAT Score (200-800)\nCredit Score (300-850)\nYear of Starting College\n\n\nMean\n\nStandard\nDeviation\n\n\n\n\nHow Many, How much, How Heavy , How few, Seldom, Often, When\n\nQuantities, with Scale and a Zero Value.\nDifferences and Ratios /Products are meaningful. (e.g Weight )\n\nQ uantitative or Ratio\n\n\nWeight, Length, Height,\nT emperature, Enzyme Activity, Dosage Amount, Reaction Rate,\nC on centration, Pulse Rate, Survival Time\n\n\nCorrelation\nCoefficient of Variation\n\n\n\n\n\nAs you go from Qualitative to Quantitative data types in the table, I hope you can detect a movement from fuzzy groups/categories to more and more crystallized numbers.\n\n\nType of Variables\n\n\nEach variable/scale can be subjected to the operations of the previous group. In the words of S.S. Stevens\n\nthe basic operations needed to create each type of scale is cumulative: to an operation listed opposite a particular scale must be added all those operations preceding it."
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/10-Basics/index.html#sec-data-viz",
    "href": "content/courses/R-for-Artists/Modules/10-Basics/index.html#sec-data-viz",
    "title": "üï∂ Science, Human Experience, Experiments, and Data",
    "section": "What Are the Parts of a Data Viz?",
    "text": "What Are the Parts of a Data Viz?"
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/10-Basics/index.html#how-to-pick-a-data-viz",
    "href": "content/courses/R-for-Artists/Modules/10-Basics/index.html#how-to-pick-a-data-viz",
    "title": "üï∂ Science, Human Experience, Experiments, and Data",
    "section": "How to pick a Data Viz?",
    "text": "How to pick a Data Viz?\nMost Data Visualizations use one or more of the following geometric attributes or aesthetics. These geometric aesthetics are used to represent qualitative or quantitative variables from your data.\n\n\nCommon Geometric Aesthetics in Charts\n\n\nWhat does this mean? We can think of simple visualizations as combinations of these aesthetics. Some examples:\n\n\nGeometries , Combinations, and Graphs\n\n\n\n\n\n\nAesthetic #1\nAesthetic #2\nShape\n\n\n\nPosition X = Quant Variable\nPosition Y = Quant Variable\nPoints/Circles with Fixed Size\n\n\nPosition X = Qual Variable\nPosition Y = Count of Qual Variable\nColumns\n\n\nPosition X = Qual Variable\nPosition Y = Qual Variable\nRectangles, with area proportional to joint(X,Y) count\n\n\n\nPosition X = Qual Variable\nPosition Y = Rank-Ordered Quant Variable\nBox + Whisker, Box length proportional to Inter-Quartile Range, whisker-length proportional to upper and lower quartile resp.\n\n\nPosition X = Quant Variable\nPosition Y = Quant Variable + Qual Variable\n\nLine and Area\nColour for Area"
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/100-GoN/index.html#fa-folder-open-slides-and-tutorials",
    "href": "content/courses/R-for-Artists/Modules/100-GoN/index.html#fa-folder-open-slides-and-tutorials",
    "title": "Lab-10: An Invitation from the Queen‚Ä¶to play Croquet",
    "section": "\n Slides and Tutorials",
    "text": "Slides and Tutorials"
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/100-GoN/index.html#fa-envelope-introduction",
    "href": "content/courses/R-for-Artists/Modules/100-GoN/index.html#fa-envelope-introduction",
    "title": "Lab-10: An Invitation from the Queen‚Ä¶to play Croquet",
    "section": "\n Introduction",
    "text": "Introduction\nNetwork Diagrams are important in data visualization to bring out relationships between diverse entities. They are used in ecology, biology, transportation, and even history!\nAnd hey, whom did Jon Snow marry?"
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/100-GoN/index.html#fa-asterisk-references",
    "href": "content/courses/R-for-Artists/Modules/100-GoN/index.html#fa-asterisk-references",
    "title": "Lab-10: An Invitation from the Queen‚Ä¶to play Croquet",
    "section": "\n References",
    "text": "References\n\nMichael Gastner, Data Analysis and Visualisation with R, Chapter 23: Networks\nDavid Schoch, Network Visualizations in R using ggraph and graphlayouts\nKonrad M. Lawson, Toilers and Gangsters:Simple Network Visualization with R for Historians"
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html",
    "href": "content/courses/R-for-Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html",
    "title": "The Grammar of Diagrams",
    "section": "",
    "text": "There are many presentation and drawing tools out there. And these allow the user full control over the diagram so generally result in prettier diagrams that can convey more information to the audience at that point in time.\nBut that point in time passes, and pretty pictures can quickly become out-of-date and, ironically, misinforming if they don‚Äôt match the reality of the system they are describing. This is especially so if one team is drawing the pretty pictures, and another team is writing the software/implementing the system.\nHaving diagrams as code that can live beside the system design/code, that the stakeholders are equally comfortable editing and viewing,reduces the gap i.e.¬†‚ÄúWhere system diagrams meet system reality‚Äù.\nWe will ‚Äúexplore‚Äù two packages to do this: DiagrammeR and nomnoml. Each of these follows a specific grammar so that sets of ‚Äúsentences‚Äù will morph into very different kinds of diagrams."
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#sequence-diagram",
    "href": "content/courses/R-for-Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#sequence-diagram",
    "title": "The Grammar of Diagrams",
    "section": "Sequence Diagram",
    "text": "Sequence Diagram\nLook at the code below: What do you think it represents?\n\nCodeDiagrammeR(\"\nsequenceDiagram\nArvind ->> Anamika: Why are you late today?\nAnamika ->> Anamika: Ulp...\nAnamika ->> Arvind: I am sorry... <br> may I come in please?\n\nArvind ->> Komal: And you? What kept you?\nKomal ->> Anamika: (Quietly) He's having a bad day, dude...\nAnamika ->> Komal: (Whisper) Boomer...\n\")\n\n\n\n\n\nThis is a simple Sequence Diagram! Shows a strictly imaginary exchange between a pair of students and an unknown Faculty Member.\nLet us now see how we can embellish this kind of diagram. Can we have a Garden of Forking Paths?\n\nCodeDiagrammeR(\"\n  graph LR\n    A-->B\n    A-->C\n    C-->E\n    B-->D\n    C-->D\n    D-->F\n    E-->F\n\")\n\n\n\n\n\n\nCodeDiagrammeR(\"\n        sequenceDiagram\n        \n        alt Anamika is always punctual\n        Arvind ->> Anamika: Why haven't you put up your Daily Reflection?\n        Anamika ->> Anamika: Ulp...\n        Note right of Anamika : I have had it today..\n        Anamika ->> Arvind: I am sorry... \n        Arvind ->> Anamika: Ok write it today\n        \n        else Anamika is usually tardy\n        Arvind ->> Anamika: Why haven't you put up your Daily Reflection?\n        Anamika ->> Anamika: Ulp...\n        Anamika ->> Arvind: I am sorry... \n        Arvind ->> Anamika: This is not acceptable and will reflect in your grade\n        end\n        \n        Arvind ->> Komal: And you? What kept you?\n        Komal ->> Anamika: (Quietly) He's having a bad day, dude...\n        Anamika ->> Komal: (Whisper) Boomer...\n        Note over Anamika,Komal: Giggle...\n\")\n\n\n\n\n\n\nFrom here: https://cyberhelp.sesync.org/blog/visualization-with-diagrammeR.html\n\nCodegrViz(\"digraph{\n\n      graph[rankdir = LR]\n  \n      node[shape = rectangle, style = filled]\n  \n      node[fillcolor = Coral, margin = 0.2]\n      A[label = 'Figure 1: Map']\n      B[label = 'Figure 2: Metrics']\n  \n      node[fillcolor = Cyan, margin = 0.2]\n      C[label = 'Figures.Rmd']\n  \n      node[fillcolor = Violet, margin = 0.2]\n      D[label = 'Analysis_1.R']\n      E[label = 'Analysis_2.R']\n  \n      subgraph cluster_0 {\n        graph[shape = rectangle]\n        style = rounded\n        bgcolor = Gold\n    \n        label = 'Data Source 1'\n        node[shape = rectangle, fillcolor = LemonChiffon, margin = 0.25]\n        F[label = 'my_dataframe_1.csv']\n        G[label = 'my_dataframe_2.csv']\n      }\n  \n      subgraph cluster_1 {\n         graph[shape = rectangle]\n         style = rounded\n         bgcolor = Gold\n    \n         label = 'Data Source 2'\n         node[shape = rectangle, fillcolor = LemonChiffon, margin = 0.25]\n         H[label = 'my_dataframe_3.csv']\n         I[label = 'my_dataframe_4.csv']\n      }\n  \n      edge[color = black, arrowhead = vee, arrowsize = 1.25]\n      C -> {A B}\n      D -> C\n      E -> C\n      F -> D\n      G -> D\n      H -> E\n      I -> E\n      \n      }\")\n\n\n\n\n\n\nCodemermaid(\"\n        graph BT\n        A((Salinity))\n        A-->B(Barnacles)\n        B-.->|-0.10|B1{Mussels}\n        A-- 0.30 -->B1\n\n        C[Air Temp]\n        C-->B\n        C-.->E(Macroalgae)\n        E-->B1\n        C== 0.89 ==>B1\n\n        style A fill:#FFF, stroke:#333, stroke-width:4px\n        style B fill:#9AA, stroke:#9AA, stroke-width:2px\n        style B1 fill:#879, stroke:#333, stroke-width:1px\n        style C fill:#ADF, stroke:#333, stroke-width:2px\n        style E fill:#9C2, stroke:#9C2, stroke-width:2px\n\n        \")\n\n\n\n\n\n\nCodeDiagrammeR(\"\nsequenceDiagram\n  Arvind ->>ticket seller: ask ticket\n  ticket seller->>database: seats\n  alt tickets available\n    database->>ticket seller: ok\n    ticket seller->>customer: confirm\n    Arvind ->>ticket seller: ok\n    ticket seller->>database: book a seat\n    ticket seller->>printer: print ticket\n  else sold out\n    database->>ticket seller: none left\n    ticket seller->>customer: sorry\n  end\n\")\n\n\n\n\n\n\nCodeDiagrammeR(\n\"graph TB;\nA(Rounded)-->B[Squared];\nB-->C{A Decision};\nC-->D[Square One];\nC-->E[Square Two];\n\n%% Now styling these blocks\nstyle A fill:#E5E25F;  \nstyle B fill:#87AB51; \nstyle C fill:#3C8937;\nstyle D fill:#23772C;  \nstyle E fill:#B6E6E6;\n\"\n)\n\n\n\n\n\n\nCode  grViz(\"\ndigraph boxes_and_circles {\n\n  # a 'graph' statement\n  graph [overlap = true, fontsize = 10,forcelabels = true]\n\n  # several 'node' statements\n  node [shape = box,fontname = Helvetica, color = red, style = filled]\n  A[label = 'This is \\\\n an internal \\\\n label', xlabel = 'This is \\\\nan external \\\\nlabel']; B; C; D; E; F\n\n  node [shape = circle, fixedsize = true, color = palegreen, width = 0.9] // sets as circles\n  1; 2; 3; 4; 5; 6; 7; 8\n\n  # several 'edge' statements\n  A->{1,2,3,4} B->2 B->3 B->4 C->A\n  1->D E->A 2->4 1->5 1->F\n  E->6 4->6 5->7 6->7 3->8 3->1\n}\n\")"
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#sequence-diagram-2",
    "href": "content/courses/R-for-Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#sequence-diagram-2",
    "title": "The Grammar of Diagrams",
    "section": "Sequence Diagram-2",
    "text": "Sequence Diagram-2"
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#sequence-diagram-3",
    "href": "content/courses/R-for-Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#sequence-diagram-3",
    "title": "The Grammar of Diagrams",
    "section": "Sequence Diagram 3",
    "text": "Sequence Diagram 3"
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#mindmap",
    "href": "content/courses/R-for-Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#mindmap",
    "title": "The Grammar of Diagrams",
    "section": "Mindmap",
    "text": "Mindmap"
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#gantt-chart",
    "href": "content/courses/R-for-Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#gantt-chart",
    "title": "The Grammar of Diagrams",
    "section": "Gantt Chart",
    "text": "Gantt Chart"
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#flow-chart",
    "href": "content/courses/R-for-Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#flow-chart",
    "title": "The Grammar of Diagrams",
    "section": "Flow chart",
    "text": "Flow chart"
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#some-definitions-on-the-grammar-of-shapes-in-nomnoml",
    "href": "content/courses/R-for-Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#some-definitions-on-the-grammar-of-shapes-in-nomnoml",
    "title": "The Grammar of Diagrams",
    "section": "Some definitions on the ‚Äúgrammar of shapes‚Äù in nomnoml\n",
    "text": "Some definitions on the ‚Äúgrammar of shapes‚Äù in nomnoml\n\n\nAssociation Types: Connectors between blocks( i.e.¬†Classifiers)\nClassifier Types: Kinds of blocks.\nDirective Types: Directives change the nature of the diagram rendered, by affective parameters like colour, direction and margins. ( Ha! VC people!!)\n\nCSS colours https://www.w3schools.com/cssref/css_colors.asp Only these colours are permitted, so use either the names or these specific colour hash codes. Any general hash code will not render.\n\nCode//association-1\n[a] - [b] \n\n//association-2\n[b] -> [c] \n\n//association_3\n[c] <-> [a]\n\n//dependency-1\n[a] <-->[d]\n\n//dependency-2\n#.ell: visual=ellipse fill=#fbfb09 bold\n#.arvind: visual=rhomb fill=#ff2234 bold\n[<ell>e]-->[a]\n//generalization-1\n[c]-:>[<arvind>k]\n\n//implementation --:>\n[k]--:>[d]\n\n\n\n\n\n\nCode//composition +-\n[a]+-[b]\n//composition +->\n[b]-+[c]\n//aggregation o-\n[c]o->[d]\n//aggregation o->\n[d]o->[a]\n//note --\n[d]--[everything happens;here]\n//hidden -/-\n[d]-/-[f]\n////////////////////////\n//weightless edge _>\n//[k]_>[d] //not working\n//weightless dashed__\n//[d]__[j] //not working\n\n\n\n\n\nClassifier Types\nThese are different kinds of blocks.\n\nCode[class]->[<abstract> abstract]\n[<abstract> abstract]-:>[<instance> instance]\n[<instance> instance]-:>[<note> note]\n[<note> note]-->[<reference> reference]\n\n\n\n\n\n\nCode[<package> package|components]-->[<frame> frame|]\n[<database> database]-->[<start> start]\n[<end> end]-o>[<state> state]\n\n\n\n\n\n\nCode[<choice> choice]--->[<sync> sync]\n[<input> input]->[<sender> sender]\n[<receiver> receiver]o-[<transceiver> transceiver]\n\n\n\n\n\n\nCode#direction:down\n#background:lightgrey\n#fill: fuchsia; green; purple\n#fillArrows: false\n#font: Courier\n[class]->[<abstract> abstract]\n[<abstract> abstract]-:>[<instance> instance]\n[<instance> instance]-:>[<note> note]\n[<note> note]-->[<reference> reference]\n\n\n\n\n\n\nCode#font: CenturySchoolbook\n#fill: lightyellow\n#stroke: green\n\n[<actor> actor]---[<usecase> usecase]\n[<usecase> usecase]<-->[<label> label]\n[<usecase> usecase]-/-[<hidden> hidden]\n\n\n\n\n\n\nCode[<table> table| a | 5 || b | 7]\n\n\n\n\n\n\n\nCode[<table> table| c | 9 ]"
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#directives",
    "href": "content/courses/R-for-Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#directives",
    "title": "The Grammar of Diagrams",
    "section": "Directives",
    "text": "Directives\nDirectives change the nature of the diagram rendered, by affective parameters like colour, direction and margins."
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#custom-classifier-styles",
    "href": "content/courses/R-for-Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#custom-classifier-styles",
    "title": "The Grammar of Diagrams",
    "section": "Custom classifier styles",
    "text": "Custom classifier styles\nA directive that starts with ‚Äú.‚Äù define a classifier‚Äôs style. The style is written as a space separated list of modifiers and key/value pairs.\n\nCode#.box: fill=#8f8 dashed\n#.blob: visual=ellipse title=bold\n#.arvind: visual=rhomb title=bold dashed fill=CornFlowerBlue\n[<box> GreenBox]\n[<blob> Blobby]\n[<arvind> Someone]"
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#nomnoml-keyvalue-pairs",
    "href": "content/courses/R-for-Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#nomnoml-keyvalue-pairs",
    "title": "The Grammar of Diagrams",
    "section": "\nnomnoml Key/value pairs",
    "text": "nomnoml Key/value pairs\n\nfill=(any css color)\nstroke=(any css color)\nalign=center align=left\ndirection=right direction=down\nvisual=actor\nvisual=class\nvisual=database\nvisual=ellipse\nvisual=end\nvisual=frame\nvisual=hidden\nvisual=input\nvisual=none\nvisual=note\nvisual=package\nvisual=receiver\nvisual=rhomb\nvisual=roundrect\nvisual=sender\nvisual=start\nvisual=sync\nvisual=table\nvisual=transceiver"
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#text-modifiers",
    "href": "content/courses/R-for-Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#text-modifiers",
    "title": "The Grammar of Diagrams",
    "section": "Text modifiers",
    "text": "Text modifiers\nbold center italic left underline\n\nCode# .box: fill=#8f8 dashed\n# .blob: visual=rhomb title=bold fill=#8f8 dashed\n\n[A]-[B]\n[B]--[<usecase>C]\n[C]-[<box> D]\n[B]--[<blob> Jabba;TheHut]\n\n\n\n\n\n\nCode[a] ->[b]\n[b] -:> [c]\n[c]o->[d]\n[d]-/-[e]\n\n\n\n\n\n\nCode#fill: lightgreen; lightblue; lightyellow; grey; white\n\n[<table> table | c | 9 ]\n\n[R | [<table> Packages |\n         Base R |\n         [ <table> tidyverse| ggplot | tidyr | readr |\n             [<table> dplyr|\n                 magrittr | Others]]]]\n\n\n\n\n\n\n\nCode#fill: lightgreen; lightblue; lightyellow; pink; white\n\n[RStudio | [R | [<table> Packages |\n                   Base R | [ tidyverse |\n                               ggplot | tidyr | readr |\n                               [dplyr]--[magrittr]\n                               [dplyr]--[Others]\n                             | tibble\n                             ]\n                 | lubridate | DiagrammeR | Lattice]]]\n\n\n\n\n\n\n\nCode[Linux]+-[Ubuntu]\n[Linux]+-[Mint]\n[Ubuntu]--[Mint]\n[Linux]+-[Rosa Linux]\n[Linux]+-[Mx Linux]\n[Debian]-+[Linux]\n\n\n[Fedora]-+[Linux]\n[Puppy Linux]-+[Linux]\n[Personal Pups]-+[Puppy Linux]"
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/110-GoD/index.html#introduction",
    "href": "content/courses/R-for-Artists/Modules/110-GoD/index.html#introduction",
    "title": "Lab-11: The Queen of Hearts, She Made some Tarts",
    "section": "Introduction",
    "text": "Introduction\nWe can use R to create complicated diagrams too ! Flow charts, Gantt charts, Org charts‚Ä¶all with R. We will use packages such as nomnoml and DiagrammeR to achieve these ends."
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/120-time/index.html#introduction",
    "href": "content/courses/R-for-Artists/Modules/120-time/index.html#introduction",
    "title": "Lab-12: Time is a Him!!",
    "section": "Introduction",
    "text": "Introduction\nTime Series data are important in data visualization where events have a temporal dimension, such as with finance, transportation, music, telecommunications for example."
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/120-time/index.html#creating-a-time-series",
    "href": "content/courses/R-for-Artists/Modules/120-time/index.html#creating-a-time-series",
    "title": "Lab-12: Time is a Him!!",
    "section": "Creating a time series",
    "text": "Creating a time series\nIn this first example, we will use simple ts data, and then do another with a tibble dataset, and then a third example with an tsibble formatted dataset.\n\nts format data\nThere are a few datasets in base R that are in ts format already.\n\nCodeAirPassengers\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1949 112 118 132 129 121 135 148 148 136 119 104 118\n1950 115 126 141 135 125 149 170 170 158 133 114 140\n1951 145 150 178 163 172 178 199 199 184 162 146 166\n1952 171 180 193 181 183 218 230 242 209 191 172 194\n1953 196 196 236 235 229 243 264 272 237 211 180 201\n1954 204 188 235 227 234 264 302 293 259 229 203 229\n1955 242 233 267 269 270 315 364 347 312 274 237 278\n1956 284 277 317 313 318 374 413 405 355 306 271 306\n1957 315 301 356 348 355 422 465 467 404 347 305 336\n1958 340 318 362 348 363 435 491 505 404 359 310 337\n1959 360 342 406 396 420 472 548 559 463 407 362 405\n1960 417 391 419 461 472 535 622 606 508 461 390 432\n\nCodestr(AirPassengers)\n\n Time-Series [1:144] from 1949 to 1961: 112 118 132 129 121 135 148 148 136 119 ...\n\n\nThis can be easily plotted using base R:\n\nCodeplot(AirPassengers)\n\n\n\n\nLet us take data that is ‚Äútime oriented‚Äù but not in ts format, and convert it to ts: the syntax of ts() is:\nSyntax:  objectName <- ts(data, start, end, frequency)\n\nwhere,\n\n    `data` represents the data vector\n    `start` represents the first observation in time series\n    `end` represents the last observation in time series\n    frequency represents number of observations per unit time. For \n    example, frequency=1 for monthly data.\nWe will pick simple numerical vector data ChickWeight:\n\nCodeChickWeight %>% head()\n\n\n\n  \n\n\nCodeChickWeight_ts <- ts(ChickWeight$weight, frequency = 2)\nplot(ChickWeight_ts)\n\n\n\n\nThe ts format is not recommended for new analysis since it does not permit inclusion of multiple time series in one dataset, nor other categorical variables for grouping etc.\n\ntibble format data\nSome ‚Äútime-oriented‚Äù datasets are available in tibble form. Let us try to plot one, the walmart_sales_weekly dataset from the timetk package:\n\nCodedata(walmart_sales_weekly, package = \"timetk\")\nwalmart_sales_weekly %>% head()\n\n\n\n  \n\n\n\nThis dataset is a tibble with a Date column. Let us plot the data using this column on the time/X-axis:\n\nCodewalmart_sales_weekly %>% \n  # convert Dept number to a **categorical factor**\n  mutate(Dept = forcats::as_factor(Dept)) %>% \n  ggplot(aes(x = Date, y = Weekly_Sales, group = Dept)) + \n  geom_point(aes(colour = Dept)) +\n  geom_line(aes(colour = Dept))\n\n\n\n\n\ntsibble format data\nFor more analysis and forecasting etc., it is useful to convert this tibble into a tsibble:\n\nCodewalmart_tsibble <- as_tsibble(walmart_sales_weekly,\n                         index = Date,\n                         key = c(id, Dept))\nwalmart_tsibble\n\n\n\n  \n\n\n\nThe 7D states the data is weekly. There is a Date column and all the other numerical variables are time-varying quantities. The categorical variables such as id, and Dept allow us to identify separate time series in the data, and these have 7 combinations hence are 7 time series in this data, as indicated.\nLet us plot Weekly_Sales, colouring the time series by Dept:\n\nCodewalmart_tsibble %>% \n  ggplot(aes( x = Date, y = Weekly_Sales, colour = Dept)) + \n           geom_line() + \n           geom_point() + \n           labs(title = \"Weekly Sales by Dept at Walmart\")\n\n\n\nFigure¬†1: Walmart Time Series\n\n\n\n\nThis first plot is that of all series together. There does seem to be an annual variation, which we should investigate later.\nNote the blue trend lines that are automatically plotted in the second graph. This is caused by the argument .smooth = TRUE.\n‚äïWe can also do a quick autoplot that seems to offer less control and is also not interactive.\n\n\n\nCodewalmart_tsibble %>% \n  dplyr::group_by(Dept) %>% \n  autoplot(Weekly_Sales)\n\n`mutate_if()` ignored the following grouping variables:\n‚Ä¢ Column `Dept`"
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/120-time/index.html#one-more-example",
    "href": "content/courses/R-for-Artists/Modules/120-time/index.html#one-more-example",
    "title": "Lab-12: Time is a Him!!",
    "section": "One more example",
    "text": "One more example\nOften we have data in table form, that is time-oriented, with a date like column, and we need to convert it into a tsibble for analysis:\n\nCodeprison <- readr::read_csv(\"https://OTexts.com/fpp3/extrafiles/prison_population.csv\")\n\nRows: 3072 Columns: 6\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr  (4): State, Gender, Legal, Indigenous\ndbl  (1): Count\ndate (1): Date\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nCodeglimpse(prison)\n\nRows: 3,072\nColumns: 6\n$ Date       <date> 2005-03-01, 2005-03-01, 2005-03-01, 2005-03-01, 2005-03-01‚Ä¶\n$ State      <chr> \"ACT\", \"ACT\", \"ACT\", \"ACT\", \"ACT\", \"ACT\", \"ACT\", \"ACT\", \"NS‚Ä¶\n$ Gender     <chr> \"Female\", \"Female\", \"Female\", \"Female\", \"Male\", \"Male\", \"Ma‚Ä¶\n$ Legal      <chr> \"Remanded\", \"Remanded\", \"Sentenced\", \"Sentenced\", \"Remanded‚Ä¶\n$ Indigenous <chr> \"ATSI\", \"Non-ATSI\", \"ATSI\", \"Non-ATSI\", \"ATSI\", \"Non-ATSI\",‚Ä¶\n$ Count      <dbl> 0, 2, 0, 5, 7, 58, 5, 101, 51, 131, 145, 323, 355, 1617, 12‚Ä¶\n\n\nWe have a Date column for the time index, and we have unique key variables like State, Gender, Legal and Indigenous. Count is the value that is variable over time. It also appears that the data is quarterly, since mosaic::inspect reports the max_diff in the Date column as \\(92\\). (Run mosaic::inspect(prison) in your Console).\n\nCodeprison_tsibble <- prison %>% \n  mutate(quarter = yearquarter(Date)) %>% \n  select(-Date) %>% # Remove the Date column now that we have quarters\n  as_tsibble(index = quarter, key = c(State, Gender, Legal, Indigenous))\n\nprison_tsibble\n\n\n\n  \n\n\n\n(Here, ATSI stands for Aboriginal or Torres Strait Islander.). We have \\(64\\) time series here, organized quarterly.\nLet us examine the key variables:\n\nCodeprison_tsibble %>% distinct(Indigenous)\n\n\n\n  \n\n\nCodeprison_tsibble %>% distinct(State)\n\n\n\n  \n\n\n\nSo we can plot the time series, faceted by / coloured by State:\n\nCodeprison_tsibble %>% \n  tsibble::index_by() %>% \n  group_by(Indigenous, State) %>% \n  #filter(State == \"NSW\") %>% \n  summarise(Total = sum(Count))  %>%\n  ggplot(aes(x = quarter, y = Total, colour = Indigenous, \n             shape = Indigenous)) + \n  geom_point() +\n  geom_line()  + \n  facet_wrap(~ State)\n\n\n\n\nHmm‚Ä¶looks like New South Wales(NSW) as something different going on compared to the rest of the states in Aus."
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/120-time/index.html#decomposing-time-series",
    "href": "content/courses/R-for-Artists/Modules/120-time/index.html#decomposing-time-series",
    "title": "Lab-12: Time is a Him!!",
    "section": "Decomposing Time Series",
    "text": "Decomposing Time Series\nWe can decompose the Weekly_Sales into components representing trends, seasonal events that repeat, and irregular noise. Since each Dept could have a different set of trends, we will do this first for one Dept, say Dept #95:\n\nCodewalmart_decomposed_season <- walmart_tsibble %>% \n  dplyr::filter(Dept == \"95\") %>% # filter for Dept 95\n  #\n  # feasts depends upon fabletools.\n  # \n  fabletools::model(\n    season = STL(Weekly_Sales ~ season(window = \"periodic\"))) \n\nwalmart_decomposed_ets <- walmart_tsibble %>% \n  dplyr::filter(Dept == \"95\") %>% # filter for Dept 95\n  #\n  # feasts depends upon fabletools.\n  # \n  fabletools::model(\n    ets = ETS(box_cox(Weekly_Sales, 0.3)))\n\nwalmart_decomposed_season %>% fabletools::components()\n\n\n\n  \n\n\nCodewalmart_decomposed_ets %>% fabletools::components()\n\n\n\n  \n\n\nCode# walmart_decomposed_arima <- walmart_tsibble %>% \n#   dplyr::filter(Dept == \"95\") %>% # filter for Dept 95\n#     arima = ARIMA(log(Weekly_Sales))\n\n\n\nCodewalmart_decomposed_season %>% \n  components() %>% \n  autoplot() + \n  labs( title = \"Seasonal Variations in Weekly Sales, Dept #95\")\n\n\n\nCodewalmart_decomposed_ets %>% \n  components() %>% \n  autoplot() + \n  labs( title = \"ETS Variations in Weekly Sales, Dept #95\")\n\nWarning: Removed 1 row containing missing values (`geom_line()`)."
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/120-time/index.html#references",
    "href": "content/courses/R-for-Artists/Modules/120-time/index.html#references",
    "title": "Lab-12: Time is a Him!!",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/140-website/index.html#introduction",
    "href": "content/courses/R-for-Artists/Modules/140-website/index.html#introduction",
    "title": "Lab-14: You‚Äôre are Nothing but a Pack of Cards!!",
    "section": "Introduction",
    "text": "Introduction\nLet‚Äôs make a website in RStudio to show off our data viz portfolio, and to share with friends, parents, prospective employers‚Ä¶\nWe will encounter a new package called blogdown and use workflows with github and a free web hosting service called Netlify to create a website where all our RMarkdowns become individual blog posts, complete with Titles, Sections, Text, Diagrams and Links!"
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/140-website/index.html#references",
    "href": "content/courses/R-for-Artists/Modules/140-website/index.html#references",
    "title": "Lab-14: You‚Äôre are Nothing but a Pack of Cards!!",
    "section": "References",
    "text": "References\n\nAllison Hill\nSharon Macliss\nYihui Xie"
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/20-intro/index.html#introduction",
    "href": "content/courses/R-for-Artists/Modules/20-intro/index.html#introduction",
    "title": "Lab-2: Down the R-abbit Hole‚Ä¶",
    "section": "Introduction",
    "text": "Introduction\nWelcome!\nLet‚Äôs start our journey to the Garden of Data Visualization, with this terrific presentation by the great ( and sadly late..) Hans Rosling.\nThe best stats you‚Äôve ever seen by Hans Rosling:\n{{% youtube \"hVimVzgtD6w\" %}}\n\nWe will run some boiler-plate R code today! Nothing ( almost! ) to code! We will get used to the tools and words of the trade: R, RStudio, installation, packages, libraries‚Ä¶.\nlinks: @Installation 00-install_name_plot.Rmd\nSlides 00-Installation.Rmd"
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/30-rmd/index.html#introduction",
    "href": "content/courses/R-for-Artists/Modules/30-rmd/index.html#introduction",
    "title": "Lab-3: Drink Me!",
    "section": "Introduction",
    "text": "Introduction\nWe will get acquainted with the RMarkdown format, which allows us to mix text narrative, code, code-developed figures and items from the web in a seamless document.\nThen we can try to create something in the lines of what Hans Rosling did.\nSomething that can:\n- provide a visualization\n- provide insight\n- tell a story\n- is reproducible"
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/30-rmd/index.html#references",
    "href": "content/courses/R-for-Artists/Modules/30-rmd/index.html#references",
    "title": "Lab-3: Drink Me!",
    "section": "References:",
    "text": "References:\n\nhttps://ysc-rmarkdown.netlify.app/slides/01-basics.html Nice RMarkdown presentation and ‚Äúcode movies‚Äù !\nhttps://rmarkdown.rstudio.com/index.html\nhttps://www.markdowntutorial.com\nhttps://andrewbtran.github.io/NICAR/2018/workflow/docs/02-rmarkdown.html\nhttps://yihui.name/tinytex/ (install!)\nhttps://github.com/rstudio/cheatsheets/blob/master/rmarkdown-2.0.pdf\nhttps://rmarkdown.rstudio.com/html_document_format.html\nhttps://rmarkdown.rstudio.com/pdf_document_format.html"
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/30-rmd/index.html#assignments",
    "href": "content/courses/R-for-Artists/Modules/30-rmd/index.html#assignments",
    "title": "Lab-3: Drink Me!",
    "section": "Assignment(s)",
    "text": "Assignment(s)\n\nComplete the markdowntutorial in [reference 1]\nCreate a fresh RMarkdown document and use as many as possible of the RMarkdown constructs from the Cheatsheet [reference 4]"
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/30-rmd/index.html#optional",
    "href": "content/courses/R-for-Artists/Modules/30-rmd/index.html#optional",
    "title": "Lab-3: Drink Me!",
    "section": "Optional",
    "text": "Optional\n\nGo to reference 3 and install tinytex. Try to knit your document into PDF also, using tinytex."
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/30-rmd/index.html#fun-stuff",
    "href": "content/courses/R-for-Artists/Modules/30-rmd/index.html#fun-stuff",
    "title": "Lab-3: Drink Me!",
    "section": "Fun Stuff",
    "text": "Fun Stuff\n\nDesir√©e De Leon, Alison Hill: rstudio4edu: A Handbook for Teaching and Learning with R and RStudio, https://rstudio4edu.github.io/rstudio4edu-book/"
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/40-working-in-r/index.html#introduction",
    "href": "content/courses/R-for-Artists/Modules/40-working-in-r/index.html#introduction",
    "title": "Lab-4: I say what I mean and I mean what I say",
    "section": "Introduction",
    "text": "Introduction\nWe will get acquainted with data and its representations in R! We will also form a view of how English Grammar ( pronouns, verbs, adjectives, figures of speech..) get metaphorized into the R World!!"
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/40-working-in-r/index.html#readings",
    "href": "content/courses/R-for-Artists/Modules/40-working-in-r/index.html#readings",
    "title": "Lab-4: I say what I mean and I mean what I say",
    "section": "Readings",
    "text": "Readings\n\nR for Data Science, Workflow: Basics Chapter: http://r4ds.had.co.nz/workflow-basics.html\nModern Dive, Getting Started Chapter: http://moderndive.com/2-getting-started.html\nR & RStudio Basics: https://bookdown.org/chesterismay/rbasics/3-rstudiobasics.html\nRStudio IDE Cheatsheet: https://github.com/rstudio/cheatsheets/blob/master/rstudio-ide.pdf"
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/50-working-in-tidyverse/index.html#introduction",
    "href": "content/courses/R-for-Artists/Modules/50-working-in-tidyverse/index.html#introduction",
    "title": "Lab-5: Twas brillig, and the slithy toves‚Ä¶",
    "section": "Introduction",
    "text": "Introduction\nWe meet the most important idea in R: tidy data. Once data is tidy, there is a great deal of insight to be obtained from it, by way of tables, graphs and explorations!\nWe will get hands on with dplyr, the R-package that belongs in the tidyverse and is a terrific toolbox to clean, transform, reorder, and summarize your data. And we will be ready to ask Questions of our data and embark on anlayzing it."
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/50-working-in-tidyverse/index.html#readings",
    "href": "content/courses/R-for-Artists/Modules/50-working-in-tidyverse/index.html#readings",
    "title": "Lab-5: Twas brillig, and the slithy toves‚Ä¶",
    "section": "Readings",
    "text": "Readings\n\nR4DS dplyr chapter\nModernDive dplyr chapter\nRStudio dplyr Cheatsheet"
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/60-GoG/index.html#fa-folder-open-slides-and-tutorials",
    "href": "content/courses/R-for-Artists/Modules/60-GoG/index.html#fa-folder-open-slides-and-tutorials",
    "title": "Lab-6: These Roses have been Painted !!",
    "section": "\n1  Slides and Tutorials",
    "text": "1  Slides and Tutorials"
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/60-GoG/index.html#fa-envelope-introduction",
    "href": "content/courses/R-for-Artists/Modules/60-GoG/index.html#fa-envelope-introduction",
    "title": "Lab-6: These Roses have been Painted !!",
    "section": "\n2  Introduction",
    "text": "2  Introduction\nAh‚Ä¶ggplot ! All those wonderful pictures and graphs, that Alice might have relished!\nMetaphors, aesthetics, geometries‚Ä¶and pictures !! ggplot seems to equate ravens to writing desks in its syntax‚Ä¶and out come graphs!!\nAnd colours: Wes Anderson! Tim Burton! The Economist‚Ä¶ and many others!!"
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/60-GoG/index.html#fa-asterisk-references",
    "href": "content/courses/R-for-Artists/Modules/60-GoG/index.html#fa-asterisk-references",
    "title": "Lab-6: These Roses have been Painted !!",
    "section": "\n3  References",
    "text": "3  References\n\nGeorge Lakoff and Mark Johnson, Metaphors We Live By, https://www.youtube.com/watch?v=lYcQcwUfo8c\nWickham and Grolemund, R for Data Science, ggplot chapter: https://r4ds.had.co.nz/data-visualisation.html\nCMDLineTips, 10 Tips to Customize Text Color, Font, Size in ggplot2 with element_text(), https://cmdlinetips.com/2021/05/tips-to-customize-text-color-font-size-in-ggplot2-with-element_text/\nCMDLineTips, How to write a simple custom ggplot theme from scratch, https://cmdlinetips.com/2022/05/how-to-write-a-simple-custom-ggplot-theme-from-scratch/\nAsha Hill @ mode.com, 12 Extensions to ggplot2 for More Powerful R Visualizations, https://mode.com/blog/r-ggplot-extension-packages/\nEmil Hvitfeldt, ggplot Trial and Error, https://www.emilhvitfeldt.com/post/2018-06-12-ggplot2-trial-and-error-us-trade-data/"
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/60-GoG/index.html#fa-icons-fun-stuff",
    "href": "content/courses/R-for-Artists/Modules/60-GoG/index.html#fa-icons-fun-stuff",
    "title": "Lab-6: These Roses have been Painted !!",
    "section": "\n4 Fun Stuff",
    "text": "4 Fun Stuff\n\nYihan Wu, Mapping ggplot geoms and aesthetic parameters, ( An interactive view of which aesthetic parameters work with which ggplot geom!! ) https://www.yihanwu.ca/post/geoms-and-aesthetic-parameters/\nhttps://www.theartstory.org/artist/kandinsky-wassily/"
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/70-wizardy/index.html#introduction",
    "href": "content/courses/R-for-Artists/Modules/70-wizardy/index.html#introduction",
    "title": "Lab-7: The Lobster Quadrille",
    "section": "Introduction",
    "text": "Introduction\nWe will hear at icing and froth to our vanilla ggplots:\n\n\nfonts, annotations, highlights and even pictures!!"
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/70-wizardy/index.html#references",
    "href": "content/courses/R-for-Artists/Modules/70-wizardy/index.html#references",
    "title": "Lab-7: The Lobster Quadrille",
    "section": "References",
    "text": "References\n\nThomas Lin Pedersen, https://www.data-imaginist.com/. The creator of ggforce, and patchwork packages.\nClaus Wilke, cowplot ‚Äì Streamlined plot theme and plot annotations for ggplot2, https://wilkelab.org/cowplot/index.html\nClaus Wilke, Spruce up your ggplot2 visualizations with formatted text, https://clauswilke.com/talk/rstudio_conf_2020/. Slides, Code, and Video !\nRobert Kabacoff, ggplot theme cheatsheet, https://rkabacoff.github.io/datavis/modifyingthemes.pdf"
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/70-wizardy/index.html#fun-stuff",
    "href": "content/courses/R-for-Artists/Modules/70-wizardy/index.html#fun-stuff",
    "title": "Lab-7: The Lobster Quadrille",
    "section": "Fun Stuff",
    "text": "Fun Stuff\n\nZuguang Gu, Circular Visualization in R,\n\n\n\n\n\n\n\n\n\nhttps://jokergoo.github.io/circlize_book/book/"
  },
  {
    "objectID": "content/courses/R-for-Artists/Modules/90-GoM/index.html#introduction",
    "href": "content/courses/R-for-Artists/Modules/90-GoM/index.html#introduction",
    "title": "Lab-9: If you please sir‚Ä¶which way to the Secret Garden?",
    "section": "Introduction",
    "text": "Introduction\nWhat is there to not like about maps!!!\nWe will learn to make static and interactive maps and to show off different kinds of data on them, data that have an inherently ‚Äúspatial‚Äù spread or significance!\nTrade Routes? Populations? Street Crime hotspots? Theatre and Food districts and popular Restaurants? Literary Paris, London and Barcelona?\nAll possible !!"
  },
  {
    "objectID": "content/labs/doe/index.html",
    "href": "content/labs/doe/index.html",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "",
    "text": "This project is a quick analysis of the Design of Experiments class carried out in the Order and Chaos course, FSP-2021-2022, at SMI MAHE, Bangalore.\nThe methodology followed was that in A.J. Lawrance‚Äôs paper 1 describing a Statistics module based on the method of Design of Experiments. The inquiry relates to Short Term Memory (STM) among students."
  },
  {
    "objectID": "content/labs/doe/index.html#structure",
    "href": "content/labs/doe/index.html#structure",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "Structure",
    "text": "Structure\nThe total number of students were 17. Eight Pairs of students were created randomly to create eight different Test tools for Short Term Memory testing.\nThe binary ( two - level ) variables/parameters that were used in the tests, were, following Lawrance:\n\nWL: Word List Length ( 7 and 15 words )\n\nSL: Syllables in the Words ( 2 and 5 syllables )\n\nST: Study Time allowed for the Respondents ( 15 and 30 seconds )\n\nOther parameters considered were a) Language b) Structure/Depiction of the Word Lists ( e.g.¬†word clouds, matrices, columns‚Ä¶), c) Whether the words would be shown or read aloud, and d) whether the respondents had to speak out, or write down, the recollected words. These parameters were discussed and abandoned as too complex to mechanize, though they could have had an impact on the STM scores.\nHence a total of 8 Tests were created by 8 pairs of students, and each team tested the remaining 15 students ( Due to COVID restrictions, this testing was carried out entirely online on MS Teams, using individual breakout rooms for the Test Teams. )\nThe data were entered into a Google Sheet and the STM scores were converted to percentages so as to be comparable across WL.\nThe data was then ‚Äúflattened‚Äù for each of the binary parameters; this was logical to do since for each parameter, the other two parameters were balanced out by the Test structure. For instance, for WL = 5, the SL and ST parameters used all the four combinations ( SL = 5, 15 ) and (ST = 15, 30 ). Hence the ‚Äúcommon sense‚Äù analysis could proceed for each of the parameters individually. Joint effects were not considered for this preliminary class."
  },
  {
    "objectID": "content/labs/doe/index.html#data",
    "href": "content/labs/doe/index.html#data",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "Data",
    "text": "Data\n\n\n\n\n  \n\n\n\nThe data has scores that have been combined into single columns for each setting for each of the parameters. For example, the column syllable_2 contains STM scores for all tests that used SL = 2-syllables in their tests. The Word Length WL and Study Time ST go through all their combinations in this column. The other columns are constructed similarly.\nBasic Plots\nWe will use Box Plots and Density Plots to compare the STM score distributions for each Parameter. To do this we need to pivot_longer the adjacent columns ( e.g.¬†syllable_2 and syllable_5) and use these names as categorical variables:\nSyllable Parameter SL\n\n\n\n\n  \n\n\n\n\n\n\nStudy Time Parameter ST\n\n\n\n\n  \n\n\n\n\n\n\nWord List Length Parameter WL"
  },
  {
    "objectID": "content/labs/doe/index.html#preliminary-observations",
    "href": "content/labs/doe/index.html#preliminary-observations",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "Preliminary Observations",
    "text": "Preliminary Observations\nClearly, based on visual inspection of the Plots, the Word Count seems to have a large effect on STM Test Scores, with fewer words ( 7 ) being easier to recall. Study Time ( 15 and 30 seconds ) also seems to have a more modest positive effect on STM scores, while Syllable Count ( 2 or 5 syllables ) seems to have a modest negative effect on STM scores."
  },
  {
    "objectID": "content/labs/doe/index.html#analysis",
    "href": "content/labs/doe/index.html#analysis",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "Analysis",
    "text": "Analysis\nWe wish to establish the significance of the effect size due to each of the Parameters. Already from the Density Plots, we can see that none of the scores are normally distributed. A quick Shapiro-Wilkes Test for each of them confirms that the scores are not normally distributed.\nHence we go for a Permutation Test to check for significance of effect.\nOn the other hand, as remarked in Ernst2, the non-parametric permutation test can be both exact and also intuitively easier for students to grasp, as I can testify from direct observation in this class. There is no need to discuss sampling distributions and means, t-tests and the like. Permutations are easily executed in R, using packages such as mosaic3.\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  stm$syllable_2\nW = 0.95508, p-value = 0.02716\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  stm$syllable_5\nW = 0.95321, p-value = 0.02211\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  stm$study_time_15\nW = 0.9068, p-value = 0.0002348\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  stm$study_time_30\nW = 0.95539, p-value = 0.0281\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  stm$list_length_7\nW = 0.90542, p-value = 0.0002085\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  stm$list_length_15\nW = 0.92806, p-value = 0.001645"
  },
  {
    "objectID": "content/labs/doe/index.html#permutation-tests",
    "href": "content/labs/doe/index.html#permutation-tests",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "Permutation Tests",
    "text": "Permutation Tests\nWe proceed with a Permutation Test for each of the Parameters. We start with the Syllable Parameter SL. We shuffle the labels ( SL- = 2 and SL+ = 5) between the scores and determine the null distribution. This is then compared with the difference in mean scores between the unpermuted sets. We continue similarly for the other two parameters.\n\n\n[1] 0.0153731\n\n\n\n\n  \n\n\n\n[1] 0.08526183\n\n\n\n\n  \n\n\n\n[1] 0.2887539"
  },
  {
    "objectID": "content/labs/doe/index.html#conclusions",
    "href": "content/labs/doe/index.html#conclusions",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "Conclusions",
    "text": "Conclusions\nFrom the above null distribution plots obtained using Permutation tests, it is clear that both Study Time ( ST ) and List Word Length ( WL) have significant effects on the Short Term Memory Scores. The probability that the observed value is obtained or exceeded by any permutation of scores is very low in both cases.\nOn the other hand, Syllable Count (SL) does not seem to affect the STM scores significantly."
  },
  {
    "objectID": "content/labs/doe/index.html#references",
    "href": "content/labs/doe/index.html#references",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "content/labs/r-labs/diagrams/index.html",
    "href": "content/labs/r-labs/diagrams/index.html",
    "title": "Lab 01 - Introduce Yourself",
    "section": "",
    "text": "At the end of this Lab, we will:\n\nhave installed R and RStudio on our machines\nunderstood how to add additional R-packages for specific features and graphic capability\nrun code within RStudio and interpret the results\nhave learnt to look for help within R and RStudio"
  },
  {
    "objectID": "content/labs/r-labs/diagrams/index.html#Check-In-R",
    "href": "content/labs/r-labs/diagrams/index.html#Check-In-R",
    "title": "Lab 01 - Introduce Yourself",
    "section": "\n3.1 Check in",
    "text": "3.1 Check in\nLaunch R by clicking this logo. You should see one console with a command line interpreter. Try typing 2 + 2 and check !\nClose R."
  },
  {
    "objectID": "content/labs/r-labs/diagrams/index.html#Check-In-RStudio",
    "href": "content/labs/r-labs/diagrams/index.html#Check-In-RStudio",
    "title": "Lab 01 - Introduce Yourself",
    "section": "\n4.1 Check in",
    "text": "4.1 Check in\nLaunch RStudio. You should get a window similar to the screenshot you see here, but yours will be empty. Look at the bottom left pane: this is the same console window you saw when you opened R in step @Check-In-R\n\nPlace your cursor where you see > and type x <- 2 + 2 again hit enter or return, then type x, and hit enter/return again.\nIf [1] 4 prints to the screen, you have successfully installed R and RStudio, and you can move onto installing packages."
  },
  {
    "objectID": "content/labs/r-labs/diagrams/index.html#save-and-share",
    "href": "content/labs/r-labs/diagrams/index.html#save-and-share",
    "title": "Lab 01 - Introduce Yourself",
    "section": "\n7.1 Save and share",
    "text": "7.1 Save and share\nSave your work so you can share your favorite plot with us. You will not like the looks of your plot if you mouse over to Export and save it. Instead, use ggplot2‚Äôs command for saving a plot with sensible defaults:\n\nShow the Codehelp(ggsave)\n\n\n\nShow the Codeggsave(\"file_name_here.pdf\", plot) # please make the filename unique!\n\n\nUpload this exported plot to Teams -> Assignments.\nBefore you do that, check that you can submit stuff/assignments on the MLS by uploading a cat picture üêà first, especially if you are an ailurophile like me. (Acceptable Breeds: Scottish Folds or Maine Coons)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html",
    "href": "content/labs/r-labs/graphics/index.html",
    "title": "Lab 04 - The Grammar of Graphics",
    "section": "",
    "text": "This RMarkdown document is part of my course on R for Artists and Designers. The material is based on A Layered Grammar of Graphics by Hadley Wickham. The intent of this Course is to build Skill in coding in R, and also appreciate R as a way to metaphorically visualize information of various kinds, using predominantly geometric figures and structures.\nAll RMarkdown files combine code, text, web-images, and figures developed using code. Everything is text; code chunks are enclosed in fences (```)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#goals",
    "href": "content/labs/r-labs/graphics/index.html#goals",
    "title": "Lab 04 - The Grammar of Graphics",
    "section": "\n2 Goals",
    "text": "2 Goals\nAt the end of this Lab session, we should: - know the types and structures of tidy data and be able to work with them - be able to create data visualizations using ggplot - Understand aesthetics and scales in `ggplot"
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#pedagogical-note",
    "href": "content/labs/r-labs/graphics/index.html#pedagogical-note",
    "title": "Lab 04 - The Grammar of Graphics",
    "section": "\n3 Pedagogical Note",
    "text": "3 Pedagogical Note\nThe method followed will be based on PRIMM:\n\n\nPREDICT Inspect the code and guess at what the code might do, write predictions\n\n\nRUN the code provided and check what happens\n\nINFER what the parameters of the code do and write comments to explain. What bells and whistles can you see?\n\nMODIFY the parameters code provided to understand the options available. Write comments to show what you have aimed for and achieved.\n\nMAKE : take an idea/concept of your own, and graph it."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#set-up",
    "href": "content/labs/r-labs/graphics/index.html#set-up",
    "title": "Lab 04 - The Grammar of Graphics",
    "section": "\n4 Set Up",
    "text": "4 Set Up\nThe setup code chunk below brings into our coding session R packages that provide specific computational abilities and also datasets which we can use.\nTo reiterate: Packages and datasets are not the same thing !! Packages are (small) collections of programs. Datasets are just‚Ä¶.information."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#a-teaser-from-john-snow",
    "href": "content/labs/r-labs/graphics/index.html#a-teaser-from-john-snow",
    "title": "Lab 04 - The Grammar of Graphics",
    "section": "\n5 A Teaser from John Snow",
    "text": "5 A Teaser from John Snow"
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#review-of-tidy-data",
    "href": "content/labs/r-labs/graphics/index.html#review-of-tidy-data",
    "title": "Lab 04 - The Grammar of Graphics",
    "section": "\n6 Review of Tidy Data",
    "text": "6 Review of Tidy Data\n‚ÄúTidy Data‚Äù is an important way of thinking about what data typically look like in R. Let‚Äôs fetch a figure from the web to show the (preferred) structure of data in R. (The syntax to bring in a web-figure is ![caption](url))\n The three features described in the figure above define the nature of tidy data:\n\n\nVariables in Columns\n\n\nObservations in Rows and\n\n\nMeasurements in Cells.\n\nData are imagined to be resulting from an experiment. Each variable represents a parameter/aspect in the experiment. Each row represents an additional datum of measurement. A cell is a single measurement on a single parameter(column) in a single observation(row)."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#kinds-of-variables",
    "href": "content/labs/r-labs/graphics/index.html#kinds-of-variables",
    "title": "Lab 04 - The Grammar of Graphics",
    "section": "\n7 Kinds of Variables",
    "text": "7 Kinds of Variables\nKinds of Variable are defined by the kind of questions they answer to:\n\nWhat/Who/Where? -> Some kind of Name. Categorical variable\nWhat Kind? How? -> Some kind of ‚ÄúType‚Äù. Factor variable\nHow Many? How large? -> Some kind of Quantity. Numerical variable. Most Figures in R are computed with variables, and therefore, with columns."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#interrogations-and-graphs",
    "href": "content/labs/r-labs/graphics/index.html#interrogations-and-graphs",
    "title": "Lab 04 - The Grammar of Graphics",
    "section": "\n8 Interrogations and Graphs",
    "text": "8 Interrogations and Graphs\nCreating graphs from data is an act of asking questions and viewing answers in a geometric way. Let us write some simple English descriptions of measures and visuals and see what commands they use in R.\n\n8.1 Components of the layered grammar of graphics\nLayers are used to create the objects on a plot. They are defined by five basic parts:\n\nData (What dataset/spreadsheet am I using?)\nMapping (What does each column do in my graph?)\nStatistical transformation (stat) (Do I have count something first?)\nGeometric object (geom) (What shape, colour, size‚Ä¶do I want?)\nPosition adjustment (position) (Where do I want it on the graph?)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#data",
    "href": "content/labs/r-labs/graphics/index.html#data",
    "title": "Lab 04 - The Grammar of Graphics",
    "section": "\n9 Data",
    "text": "9 Data\nWe will use ‚Äúreal world‚Äù data. Let‚Äôs use the penguins dataset in the palmerpenguins package. Run ?penguins in the console to get more information about this dataset.\n\n9.1 Head\n\n\n\n\n  \n\n\n\n\n9.2 Tail\n\n\n\n\n  \n\n\n\n\n9.3 Dim\n\n\n[1] 344   8\n\n\nSo we know what our data looks like. We pass this data to ggplot use to plot as follows: in R this creates an empty graph sheet!! Because we have not (yet) declared the geometric shapes we want to use to plot our information."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#mapping",
    "href": "content/labs/r-labs/graphics/index.html#mapping",
    "title": "Lab 04 - The Grammar of Graphics",
    "section": "\n10 Mapping",
    "text": "10 Mapping\nNow that we have told R what data to use, we need to state what variables to plot and how.\nAesthetic Mapping defines how the variables are applied to the plot, i.e.¬†we take a variable from the data and ‚Äúmetaphorize‚Äù it into a geometric feature. We can map variables metaphorically to a variety of geometric things: coordinate, length, height, size, shape, colour, alpha(how dark?)‚Ä¶.\nThe syntax uses: aes(some_geometric_thing = some_variable)\nRemember variable = column.\nSo if we were graphing information from penguins, we might map a penguin‚Äôs flipper_length_mm column to the \\(x\\) position, and the body_mass_g column to the \\(y\\) position.\n\n10.1 Mapping Example-1\nWe can try another example of aesthetic mapping with the same dataset:\n\n10.1.1 Plot-1a\n\n\n\n\n\n\n10.1.2 Plot-1b\n\n10.1.3 Plot-1c\n\n10.2 Mapping Example-2\nWe can try another example of aesthetic mapping with the same dataset:\n\n10.2.1 Plot-2a\n\n\n\n\n\n\n10.2.2 Plot-2b\n\n10.2.3 Plot-2c\n\n10.3 Mapping Example-3\nWe can try another example of aesthetic mapping with the same dataset:\n\n10.3.1 Plot-3a\n\n\n\n\n\n\n10.3.2 Plot-3b\n\n10.3.3 Plot-3c\n\n10.4 Mapping Example-4\nWe can try another example of aesthetic mapping with the same dataset:\n\n10.4.1 Plot-4a\n\n\n\n\n\n\n10.4.2 Plot-4b\n\n10.4.3 Plot-4c"
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#geometric-objects",
    "href": "content/labs/r-labs/graphics/index.html#geometric-objects",
    "title": "Lab 04 - The Grammar of Graphics",
    "section": "\n11 Geometric objects",
    "text": "11 Geometric objects\nGeometric objects (geoms) control the type of plot you create. Geoms are classified by their dimensionality:\n\n0 dimensions - point, text\n1 dimension - path, line\n2 dimensions - polygon, interval\n\nEach geom can only display certain aesthetics or visual attributes of the geom. For example, a point geom has position, color, shape, and size aesthetics.\nWe can also stack up geoms on top of one another to add layers to the graph.\n\n11.1 Plot1\n\n\nWarning: Removed 2 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\n11.2 Plot2\n\n\nWarning: Removed 2 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\n11.3 Plot3\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPosition determines the starting location (origin) of each bar\nHeight determines how tall to draw the bar. Here the height is based on the number of observations in the dataset for each possible number of cylinders."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#position-adjustment",
    "href": "content/labs/r-labs/graphics/index.html#position-adjustment",
    "title": "Lab 04 - The Grammar of Graphics",
    "section": "\n12 Position adjustment",
    "text": "12 Position adjustment\nSometimes with dense data we need to adjust the position of elements on the plot, otherwise data points might obscure one another. Bar plots frequently stack or dodge the bars to avoid overlap:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSometimes scatterplots with few unique \\(x\\) and \\(y\\) values are jittered (random noise is added) to reduce overplotting.\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#statistical-transformation",
    "href": "content/labs/r-labs/graphics/index.html#statistical-transformation",
    "title": "Lab 04 - The Grammar of Graphics",
    "section": "\n13 Statistical transformation",
    "text": "13 Statistical transformation\nA statistical transformation (stat) pre-transforms the data, before plotting. For instance, in a bar graph you might summarize the data by counting the total number of observations within a set of categories, and then plotting the count.\n\n13.1 Count\n\n\n\n\n  \n\n\n\n\n13.2 Count and Bar Graph\n\n\n\n\n\n\n13.3 Tidy Count and Bar Graph\n\n\n\n\n\n\n13.4 Count inside the Plot\n\n\n\n\n\nSometimes you don‚Äôt need to make a statistical transformation. For example, in a scatterplot you use the raw values for the \\(x\\) and \\(y\\) variables to map onto the graph. In these situations, the statistical transformation is an identity transformation - the stat simply passes in the original dataset and exports the exact same dataset."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#scale",
    "href": "content/labs/r-labs/graphics/index.html#scale",
    "title": "Lab 04 - The Grammar of Graphics",
    "section": "\n14 Scale",
    "text": "14 Scale\nA scale controls how data is mapped to aesthetic attributes, so we need one scale for every aesthetic property employed in a layer. For example, this graph defines a scale for color:\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nThe scale can be changed to use a different color palette:\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nNow we are using a different palette, but the scale is still consistent: all Adelie penguins utilize the same color, whereas Chinstrap use a new color but each Adelie still uses the same, consistent color."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#coordinate-system",
    "href": "content/labs/r-labs/graphics/index.html#coordinate-system",
    "title": "Lab 04 - The Grammar of Graphics",
    "section": "\n15 Coordinate system",
    "text": "15 Coordinate system\nA coordinate system (coord) maps the position of objects onto the plane of the plot, and controls how the axes and grid lines are drawn. Plots typically use two coordinates (\\(x, y\\)), but could use any number of coordinates. Most plots are drawn using the Cartesian coordinate system:\n\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nThis system requires a fixed and equal spacing between values on the axes. That is, the graph draws the same distance between 1 and 2 as it does between 5 and 6. The graph could be drawn using a semi-log coordinate system which logarithmically compresses the distance on an axis:\n\n\n\n\n\nOr could even be drawn using polar coordinates:"
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#faceting",
    "href": "content/labs/r-labs/graphics/index.html#faceting",
    "title": "Lab 04 - The Grammar of Graphics",
    "section": "\n16 Faceting",
    "text": "16 Faceting\nFaceting can be used to split the data up into subsets of the entire dataset. This is a powerful tool when investigating whether patterns are the same or different across conditions, and allows the subsets to be visualized on the same plot (known as conditioned or trellis plots). The faceting specification describes which variables should be used to split up the data, and how they should be arranged.\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#defaults",
    "href": "content/labs/r-labs/graphics/index.html#defaults",
    "title": "Lab 04 - The Grammar of Graphics",
    "section": "\n17 Defaults",
    "text": "17 Defaults\nRather than explicitly declaring each component of a layered graphic (which will use more code and introduces opportunities for errors), we can establish intelligent defaults for specific geoms and scales. For instance, whenever we want to use a bar geom, we can default to using a stat that counts the number of observations in each group of our variable in the \\(x\\) position.\nConsider the following scenario: you wish to generate a scatterplot visualizing the relationship between penguins‚Äô bill_length and their body_mass. With no defaults, the code to generate this graph is:\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nThe above code:\n\nCreates a new plot object (ggplot)\n\nAdds a layer (layer)\n\nSpecifies the data (penguins)\nMaps engine bill length to the \\(x\\) position and body mass to the \\(y\\) position (mapping)\nUses the point geometric transformation (geom = \"point\")\nImplements an identity transformation and position (stat = \"identity\" and position = \"identity\")\n\n\nEstablishes two continuous position scales (scale_x_continuous and scale_y_continuous)\nDeclares a cartesian coordinate system (coord_cartesian)\n\nHow can we simplify this using intelligent defaults?\n\nWe only need to specify one geom and stat, since each geom has a default stat.\nCartesian coordinate systems are most commonly used, so it should be the default.\n\nDefault scales can be added based on the aesthetic and type of variables.\n\nContinuous values are transformed with a linear scaling.\nDiscrete values are mapped to integers.\nScales for aesthetics such as color, fill, and size can also be intelligently defaulted.\n\n\n\nUsing these defaults, we can rewrite the above code as:\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nThis generates the exact same plot, but uses fewer lines of code. Because multiple layers can use the same components (data, mapping, etc.), we can also specify that information in the ggplot() function rather than in the layer() function:\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nAnd as we will learn, function arguments in R use specific ordering, so we can omit the explicit call to data and mapping:\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "content/labs/r-labs/installation/index.html",
    "href": "content/labs/r-labs/installation/index.html",
    "title": "Lab 01 - Introduce Yourself",
    "section": "",
    "text": "At the end of this Lab, we will:\n\nhave installed R and RStudio on our machines\nunderstood how to add additional R-packages for specific features and graphic capability\nrun code within RStudio and interpret the results\nhave learnt to look for help within R and RStudio"
  },
  {
    "objectID": "content/labs/r-labs/installation/index.html#Check-In-R",
    "href": "content/labs/r-labs/installation/index.html#Check-In-R",
    "title": "Lab 01 - Introduce Yourself",
    "section": "\n3.1 Check in",
    "text": "3.1 Check in\nLaunch R by clicking this logo. You should see one console with a command line interpreter. Try typing 2 + 2 and check !\nClose R."
  },
  {
    "objectID": "content/labs/r-labs/installation/index.html#Check-In-RStudio",
    "href": "content/labs/r-labs/installation/index.html#Check-In-RStudio",
    "title": "Lab 01 - Introduce Yourself",
    "section": "\n4.1 Check in",
    "text": "4.1 Check in\nLaunch RStudio. You should get a window similar to the screenshot you see here, but yours will be empty. Look at the bottom left pane: this is the same console window you saw when you opened R in step @Check-In-R\n\nPlace your cursor where you see > and type x <- 2 + 2 again hit enter or return, then type x, and hit enter/return again.\nIf [1] 4 prints to the screen, you have successfully installed R and RStudio, and you can move onto installing packages."
  },
  {
    "objectID": "content/labs/r-labs/installation/index.html#save-and-share",
    "href": "content/labs/r-labs/installation/index.html#save-and-share",
    "title": "Lab 01 - Introduce Yourself",
    "section": "\n7.1 Save and share",
    "text": "7.1 Save and share\nSave your work so you can share your favorite plot with us. You will not like the looks of your plot if you mouse over to Export and save it. Instead, use ggplot2‚Äôs command for saving a plot with sensible defaults:\n\nShow the Codehelp(ggsave)\n\n\n\nShow the Codeggsave(\"file_name_here.pdf\", plot) # please make the filename unique!\n\n\nUpload this exported plot to Teams -> Assignments.\nBefore you do that, check that you can submit stuff/assignments on the MLS by uploading a cat picture üêà first, especially if you are an ailurophile like me. (Acceptable Breeds: Scottish Folds or Maine Coons)"
  },
  {
    "objectID": "content/labs/r-labs/maps/index.html",
    "href": "content/labs/r-labs/maps/index.html",
    "title": "Lab 01 - Introduce Yourself",
    "section": "",
    "text": "At the end of this Lab, we will:\n\nhave installed R and RStudio on our machines\nunderstood how to add additional R-packages for specific features and graphic capability\nrun code within RStudio and interpret the results\nhave learnt to look for help within R and RStudio"
  },
  {
    "objectID": "content/labs/r-labs/maps/index.html#Check-In-R",
    "href": "content/labs/r-labs/maps/index.html#Check-In-R",
    "title": "Lab 01 - Introduce Yourself",
    "section": "\n3.1 Check in",
    "text": "3.1 Check in\nLaunch R by clicking this logo. You should see one console with a command line interpreter. Try typing 2 + 2 and check !\nClose R."
  },
  {
    "objectID": "content/labs/r-labs/maps/index.html#Check-In-RStudio",
    "href": "content/labs/r-labs/maps/index.html#Check-In-RStudio",
    "title": "Lab 01 - Introduce Yourself",
    "section": "\n4.1 Check in",
    "text": "4.1 Check in\nLaunch RStudio. You should get a window similar to the screenshot you see here, but yours will be empty. Look at the bottom left pane: this is the same console window you saw when you opened R in step @Check-In-R\n\nPlace your cursor where you see > and type x <- 2 + 2 again hit enter or return, then type x, and hit enter/return again.\nIf [1] 4 prints to the screen, you have successfully installed R and RStudio, and you can move onto installing packages."
  },
  {
    "objectID": "content/labs/r-labs/maps/index.html#save-and-share",
    "href": "content/labs/r-labs/maps/index.html#save-and-share",
    "title": "Lab 01 - Introduce Yourself",
    "section": "\n7.1 Save and share",
    "text": "7.1 Save and share\nSave your work so you can share your favorite plot with us. You will not like the looks of your plot if you mouse over to Export and save it. Instead, use ggplot2‚Äôs command for saving a plot with sensible defaults:\n\nShow the Codehelp(ggsave)\n\n\n\nShow the Codeggsave(\"file_name_here.pdf\", plot) # please make the filename unique!\n\n\nUpload this exported plot to Teams -> Assignments.\nBefore you do that, check that you can submit stuff/assignments on the MLS by uploading a cat picture üêà first, especially if you are an ailurophile like me. (Acceptable Breeds: Scottish Folds or Maine Coons)"
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html",
    "href": "content/labs/r-labs/networks/index.html",
    "title": "Lab 07 - The Grammar of Networks",
    "section": "",
    "text": "This RMarkdown document is part of my course on R for Artists and Designers. The material is based on A Layered Grammar of Graphics by Hadley Wickham. The intent of this Course is to build Skill in coding in R, and also appreciate R as a way to metaphorically visualize information of various kinds, using predominantly geometric figures and structures.\nAll RMarkdown files combine code, text, web-images, and figures developed using code. Everything is text; code chunks are enclosed in fences (```)\n\nAt the end of this Lab session, we should:\n\nknow the types and structures of network data and be able to work with them\nunderstand the basics of modern network packages in R\nbe able to create network visualizations using tidygraph, ggraph( static visualizations ) and visNetwork (interactive visualizations)\nsee directions for how the network metaphor applies in a variety of domains (e.g.¬†biology/ecology, ideas/influence, technology, transportation, to name a few)\n\nThe method followed will be based on PRIMM:\n\n\nPREDICT Inspect the code and guess at what the code might do, write predictions\n\n\nRUN the code provided and check what happens\n\nINFER what the parameters of the code do and write comments to explain. What bells and whistles can you see?\n\nMODIFY the parameters code provided to understand the options available. Write comments to show what you have aimed for and achieved.\n\nMAKE : take an idea/concept of your own, and graph it.\n\nThe setup code chunk below brings into our coding session R packages that provide specific computational abilities and also datasets which we can use.\nTo reiterate: Packages and datasets are not the same thing !! Packages are (small) collections of programs. Datasets are just‚Ä¶.information.\n\nNetwork graphs are characterized by two key terms: nodes and edges\n\n\nNodes : Entities\n\nMetaphors: Individual People? Things? Ideas? Places? to be connected in the network.\nSynonyms: vertices. Nodes have IDs.\n\n\n\nEdges: Connections\n\nMetaphors: Interactions? Relationships? Influence? Letters sent and received? Dependence? between the entities.\nSynonyms: links, ties.\n\n\n\nIn R, we create network representations using node and edge information. One way in which these could be organized are:\n- Node list: a data frame with a single column listing the node IDs found in the edge list. You can also add attribute columns to the data frame such as the names of the nodes or grouping variables. ( Type? Class? Family? Country? Subject? Race? )\n\nNode Table\n\n\n\n\n\n\n\nID\nNode Name\nAttribute? Qualities?Categories? Family? Country?Planet?\n\n\n1\nNed\nNursery School Teacher\n\n\n2\nJaguar Paw\nMain Character, Apocalypto\n\n\n3\nJohn Snow\nEpidemiologist\n\n\n\n\n\nEdge list: data frame containing two columns: source node and destination node of an edge. Source and Destination have node IDs.\n\nWeighted network graph: An edge list can also contain additional columns describing attributes of the edges such as a magnitude aspect for an edge. If the edges have a magnitude attribute the graph is considered weighted.\n\n\nEdges Table\n\nFrom\nTo\nRelationship\nWeightage\n\n\n\n1\n3\nFinancial Dealings\n6\n\n\n2\n1\nHistory Lessons\n2\n\n\n2\n3\nVaccination\n15\n\n\n\n\n\nLayout: A geometric arrangement of nodes and edges.\n\nMetaphors: Location? Spacing? Distance? Coordinates? Colour? Shape? Size? Provides visual insight due to the arrangement.\n\n\n\n\nLayout Algorithms : Method to arranges nodes and edges with the aim of optimizing some metric .\n\nMetaphors: Nodes are masses and edges are springs. The Layout algorithm minimizes the stretching and compressing of all springs.(BTW, are the Spring Constants K the same for all springs?‚Ä¶)\n\n\n\n\n\nDirected and undirected network graph: If the distinction between source and target is meaningful, the network is directed. If the distinction is not meaningful, the network is undirected. Directed edges represent an ordering of nodes, like a relationship extending from one node to another, where switching the direction would change the structure of the network. Undirected edges are simply links between nodes where order does not matter.\n\nExamples:\n\nThe World Wide Web is an example of a directed network because hyperlinks connect one Web page to another, but not necessarily the other way around.\nCo-authorship networks represent examples of un-directed networks, where nodes are authors and they are connected by an edge if they have written a publication together\nWhen people send e-mail to each other, the distinction between the sender (source) and the recipient (target) is clearly meaningful, therefore the network is directed.\n\n\n\nConnected and Disconnected graphs: If there is some path from any node to any other node, the Networks is said to be Connected. Else, Disconnected.\n\n\ntidygraph and ggraph are modern R packages for network data. Graph Data setup and manipulation is done in tidygraph and graph visualization with ggraph.\n\n\ntidygraph Data -> ‚ÄúNetwork Object‚Äù in R.\n\nggraph Network Object -> Plots using a chosen layout/algo.\n\nBoth leverage the power of igraph, which is the Big Daddy of all network packages. We will be using the Grey‚Äôs Anatomy dataset in our first foray into networks.\n\n\nShow the Codegrey_nodes <- read_csv(\"../../../materials/data/networks/grey_nodes.csv\")\ngrey_edges <- read_csv(\"../../../materials/data/networks/grey_edges.csv\")\n\n\n# grey_nodes <- read_delim(\"./Data/greys-nodes.csv\", delim = \";\")\n# ger_edges <- read_delim(\"~/Downloads/grey-edges.csv\", \n#     delim = \";\", escape_double = FALSE, trim_ws = TRUE)\ngrey_nodes\n\n\n\n  \n\n\nShow the Codegrey_edges\n\n\n\n  \n\n\n\n\n\n\n Download Grey Nodes as csv\n\n\n\n\n Download Grey Edges data as csv\n\n\n\n\n\nQuestions and Inferences #1:\n\n\nLook at the console output thumbnail. What does for example name = col_character mean? What attributes (i.e.¬†extra information) are seen for Nodes and Edges? Understand the data in both nodes and edges as shown in the second and third thumbnails. Write some comments and inferences here.\n\n\n\nKey function:\n\n\ntbl_graph(): (aka ‚Äútibble graph‚Äù). Key arguments: nodes, edges and directed. Note this is a very versatile command and can take many input forms, such as data structures that result from other packages. Type ?tbl_graph in the Console and see the Usage section.\n\n\nShow the Codega <- tbl_graph(nodes = grey_nodes, \n                edges = grey_edges, \n                directed = FALSE)\nga\n\n# A tbl_graph: 54 nodes and 57 edges\n#\n# An undirected simple graph with 4 components\n#\n# Node Data: 54 √ó 7 (active)\n  name               sex   race  birthyear position  season sign  \n  <chr>              <chr> <chr>     <dbl> <chr>      <dbl> <chr> \n1 Addison Montgomery F     White      1967 Attending      1 Libra \n2 Adele Webber       F     Black      1949 Non-Staff      2 Leo   \n3 Teddy Altman       F     White      1969 Attending      6 Pisces\n4 Amelia Shepherd    F     White      1981 Attending      7 Libra \n5 Arizona Robbins    F     White      1976 Attending      5 Leo   \n6 Rebecca Pope       F     White      1975 Non-Staff      3 Gemini\n# ‚Ä¶ with 48 more rows\n#\n# Edge Data: 57 √ó 4\n   from    to weight type    \n  <int> <int>  <dbl> <chr>   \n1     5    47      2 friends \n2    21    47      4 benefits\n3     5    46      1 friends \n# ‚Ä¶ with 54 more rows\n\n\n\n\nQuestions and Inferences #2:\n\n\nQuestions and Inferences: What information does the graph object contain? What attributes do the nodes have? What about the edges?\n\n\n\n3a. Quick Plot: autograph() This is to check quickly is the data is imported properly and to decide upon going on to a more elaborate plotting.\n\nShow the Codeautograph(ga)\n\n\n\n\n\n\nQuestions and Inferences #3:\n\n\nQuestions and Inferences: Describe this graph, in simple words here. Try to use some of the new domain words we have just acquired: nodes/edges, connected/disconnected, directed/undirected.\n\n\n3b. More elaborate plot\nKey functions:\n\n\nggraph(layout = \"......\"): Create classic node-edge diagrams; i.e. Sets up the graph. Rather like ggplot for networks!\n\nTwo kinds of geom: one set for nodes, and another for edges\n\ngeom_node_point(aes(.....)): Draws node as ‚Äúpoints‚Äù. Alternatives are circle / arc_bar / tile / voronoi. Remember the geoms that we have seen before in Grammar of Graphics!\ngeom_edge_link(aes(.....)): Draws edges as ‚Äúlinks‚Äù. Alternatives are arc / bend / elbow / hive / loop / parallel / diagonal / point / span /tile.\ngeom_node_text(aes(label = ......), repel = TRUE): Adds text labels (non-overlapping). Alternatives are label /...\nlabs(title = \"....\", subtitle = \"....\", caption = \"....\"): Change main titles, axis labels and legend titles. We know this from our work with ggplot.\n\n\nShow the Code# Write Comments next to each line \n# About what that line does for the overall graph\n\nggraph(graph = ga,  layout = \"kk\") +\n  #\n  \n  geom_edge_link(width = 2, color = \"pink\") +\n  #\n  \n  geom_node_point(\n    shape = 21,\n    size = 8,\n    fill = \"blue\",\n    color = \"green\",\n    stroke = 2\n  ) +\n  #\n  \n  labs(title = \"Whoo Hoo! My first silly Grey's Anatomy graph in R!\",\n       subtitle = \"Why did Ramesh put me in this course...\",\n       caption = \"Bro, they are doing **cool** things in the other\n       classes...\") +\n  theme_graph(base_family = \"fira\") +\n  theme(legend.position = \"top\")\n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` in the `default_aes` field and elsewhere instead.\n\n\n\n\n\n\n\nQuestions and Inferences #3:\n\n\nQuestions and Inferences: What parameters have been changed here, compared to the earlier graph? Where do you see these changes in the code above?\n\n\nLet us Play with this graph and see if we can make some small changes. Colour? Fill? Width? Size? Stroke? Labs? Of course!\n\nShow the Code# Change the parameters in each of the commands here to new ones\n# Use fixed values for colours or sizes...etc. \n\nggraph(graph = ga,  layout = \"kk\") + \n  geom_edge_link(width = 2) + \n  geom_node_point(shape = 21, size = 8, \n                  fill = \"blue\", \n                  color = \"green\", \n                  stroke = 2) +\n  labs(title = \"Whoo Hoo! My next silly Grey's Anatomy graph in R!\",\n       subtitle = \"Why did Ramesh put me in this course...\",\n       caption = \"Bro, they are doing cool things in the other \n       classes...\") +\n  theme_graph(base_family = \"fira\") +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\nQuestions and Inferences #4:\n\n\nQuestions and Inferences: What did the shape parameter achieve? What are the possibilities with shape? How about including alpha?\n\n\n3c. Aesthetic Mapping from Node and Edge attribute columns\nUp to now, we have assigned specific numbers to geometric aesthetics such as shape and size. Now we are ready ( maybe ?) change the meaning and significance of the entire graph and each element within it, and use aesthetics / metaphoric mappings to achieve new meanings or insights. Let us try using aes() inside each geom to map a variable to a geometric aspect.\nDon‚Äôt try to use more than 2 aesthetic mappings simultaneously!!\nThe node elements we can tweak are:\n\nTypes of Nodes: geom_node_****()\n\nNode Parameters: inside geom_node_****(aes(...............))\n-aes(alpha  = node-variable) : opacity; a value between 0 and 1\n-aes(shape  = node-variable) : node shape\n-aes(colour = node-variable) : node colour\n-aes(fill   = node-variable) : fill colour for node\n-aes(size   = node-variable) : size of node\n\nThe edge elements we can tweak are:\n\nType of Edges‚Äù geom_edge_****()\n\nEdge Parameters: inside geom_edge_****(aes(...............))\n-aes(colour = edge-variable) : colour of the edge\n-aes(width  = edge-variable) : width of the edge\n-aes(label  = some_variable) : labels for the edge\n\nType ?geom_node_point and ?geom-edge_link in your Console for more information.\n\nShow the Codeggraph(graph = ga, layout = \"fr\") +\n  geom_edge_link0(aes(width = weight)) + # add mapping here\n  \n  geom_node_point(aes(color = race), size = 6) + # add mapping here\n\n  # geom_node_label(aes(label = name), # modify this mapping\n  #                 repel = TRUE, max.overlaps = 20,\n  #                 alpha = 0.6,\n  #                 size = 3) +\n\n  labs(title = \"Whoo Hoo! Yet another Grey's Anatomy graph in R!\") +\n  theme_graph(base_family = \"fira\") +\n  theme(legend.position = \"right\")\n\n\n\n\n\n\nQuestions and Inferences #5:\n\n\nQuestions and Inferences: Describe some of the changes here. What types of edges worked? Which variables were you able to use for nodes and edges and how? What did not work with either of the two?\n\n\n\n\nShow the Code# Arc diagram\nggraph(ga, layout = \"linear\") +\n  geom_edge_arc(aes(width = weight), alpha = 0.8) +\n  scale_edge_width(range = c(0.2, 2)) +\n  geom_node_point(size = 2, colour = \"red\") +\n  labs(edge_width = \"Weight\") +\n  theme_graph(base_family = \"fira\") +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\nQuestions and Inferences #6:\n\n\nQuestions and Inferences: How does this graph look ‚Äúmetaphorically‚Äù different? Do you see a difference in the relationships between people here? Why?\n\n\n\nShow the Code# Coord diagram, circular\nggraph(ga, layout = \"linear\", circular = TRUE) + \n  geom_edge_arc(aes(width = weight), alpha = 0.8) + \n  scale_edge_width(range = c(0.2, 2)) +\n  geom_node_point(size = 4,colour = \"red\") + \n  geom_node_text(aes(label = name),repel = TRUE, size = 3,\n                 max.overlaps = 20) +\n  labs(edge_width = \"Weight\") +\n  theme_graph() +\n  theme(legend.position = \"right\", \n        aspect.ratio = 1,\n        text = element_text(family = \"fira\"))\n\n\n\n\n\n\nQuestions and Inferences #7:\n\n\nQuestions and Inferences: How does this graph look ‚Äúmetaphorically‚Äù different? Do you see a difference in the relationships between people here? Why?\n\n\n\nThese provide for some alternative metaphorical views of networks. Note that not all layouts are possible for all datasets!!\n\nShow the Code# setting theme_graph \nset_graph_style(family = \"fira\")\n\n# This dataset contains the graph that describes the class \n# hierarchy for the Flare visualization library.\n# Type ?flare in your Console\nhead(flare$vertices)\n\n\n\n  \n\n\nShow the Codehead(flare$edges)\n\n\n\n  \n\n\nShow the Code# flare class hierarchy\ngraph = tbl_graph(edges = flare$edges, nodes = flare$vertices)\n\n# dendrogram\nggraph(graph, layout = \"dendrogram\") + \n  geom_edge_diagonal() + \n  labs(title = \"Dendrogram\")\n\n\n\nShow the Code# circular dendrogram\nggraph(graph, layout = \"dendrogram\", circular = TRUE) + \n  geom_edge_diagonal() + \n  geom_node_point(aes(filter = leaf)) + \n  coord_fixed()+ \n  labs(title = \"Circular Dendrogram\")\n\n\n\nShow the Code# rectangular tree map\nggraph(graph, layout = \"treemap\", weight = size) + \n  geom_node_tile(aes(fill = depth), size = 0.25) + \n  labs(title = \"Rectangular Tree Map\")\n\n\n\nShow the Code# circular tree map\nggraph(graph, layout = \"circlepack\", weight = size) + \n  geom_node_circle(aes(fill = depth), size = 0.25, n = 50) + \n  coord_fixed() + \n  labs(title = \"Circular Tree Map\")\n\n\n\nShow the Code# icicle\nggraph(graph, layout = \"partition\") + \n  geom_node_tile(aes(y = -y, fill = depth))\n\n\n\nShow the Code# sunburst (circular icicle)\nggraph(graph, layout = \"partition\", circular = TRUE) +\n  geom_node_arc_bar(aes(fill = depth)) +\n  coord_fixed() + \n  labs(title = \"Circular Icicle\")\n\n\n\n\n\n\nQuestions and Inferences #8:\n\n\nQuestions and Inferences: How do graphs look ‚Äúmetaphorically‚Äù different? Do they reveal different aspects of the group? How?\n\n\n\nFaceting allows to create sub-plots according to the values of a qualitative attribute on nodes or edges.\n\nShow the Code# setting theme_graph \nset_graph_style(family = \"fira\")\n\n\n# facet edges by type\nggraph(ga,layout = \"linear\", circular = TRUE) + \n  geom_edge_link(aes(color = type)) + \n  geom_node_point() +\n  facet_edges(~ type) +\n  theme(aspect.ratio = 1)\n\n\n\nShow the Code# facet nodes by sex\nggraph(ga,layout = \"linear\", circular = TRUE) + \n  geom_edge_link() + \n  geom_node_point() +\n  facet_nodes(~race) +\n  theme(aspect.ratio = 1)\n\n\n\nShow the Code# facet both nodes and edges\nggraph(ga,layout = \"linear\", circular = TRUE) + \n  geom_edge_link(aes(color = type)) + \n  geom_node_point() +\n  facet_graph(type ~ race) + \n  th_foreground(border = TRUE) +\n  theme(aspect.ratio = 1, legend.position = \"top\")\n\n\n\n\n\n\nQuestions and Inferences #9:\n\n\nQuestions and Inferences: Does splitting up the main graph into subnetworks give you more insight? Describe some of these.\n\n\n\nThe data frame graph representation can be easily augmented with metrics or statistics computed on the graph. Remember how we computed counts with the penguin dataset in Grammar of Graphics.\nBefore computing a metric on nodes or edges use the activate() function to activate either node or edge data frames. Use dplyr verbs (filter, arrange, mutate) to achieve your computation in the proper way.\n\nCentrality is a an ‚Äúill-defined‚Äù metric of node and edge importance in a network. It is therefore calculated in many ways. Type ?centrality in your Console.\n\n\nStandards\n\n\nLet‚Äôs add a few columns to the nodes and edges based on network centrality measures:\n\nShow the Codega %>% \n  activate(nodes) %>% \n  \n  # Most connections?\n  mutate(degree = centrality_degree(mode = c(\"in\"))) %>% \n  filter(degree > 0) %>% \n  \n  activate(edges) %>% \n  # \"Busiest\" edge?\n  mutate(betweenness = centrality_edge_betweenness())\n\n# A tbl_graph: 54 nodes and 57 edges\n#\n# An undirected simple graph with 4 components\n#\n# Edge Data: 57 √ó 5 (active)\n   from    to weight type     betweenness\n  <int> <int>  <dbl> <chr>          <dbl>\n1     5    47      2 friends         20.3\n2    21    47      4 benefits        44.7\n3     5    46      1 friends         39  \n4     5    41      1 friends         66.3\n5    18    41      6 friends         39  \n6    21    41     12 benefits        91.5\n# ‚Ä¶ with 51 more rows\n#\n# Node Data: 54 √ó 8\n  name               sex   race  birthyear position  season sign   degree\n  <chr>              <chr> <chr>     <dbl> <chr>      <dbl> <chr>   <dbl>\n1 Addison Montgomery F     White      1967 Attending      1 Libra       3\n2 Adele Webber       F     Black      1949 Non-Staff      2 Leo         1\n3 Teddy Altman       F     White      1969 Attending      6 Pisces      4\n# ‚Ä¶ with 51 more rows\n\n\nPackages tidygraph and ggraph can be pipelined to perform analysis and visualization tasks in one go.\n\nShow the Code# setting theme_graph \nset_graph_style()\n\nga %>% \n  activate(nodes) %>% \n  \n  # Who has the most connections?\n  mutate(degree = centrality_degree()) %>% \n  \n  activate(edges) %>% \n  # Who is the go-through person?\n  mutate(betweenness = centrality_edge_betweenness()) %>%\n  \n  # Now to continue with plotting\n  ggraph(layout = \"nicely\") +\n  geom_edge_link(aes(alpha = betweenness)) +\n  geom_node_point(aes(size = degree, colour = degree)) + \n  \n  # discrete colour legend\n  scale_color_gradient(guide = \"legend\") +\n  theme_graph(base_family = \"fira\")\n\n\n\nShow the Code# or even less typing\n  ggraph(ga,layout = \"nicely\") +\n  geom_edge_link(aes(alpha = centrality_edge_betweenness())) +\n  geom_node_point(aes(colour = centrality_degree(), \n                      size = centrality_degree())) + \n  scale_color_gradient(guide = \"legend\",\n                       low = \"green\",\n                       high = \"red\") +\n  theme_graph(base_family = \"fira\")\n\n\n\n\n\n\nQuestions and Inferences #10:\n\n\nQuestions and Inferences: How do the Centrality Measures show up in the graph? Would you ‚Äúagree‚Äù with the way we have done it? Try to modify the aesthetics by copy-pasting this chunk below and see how you can make an alternative representation.\n\n\n\nWho is close to whom? Which are the groups you can see?\n\nShow the Code# setting theme_graph \nset_graph_style()\n\n\n# visualize communities of nodes\nga %>% \n  activate(nodes) %>%\n  mutate(community = as.factor(group_louvain())) %>% \n  ggraph(layout = \"graphopt\") + \n  geom_edge_link() + \n  geom_node_point(aes(color = community), size = 5) +\n  theme_graph(base_family = \"fira\")\n\n\n\n\n\n\nQuestions and Inferences #11:\n\n\nQuestions and Inferences: Is the Community depiction clear? How would you do it, with which aesthetic? Copy Paste this chunk below and try.\n\n\n\nExploring the VisNetwork package. Make graphs wiggle and shake using tidy commands! The package implements interactivity using the physical metaphor of weights and springs we discussed earlier.\nThe visNetwork() function uses a nodes list and edges list to create an interactive graph. The nodes list must include an ‚Äúid‚Äù column, and the edge list must have ‚Äúfrom‚Äù and ‚Äúto‚Äù columns. The function also plots the labels for the nodes, using the names of the cities from the ‚Äúlabel‚Äù column in the node list.\n\nShow the Codelibrary(visNetwork)\n\n# Prepare the data for plotting by visNetwork\ngrey_nodes\n\n\n\n  \n\n\nShow the Codegrey_edges\n\n\n\n  \n\n\nShow the Code# Relabel greys anatomy nodes and edges for VisNetwork\ngrey_nodes_vis <- grey_nodes %>% \n  rowid_to_column(var = \"id\") %>% \n  rename(\"label\" = name) %>% \n  mutate(sex = case_when(sex == \"F\" ~ \"Female\",\n                         sex == \"M\" ~ \"Male\")) %>% \n  replace_na(., list(sex = \"Transgender?\")) %>% \n  rename(\"group\" = sex)\ngrey_nodes_vis\n\n\n\n  \n\n\nShow the Codegrey_edges_vis <- grey_edges %>% \n  select(from, to) %>% \n  left_join(., grey_nodes_vis, \n            by = c(\"from\" = \"label\")) %>% \n  left_join(., grey_nodes_vis, \n            by = c(\"to\" = \"label\")) %>%\n  select(\"from\"= id.x, \"to\" = id.y)\ngrey_edges_vis\n\n\n\n  \n\n\n\nUsing fontawesome icons\n\nShow the Codegrey_nodes_vis %>%\n\n  visNetwork(nodes = ., edges = grey_edges_vis) %>% \n  visNodes(font = list(size = 40)) %>% \n  \n  # Colour and icons for each of the gender-groups\n  visGroups(groupname = \"Female\", shape = \"icon\", \n            icon = list(code = \"f182\", size = 75, color = \"tomato\"),\n            shadow = list(enabled = TRUE)) %>% \n  \n  visGroups(groupname = \"Male\", shape = \"icon\", \n            icon = list(code = \"f183\", size = 75, color = \"slateblue\"), \n            shadow = list(enabled = TRUE)) %>% \n  \n  visGroups(groupname = \"Transgender?\", shape = \"icon\", \n            icon = list(code = \"f22c\", size = 75, color = \"fuchsia\"), \n            shadow = list(enabled = TRUE)) %>% \n  \n  #visLegend() %>%\n  #Add the fontawesome icons!!\n  addFontAwesome() %>% \n  \n  # Add Interaction Controls\n  visInteraction(navigationButtons = TRUE,\n                 hover = TRUE,\n                 selectConnectedEdges = TRUE,\n                 hoverConnectedEdges = TRUE,\n                 zoomView = TRUE)\n\n\n\n\n\nThere is another family of icons available in visNetwork, called ionicons. Let‚Äôs see how they look:\n\nShow the Codegrey_nodes_vis %>%\n\n  visNetwork(nodes = ., edges = grey_edges_vis,) %>%\n  visLayout(randomSeed = 12345) %>%\n  visNodes(font = list(size = 50)) %>%\n  visEdges(color = \"green\") %>%\n  visGroups(\n    groupname = \"Female\",\n    shape = \"icon\",\n    icon = list(\n      face = 'Ionicons',\n      code = \"f25d\",\n      color = \"fuchsia\",\n      size = 125\n    )\n  ) %>%\n  \n  visGroups(\n    groupname = \"Male\",\n    shape = \"icon\",\n    icon = list(\n      face = 'Ionicons',\n      code = \"f202\",\n      color = \"green\",\n      size = 125\n    )\n  ) %>%\n  \n  visGroups(\n    groupname = \"Transgender?\",\n    shape = \"icon\",\n    icon = list(\n      face = 'Ionicons',\n      code = \"f233\",\n      color = \"dodgerblue\",\n      size = 125\n    )\n  ) %>%\n  visLegend() %>%\n  addIonicons() %>%\n  visInteraction(\n    navigationButtons = TRUE,\n    hover = TRUE,\n    selectConnectedEdges = TRUE,\n    hoverConnectedEdges = TRUE,\n    zoomView = TRUE\n  )\n\n\n\n\n\nSome idea of interactivity and controls with visNetwork:\n\nShow the Codelibrary(visNetwork)\n# let's look again at the data\nstarwars_nodes <- read_csv(\"../../../materials/data/networks/star-wars-network-nodes.csv\")\n\nRows: 22 Columns: 2\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (1): name\ndbl (1): id\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nShow the Codestarwars_edges <- read_csv(\"../../../materials/data/networks/star-wars-network-edges.csv\")\n\nRows: 60 Columns: 3\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (2): source, target\ndbl (1): weight\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n Download Nodes data as csv\n\n\n\n\n Download Edges data as csv\n\n\n\n\nShow the Code# We need to rename starwars nodes dataframe and edge dataframe columns for visNetwork\nstarwars_nodes_vis <- \n  starwars_nodes %>% \n  rename(\"label\" = name)\n\n# Convert from and to columns to **node ids**\nstarwars_edges_vis <- \n  starwars_edges %>% \n  \n  # Matching Source <- Source Node id (\"id.x\")\n  left_join(., starwars_nodes_vis, by = c(\"source\" = \"label\")) %>% \n  \n  # Matching Target <- Target Node id (\"id.y\")\n  left_join(., starwars_nodes_vis, by = c(\"target\" = \"label\")) %>% \n  \n# Select \"id.x\" and \"id.y\" ONLY\n# Rename them as \"from\" and \"to\"\n# keep \"weight\" column for aesthetics of edges\n  select(\"from\" = id.x, \"to\" = id.y, \"value\" = weight)\n\n# Check everything once\nstarwars_nodes_vis\n\n\n\n  \n\n\nShow the Codestarwars_edges_vis\n\n\n\n  \n\n\n\nOk, let‚Äôs make things move and shake!!\n\nShow the CodevisNetwork(nodes = starwars_nodes_vis,\n           edges = starwars_edges_vis) %>% \n  visNodes(font = list(size = 30), shape = \"icon\", \n           icon = list(code = \"f1e3\", size = 75)) %>% \n  addFontAwesome() %>% \n  visEdges(color = \"red\")\n\n\n\n\n\n\nShow the CodevisNetwork(nodes = starwars_nodes_vis,\n           edges = starwars_edges_vis) %>% \n  visNodes(font = list(size = 30)) %>% \n  visEdges(color = \"red\")\n\n\n\n\n\n\n\nStep 1. Fire up a new RMarkdown. Write your name, file_name and date.\nStep 2. Take any one of the ‚ÄúMake1-Datasets‚Äù datasets decribed below.\nStep 3. RMarkdown contents:\n\nIntroduce / Inspect in R your data and describe\nIntroduce your Purpose\nCreate graph objects.\nWrite comments in the code\nWrite narrative in text with sections, bold ,italic etc.\n\nStep 4. Knit before you submit. Submit only your knittable .Rmd file.\n\n\nAirline Data:\n\n\nStart with this bit of code in your second chunk, after set up\n\n\n\nShow the Codeairline_nodes <- read_csv(\"../../../materials/data/networks/AIRLINES-NODES.csv\") %>% mutate(Id = Id + 1)\nairline_edges <- read_csv(\"../../../materials/data/networks/AIRLINES-EDGES.csv\") %>%\nmutate(Source = Source + 1, Target = Target + 1)\n\n\n\n\n\n Download data as csv\n\n\n\n\n Download data as csv\n\n\n\n\nThe Famous Zachary Karate Club dataset\n\n\nStart with pulling this data into your Rmarkdown:\n\ndata(\"karate\",package= \"igraphdata\")\nkarate\n\nTry ?karate in the console\nNote that this is not a set of nodes, nor edges, but already a graph-object!\nSo no need to create a graph object using tbl_graph.\nYou will need to just go ahead and plot using ggraph.\n\n\nGame of Thrones:\n\n\nStart with pulling this data into your Rmarkdown:\n\n\nShow the CodeGoT <- read_rds(\"../../../materials/data/networks/GoT.RDS\")\n\n\n\n\n\n Download data as RDS\n\n\n\n\nNote that this is a list of 7 graphs from Game of Thrones.\nSelect one using GoT[[index]] where index = 1‚Ä¶7 and then plot directly.\nTry to access the nodes and edges and modify them using any attribute data\n\n\nAny other graph dataset from igraphdata (type ?igraphdata in console)\n\n\nAsk me for help if you need any\n\nThis is in groups. Groups of 4. To be announced\nYou need to create a Network Graph for your favourite Book, play, TV serial or Show. (E.g. Friends, BBT, or LB or HIMYM‚Ä¶or Hamlet, Little Women , Pride and Prejudice, or LoTR)\nStep 1. Go to: Literary Networks for instructions. (Instructions are on also Teams -> Files.)\nStep 2. Make your data using the instructions.\n\nIn the nodes excel, use id and names as your columns. Any other details in other columns to the right.\nIn your edges excel, use from and to are your first columns. Entries in these columns can be names or ids but be consistent and don‚Äôt mix.\n\nStep 3. Decide on 3 answers that you to seek and plan to make graphs for.\nStep 4. Create graph objects. Say 3 visualizations.\nStep 5. Write comments/answers in the code and narrative text. Add pictures from the web using Markdown syntax.\nStep 6. Write Reflection ( ok, a short one!) inside your RMarkdown. Make sure it knits!!\nStep 7. Group Submission: Submit the knittable .Rmd file AND the data. RMarkdown with joint authorship. Each person submits on their Assignments. All get the same grade on this one.\nAsk me for clarifications on what to do after you have read the Instructions in your group.\n\n\nThomas Lin Pedersen - 1 giraffe, 2 giraffe,GO!\nIgraph: Network Analysis and Visualization. https://CRAN.R-project.org/package=igraph.\nPedersen, Thomas Lin. 2017a. Ggraph: An Implementation of Grammar of Graphics for Graphs and Networks. https://CRAN.R-project.org/package=ggraph.\n‚Äî‚Äî‚Äî. 2017b. Tidygraph: A Tidy Api for Graph Manipulation. https://CRAN.R-project.org/package=tidygraph.\nTyner, Sam, Fran√ßois Briatte, and Heike Hofmann. 2017. ‚ÄúNetwork Visualization with ggplot2.‚Äù The R Journal 9 (1): 27‚Äì59. https://journal.r-project.org/archive/2017/RJ-2017-023/index.html.\nNetwork Datasets https://icon.colorado.edu/#!/networks\nYunran Chen, Introduction to Network Analysis Using R"
  },
  {
    "objectID": "content/labs/r-labs/pronouns/index.html",
    "href": "content/labs/r-labs/pronouns/index.html",
    "title": "Lab-02: Pronouns and Data",
    "section": "",
    "text": "Understand different kinds of data variables\nAppreciate how they can be identified based on the Interrogative Pronouns they answer to\nUnderstand how each kind of variable lends itself to a specific geometric aspect in the data visualization.\nUnderstand how ask Questions of Data to develop Visualizations"
  },
  {
    "objectID": "content/labs/r-labs/pronouns/index.html#set-up",
    "href": "content/labs/r-labs/pronouns/index.html#set-up",
    "title": "Lab-02: Pronouns and Data",
    "section": "\n2.1 Set Up",
    "text": "2.1 Set Up\nThe setup code chunk below brings into our coding session R packages that provide specific computational abilities and also datasets which we can use.\nTo reiterate: Packages and datasets are not the same thing !! Packages are (small) collections of programs. Datasets are just‚Ä¶.information."
  },
  {
    "objectID": "content/labs/r-labs/pronouns/index.html#the-penguins-dataset",
    "href": "content/labs/r-labs/pronouns/index.html#the-penguins-dataset",
    "title": "Lab-02: Pronouns and Data",
    "section": "\n5.1 The penguins dataset",
    "text": "5.1 The penguins dataset\nnames(penguins) # Column, i.e. Variable names\n[1] ‚Äúspecies‚Äù ‚Äúisland‚Äù ‚Äúbill_length_mm‚Äù\n[4] ‚Äúbill_depth_mm‚Äù ‚Äúflipper_length_mm‚Äù ‚Äúbody_mass_g‚Äù\n[7] ‚Äúsex‚Äù ‚Äúyear‚Äù\nhead(penguins) # first six rows\n\n\n\n  \n\n\ntail(penguins) # Last six rows\n\n\n\n  \n\n\ndim(penguins) # Size of dataset\n[1] 344 8\n# Check for missing data\nany(is.na(penguins) == TRUE)\n[1] TRUE\n\n\n\n\n\n\nInspect the Data\n\n\n\n\nWhat are the variable names()?\nWhat would be the Question you might have asked to obtain each of the variables?\nWhat further questions/meta questions would you ask to ‚Äúprocess‚Äù that variable? ( Hint: Add another word after any of the Interrogative Pronouns, e.g.¬†How‚Ä¶MANY?)\nWhere might the answers take your story?\n\n\n\n\n\n\n\n\n\nYour Turn #1\n\n\n\nState a few questions after discussion with your friend and state possible variables, or what you could DO with the variables, as an answer.\nE.g. Q. How many penguins? A. We need to count‚Ä¶rows?"
  },
  {
    "objectID": "content/labs/r-labs/pronouns/index.html#pronouns-and-variables",
    "href": "content/labs/r-labs/pronouns/index.html#pronouns-and-variables",
    "title": "Lab-02: Pronouns and Data",
    "section": "\n5.2 Pronouns and Variables",
    "text": "5.2 Pronouns and Variables\nIn the Table below, we have a rough mapping of interrogative pronouns to the kinds of variables in the data:\n\n\n\n\n\n\n\n\n\nPronoun\nAnswer\nVariable / Scale\nE xample\nWhat O perations?\n\n\n\nWhat, Who, Where, Whom, Which\nName, Place, Animal, Thing\nQ ualitative / N ominal**\nName\n\nCount no. of cases\nMode\n\n\n\nHow, What Kind, What Sort\nA Manner / Method, Type or A ttribute from a list, with list items in some ‚Äù or der**‚Äù ( e.g.¬†good, better, i mproved, best..)\nQ ualitative / O rdinal**\n\n\nSo cio-ec onomic\n\nstatus\n(‚Äúlow\ni\nncome, middle i ncome,\nhigh i ncome)\n\n\nedu cation\nlevel\n\n\n(‚Äúhigh\nsch ool‚Äù,‚Äù\nBS‚Äù ,‚ÄúMS‚Äù, ‚ÄúPhD‚Äù)\n\n\nincome\nlevel\n\n\n(‚Äúless\nthan\n50K‚Äù, ‚Äú5 0K-100 K‚Äù ,‚Äúover\n100K‚Äù)\n\n\nSatisf action ra\ntin g(‚Äúext remely\ndis like‚Äù, ‚Äù\ndis like‚Äù, ‚Äù n\neut ral‚Äù,‚Äù like‚Äù, ‚Äù\next remely l ike‚Äù).\n\n\n\n\nMedian\nP ercentiles\n\n\n\nHow Many / Much / Heavy? Few? Seldom? Often? When?\n\nQ uantities with Scale.\nD iff e rences are me a ningful, but not products or ratios\n\nQu antitative / Interval\n\npH\n\nSAT\nscore ( 20 0-800)\n\n\nCredit\nscore\n(30 0-850)\n\nYear of St arting in C ollege\n\n\nMean\n\nStandard\nDeviation\n\n\n\n\nHow Many / Much / Heavy? Few? Seldom? Often? When?\n\nQu a ntities, with Scale and a Zero Value.\nDi f ferences and Ratios / Products are me a ningful. (e.g Weight )\n\nQua n titative / Ratio**\n\n\n\nW eight,\nL ength,\n\n\nHeight\n\nTe\n\nm pe rature in\nKelvin\n\nEnzyme\n\na ct ivity, dose\na mount,\nre action rate, flow\nr at e,conc e nt ration\n\nPulse\nSu rvival\n\ntime\n\n\n\nCo\n\nrre lation - Coeff of\nV ariation\n\n\n\n\nAs you go from Qualitative to Quantitative data types in the table, I hope you can detect a movement from fuzzy groups/categories to more and more crystallized numbers. Each variable/scale can be subjected to the operations of the previous group. In the words of S.S. Stevens (https://psychology.okstate.edu/faculty/jgrice/psyc3214/Stevens_FourScales_1946.pdf)\n\nthe basic operations needed to create each type of scale is cumulative: to an operation listed opposite a particular scale must be added all those operations preceding it.\n\nDo think about this as you work with data.\n\nDo take a look at these references:\n\nhttps://stats.idre.ucla.edu/other/mult-pkg/whatstat/what-is-the-difference-between-categorical-ordinal-and-interval-variables/\nhttps://www.freecodecamp.org/news/types-of-data-in-statistics-nominal-ordinal-interval-and-ratio-data-types-explained-with-examples/"
  },
  {
    "objectID": "content/labs/r-labs/pronouns/index.html#the-mpg-dataset",
    "href": "content/labs/r-labs/pronouns/index.html#the-mpg-dataset",
    "title": "Lab-02: Pronouns and Data",
    "section": "\n5.3 The mpg dataset",
    "text": "5.3 The mpg dataset\nnames(mpg) # Column, i.e. Variable names\n[1] ‚Äúmanufacturer‚Äù ‚Äúmodel‚Äù ‚Äúdispl‚Äù ‚Äúyear‚Äù ‚Äúcyl‚Äù\n[6] ‚Äútrans‚Äù ‚Äúdrv‚Äù ‚Äúcty‚Äù ‚Äúhwy‚Äù ‚Äúfl‚Äù\n[11] ‚Äúclass‚Äù\nhead(mpg) # first six rows\n\n\n\n  \n\n\ntail(mpg) # Last six rows\n\n\n\n  \n\n\ndim(mpg) # Size of dataset\n[1] 234 11\n# Check for missing data\nany(is.na(mpg) == TRUE)\n[1] FALSE\n\n5.3.1 YOUR TURN-2\nLook carefully at the variables here. How would you interpret say the cyl variable? Is it a number and therefore Quantitative, or could it be something else?"
  },
  {
    "objectID": "content/labs/r-labs/pronouns/index.html#single-qualitativecategorical-nominal-variable",
    "href": "content/labs/r-labs/pronouns/index.html#single-qualitativecategorical-nominal-variable",
    "title": "Lab-02: Pronouns and Data",
    "section": "\n6.1 Single Qualitative/Categorical/ Nominal Variable",
    "text": "6.1 Single Qualitative/Categorical/ Nominal Variable\n\nQuestions: Which? What Kind? How? How many of each Kind?\n\n\nIsland ( Which island ? )\nSpecies ( Which Species? )\n\n\nCalculations: No of levels / Counts for each level\n\n\n\n\ncount / tally of no. of penguins on each island or in each species\n\nsort and order by island or species\n\n\nCharts: Bar Chart / Pie Chart / Tree Map\n\n\n\ngeom_bar / geom_bar + coord_polar() / Find out!!\n\npenguins %>% count(species)\n\n\n\n  \n\n\nggplot(penguins) + geom_bar(aes(x = island))\n\n\n\nggplot(penguins) + geom_bar(aes(x = sex))\n\n\n\n\n6.1.1 YOUR TURN-3"
  },
  {
    "objectID": "content/labs/r-labs/pronouns/index.html#single-quantitative-variable",
    "href": "content/labs/r-labs/pronouns/index.html#single-quantitative-variable",
    "title": "Lab-02: Pronouns and Data",
    "section": "\n6.2 Single Quantitative Variable",
    "text": "6.2 Single Quantitative Variable\n\nQuestions: How many? How few? How often? How much?\nCalculations: max / min / mean / mode / (units)\n\n\n\nmax(), min(), range(), mean(), mode(), summary()\n\n\n\nCharts: Bar Chart / Histogram / Density\n\n\ngeom_histogram() / geom_density()\n\n\n\n\nmax(penguins$bill_length_mm)\n[1] 59.6\nrange(penguins$bill_length_mm, na.rm =TRUE) \n[1] 32.1 59.6\nsummary(penguins$flipper_length_mm)\nMin. 1st Qu. Median Mean 3rd Qu. Max. 172 190 197 201 213 231\nggplot(penguins) + geom_density(aes(bill_length_mm))\n\n\n\nggplot(penguins) + geom_histogram(aes(x = bill_length_mm))\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n6.2.1 YOUR TURN-4\nAre all the above Quantitative variables ratio variables? Justify."
  },
  {
    "objectID": "content/labs/r-labs/pronouns/index.html#two-variables-quantitative-vs-quantitative",
    "href": "content/labs/r-labs/pronouns/index.html#two-variables-quantitative-vs-quantitative",
    "title": "Lab-02: Pronouns and Data",
    "section": "\n6.3 Two Variables: Quantitative vs Quantitative",
    "text": "6.3 Two Variables: Quantitative vs Quantitative\nWe can easily extend our intuition about one quantitative variable, to a pair of them. What Questions can we ask?\n\nQuestions: How many of this vs How many of that? Does this depend upon that? How are they related? (Remember \\(y = mx + c\\) and friends?)\nCalculations: Correlation / Covariance / T-test / Chi-Square Test for Two Means etc. We won‚Äôt go into this here !\nCharts: Scatter Plot / Line Plot / Regression i.e.¬†best fit lines\n\ncor(penguins$bill_length_mm, penguins$bill_depth_mm)\n[1] -0.2286256\nggplot(penguins) +\n  geom_point(aes(x = flipper_length_mm,\n                 y = body_mass_g))\n\n\n\nggplot(penguins) +\n  geom_point(aes(x = flipper_length_mm, \n                 y = bill_length_mm))\n\n\n\n\n6.3.1 YOUR TURN-5"
  },
  {
    "objectID": "content/labs/r-labs/pronouns/index.html#two-variables-categorical-vs-categorical",
    "href": "content/labs/r-labs/pronouns/index.html#two-variables-categorical-vs-categorical",
    "title": "Lab-02: Pronouns and Data",
    "section": "\n6.4 Two Variables: Categorical vs Categorical",
    "text": "6.4 Two Variables: Categorical vs Categorical\nWhat sort of question could we ask that involves two categorical variables?\n\nQuestions: How Many of this Kind( ~x) are How Many of that Kind( ~y ) ?\n\nCalculations: Counts and Tallies sliced by Category\n\n\ncounts , tally\n\n\n\n\nCharts: Stacked Bar Charts / Grouped Bar Charts / Segmented Bar Chart / Mosaic Chart\n\ngeom_bar()\nUse the second Categorical variables to modify fill, color.\nAlso try to vary the parameter position of the bars.\n\n\n\nggplot(penguins) + geom_bar(aes(x = island, \n                                fill = species),\n                            position = \"stack\")\n\n\n\nStoryline: ‡§§‡•Ä‡§® ‡§™‡•á‡§®‡§ó‡•Ä‡§®‡•§ ‡§î‡§∞ ‡§§‡•Å‡§Æ ‡§≠‡•Ä ‡§§‡•Ä‡§®(Oh never mind!)\n\n6.4.1 YOUR TURN-6"
  },
  {
    "objectID": "content/labs/r-labs/pronouns/index.html#two-variables-quantitative-vs-qualitative",
    "href": "content/labs/r-labs/pronouns/index.html#two-variables-quantitative-vs-qualitative",
    "title": "Lab-02: Pronouns and Data",
    "section": "\n6.5 Two Variables: Quantitative vs Qualitative",
    "text": "6.5 Two Variables: Quantitative vs Qualitative\nFinally, what if we want to look at Quant variables and Qual variables together? What questions could we ask?\n\nQuestions: How much of this is Which Kind of that? How many vs Which? How many vs How?\nCalculations: Counts, Means, Ranges etc., grouped by Categorical variable.\n\nggplot(penguins) + \n    geom_density(aes(x = body_mass_g, \n                 color = island, \n                 fill = island), \n                 alpha = 0.3)\n\n\n\n\nCharts: Bar Chart using group / density plots by group / violin plots by group / box plots by group\n\n\n\ngeom_bar / geom_density / geom_violin / geom_boxplot using Categorical Variable for grouping\n\nggplot(penguins) + \n    geom_density(aes(x = body_mass_g, \n                 color = island, \n                 fill = island), \n                 alpha = 0.3)\n\n\n\nggplot(penguins) + \n  geom_histogram(aes(x = flipper_length_mm,\n                 fill = sex))\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n6.5.1 YOUR TURN-7\n\n6.5.2 Time to Play\n\nCreate a fresh RMarkdown and similarly analyse two datasets of the following data sets\n\n\nAny dataset in your R installation. Type data() in your console to see what is available.\ndiamonds . This dataset is part of the tidyverse package so just type diamonds in your code and there it is.\ngapminder !! Yes!!You will need to install the gapminder package to access this dataset\nmosaicData package datasets. Install mosaicData\ndata.world: Find Datasets of your choice: https://docs.data.world/en/64499-64516-Quickstarts-and-tutorials.html\nkaggle: https://www.kaggle.com/datasets"
  },
  {
    "objectID": "content/labs/r-labs/r-labs-listing.html",
    "href": "content/labs/r-labs/r-labs-listing.html",
    "title": "The Foundation Series",
    "section": "",
    "text": "Lab 01 - Introduce Yourself\n\n\nInstalling and Getting Started with R!\n\n\nPart of my R for Artists and Designers course using the idea of metaphors in written language.\n\n\n\n\n\n\nJul 6, 2021\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\nLab 01 - Introduce Yourself\n\n\nInstalling and Getting Started with R!\n\n\nPart of my R for Artists and Designers course using the idea of metaphors in written language.\n\n\n\n\n\n\nJul 6, 2021\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\nLab 01 - Introduce Yourself\n\n\nInstalling and Getting Started with R!\n\n\nPart of my R for Artists and Designers course using the idea of metaphors in written language.\n\n\n\n\n\n\nJul 6, 2021\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\nLab 01 - Introduce Yourself\n\n\nInstalling and Getting Started with R!\n\n\nPart of my R for Artists and Designers course using the idea of metaphors in written language.\n\n\n\n\n\n\nJul 6, 2021\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\nLab 04 - The Grammar of Graphics\n\n\nInstalling and Getting Started with R!\n\n\nPart of my R for Artists and Designers course using the idea of metaphors in written language.\n\n\n\n\n\n\nJul 6, 2021\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\nLab 07 - The Grammar of Networks\n\n\nInstalling and Getting Started with R!\n\n\nPart of my R for Artists and Designers course using the idea of metaphors in written language.\n\n\n\n\n\n\nJun 21, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\nLab-02: Pronouns and Data\n\n\n\n\n\nPart of my R for Artists and Designers course using the idea of metaphors in written language.\n\n\n\n\n\n\nJul 6, 2021\n\n\nArvind Venkatadri\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/labs/r-labs/time/index.html",
    "href": "content/labs/r-labs/time/index.html",
    "title": "Lab 01 - Introduce Yourself",
    "section": "",
    "text": "At the end of this Lab, we will:\n\nhave installed R and RStudio on our machines\nunderstood how to add additional R-packages for specific features and graphic capability\nrun code within RStudio and interpret the results\nhave learnt to look for help within R and RStudio"
  },
  {
    "objectID": "content/labs/r-labs/time/index.html#Check-In-R",
    "href": "content/labs/r-labs/time/index.html#Check-In-R",
    "title": "Lab 01 - Introduce Yourself",
    "section": "\n3.1 Check in",
    "text": "3.1 Check in\nLaunch R by clicking this logo. You should see one console with a command line interpreter. Try typing 2 + 2 and check !\nClose R."
  },
  {
    "objectID": "content/labs/r-labs/time/index.html#Check-In-RStudio",
    "href": "content/labs/r-labs/time/index.html#Check-In-RStudio",
    "title": "Lab 01 - Introduce Yourself",
    "section": "\n4.1 Check in",
    "text": "4.1 Check in\nLaunch RStudio. You should get a window similar to the screenshot you see here, but yours will be empty. Look at the bottom left pane: this is the same console window you saw when you opened R in step @Check-In-R\n\nPlace your cursor where you see > and type x <- 2 + 2 again hit enter or return, then type x, and hit enter/return again.\nIf [1] 4 prints to the screen, you have successfully installed R and RStudio, and you can move onto installing packages."
  },
  {
    "objectID": "content/labs/r-labs/time/index.html#save-and-share",
    "href": "content/labs/r-labs/time/index.html#save-and-share",
    "title": "Lab 01 - Introduce Yourself",
    "section": "\n7.1 Save and share",
    "text": "7.1 Save and share\nSave your work so you can share your favorite plot with us. You will not like the looks of your plot if you mouse over to Export and save it. Instead, use ggplot2‚Äôs command for saving a plot with sensible defaults:\n\nShow the Codehelp(ggsave)\n\n\n\nShow the Codeggsave(\"file_name_here.pdf\", plot) # please make the filename unique!\n\n\nUpload this exported plot to Teams -> Assignments.\nBefore you do that, check that you can submit stuff/assignments on the MLS by uploading a cat picture üêà first, especially if you are an ailurophile like me. (Acceptable Breeds: Scottish Folds or Maine Coons)"
  },
  {
    "objectID": "content/posts/01-intro/history.html",
    "href": "content/posts/01-intro/history.html",
    "title": "Boston Terrier",
    "section": "",
    "text": "The Boston Terrier is a breed of dog originating in the United States of America. This ‚ÄúAmerican Gentleman‚Äù was accepted in 1893 by the American Kennel Club as a non-sporting breed.1 Color and markings are important when distinguishing this breed from the AKC standard. They should be either black, brindle or seal with white markings.2 Boston Terriers are small and compact with a short tail and erect ears. The AKC says they are highly intelligent and very easily trained.3 They are friendly and can be stubborn at times. The average life span of a Boston Terrier is around 11 to 13 years.4 The American Kennel Club ranked the Boston Terrier as the 21st most popular breed in 2019.5"
  },
  {
    "objectID": "content/posts/01-intro/history.html#history",
    "href": "content/posts/01-intro/history.html#history",
    "title": "Boston Terrier",
    "section": "History",
    "text": "History\n\n\n\n\n\nTerrier Seated\n\n\n\n\nThe Boston terrier breed originated around 1875, when Robert C. Hooper of Boston purchased from Edward Burnett a dog named Judge (known later as Hooper‚Äôs Judge), which was of a bull and terrier type lineage. Hooper‚Äôs Judge is directly related to the original bull and terrier breeds of the 19th and early 20th centuries. The American Kennel Club cites Hooper‚Äôs Judge as the ancestor of all true modern Boston Terriers. Judge weighed about 32 pounds (15 kg).\n\n\n\n\nJudge was bred to Burnett‚Äôs Gyp (or Kate). Gyp was a white bulldog-type female, owned by Edward Burnett, of Southboro, Massachusetts. She weighed about 20 pounds (9.1 kg), was stocky and strong and had the typical blocky head now shown in Bostons. From this foundation of the breed, subsequent breeders refined the breed into its modern day presentation. Bred down in size from fighting dogs of the bull and terrier types, the Boston Terrier originally weighed up to 44 pounds (20 kg) (Old Boston Bulldogs).\n\n\n\n\n\n\nA young male Boston Terrier with a Brown brindle coat\n\nThe breed was first shown in Boston in 1870. By 1889 the breed had become sufficiently popular in Boston that fanciers formed the American Bull Terrier Club, the breed‚Äôs nickname, ‚Äúroundheads‚Äù. Shortly after, at the suggestion of James Watson (a noted writer and authority), the club changed its name to the Boston Terrier Club and in 1893 it was admitted to membership in the American Kennel Club, thus making it the first US breed to be recognized. It is one of a small number of breeds to have originated in the United States. The Boston Terrier was the first non-sporting dog breed in the US.\nIn the early years, the color and markings were not very important to the breed‚Äôs standard. By the 20th century the breed‚Äôs distinctive markings and color were written into the standard, becoming an essential feature. The Boston Terrier has lost most of its aggressive nature, preferring the company of humans, although some males will still challenge other dogs if they feel their territory is being invaded. Boston University has used Rhett the Boston Terrier as their mascot since 1922. Wofford College in Spartanburg, SC has had a live Boston Terrier mascot named Blitz since 2003 that attends home football games. The Boston Terrier has also been the official state dog of Massachusetts since 1979."
  },
  {
    "objectID": "content/posts/01-intro/history.html#description",
    "href": "content/posts/01-intro/history.html#description",
    "title": "Boston Terrier",
    "section": "Description",
    "text": "Description\nThe Boston Terrier is a compactly built, well-proportioned dog. It has a square-looking head with erect ears and a slightly arched neck. The muzzle is short and generally wrinkle-free, with an even or a slightly undershot bite. The chest is broad and the tail is short. According to international breed standards, the dog should weigh no more than 25 pounds (11 kg). Boston Terriers usually stand up to 15‚Äì17 inches (380‚Äì430 mm) at the withers.\nThe American Kennel Club divides the breed into three classes: under 15 pounds, 15 pounds and under 20 pounds, 20 pounds and not exceeding 25 pounds.\nCoat and color\nThe Boston Terrier is characteristically marked with white in proportion to either black, brindle, seal (color of a wet seal, a very dark brown that looks black except in the bright sun), or a combination of the three. Any other color is not accepted as a Boston Terrier by the American Kennel Club, as they are usually obtained by crossbreeding with other breeds and the dog loses its characteristic ‚Äútuxedo‚Äù appearance.6 Any Boston Terrier from AKC parentage regardless of the color, or if it is a splash or has a blue eye or weak ears, can be and are registered by the AKC and participate in any AKC sporting events.\n\n\n\nA female Boston Terrier with a black coat\n\nAccording to the American Kennel Club, the Boston Terrier‚Äôs markings are broken down into two categories: Required, which consists of a white chest, white muzzle band, and a white band between the eyes; and Desired, which includes the Required markings plus a white collar, white on the forelegs, forelegs, up to the hocks on the rear legs. For conformation showing, symmetrical markings are preferred. Due to the Boston Terrier‚Äôs markings resembling formal wear, in addition to its refined and pleasant personality, the breed is commonly referred to as ‚Äúthe American Gentleman.‚Äù\n\n\n\nAn adult male Boston Terrier with a black coat"
  },
  {
    "objectID": "content/posts/01-intro/history.html#temperament",
    "href": "content/posts/01-intro/history.html#temperament",
    "title": "Boston Terrier",
    "section": "Temperament",
    "text": "Temperament\nBoston Terrier is a gentle breed that typically has a strong, happy-go-lucky, and friendly personality with a merry sense of humor. Boston Terriers are generally eager to please their owner and can be easily trained. They can be very protective of their owners, which may result in aggressive and territorial behavior toward other pets and strangers. The breed requires only a minimal amount of grooming.\nWhile originally bred for fighting as well as hunting rats in garment factories, they were later bred for companionship. They are not considered terriers by the American Kennel Club, however, but are part of the non-sporting group.\nBoth females and males are generally quiet and bark only when necessary, Their usually sensible attitude toward barking makes them excellent choices for apartment dwellers. They enjoy being around people, get along well with children, the elderly, other canines, and non-canine pets, if properly socialized."
  },
  {
    "objectID": "content/posts/01-intro/history.html#grooming",
    "href": "content/posts/01-intro/history.html#grooming",
    "title": "Boston Terrier",
    "section": "Grooming",
    "text": "Grooming\nWith a short, shiny, smooth coat, Boston Terriers require little grooming. Bostons produce light shedding, and weekly brushing of their fine coat is effective at removing loose hair. Brushing promotes the health of the coat because it distributes skin oils, and it also encourages new hair growth. Occasional bathing is suitable for the breed.7\nThe nails of Boston Terriers require regular trimming. Overgrown nails not only have the potential to inflict pain on the breed, but they can also make walking difficult or tear off after getting snagged on something.\nSimilarly to nail trimming, tooth brushing should also be done regularly to promote good oral health. The risk of the breed developing oral pain, gum infection, or bad breath can be decreased with regular tooth brushing that removes plaque buildup and other bacteria. In addition, poor dental hygiene can lead to tooth root abscesses that can lead to damage around the tissue and eventually lead to the loss of teeth."
  },
  {
    "objectID": "content/posts/02-authoring/authoring.html",
    "href": "content/posts/02-authoring/authoring.html",
    "title": "Housing Prices",
    "section": "",
    "text": "In this analysis, we build a model predicting sale prices of houses based on data on houses that were sold in the Duke Forest neighborhood of Durham, NC around November 2020. Let‚Äôs start by loading the packages we‚Äôll use for the analysis.\n\nPackageslibrary(openintro)  # for data\nlibrary(tidyverse)  # for data wrangling and visualization\nlibrary(knitr)      # for tables\nlibrary(broom)      # for model summary\n\n\nWe present the results of exploratory data analysis in Section¬†2 and the regression model in Section¬†3."
  },
  {
    "objectID": "content/posts/02-authoring/authoring.html#sec-eda",
    "href": "content/posts/02-authoring/authoring.html#sec-eda",
    "title": "Housing Prices",
    "section": "\n2 Exploratory data analysis",
    "text": "2 Exploratory data analysis\nThe data contains 98 houses. As part of the exploratory analysis let‚Äôs visualize and summarize the relationship between areas and prices of these houses.\n\n2.1 Data visualization\nFigure¬†1 shows two histograms displaying the distributions of price and area individually.\n\nCodeggplot(duke_forest, aes(x = price)) +\n  geom_histogram(binwidth = 50000) +\n  labs(title = \"Histogram of prices\")\n\nggplot(duke_forest, aes(x = area)) +\n  geom_histogram(binwidth = 250) +\n  labs(title = \"Histogram of areas\")\n\n\n\n\n\n(a) Histogram of prices\n\n\n\n\n\n\n(b) Histogram of areas\n\n\n\n\nFigure¬†1: Histograms of individual variables\n\n\n\nFigure¬†2 displays the relationship between these two variables in a scatterplot.\n\nCodeggplot(duke_forest, aes(x = area, y = price)) +\n  geom_point() +\n  labs(title = \"Price and area of houses in Duke Forest\")\n\n\n\nFigure¬†2: Scatterplot of price vs.¬†area of houses in Duke Forest\n\n\n\n\n\n2.2 Summary statistics\nTable¬†1 displays basic summary statistics for these two variables.\n\nCodeduke_forest %>%\n  summarise(\n    `Median price` = median(price),\n    `IQR price` = IQR(price),\n    `Median area` = median(area),\n    `IQR area` = IQR(area),\n    `Correlation, r` = cor(price, area)\n    ) %>%\n  kable(digits = c(0, 0, 0, 0, 2))\n\n\n\nTable¬†1: Summary statistics for price and area of houses in Duke Forest\n\nMedian price\nIQR price\nMedian area\nIQR area\nCorrelation, r\n\n\n540000\n193125\n2623\n1121\n0.67"
  },
  {
    "objectID": "content/posts/02-authoring/authoring.html#sec-model",
    "href": "content/posts/02-authoring/authoring.html#sec-model",
    "title": "Housing Prices",
    "section": "\n3 Modeling",
    "text": "3 Modeling\nWe can fit a simple linear regression model of the form shown in Equation¬†1.\n\nprice = \\hat{\\beta_0} + \\hat{\\beta_1} \\times area + \\epsilon\n\\tag{1}\nTable¬†2 shows the regression output for this model.\n\nCodeprice_fit <- lm(price ~ area, data = duke_forest)\n  \nprice_fit %>%\n  tidy() %>%\n  kable(digits = c(0, 0, 2, 2, 2))\n\n\n\nTable¬†2: Linear regression model for predicting price from area\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n(Intercept)\n116652\n53302.46\n2.19\n0.03\n\n\narea\n159\n18.17\n8.78\n0.00\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis is a pretty incomplete analysis, but hopefully the document provides a good overview of some of the authoring features of Quarto !"
  },
  {
    "objectID": "content/posts/02-authoring/authoring.html#crossreferences",
    "href": "content/posts/02-authoring/authoring.html#crossreferences",
    "title": "Housing Prices",
    "section": "\n4 Crossreferences",
    "text": "4 Crossreferences\nWe present the results of exploratory data analysis in Section¬†2 and the regression model in Section¬†3 .\nFigure¬†2 displays the relationship between these two variables in a scatterplot.\nTable¬†1 displays basic summary statistics for these two variables.\nWe can fit a simple linear regression model of the form shown in Equation¬†1."
  },
  {
    "objectID": "content/posts/02-authoring/callout-boxes.html",
    "href": "content/posts/02-authoring/callout-boxes.html",
    "title": "Callout Boxes",
    "section": "",
    "text": ":::{.callout-note}\nNote that there are five types of callouts, including:\n`note`, `warning`, `important`, `tip`, and `caution`.\n:::\n\n:::{.callout-tip}\n## Tip With Caption\n\nThis is an example of a callout with a caption.\n:::\n\n:::{.callout-caution collapse=\"true\"}\n## Expand To Learn About Collapse\n\nThis is an example of a 'folded' caution callout that can be expanded by the user. You can use `collapse=\"true\"` to collapse it by default or `collapse=\"false\"` to make a collapsible callout that is expanded by default.\n:::"
  },
  {
    "objectID": "content/posts/02-authoring/figure-layout.html",
    "href": "content/posts/02-authoring/figure-layout.html",
    "title": "Figure Layouts",
    "section": "",
    "text": "# returns a tibble of the files\n# \nfs::dir_info(here::here(\"content/materials/images\"))\n\n# A tibble: 7 √ó 18\n  path             type     size permi‚Ä¶¬π modification_time   user  group devic‚Ä¶¬≤\n  <fs::path>       <fct> <fs::b> <fs::p> <dttm>              <chr> <chr>   <dbl>\n1 ‚Ä¶Terrier001.jpeg file   31.11K rw-     2022-12-29 16:20:01 <NA>  <NA>   2.93e9\n2 ‚Ä¶en_Forever.jpeg file   18.72K rw-     2022-12-29 16:20:01 <NA>  <NA>   2.93e9\n3 ‚Ä¶rrier_male.jpeg file   54.81K rw-     2022-12-29 16:20:01 <NA>  <NA>   2.93e9\n4 ‚Ä¶ages/avatar.jpg file    8.73K rw-     2022-09-17 06:19:54 <NA>  <NA>   2.93e9\n5 ‚Ä¶oston-sleep.png file  101.59K rw-     2022-12-29 16:20:01 <NA>  <NA>   2.93e9\n6 ‚Ä¶ton-terrier.png file   72.69K rw-     2022-12-29 16:20:01 <NA>  <NA>   2.93e9\n7 ‚Ä¶c_Library).jpeg file   32.09K rw-     2022-12-29 16:20:01 <NA>  <NA>   2.93e9\n# ‚Ä¶ with 10 more variables: hard_links <dbl>, special_device_id <dbl>,\n#   inode <dbl>, block_size <dbl>, blocks <dbl>, flags <int>, generation <dbl>,\n#   access_time <dttm>, change_time <dttm>, birth_time <dttm>, and abbreviated\n#   variable names ¬π‚Äãpermissions, ¬≤‚Äãdevice_id\n\n\n\n# returns paths/files\nfs::dir_ls(here::here(\"content/materials/images\"))\n\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/330px-BostonTerrier001.jpeg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/330px-Boston_Terrier_Eden_Forever.jpeg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/330px-Boston_Terrier_male.jpeg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/avatar.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/boston-sleep.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/boston-terrier.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/Terrier_Seated_(Boston_Public_Library).jpeg"
  },
  {
    "objectID": "content/posts/02-authoring/figure-layout.html#figures",
    "href": "content/posts/02-authoring/figure-layout.html#figures",
    "title": "Figure Layouts",
    "section": "Figures",
    "text": "Figures\nBasic markdown syntax:\n   ![Boston Terrier](images/boston-terrier.png)\n\n\nBoston Terrier"
  },
  {
    "objectID": "content/posts/02-authoring/figure-layout.html#fragments",
    "href": "content/posts/02-authoring/figure-layout.html#fragments",
    "title": "Figure Layouts",
    "section": "Fragments",
    "text": "Fragments\n\n\n   ![Boston terrier](images/boston-terrier.png){fig-align=\"left\"}\n\n\n![](images/boston-terrier.png){fig-align=\"right\" fig-alt=\"A photo a Boston Terrier.\"}"
  },
  {
    "objectID": "content/posts/02-authoring/figure-layout.html#subfigures",
    "href": "content/posts/02-authoring/figure-layout.html#subfigures",
    "title": "Figure Layouts",
    "section": "Subfigures",
    "text": "Subfigures\n::: {#fig-bostons layout-ncol=2}\n\n![Excited](images/boston-terrier.png){#fig-boston width=\"250px\"}\n\n![Sleeping](images/boston-sleep.png){#fig-sleep width=\"250px\"}\n\nTwo states of Howard\n\n:::"
  },
  {
    "objectID": "content/posts/02-authoring/figure-layout.html#subfigures-1",
    "href": "content/posts/02-authoring/figure-layout.html#subfigures-1",
    "title": "Figure Layouts",
    "section": "Subfigures",
    "text": "Subfigures\n\n\n\n\n\n(a) Excited\n\n\n\n\n\n\n(b) Sleeping\n\n\n\n\nFigure¬†1: Two states of Howard"
  },
  {
    "objectID": "content/posts/03-computation/inline-code.html",
    "href": "content/posts/03-computation/inline-code.html",
    "title": "Inline Code",
    "section": "",
    "text": "library(tidyverse)\n\n‚îÄ‚îÄ Attaching packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 1.3.2 ‚îÄ‚îÄ\n‚úî ggplot2 3.4.0      ‚úî purrr   1.0.0 \n‚úî tibble  3.1.8      ‚úî dplyr   1.0.10\n‚úî tidyr   1.2.1      ‚úî stringr 1.5.0 \n‚úî readr   2.1.3      ‚úî forcats 0.5.2 \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n\nlibrary(palmerpenguins)\nThe dataset contains 344 penguin size measurements from Adelie, Gentoo, Chinstrap species across Torgersen, Biscoe, Dream islands.."
  },
  {
    "objectID": "content/posts/03-computation/inline-code.html#what-about-formatting",
    "href": "content/posts/03-computation/inline-code.html#what-about-formatting",
    "title": "Inline Code",
    "section": "What about formatting?",
    "text": "What about formatting?\n\npen_summary <- penguins |> \n  group_by(species) |> \n  summarize(avg_mass = mean(body_mass_g, na.rm = TRUE))\n\nThe average body mass by species is 3700.6622517, 3733.0882353, 5076.0162602.\nWe can do better!\n\nbody_mass <- scales::label_number(big.mark = \",\", accuracy = 0.1, suffix = \"g\")(pull(pen_summary, avg_mass))\n\nbody_mass\n\n[1] \"3,700.7g\" \"3,733.1g\" \"5,076.0g\"\n\n\nThe average body mass by species is 3,700.7g, 3,733.1g, 5,076.0g.\nWe can still do better!\n\nmass_reporter <- glue::glue_collapse(body_mass, sep = \", \", last = \", and \")\n\nThe average body mass by species is 3,700.7g, 3,733.1g, and 5,076.0g."
  },
  {
    "objectID": "content/posts/03-computation/inline-code.html#reporting-with-lists",
    "href": "content/posts/03-computation/inline-code.html#reporting-with-lists",
    "title": "Inline Code",
    "section": "Reporting with lists",
    "text": "Reporting with lists\nCredit to TJ Mahr\n\nknitted <- list(\n  when = format(Sys.Date()),\n  with = system(\"quarto --version\", intern = TRUE)\n)\n\nReported prepared on 2023-01-05 with quarto version 1.2.280."
  },
  {
    "objectID": "content/posts/03-computation/visual-editor.html",
    "href": "content/posts/03-computation/visual-editor.html",
    "title": "Visual Editor for Quarto",
    "section": "",
    "text": "RStudio 2022.07.1 comes with support for the Visual Mode of editing Quarto and other markdown-based files!\nThis is a WYSIWYM editor, meaning:\n\nWYSIWYM is an acronym that stands for What you see is what you mean. This was positioned to not be confused with WYSIWYG (what you see is what you get). The idea behind WYSIWYG is to display text on screen in much the exact same way as they will appear when printed on paper.\n\nWYSIWYM means that it can be translated differently, (where) the same content can lead to different output formats .\n\nThe Visual Markdown mode in RStudio allows for editing in plain text or visual mode, along with a visual representation of what it will actually look like while maintaining the ability to output to HTML or PDF.\nFull guide guide from the RStudio dev team that covers all the major topics and sub topics of the new features.\n\n\nOS\nDownload\nSize\nSHA-256\n\n\n\nWindows 10/11\n\nRStudio-2022.07.1-554.exe(opens in a new tab)\n\n190.14 MB\n5ab6215b\n\n\nmacOS 10.15+\n\nRStudio-2022.07.1-554.dmg(opens in a new tab)\n\n221.04 MB\n7b1a2285\n\n\nUbuntu 18+/Debian 10+\n\nrstudio-2022.07.1-554-amd64.deb(opens in a new tab)\n\n132.91 MB\n74b9e751\n\n\nUbuntu 22\n\nrstudio-2022.07.1-554-amd64.deb(opens in a new tab)\n\n145.33 MB\n92f2ab75\n\n\nFedora 19/Red Hat 7\n\nrstudio-2022.07.1-554-x86_64.rpm(opens in a new tab)\n\n103.29 MB\n0fc15d16\n\n\nFedora 34/Red Hat 8\n\nrstudio-2022.07.1-554-x86_64.rpm(opens in a new tab)\n\n149.77 MB\n0c4ef334\n\n\nOpenSUSE 15\n\nrstudio-2022.07.1-554-x86_64.rpm(opens in a new tab)\n\n133.76 MB\n45f277d0\n\n\n\n\n\nMarkdown documents can be edited in either source or visual button at the top-right of the document toolbar (or alternatively the ‚åò + ‚áß + F4 keyboard shortcut):\n\n\nPlease see long section of the guide.\n\nIf you have a workflow that involves editing in both visual and source mode, you may want to ensure that the same markdown is written no matter which mode edits originate from. You can accomplish this using the canonical option. For example:\n---\ntitle: \"My Document\"\neditor_options:\n  markdown:\n    wrap: 72\n    references: \n      location: block\n    canonical: true\n---\n\nThe editor toolbar includes buttons for the most commonly used formatting commands:\n\nAdditional commands are available on the Format, Insert, and Table menus:\n\n\n\n\n\n\n\n\n\n\n\n\nCommand\nKeyboard Shortcut\nMarkdown Shortcut\n\n\n\nBold\n‚åò B\n**bold**\n\n\nItalic\n‚åò I\n*italic*\n\n\nCode\n‚åò D\n`code`\n\n\nLink\n‚åò K\n<href>\n\n\nHeading 1\n‚å•‚åò 1\n#\n\n\nHeading 2\n‚å•‚åò 2\n##\n\n\nHeading 3\n‚å•‚åò 3\n###\n\n\nR Code Chunk\n‚å•‚åò I\n```{r}\n\n\n\nYou can also use the catch-all ‚åò/ shortcut to insert just about anything. Just execute the shortcut then type what you want to insert. For example:\nUse the bullet\n\nOutput\n\nOr numbered\n\nNumber list!\n\nHere‚Äôs a link - how to turn into an image?\n\nEXAMPLE LINK: https://rstudio.github.io/visual-markdown-editing/images/visual-editing-omni-list.png"
  },
  {
    "objectID": "content/posts/03-computation/visual-editor.html#equations",
    "href": "content/posts/03-computation/visual-editor.html#equations",
    "title": "Visual Editor for Quarto",
    "section": "Equations",
    "text": "Equations\nLaTeX equations are authored using standard Pandoc markdown syntax (the editor will automatically recognize the syntax and treat the equation as math). When you aren‚Äôt directly editing an equation it will appear as rendered math:\n\\[\nP(E) = {n \\choose k} p^k (2-p)^{n-k}\n\\]\n\nFootnotes\n\nYou can include footnotes using the Insert -> Footnote command (or the ‚áß ‚åò F7 keyboard shortcut). Footnote editing occurs in a pane immediately below the main document:1 <- NOTE THE FOOTMARK"
  },
  {
    "objectID": "content/posts/03-computation/visual-editor.html#inserting-citations",
    "href": "content/posts/03-computation/visual-editor.html#inserting-citations",
    "title": "Visual Editor for Quarto",
    "section": "Inserting Citations",
    "text": "Inserting Citations\nYou insert citations by either using the Insert -> Citation command or by using markdown syntax directly (e.g.¬†[@cite]).\nCitations go inside square brackets and are separated by semicolons. Each citation must have a key, composed of ‚Äò@‚Äô + the citation identifier from the database, and may optionally have a prefix, a locator, and a suffix. The citation key must begin with a letter, digit, or _, and may contain alphanumerics, _, and internal punctuation characters (:.#$%&-+?<>~/). Here are some examples:\n\n(Rottman-Sagebiel et al. 2018)\nDEMO OF CITATION WITH INSERT"
  },
  {
    "objectID": "content/posts/03-computation/visual-editor.html#embedded-code",
    "href": "content/posts/03-computation/visual-editor.html#embedded-code",
    "title": "Visual Editor for Quarto",
    "section": "Embedded Code",
    "text": "Embedded Code\nSource code which you include in an Quarto document can either by for display only or can be executed by knitr as part of rendering. Code can furthermore be either inline or block (e.g.¬†an Rmd code chunk).\nDisplaying Code\nTo display but not execute code, either use the Insert -> Code Block menu item, or start a new line and type either:\n\n``` (for a plain code block); or\n```<lang> (where <lang> is a language) for a code block with syntax highlighting.\n\nThen press the Enter key. To display code inline, simply surround text with backticks (`code`), or use the Format -> Code menu item.\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\npenguins %>% \n  ggplot(aes(x = body_weight_g, y = flipper_length_mm, color = species)) +\n  geom_point()\nCode Chunks\nTo insert an executable code chunk, use the Insert -> Code Chunk menu item, or start a new line and type:\n```{r}\nThen press the Enter key. Note that r could be another language supported by knitr (e.g.¬†python or sql) and you can also include a chunk label and other chunk options.\nTo include inline R code, you just create normal inline code (e.g.¬†by using backticks or the ‚åò D shortcut) but preface it with r. For example, this inline code will be executed by knitr: 2023-01-05. Note that when the code displays in visual mode it won‚Äôt have the backticks (but they will still appear in source mode).\n\npenguin_plot <- penguins %>% \n  na.omit() %>% \n  ggplot(aes(x = body_mass_g, y = flipper_length_mm)) +\n  geom_point(aes(color = species)) +\n  labs(title = \"Important Penguins\") +\n  geom_smooth(method = \"lm\", color = \"black\")\n\npenguin_plot\n\n\n\n\nR generated Tables\n\nlibrary(gt)\n\npenguins %>% \n  na.omit() %>% \n  select(species, bill_length_mm, body_mass_g) %>% \n  head() %>% \n  gt()\n\n\n\n\n\n\nspecies\n      bill_length_mm\n      body_mass_g\n    \n\n\nAdelie\n39.1\n3750\n\n\nAdelie\n39.5\n3800\n\n\nAdelie\n40.3\n3250\n\n\nAdelie\n36.7\n3450\n\n\nAdelie\n39.3\n3650\n\n\nAdelie\n38.9\n3625\n\n\n\n\n\n\n\nlibrary(reactable)\npenguins %>% \n  filter(species == \"Adelie\") %>% \n  na.omit() %>% \n  select(species, bill_length_mm, body_mass_g) %>% \n  reactable(defaultPageSize = 5)"
  },
  {
    "objectID": "content/posts/04-static/bootswatch-themed.html",
    "href": "content/posts/04-static/bootswatch-themed.html",
    "title": "Bootswatch Themed QMD",
    "section": "",
    "text": "Plots\nThis is the world‚Äôs most amazing plot. Everyone loves penguins.\nPenguins are fancy.\nThere are 344 in the dataset of interest.\n\nShow the codepenguins %>% \n  ggplot(aes(x = bill_length_mm, y = body_mass_g, color = species)) +\n  geom_point()\n\n\n\n\nYou can also include tables. Tables are super cool. I love tables and I love penguins. I am subject to oversight by the penguin overlords. The eternals are friends with the penguins.\n\nShow the codepenguins %>% \n  na.omit() %>% \n  group_by(species, sex) %>% \n  rename(\n    body_mass = body_mass_g, bill_length = bill_length_mm, \n    bill_depth = bill_depth_mm\n    ) %>% \n  summarise(\n    n = n(),\n    across(\n      .cols = c(body_mass, bill_length, bill_depth),\n      .fns = list(mean = mean, sd = sd)\n    ),\n    .groups = \"drop\"\n  ) %>% \n  gt(rowname_col = \"sex\") %>% \n  cols_label(\n    n = \"N\", body_mass_mean = \"Mean\", body_mass_sd = \"SD\", \n    bill_length_mean= \"Mean\", bill_length_sd = \"SD\",\n    bill_depth_mean = \"Mean\", bill_depth_sd = \"SD\"\n    ) %>% \n  gt::tab_spanner(\n    label = \"Body Mass (g)\",\n    columns = 4:5\n  ) %>% \n  gt::tab_spanner(\n    label = \"Bill Length (mm)\",\n    columns = 6:7\n  ) %>% \n  gt::tab_spanner(\n    label = \"Bill Depth (mm)\",\n    columns = 8:9\n  ) %>% \n  fmt_number(\n    columns = c(where(is.numeric), -n)\n  )\n\n\n\n\n\n\n\n\n      species\n      N\n      \n        Body Mass (g)\n      \n      \n        Bill Length (mm)\n      \n      \n        Bill Depth (mm)\n      \n    \n\nMean\n      SD\n      Mean\n      SD\n      Mean\n      SD\n    \n\n\n\nfemale\nAdelie\n73\n3,368.84\n269.38\n37.26\n2.03\n17.62\n0.94\n\n\nmale\nAdelie\n73\n4,043.49\n346.81\n40.39\n2.28\n19.07\n1.02\n\n\nfemale\nChinstrap\n34\n3,527.21\n285.33\n46.57\n3.11\n17.59\n0.78\n\n\nmale\nChinstrap\n34\n3,938.97\n362.14\n51.09\n1.56\n19.25\n0.76\n\n\nfemale\nGentoo\n58\n4,679.74\n281.58\n45.56\n2.05\n14.24\n0.54\n\n\nmale\nGentoo\n61\n5,484.84\n313.16\n49.47\n2.72\n15.72\n0.74"
  },
  {
    "objectID": "content/posts/04-static/old-rmarkdown.html",
    "href": "content/posts/04-static/old-rmarkdown.html",
    "title": "Old Markdown - reactable example",
    "section": "",
    "text": "This is an example from reactable documentation - showing the Twitter followers of some politicians.\nExample adapted from {reactable} documentation\n\nlibrary(reactable)\nlibrary(htmltools)\nlibrary(dplyr)\n\ndata <- read.csv(\"https://glin.github.io/reactable/articles/twitter-followers/twitter_followers.csv\",\n                 stringsAsFactors = FALSE)\n\n# Render a bar chart with a label on the left\nbar_chart <- function(label, width = \"100%\", height = \"14px\", fill = \"#00bfc4\", background = NULL) {\n  bar <- div(style = list(background = fill, width = width, height = height))\n  chart <- div(style = list(flexGrow = 1, marginLeft = \"6px\", background = background), bar)\n  div(style = list(display = \"flex\", alignItems = \"center\"), label, chart)\n}\n\ntbl <- reactable(\n  data,\n  pagination = FALSE,\n  defaultSorted = \"exclusive_followers_pct\",\n  defaultColDef = colDef(headerClass = \"header\", align = \"left\"),\n  columns = list(\n    account = colDef(\n      cell = function(value) {\n        url <- paste0(\"https://twitter.com/\", value)\n        tags$a(href = url, target = \"_blank\", paste0(\"@\", value))\n      },\n      width = 150\n    ),\n    followers = colDef(\n      defaultSortOrder = \"desc\",\n      cell = function(value) {\n        width <- paste0(value * 100 / max(data$followers), \"%\")\n        value <- format(value, big.mark = \",\")\n        value <- format(value, width = 9, justify = \"right\")\n        bar <- div(\n          class = \"bar-chart\",\n          style = list(marginRight = \"6px\"),\n          div(class = \"bar\", style = list(width = width, backgroundColor = \"#3fc1c9\"))\n        )\n        div(class = \"bar-cell\", span(class = \"number\", value), bar)\n      }\n    ),\n    exclusive_followers_pct = colDef(\n      name = \"Exclusive Followers\",\n      defaultSortOrder = \"desc\",\n      cell = JS(\"function(cellInfo) {\n        // Format as percentage\n        const pct = (cellInfo.value * 100).toFixed(1) + '%'\n        // Pad single-digit numbers\n        let value = pct.padStart(5)\n        // Show % on first row only\n        if (cellInfo.viewIndex > 0) {\n          value = value.replace('%', ' ')\n        }\n        // Render bar chart\n        return (\n          '<div class=\\\"bar-cell\\\">' +\n            '<span class=\\\"number\\\">' + value + '</span>' +\n            '<div class=\\\"bar-chart\\\" style=\\\"background-color: #e1e1e1\\\">' +\n              '<div class=\\\"bar\\\" style=\\\"width: ' + pct + '; background-color: #fc5185\\\"></div>' +\n            '</div>' +\n          '</div>'\n        )\n      }\"),\n      html = TRUE\n    )\n  ),\n  compact = TRUE,\n  class = \"followers-tbl\"\n)\n\ntbl\n\n\n\n\n\n\n\n# Add Google Fonts to the page\ntags$link(href = \"https://fonts.googleapis.com/css?family=Karla:400,700|Fira+Mono&display=fallback\",\n          rel = \"stylesheet\")\n\n\n\n\n\n/* CSS for the R Markdown document, inserted through a ```{css} code chunk */\n\n/* Styles for the table container, title, and subtitle */\n.twitter-followers {\n  /* Center the table */\n  margin: 0 auto;\n  /* Reduce the table width */\n  width: 575px;\n  font-family: Karla, \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n}\n\n.followers-header {\n  margin: 8px 0;\n  font-size: 16px;\n}\n\n.followers-title {\n  font-size: 20px;\n  font-weight: 600;\n}\n\n/* Styles for the table */\n.followers-tbl {\n  font-size: 14px;\n  line-height: 18px;\n}\n\n.followers-tbl a {\n  color: inherit;\n}\n\n/* Styles for the column headers */\n.header {\n  border-bottom: 2px solid #555;\n  font-size: 13px;\n  font-weight: 400;\n  text-transform: uppercase;\n}\n\n.header:hover {\n  background-color: #eee;\n}\n\n/* Styles for the bar charts */\n.bar-cell {\n  display: flex;\n  align-items: center;\n}\n\n.number {\n  font-family: \"Fira Mono\", Consolas, Monaco, monospace;\n  font-size: 13.5px;\n  white-space: pre;\n}\n\n.bar-chart {\n  flex-grow: 1;\n  margin-left: 6px;\n  height: 14px;\n}\n\n.bar {\n  height: 100%;\n}"
  },
  {
    "objectID": "content/posts/04-static/penguin-report.html",
    "href": "content/posts/04-static/penguin-report.html",
    "title": "Penguins Distilled",
    "section": "",
    "text": "Diagram of penguin head with indication of bill length and bill depth.\n\n\nLiterate Programming\nPer Donald Knuth\n\nA programming paradigm introduced by Donald Knuth in which a computer program is given an explanation of its logic in a natural language, such as English, interspersed with snippets of macros and traditional source code, from which compilable source code can be generated.\n\nInitial explore\nWe can do a quick exploration of the data with skimr::skim(). This will report the counts of various variables, along with some basic descriptive statistics. The skimr package is fantastic for quickly getting a sense of your datasets.\nAhead of skimr there are 344 penguins in this dataset, and the unique species are Adelie, Gentoo, Chinstrap.\nPer the rOpenSci skimr docs:\n\nskimr provides a frictionless approach to summary statistics which conforms to the principle of least surprise, displaying summary statistics the user can skim quickly to understand their data. It handles different data types and returns a skim_df object which can be included in a pipeline or displayed nicely for the human reader.\n\n\npenguins %>% \n  group_by(species) %>% \n  skimr::skim() %>% \n  select(-contains(\"numeric.p\"))\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n344\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nspecies\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nspecies\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nisland\nAdelie\n0\n1.00\nFALSE\n3\nDre: 56, Tor: 52, Bis: 44\n\n\nisland\nChinstrap\n0\n1.00\nFALSE\n1\nDre: 68, Bis: 0, Tor: 0\n\n\nisland\nGentoo\n0\n1.00\nFALSE\n1\nBis: 124, Dre: 0, Tor: 0\n\n\nsex\nAdelie\n6\n0.96\nFALSE\n2\nfem: 73, mal: 73\n\n\nsex\nChinstrap\n0\n1.00\nFALSE\n2\nfem: 34, mal: 34\n\n\nsex\nGentoo\n5\n0.96\nFALSE\n2\nmal: 61, fem: 58\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nspecies\nn_missing\ncomplete_rate\nmean\nsd\nhist\n\n\n\nbill_length_mm\nAdelie\n1\n0.99\n38.79\n2.66\n‚ñÅ‚ñÜ‚ñá‚ñÜ‚ñÅ\n\n\nbill_length_mm\nChinstrap\n0\n1.00\n48.83\n3.34\n‚ñÇ‚ñá‚ñá‚ñÖ‚ñÅ\n\n\nbill_length_mm\nGentoo\n1\n0.99\n47.50\n3.08\n‚ñÉ‚ñá‚ñÜ‚ñÅ‚ñÅ\n\n\nbill_depth_mm\nAdelie\n1\n0.99\n18.35\n1.22\n‚ñÇ‚ñÜ‚ñá‚ñÉ‚ñÅ\n\n\nbill_depth_mm\nChinstrap\n0\n1.00\n18.42\n1.14\n‚ñÖ‚ñá‚ñá‚ñÜ‚ñÇ\n\n\nbill_depth_mm\nGentoo\n1\n0.99\n14.98\n0.98\n‚ñÖ‚ñá‚ñá‚ñÜ‚ñÇ\n\n\nflipper_length_mm\nAdelie\n1\n0.99\n189.95\n6.54\n‚ñÅ‚ñÜ‚ñá‚ñÖ‚ñÅ\n\n\nflipper_length_mm\nChinstrap\n0\n1.00\n195.82\n7.13\n‚ñÅ‚ñÖ‚ñá‚ñÖ‚ñÇ\n\n\nflipper_length_mm\nGentoo\n1\n0.99\n217.19\n6.48\n‚ñÇ‚ñá‚ñá‚ñÜ‚ñÉ\n\n\nbody_mass_g\nAdelie\n1\n0.99\n3700.66\n458.57\n‚ñÖ‚ñá‚ñá‚ñÉ‚ñÇ\n\n\nbody_mass_g\nChinstrap\n0\n1.00\n3733.09\n384.34\n‚ñÅ‚ñÖ‚ñá‚ñÉ‚ñÅ\n\n\nbody_mass_g\nGentoo\n1\n0.99\n5076.02\n504.12\n‚ñÉ‚ñá‚ñá‚ñá‚ñÇ\n\n\nyear\nAdelie\n0\n1.00\n2008.01\n0.82\n‚ñá‚ñÅ‚ñá‚ñÅ‚ñá\n\n\nyear\nChinstrap\n0\n1.00\n2007.97\n0.86\n‚ñá‚ñÅ‚ñÜ‚ñÅ‚ñá\n\n\nyear\nGentoo\n0\n1.00\n2008.08\n0.79\n‚ñÜ‚ñÅ‚ñá‚ñÅ‚ñá\n\n\n\n\n\nSpecific statistics\nWe can also explore specific statistics\nThe penguins split by species show a specific relationship between weight and flipper length, where the Adelie female penguins are the lighest and have the shortest flippers.\n\npenguins %>% \n  group_by(species, sex) %>% \n  summarize(\n    n = n(), \n    weight = mean(body_mass_g, na.rm = TRUE),\n    flipper_length = mean(flipper_length_mm, na.rm = TRUE)\n    ) %>% \n  arrange(desc(weight))\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 √ó 5\n# Groups:   species [3]\n  species   sex        n weight flipper_length\n  <fct>     <fct>  <int>  <dbl>          <dbl>\n1 Gentoo    male      61  5485.           222.\n2 Gentoo    female    58  4680.           213.\n3 Gentoo    <NA>       5  4588.           216.\n4 Adelie    male      73  4043.           192.\n5 Chinstrap male      34  3939.           200.\n6 Adelie    <NA>       6  3540            186.\n7 Chinstrap female    34  3527.           192.\n8 Adelie    female    73  3369.           188.\n\n\nLooks like the Adelie are the lightest penguin. I want to see their distribution along with the overall distribution.\n\npenguins %>% \n  filter(is.na(sex))\n\n# A tibble: 11 √ó 8\n   species island    bill_length_mm bill_depth_mm flipper_‚Ä¶¬π body_‚Ä¶¬≤ sex    year\n   <fct>   <fct>              <dbl>         <dbl>      <int>   <int> <fct> <int>\n 1 Adelie  Torgersen           NA            NA           NA      NA <NA>   2007\n 2 Adelie  Torgersen           34.1          18.1        193    3475 <NA>   2007\n 3 Adelie  Torgersen           42            20.2        190    4250 <NA>   2007\n 4 Adelie  Torgersen           37.8          17.1        186    3300 <NA>   2007\n 5 Adelie  Torgersen           37.8          17.3        180    3700 <NA>   2007\n 6 Adelie  Dream               37.5          18.9        179    2975 <NA>   2007\n 7 Gentoo  Biscoe              44.5          14.3        216    4100 <NA>   2007\n 8 Gentoo  Biscoe              46.2          14.4        214    4650 <NA>   2008\n 9 Gentoo  Biscoe              47.3          13.8        216    4725 <NA>   2009\n10 Gentoo  Biscoe              44.5          15.7        217    4875 <NA>   2009\n11 Gentoo  Biscoe              NA            NA           NA      NA <NA>   2009\n# ‚Ä¶ with abbreviated variable names ¬π‚Äãflipper_length_mm, ¬≤‚Äãbody_mass_g\n\n\n\nsmaller <- palmerpenguins::penguins %>% \n  filter(species == \"Adelie\", \n         !is.na(body_mass_g))\n\nCleanup the data\nIf you noticed above, there was some NA or missing data. We can remove those rows for now.\n\npenguins_clean <- penguins %>% \n  na.omit() %>% \n  mutate(species_num = as.numeric(species))\n\nPlot Section\nLet‚Äôs move on to some plots, for the overall distributions and for just the Adelie penguins. The overall distribution of the data by species shows some overlap in body weight for Adelie/Chinstrap, but more of a separation for the Gentoo penguins.\n\npenguins %>% \n  ggplot(aes(body_mass_g, fill = species)) + \n  geom_density(color = \"white\", alpha = 0.5) +\n  scale_fill_manual(values = c(\"darkorange\",\"purple\",\"cyan4\")) +\n  labs(x = \"Penguin Bins\")\n\n\n\n\nWhen we compare just within the Adelie penguins, we can see more of a specific separation of male vs female. However, there is still a decent amount of overlapping data.\n\npenguin_plot <- smaller %>% \n  filter(!is.na(sex)) %>% \n  ggplot(aes(body_mass_g, fill = sex)) + \n  geom_density(color = \"white\", alpha = 0.5) +\n    scale_fill_manual(values = c(\"darkorange\",\"purple\")) +\n  labs(x = \"Penguin Bins\")\n\npenguin_plot\n\n\n\n\nLastly we can fit a basic linear model comparing body weight in grams to the flipper length of the penguins by specific species. There is a strong linear relationship, although it‚Äôs a bit difficult to distinguish between Chinstrap and Adelie penguins.\n\npenguin_size_plot <- penguins_clean %>% \n  ggplot(aes(x = body_mass_g, y = flipper_length_mm, color = species)) + \n  scale_color_manual(values = c(\"darkorange\",\"purple\",\"cyan4\")) +\n  geom_point(size = 2, alpha = 0.5) +\n  labs(x = \"Mass (g)\", y = \"Flipper Length (mm)\") +\n  geom_smooth(aes(group = \"none\"), method = \"lm\")\n\npenguin_size_plot\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nWe can save the overall distribution and the linear model plot.\n\nggsave(\"penguin-dist.png\", penguin_plot, \n  dpi = \"retina\", height = 8, width = 8)\n\nggsave(\"penguin-smooth.png\", penguin_size_plot, \n  dpi = \"retina\", height = 8, width = 8)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nModeling section\nMoving on to some basic modeling we can see if what kind of relationships we observe in the data. Note that I‚Äôm really not following any plan, just indicating how you can fit some different models all at once with dplyr + broom.\n\nmodel_inputs <- tibble(\n  model_form = c(\n    list(flipper_length_mm ~ body_mass_g),\n    list(species_num ~ bill_length_mm + body_mass_g + sex),\n    list(flipper_length_mm ~ bill_length_mm + species)\n    ),\n  data = list(penguins_clean)\n) \n\nmodel_metrics <- model_inputs %>% \n  rowwise(model_form, data) %>% \n  summarize(lm = list(lm(model_form, data = data)), .groups = \"drop\") %>% \n  rowwise(model_form, lm, data) %>% \n  summarise(broom::glance(lm), .groups = \"drop\")\n\nWrap up\nWe can then take the model outcomes and throw them into a quick gt table.\n\nmodel_metrics %>% \n  select(model_form, r.squared:p.value) %>% \n  mutate(model_form = as.character(model_form)) %>% \n  gt::gt() %>% \n  gt::fmt_number(r.squared:statistic) %>% \n  gt::fmt_scientific(p.value) %>% \n  gt::cols_width(\n    model_form ~ px(150)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel_form\n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n    \n\n\nflipper_length_mm ~ body_mass_g\n0.76\n0.76\n6.85\n1,060.30\n3.13 √ó 10‚àí105\n\n\n\nspecies_num ~ bill_length_mm + body_mass_g + sex\n0.84\n0.84\n0.36\n583.59\n2.45 √ó 10‚àí131\n\n\n\nflipper_length_mm ~ bill_length_mm + species\n0.83\n0.83\n5.83\n529.22\n1.66 √ó 10‚àí125\n\n\n\n\n\n\n\nOverall, this was a quick overview of the beauty of literate programming. We have R code that is self-documenting, as we capture our thoughts and the outputs in a single document. We know at some level that the code works since it ‚Äúlogs‚Äù the outputs at various stages and could still output to additional log files. To render it has to run successfully in a linear fashion, and it is human readable as code, via the visual editor or even in version control like Git!"
  },
  {
    "objectID": "content/posts/05-presentations/fragments.html#make-these-columns-appear-in-order",
    "href": "content/posts/05-presentations/fragments.html#make-these-columns-appear-in-order",
    "title": "Presentation with Columns?",
    "section": "Make these columns appear in order",
    "text": "Make these columns appear in order\n\n\nThese appear first\n\nMake\nYour\nList\n\n\nThen this\n\n```{r}\nhead(mtcars)\n```\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1"
  },
  {
    "objectID": "content/posts/05-presentations/revealjs-penguins.html#section",
    "href": "content/posts/05-presentations/revealjs-penguins.html#section",
    "title": "Penguin Report Presentation",
    "section": "",
    "text": "Diagram of penguin head with indication of bill length and bill depth."
  },
  {
    "objectID": "content/posts/05-presentations/revealjs-penguins.html#literate-programming",
    "href": "content/posts/05-presentations/revealjs-penguins.html#literate-programming",
    "title": "Penguin Report Presentation",
    "section": "Literate Programming",
    "text": "Literate Programming\nPer Donald Knuth\n\nA programming paradigm introduced by Donald Knuth in which a computer program is given an explanation of its logic in a natural language, such as English, interspersed with snippets of macros and traditional source code, from which compilable source code can be generated."
  },
  {
    "objectID": "content/posts/05-presentations/revealjs-penguins.html#specific-statistics",
    "href": "content/posts/05-presentations/revealjs-penguins.html#specific-statistics",
    "title": "Penguin Report Presentation",
    "section": "Specific statistics",
    "text": "Specific statistics\n\npenguins %>% \n  group_by(species, sex) %>% \n  summarize(\n    n = n(), \n    weight = mean(body_mass_g, na.rm = TRUE),\n    flipper_length = mean(flipper_length_mm, na.rm = TRUE)\n    ) %>% \n  arrange(desc(weight))\n\n# A tibble: 8 √ó 5\n# Groups:   species [3]\n  species   sex        n weight flipper_length\n  <fct>     <fct>  <int>  <dbl>          <dbl>\n1 Gentoo    male      61  5485.           222.\n2 Gentoo    female    58  4680.           213.\n3 Gentoo    <NA>       5  4588.           216.\n4 Adelie    male      73  4043.           192.\n5 Chinstrap male      34  3939.           200.\n6 Adelie    <NA>       6  3540            186.\n7 Chinstrap female    34  3527.           192.\n8 Adelie    female    73  3369.           188."
  },
  {
    "objectID": "content/posts/05-presentations/revealjs-penguins.html#cleanup-the-data",
    "href": "content/posts/05-presentations/revealjs-penguins.html#cleanup-the-data",
    "title": "Penguin Report Presentation",
    "section": "Cleanup the data",
    "text": "Cleanup the data\nIf you noticed above, there was some NA or missing data. We can remove those rows for now.\n\npenguins_clean <- penguins %>% \n  na.omit() %>% \n  mutate(species_num = as.numeric(species))"
  },
  {
    "objectID": "content/posts/05-presentations/revealjs-penguins.html#plot-section",
    "href": "content/posts/05-presentations/revealjs-penguins.html#plot-section",
    "title": "Penguin Report Presentation",
    "section": "Plot Section",
    "text": "Plot Section\nLet‚Äôs move on to some plots, for the overall distributions and for just the Adelie penguins. The overall distribution of the data by species shows some overlap in body weight for Adelie/Chinstrap, but more of a separation for the Gentoo penguins."
  },
  {
    "objectID": "content/posts/05-presentations/revealjs-penguins.html#distribution",
    "href": "content/posts/05-presentations/revealjs-penguins.html#distribution",
    "title": "Penguin Report Presentation",
    "section": "Distribution",
    "text": "Distribution\n\npenguins %>% \n  ggplot(aes(body_mass_g, fill = species)) + \n  geom_density(color = \"white\", alpha = 0.5) +\n  scale_fill_manual(values = c(\"darkorange\",\"purple\",\"cyan4\")) +\n  labs(x = \"Penguin Bins\")"
  },
  {
    "objectID": "content/posts/05-presentations/revealjs-penguins.html#lm-scatter-plot",
    "href": "content/posts/05-presentations/revealjs-penguins.html#lm-scatter-plot",
    "title": "Penguin Report Presentation",
    "section": "LM + Scatter Plot",
    "text": "LM + Scatter Plot\n\npenguin_size_plot <- penguins_clean %>% \n  ggplot(aes(x = body_mass_g, y = flipper_length_mm, color = species)) + \n  scale_color_manual(values = c(\"darkorange\",\"purple\",\"cyan4\")) +\n  geom_point(size = 2, alpha = 0.5) +\n  labs(x = \"Mass (g)\", y = \"Flipper Length (mm)\") +\n  geom_smooth(aes(group = \"none\"), method = \"lm\")\n\npenguin_size_plot"
  },
  {
    "objectID": "content/posts/05-presentations/revealjs-penguins.html#modeling-section",
    "href": "content/posts/05-presentations/revealjs-penguins.html#modeling-section",
    "title": "Penguin Report Presentation",
    "section": "Modeling section",
    "text": "Modeling section\nMoving on to some basic modeling we can see if what kind of relationships we observe in the data. Note that I‚Äôm really not following any plan, just indicating how you can fit some different models all at once with dplyr + broom."
  },
  {
    "objectID": "content/posts/05-presentations/revealjs-penguins.html#section-1",
    "href": "content/posts/05-presentations/revealjs-penguins.html#section-1",
    "title": "Penguin Report Presentation",
    "section": "",
    "text": "model_inputs <- tibble(\n  model_form = c(\n    list(flipper_length_mm ~ body_mass_g),\n    list(species_num ~ bill_length_mm + body_mass_g + sex),\n    list(flipper_length_mm ~ bill_length_mm + species)\n    ),\n  data = list(penguins_clean)\n) \n\nmodel_metrics <- model_inputs %>% \n  rowwise(model_form, data) %>% \n  summarize(lm = list(lm(model_form, data = data)), .groups = \"drop\") %>% \n  rowwise(model_form, lm, data) %>% \n  summarise(broom::glance(lm), .groups = \"drop\")"
  },
  {
    "objectID": "content/posts/05-presentations/revealjs-penguins.html#section-2",
    "href": "content/posts/05-presentations/revealjs-penguins.html#section-2",
    "title": "Penguin Report Presentation",
    "section": "",
    "text": "model_metrics %>% \n  select(model_form, r.squared:p.value) %>% \n  mutate(model_form = as.character(model_form)) %>% \n  gt::gt() %>% \n  gt::fmt_number(r.squared:statistic) %>% \n  gt::fmt_scientific(p.value) %>% \n  gt::cols_width(\n    model_form ~ px(150)\n  )\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n  \n  \n  \n    \n      model_form\n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n    \n  \n  \n    flipper_length_mm ~ body_mass_g\n0.76\n0.76\n6.85\n1,060.30\n3.13 √ó 10‚àí105\n    species_num ~ bill_length_mm + body_mass_g + sex\n0.84\n0.84\n0.36\n583.59\n2.45 √ó 10‚àí131\n    flipper_length_mm ~ bill_length_mm + species\n0.83\n0.83\n5.83\n529.22\n1.66 √ó 10‚àí125"
  },
  {
    "objectID": "content/posts/07-visuals/figure-layout-2.html",
    "href": "content/posts/07-visuals/figure-layout-2.html",
    "title": "Figure Layouts-2",
    "section": "",
    "text": "::: {#fig-bostons layout-nrow=2}\n\n![Excited](boston-terrier.png){#fig-boston width=\"250px\"}\n\n![Sleeping](boston-sleep.png){#fig-sleep width=\"250px\"}\n\n![Still Excited](boston-terrier.png){#fig-boston width=\"250px\"}\n\n![Still sleeping](boston-sleep.png){#fig-sleep width=\"250px\"}\n\nTwo states of Howard, twice\n\n:::"
  },
  {
    "objectID": "content/posts/07-visuals/figure-layout-2.html#subfigures-1",
    "href": "content/posts/07-visuals/figure-layout-2.html#subfigures-1",
    "title": "Figure Layouts-2",
    "section": "Subfigures",
    "text": "Subfigures\n\n\n\n\n\n(a) Excited\n\n\n\n\n\n\n(b) Sleeping\n\n\n\n\n\n\n\n\nStill Excited\n\n\n\n\n\n\nStill sleeping\n\n\n\n\nFigure¬†1: Two states of Howard, twice"
  },
  {
    "objectID": "content/posts/07-visuals/figure-layout-2.html#subfigures-2",
    "href": "content/posts/07-visuals/figure-layout-2.html#subfigures-2",
    "title": "Figure Layouts-2",
    "section": "Subfigures",
    "text": "Subfigures\n\n\n\n\n\nExcited\n\n\n\n\n\n\nSleeping\n\n\n\n\n\n\nStill Excited\n\n\n\n\n\n\n\n\nStill sleeping\n\n\n\n\nTwo states of Howard, twice"
  },
  {
    "objectID": "content/posts/07-visuals/figure-layout.html",
    "href": "content/posts/07-visuals/figure-layout.html",
    "title": "Figure Layout",
    "section": "",
    "text": "library(ggplot2)"
  },
  {
    "objectID": "content/posts/07-visuals/figure-layout.html#figures",
    "href": "content/posts/07-visuals/figure-layout.html#figures",
    "title": "Figure Layout",
    "section": "Figures",
    "text": "Figures\nBasic markdown syntax:\n         ![Boston Terrier](boston-terrier.png)\n\n\nBoston Terrier"
  },
  {
    "objectID": "content/posts/07-visuals/figure-layout.html#fragments",
    "href": "content/posts/07-visuals/figure-layout.html#fragments",
    "title": "Figure Layout",
    "section": "Fragments",
    "text": "Fragments\n\n\n       ![Boston terrier](boston-terrier.png){fig-align=\"left\"}\n\n\n\n ![](boston-terrier.png){fig-align=\"right\" fig-alt=\"A photo a Boston Terrier.\"}"
  },
  {
    "objectID": "content/posts/07-visuals/figure-layout.html#subfigures",
    "href": "content/posts/07-visuals/figure-layout.html#subfigures",
    "title": "Figure Layout",
    "section": "Subfigures",
    "text": "Subfigures\n\n\n\n\n![Excited](boston-terrier.png){#fig-boston width=\"250px\"}\n\n![Sleeping](boston-sleep.png){#fig-sleep width=\"250px\"}\n\nTwo states of Howard\n\n\n\nFigure¬†1: ?(caption)"
  },
  {
    "objectID": "content/posts/07-visuals/figure-layout.html#subfigures-1",
    "href": "content/posts/07-visuals/figure-layout.html#subfigures-1",
    "title": "Figure Layout",
    "section": "Subfigures",
    "text": "Subfigures\n\n\n\n\n\n(a) Excited\n\n\n\n\n\n\n(b) Sleeping\n\n\n\n\nFigure¬†2: Two states of Howard"
  },
  {
    "objectID": "content/posts/07-visuals/figure-layout.html#custom-grid",
    "href": "content/posts/07-visuals/figure-layout.html#custom-grid",
    "title": "Figure Layout",
    "section": "Custom grid",
    "text": "Custom grid\n\n\nThis column takes 1/2 of the page\n\nggplot(mtcars, aes(x = cyl, y = disp)) +\n  geom_boxplot()\n\nWarning: Continuous x aesthetic\n‚Ñπ did you forget `aes(group = ...)`?\n\n\n\n\n\n\n\nThis column takes 1/2 of the page\n\nhead(mtcars, 5)[1:4] |> knitr::kable()\n\n\n\n\nmpg\ncyl\ndisp\nhp\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n\n\nDatsun 710\n22.8\n4\n108\n93\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n\n\nHornet Sportabout\n18.7\n8\n360\n175"
  },
  {
    "objectID": "content/posts/07-visuals/gt-summary.html#introduction",
    "href": "content/posts/07-visuals/gt-summary.html#introduction",
    "title": "Overview of using gtsummary\n",
    "section": "Introduction",
    "text": "Introduction\nReproducible reports are an important part of good practices. We often need to report the results from a table in the text of an Quarto report. Inline reporting has been made simple with inline_text(). The inline_text() function reports statistics from {gtsummary} tables inline in an Quarto report."
  },
  {
    "objectID": "content/posts/07-visuals/gt-summary.html#setup",
    "href": "content/posts/07-visuals/gt-summary.html#setup",
    "title": "Overview of using gtsummary\n",
    "section": "Setup",
    "text": "Setup\nBefore going through the tutorial, install and load {gtsummary}.\n\n# install.packages(\"gtsummary\")\nlibrary(gtsummary)"
  },
  {
    "objectID": "content/posts/07-visuals/gt-summary.html#example-data-set",
    "href": "content/posts/07-visuals/gt-summary.html#example-data-set",
    "title": "Overview of using gtsummary\n",
    "section": "Example data set",
    "text": "Example data set\nWe‚Äôll be using the trial data set throughout this example.\n\nThis set contains data from 200 patients who received one of two types of chemotherapy (Drug A or Drug B). The outcomes are tumor response and death.\n\nFor brevity in the tutorial, let‚Äôs keep a subset of the variables from the trial data set.\n\ntrial2 <-\n  trial %>%\n  select(trt, marker, stage)"
  },
  {
    "objectID": "content/posts/07-visuals/gt-summary.html#inline_text_tbl_summary",
    "href": "content/posts/07-visuals/gt-summary.html#inline_text_tbl_summary",
    "title": "Overview of using gtsummary\n",
    "section": "Inline results from tbl_summary()",
    "text": "Inline results from tbl_summary()\nFirst create a basic summary table using tbl_summary() (review tbl_summary() vignette for detailed overview of this function if needed).\n\ntab1 <- tbl_summary(trial2, by = trt)\ntab1\n\n\n\n\n\n\nCharacteristic\n      \nDrug A, N = 981\n\n      \nDrug B, N = 1021\n\n    \n\n\nMarker Level (ng/mL)\n0.84 (0.24, 1.57)\n0.52 (0.19, 1.20)\n\n\n¬†¬†¬†¬†Unknown\n6\n4\n\n\nT Stage\n\n\n\n\n¬†¬†¬†¬†T1\n28 (29%)\n25 (25%)\n\n\n¬†¬†¬†¬†T2\n25 (26%)\n29 (28%)\n\n\n¬†¬†¬†¬†T3\n22 (22%)\n21 (21%)\n\n\n¬†¬†¬†¬†T4\n23 (23%)\n27 (26%)\n\n\n\n\n1 Median (IQR); n (%)\n    \n\n\n\n\nTo report the median (IQR) of the marker levels in each group, use the following commands inline.\n\nThe median (IQR) marker level in the Drug A and Drug B groups are 0.84 (0.24, 1.57) and 0.52 (0.19, 1.20), respectively. Here‚Äôs how the line will appear in your report.\n\n\nThe median (IQR) marker level in the Drug A and Drug B groups are 0.84 (0.24, 1.57) and 0.52 (0.19, 1.20), respectively. If you display a statistic from a categorical variable, include the level argument.\n\n\n25 (25%) resolves to ‚Äú25 (25%)‚Äù"
  },
  {
    "objectID": "content/posts/07-visuals/gt-summary.html#inline_text_tbl_regression",
    "href": "content/posts/07-visuals/gt-summary.html#inline_text_tbl_regression",
    "title": "Overview of using gtsummary\n",
    "section": "Inline results from tbl_regression()",
    "text": "Inline results from tbl_regression()\nSimilar syntax is used to report results from tbl_regression() and tbl_uvregression() tables. Refer to the tbl_regression() vignette if you need detailed guidance on using these functions.\nLet‚Äôs first create a regression model.\n\n# build logistic regression model\nm1 <- glm(response ~ age + stage, trial, family = binomial(link = \"logit\"))\n\nNow summarize the results with tbl_regression(); exponentiate to get the odds ratios.\n\ntbl_m1 <- tbl_regression(m1, exponentiate = TRUE)\ntbl_m1\n\n\n\n\n\n\nCharacteristic\n      \nOR1\n\n      \n95% CI1\n\n      p-value\n    \n\n\nAge\n1.02\n1.00, 1.04\n0.091\n\n\nT Stage\n\n\n\n\n\n¬†¬†¬†¬†T1\n‚Äî\n‚Äî\n\n\n\n¬†¬†¬†¬†T2\n0.58\n0.24, 1.37\n0.2\n\n\n¬†¬†¬†¬†T3\n0.94\n0.39, 2.28\n0.9\n\n\n¬†¬†¬†¬†T4\n0.79\n0.33, 1.90\n0.6\n\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n    \n\n\n\n\nTo report the result for age, use the following commands inline.\n\n1.02 (95% CI 1.00, 1.04; p=0.091) Here‚Äôs how the line will appear in your report.\n\n\n1.02 (95% CI 1.00, 1.04; p=0.091) It is reasonable that you‚Äôll need to modify the text. To do this, use the pattern argument. The pattern argument syntax follows glue::glue() format with referenced R objects being inserted between curly brackets. The default is pattern = \"{estimate} ({conf.level*100}% CI {conf.low}, {conf.high}; {p.value})\". You have access the to following fields within the pattern argument.\n\n\n\n\n\n\n\n\nParameter\n      Description\n    \n\n\n\n{estimate}\n\nprimary estimate (e.g. model coefficient, odds ratio)\n\n\n\n{conf.low}\n\nlower limit of confidence interval\n\n\n\n{conf.high}\n\nupper limit of confidence interval\n\n\n\n{p.value}\n\np-value\n\n\n\n{conf.level}\n\nconfidence level of interval\n\n\n\n{N}\n\nnumber of observations\n\n\n\n\n\n\n\nAge was not significantly associated with tumor response (OR 1.02; 95% CI 1.00, 1.04; p=0.091). Age was not significantly associated with tumor response (OR 1.02; 95% CI 1.00, 1.04; p=0.091). If you‚Äôre printing results from a categorical variable, include the level argument, e.g. inline_text(tbl_m1, variable = stage, level = \"T3\") resolves to ‚Äú0.94 (95% CI 0.39, 2.28; p=0.9)‚Äù.\n\nThe inline_text function has arguments for rounding the p-value (pvalue_fun) and the coefficients and confidence interval (estimate_fun). These default to the same rounding performed in the table, but can be modified when reporting inline.\nFor more details about inline code, review to the RStudio documentation page."
  },
  {
    "objectID": "content/posts/07-visuals/plot-layout.html",
    "href": "content/posts/07-visuals/plot-layout.html",
    "title": "Plot Layout",
    "section": "",
    "text": "library(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union"
  },
  {
    "objectID": "content/posts/07-visuals/plot-layout.html#figure-layout-ncol",
    "href": "content/posts/07-visuals/plot-layout.html#figure-layout-ncol",
    "title": "Plot Layout",
    "section": "Figure layout, ncol",
    "text": "Figure layout, ncol\n\n```{r}\n#| layout-ncol: 2\n#| fig-cap:\n#|   - \"Speed and Stopping Distances of Cars\"\n#|   - \"Engine displacement and fuel efficiency in Cars\"\n\ncars %>% \n  ggplot(aes(x = speed, y = dist)) +\n  geom_point()\n\nmtcars %>% \n  ggplot(aes(x = disp, y = mpg)) +\n  geom_point()\n```\n\n\n\n\n\nSpeed and Stopping Distances of Cars\n\n\n\n\n\n\nEngine displacement and fuel efficiency in Cars"
  },
  {
    "objectID": "content/posts/07-visuals/plot-layout.html#subcaptions",
    "href": "content/posts/07-visuals/plot-layout.html#subcaptions",
    "title": "Plot Layout",
    "section": "Subcaptions:",
    "text": "Subcaptions:\n\n```{r}\n#| label: fig-charts\n#| fig-cap: Charts\n#| fig-subcap:\n#|   - \"Cars\"\n#|   - \"mtcars\"\n#| layout-ncol: 2\n\ncars %>% \n  ggplot(aes(x = speed, y = dist)) +\n  geom_point()\n\nmtcars %>% \n  ggplot(aes(x = disp, y = mpg)) +\n  geom_point()\n```\n\n\n\n\n\n(a) Cars\n\n\n\n\n\n\n(b) mtcars\n\n\n\n\nFigure¬†1: Charts"
  },
  {
    "objectID": "content/posts/07-visuals/plot-layout.html#figure-layout-custom",
    "href": "content/posts/07-visuals/plot-layout.html#figure-layout-custom",
    "title": "Plot Layout",
    "section": "Figure layout, custom",
    "text": "Figure layout, custom\n\n```{r}\n#| layout: [[45,-10, 45], [50, 50]]\n#| fig-height: 5\n#| fig-align: center\n#| message: false\n#| warning: false\n#| dpi: 300\n\ncars %>% \n  ggplot(aes(x = speed, y = dist)) +\n  geom_smooth() +\n  theme(text = element_text(size = 20))\n\ncars %>% \n  ggplot(aes(x = speed, y = dist)) +\n  geom_point()+\n  theme(text = element_text(size = 20))\n\nmtcars %>% \n  ggplot(aes(x = disp, y = mpg)) +\n  geom_point()+\n  theme(text = element_text(size = 20))\n\nmtcars %>% \n  ggplot(aes(x = cyl, y = mpg, group = cyl, color = factor(cyl))) +\n  geom_boxplot() +\n  geom_jitter() +\n  theme(legend.position = \"none\")+\n  theme(text = element_text(size = 20))\n```"
  },
  {
    "objectID": "content/posts/07-visuals/plots.html",
    "href": "content/posts/07-visuals/plots.html",
    "title": "Plots",
    "section": "",
    "text": "library(palmerpenguins)\nlibrary(ggplot2)\nggplot2::theme_set(ggplot2::theme_minimal())\npenguins <- na.omit(penguins)"
  },
  {
    "objectID": "content/posts/07-visuals/plots.html#ggplot2",
    "href": "content/posts/07-visuals/plots.html#ggplot2",
    "title": "Plots",
    "section": "ggplot2",
    "text": "ggplot2\nCredit to Alison Hill + Allison Horst\n\n```{r}\nmass_flipper <- ggplot(data = penguins, \n                       aes(x = flipper_length_mm,\n                           y = body_mass_g)) +\n  geom_point(aes(color = species, \n                 shape = species),\n             size = 3,\n             alpha = 0.8) +\n  scale_color_manual(values = c(\"darkorange\",\"purple\",\"cyan4\")) +\n  labs(title = \"Penguin size, Palmer Station LTER\",\n       subtitle = \"Flipper length and body mass for Adelie, Chinstrap and Gentoo Penguins\",\n       x = \"Flipper length (mm)\",\n       y = \"Body mass (g)\",\n       color = \"Penguin species\",\n       shape = \"Penguin species\") +\n  theme(legend.position = c(0.2, 0.7),\n        plot.title.position = \"plot\",\n        plot.caption = element_text(hjust = 0, face= \"italic\"),\n        plot.caption.position = \"plot\",\n        plot.background = element_rect(color = \"black\"))\n```"
  },
  {
    "objectID": "content/posts/07-visuals/plots.html#basic-plot",
    "href": "content/posts/07-visuals/plots.html#basic-plot",
    "title": "Plots",
    "section": "Basic plot",
    "text": "Basic plot\n\n```{r}\nmass_flipper\n```"
  },
  {
    "objectID": "content/posts/07-visuals/plots.html#basic-plot-1",
    "href": "content/posts/07-visuals/plots.html#basic-plot-1",
    "title": "Plots",
    "section": "Basic plot",
    "text": "Basic plot\nIncreasing the width/DPI only affects the scaling of the image, it will not overflow.\n\n```{r}\n#| fig-width: 10\n#| fig-height: 4\n#| fig-dpi: 600\nmass_flipper\n```"
  },
  {
    "objectID": "content/posts/07-visuals/plots.html#aside",
    "href": "content/posts/07-visuals/plots.html#aside",
    "title": "Plots",
    "section": "Aside",
    "text": "Aside\n\n\n\n\n\n\nThe palmerpenguins R package contains two datasets that we believe are a viable alternative to Anderson‚Äôs Iris data (see datasets::iris). In this introductory vignette, we‚Äôll highlight some of the properties of these datasets that make them useful for statistics and data science education, as well as software documentation and testing."
  },
  {
    "objectID": "content/posts/07-visuals/plots.html#overflow-content",
    "href": "content/posts/07-visuals/plots.html#overflow-content",
    "title": "Plots",
    "section": "Overflow Content",
    "text": "Overflow Content\nThere are many options for overflow, either left/right\n\n```{r}\n#| column: body-outset-right\n#| fig-width: 10\nmass_flipper\n```\n\n\n\n\n\n```{r}\n#| column: screen-inset-right\n#| fig-width: 10\nmass_flipper\n```\n\n\n\n\n\n```{r}\n#| column: page-inset-left\n#| fig-width: 10\nmass_flipper\n```\n\n\n\n\n\n```{r}\n#| column: screen-left\n#| fig-width: 10\nmass_flipper\n```"
  },
  {
    "objectID": "content/posts/07-visuals/stat-html.html",
    "href": "content/posts/07-visuals/stat-html.html",
    "title": "gtsummary + R Markdown",
    "section": "",
    "text": "library(gtsummary)\nlibrary(tidyverse)\nlibrary(survival)"
  },
  {
    "objectID": "content/posts/07-visuals/stat-html.html#gtsummary-tables",
    "href": "content/posts/07-visuals/stat-html.html#gtsummary-tables",
    "title": "gtsummary + R Markdown",
    "section": "gtsummary tables",
    "text": "gtsummary tables\nTables created with {gtsummary} can be integrated into R markdown documents. The {gtsummary} package was written to be a companion to the {gt} package from RStudio, and {gtsummary} tables are printed using {gt} when possible. Currently, {gt} supports HTML output, with LaTeX and RTF planned for the future.\n\npatient_characteristics <-\n  trial %>%\n  select(trt, age, grade, response) %>%\n  tbl_summary(by = trt)  \npatient_characteristics\n\n\n\n\n\n\nCharacteristic\n      \nDrug A, N = 981\n\n      \nDrug B, N = 1021\n\n    \n\n\nAge\n46 (37, 59)\n48 (39, 56)\n\n\n¬†¬†¬†¬†Unknown\n7\n4\n\n\nGrade\n\n\n\n\n¬†¬†¬†¬†I\n35 (36%)\n33 (32%)\n\n\n¬†¬†¬†¬†II\n32 (33%)\n36 (35%)\n\n\n¬†¬†¬†¬†III\n31 (32%)\n33 (32%)\n\n\nTumor Response\n28 (29%)\n33 (34%)\n\n\n¬†¬†¬†¬†Unknown\n3\n4\n\n\n\n\n1 Median (IQR); n (%)\n    \n\n\n\n\nWith HTML output, you can include complex tables with footnotes, indentation, and spanning table headers.\n\n# Side-by-side Regression Models\n# logistic regression model\nt1 <-\n  glm(response ~ trt + grade + age, trial, family = binomial) %>%\n  tbl_regression(exponentiate = TRUE)\n# time to death Cox model\nt2 <-\n  coxph(Surv(ttdeath, death) ~ trt + grade + age, trial) %>%\n  tbl_regression(exponentiate = TRUE)\n\n# printing merged table\ntbl_merge(\n  tbls = list(t1, t2),\n  tab_spanner = c(\"**Tumor Response**\", \"**Time to Death**\")\n)\n\n\n\n\n\n\n\nCharacteristic\n      \n        Tumor Response\n      \n      \n        Time to Death\n      \n    \n\n\nOR1\n\n      \n95% CI1\n\n      p-value\n      \nHR1\n\n      \n95% CI1\n\n      p-value\n    \n\n\n\nChemotherapy Treatment\n\n\n\n\n\n\n\n\n¬†¬†¬†¬†Drug A\n‚Äî\n‚Äî\n\n‚Äî\n‚Äî\n\n\n\n¬†¬†¬†¬†Drug B\n1.13\n0.60, 2.13\n0.7\n1.30\n0.88, 1.92\n0.2\n\n\nGrade\n\n\n\n\n\n\n\n\n¬†¬†¬†¬†I\n‚Äî\n‚Äî\n\n‚Äî\n‚Äî\n\n\n\n¬†¬†¬†¬†II\n0.85\n0.39, 1.85\n0.7\n1.21\n0.73, 1.99\n0.5\n\n\n¬†¬†¬†¬†III\n1.01\n0.47, 2.15\n>0.9\n1.79\n1.12, 2.86\n0.014\n\n\nAge\n1.02\n1.00, 1.04\n0.10\n1.01\n0.99, 1.02\n0.3\n\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval, HR = Hazard Ratio"
  },
  {
    "objectID": "content/posts/07-visuals/stat-html.html#inline-reporting",
    "href": "content/posts/07-visuals/stat-html.html#inline-reporting",
    "title": "gtsummary + R Markdown",
    "section": "inline reporting",
    "text": "inline reporting\nAny number/statistic from a {gtsummary} table can be reported inline in a R markdown document using the inline_text() function. See example below:\n\nAmong patients who received Drug A, 31 (32%) had grade III tumors.\n\nFor detailed examples using functions from {gtsummary}, visit the {gtsummary} website."
  },
  {
    "objectID": "content/posts/08-knitr/fa-example.html",
    "href": "content/posts/08-knitr/fa-example.html",
    "title": "Font Awesome Quarto Extension",
    "section": "",
    "text": "This extension allows you to use font-awesome icons in your Quarto HTML and PDF documents. It provides an {{< fa >}} shortcode:\n{{< fa icon-name >}}\nFor example:\n\n\nShortcode\nIcon\n\n\n\n{{< fa thumbs-up >}}\n\n\n\n{{< fa folder >}}\n\n\n\n{{< fa chess-pawn >}}\n\n\n\n{{< fa brands bluetooth >}}"
  },
  {
    "objectID": "content/posts/08-knitr/lightbox-extension.html",
    "href": "content/posts/08-knitr/lightbox-extension.html",
    "title": "Example Lightbox Document",
    "section": "",
    "text": "Here is a simple image with a description. This also overrides the description position and places it to the left of the image.\n\n\nBeach in Chilmark"
  },
  {
    "objectID": "content/posts/08-knitr/lightbox-extension.html#elsewhere",
    "href": "content/posts/08-knitr/lightbox-extension.html#elsewhere",
    "title": "Example Lightbox Document",
    "section": "Elsewhere",
    "text": "Elsewhere\nThe below demonstrates placing more than one image in a gallery. Note the usage of the layout-ncol which arranges the images on the page side by date. Adding the group attribute to the markdown images places the images in a gallery grouped together based upon the group name provided.\n\n\n\n\nAquinnah\n\n\n\n\nOak Bluffs\n\n\n\n\n\n\nVineyard lighthouse"
  },
  {
    "objectID": "content/posts/08-knitr/lightbox-extension.html#credits",
    "href": "content/posts/08-knitr/lightbox-extension.html#credits",
    "title": "Example Lightbox Document",
    "section": "Credits",
    "text": "Credits\nThe images in this example were used under the Unsplash license, view originals below:\n\nChilmark Beach\nAquinnah\nGingerbread House\nEdgartown Light\nEdgartown Sailboat"
  },
  {
    "objectID": "content/posts/08-knitr/penguin-params.html",
    "href": "content/posts/08-knitr/penguin-params.html",
    "title": "Penguins Parametric Reports",
    "section": "",
    "text": "We have data about 344 penguins. Only 193 are classified asAdelie. The distribution of the Adelie penguins are shown below:"
  },
  {
    "objectID": "content/posts/09-using-lordicons/index.html",
    "href": "content/posts/09-using-lordicons/index.html",
    "title": "Using Lordicons, Fontawesome Icons,Academicons, and Iconify Icons",
    "section": "",
    "text": "This is just a compilation of the example files from the Quarto website, so that I can have ready-made code to copy and paste."
  },
  {
    "objectID": "content/posts/09-using-lordicons/index.html#installation",
    "href": "content/posts/09-using-lordicons/index.html#installation",
    "title": "Using Lordicons, Fontawesome Icons,Academicons, and Iconify Icons",
    "section": "Installation",
    "text": "Installation\nType these in your Terminal:\n\nIconify: quarto install extension mcanouil/quarto-iconify\n\nFontAwesome: quarto install extension quarto-ext/fontawesome\n\nLordicons: quarto install extension jmgirard/lordicon\n\nAcademicons: quarto install extension schochastics/academicons\n\n\nThese extensions allows you to use a variety of icons in your Quarto HTML documents."
  },
  {
    "objectID": "content/posts/09-using-lordicons/index.html#using-lordicon-shortcodes",
    "href": "content/posts/09-using-lordicons/index.html#using-lordicon-shortcodes",
    "title": "Using Lordicons, Fontawesome Icons,Academicons, and Iconify Icons",
    "section": "Using Lordicon Shortcodes",
    "text": "Using Lordicon Shortcodes\nThe {{< li >}} shortcode renders an icon (specified by its code) after downloading it the lordicon CDN. The {{< lif >}} shortcode renders an icon (specified by its filepath) from a local .json file. Both shortcodes support the same arguments for customization, described below.\n\n\n\n\n\n\n\nPseudocode\nExample Code\nRendered\n\n\n\n{{< li code >}}\n{{< li wlpxtupd >}}\n\n\n\n{{< lif file >}}\n{{< lif church.json >}}\n\n\n\n\nTriggers\ntrigger controls the icon‚Äôs animation type. When using the loop or loop-on-hover triggers, you can also set an optional delay (in ms) between loops.\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n{{< li wxnxiano >}}\n\n\n\n{{< li wxnxiano trigger=click >}}\n\n\n\n{{< li wxnxiano trigger=hover >}}\n\n\n\n{{< li wxnxiano trigger=loop >}}\n\n\n\n{{< li wxnxiano trigger=loop delay=1000 >}}\n\n\n\n{{< li wxnxiano trigger=loop-on-hover >}}\n\n\n\n{{< li wxnxiano trigger=loop-on-hover delay=1000 >}}\n\n\n\n{{< li wxnxiano trigger=morph >}}\n\n\n\n{{< li wxnxiano trigger=boomerang >}}\n\n\n\nSpeed\nspeed controls how quickly the icon‚Äôs animation plays.\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n{{< li lupuorrc trigger=loop speed=0.5 >}}\n\n\n\n{{< li lupuorrc trigger=loop speed=1.0 >}}\n\n\n\n{{< li lupuorrc trigger=loop speed=2.0 >}}\n\n\n\nColors\ncolors controls the icon‚Äôs coloring. Outline icons typically have just a primary and secondary color, but flat and lineal icons can have many more. Each color should be given in rank:color format (where ranks are primary, secondary, tertiary, etc.) and multiple colors should be separated by commas. Colors can be given in HTML color names or hexcodes.\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n{{< li lupuorrc >}}\n\n\n\n{{< li lupuorrc colors=primary:gold >}}\n\n\n\n{{< li lupuorrc colors=primary:gray,secondary:orange >}}\n\n\n\n{{< li lupuorrc colors=primary:#4030e8,secondary:#ee66aa >}}\n\n\n\nStroke\nstroke controls how thick the lines in an icon are.\n\n\nShortcode\nIcon\n\n\n\n{{< li lupuorrc stroke=50 >}}\n\n\n\n{{< li lupuorrc stroke=100 >}}\n\n\n\n{{< li lupuorrc stroke=150 >}}\n\n\n\nScale\nscale controls how large or zoomed in the icon is.\n\n\nShortcode\nIcon\n\n\n\n{{< li lupuorrc scale=25 >}}\n\n\n\n{{< li lupuorrc scale=50 >}}\n\n\n\n{{< li lupuorrc scale=100 >}}\n\n\n\nAxis X\nx controls the horizontal position of the center of the icon.\n\n\nShortcode\nIcon\n\n\n\n{{< li lupuorrc x=25 >}}\n\n\n\n{{< li lupuorrc x=50 >}}\n\n\n\n{{< li lupuorrc x=100 >}}\n\n\n\nAxis Y\ny controls the vertical position of the center of the icon.\n\n\nShortcode\nIcon\n\n\n\n{{< li lupuorrc y=25 >}}\n\n\n\n{{< li lupuorrc y=50 >}}\n\n\n\n{{< li lupuorrc y=100 >}}"
  },
  {
    "objectID": "content/posts/09-using-lordicons/index.html#using-academicons-shortcodes",
    "href": "content/posts/09-using-lordicons/index.html#using-academicons-shortcodes",
    "title": "Using Lordicons, Fontawesome Icons,Academicons, and Iconify Icons",
    "section": "Using Academicons Shortcodes",
    "text": "Using Academicons Shortcodes\nThis extension allows you to use academicons in your Quarto HTML documents. It provides an {{< ai >}} shortcode:\n\n\nMandatory <icon-name>:\n{{< ai <icon-name> >}}\n\n\nOptional <size=...>:\n{{< ai <icon-name> <size=...> >}}\n\n\nFor example:\n\n\nShortcode\nIcon\n\n\n\n{{< ai arxiv >}}\n\n\n\n{{< ai google-scholar >}}\n\n\n\n{{< ai open-access >}}\n\n\n\n{{< ai open-access size=5x >}}"
  },
  {
    "objectID": "content/posts/09-using-lordicons/index.html#using-fontawesome-icons",
    "href": "content/posts/09-using-lordicons/index.html#using-fontawesome-icons",
    "title": "Using Lordicons, Fontawesome Icons,Academicons, and Iconify Icons",
    "section": "Using Fontawesome Icons",
    "text": "Using Fontawesome Icons\nThis extension allows you to use font-awesome icons in your Quarto HTML and PDF documents. It provides an {{< fa >}} shortcode:\n\n\nMandatory <icon-name>:\n{{< fa <icon-name> >}}\n\n\nOptional <group>, <size=...>, and <title=...>:\n{{< fa <group> <icon-name> <size=...> <title=...> >}}\n\n\nFor example:\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n{{< fa thumbs-up >}}\n\n\n\n{{< fa folder >}}\n\n\n\n{{< fa chess-pawn >}}\n\n\n\n{{< fa brands bluetooth >}}\n\n\n\n\n{{< fa brands twitter size=2xl >}} (HTML only)\n\n\n\n\n{{< fa brands github size=5x >}} (HTML only)\n\n\n\n{{< fa battery-half size=Huge >}}\n\n\n\n{{< fa envelope title=\"An envelope\" >}}"
  },
  {
    "objectID": "content/posts/09-using-lordicons/index.html#using-iconify-shortcodes",
    "href": "content/posts/09-using-lordicons/index.html#using-iconify-shortcodes",
    "title": "Using Lordicons, Fontawesome Icons,Academicons, and Iconify Icons",
    "section": "Using Iconify Shortcodes",
    "text": "Using Iconify Shortcodes\nThis extension allows you to use Iconify icons in your Quarto HTML documents. It provides an {{< iconify >}} shortcode:\n\n\nMandatory <icon-name>:\n{{< iconify <icon-name> >}}\n\n\nOptional <set> (default is fluent-emoji) <size=...>, <width=...>, <height=...>, <flip=...>, and <rotate=...>:\n{{< iconify <set> <icon-name> <size=...> <width=...> <height=...> <flip=...> <rotate=...> >}}\nIf <size=...> is defined, <width=...> and <height=...> are not used.\nSee https://docs.iconify.design/iconify-icon/ for more details.\n\n\nFor example:\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n{{< iconify exploding-head >}}\n\n\n\n{{< iconify exploding-head size=2xl >}}\n\n\n\n{{< iconify exploding-head size=5x rotate=180deg >}}\n\n\n\n{{< iconify exploding-head size=Huge >}}\n\n\n\n{{< iconify fluent-emoji-high-contrast 1st-place-medal >}}\n\n\n\n{{< iconify twemoji 1st-place-medal >}}\n\n\n\n{{< iconify line-md loading-alt-loop >}}\n\n\n\n{{< iconify fa6-brands apple width=50px height=10px rotate=90deg flip=vertical >}}"
  },
  {
    "objectID": "content/posts/10-using-nutshell/index.html",
    "href": "content/posts/10-using-nutshell/index.html",
    "title": "Nutshell: Expandable Explanations",
    "section": "",
    "text": "Nutshell is a tool to make ‚Äúexpandable explanations‚Äù, like this! This lets your readers learn only the details they need, just-in-time, always-in-context.\nUnlike links, Nutshell lets you include only the snippet you need, not the whole page. Plus, instead of being lost in a jungle of new tabs, your reader stays on one page, keeping their flow of reading. Even if you interrupt a sentence, Nutshell recaps the sentence afterwards, so your reader never loses context.\nYou can find more information on the nutshell webpage and here is a live demo !"
  },
  {
    "objectID": "content/posts/10-using-nutshell/index.html#test",
    "href": "content/posts/10-using-nutshell/index.html#test",
    "title": "Nutshell: Expandable Explanations",
    "section": "Test",
    "text": "Test\nThis is a senseless paragraph"
  },
  {
    "objectID": "content/posts/10-using-nutshell/index.html#testing-links",
    "href": "content/posts/10-using-nutshell/index.html#testing-links",
    "title": "Nutshell: Expandable Explanations",
    "section": "Testing Links",
    "text": "Testing Links\n\n:link to senseless paragraph\n\n:link to wikipedia article\n:link to invisible sections"
  },
  {
    "objectID": "content/posts/10-using-nutshell/index.html#x-invisible",
    "href": "content/posts/10-using-nutshell/index.html#x-invisible",
    "title": "Nutshell: Expandable Explanations",
    "section": ":x invisible",
    "text": ":x invisible\nUse ## :x header to include an invisible secion that can be linked to via nutshell"
  },
  {
    "objectID": "content/posts/12-tufte-style-article/index.html",
    "href": "content/posts/12-tufte-style-article/index.html",
    "title": "A Tufte Handout Example",
    "section": "",
    "text": "The Tufte handout style is a style that Edward Tufte uses in his books and handouts. Tufte‚Äôs style is known for its extensive use of sidenotes, tight integration of graphics with text, and well-set typography. This style has been implemented in LaTeX and HTML/CSS1, respectively. We have ported both implementations into the tufte package. If you want LaTeX/PDF output, you may use the tufte_handout format for handouts, and tufte_book for books. For HTML output, use tufte_html. These formats can be either specified in the YAML metadata at the beginning of an R Markdown document (see an example below), or passed to the rmarkdown::render() function. See Allaire et al. (2022) for more information about rmarkdown.\n---\ntitle: \"An Example Using the Tufte Style\"\nauthor: \"John Smith\"\noutput:\n  tufte::tufte_handout: default\n  tufte::tufte_html: default\n---\nThere are two goals of this package:\n\nTo produce both PDF and HTML output with similar styles from the same R Markdown document;\nTo provide simple syntax to write elements of the Tufte style such as side notes and margin figures, e.g.¬†when you want a margin figure, all you need to do is the chunk option fig.margin = TRUE, and we will take care of the details for you, so you never need to think about \\begin{marginfigure} \\end{marginfigure} or <span class=\"marginfigure\"> </span>; the LaTeX and HTML code under the hood may be complicated, but you never need to learn or write such code.\n\nIf you have any feature requests or find bugs in tufte, please do not hesitate to file them to https://github.com/rstudio/tufte/issues. For general questions, you may ask them on StackOverflow: https://stackoverflow.com/tags/rmarkdown."
  },
  {
    "objectID": "content/posts/12-tufte-style-article/index.html#margin-figures",
    "href": "content/posts/12-tufte-style-article/index.html#margin-figures",
    "title": "A Tufte Handout Example",
    "section": "Margin Figures",
    "text": "Margin Figures\nImages and graphics play an integral role in Tufte‚Äôs work. To place figures in the margin you can use the knitr chunk option fig.margin = TRUE. For example:\n\nlibrary(ggplot2)\nmtcars2 <- mtcars\nmtcars2$am <- factor(\n  mtcars$am, labels = c('automatic', 'manual')\n)\nggplot(mtcars2, aes(hp, mpg, color = am)) +\n  geom_point() + geom_smooth() +\n  theme(legend.position = 'bottom')\n\n\n\n\nFigure¬†1: MPG vs horsepower, colored by transmission.\n\n\n\nNote the use of the fig.cap chunk option to provide a figure caption. You can adjust the proportions of figures using the fig.width and fig.height chunk options. These are specified in inches, and will be automatically scaled down to fit within the handout margin."
  },
  {
    "objectID": "content/posts/12-tufte-style-article/index.html#arbitrary-margin-content",
    "href": "content/posts/12-tufte-style-article/index.html#arbitrary-margin-content",
    "title": "A Tufte Handout Example",
    "section": "Arbitrary Margin Content",
    "text": "Arbitrary Margin Content\nIn fact, you can include anything in the margin using the knitr engine named marginfigure. Unlike R code chunks ```{r}, you write a chunk starting with ```{marginfigure} instead, then put the content in the chunk. See an example on the right about the first fundamental theorem of calculus.\n\nWe know from _the first fundamental theorem of calculus_ that for $x$ in $[a, b]$:\n$$\\frac{d}{dx}\\left( \\int_{a}^{x} f(u)\\,du\\right)=f(x).$$\n\nFor the sake of portability between LaTeX and HTML, you should keep the margin content as simple as possible (syntax-wise) in the marginefigure blocks. You may use simple Markdown syntax like **bold** and _italic_ text, but please refrain from using footnotes, citations, or block-level elements (e.g.¬†blockquotes and lists) there.\nNote: if you set echo = FALSE in your global chunk options, you will have to add echo = TRUE to the chunk to display a margin figure, for example ```{marginfigure, echo = TRUE}."
  },
  {
    "objectID": "content/posts/12-tufte-style-article/index.html#full-width-figures",
    "href": "content/posts/12-tufte-style-article/index.html#full-width-figures",
    "title": "A Tufte Handout Example",
    "section": "Full Width Figures",
    "text": "Full Width Figures\nYou can arrange for figures to span across the entire page by using the chunk option fig.fullwidth = TRUE.\n\nggplot(diamonds, aes(carat, price)) + geom_smooth() +\n  facet_grid(~ cut)\n\n\n\nFigure¬†2: A full width figure.\n\n\n\n\nOther chunk options related to figures can still be used, such as fig.width, fig.cap, out.width, and so on. For full width figures, usually fig.width is large and fig.height is small. In the above example, the plot size is \\(10 \\times 2\\)."
  },
  {
    "objectID": "content/posts/12-tufte-style-article/index.html#arbitrary-full-width-content",
    "href": "content/posts/12-tufte-style-article/index.html#arbitrary-full-width-content",
    "title": "A Tufte Handout Example",
    "section": "Arbitrary Full Width Content",
    "text": "Arbitrary Full Width Content\nAny content can span to the full width of the page. This feature requires Pandoc 2.0 or above. All you need is to put your content in a fenced Div with the class fullwidth, e.g.,\n::: {.fullwidth}\nAny _full width_ content here.\n:::\nBelow is an example:\n\nR is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under the terms of the GNU General Public License versions 2 or 3. For more information about these matters see https://www.gnu.org/licenses/."
  },
  {
    "objectID": "content/posts/12-tufte-style-article/index.html#main-column-figures",
    "href": "content/posts/12-tufte-style-article/index.html#main-column-figures",
    "title": "A Tufte Handout Example",
    "section": "Main Column Figures",
    "text": "Main Column Figures\nBesides margin and full width figures, you can of course also include figures constrained to the main column. This is the default type of figures in the LaTeX/HTML output.\n\nggplot(diamonds, aes(cut, price)) + geom_boxplot()\n\n\n\nFigure¬†3: A figure in the main column."
  },
  {
    "objectID": "content/posts/13-rapp-sample-document/index.html",
    "href": "content/posts/13-rapp-sample-document/index.html",
    "title": "Sample Blog Post Template",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\nThis is inline code plus a small code chunk.\n\nlibrary(tidyverse)\n\nggplot(mpg) +\n  geom_jitter(aes(cty, hwy), size = 4, alpha = 0.5) \n\n\n\n\n\n\n\n\n\n\nTransforming OLS estimates\nMaximizing likelihood\n\n\n\n\nCodepreds_lm %>% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  theme_minimal(base_size = 12) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\nCodeglm.mod <- glm(sex ~ body_mass_g + bill_length_mm + species, family = binomial, data = dat)\n\npreds <- dat %>% \n  mutate(\n    prob.fit = glm.mod$fitted.values,\n    prediction = if_else(prob.fit > 0.5, 'male', 'female'),\n    correct = if_else(sex == prediction, 'correct', 'incorrect')\n  )\n\n\npreds %>% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  theme_minimal(base_size = 10) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\\[\n\\int_0^1 f(x) \\ dx\n\\]\n\n\n\n\ngeom_density(\n  mapping = NULL,\n  data = NULL,\n  stat = \"density\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  outline.type = \"upper\"\n)\n\n\nstat_density(\n  mapping = NULL,\n  data = NULL,\n  geom = \"area\",\n  position = \"stack\",\n  ...,\n  bw = \"nrd0\",\n  adjust = 1,\n  kernel = \"gaussian\",\n  n = 512,\n  trim = FALSE,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\n\n\n\n\nggplot(data = gapminder::gapminder, mapping = aes(x = lifeExp, fill = continent)) +\n  stat_density(position = \"identity\", alpha = 0.5)\n\n\n\n\nBla bla bla. This is a caption in the margin. Super cool isn‚Äôt it?"
  },
  {
    "objectID": "content/posts/listing.html",
    "href": "content/posts/listing.html",
    "title": "The Foundation Series",
    "section": "",
    "text": "A Tufte Handout Example\n\n\n\n\n\n\nJJ Allaire and Yihui Xie\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\nBootswatch Themed QMD\n\n\n\n\n\n\nArvind Venkatadri\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoston Terrier\n\n\n\n\n\n\nArvind Venkatadri\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\nCallout Boxes\n\n\n\n\n\n\nArvind Venkatadri\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\nCallouts in PDF\n\n\n\n\n\n\nArvind Venkatadri\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample Lightbox Document\n\n\n\n\n\n\nArvind Venkatadri\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure Layout\n\n\n\n\n\n\nArvind Venkatadri\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure Layouts\n\n\n\n\n\n\nArvind Venkatadri\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure Layouts-2\n\n\n\n\n\n\nArvind Venkatadri\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\nFont Awesome Quarto Extension\n\n\n\n\n\n\nArvind Venkatadri\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\nHousing Prices\n\n\n\n\n\n\nArvind Venkatadri\n\n\nJan 8, 2023\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\nInline Code\n\n\n\n\n\n\nArvind Venkatadri\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\nNutshell: Expandable Explanations\n\n\n\n\n\n\nDavid Schoch\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\nOld Markdown - reactable example\n\n\n\n\n\n\nTom Mock\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\nOverview of using gtsummary\n\n\n\n\n\n\nArvind Venkatadri\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPenguin Report Presentation\n\n\n\n\n\n\nTom Mock\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPenguins Distilled\n\n\nA great new article on Penguins\n\n\n\nTom Mock\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\nPenguins Parametric Reports\n\n\n\n\n\n\nArvind Venkatadri\n\n\nJul 13, 2022\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\nPlot Layout\n\n\n\n\n\n\nArvind Venkatadri\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\nPlots\n\n\n\n\n\n\nArvind Venkatadri\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\nPresentation with Columns?\n\n\n\n\n\n\nArvind Venkatadri\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\nSample Blog Post Template\n\n\nDescription: This is a test post. In this post, I try out different functionalities\n\n\n\nArvind Venkatadri\n\n\nJun 1, 2022\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\nUsing Lordicons, Fontawesome Icons,Academicons, and Iconify Icons\n\n\n\n\n\n\nArvind Venkatadri\n\n\nJan 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisual Editor for Quarto\n\n\n\n\n\n\nThomas Mock\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\ngtsummary + R Markdown\n\n\n\n\n\n\nArvind Venkatadri\n\n\nInvalid Date\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "",
    "text": "This is an attempt to rework , using tidygraph and ggraph, much of Network Visualization with R Polnet 2018 Workshop Tutorial, Washington, DC by Prof.¬†Katherine (Katya) Ognyanova.\nThe aim is to get a working acquaintance with both these packages and also to appreciate some of the concepts in Networks. My code is by no means intended to be elegant; it merely works and there are surely many improvements that people may think of!\nI have attempted to write code for the Sections 2:5.\nI have retained Prof.¬†Ognyanova‚Äôs text in all places."
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#contents",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#contents",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "CONTENTS",
    "text": "CONTENTS\n\nWorking with colors in R plots\nReading in the network data\nNetwork plots in ‚Äòigraph‚Äô\nPlotting two-mode networks\nPlotting multiplex networks\nQuick example using ‚Äònetwork‚Äô\nSimple plot animations in R\nInteractive JavaScript networks\nInteractive and dynamic networks with ndtv-d3\nPlotting networks on a geographic map"
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#dataset-1-edgelist--",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#dataset-1-edgelist--",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "‚Äî‚Äî-~~ DATASET 1: edgelist ~~‚Äî‚Äî-",
    "text": "‚Äî‚Äî-~~ DATASET 1: edgelist ~~‚Äî‚Äî-\n\n# Read in the data:\nnodes <- read.csv(\"./Dataset1-Media-Example-NODES.csv\", header = T, as.is = T)\nlinks <- read.csv(\"./Dataset1-Media-Example-EDGES.csv\", header = T, as.is = T)\n\n\n# Examine the data:\nhead(nodes)\n\n   id               media media.type type.label audience.size\n1 s01            NY Times          1  Newspaper            20\n2 s02     Washington Post          1  Newspaper            25\n3 s03 Wall Street Journal          1  Newspaper            30\n4 s04           USA Today          1  Newspaper            32\n5 s05            LA Times          1  Newspaper            20\n6 s06       New York Post          1  Newspaper            50\n\nhead(links)\n\n  from  to      type weight\n1  s01 s02 hyperlink     22\n2  s01 s03 hyperlink     22\n3  s01 s04 hyperlink     21\n4  s01 s15   mention     20\n5  s02 s01 hyperlink     23\n6  s02 s03 hyperlink     21\n\n\nConverting the data to an igraph object:\nThe graph_from_data_frame() function takes two data frames: ‚Äòd‚Äô and ‚Äòvertices‚Äô. - ‚Äòd‚Äô describes the edges of the network - it should start with two columns containing the source and target node IDs for each network tie. - ‚Äòvertices‚Äô should start with a column of node IDs. It can be omitted. - Any additional columns in either data frame are interpreted as attributes.\nNOTE: ID columns need not be numbers or integers!!\n\nnet <- graph_from_data_frame(d = links, vertices = nodes, directed = T)\n\n# Examine the resulting object:\nclass(net)\n\n[1] \"igraph\"\n\nnet\n\nIGRAPH 8d2aa7a DNW- 17 49 -- \n+ attr: name (v/c), media (v/c), media.type (v/n), type.label (v/c),\n| audience.size (v/n), type (e/c), weight (e/n)\n+ edges from 8d2aa7a (vertex names):\n [1] s01->s02 s01->s03 s01->s04 s01->s15 s02->s01 s02->s03 s02->s09 s02->s10\n [9] s03->s01 s03->s04 s03->s05 s03->s08 s03->s10 s03->s11 s03->s12 s04->s03\n[17] s04->s06 s04->s11 s04->s12 s04->s17 s05->s01 s05->s02 s05->s09 s05->s15\n[25] s06->s06 s06->s16 s06->s17 s07->s03 s07->s08 s07->s10 s07->s14 s08->s03\n[33] s08->s07 s08->s09 s09->s10 s10->s03 s12->s06 s12->s13 s12->s14 s13->s12\n[41] s13->s17 s14->s11 s14->s13 s15->s01 s15->s04 s15->s06 s16->s06 s16->s17\n[49] s17->s04\n\n\nThe description of an igraph object starts with four letters: -D or U, for a directed or undirected graph -N for a named graph (where nodes have a name attribute) -W for a weighted graph (where edges have a weight attribute) -B for a bipartite (two-mode) graph (where nodes have a type attribute) The two numbers that follow (17 49) refer to the number of nodes and edges in the graph. The description also lists node & edge attributes.\nWe can access the nodes, edges, and their attributes:\n\nE(net)\n\n+ 49/49 edges from 8d2aa7a (vertex names):\n [1] s01->s02 s01->s03 s01->s04 s01->s15 s02->s01 s02->s03 s02->s09 s02->s10\n [9] s03->s01 s03->s04 s03->s05 s03->s08 s03->s10 s03->s11 s03->s12 s04->s03\n[17] s04->s06 s04->s11 s04->s12 s04->s17 s05->s01 s05->s02 s05->s09 s05->s15\n[25] s06->s06 s06->s16 s06->s17 s07->s03 s07->s08 s07->s10 s07->s14 s08->s03\n[33] s08->s07 s08->s09 s09->s10 s10->s03 s12->s06 s12->s13 s12->s14 s13->s12\n[41] s13->s17 s14->s11 s14->s13 s15->s01 s15->s04 s15->s06 s16->s06 s16->s17\n[49] s17->s04\n\nV(net)\n\n+ 17/17 vertices, named, from 8d2aa7a:\n [1] s01 s02 s03 s04 s05 s06 s07 s08 s09 s10 s11 s12 s13 s14 s15 s16 s17\n\nE(net)$type\n\n [1] \"hyperlink\" \"hyperlink\" \"hyperlink\" \"mention\"   \"hyperlink\" \"hyperlink\"\n [7] \"hyperlink\" \"hyperlink\" \"hyperlink\" \"hyperlink\" \"hyperlink\" \"hyperlink\"\n[13] \"mention\"   \"hyperlink\" \"hyperlink\" \"hyperlink\" \"mention\"   \"mention\"  \n[19] \"hyperlink\" \"mention\"   \"mention\"   \"hyperlink\" \"hyperlink\" \"mention\"  \n[25] \"hyperlink\" \"hyperlink\" \"mention\"   \"mention\"   \"mention\"   \"hyperlink\"\n[31] \"mention\"   \"hyperlink\" \"mention\"   \"mention\"   \"mention\"   \"hyperlink\"\n[37] \"mention\"   \"hyperlink\" \"mention\"   \"hyperlink\" \"mention\"   \"mention\"  \n[43] \"mention\"   \"hyperlink\" \"hyperlink\" \"hyperlink\" \"hyperlink\" \"mention\"  \n[49] \"hyperlink\"\n\nV(net)$media\n\n [1] \"NY Times\"            \"Washington Post\"     \"Wall Street Journal\"\n [4] \"USA Today\"           \"LA Times\"            \"New York Post\"      \n [7] \"CNN\"                 \"MSNBC\"               \"FOX News\"           \n[10] \"ABC\"                 \"BBC\"                 \"Yahoo News\"         \n[13] \"Google News\"         \"Reuters.com\"         \"NYTimes.com\"        \n[16] \"WashingtonPost.com\"  \"AOL.com\"            \n\n\n\n# Using tidygraph\ntbl_graph(nodes, links, directed = TRUE) %>% \n  activate(edges) %>% \n  select(type)\n\n# A tbl_graph: 17 nodes and 49 edges\n#\n# A directed multigraph with 1 component\n#\n# Edge Data: 49 √ó 3 (active)\n   from    to type     \n  <int> <int> <chr>    \n1     1     2 hyperlink\n2     1     3 hyperlink\n3     1     4 hyperlink\n4     1    15 mention  \n5     2     1 hyperlink\n6     2     3 hyperlink\n# ‚Ä¶ with 43 more rows\n#\n# Node Data: 17 √ó 5\n  id    media               media.type type.label audience.size\n  <chr> <chr>                    <int> <chr>              <int>\n1 s01   NY Times                     1 Newspaper             20\n2 s02   Washington Post              1 Newspaper             25\n3 s03   Wall Street Journal          1 Newspaper             30\n# ‚Ä¶ with 14 more rows\n\ntbl_graph(nodes, links, directed = TRUE) %>% \n  activate(nodes) %>% \n  select(media)\n\n# A tbl_graph: 17 nodes and 49 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 17 √ó 1 (active)\n  media              \n  <chr>              \n1 NY Times           \n2 Washington Post    \n3 Wall Street Journal\n4 USA Today          \n5 LA Times           \n6 New York Post      \n# ‚Ä¶ with 11 more rows\n#\n# Edge Data: 49 √ó 4\n   from    to type      weight\n  <int> <int> <chr>      <int>\n1     1     2 hyperlink     22\n2     1     3 hyperlink     22\n3     1     4 hyperlink     21\n# ‚Ä¶ with 46 more rows\n\n\nOr find specific nodes and edges by attribute:(that returns objects of type vertex sequence / edge sequence)\n\nV(net)[media == \"BBC\"]\n\n+ 1/17 vertex, named, from 8d2aa7a:\n[1] s11\n\nE(net)[type == \"mention\"]\n\n+ 20/49 edges from 8d2aa7a (vertex names):\n [1] s01->s15 s03->s10 s04->s06 s04->s11 s04->s17 s05->s01 s05->s15 s06->s17\n [9] s07->s03 s07->s08 s07->s14 s08->s07 s08->s09 s09->s10 s12->s06 s12->s14\n[17] s13->s17 s14->s11 s14->s13 s16->s17\n\n\n\n#Using tidygraph\ntbl_graph(nodes, links, directed = TRUE) %>% \n  activate(nodes) %>% \n  filter(media == \"BBC\")\n\n# A tbl_graph: 1 nodes and 0 edges\n#\n# A rooted tree\n#\n# Node Data: 1 √ó 5 (active)\n  id    media media.type type.label audience.size\n  <chr> <chr>      <int> <chr>              <int>\n1 s11   BBC            2 TV                    34\n#\n# Edge Data: 0 √ó 4\n# ‚Ä¶ with 4 variables: from <int>, to <int>, type <chr>, weight <int>\n\ntbl_graph(nodes, links, directed = TRUE) %>% \n  activate(edges) %>% \n  filter(type == \"mention\")\n\n# A tbl_graph: 17 nodes and 20 edges\n#\n# A directed simple graph with 3 components\n#\n# Edge Data: 20 √ó 4 (active)\n   from    to type    weight\n  <int> <int> <chr>    <int>\n1     1    15 mention     20\n2     3    10 mention      2\n3     4     6 mention      1\n4     4    11 mention     22\n5     4    17 mention      2\n6     5     1 mention      1\n# ‚Ä¶ with 14 more rows\n#\n# Node Data: 17 √ó 5\n  id    media               media.type type.label audience.size\n  <chr> <chr>                    <int> <chr>              <int>\n1 s01   NY Times                     1 Newspaper             20\n2 s02   Washington Post              1 Newspaper             25\n3 s03   Wall Street Journal          1 Newspaper             30\n# ‚Ä¶ with 14 more rows\n\n\nIf you need them, you can extract an edge list or a matrix back from the igraph networks.\n\nas_edgelist(net, names = T)\n\n      [,1]  [,2] \n [1,] \"s01\" \"s02\"\n [2,] \"s01\" \"s03\"\n [3,] \"s01\" \"s04\"\n [4,] \"s01\" \"s15\"\n [5,] \"s02\" \"s01\"\n [6,] \"s02\" \"s03\"\n [7,] \"s02\" \"s09\"\n [8,] \"s02\" \"s10\"\n [9,] \"s03\" \"s01\"\n[10,] \"s03\" \"s04\"\n[11,] \"s03\" \"s05\"\n[12,] \"s03\" \"s08\"\n[13,] \"s03\" \"s10\"\n[14,] \"s03\" \"s11\"\n[15,] \"s03\" \"s12\"\n[16,] \"s04\" \"s03\"\n[17,] \"s04\" \"s06\"\n[18,] \"s04\" \"s11\"\n[19,] \"s04\" \"s12\"\n[20,] \"s04\" \"s17\"\n[21,] \"s05\" \"s01\"\n[22,] \"s05\" \"s02\"\n[23,] \"s05\" \"s09\"\n[24,] \"s05\" \"s15\"\n[25,] \"s06\" \"s06\"\n[26,] \"s06\" \"s16\"\n[27,] \"s06\" \"s17\"\n[28,] \"s07\" \"s03\"\n[29,] \"s07\" \"s08\"\n[30,] \"s07\" \"s10\"\n[31,] \"s07\" \"s14\"\n[32,] \"s08\" \"s03\"\n[33,] \"s08\" \"s07\"\n[34,] \"s08\" \"s09\"\n[35,] \"s09\" \"s10\"\n[36,] \"s10\" \"s03\"\n[37,] \"s12\" \"s06\"\n[38,] \"s12\" \"s13\"\n[39,] \"s12\" \"s14\"\n[40,] \"s13\" \"s12\"\n[41,] \"s13\" \"s17\"\n[42,] \"s14\" \"s11\"\n[43,] \"s14\" \"s13\"\n[44,] \"s15\" \"s01\"\n[45,] \"s15\" \"s04\"\n[46,] \"s15\" \"s06\"\n[47,] \"s16\" \"s06\"\n[48,] \"s16\" \"s17\"\n[49,] \"s17\" \"s04\"\n\nas_adjacency_matrix(net, attr = \"weight\")\n\n17 x 17 sparse Matrix of class \"dgCMatrix\"\n\n\n  [[ suppressing 17 column names 's01', 's02', 's03' ... ]]\n\n\n                                                     \ns01  . 22 22 21 .  .  .  .  .  .  .  .  .  . 20  .  .\ns02 23  . 21  . .  .  .  .  1  5  .  .  .  .  .  .  .\ns03 21  .  . 22 1  .  .  4  .  2  1  1  .  .  .  .  .\ns04  .  . 23  . .  1  .  .  .  . 22  3  .  .  .  .  2\ns05  1 21  .  . .  .  .  .  2  .  .  .  .  . 21  .  .\ns06  .  .  .  . .  1  .  .  .  .  .  .  .  .  . 21 21\ns07  .  .  1  . .  .  . 22  . 21  .  .  .  4  .  .  .\ns08  .  .  2  . .  . 21  . 23  .  .  .  .  .  .  .  .\ns09  .  .  .  . .  .  .  .  . 21  .  .  .  .  .  .  .\ns10  .  .  2  . .  .  .  .  .  .  .  .  .  .  .  .  .\ns11  .  .  .  . .  .  .  .  .  .  .  .  .  .  .  .  .\ns12  .  .  .  . .  2  .  .  .  .  .  . 22 22  .  .  .\ns13  .  .  .  . .  .  .  .  .  .  . 21  .  .  .  .  1\ns14  .  .  .  . .  .  .  .  .  .  1  . 21  .  .  .  .\ns15 22  .  .  1 .  4  .  .  .  .  .  .  .  .  .  .  .\ns16  .  .  .  . . 23  .  .  .  .  .  .  .  .  .  . 21\ns17  .  .  .  4 .  .  .  .  .  .  .  .  .  .  .  .  .\n\n# Using tidygraph\n# No direct command seems available ...\n\n\n# Or data frames describing nodes and edges:\nigraph::as_data_frame(x = net, what = \"edges\")\n\n   from  to      type weight\n1   s01 s02 hyperlink     22\n2   s01 s03 hyperlink     22\n3   s01 s04 hyperlink     21\n4   s01 s15   mention     20\n5   s02 s01 hyperlink     23\n6   s02 s03 hyperlink     21\n7   s02 s09 hyperlink      1\n8   s02 s10 hyperlink      5\n9   s03 s01 hyperlink     21\n10  s03 s04 hyperlink     22\n11  s03 s05 hyperlink      1\n12  s03 s08 hyperlink      4\n13  s03 s10   mention      2\n14  s03 s11 hyperlink      1\n15  s03 s12 hyperlink      1\n16  s04 s03 hyperlink     23\n17  s04 s06   mention      1\n18  s04 s11   mention     22\n19  s04 s12 hyperlink      3\n20  s04 s17   mention      2\n21  s05 s01   mention      1\n22  s05 s02 hyperlink     21\n23  s05 s09 hyperlink      2\n24  s05 s15   mention     21\n25  s06 s06 hyperlink      1\n26  s06 s16 hyperlink     21\n27  s06 s17   mention     21\n28  s07 s03   mention      1\n29  s07 s08   mention     22\n30  s07 s10 hyperlink     21\n31  s07 s14   mention      4\n32  s08 s03 hyperlink      2\n33  s08 s07   mention     21\n34  s08 s09   mention     23\n35  s09 s10   mention     21\n36  s10 s03 hyperlink      2\n37  s12 s06   mention      2\n38  s12 s13 hyperlink     22\n39  s12 s14   mention     22\n40  s13 s12 hyperlink     21\n41  s13 s17   mention      1\n42  s14 s11   mention      1\n43  s14 s13   mention     21\n44  s15 s01 hyperlink     22\n45  s15 s04 hyperlink      1\n46  s15 s06 hyperlink      4\n47  s16 s06 hyperlink     23\n48  s16 s17   mention     21\n49  s17 s04 hyperlink      4\n\nigraph::as_data_frame(x = net, what = \"vertices\")\n\n    name               media media.type type.label audience.size\ns01  s01            NY Times          1  Newspaper            20\ns02  s02     Washington Post          1  Newspaper            25\ns03  s03 Wall Street Journal          1  Newspaper            30\ns04  s04           USA Today          1  Newspaper            32\ns05  s05            LA Times          1  Newspaper            20\ns06  s06       New York Post          1  Newspaper            50\ns07  s07                 CNN          2         TV            56\ns08  s08               MSNBC          2         TV            34\ns09  s09            FOX News          2         TV            60\ns10  s10                 ABC          2         TV            23\ns11  s11                 BBC          2         TV            34\ns12  s12          Yahoo News          3     Online            33\ns13  s13         Google News          3     Online            23\ns14  s14         Reuters.com          3     Online            12\ns15  s15         NYTimes.com          3     Online            24\ns16  s16  WashingtonPost.com          3     Online            28\ns17  s17             AOL.com          3     Online            33\n\n#Using tidygraph\ntbl_graph(nodes, links, directed = TRUE) %>% \n  activate(nodes) %>% \n  as_tibble()\n\n# A tibble: 17 √ó 5\n   id    media               media.type type.label audience.size\n   <chr> <chr>                    <int> <chr>              <int>\n 1 s01   NY Times                     1 Newspaper             20\n 2 s02   Washington Post              1 Newspaper             25\n 3 s03   Wall Street Journal          1 Newspaper             30\n 4 s04   USA Today                    1 Newspaper             32\n 5 s05   LA Times                     1 Newspaper             20\n 6 s06   New York Post                1 Newspaper             50\n 7 s07   CNN                          2 TV                    56\n 8 s08   MSNBC                        2 TV                    34\n 9 s09   FOX News                     2 TV                    60\n10 s10   ABC                          2 TV                    23\n11 s11   BBC                          2 TV                    34\n12 s12   Yahoo News                   3 Online                33\n13 s13   Google News                  3 Online                23\n14 s14   Reuters.com                  3 Online                12\n15 s15   NYTimes.com                  3 Online                24\n16 s16   WashingtonPost.com           3 Online                28\n17 s17   AOL.com                      3 Online                33\n\ntbl_graph(nodes, links, directed = TRUE)%>% \n  activate(edges) %>% \n  as_tibble()\n\n# A tibble: 49 √ó 4\n    from    to type      weight\n   <int> <int> <chr>      <int>\n 1     1     2 hyperlink     22\n 2     1     3 hyperlink     22\n 3     1     4 hyperlink     21\n 4     1    15 mention       20\n 5     2     1 hyperlink     23\n 6     2     3 hyperlink     21\n 7     2     9 hyperlink      1\n 8     2    10 hyperlink      5\n 9     3     1 hyperlink     21\n10     3     4 hyperlink     22\n# ‚Ä¶ with 39 more rows\n\n\n\n# You can also access the network matrix directly:\nnet[1,]\n\ns01 s02 s03 s04 s05 s06 s07 s08 s09 s10 s11 s12 s13 s14 s15 s16 s17 \n  0  22  22  21   0   0   0   0   0   0   0   0   0   0  20   0   0 \n\nnet[5,7]\n\n[1] 0\n\n# Using tidygraph\n# Does not seem possible, even with `as.matrix()`.\n# Returns tibbles only as in the code chunk above\n\n\n# First attempt to plot the graph:\nplot(net) # not pretty!\n\n\n\n# Removing loops from the graph:\nnet <-\n  igraph::simplify(net, remove.multiple = F, remove.loops = T)\n\n# Let's and reduce the arrow size and remove the labels:\nplot(net, edge.arrow.size = .4, vertex.label = NA)\n\n\n\n\n\n# Using tidygraph\ntbl_graph(nodes, links, directed = TRUE) %>%\n  ggraph(., layout = \"graphopt\") +\n  geom_edge_link(\n    color = \"grey\",\n    end_cap = circle(0.2, \"cm\"),\n    start_cap = circle(0.2, \"cm\"),\n    # clears an area near the node\n    \n    arrow = arrow(\n      type = \"closed\",\n      ends = \"last\",\n      length = unit(3, \"mm\")\n    )\n  ) +\n  geom_node_point(size = 6, shape = 21, fill = \"orange\") +\n  geom_node_text(aes(label = id))\n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` in the `default_aes` field and elsewhere instead.\n\n\n\n\n# Removing loops from the graph:\n# From the docs:\n# convert() is a shorthand for performing both `morph` and `crystallise` along with extracting a single tbl_graph (defaults to the first). For morphs w(h)ere you know they only create a single graph, and you want to keep it, this is an easy way.\n#\ntbl_graph(nodes, links, directed = TRUE) %>%\n\n  convert(to_simple) %>%\n  ggraph(., layout = \"graphopt\") +\n  geom_edge_link(\n    color = \"grey\",\n    end_cap = circle(0.2, \"cm\"),\n    start_cap = circle(0.2, \"cm\"),\n \n    arrow = arrow(\n      type = \"closed\",\n      ends = \"last\",\n      length = unit(3, \"mm\")\n    )\n  ) +\n  geom_node_point(size = 6, shape = 21, fill = \"orange\")"
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#dataset-2-matrix",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#dataset-2-matrix",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "‚Äî‚Äî-~~ DATASET 2: matrix ‚Äî‚Äî‚Äì",
    "text": "‚Äî‚Äî-~~ DATASET 2: matrix ‚Äî‚Äî‚Äì\n\n# Read in the data:\nnodes2 <- read.csv(\"./Dataset2-Media-User-Example-NODES.csv\", header = T, as.is = T)\nlinks2 <- read.csv(\"./Dataset2-Media-User-Example-EDGES.csv\", header = T, row.names = 1)\n\n# Examine the data:\nhead(nodes2)\n\n   id   media media.type media.name audience.size\n1 s01     NYT          1  Newspaper            20\n2 s02    WaPo          1  Newspaper            25\n3 s03     WSJ          1  Newspaper            30\n4 s04    USAT          1  Newspaper            32\n5 s05 LATimes          1  Newspaper            20\n6 s06     CNN          2         TV            56\n\nhead(links2)\n\n    U01 U02 U03 U04 U05 U06 U07 U08 U09 U10 U11 U12 U13 U14 U15 U16 U17 U18 U19\ns01   1   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\ns02   0   0   0   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\ns03   0   0   0   0   0   1   1   1   1   0   0   0   0   0   0   0   0   0   0\ns04   0   0   0   0   0   0   0   0   1   1   1   0   0   0   0   0   0   0   0\ns05   0   0   0   0   0   0   0   0   0   0   1   1   1   0   0   0   0   0   0\ns06   0   0   0   0   0   0   0   0   0   0   0   0   1   1   0   0   1   0   0\n    U20\ns01   0\ns02   1\ns03   0\ns04   0\ns05   0\ns06   0\n\n# links2 is a matrix for a two-mode network:\nlinks2 <- as.matrix(links2)\ndim(links2)\n\n[1] 10 20\n\ndim(nodes2)\n\n[1] 30  5\n\n\nNote: What is a two-mode network? A network that as a node$type variable and can be a bipartite or a k-partite network as a result.\n\n# Create an igraph network object from the two-mode matrix:\nnet2 <- igraph::graph_from_incidence_matrix(links2)\n\n# To transform a one-mode network matrix into an igraph object,\n# we would use graph_from_adjacency_matrix()\n\n# A built-in vertex attribute 'type' shows which mode vertices belong to.\ntable(V(net2)$type)\n\n\nFALSE  TRUE \n   10    20 \n\n# Basic igraph plot\nplot(net2,vertex.label = NA)\n\n\n\n\n\n# using tidygraph\n# For all objects that are not node and edge data_frames\n# tidygraph uses `as_tbl_graph()`\n# \ngraph <- as_tbl_graph(links2)\ngraph %>% activate(nodes) %>% as_tibble()\n\n# A tibble: 30 √ó 2\n   type  name \n   <lgl> <chr>\n 1 FALSE s01  \n 2 FALSE s02  \n 3 FALSE s03  \n 4 FALSE s04  \n 5 FALSE s05  \n 6 FALSE s06  \n 7 FALSE s07  \n 8 FALSE s08  \n 9 FALSE s09  \n10 FALSE s10  \n# ‚Ä¶ with 20 more rows\n\ngraph %>% activate(edges) %>% as_tibble()\n\n# A tibble: 31 √ó 3\n    from    to weight\n   <int> <int>  <dbl>\n 1     1    11      1\n 2     1    12      1\n 3     1    13      1\n 4     2    14      1\n 5     2    15      1\n 6     2    30      1\n 7     3    16      1\n 8     3    17      1\n 9     3    18      1\n10     3    19      1\n# ‚Ä¶ with 21 more rows\n\ngraph %>% \n  ggraph(., layout = \"graphopt\") + \n  geom_edge_link(color = \"grey\") + \n  geom_node_point(fill = \"orange\", \n                  shape = 21, size = 6, \n                  color = \"black\")\n\n\n\n\n\n# Examine the resulting object:\nclass(net2)\n\n[1] \"igraph\"\n\nnet2\n\nIGRAPH 9008ce1 UN-B 30 31 -- \n+ attr: type (v/l), name (v/c)\n+ edges from 9008ce1 (vertex names):\n [1] s01--U01 s01--U02 s01--U03 s02--U04 s02--U05 s02--U20 s03--U06 s03--U07\n [9] s03--U08 s03--U09 s04--U09 s04--U10 s04--U11 s05--U11 s05--U12 s05--U13\n[17] s06--U13 s06--U14 s06--U17 s07--U14 s07--U15 s07--U16 s08--U16 s08--U17\n[25] s08--U18 s08--U19 s09--U06 s09--U19 s09--U20 s10--U01 s10--U11\n\n\nNote: The remaining attributes for the nodes ( in data frame nodes2) are not (yet) a part of the graph, either with igraph or with tidygraph."
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#plot-parameters-in-igraph",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#plot-parameters-in-igraph",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "‚Äî‚Äî~~ Plot parameters in igraph ‚Äî‚Äî‚Äì",
    "text": "‚Äî‚Äî~~ Plot parameters in igraph ‚Äî‚Äî‚Äì\nCheck out the node options (starting with ‚Äòvertex.‚Äô) and the edge options (starting with ‚Äòedge.‚Äô).\n\n?igraph.plotting\n\nstarting httpd help server ... done\n\n\nWe can set the node & edge options in two ways - one is to specify them in the plot() function, as we are doing below.\n\nPlot with curved edges (edge.curved = .1) and reduce arrow size:\n\n\nplot(net, edge.arrow.size = .4, edge.curved = .1)\n\n\n\n# Using tidygraph\ngraph <- tbl_graph(nodes, links, directed = TRUE)\ngraph\n\n# A tbl_graph: 17 nodes and 49 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 17 √ó 5 (active)\n  id    media               media.type type.label audience.size\n  <chr> <chr>                    <int> <chr>              <int>\n1 s01   NY Times                     1 Newspaper             20\n2 s02   Washington Post              1 Newspaper             25\n3 s03   Wall Street Journal          1 Newspaper             30\n4 s04   USA Today                    1 Newspaper             32\n5 s05   LA Times                     1 Newspaper             20\n6 s06   New York Post                1 Newspaper             50\n# ‚Ä¶ with 11 more rows\n#\n# Edge Data: 49 √ó 4\n   from    to type      weight\n  <int> <int> <chr>      <int>\n1     1     2 hyperlink     22\n2     1     3 hyperlink     22\n3     1     4 hyperlink     21\n# ‚Ä¶ with 46 more rows\n\ngraph %>% ggraph(., layout = \"graphopt\") +\n  geom_edge_arc(\n    color = \"grey\",\n    strength = 0.1,\n    end_cap = circle(.2, \"cm\"),\n\n    arrow = arrow(\n      type = \"closed\",\n      ends = \"both\",\n      length = unit(3, \"mm\")\n    )\n  ) +\n  geom_node_point(\n    fill = \"orange\",\n    shape = 21,\n    size = 8,\n    color = \"black\"\n  ) +\n  geom_node_text(aes(label = id))\n\n\n\n\n\nSet node color to orange and the border color to hex 555555\nReplace the vertex label with the node names stored in ‚Äúmedia‚Äù\n\n\nplot(\n  net,\n  edge.arrow.size = .2,\n  edge.curved = 0,\n  vertex.color = \"orange\",\n  vertex.frame.color = \"#555555\",\n  vertex.label = V(net)$media,\n  vertex.label.color = \"black\",\n  vertex.label.cex = .7\n)\n\n\n\n# Using tidygraph\n#graph <- tbl_graph(nodes, links, directed = TRUE)\n#graph\ngraph %>%\n  ggraph(., layout = \"gem\") +\n  geom_edge_link(\n    color = \"grey\",\n    end_cap = circle(.3, \"cm\"),\n    \n    arrow = arrow(\n      type = \"closed\",\n      ends = \"both\",\n      length = unit(1, \"mm\")\n    )\n  ) +\n  geom_node_point(\n    fill = \"orange\",\n    shape = 21,\n    size = 6,\n    color = \"#555555\"\n  ) +\n  geom_node_text(aes(label = media))\n\n\n\n\nThe second way to set attributes is to add them to the igraph object.\n\nGenerate colors based on media type:\n\n\ncolrs <- c(\"gray50\", \"tomato\", \"gold\")\nV(net)$color <- colrs[V(net)$media.type]\nplot(net)\n\n\n\n\n\nCompute node degrees (#links) and use that to set node size:\n\n\ndeg <- igraph::degree(net, mode = \"all\")\nV(net)$size <- deg*3\n# Alternatively, we can set node size based on audience size:\nV(net)$size <- V(net)$audience.size*0.7\nV(net)$size\n\n [1] 14.0 17.5 21.0 22.4 14.0 35.0 39.2 23.8 42.0 16.1 23.8 23.1 16.1  8.4 16.8\n[16] 19.6 23.1\n\n# The labels are currently node IDs.\n# Setting them to NA will render no labels:\nV(net)$label.color <- \"black\"\nV(net)$label <- NA\n\n# Set edge width based on weight:\nE(net)$width <- E(net)$weight/6\n\n#change arrow size and edge color:\nE(net)$arrow.size <- .2\nE(net)$edge.color <- \"gray80\"\n\n# We can even set the network layout:\ngraph_attr(net, \"layout\") <- layout_with_lgl\nplot(net)\n\n\n\n\n\n# Using tidygraph\n# graph <- tbl_graph(nodes, links, directed = TRUE)\n# graph\ngraph %>%\n  activate(nodes) %>%\n  mutate(size = centrality_degree()) %>%\n  ggraph(., layout = \"lgl\") +\n  geom_edge_link(\n    aes(width = weight),\n    color = \"grey80\",\n    end_cap = circle(.2, \"cm\"),\n    arrow = arrow(\n      type = \"closed\",\n      ends = \"last\",\n      length = unit(1, \"mm\")\n    )\n  ) +\n  geom_node_point(aes(fill = type.label, size = size),\n                  shape = 21,\n                  color = \"black\") +\n  scale_fill_manual(name = \"Media Type\",\n                    values = c(\"grey50\", \"gold\", \"tomato\")) +\n  scale_edge_width(range = c(0.2, 1.5), guide = \"none\") +\n  scale_size_continuous(\"Degree\", range =  c(2, 16)) +\n  \n  guides(fill = guide_legend(title = \"Media Type\",\n                             override.aes = list(pch = 21, size = 4)))\n\n\n\n\nWe can also override the attributes explicitly in the plot:\n\nplot(net, edge.color = \"orange\", vertex.color = \"gray50\")\n\n\n\n\nWe can also add a legend explaining the meaning of the colors we used:\n\nplot(net)\nlegend(x = -2.1, y = -1.1, \n       c(\"Newspaper\",\"Television\", \"Online News\"), \n       pch = 21,col = \"#777777\", \n       pt.bg = colrs, pt.cex = 2.5, bty = \"n\", ncol = 1)\n\n\n\n# legends are automatic with the tidygraph + ggraph flow\n\nSometimes, especially with semantic networks, we may be interested in plotting only the labels of the nodes:\n\nplot(net, vertex.shape = \"none\", vertex.label = V(net)$media,\n     vertex.label.font = 2, vertex.label.color = \"gray40\",\n     vertex.label.cex = .7, edge.color = \"gray85\")\n\n\n\n#using tidygraph\n\nggraph(net, layout = \"gem\") +\n  geom_edge_link(color = \"grey80\", width = 2,\n                 end_cap = circle(0.5,\"cm\"), \n                 start_cap = circle(0.5, \"cm\")) +\n    geom_node_text(aes(label = media))\n\n\n\n\nLet‚Äôs color the edges of the graph based on their source node color. We‚Äôll get the starting node for each edge with ends().\nNote: Edge attribute is being set by start node.\n\nedge.start <- ends(net, es = E(net), names = F)[,1]\nedge.col <- V(net)$color[edge.start] # How simple this is !!!\n# The three colors are recycled \n# \nplot(net, edge.color = edge.col, edge.curved = .4)\n\n\n\n\nNOTE: The source node colour has been set using the media.type, which is a node attribute. Node attributes are not typically accessible to edges. So we need to build a combo data frame using dplyr, so that edges can use this node attribute. ( There may be other ways‚Ä¶)\n\n# Using tidygraph\n# Make a \"combo\" data frame of nodes *and* edges with left_join()\n# Join by `from` so that type.label is based on from = edge.start\n\nlinks %>%\n  left_join(., nodes, by = c(\"from\" = \"id\")) %>%\n  tbl_graph(edges = ., nodes = nodes) %>%\n  \n  mutate(size = centrality_degree()) %>%\n  \n  ggraph(., layout = \"lgl\") +\n  geom_edge_arc(aes(color = type.label,\n                    width = weight),\n                strength = 0.3)  +\n  geom_node_point(aes(fill = type.label,\n                      # type.label is now available as edge attribute\n                      size = size),\n                  shape = 21,\n                  color = \"black\") +\n  scale_fill_manual(\n    name = \"Media Type\",\n    values = c(\"grey50\", \"gold\", \"tomato\"),\n    guide = \"legend\"\n  ) +\n  scale_edge_color_manual(name = \"Source Type\",\n                          values = c(\"grey80\", \"gold\", \"tomato\")) +\n  scale_edge_width(range = c(0.2, 1.5), guide = \"none\") +\n  scale_size_continuous(\"Degree\", range =  c(2, 16)) +\n  # not \"limits\"!\n  guides(fill = guide_legend(override.aes = list(pch = 21,\n                                                 size = 4)))"
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#network-layouts-in-igraph",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#network-layouts-in-igraph",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "‚Äî‚Äî-~~ Network Layouts in ‚Äòigraph‚Äô ‚Äî‚Äî‚Äì",
    "text": "‚Äî‚Äî-~~ Network Layouts in ‚Äòigraph‚Äô ‚Äî‚Äî‚Äì\nNetwork layouts are algorithms that return coordinates for each node in a network.\nLet‚Äôs generate a slightly larger 100-node graph using a preferential attachment model (Barabasi-Albert).\n\nnet.bg <- sample_pa(n =  100, power =  1.2)\nV(net.bg)$size <- 8\nV(net.bg)$frame.color <- \"white\"\nV(net.bg)$color <- \"orange\"\nV(net.bg)$label <- \"\"\nE(net.bg)$arrow.mode <- 0\nplot(net.bg)\n\n\n\n# Using tidygraph\ngraph <- play_barabasi_albert(n = 100, power = 1.2)\ngraph %>%\n  ggraph(., layout = \"graphopt\") +\n  geom_edge_link(color = \"grey\") +\n  geom_node_point(color = \"orange\", size = 4) +\n  theme_graph()\n\n\n\n\nNow let‚Äôs plot this network using the layouts available in igraph. You can set the layout in the plot function:\n\nplot(net.bg, layout = layout_randomly)\n\n\n\n\nOr calculate the vertex coordinates in advance:\n\nl <- layout_in_circle(net.bg)\nplot(net.bg, layout = l)\n\n\n\n# Using tidygraph\n# graph <- play_barabasi_albert(n = 100, power = 1.2)\ngraph %>% ggraph(., layout = \"circle\") +\n  geom_edge_link(color = \"grey\") +\n  geom_node_point(color = \"orange\", size = 2) +\n  theme_graph() +\n  theme(aspect.ratio = 1)\n\n\n\n\nl is simply a matrix of x,y coordinates (N x 2) for the N nodes in the graph. You can generate your own:\n\nl\n\n                [,1]          [,2]\n  [1,]  1.000000e+00  0.000000e+00\n  [2,]  9.980267e-01  6.279052e-02\n  [3,]  9.921147e-01  1.253332e-01\n  [4,]  9.822873e-01  1.873813e-01\n  [5,]  9.685832e-01  2.486899e-01\n  [6,]  9.510565e-01  3.090170e-01\n  [7,]  9.297765e-01  3.681246e-01\n  [8,]  9.048271e-01  4.257793e-01\n  [9,]  8.763067e-01  4.817537e-01\n [10,]  8.443279e-01  5.358268e-01\n [11,]  8.090170e-01  5.877853e-01\n [12,]  7.705132e-01  6.374240e-01\n [13,]  7.289686e-01  6.845471e-01\n [14,]  6.845471e-01  7.289686e-01\n [15,]  6.374240e-01  7.705132e-01\n [16,]  5.877853e-01  8.090170e-01\n [17,]  5.358268e-01  8.443279e-01\n [18,]  4.817537e-01  8.763067e-01\n [19,]  4.257793e-01  9.048271e-01\n [20,]  3.681246e-01  9.297765e-01\n [21,]  3.090170e-01  9.510565e-01\n [22,]  2.486899e-01  9.685832e-01\n [23,]  1.873813e-01  9.822873e-01\n [24,]  1.253332e-01  9.921147e-01\n [25,]  6.279052e-02  9.980267e-01\n [26,] -1.608143e-16  1.000000e+00\n [27,] -6.279052e-02  9.980267e-01\n [28,] -1.253332e-01  9.921147e-01\n [29,] -1.873813e-01  9.822873e-01\n [30,] -2.486899e-01  9.685832e-01\n [31,] -3.090170e-01  9.510565e-01\n [32,] -3.681246e-01  9.297765e-01\n [33,] -4.257793e-01  9.048271e-01\n [34,] -4.817537e-01  8.763067e-01\n [35,] -5.358268e-01  8.443279e-01\n [36,] -5.877853e-01  8.090170e-01\n [37,] -6.374240e-01  7.705132e-01\n [38,] -6.845471e-01  7.289686e-01\n [39,] -7.289686e-01  6.845471e-01\n [40,] -7.705132e-01  6.374240e-01\n [41,] -8.090170e-01  5.877853e-01\n [42,] -8.443279e-01  5.358268e-01\n [43,] -8.763067e-01  4.817537e-01\n [44,] -9.048271e-01  4.257793e-01\n [45,] -9.297765e-01  3.681246e-01\n [46,] -9.510565e-01  3.090170e-01\n [47,] -9.685832e-01  2.486899e-01\n [48,] -9.822873e-01  1.873813e-01\n [49,] -9.921147e-01  1.253332e-01\n [50,] -9.980267e-01  6.279052e-02\n [51,] -1.000000e+00 -3.216286e-16\n [52,] -9.980267e-01 -6.279052e-02\n [53,] -9.921147e-01 -1.253332e-01\n [54,] -9.822873e-01 -1.873813e-01\n [55,] -9.685832e-01 -2.486899e-01\n [56,] -9.510565e-01 -3.090170e-01\n [57,] -9.297765e-01 -3.681246e-01\n [58,] -9.048271e-01 -4.257793e-01\n [59,] -8.763067e-01 -4.817537e-01\n [60,] -8.443279e-01 -5.358268e-01\n [61,] -8.090170e-01 -5.877853e-01\n [62,] -7.705132e-01 -6.374240e-01\n [63,] -7.289686e-01 -6.845471e-01\n [64,] -6.845471e-01 -7.289686e-01\n [65,] -6.374240e-01 -7.705132e-01\n [66,] -5.877853e-01 -8.090170e-01\n [67,] -5.358268e-01 -8.443279e-01\n [68,] -4.817537e-01 -8.763067e-01\n [69,] -4.257793e-01 -9.048271e-01\n [70,] -3.681246e-01 -9.297765e-01\n [71,] -3.090170e-01 -9.510565e-01\n [72,] -2.486899e-01 -9.685832e-01\n [73,] -1.873813e-01 -9.822873e-01\n [74,] -1.253332e-01 -9.921147e-01\n [75,] -6.279052e-02 -9.980267e-01\n [76,] -1.836910e-16 -1.000000e+00\n [77,]  6.279052e-02 -9.980267e-01\n [78,]  1.253332e-01 -9.921147e-01\n [79,]  1.873813e-01 -9.822873e-01\n [80,]  2.486899e-01 -9.685832e-01\n [81,]  3.090170e-01 -9.510565e-01\n [82,]  3.681246e-01 -9.297765e-01\n [83,]  4.257793e-01 -9.048271e-01\n [84,]  4.817537e-01 -8.763067e-01\n [85,]  5.358268e-01 -8.443279e-01\n [86,]  5.877853e-01 -8.090170e-01\n [87,]  6.374240e-01 -7.705132e-01\n [88,]  6.845471e-01 -7.289686e-01\n [89,]  7.289686e-01 -6.845471e-01\n [90,]  7.705132e-01 -6.374240e-01\n [91,]  8.090170e-01 -5.877853e-01\n [92,]  8.443279e-01 -5.358268e-01\n [93,]  8.763067e-01 -4.817537e-01\n [94,]  9.048271e-01 -4.257793e-01\n [95,]  9.297765e-01 -3.681246e-01\n [96,]  9.510565e-01 -3.090170e-01\n [97,]  9.685832e-01 -2.486899e-01\n [98,]  9.822873e-01 -1.873813e-01\n [99,]  9.921147e-01 -1.253332e-01\n[100,]  9.980267e-01 -6.279052e-02\n\nl <- cbind(1:vcount(net.bg), c(1, vcount(net.bg):2))\nplot(net.bg, layout = l)\n\n\n\n# Using tidygraph\n# graph <- play_barabasi_albert(n = 100, power = 1.2)\ngraph %>% ggraph(., layout = l) +\n  geom_edge_link(color = \"grey\") +\n  geom_node_point(color = \"orange\", size = 2)+\n  theme_graph()\n\n\n\n\nThis layout is just an example and not very helpful - thankfully igraph has a number of built-in layouts, including:\n\nRandomly placed vertices\n\n\nl <- layout_randomly(net.bg)\nplot(net.bg, layout = l)\n\n\n\n# Using tidygraph\n# graph <- play_barabasi_albert(n = 100, power = 1.2)\ngraph %>% ggraph(., layout = layout_randomly(.)) + \n  geom_edge_link0(colour = \"grey\") + \n  geom_node_point(colour = \"orange\", size = 4)\n\n\n\n\n\nCircle layout\n\n\nl <- layout_in_circle(net.bg)\nplot(net.bg, layout = l)\n\n\n\n# Using tidygraph\n# graph <- play_barabasi_albert(n = 100, power = 1.2)\ngraph %>% ggraph(., layout = layout_in_circle(.)) + \n  geom_edge_link0(colour = \"grey\") + \n  geom_node_point(colour = \"orange\") +\n  theme(aspect.ratio = 1)\n\n\n\n\n\n3D sphere layout\n\n\nl <- layout_on_sphere(net.bg)\nplot(net.bg, layout = l)\n\n\n\n# Using tidygraph\n# graph <- play_barabasi_albert(n = 100, power = 1.2)\ngraph %>% ggraph(., layout = layout_on_sphere(.)) + \n  geom_edge_link0(colour = \"grey\") + \n  geom_node_point(colour = \"orange\")\n\n\n\n\n\nThe Fruchterman-Reingold force-directed algorithm: Nice but slow, most often used in graphs smaller than ~1000 vertices.\n\n\nl <- layout_with_fr(net.bg)\nplot(net.bg, layout = l)\n\n\n\n# Using tidygraph\n# graph <- play_barabasi_albert(n = 100, power = 1.2)\ngraph %>% ggraph(., layout = layout_with_fr(.)) + \n  geom_edge_link0(colour = \"grey\") + \n  geom_node_point(colour = \"orange\")\n\n\n\n\nYou will also notice that the F-R layout is not deterministic - different runs will result in slightly different configurations. Saving the layout in l allows us to get the exact same result multiple times.\n\npar(mfrow = c(2,2), mar = c(1,1,1,1))\nplot(net.bg, layout = layout_with_fr)\nplot(net.bg, layout = layout_with_fr)\nplot(net.bg, layout = l)\nplot(net.bg, layout = l)\n\n\n\n\nBy default, the coordinates of the plots are rescaled to the [-1,1] interval for both x and y. You can change that with the parameter rescale = FALSE and rescale your plot manually by multiplying the coordinates by a scalar. You can use norm_coords to normalize the plot with the boundaries you want. This way you can create more compact or spread out layout versions.\n\n#Get the layout coordinates:\nl <- layout_with_fr(net.bg)\n# Normalize them so that they are in the -1, 1 interval:\nl <- norm_coords(l, ymin = -1, ymax = 1, xmin = -1, xmax = 1)\n\npar(mfrow = c(2,2), mar = c(0,0,0,0))\nplot(net.bg, rescale = F, layout = l*0.4)\nplot(net.bg, rescale = F, layout = l*0.8)\nplot(net.bg, rescale = F, layout = l*1.2)\nplot(net.bg, rescale = F, layout = l*1.6)\n\n\n\n# Using tidygraph\n# Can't do this with tidygraph ( multiplying layout * scalar ), it seems\n\nAnother popular force-directed algorithm that produces nice results for connected graphs is Kamada Kawai. Like Fruchterman Reingold, it attempts to minimize the energy in a spring system.\n\nl <- layout_with_kk(net.bg)\nplot(net.bg, layout = l)\n\n\n\n# Using tidygraph\n# graph <- play_barabasi_albert(n = 100, power = 1.2)\ngraph %>% ggraph(., layout = layout_with_kk(.)) + \n  geom_edge_link0(colour = \"grey\") + \n  geom_node_point(colour = \"orange\", size = 4)\n\n\n\n\nThe MDS (multidimensional scaling) algorithm tries to place nodes based on some measure of similarity or distance between them. More similar/less distant nodes are placed closer to each other. By default, the measure used is based on the shortest paths between nodes in the network. That can be changed with the dist parameter.\n\nplot(net.bg, layout = layout_with_mds)\n\n\n\n# Using tidygraph\n# graph <- play_barabasi_albert(n = 100, power = 1.2)\ngraph %>% ggraph(., layout = layout_with_mds(.)) + \n  geom_edge_link0(colour = \"grey\") + \n  geom_node_point(colour = \"orange\", size = 4)\n\n\n\n\nThe LGL algorithm is for large connected graphs. Here you can specify a root- the node that will be placed in the middle of the layout.\n\nplot(net.bg, layout = layout_with_lgl)\n\n\n\n# Using tidygraph\n# graph <- play_barabasi_albert(n = 100, power = 1.2)\ngraph %>% ggraph(., layout = layout_with_lgl(.)) + \n  geom_edge_link0(colour = \"grey\") + \n  geom_node_point(colour = \"orange\", size = 4)\n\n\n\n\nBy default, igraph uses a layout called layout_nicely which selects an appropriate layout algorithm based on the properties of the graph. Check out all available layouts in igraph:\n\n?igraph::layout_\n\n\nlayouts <- grep(\"^layout_\", ls(\"package:igraph\"), value = TRUE)[-1]\n\n# Remove layouts that do not apply to our graph.\nlayouts <- layouts[!grepl(\"bipartite|merge|norm|sugiyama|tree\", layouts)]\n\npar(mfrow = c(3,3), mar = c(1,1,1,1))\n\nfor (layout in layouts) {\n  print(layout)\n  l <- do.call(layout, list(net))\n  plot(net, edge.arrow.mode = 0, layout = l, main = layout) }\n\n[1] \"layout_as_star\"\n\n\n[1] \"layout_components\"\n\n\n[1] \"layout_in_circle\"\n\n\n[1] \"layout_nicely\"\n\n\n[1] \"layout_on_grid\"\n\n\n[1] \"layout_on_sphere\"\n\n\n[1] \"layout_randomly\"\n\n\n[1] \"layout_with_dh\"\n\n\n[1] \"layout_with_drl\"\n\n\n\n\n\n[1] \"layout_with_fr\"\n\n\n[1] \"layout_with_gem\"\n\n\n[1] \"layout_with_graphopt\"\n\n\n[1] \"layout_with_kk\"\n\n\n[1] \"layout_with_lgl\"\n\n\n[1] \"layout_with_mds\""
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#highlighting-specific-nodes-or-links",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#highlighting-specific-nodes-or-links",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "‚Äî‚Äî-~~ Highlighting specific nodes or links ‚Äî‚Äî‚Äì",
    "text": "‚Äî‚Äî-~~ Highlighting specific nodes or links ‚Äî‚Äî‚Äì\nSometimes we want to focus the visualization on a particular node or a group of nodes. Let‚Äôs represent distance from the NYT:\n\n\ndistances() calculates shortest path from vertices in ‚Äòv‚Äô to ones in ‚Äòto‚Äô.\n\n\ndist.from.NYT <- distances(net, \n                           v = V(net)[media == \"NY Times\"], \n                           to = V(net), \n                           weights = NA)\n\n#Set colors to plot the distances:\noranges <- colorRampPalette(c(\"dark red\", \"gold\"))\ncol <- oranges(max(dist.from.NYT)+1)\ncol <- col[dist.from.NYT+1]\n\n# Let's have same coordinates for Nodes in both graph renderings\n# Then we can verify that the distance calculations are the same for both renderings\ncoords <- igraph::layout_nicely(net)\nplot(net, vertex.label = dist.from.NYT,\n     vertex.color = col, vertex.label.color = \"black\",\n     layout = coords)\n\n\n\n\n\n# Using tidygraph\ngraph <- tbl_graph(nodes, links, directed = TRUE)\ngraph\n\n# A tbl_graph: 17 nodes and 49 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 17 √ó 5 (active)\n  id    media               media.type type.label audience.size\n  <chr> <chr>                    <int> <chr>              <int>\n1 s01   NY Times                     1 Newspaper             20\n2 s02   Washington Post              1 Newspaper             25\n3 s03   Wall Street Journal          1 Newspaper             30\n4 s04   USA Today                    1 Newspaper             32\n5 s05   LA Times                     1 Newspaper             20\n6 s06   New York Post                1 Newspaper             50\n# ‚Ä¶ with 11 more rows\n#\n# Edge Data: 49 √ó 4\n   from    to type      weight\n  <int> <int> <chr>      <int>\n1     1     2 hyperlink     22\n2     1     3 hyperlink     22\n3     1     4 hyperlink     21\n# ‚Ä¶ with 46 more rows\n\n# Set up NY Times as root node first\n# V(net)[media == \"NY Times\"] cannot be used since it returns an `igraph.vs` ( i.e. a list ) object. \n# We need an integer node id.\nroot_nyt <- graph %>%\n  activate(nodes) %>%\n  as_tibble() %>%\n  rowid_to_column(var = \"node_id\") %>%\n  filter(media == \"NY Times\") %>%\n  select(node_id) %>% as_vector()\nroot_nyt\n\nnode_id \n      1 \n\ngraph %>%\n  activate(nodes) %>%\n  mutate(size = centrality_degree()) %>%\n  \n  # new stuff:\n  # breadth first search for all distances from the root node\n  mutate(order = bfs_dist(root = root_nyt)) %>%\n  \n  ggraph(., layout = coords) + # same layout\n  geom_edge_link(\n    aes(width = weight),\n    color = \"grey80\",\n    end_cap = circle(.2, \"cm\"),\n    arrow = arrow(\n      type = \"closed\",\n      ends = \"last\",\n      length = unit(1, \"mm\")\n    )\n  ) +\n  geom_node_point(aes(fill = order,\n                      size = size),\n                  shape = 21,\n                  color = \"black\") +\n  \n  geom_node_text(aes(label = order)) +\n  \n  scale_fill_gradient(\n    name = \"Distance from NY Times\",\n    low = \"dark red\",\n    high = \"gold\",\n    guide = \"legend\"\n  ) +\n  scale_edge_width(range = c(0.2, 1.5), guide = \"none\") +\n  scale_size_continuous(\"Degree\", range =  c(2, 16)) +\n  guides(fill = guide_legend(override.aes = list(pch = 21,\n                                                 size = 4)))\n\n\n\n\nOr, a bit more readable:\n\nplot(net, vertex.color = col, \n     vertex.label = dist.from.NYT, edge.arrow.size = .6,\n     vertex.label.color = \"white\", \n     vertex.size = V(net)$size*1.6, \n     edge.width = 2,\n     layout = norm_coords(layout_with_lgl(net))*1.4, rescale = F)"
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#path-highlighting",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#path-highlighting",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "Path Highlighting",
    "text": "Path Highlighting\nWe can also highlight paths between the nodes in the network.\n\nSay here between MSNBC and the New York Post\n\n\nnews.path <- shortest_paths(net,\n                            from  =  V(net)[media == \"MSNBC\"],\n                            to   =  V(net)[media == \"New York Post\"],\n                            output  =  \"both\")  #both path nodes and edges\nnews.path.distance <- distances(net,\n                                V(net)[media == \"MSNBC\"],\n                                V(net)[media == \"New York Post\"] )\nnews.path\n\n$vpath\n$vpath[[1]]\n+ 4/17 vertices, named, from 8e65729:\n[1] s08 s03 s12 s06\n\n\n$epath\n$epath[[1]]\n+ 3/48 edges from 8e65729 (vertex names):\n[1] s08->s03 s03->s12 s12->s06\n\n\n$predecessors\nNULL\n\n$inbound_edges\nNULL\n\nnews.path.distance\n\n    s06\ns08   5\n\n#Generate edge color variable to plot the path:\necol <- rep(\"gray80\", ecount(net))\necol[unlist(news.path$epath)] <- \"orange\"\n\n#Generate edge width variable to plot the path:\new <- rep(2, ecount(net))\new[unlist(news.path$epath)] <- 4\n\n#Generate node color variable to plot the path:\nvcol <- rep(\"gray40\", vcount(net))\nvcol[unlist(news.path$vpath)] <- \"gold\"\n\nplot(net, vertex.color = vcol, \n     edge.color = ecol,\n     edge.width = ew, \n     edge.arrow.mode = 0,\n     ## added lines\n     vertex.label = V(net)$media,\n     vertex.label.font = 2, \n     vertex.label.color = \"gray40\",\n     vertex.label.cex = .7,\n     layout = coords * 1.5)\n\n\n\n\n\n# Using tidygraph\n# We need to use:\n# to_shortest_path(graph, from, to, mode = \"out\", weights = NULL)\n# Let's set up `to` and `from` nodes\n#\n# V(net)[media == \"NY Times\"] cannot be used since it returns an `igraph.vs` ( i.e. a list ) object.\n# We need integer node ids for `from` and `to` in `to_shortest_path`\n\nmsnbc <- graph %>%\n  activate(nodes) %>%\n  as_tibble() %>%\n  rowid_to_column(var = \"node_id\") %>%\n  filter(media == \"MSNBC\") %>%\n  select(node_id) %>% as_vector()\nmsnbc\n\nnode_id \n      8 \n\nnypost <- graph %>%\n  activate(nodes) %>%\n  as_tibble() %>%\n  rowid_to_column(var = \"node_id\") %>%\n  filter(media == \"New York Post\") %>%\n  select(node_id) %>% as_vector()\nnypost\n\nnode_id \n      6 \n\n# Let's create a fresh graph object using morph\n# However we want to merge it back with the original `graph`\n# to get an overlay plot\n#\n# # Can do this to obtain a separate graph\n# convert(to_shortest_path,from = msnbc,to = nypost)\n# However we want to merge it back with the original `graph`\n# to get an overlay plot\nmsnbc_nyp <-\n  graph %>%\n  # first mark all nodes and edges as *not* on the shortest path\n  activate(nodes) %>%\n  mutate(shortest_path_node = FALSE) %>%\n  activate(edges) %>%\n  mutate(shortest_path_edge = FALSE) %>%\n  \n  # Find shortest path between MSNBC and NY Post\n  morph(to_shortest_path, from = msnbc, to = nypost) %>%\n  \n  # Now to mark the shortest_path nodes as TRUE\n  activate(nodes) %>%\n  mutate(shortest_path_node = TRUE) %>%\n  \n  # Now to mark the shortest_path edges as TRUE\n  activate(edges) %>%\n  mutate(shortest_path_edge = TRUE) %>%\n  #\n  # Merge back into main graph; Still saving it as a `msnbc_nyp`\n  unmorph()\nmsnbc_nyp\n\n# A tbl_graph: 17 nodes and 49 edges\n#\n# A directed multigraph with 1 component\n#\n# Edge Data: 49 √ó 5 (active)\n   from    to type      weight shortest_path_edge\n  <int> <int> <chr>      <int> <lgl>             \n1     1     2 hyperlink     22 FALSE             \n2     1     3 hyperlink     22 FALSE             \n3     1     4 hyperlink     21 FALSE             \n4     1    15 mention       20 FALSE             \n5     2     1 hyperlink     23 FALSE             \n6     2     3 hyperlink     21 FALSE             \n# ‚Ä¶ with 43 more rows\n#\n# Node Data: 17 √ó 6\n  id    media               media.type type.label audience.size shortest_path_n‚Ä¶\n  <chr> <chr>                    <int> <chr>              <int> <lgl>           \n1 s01   NY Times                     1 Newspaper             20 FALSE           \n2 s02   Washington Post              1 Newspaper             25 FALSE           \n3 s03   Wall Street Journal          1 Newspaper             30 TRUE            \n# ‚Ä¶ with 14 more rows\n\nmsnbc_nyp %>%\n  activate(nodes) %>%\n  mutate(size = centrality_degree()) %>%\n  ggraph(layout = coords) +\n  #geom_edge_link0(colour = \"grey\") +\n  geom_edge_link0(aes(colour = shortest_path_edge,\n                      width = shortest_path_edge)) +\n  \n  geom_node_point(aes(size = size,\n                      fill = shortest_path_node), shape = 21) +\n  geom_node_text(aes(label = media)) +\n  \n  scale_size_continuous(\"Degree\", range =  c(2, 16)) +\n  scale_fill_manual(\"Shortest Path\",\n                    values = c(\"grey\", \"gold\")) +\n  \n  scale_edge_width_manual(values = c(1, 4)) +\n  \n  scale_edge_colour_manual(values = c(\"grey\", \"orange\")) +\n  guides(\n    fill = guide_legend(override.aes = list(pch = 21,\n                                            size = 6)),\n    edge_colour = \"none\",\n    edge_width = \"none\"\n  )\n\n\n\n\n\nHighlight the edges going into or out of a vertex, for instance the WSJ. For a single node, use incident(), for multiple nodes use incident_edges()\n\n\n\ninc.edges <-\n  incident(net, V(net)[media == \"Wall Street Journal\"], mode = \"all\")\n\n#Set colors to plot the selected edges.\necol <- rep(\"gray80\", ecount(net))\necol[inc.edges] <- \"orange\"\nvcol <- rep(\"grey40\", vcount(net))\nvcol[V(net)$media == \"Wall Street Journal\"] <- \"gold\"\nplot(\n  net,\n  vertex.color = vcol,\n  edge.color = ecol,\n  edge.width = 2,\n  layout = coords\n)\n\n\n\n\n\n# Using tidygraph\nwsj <- graph %>% \n  activate(nodes) %>% \n  as_tibble() %>% \n  rowid_to_column(var = \"node_id\") %>% \n  filter(media == \"Wall Street Journal\") %>% \n  select(node_id) %>% as_vector()\n\ngraph %>% \n  activate(nodes) %>% \n  mutate(wsj_adjacent = node_is_adjacent(to = wsj, mode = \"all\", \n                                         include_to = TRUE),\n         size = centrality_degree()) %>% \n  mutate(WSJ = if_else(media == \"Wall Street Journal\", TRUE, FALSE)) %>% \n  activate(edges) %>% \n  mutate(wsj_links = edge_is_incident(wsj)) %>% \n  \n  ggraph(., layout = coords) +\n  geom_edge_link0(aes(colour = wsj_links), width = 2) + \n  \n  geom_node_point(aes(fill = WSJ, \n                      size = size),shape = 21) +\n  \n  geom_node_text(aes(label = media), repel = TRUE) + \n  \n  scale_fill_manual(\"WSJ Neighbours\", \n                      values = c(\"grey\", \"gold\"), \n                      guide = guide_legend(override.aes = \n                                             list(pch = 21, \n                                                  size = 5))) + \n  scale_edge_colour_manual(\"WSJ Links\", \n                      values = c(\"grey\", \"orange\")) + \n  scale_size(\"Degree\", range = c( 2, 16)) +\n  ggtitle(label = \"Highlighting WSJ Neighbours and Links\") +\n  guides(shape = \"none\", fill = \"none\" #, colour = \"none\"\n  )"
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#highlight-neighbours",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#highlight-neighbours",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "Highlight Neighbours",
    "text": "Highlight Neighbours\nOr we can highlight the immediate neighbors of a vertex, say WSJ. The neighbors function finds all nodes one step out from the focal actor. To find the neighbors for multiple nodes, use adjacent_vertices(). To find node neighborhoods going more than one step out, use function ego() with parameter order set to the number of steps out to go from the focal node(s).\n\nneigh.nodes <- neighbors(net, V(net)[media == \"Wall Street Journal\"], mode = \"out\")\n\n# Set colors to plot the neighbors:\nvcol[neigh.nodes] <- \"#ff9d00\"\nplot(net, vertex.color = vcol)\n\n\n\n\n\n# Using tidygraph\nwsj <- graph %>% \n  activate(nodes) %>% \n  as_tibble() %>% \n  rowid_to_column(var = \"node_id\") %>% \n  filter(media == \"Wall Street Journal\") %>% \n  select(node_id) %>% as_vector()\n\ngraph %>% \n  activate(nodes) %>% \n  mutate(wsj_adjacent = node_is_adjacent(to = wsj, mode = \"all\", \n  # remove WSJ from the list!\n  # highlight only the neighbours\n  \n                                         include_to = FALSE),\n         size = centrality_degree()) %>% \n  mutate(WSJ = if_else(media == \"Wall Street Journal\", TRUE, FALSE)) %>% \n  activate(edges) %>% \n  mutate(wsj_links = edge_is_incident(wsj)) %>% \n  \n  ggraph(., layout = coords) +\n  geom_edge_link0(aes(colour = wsj_links), width = 2) + \n  \n  geom_node_point(aes(fill = wsj_adjacent, \n                      size = size),shape = 21) +\n  \n  geom_node_text(aes(label = media), repel = TRUE) + \n  \n  scale_fill_manual(\"WSJ Neighbours\", \n                      values = c(\"grey\", \"gold\"), \n                      guide = guide_legend(override.aes = \n                                             list(pch = 21, \n                                                  size = 5))) + \n  scale_edge_colour_manual(\"WSJ Links\", \n                      values = c(\"grey\", \"orange\")) + \n  scale_size(\"Degree\", range = c( 2, 16)) +\n  ggtitle(label = \"Highlighting WSJ Neighbours and Links\") +\n  guides(shape = \"none\", fill = \"none\" #, colour = \"none\"\n  )\n\n\n\n\nAnother way to draw attention to a group of nodes: (This is generally not recommended since, depending on layout, nodes that are not ‚Äòmarked‚Äô can accidentally get placed on top of the mark)\n\nplot(net, mark.groups = c(1,4,5,8), mark.col = \"#C5E5E7\", mark.border = NA)\n\n\n\n# Mark multiple groups:\nplot(net, mark.groups = list(c(1,4,5,8), c(15:17)),\n          mark.col = c(\"#C5E5E7\",\"#ECD89A\"), mark.border = NA)"
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#interactive-plotting-with-tkplot",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#interactive-plotting-with-tkplot",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "‚Äî‚Äî-~~ Interactive plotting with ‚Äòtkplot‚Äô ‚Äî‚Äî‚Äì",
    "text": "‚Äî‚Äî-~~ Interactive plotting with ‚Äòtkplot‚Äô ‚Äî‚Äî‚Äì\nR and igraph offer interactive plotting capabilities (mostly helpful for small networks)\n\ntkid <- tkplot(net) #tkid is the id of the tkplot\n\nl <- tkplot.getcoords(tkid) # grab the coordinates from tkplot\nplot(net, layout = l)"
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#other-ways-to-represent-a-network",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#other-ways-to-represent-a-network",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "‚Äî‚Äî-~~ Other ways to represent a network ‚Äî‚Äî‚Äì",
    "text": "‚Äî‚Äî-~~ Other ways to represent a network ‚Äî‚Äî‚Äì\nOne reminder that there are other ways to represent a network:\n\nHeatmap of the network matrix:\n\n\nnetm <- as_adjacency_matrix(net, attr = \"weight\", sparse = F)\ncolnames(netm) <- V(net)$media\nrownames(netm) <- V(net)$media\n\npalf <- colorRampPalette(c(\"gold\", \"dark orange\"))\n\n# The Rowv & Colv parameters turn dendrograms on and off\nheatmap(netm[,17:1], Rowv  =  NA, Colv  =  NA, col  =  palf(20),\n        scale = \"none\", margins = c(10,10) )\n\n\n\n\n\nDegree distribution\n\n\ndeg.dist <- degree_distribution(net, cumulative = T, mode = \"all\")\n# degree is available in `sna` too\nplot(x = 0:max(igraph::degree(net)), y = 1-deg.dist, pch = 19, cex = 1.4, col = \"orange\", xlab = \"Degree\", ylab = \"Cumulative Frequency\")\n\n\n\n# Using Tidygraph\n# https://stackoverflow.com/questions/18356860/cumulative-histogram-with-ggplot2\ngraph %>% \n  activate(nodes) %>% \n  mutate(degree = centrality_degree(mode = \"all\")) %>% \n  as_tibble() %>% \n  ggplot(aes(x = degree, y = stat(count))) +\n  # geom_histogram(aes(y = cumsum(..count..)), binwidth = 1) + \n  stat_bin(aes(y = cumsum(..count..)),\n                binwidth = 1,# Ta-Da !!\n                geom =\"point\",color =\"orange\", size = 5)\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `after_stat(count)` instead."
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#plotting-two-mode-networks",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#plotting-two-mode-networks",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "4. Plotting two-mode networks",
    "text": "4. Plotting two-mode networks\n\nhead(nodes2)\n\n   id   media media.type media.name audience.size\n1 s01     NYT          1  Newspaper            20\n2 s02    WaPo          1  Newspaper            25\n3 s03     WSJ          1  Newspaper            30\n4 s04    USAT          1  Newspaper            32\n5 s05 LATimes          1  Newspaper            20\n6 s06     CNN          2         TV            56\n\nhead(links2)\n\n    U01 U02 U03 U04 U05 U06 U07 U08 U09 U10 U11 U12 U13 U14 U15 U16 U17 U18 U19\ns01   1   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\ns02   0   0   0   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\ns03   0   0   0   0   0   1   1   1   1   0   0   0   0   0   0   0   0   0   0\ns04   0   0   0   0   0   0   0   0   1   1   1   0   0   0   0   0   0   0   0\ns05   0   0   0   0   0   0   0   0   0   0   1   1   1   0   0   0   0   0   0\ns06   0   0   0   0   0   0   0   0   0   0   0   0   1   1   0   0   1   0   0\n    U20\ns01   0\ns02   1\ns03   0\ns04   0\ns05   0\ns06   0\n\nnet2\n\nIGRAPH 9008ce1 UN-B 30 31 -- \n+ attr: type (v/l), name (v/c)\n+ edges from 9008ce1 (vertex names):\n [1] s01--U01 s01--U02 s01--U03 s02--U04 s02--U05 s02--U20 s03--U06 s03--U07\n [9] s03--U08 s03--U09 s04--U09 s04--U10 s04--U11 s05--U11 s05--U12 s05--U13\n[17] s06--U13 s06--U14 s06--U17 s07--U14 s07--U15 s07--U16 s08--U16 s08--U17\n[25] s08--U18 s08--U19 s09--U06 s09--U19 s09--U20 s10--U01 s10--U11\n\nplot(net2)\n\n\n\n\nThis time we will make nodes look different based on their type. Media outlets are blue squares, audience nodes are orange circles:\n\nV(net2)$color <- c(\"steel blue\", \"orange\")[V(net2)$type+1]\nV(net2)$shape <- c(\"square\", \"circle\")[V(net2)$type+1]\n\n# Media outlets will have name labels, audience members will not:\nV(net2)$label <- \"\"\nV(net2)$label[V(net2)$type == F] <- nodes2$media[V(net2)$type == F]\nV(net2)$label.cex = .6\nV(net2)$label.font = 2\n\nplot(net2, vertex.label.color = \"white\", vertex.size = (2-V(net2)$type)*8)\n\n\n\n\n\n# Using tidygraph\nas_tbl_graph(x = links2, directed = TRUE) %>%\n  activate(nodes) %>%\n  left_join(nodes2, by = c(\"name\" = \"id\")) %>%\n  ggraph(layout = \"nicely\") +\n  geom_edge_link0() +\n  geom_node_point(aes(shape = type, fill = type, size = type)) +\n  geom_node_text(aes(label = if_else(type, \"\", media)), colour = \"white\", size = 3) +\n  \n  scale_shape_manual(\n    \"Type\",\n    values = c(22, 21),\n    labels = c(\"Media\", \"Persons\"),\n    guide = guide_legend(override.aes = list(size = 6))\n  ) +\n  \n  scale_fill_manual(\n    \"Type\",\n    values = c(\"dodgerblue\", \"orange\"),\n    labels = c(\"Media\", \"Persons\")\n  ) +\n  \n  scale_size_manual(values = c(10, 4), guide = \"none\") \n\n\n\n\nigraph has a built-in bipartite layout, though it‚Äôs not the most helpful:\n\nplot(net2, vertex.label = NA, vertex.size = 7, layout = layout_as_bipartite)\n\n\n\n# using tidygraph\nas_tbl_graph(x = links2, directed = TRUE) %>%\n  activate(nodes) %>%\n  left_join(nodes2, by = c(\"name\" = \"id\")) %>%\n  ggraph(., layout = \"igraph\", algorithm = \"bipartite\") +\n  geom_edge_link0() +\n  geom_node_point(aes(shape = type, fill = type, size = type)) +\n  geom_node_text(aes(label = if_else(type, \"\", media)), colour = \"white\", size = 3) +\n  \n  scale_shape_manual(\n    \"Type\",\n    values = c(22, 21),\n    labels = c(\"Media\", \"Persons\"),\n    guide = guide_legend(override.aes = list(size = 6))\n  ) +\n  \n  scale_fill_manual(\n    \"Type\",\n    values = c(\"dodgerblue\", \"orange\"),\n    labels = c(\"Media\", \"Persons\")\n  ) +\n  \n  scale_size_manual(values = c(10, 4), guide = \"none\") \n\n\n\n\n\nUsing text as nodes:\n\n\npar(mar = c(0,0,0,0))\nplot(net2, vertex.shape = \"none\", vertex.label = nodes2$media,\n     vertex.label.color = V(net2)$color, vertex.label.font = 2,\n     vertex.label.cex = .95, edge.color = \"gray70\",  edge.width = 2)\n\n\n\n# Using tidygraph\nas_tbl_graph(x = links2, directed = TRUE) %>%\n  activate(nodes) %>%\n  left_join(nodes2, by = c(\"name\" = \"id\")) %>%\n  ggraph(layout = \"nicely\") +\n  geom_edge_link(end_cap = circle(.4,\"cm\"), \n                 start_cap = circle(0.4, \"cm\")) +\n  # geom_node>point(aes(shape = type, fill = type, size = type)) +\n  geom_node_text(aes(label= media, colour = type), size = 4) +\n  \n  scale_shape_manual(\n    \"Type\",\n    values = c(22, 21),\n    labels = c(\"Media\", \"Persons\"),\n    guide = guide_legend(override.aes = list(size = 4))\n  ) +\n  \n  scale_fill_manual(\n    \"Type\",\n    values = c(\"dodgerblue\", \"orange\"),\n    labels = c(\"Media\", \"Persons\")\n  ) +\n  \n  scale_size_manual(values = c(10, 4), guide = \"none\") \n\n\n\n\n\nUsing images as nodes You will need the ‚Äòpng‚Äô package to do this:\n\n\n# install.packages(\"png\")\nlibrary(\"png\")\n\nimg.1 <- readPNG(\"./images/news.png\")\nimg.2 <- readPNG(\"./images/user.png\")\n\nV(net2)$raster <- list(img.1, img.2)[V(net2)$type+1]\n\npar(mar = c(3,3,3,3))\n\nplot(net2, vertex.shape = \"raster\", vertex.label = NA,\n     vertex.size = 16, vertex.size2 = 16, edge.width = 2)\n\n\n# By the way, you can also add any image you want to any plot. For example, many #network graphs could be improved by a photo of a puppy carrying a basket full of kittens.\nimg.3 <- readPNG(\"./images/puppy.png\")\nrasterImage(img.3,  xleft = -1.7, xright = 0, ybottom = -1.2, ytop = 0)\n\n\n\n# The numbers after the image are coordinates for the plot.\n# The limits of your plotting area are given in par()$usr\n\n\n# Using ~~tidygraph~~ visNetwork\n# See this cheatsheet:\n# system.file(\"fontAwesome/Font_Awesome_Cheatsheet.pdf\", package = \"visNetwork\")\nlibrary(visNetwork)\n\nas_tbl_graph(x = links2, directed = TRUE) %>%\n  activate(nodes) %>%\n  left_join(nodes2, by = c(\"name\" = \"id\")) %>% \n  \n  # visNetwork needs a \"group\" variable for grouping...\n  mutate(group = as.character(type)) %>% \n  visIgraph(.) %>% \n  visGroups(groupname = \"FALSE\",shape = \"icon\", \n            icon = list(code = \"f26c\", size = 75, color = \"orange\")) %>% \n  visGroups(groupname = \"TRUE\",shape = \"icon\", \n            icon = list(code = \"f007\", size = 75)) %>% \n  addFontAwesome()\n\n\n\n\n\nWe can also generate and plot bipartite projections for the two-mode network : (co-memberships are easy to calculate by multiplying the network matrix by its transposed matrix, or using igraph‚Äôs bipartite.projection function)\n\nnet2.bp <- bipartite.projection(net2)\n\n#We can calculate the projections manually as well:\nas_incidence_matrix(net2)  %*% t(as_incidence_matrix(net2))\n\n    s01 s02 s03 s04 s05 s06 s07 s08 s09 s10\ns01   3   0   0   0   0   0   0   0   0   1\ns02   0   3   0   0   0   0   0   0   1   0\ns03   0   0   4   1   0   0   0   0   1   0\ns04   0   0   1   3   1   0   0   0   0   1\ns05   0   0   0   1   3   1   0   0   0   1\ns06   0   0   0   0   1   3   1   1   0   0\ns07   0   0   0   0   0   1   3   1   0   0\ns08   0   0   0   0   0   1   1   4   1   0\ns09   0   1   1   0   0   0   0   1   3   0\ns10   1   0   0   1   1   0   0   0   0   2\n\nt(as_incidence_matrix(net2)) %*%   as_incidence_matrix(net2)\n\nWarning in rm(list = cmd, envir = .tkplot.env): object 'tkp.1' not found\n\n\n    U01 U02 U03 U04 U05 U06 U07 U08 U09 U10 U11 U12 U13 U14 U15 U16 U17 U18 U19\nU01   2   1   1   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0\nU02   1   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\nU03   1   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\nU04   0   0   0   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\nU05   0   0   0   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\nU06   0   0   0   0   0   2   1   1   1   0   0   0   0   0   0   0   0   0   1\nU07   0   0   0   0   0   1   1   1   1   0   0   0   0   0   0   0   0   0   0\nU08   0   0   0   0   0   1   1   1   1   0   0   0   0   0   0   0   0   0   0\nU09   0   0   0   0   0   1   1   1   2   1   1   0   0   0   0   0   0   0   0\nU10   0   0   0   0   0   0   0   0   1   1   1   0   0   0   0   0   0   0   0\nU11   1   0   0   0   0   0   0   0   1   1   3   1   1   0   0   0   0   0   0\nU12   0   0   0   0   0   0   0   0   0   0   1   1   1   0   0   0   0   0   0\nU13   0   0   0   0   0   0   0   0   0   0   1   1   2   1   0   0   1   0   0\nU14   0   0   0   0   0   0   0   0   0   0   0   0   1   2   1   1   1   0   0\nU15   0   0   0   0   0   0   0   0   0   0   0   0   0   1   1   1   0   0   0\nU16   0   0   0   0   0   0   0   0   0   0   0   0   0   1   1   2   1   1   1\nU17   0   0   0   0   0   0   0   0   0   0   0   0   1   1   0   1   2   1   1\nU18   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   1   1   1\nU19   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   1   1   1   2\nU20   0   0   0   1   1   1   0   0   0   0   0   0   0   0   0   0   0   0   1\n    U20\nU01   0\nU02   0\nU03   0\nU04   1\nU05   1\nU06   1\nU07   0\nU08   0\nU09   0\nU10   0\nU11   0\nU12   0\nU13   0\nU14   0\nU15   0\nU16   0\nU17   0\nU18   0\nU19   1\nU20   2\n\npar(mfrow = c(1, 2))\n\nplot(\n  net2.bp$proj1,\n  vertex.label.color = \"black\",\n  vertex.label.dist = 2,\n  vertex.label = nodes2$media[!is.na(nodes2$media.type)]\n)\n\nplot(\n  net2.bp$proj2,\n  vertex.label.color = \"black\",\n  vertex.label.dist = 2,\n  vertex.label = nodes2$media[is.na(nodes2$media.type)]\n)\n\n\n\n\n\n# Using tidygraph\n# Calculate projections and add attributes/labels\nproj1 <-\n  as_incidence_matrix(net2)  %*% t(as_incidence_matrix(net2)) %>%\n  as_tbl_graph() %>%\n  activate(nodes) %>%\n  left_join(., nodes2, by = c(\"name\" = \"id\"))\n\nWarning in (is.null(rownames(x)) && is.null(colnames(x))) || colnames(x) == :\n'length(x) = 10 > 1' in coercion to 'logical(1)'\n\nproj2 <-\n  t(as_incidence_matrix(net2)) %*%   as_incidence_matrix(net2) %>% as_tbl_graph() %>%\n  activate(nodes) %>%\n  left_join(., nodes2, by = c(\"name\" = \"id\"))\n\nWarning in (is.null(rownames(x)) && is.null(colnames(x))) || colnames(x) == :\n'length(x) = 20 > 1' in coercion to 'logical(1)'\n\np1 <- proj1 %>%\n  ggraph(layout = \"graphopt\") +\n  geom_edge_link0() +\n  geom_node_point(size = 6, colour = \"orange\") +\n  geom_node_text(aes(label = media), repel = TRUE)\n\np2 <- proj2 %>%\n  ggraph(layout = \"graphopt\") +\n  geom_edge_link0() +\n  geom_node_point(\n    aes(colour = media.type),\n    size = 6,\n    shape  = 15,\n    colour = \"dodgerblue\"\n  ) +\n  geom_node_text(aes(label = media), repel = TRUE)\n\np1 + p2"
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#plotting-multiplex-networks",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#plotting-multiplex-networks",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "5. Plotting multiplex networks",
    "text": "5. Plotting multiplex networks\nIn some cases, the networks we want to plot are multigraphs: they can have multiple edges connecting the same two nodes. A related concept, multiplex networks, contain multiple types of ties ‚Äì e.g. friendship, romantic, and work relationships between individuals.\nIn our example network, we also have two tie types: hyperlinks and mentions. One thing we can do is plot each type of tie separately:\n\nE(net)$width <- 2\nplot(\n  net,\n  edge.color = c(\"dark red\", \"slategrey\")[(E(net)$type == \"hyperlink\") +\n                                            1],\n  vertex.color = \"gray40\",\n  layout = layout_in_circle,\n  edge.curved = .3\n)\n\n\n\n# Another way to delete edges using the minus operator:\nnet.m <- net - E(net)[E(net)$type == \"hyperlink\"]\nnet.h <- net - E(net)[E(net)$type == \"mention\"]\n\n#Plot the two links separately:\npar(mfrow = c(1, 2))\n\nplot(net.h,\n     vertex.color = \"orange\",\n     layout = layout_with_fr,\n     main = \"Tie: Hyperlink\")\nplot(net.m,\n     vertex.color = \"lightsteelblue2\",\n     layout = layout_with_fr,\n     main = \"Tie: Mention\")\n\n\n\n\n\nMake sure the nodes stay in the same place in both plots:\n\n\npar(mfrow = c(1, 2), mar = c(1, 1, 4, 1))\n\nl <- layout_with_fr(net)\nplot(net.h,\n     vertex.color = \"orange\",\n     layout = l,\n     main = \"Tie: Hyperlink\")\nplot(net.m,\n     vertex.color = \"lightsteelblue2\",\n     layout = l,\n     main = \"Tie: Mention\")\n\n\n\n\n\n#Using tidygraph\n\nlayout <- layout_in_circle(net)\np1 <- tbl_graph(nodes, links, directed = TRUE) %>% \n  activate(nodes) %>% \n  mutate(size = centrality_degree()) %>% \n  activate(edges) %>% \n  filter(type == \"hyperlink\") %>% \n  \n  # reusing the earlier computed layout\n  ggraph(layout = layout) +\n  geom_edge_arc(strength = 0.05) +\n  geom_node_point(aes(size = size), shape = 21, \n                  fill = \"orange\") +\n  scale_size(range = c(2, 12)) + \n  labs(title = \"Tie: Hyperlink\") + \n  theme(aspect.ratio = 1,,\n        legend.position = \"bottom\")\n\np2 <- tbl_graph(nodes, links, directed = TRUE) %>% \n  activate(nodes) %>% \n  mutate(size = centrality_degree()) %>% \n  activate(edges) %>% \n  filter(type == \"mention\") %>% \n   # reusing the earlier computed layout\n  ggraph(layout = layout) +\n  geom_edge_arc(strength = 0.05) +\n  geom_node_point(aes(size = size), shape = 21, \n                  fill = \"lightsteelblue2\") +\n  scale_size(range = c(2, 12)) + \n  labs(title = \"Tie: Mention\") + \n  theme(aspect.ratio = 1, legend.position = \"bottom\")\n\nwrap_plots(p1, p2,guides = \"collect\") & \n  # note this \"pipe\" for patchwork!\n  theme(legend.position = \"none\")\n\n\n\n\nIn our example network, we don‚Äôt have node dyads connected by multiple types of connections (we never have both a ‚Äòhyperlink‚Äô and a ‚Äòmention‚Äô tie between the same two news outlets) ‚Äì however that could happen.\nNote: See the edges between s03 and s10‚Ä¶these are in opposite directions. So no dyads.\n\nlayout <- layout_in_circle(net)\ntbl_graph(nodes, links, directed = TRUE) %>%  \n  activate(nodes) %>% \n  mutate(size = centrality_degree()) %>% \n\n  # reusing the earlier computed layout\n  ggraph(layout = layout) +\n  geom_edge_arc(strength = 0.05, aes(colour = type)) +\n  geom_node_point(aes(size = size), shape = 21, \n                  fill = \"orange\") +\n  geom_node_text(aes(label = id), repel = TRUE) +\n  scale_size(range = c(2, 12)) + \n  labs(title = \"Tie: Hyperlink\") + \n  theme(aspect.ratio = 1,,\n        legend.position = \"bottom\")\n\n\n\n\nOne challenge in visualizing multiplex networks is that multiple edges between the same two nodes may get plotted on top of each other in a way that makes them impossible to distinguish. For example, let us generate a simple multiplex network with two nodes and three ties between them:\n\nmultigtr <- graph(edges = c(1, 2, 1, 2, 1, 2), n = 2)\n\nl <- layout_with_kk(multigtr)\n\n# Let's just plot the graph:\nplot(\n  multigtr,\n  vertex.color = \"lightsteelblue\",\n  vertex.frame.color = \"white\",\n  vertex.size = 40,\n  vertex.shape = \"circle\",\n  vertex.label = NA,\n  edge.color = c(\"gold\", \"tomato\", \"yellowgreen\"),\n  edge.width = 10,\n  edge.arrow.size = 5,\n  edge.curved = 0.1,\n  layout = l\n)\n\n\n\n# Using tidygraph\nmultigtr %>% \n  as_tbl_graph() %>% \n  activate(edges) %>% \n  mutate(edge_col = c(\"gold\", \"tomato\", \"yellowgreen\")) %>% \nggraph(., layout = l) +\n  geom_edge_arc(strength = 0.1, aes(colour = edge_col)) + \n  geom_node_point(size = 4, colour = \"lightsteelblue\") +\n  theme(legend.position = \"none\")\n\n\n\n\nBecause all edges in the graph have the same curvature, they are drawn over each other so that we only see the last one. What we can do is assign each edge a different curvature. One useful function in ‚Äòigraph‚Äô called curve_multiple() can help us here. For a graph G, curve.multiple(G) will generate a curvature for each edge that maximizes visibility.\n\nplot(\n  multigtr,\n  vertex.color = \"lightsteelblue\",\n  vertex.frame.color = \"white\",\n  vertex.size = 40,\n  vertex.shape = \"circle\",\n  vertex.label = NA,\n  edge.color = c(\"gold\", \"tomato\", \"yellowgreen\"),\n  edge.width = 10,\n  edge.arrow.size = 5,\n  edge.curved = curve_multiple(multigtr),\n  layout = l\n)\n\n\n\n\n\nmultigtr %>% \n  as_tbl_graph() %>% \n  activate(edges) %>% \n  mutate(edge_col = c(\"gold\", \"tomato\", \"yellowgreen\")) %>% \nggraph(., layout = l) +\n  geom_edge_fan(strength = 0.1, aes(colour = edge_col),width = 2) + \n  geom_node_point(size = 4, colour = \"lightsteelblue\") +\n  theme(legend.position = \"none\")\n\n\n\n\nAnd that is the end of this reoworked tutorial! Hope you enjoyed it and found it useful!!"
  },
  {
    "objectID": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html",
    "href": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html",
    "title": "A TRIZ Analysis of Lawrence of Arabia",
    "section": "",
    "text": "The movie Lawrence of Arabia is justly famous for its terrific story, great cast of characters and actors, and some truly legendary scenes. Here I take one of the iconic scenes from the movie, the attack on the port town of Aqaba and interpret the entire event and the build up to it from a TRIZ viewpoint.\nI will give a short description of the movie plot first.\nI then discuss the source of the problem, how it occurred to Lawrence, and how he went about solving it in this dramatic fashion. I give an analysis of the Problem from a (classical) TRIZ perspective, including the formulation of the Contradiction, Identification of Causes, the statement of the Ideal Final Result, and finally using the TRIZ Contradiction Matrix to find Inventive Principles that inform Lawrence‚Äôs solution. I also dwell in passing upon aspects of how this story can be interpreted as a manifestation of Mihaly Csikszentmihalyi‚Äôs Creativity Systems Model.\nThis piece stems from my teaching a course on Creative Thinking and Problem Solving based on TRIZ, titled Play and Invent, over the past 8 years or more at the Srishti Manipal Institute of Art, Design, and Technology, Bangalore, India. (https://srishtimanipalinstitute.in)."
  },
  {
    "objectID": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html#lawrence-of-arabia-a-summary",
    "href": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html#lawrence-of-arabia-a-summary",
    "title": "A TRIZ Analysis of Lawrence of Arabia",
    "section": "Lawrence of Arabia: a Summary",
    "text": "Lawrence of Arabia: a Summary\nThe movie is the story of T.E. Lawrence, the English officer who successfully united and led the diverse, often warring, Arab tribes during World War I in order to fight the Turks. The stellar cast includes Peter O‚ÄôToole as Lawrence, Omar Sharif as Ali, Alec Guinness as Prince Feisal, Anthony Quinn as Auda Abu Tayi, Claude Raines as Dryden, and Anthony Quayle as Col. Brighton. The director was David Lean. The editing of the film by Anne Coates is also much admired. (https://womenfilmeditors.princeton.edu/tag/lawrence-of-arabia/)\nLawrence is a complex, talented, and yet simple man, who is extremely well read (Greek philosophy and the Koran, for example) and is also an expert in Arab affairs and has considerable skill at map-making. Due to his being interpreted as insolent and insubordinate , he is given a lowly job at the HQ in Cairo. Dryden manages to convince the General that Lawrence should be allowed to go into Arabia and to find out what kind of long-term plans Prince Feisal is making for Arabia.\nHere is the map of the events that are unfolding in the movie at this time.1\n\n\n\n\n\nLawrence encounters Ali in dramatic fashion at the Masturah Well, on the way to meet Feisal, and his Arab guide is shot by Ali, a direct experience for Lawrence of inter-tribe rivalry in Arabia. (Ali is a Harith, and Tafas the guide was a Hashemi). Lawrence peremptorily rejects an offer of help from Ali, and finds his way alone to Wadi Safra, where Feisal is camped. He is met by Col. Brighton as he nears the camp. Both enter camp just in time to witness another bombing raid by Turkish airplanes.\nLater in the meeting with Feisal, Brighton tries to convince Feisal to retreat to Yenbo (Yanbu) and be out of range for the Turks, and where the British Army would supply them, train them to fight against the Turks. Feisal reluctantly accepts this plan, though he would rather the British navy take the port city of Aqaba and supply his army from there. Brighton simply scoffs at that idea, because the Turkish have 12 inch guns at Aqaba and the British have other things to do.\nLawrence has already intrigued Feisal by completing a verse from the Koran as it was being read by Selim, the cleric. At the end of the meeting, Feisal confronts Lawrence alone, as to his intentions in Arabia and finds out, to his astonishment, that Lawrence has his own interpretation of what his tasks and loyalties were, and these did not necessarily coincide with those of Brighton. In fact, Lawrence is not in favour of the Arab Army‚Äôs retreat to Yenbo, as it would become one small part of the British Army. As a parting remark, Feisal says to Lawrence that the Arabs need what no man can provide, a miracle.\nHere is the video of that terrific tent meeting scene."
  },
  {
    "objectID": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html#lawrences-problem",
    "href": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html#lawrences-problem",
    "title": "A TRIZ Analysis of Lawrence of Arabia",
    "section": "Lawrence‚Äôs Problem",
    "text": "Lawrence‚Äôs Problem\nLawrence does not sleep that night. Provoked by Feisal‚Äôs parting remark, he sits up all night on a sand dune close to the camp, thinking about how Aqaba could be taken, since he wants the Arabs to continue fighting from where they were, and even advance if possible with British help. His detailed understanding of the Arabian geography, his knowledge of the Aqaba port and its fortifications, all come to the fore here. Aqaba is a port at the head end of a narrow gulf to the east of the Sinai Peninsula.\nIn the early morning, seemingly in a eureka moment, he decides that attacking Aqaba from the landward side would be a good solution, since the guns there could not be turned around.\nHere is Lawrence trying to convince Ali about this plan:\n\n\n\n\nLawrence does not inform Brighton of his plans, nor even Feisal. It is Ali who informs Feisal of this enterprise. Clearly, Lawrence does not consider Brighton as a member of his Field (as defined by Csikszentmihalyi in his Creativity Systems Model), but Feisal is a Field Member to Ali.\nApropos, the act of sitting up all night can be seen as the Incubation and Elaboration stages of the 5 Stages of Creativity from Csikszentmihalyi (Preparation, Incubation, Insight, Elaboration, Execution)."
  },
  {
    "objectID": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html#a-triz-analysis-of-the-plan-to-take-aqaba",
    "href": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html#a-triz-analysis-of-the-plan-to-take-aqaba",
    "title": "A TRIZ Analysis of Lawrence of Arabia",
    "section": "A TRIZ Analysis of the Plan to Take Aqaba",
    "text": "A TRIZ Analysis of the Plan to Take Aqaba\nFor a TRIZ workflow, we proceed as follows:\nFirst, using the method described in Open Source TRIZ, (https://www.youtube.com/watch?v=cah0OhCH55k), we identify knobs or parameters within the situation and see how turning these could lead to identifying a Cause for a Problem in the form of a Contradiction.\nHere below is a quick Ishikawa Diagram for this purpose:\n\n\n\n\n\n\n\n\nTurning the knobs/parameters in the Ishikawa Diagram, it seems that if the British allies attack Aqaba, they may win, BUT they may lose a few warships. If the Arabs want to attack, they are too small in number and have no warships, and hence their chances of success are very slim. Herein lies the Contradiction, which we can now specify as an Administrative Contradiction(AC) in plain English:\n\n\nAC: The Arabs need the British to supply them via Aqaba port. Aqaba has huge guns and they will sink the British ships in that narrow gulf if they try a naval attack. So the Arabs need to take Aqaba without losing British ships.\n\n\nNext, based on this Contradiction and the inspection of the Ishikawa Diagram above, we are now ready to define the Ideal Final Result:\n\n\nIFR: The Arabs need to attack and take Aqaba port, and the big guns there should have no effect.\n\n\nNote how the tone of this IFR is like a ‚Äúeat my cake and have it too‚Äù. Very typical for IFRs, this impossible-sounding tone!\nLet us take the AC and convert it into a Technical Contradiction(TC). We will look at the 48 TRIZ Parameters in the TRIZ Contradiction Matrix (PDF) and see which Parameter we want to improve, while not worsening another. Here is what we can attempt, stating the Contradiction both ways2:\n\n- TC 1: Increase Duration of Action of a Moving Object (12) and not worsen Stability of Objects Composition (21)- TC 2: Increase Stability of Objects Composition (21) and not worsen Duration of Action of a Moving Object (12)\n\nHere we choose these Parameters based on our IFR that the guns at Aqaba should not affect the Arab attack at all. The Parameters chosen from the TRIZ Matrix can be thought of as metaphors for the knobs that lie within our AC. Going from the AC to the TC is an act of making metaphors. We could easily have chosen the Parameter Power(18) or Illumination Intensity(23) to ‚Äúmetaphorize‚Äù the effectiveness of the attack, if our imaginations run in that direction. There is here a considerable flexibility and possibility for imaginative interpretations of the AC, but using the language of TRIZ.\nWe could even stretch to making a Physical Contradiction(PC)3 happen:\n\n\nPC: The Ships must be near the guns but not be near enough to be shot at. (They must be near and not near at the same time)"
  },
  {
    "objectID": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html#solving-the-technical-contradiction",
    "href": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html#solving-the-technical-contradiction",
    "title": "A TRIZ Analysis of Lawrence of Arabia",
    "section": "Solving the Technical Contradiction",
    "text": "Solving the Technical Contradiction\nLet us take the both the TC-s into the Contradiction Matrix and arrive at the list of TRIZ Inventive Principles. Here is the Matrix solution for TC-1 in the figure below:\n\n\n\n\n\n\n\n\nThe Inventive Principles are:(TC1)\n\nIP 13 (The Other Way Around)\n\nIP 35 (Parameter Change)\n\nIP 24 (Intermediary)\n\nIP 40 (Composite Materials)\n\nand (TC2)\n\nIP 10 (Preliminary Action)\n\nIP 5 (Merging)\n\nIP 35 (Parameter Change)\n\nIP 13 (The Other Way Around)\n\nHow are we to apply these Inventive Principles? Here again is an imaginative exercise as we map these Generalized Solutions back into the Problem at hand:\n\nIP 13: The Other Way Around. How? Not attack by sea? Wait‚Ä¶ATTACK BY LAND!! Change the DIRECTION of Attack! So attack from the other side, the land side!! (We could retrospectively add this parameter to the Ishikawa Diagram too). Will this work? Yes, the guns can‚Äôt turn around !!\nIP 35: Parameter Change. But ‚Äúships‚Äù on land?? Note, the desert is an ocean into which no oar is dipped. Sand and Water are both Resources in the problem, as we have duly noted in the Ishikawa Diagram. So a different kind of ocean and therefore a different kind of ship? At a stretch, we can say the warships of the British Navy are being substituted with the use of ‚Ä¶.Camels!! And, metaphorically speaking, it is still an attack using ships‚Ä¶.The Ships of the Desert!! Parameter Change.\nIP 10: Prior Action. How? Lawrence and Ali are far from Aqaba and cannot do anything ‚Äúin advance‚Äù. What could this be?\nIP 5: Merging. However, on the way to Aqaba, Lawrence and Ali must recruit the Howeitat tribe ‚Äúin advance‚Äù of their attack !! As Lawrence tells Ali, If 50 men came out of the Nefud Desert, they might be 50 men other men would join. This is in accordance with what IP 10 is suggesting, to get other tribes to join in, in advance of the attack.\nIP 40: Composite Materials. What object within the situation can we reconstitute with smaller pieces of different types? The British Army‚Ä¶so an army made up of pieces? Yes! The Tribes need to unite into one composite army.\nAnd, instead of large warships, the Arabs switch to a composite force with camels‚Ä¶\n\nSo IP 13 works nicely now, along with IP 35 and IP 40, to give us a camel-borne attack from the landward side. IP 10 also teams up with IP 40 and IP 5 to give the idea of tribe unification.\nAnd so Lawrence and Ali, with the help of Auda Abu Tayi, attack Aqaba port from the landward side by crossing the Nefud desert on camels, and take it! And we have justified their decision using TRIZ !!\nHere is the final solution in action !!\n\n\n\n\n I hope that was as much fun to read as it was for me to write it up !!"
  },
  {
    "objectID": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html#points-to-ponder",
    "href": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html#points-to-ponder",
    "title": "A TRIZ Analysis of Lawrence of Arabia",
    "section": "Points to Ponder",
    "text": "Points to Ponder\n\nDo we each of us need a Dryden to vouch for us and help us get access to the Field?\nDoes TRIZ work in both mundane and industrial contexts? (Yes of course!)\nCan we just take the 40 Inventive Principles directly and throw them at every Problem, without necessarily going through the process of creating Contradictions and IFR? Hipple‚Äôs book has a remark in this direction."
  },
  {
    "objectID": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html#references",
    "href": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html#references",
    "title": "A TRIZ Analysis of Lawrence of Arabia",
    "section": "References",
    "text": "References\n\nLawrence of Arabia at the Internet Movie Data Base https://www.imdb.com/title/tt0056172/\n\nMihaly Csikszentmihalyi, Creativity, Flow, and the Psychology of Discovery and Invention. Harper Perennial; Reprint edition (August 6, 2013)\nJack Hipple, The Ideal Result and How to Achieve It. Springer; 2012th edition (June 26, 2012)\nValery Souchkov, Defining Contradictions. http://www.xtriz.com/Training/TRIZ_DefineContradiction_Tutorial.pdf"
  },
  {
    "objectID": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html",
    "href": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html",
    "title": "The TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad",
    "section": "",
    "text": "Here we go with another of my TRIZ Chronicles. The first edition analysing the famous movie Lawrence of Arabia is here.\nThe recent series of ads by Spotify have been very well received, by the ad industry as well. There have been several of these ads, each with a solid little story and a great punch line. There is one popular one that I have selected here, featuring a Mother shopping for veggies, while her son watches (and ‚Äúlistens‚Äù) to her bargaining with the shopkeeper. Here I take just that highly effective ad and interpret it from a TRIZ viewpoint.\nThis piece stems from my teaching a course on Creative Thinking and Problem Solving based on TRIZ, titled Play and Invent, over the past 8 years or more at the Srishti Manipal Institute of Art, Design, and Technology, Bangalore, India. (https://srishtimanipalinstitute.in)."
  },
  {
    "objectID": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#the-famous-spotify-ad",
    "href": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#the-famous-spotify-ad",
    "title": "The TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad",
    "section": "The Famous Spotify Ad",
    "text": "The Famous Spotify Ad\nLet us watch the Spotify ad first, before analyzing it!"
  },
  {
    "objectID": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#the-young-mans-problem",
    "href": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#the-young-mans-problem",
    "title": "The TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad",
    "section": "The Young Man‚Äôs Problem",
    "text": "The Young Man‚Äôs Problem\nIn order to make a story out of this, I want make a Protagonist out the young man in the ad. It is he who has the problem and he who is going to apply TRIZ to solve it. I discuss the source of his Problem and give an analysis of the Problem from a (classical) TRIZ perspective, including the formulation of the Contradiction, Identification of Causes, the statement of the Ideal Final Result, and finally using the TRIZ Contradiction Matrix to find Inventive Principles, that lead to the solution, which of course, is meant to unerringly include Spotify !\nFirst a philosophical digression:‚Äî\nSeveral authors have taken a Game View of life. James P Carse‚Äôs famous book titled Finite and Infinite Games speaks of Play, Types of Games, Rules, Winning and our own aims in the Game itself. A similar articulation is, in my opinion, that of Mihaly Csikszentlmihalyi in his concept of Flow, shown here below:\n\n\nFrom Sketchplanations\n\n\nWhen the Game presents very little Challenge, we are bored. When the Game demands extreme skills the challenge is too much for us and we experience anxiety. When the Challenge presented is just barely matched by our Skill, we are in the zone of Flow, or what I call Play.\nA good metaphoric image for this experience is as follows:‚Äî that we live in a space where the Floor of Boredom is always rising and would crush us against our Ceiling of Anxiety. One Way to deal with this is to develop more Skills and push the Ceiling away, effectively moving into the zone of Flow. Another Way of looking at this is what Carse suggests: When Play is no longer possible, change the Game.\nSo what does all this have to do with getting veggies?\nThe ad is, in my opinion, all about Boredom, and how to avoid it. And not offend anybody. The Young Man (hereinafter, ‚ÄúYM‚Äù) simply has to accompany his Mom, and be there while she gets the veggies. I will exaggerate his irritation and his boredom at the risk of offending young people likely to read this, and say that he would rather not be there but he does not want to hurt Mom.\nWe are now ready for the TRIZ based Analysis of this Problem!"
  },
  {
    "objectID": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#a-triz-analysis-of-a-visit-to-the-subzi-mandi",
    "href": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#a-triz-analysis-of-a-visit-to-the-subzi-mandi",
    "title": "The TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad",
    "section": "A TRIZ Analysis of a Visit to the Subzi Mandi",
    "text": "A TRIZ Analysis of a Visit to the Subzi Mandi\nFor a TRIZ workflow, we proceed as before:\nFirst, using the method described in Open Source TRIZ, we identify knobs or parameters within the situation and see how turning these could lead to identifying a Cause for a Problem in the form of a Contradiction.\nHere below is a quick Ishikawa Diagram to help us identify the Parameters of this Problem:\n\n\n\n\n\n\n\n\nTurning the knobs/parameters in the Ishikawa Diagram, it seems that if the YM goes to the market with Mom, he would most likely get bored, but would please Mom. If he doesn‚Äôt go, then he chills at home, but Mom is going to justifiably furious. Herein lies the Contradiction, which we can now specify as an Administrative Contradiction(AC) in plain English:\n\n\nAC: The YM wants to chill at home but Mom wants him to take her veggie shopping. He has to put up with the Waste of Time, and being bored, and Stress at being away from friends.\n\n\nNext, based on this Contradiction and the inspection of the Ishikawa Diagram above, we are now ready to define a TRIZ Ideal Final Result:\n\n\nIFR: The YM must go to the Market and not be bored.\n\n\nNote again the impossible sounding way of expressing the IFR! One needs practice, like the Queen in Alice in Wonderland, who could think of Six Impossible Things before Breakfast ! Also note there could be other ways of specifying the IFR. See below, section Alternative Ideas for IFR.\nLet us take the AC and convert it into a Technical Contradiction(TC), keeping this IFR in mind. We will look at the 48 TRIZ Parameters in the TRIZ Contradiction Matrix (PDF) and see which Parameter we want to improve, while not worsening another. Here is what we can obtain. We will analyze each Contradiction both ways1:\n\n- TC 1: Improve Loss of Time (26) and not worsen Effect of External Harmful Factors (30)\n- TC 2: Improve Increase Productivity (44) and not worsen Stress (19)\n\nHere we choose these Parameters based on our IFR that while going to the Market may be unavoidable, Boredom need not ensue. Parameters chosen from the TRIZ Matrix can be thought of as metaphors for the knobs that lie within our AC. Going from the AC to the TC is an act of making metaphors. We could easily have chosen the Parameter Noise(29) as the ‚Äúmetaphoric thing‚Äù to avoid, but the current IFR doesn‚Äôt quite support that. There is here a considerable flexibility and possibility for imaginative interpretations of the AC, but using the language of TRIZ.\nWe could also formulate a Physical Contradiction(PC)2:\n\n\nPC: The YM must be in the market and not be in the market at the same time.\n\n\nwhich is aimed squarely at one of the Assumptions in the Problem, that the YM simply has to go. Again, if the IFR is formulated differently we could obtain a very different set of AC and PC. See below, section Alternative Ideas for IFR.\nIn a future post, we will deal with using the PC and the TRIZ Separation Principles to solve Problems."
  },
  {
    "objectID": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#solving-the-technical-contradiction",
    "href": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#solving-the-technical-contradiction",
    "title": "The TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad",
    "section": "Solving the Technical Contradiction",
    "text": "Solving the Technical Contradiction\nLet us take the both the TC-s into the Contradiction Matrix and arrive at the list of TRIZ Inventive Principles. Here is the Matrix solution for TC-1 in the figure below:\n\n\n\n\n\n\n\n\n\nThe two squares for the TC1 have been circled in red, solving TC-1.\nThe Inventive Principles are:(TC1, TC2, both ways)\n\n1(Segmentation)\n35(Parameter Change)\n21(Skipping)\n18(Mechanical Vibration) (!!)\n2(Taking Out/Separation)\n10(Prior Action)\n\n36(Phase Transitions)\nand with TC2:\n\n3(Local Quality)\n14(Spheroidality/Curvature)\n9(Preliminary Anti-Action)\n37(Thermal Expansion)\n40(Composite Materials)\n25(Self Service)\n24(Intermediary)\n\nThat is a considerable list for us to try to use!! Let us apply some these Inventive Principles! Viewing these Inventive Principles as we Generalized Solutions we try to map these back into the Problem at hand:\n\n\n35(Parameter Change): Which Parameter to change? Location? No. Sound? Change the ‚ÄúBargaining Talk‚Äù into what? Sweet Musical Lyrics!!üéµü§£\n\n18(Mechanical Vibration) : What, make noise of your own? Yes! Play Music !!üîâ ü§£\n\n14(Spheroidality): Wear ‚Äúspherical‚Äù headphones!!üéß! Create a ‚Äúsound sphere‚Äù! This is a long shot!!\n\n3(Local Quality): also indicates the creation of a ‚Äúlocal‚Äù cocoon around the YM, but needs to be combined with 18(Mechanical Vibration) to truly arrive at the musical solution!\n\nOne could make decent interpretations of 2(Taking Out/Separation), and 24(Intermediary), but we are already there! The rest are perhaps (at least to me!) not very evocative, unless 37(Thermal Expansion) means ‚Äúthrow a temper tantrum at Mom‚Äù? Never! So there you have it! The Cinderella song played on Spotify becomes not just a noise canceller but actually seems to substitute the very conversation between Mom and the vendor. And the YM has successfully attained Flow ! And the IFR too, since with the music in his head, he is effectively ‚Äúin the marketplace and not in the marketplace at the same time!\nAnd I attained Flow in writing this!!"
  },
  {
    "objectID": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#alternative-ideas-for-ifr",
    "href": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#alternative-ideas-for-ifr",
    "title": "The TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad",
    "section": "Alternative Ideas for IFR",
    "text": "Alternative Ideas for IFR\nWe note in passing that there is more than one way of formulating the Ideal Final Result. Here are two more examples:\n\n\nIFR2: The veggies should arrive without (the YM) going to the Market\nIFR3: Food should be prepared without having to go buy veggies.\n\n\nClearly these are at least as good as the one we have chosen, sounding nicely ‚Äúimpossible‚Äù in their own right! The point is that in the analysis of the Problem, we do need to ask Who has the Problem, as we did, and the IFR needs to stem from there. These alternative IFRs could well be the Voice of (another) Customer.\nIf there is any interesting situation that could be analyzed with TRIZ, please send me a DM! Thanks !"
  },
  {
    "objectID": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#references",
    "href": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#references",
    "title": "The TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad",
    "section": "References",
    "text": "References\n\nJames P Carse, Finite and Infinite Games, Free Press, 1986. ISBN: 0-02-905980-1\nMihaly Csikszentmihalyi, Creativity, Flow, and the Psychology of Discovery and Invention. Harper Perennial; Reprint edition (August 6, 2013)\nJack Hipple, The Ideal Result and How to Achieve It. Springer; 2012th edition (June 26, 2012)\nValery Souchkov, Defining Contradictions. http://www.xtriz.com/Training/TRIZ_DefineContradiction_Tutorial.pdf"
  },
  {
    "objectID": "content/projects/fsp-discussions/index.html",
    "href": "content/projects/fsp-discussions/index.html",
    "title": "FSP Discussions 2021",
    "section": "",
    "text": "Slides and Tutorials\n.nbsp;.nbsp;"
  },
  {
    "objectID": "content/projects/fsp-discussions/index.html#introduction",
    "href": "content/projects/fsp-discussions/index.html#introduction",
    "title": "FSP Discussions 2021",
    "section": "Introduction",
    "text": "Introduction"
  },
  {
    "objectID": "content/projects/fsp-discussions/index.html#this-is-a-summary-of-the-group-discussion-among",
    "href": "content/projects/fsp-discussions/index.html#this-is-a-summary-of-the-group-discussion-among",
    "title": "FSP Discussions 2021",
    "section": "This is a summary of the group discussion among:",
    "text": "This is a summary of the group discussion among:\n1. Sadhvi Jawa\n2. Minashshi Singh\n3. Vidhu Gandhi\n4. Yash Bhandari\n5. Arvind Venkatadri"
  },
  {
    "objectID": "content/projects/fsp-doe/index.html",
    "href": "content/projects/fsp-doe/index.html",
    "title": "A Design of Experiments Class",
    "section": "",
    "text": "Code Lab"
  },
  {
    "objectID": "content/projects/fsp-doe/index.html#fa-envelope-introduction",
    "href": "content/projects/fsp-doe/index.html#fa-envelope-introduction",
    "title": "A Design of Experiments Class",
    "section": " Introduction",
    "text": "Introduction\nThis is a brief description and analysis of a Design of Experiments module conducted as a part of the Order and Chaos course, in the Foundation Studies Program (FSP 2021-2022) at SMI, MAHE, Bangalore."
  },
  {
    "objectID": "content/projects/fsp-doe/index.html#context",
    "href": "content/projects/fsp-doe/index.html#context",
    "title": "A Design of Experiments Class",
    "section": "Context",
    "text": "Context\nA Short Term Memory(STM) Test was the investigative tool used to verify several Hypotheses that were documented on the subject of STM.\nThis article describes the statistical analysis that was done with the readings. In particular, Permutations Tests were used to verify the effect size for each of three parameters that were hypothesized.\nFor more information, please click on the icon above to look at the Lab document."
  },
  {
    "objectID": "content/projects/fsp-doe/index.html#references",
    "href": "content/projects/fsp-doe/index.html#references",
    "title": "A Design of Experiments Class",
    "section": "References",
    "text": "References\n\nLawrance, A. J. 1996. ‚ÄúA Design of Experiments Workshop as an Introduction to Statistics.‚Äù American Statistician 50 (2): 156‚Äì58. doi:10.1080/00031305.1996.10474364.\nErnst, Michael D. 2004. ‚ÄúPermutation Methods: A Basis for Exact Inference.‚Äù Statistical Science 19 (4): 676‚Äì85. doi:10.1214/088342304000000396.\nPruim R, Kaplan DT, Horton NJ (2017). ‚ÄúThe mosaic Package: Helping Students to ‚ÄòThink with Data‚Äô Using R.‚Äù The R Journal, 9(1), 77‚Äì102. https://journal.r-project.org/archive/2017/RJ-2017-024/index.html."
  },
  {
    "objectID": "content/projects/fsp-manifesto/index.html",
    "href": "content/projects/fsp-manifesto/index.html",
    "title": "My Teaching Manifesto",
    "section": "",
    "text": "This is a short Statement of Values, Beliefs, and Content in my Teaching.\n\nArvind Venkatadri."
  },
  {
    "objectID": "content/projects/fsp-portfolio/index.html",
    "href": "content/projects/fsp-portfolio/index.html",
    "title": "Teaching in this Pandemic Year 2020-2021",
    "section": "",
    "text": "This is a short Portfolio of Teaching Initiatives and Student Outcomes during this pandemic year, 2020-2021, from Arvind Venkatadri."
  },
  {
    "objectID": "content/projects/fsp-portfolio-2022/index.html",
    "href": "content/projects/fsp-portfolio-2022/index.html",
    "title": "The Foundation Series",
    "section": "",
    "text": "This is a short Portfolio of Teaching Initiatives and Student Outcomes during this post(?)-pandemic year, 2021-2022, from Arvind Venkatadri."
  },
  {
    "objectID": "content/projects/listing.html",
    "href": "content/projects/listing.html",
    "title": "The Foundation Series",
    "section": "",
    "text": "This is a dummy blog posts\n\n\nThis is a test post. In this post, I try out different functionalities\n\n\n\nArvind Venkatadri\n\n\nJun 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject 2\n\n\nThis is a test post. In this post, I try out different functionalities\n\n\n\nArvind Venkatadri\n\n\nJun 1, 2022\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\nA Design of Experiments Class\n\n\nA Design of Experiments Class\n\n\n\nArvind Venkatadri\n\n\nJan 31, 2022\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\nA TRIZ Analysis of Lawrence of Arabia\n\n\nThe Attack on Aqaba: A TRIZ Analysis\n\n\n\nArvind Venkatadri\n\n\nDec 20, 2022\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\nA Tidygraph version of a Popular Network Science Tutorial\n\n\n\n\n\n\nArvind Venkatadri\n\n\nJun 16, 2021\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\nFSP Discussions 2021\n\n\n\n\n\n\nArvind Venkatadri\n\n\nJul 27, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy Teaching Manifesto\n\n\n\n\n\n\nArvind Venkatadri\n\n\nAug 20, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResources for Order and Chaos\n\n\n\n\n\n\nArvind Venkatadri\n\n\nDec 12, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTeaching in this Pandemic Year 2020-2021\n\n\n\n\n\n\nArvind Venkatadri\n\n\nAug 20, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad\n\n\nPunjabi Pop and Getting the Veggies\n\n\n\nArvind Venkatadri\n\n\nJan 9, 2023\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\n\n\n\n\nArvind Venkatadri\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/projects/project-1/index.html",
    "href": "content/projects/project-1/index.html",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\nThis is inline code plus a small code chunk.\n\nShow the Codelibrary(tidyverse)\n\nggplot(mpg) +\n  geom_jitter(aes(cty, hwy), size = 4, alpha = 0.5) \n\n\n\n\n\n\n\n\n\n\nTransforming OLS estimates\nMaximizing likelihood\n\n\n\n\nShow the Codepreds_lm %>% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  theme_minimal(base_size = 12) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\nShow the Codeglm.mod <- glm(sex ~ body_mass_g + bill_length_mm + species, family = binomial, data = dat)\n\npreds <- dat %>% \n  mutate(\n    prob.fit = glm.mod$fitted.values,\n    prediction = if_else(prob.fit > 0.5, 'male', 'female'),\n    correct = if_else(sex == prediction, 'correct', 'incorrect')\n  )\n\n\npreds %>% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  theme_minimal(base_size = 10) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\\[\n\\int_0^1 f(x) \\ dx\n\\]\n\n\n\n\nShow the Codegeom_density(\n  mapping = NULL,\n  data = NULL,\n  stat = \"density\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  outline.type = \"upper\"\n)\n\n\n\nShow the Codestat_density(\n  mapping = NULL,\n  data = NULL,\n  geom = \"area\",\n  position = \"stack\",\n  ...,\n  bw = \"nrd0\",\n  adjust = 1,\n  kernel = \"gaussian\",\n  n = 512,\n  trim = FALSE,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\n\n\n\n\n\nShow the Codeggplot(data = gapminder::gapminder, mapping = aes(x = lifeExp, fill = continent)) +\n  stat_density(position = \"identity\", alpha = 0.5)\n\n\n\n\nBla bla bla. This is a caption in the margin. Super cool isn‚Äôt it?"
  },
  {
    "objectID": "content/projects/project-2/index.html",
    "href": "content/projects/project-2/index.html",
    "title": "Project 2",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\nThis is inline code plus a small code chunk.\n\nShow the Codelibrary(tidyverse)\n\nggplot(mpg) +\n  geom_jitter(aes(cty, hwy), size = 4, alpha = 0.5) \n\n\n\n\n\n\n\n\n\n\nTransforming OLS estimates\nMaximizing likelihood\n\n\n\n\nShow the Codepreds_lm %>% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  theme_minimal(base_size = 12) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\nShow the Codeglm.mod <- glm(sex ~ body_mass_g + bill_length_mm + species, family = binomial, data = dat)\n\npreds <- dat %>% \n  mutate(\n    prob.fit = glm.mod$fitted.values,\n    prediction = if_else(prob.fit > 0.5, 'male', 'female'),\n    correct = if_else(sex == prediction, 'correct', 'incorrect')\n  )\n\n\npreds %>% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  theme_minimal(base_size = 10) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\\[\n\\int_0^1 f(x) \\ dx\n\\]\n\n\n\n\nShow the Codegeom_density(\n  mapping = NULL,\n  data = NULL,\n  stat = \"density\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  outline.type = \"upper\"\n)\n\n\n\nShow the Codestat_density(\n  mapping = NULL,\n  data = NULL,\n  geom = \"area\",\n  position = \"stack\",\n  ...,\n  bw = \"nrd0\",\n  adjust = 1,\n  kernel = \"gaussian\",\n  n = 512,\n  trim = FALSE,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\n\n\n\n\n\nShow the Codeggplot(data = gapminder::gapminder, mapping = aes(x = lifeExp, fill = continent)) +\n  stat_density(position = \"identity\", alpha = 0.5)\n\n\n\n\nBla bla bla. This is a caption in the margin. Super cool isn‚Äôt it?"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#image-right",
    "href": "content/slides/projects-slides/portfolio/index.html#image-right",
    "title": "Introduction to Networks in R",
    "section": ".image-right",
    "text": ".image-right\n\n\nWe can use the .image-right and .image-left classes to insert images in the background\nThese images will be placed behind most other content"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#image-left",
    "href": "content/slides/projects-slides/portfolio/index.html#image-left",
    "title": "Introduction to Networks in R",
    "section": ".image-left",
    "text": ".image-left\n\n\n\n\nIt is therefore recommened that you use multiple columns to only have text on the background area"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#section",
    "href": "content/slides/projects-slides/portfolio/index.html#section",
    "title": "Introduction to Networks in R",
    "section": "",
    "text": "background images"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#setting-background-colors",
    "href": "content/slides/projects-slides/portfolio/index.html#setting-background-colors",
    "title": "Introduction to Networks in R",
    "section": "Setting background colors",
    "text": "Setting background colors\nyou can set your background as you normally would"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#video-slide-title",
    "href": "content/slides/projects-slides/portfolio/index.html#video-slide-title",
    "title": "Introduction to Networks in R",
    "section": "Video Slide Title",
    "text": "Video Slide Title\nThis slides‚Äôs background video will play in a loop with audio muted."
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#slide-title",
    "href": "content/slides/projects-slides/portfolio/index.html#slide-title",
    "title": "Introduction to Networks in R",
    "section": "Slide Title",
    "text": "Slide Title"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#further-modifying-theme",
    "href": "content/slides/projects-slides/portfolio/index.html#further-modifying-theme",
    "title": "Introduction to Networks in R",
    "section": "Further Modifying theme",
    "text": "Further Modifying theme\nIf you want to modify theme, you can specify the .scss my modifying the yaml to look like this\nformat: \n  letterbox-revealjs:\n    theme: [default, style.scss]"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#modifying-letterbox-background",
    "href": "content/slides/projects-slides/portfolio/index.html#modifying-letterbox-background",
    "title": "Introduction to Networks in R",
    "section": "Modifying letterbox background",
    "text": "Modifying letterbox background\nThe background colors can be with with, where #444444 represents the new background color and #222222 represents the color of the shadow\n.quarto-light {\n  background-color: #444444;\n}\n\n.quarto-dark {\n  background-color: #444444;\n}\n\n.slides {\n  box-shadow: #222222 0px 0px 30px 0px;\n}"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#quarto",
    "href": "content/slides/projects-slides/portfolio/index.html#quarto",
    "title": "Introduction to Networks in R",
    "section": "Quarto",
    "text": "Quarto\nQuarto enables you to weave together content and executable code into a finished presentation. To learn more about Quarto presentations see https://quarto.org/docs/presentations/."
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#bullets",
    "href": "content/slides/projects-slides/portfolio/index.html#bullets",
    "title": "Introduction to Networks in R",
    "section": "Bullets",
    "text": "Bullets\nWhen you click the Render button a document will be generated that includes:\n\nContent authored with markdown\nOutput from executable code"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#code",
    "href": "content/slides/projects-slides/portfolio/index.html#code",
    "title": "Introduction to Networks in R",
    "section": "Code",
    "text": "Code\nWhen you click the Render button a presentation will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\n[1] 2"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/LICENSE.html",
    "href": "content/slides/projects-slides/portfolio/LICENSE.html",
    "title": "The Foundation Series",
    "section": "",
    "text": "MIT License\nCopyright (c) 2022 quarto-letterbox authors\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ‚ÄúSoftware‚Äù), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED ‚ÄúAS IS‚Äù, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "content/slides/r-slides/graphics/index.html#image-right",
    "href": "content/slides/r-slides/graphics/index.html#image-right",
    "title": "The Grammar of Graphics in R",
    "section": ".image-right",
    "text": ".image-right\n\n\nWe can use the .image-right and .image-left classes to insert images in the background\nThese images will be placed behind most other content"
  },
  {
    "objectID": "content/slides/r-slides/graphics/index.html#image-left",
    "href": "content/slides/r-slides/graphics/index.html#image-left",
    "title": "The Grammar of Graphics in R",
    "section": ".image-left",
    "text": ".image-left\n\n\n\n\nIt is therefore recommened that you use multiple columns to only have text on the background area"
  },
  {
    "objectID": "content/slides/r-slides/graphics/index.html#section",
    "href": "content/slides/r-slides/graphics/index.html#section",
    "title": "The Grammar of Graphics in R",
    "section": "",
    "text": "background images"
  },
  {
    "objectID": "content/slides/r-slides/graphics/index.html#setting-background-colors",
    "href": "content/slides/r-slides/graphics/index.html#setting-background-colors",
    "title": "The Grammar of Graphics in R",
    "section": "Setting background colors",
    "text": "Setting background colors\nyou can set your background as you normally would"
  },
  {
    "objectID": "content/slides/r-slides/graphics/index.html#video-slide-title",
    "href": "content/slides/r-slides/graphics/index.html#video-slide-title",
    "title": "The Grammar of Graphics in R",
    "section": "Video Slide Title",
    "text": "Video Slide Title\nThis slides‚Äôs background video will play in a loop with audio muted."
  },
  {
    "objectID": "content/slides/r-slides/graphics/index.html#slide-title",
    "href": "content/slides/r-slides/graphics/index.html#slide-title",
    "title": "The Grammar of Graphics in R",
    "section": "Slide Title",
    "text": "Slide Title"
  },
  {
    "objectID": "content/slides/r-slides/graphics/index.html#further-modifying-theme",
    "href": "content/slides/r-slides/graphics/index.html#further-modifying-theme",
    "title": "The Grammar of Graphics in R",
    "section": "Further Modifying theme",
    "text": "Further Modifying theme\nIf you want to modify theme, you can specify the .scss my modifying the yaml to look like this\nformat: \n  letterbox-revealjs:\n    theme: [default, style.scss]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/index.html#modifying-letterbox-background",
    "href": "content/slides/r-slides/graphics/index.html#modifying-letterbox-background",
    "title": "The Grammar of Graphics in R",
    "section": "Modifying letterbox background",
    "text": "Modifying letterbox background\nThe background colors can be with with, where #444444 represents the new background color and #222222 represents the color of the shadow\n.quarto-light {\n  background-color: #444444;\n}\n\n.quarto-dark {\n  background-color: #444444;\n}\n\n.slides {\n  box-shadow: #222222 0px 0px 30px 0px;\n}"
  },
  {
    "objectID": "content/slides/r-slides/graphics/index.html#quarto",
    "href": "content/slides/r-slides/graphics/index.html#quarto",
    "title": "The Grammar of Graphics in R",
    "section": "Quarto",
    "text": "Quarto\nQuarto enables you to weave together content and executable code into a finished presentation. To learn more about Quarto presentations see https://quarto.org/docs/presentations/."
  },
  {
    "objectID": "content/slides/r-slides/graphics/index.html#bullets",
    "href": "content/slides/r-slides/graphics/index.html#bullets",
    "title": "The Grammar of Graphics in R",
    "section": "Bullets",
    "text": "Bullets\nWhen you click the Render button a document will be generated that includes:\n\nContent authored with markdown\nOutput from executable code"
  },
  {
    "objectID": "content/slides/r-slides/graphics/index.html#code",
    "href": "content/slides/r-slides/graphics/index.html#code",
    "title": "The Grammar of Graphics in R",
    "section": "Code",
    "text": "Code\nWhen you click the Render button a presentation will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\n[1] 2"
  },
  {
    "objectID": "content/slides/r-slides/graphics/LICENSE.html",
    "href": "content/slides/r-slides/graphics/LICENSE.html",
    "title": "The Foundation Series",
    "section": "",
    "text": "MIT License\nCopyright (c) 2022 quarto-letterbox authors\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ‚ÄúSoftware‚Äù), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED ‚ÄúAS IS‚Äù, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "content/slides/r-slides/nature-of-data/index.html#claude-shannon-and-information",
    "href": "content/slides/r-slides/nature-of-data/index.html#claude-shannon-and-information",
    "title": "The Nature of Data",
    "section": "Claude Shannon and Information",
    "text": "Claude Shannon and Information\n\nhttps://plus.maths.org/content/information-surprise"
  },
  {
    "objectID": "content/slides/r-slides/nature-of-data/index.html#human-experience-is.data",
    "href": "content/slides/r-slides/nature-of-data/index.html#human-experience-is.data",
    "title": "The Nature of Data",
    "section": "Human Experience is‚Ä¶.Data??",
    "text": "Human Experience is‚Ä¶.Data??"
  },
  {
    "objectID": "content/slides/r-slides/nature-of-data/index.html#experiments-and-hypotheses-a-kitchen-experiment",
    "href": "content/slides/r-slides/nature-of-data/index.html#experiments-and-hypotheses-a-kitchen-experiment",
    "title": "The Nature of Data",
    "section": "Experiments and Hypotheses: A Kitchen Experiment",
    "text": "Experiments and Hypotheses: A Kitchen Experiment\n\n\nInputs are: Ingredients, Recipes, Processes\nOutputs are: Taste, Texture, Colour, Quantity!!\n\nUsed without permission from https://safetyculture.com/topics/design-of-experiments/"
  },
  {
    "objectID": "content/slides/r-slides/nature-of-data/index.html#what-is-the-result-of-an-experiment",
    "href": "content/slides/r-slides/nature-of-data/index.html#what-is-the-result-of-an-experiment",
    "title": "The Nature of Data",
    "section": "What is the Result of an Experiment?",
    "text": "What is the Result of an Experiment?\n\n\n\nAll experiments give us data about phenomena\nWe obtain data about the things that happen: Outputs\nWhat makes things happen?: Inputs\nHow?: Process\nWhen? Factors\nHow much ‚Äúoutput‚Äù is caused by how much ‚Äúinput‚Äù? Effect Size\n\n\nAll Experiments stem from Human Curiosity, a Hypothesis, and a Desire to Find out and Talk about Something"
  },
  {
    "objectID": "content/slides/r-slides/nature-of-data/index.html#a-famous-lady-and-her-famous-experiment",
    "href": "content/slides/r-slides/nature-of-data/index.html#a-famous-lady-and-her-famous-experiment",
    "title": "The Nature of Data",
    "section": "A Famous Lady and her Famous Experiment",
    "text": "A Famous Lady and her Famous Experiment\n\n\n\n\n\n\n\n\n\n\nIn 1853, Turkey declared war on Russia. After the Russian Navy destroyed a Turkish squadron in the Black Sea, Great Britain and France joined with Turkey. In September of the following year, the British landed on the Crimean Peninsula and set out, with the French and Turks, to take the Russian naval base at Sevastopol.\nWhat followed was a tragicomedy of errors ‚Äì failure of supply, failed communications, international rivalries. Conditions in the armies were terrible, and disease ate through their ranks. They finally did take Sevastopol a year later, after a ghastly assault. It was ugly business all around. Well over half a million soldiers lost their lives during the Crimean War."
  },
  {
    "objectID": "content/slides/r-slides/nature-of-data/index.html#florence-nightingales-data",
    "href": "content/slides/r-slides/nature-of-data/index.html#florence-nightingales-data",
    "title": "The Nature of Data",
    "section": "Florence Nightingale‚Äôs Data",
    "text": "Florence Nightingale‚Äôs Data\n\n\n\n\n\nMonth\nYear\nDisease.rate\nWounds.rate\nOther.rate\n\n\n\n\nApr\n1854\n1.4\n0.0\n7.0\n\n\nMay\n1854\n6.2\n0.0\n4.6\n\n\nJun\n1854\n4.7\n0.0\n2.5\n\n\nJul\n1854\n150.0\n0.0\n9.6\n\n\nAug\n1854\n328.5\n0.4\n11.9\n\n\nSep\n1854\n312.2\n32.1\n27.7\n\n\nOct\n1854\n197.0\n51.7\n50.1\n\n\nNov\n1854\n340.6\n115.8\n42.8\n\n\nDec\n1854\n631.5\n41.7\n48.0\n\n\nJan\n1855\n1022.8\n30.7\n120.0"
  },
  {
    "objectID": "content/slides/r-slides/nature-of-data/index.html#how-does-data-look-like-then",
    "href": "content/slides/r-slides/nature-of-data/index.html#how-does-data-look-like-then",
    "title": "The Nature of Data",
    "section": "How Does Data look Like, then?",
    "text": "How Does Data look Like, then?\n\n\n\n\n\nTypes of Variables - Using Interrogative Pronouns\n\nNominal: What? Who? Where? (Factors, Dimensions)\nOrdinal: Which Types? What Sizes? How Big? (Factors, Dimensions)\nInterval: How Often? (Numbers, Facts)\nRatio: How many? How much? How heavy? (Numbers, Facts)"
  },
  {
    "objectID": "content/slides/r-slides/nature-of-data/index.html#types-of-variables---using-interrogative-pronouns",
    "href": "content/slides/r-slides/nature-of-data/index.html#types-of-variables---using-interrogative-pronouns",
    "title": "The Nature of Data",
    "section": "Types of Variables - Using Interrogative Pronouns",
    "text": "Types of Variables - Using Interrogative Pronouns\n\nNominal: What? Who? Where? (Factors, Dimensions)\nOrdinal: Which Types? What Sizes? How Big? (Factors, Dimensions)\nInterval: How Often? (Numbers, Facts)\nRatio: How many? How much? How heavy? (Numbers, Facts)"
  },
  {
    "objectID": "content/slides/r-slides/nature-of-data/index.html#types-of-variables-in-nightingale-data",
    "href": "content/slides/r-slides/nature-of-data/index.html#types-of-variables-in-nightingale-data",
    "title": "The Nature of Data",
    "section": "Types of Variables in Nightingale Data",
    "text": "Types of Variables in Nightingale Data\n\n\n\nUsing Interrogative Pronouns:1\n\nNominal: None\nOrdinal: (Factors, Dimensions)\n\nHOW? War, Disease, Other\n\nInterval: (Numbers, Facts)\n\nWHEN? Year, Month\n\nRatio: (Numbers, Facts)\n\nHOW MANY? Rate of Deaths (War, Disease, Other)\n\n\n\n\n\n\n\nNightingale‚Äôs data table had dimensions coded into column names. This is not considered tidy in the modern age."
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#image-right",
    "href": "content/slides/r-slides/networks/index.html#image-right",
    "title": "Introduction to Networks in R",
    "section": ".image-right",
    "text": ".image-right\n\n\nWe can use the .image-right and .image-left classes to insert images in the background\nThese images will be placed behind most other content"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#image-left",
    "href": "content/slides/r-slides/networks/index.html#image-left",
    "title": "Introduction to Networks in R",
    "section": ".image-left",
    "text": ".image-left\n\n\n\n\nIt is therefore recommened that you use multiple columns to only have text on the background area"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#section",
    "href": "content/slides/r-slides/networks/index.html#section",
    "title": "Introduction to Networks in R",
    "section": "",
    "text": "background images"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#setting-background-colors",
    "href": "content/slides/r-slides/networks/index.html#setting-background-colors",
    "title": "Introduction to Networks in R",
    "section": "Setting background colors",
    "text": "Setting background colors\nyou can set your background as you normally would"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#video-slide-title",
    "href": "content/slides/r-slides/networks/index.html#video-slide-title",
    "title": "Introduction to Networks in R",
    "section": "Video Slide Title",
    "text": "Video Slide Title\nThis slides‚Äôs background video will play in a loop with audio muted."
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#slide-title",
    "href": "content/slides/r-slides/networks/index.html#slide-title",
    "title": "Introduction to Networks in R",
    "section": "Slide Title",
    "text": "Slide Title"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#further-modifying-theme",
    "href": "content/slides/r-slides/networks/index.html#further-modifying-theme",
    "title": "Introduction to Networks in R",
    "section": "Further Modifying theme",
    "text": "Further Modifying theme\nIf you want to modify theme, you can specify the .scss my modifying the yaml to look like this\nformat: \n  letterbox-revealjs:\n    theme: [default, style.scss]"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#modifying-letterbox-background",
    "href": "content/slides/r-slides/networks/index.html#modifying-letterbox-background",
    "title": "Introduction to Networks in R",
    "section": "Modifying letterbox background",
    "text": "Modifying letterbox background\nThe background colors can be with with, where #444444 represents the new background color and #222222 represents the color of the shadow\n.quarto-light {\n  background-color: #444444;\n}\n\n.quarto-dark {\n  background-color: #444444;\n}\n\n.slides {\n  box-shadow: #222222 0px 0px 30px 0px;\n}"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#quarto",
    "href": "content/slides/r-slides/networks/index.html#quarto",
    "title": "Introduction to Networks in R",
    "section": "Quarto",
    "text": "Quarto\nQuarto enables you to weave together content and executable code into a finished presentation. To learn more about Quarto presentations see https://quarto.org/docs/presentations/."
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#bullets",
    "href": "content/slides/r-slides/networks/index.html#bullets",
    "title": "Introduction to Networks in R",
    "section": "Bullets",
    "text": "Bullets\nWhen you click the Render button a document will be generated that includes:\n\nContent authored with markdown\nOutput from executable code"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#code",
    "href": "content/slides/r-slides/networks/index.html#code",
    "title": "Introduction to Networks in R",
    "section": "Code",
    "text": "Code\nWhen you click the Render button a presentation will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\n[1] 2"
  },
  {
    "objectID": "content/slides/r-slides/networks/LICENSE.html",
    "href": "content/slides/r-slides/networks/LICENSE.html",
    "title": "The Foundation Series",
    "section": "",
    "text": "MIT License\nCopyright (c) 2022 quarto-letterbox authors\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ‚ÄúSoftware‚Äù), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED ‚ÄúAS IS‚Äù, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "content/slides/r-slides/r-slides-listing.html",
    "href": "content/slides/r-slides/r-slides-listing.html",
    "title": "The Foundation Series",
    "section": "",
    "text": "Introduction to Networks in R\n\n\nUsing tidygraph and visNetwork\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nMIT License\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nMIT License\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nThe Grammar of Graphics in R\n\n\nUsing the tidyverse\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nThe Nature of Data\n\n\n\n\n\n\n\n\n\n\n\n\nArvind Venkatadri\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorking in R\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/slides/r-slides/working-in-R/index.html#introduction",
    "href": "content/slides/r-slides/working-in-R/index.html#introduction",
    "title": "Working in R",
    "section": "Introduction",
    "text": "Introduction"
  },
  {
    "objectID": "content/slides/r-slides/working-in-R/index.html#data-structures",
    "href": "content/slides/r-slides/working-in-R/index.html#data-structures",
    "title": "Working in R",
    "section": "Data Structures",
    "text": "Data Structures"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Metaphorics",
    "section": "",
    "text": "Hi, I‚Äôm Arvind Venkatadri.\nI‚Äôm an Adjunct Professor at DSU, Bangalore, INDIA, with a passion for R, data visualization, and Creative Thinking and Problem Solving with TRIZ. On this blog, I share and teach what I learn.\nTo get started, you can check out my courses. You can find me on Twitter or GitHub and YouTube. Feel free to reach out to me via mail !"
  },
  {
    "objectID": "readme.html",
    "href": "readme.html",
    "title": "The Foundation Series",
    "section": "",
    "text": "Preparatory Work to moving my full website to Quarto!"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/20-Basic-Simulation-Test/index.html#introduction-the-lady-who-drank-tea",
    "href": "content/courses/Basics-of-Modeling/Modules/20-Basic-Simulation-Test/index.html#introduction-the-lady-who-drank-tea",
    "title": "‚àù Simulation Tests",
    "section": "\n1 Introduction: The Lady who Drank Tea",
    "text": "1 Introduction: The Lady who Drank Tea\n\nThere is a famous story about a lady who claimed that tea with milk tasted different depending on whether the milk was added to the tea or the tea added to the milk. The story is famous because of the setting in which she made this claim. She was attending a party in Cambridge, England, in the 1920s. Also in attendance were a number of university dons and their wives. The scientists in attendance scoffed at the woman and her claim. What, after all, could be the difference?\nAll the scientists but one, that is. Rather than simply dismiss the woman‚Äôs claim, he proposed that they decide how one should test the claim. The tenor of the conversation changed at this suggestion, and the scientists began to discuss how the claim should be tested. Within a few minutes cups of tea with milk had been prepared and presented to the woman for tasting.\nAt this point, you may be wondering who the innovative scientist was and what the results of the experiment were. The scientist was R. A. Fisher, who first described this situation as a pedagogical example in his 1925 book on statistical methodology1. Fisher developed statistical methods that are among the most important and widely used methods to this day, and most of his applications were biological.1¬†R.A. Fisher. Statistical Methods for Research Workers. Oliver & Boyd, 1925\n\n\n1.1 Game\nLet‚Äôs try an experiment. I‚Äôll flip 10 coins. You guess which are heads and which are tails, and we‚Äôll see how you do. Please write down a sequence of ‚ÄúH‚Äù or ‚ÄúT‚Äù. Comparing with your classmates, we will undoubtedly see that some of you did better and others worse.\nWhat would be your impression of one of you got 9 guesses correct? Is that SKILL or is that something else? What would be your immediate reaction and next move?\n\n1.2 Analysis\nBack to the Lady who drank Tea !!\n\nLet‚Äôs suppose we decide to test the lady with ten cups of tea. We‚Äôll flip a coin to decide which way to prepare the cups. If we flip a head, we will pour the milk in first; if tails, we put the tea in first. Then we present the ten cups to the lady and have her state which ones she thinks were prepared each way.\nIt is easy to give her a score (9 out of 10, or 7 out of 10, or whatever it happens to be). It is trickier to figure out what to do with her score. Even if she is just guessing and has no idea, she could get lucky and get quite a few correct ‚Äì maybe even all 10. But how likely is that?\nNow let‚Äôs suppose the lady gets 9 out of 10 correct. That‚Äôs not perfect, but it is better than we would expect for someone who was just guessing. On the other hand, it is not impossible to get 9 out of 10 just by guessing.\nSo here is Fisher‚Äôs great idea: Let‚Äôs figure out how hard it is to get 9 out of 10 by guessing. If it‚Äôs not so hard to do, then perhaps that‚Äôs just what happened ( that she was guessing ), so we won‚Äôt be too impressed with the lady‚Äôs tea tasting ability. On the other hand, if it is really unusual to get 9 out of 10 correct by guessing, then we will have some evidence that she must be able to tell something ( and has an unusual Skill).\nBut how do we figure out how unusual it is to get 9 out of 10 just by guessing? Let‚Äôs just flip a bunch of coins and keep track. If the lady is just guessing, she might as well be flipping a coin.\nSo here‚Äôs the plan. We‚Äôll flip 10 coins. We‚Äôll call the heads correct guesses and the tails incorrect guesses.\n\n\n\nheads\n   0    1    2    3    4    5    6    7    8    9   10 \n   9  104  429 1152 2058 2428 2028 1243  442   97   10 \n\n\n\n\n\nSo what do we conclude? It is possible that the lady could get 9 or 10 correct just by guessing, but it is not very likely (it only happened in about 3% of our simulations). So one of two things must be true:\n‚Ä¢ The lady got unusually ‚Äúlucky‚Äù, or\n‚Ä¢ The lady is not just guessing"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/20-Basic-Simulation-Test/index.html#commentary",
    "href": "content/courses/Basics-of-Modeling/Modules/20-Basic-Simulation-Test/index.html#commentary",
    "title": "‚àù Simulation Tests",
    "section": "\n2 Commentary",
    "text": "2 Commentary\nFirst we realize something is surprising, and that we have a question or doubt. This is based on something we see, or measure, a test statistic. In our story, it is the score of \\(10/10\\) that the Lady was able to achieve about how the Tea was made.\nWe then assume the Lady is guessing and somehow by chance able to guess correctly. This would be our‚Ä¶.NULL Hypothesis. This is our (conservative) belief about the Real World.\nWe then randomly generate many Parallel Counterfactual Worlds, where we repeat the experiment many many times, each time calculating the test statistic, under the assumption of the NULL Hypothesis is TRUE.\nWe see how often our Parallel Worlds can mimic or exceed Real World measurement of the the test statistic by comparison. If this is common (i.e.¬†probability is high) we say we cannot reject the NULL Hypothesis (and the Lady is lucky). If the occurrence is rare, as in our case, we say we have reason to reject the NULL Hypothesis and reason to believe an underlying pattern (and Lady‚Äôs ability is beyond Question !)\nThis is the essence of the Simulation Method in statistical modelling. Take one more look at the picture from Allen Downey‚Äôs blog2, below:2¬†Allen Downey, There is still only one test, https://allendowney.blogspot.com/2016/06/there-is-still-only-one-test.html"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/20-Basic-Simulation-Test/index.html#conclusion",
    "href": "content/courses/Basics-of-Modeling/Modules/20-Basic-Simulation-Test/index.html#conclusion",
    "title": "‚àù Simulation Tests",
    "section": "\n3 Conclusion",
    "text": "3 Conclusion"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/20-Basic-Simulation-Test/index.html#references",
    "href": "content/courses/Basics-of-Modeling/Modules/20-Basic-Simulation-Test/index.html#references",
    "title": "‚àù Simulation Tests",
    "section": "\n4 References",
    "text": "4 References\n\nLaura Chihara, Tim Hesterberg, Mathematical Statistics with Resampling and R, Wiley, 2019.\nDaniel Kaplan, Statistical Modelling, Available free on the web. https://dtkaplan.github.io/SM2-bookdown/\nRussell A. Poldrack, Statistical Thinking for the 21st Century. Available free on the web. https://statsthinking21.github.io/statsthinking21-core-site/\nMine √áetinkaya-Rundel & Johanna Hardin, An Introduction to Modern Statistics, Available free on the web. https://www.openintro.org/book/ims/.\nStart Teaching Stats with R, by the Mosaic Project. All about the mosaic package in R. Available free on the web. https://github.com/ProjectMOSAIC/LittleBooks/raw/master/Starting/MOSAIC-StartTeaching.pdf\nD. Salsburg. The Lady Tasting Tea: How statistics revolutionized science in the twentieth century. W.H. Freeman, New York, 2001\nhttps://timesofindia.indiatimes.com/sports/cricket/icc-mens-t20-world-cup/in-numbers-virat-kohli-and-his-strange-luck-with-the-coin-toss/articleshow/87538443.cms"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/20-Basic-Simulation-Test/files/simulation.html",
    "href": "content/courses/Basics-of-Modeling/Modules/20-Basic-Simulation-Test/files/simulation.html",
    "title": "simulation",
    "section": "",
    "text": "In this module we will use simulation to solve several problems in Business Decision Making."
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/40-Means/index.html",
    "href": "content/courses/Basics-of-Modeling/Modules/40-Means/index.html",
    "title": "üÉè Permutation Test for Two Means",
    "section": "",
    "text": "R Tutorial¬†¬†\n  Orange Tutorial\n\n  Radiant Tutorial¬†\n  Datasets"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/40-Means/index.html#introduction",
    "href": "content/courses/Basics-of-Modeling/Modules/40-Means/index.html#introduction",
    "title": "üÉè Permutation Test for Two Means",
    "section": "\n2 Introduction",
    "text": "2 Introduction\nWe saw from the diagram created by Allen Downey that there is only one test! We will now use this philosophy to develop a technique that allows us to mechanize several Statistical Models in that way, with nearly identical code.\nWe will use two packages in R, mosaic and the relatively new infer package, to develop our intuition for what are called permutation based statistical tests."
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/40-Means/index.html#hypothesis-testing-using-permutation",
    "href": "content/courses/Basics-of-Modeling/Modules/40-Means/index.html#hypothesis-testing-using-permutation",
    "title": "üÉè Permutation Test for Two Means",
    "section": "\n3 Hypothesis Testing using Permutation",
    "text": "3 Hypothesis Testing using Permutation\nFrom Reference #1:\n\nHypothesis testing can be thought of as a 4-step process:\n\nState the null and alternative hypotheses.\nCompute a test statistic.\nDetermine the p-value.\n\nDraw a conclusion.\nIn a traditional introductory statistics course, once this general framework has been mastered, the main work is in applying the correct formula to compute the standard test statistics in step 2 and using a table or computer to determine the p-value based on the known (usually approximate) theoretical distribution of the test statistic under the null hypothesis.\nIn a simulation-based approach, steps 2 and 3 change. In Step 2, it is no longer required that the test statistic be normalized to conform with a known, named distribution. Instead, natural test statistics, like the difference between two sample means \\(y1 ‚àí y2\\) can be used.\nIn Step 3, we use randomization to approximate the sampling distribution of the test statistic. Our lady tasting tea example demonstrates how this can be done from first principles. More typically, we will use randomization to create new simulated data sets ( ‚ÄúParallel Worlds‚Äù) that are like our original data in some ways, but make the null hypothesis true. For each simulated data set, we calculate our test statistic, just as we did for the original sample. Together, this collection of test statistics computed from the simulated samples constitute our randomization distribution.\nWhen creating a randomization distribution, we will attempt to satisfy 3 guiding principles.\n\nBe consistent with the null hypothesis. We need to simulate a world in which the null hypothesis is true. If we don‚Äôt do this, we won‚Äôt be testing our null hypothesis.\nUse the data in the original sample. The original data should shed light on some aspects of the distribution that are not determined by null hypothesis. For example, a null hypothesis about a mean doesn‚Äôt tell us about the shape of the population distribution, but the data give us some indication.\nReflect the way the original data were collected.\n\n\nFrom Chihara and Hesterberg:\n\nThis is the core idea of statistical significance or classical hypothesis testing ‚Äì to calculate how often pure random chance would give an effect as large as that observed in the data, in the absence of any real effect. If that probability is small enough, we conclude that the data provide convincing evidence of a real effect.\n\n\n3.1 Permutations tests using mosaic::shuffle()\n\nThe mosaic package provides the shuffle() function as a synonym for sample(). When used without additional arguments, this will permute its first argument.\n\nShow the Codeshuffle(1:10)\n\n [1]  4  6  1  7  3  5  2  8  9 10\n\n\nApplying shuffle() to an explanatory variable in a model allows us to test the null hypothesis that the explanatory variable has, in fact, no explanatory power. This idea can be used to test\n\nthe equivalence of two or more means,\nthe equivalence of two or more proportions,\nwhether a regression parameter is 0.\n\nWe will now see examples of each of these models using Permutations."
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/40-Means/index.html#testing-for-two-or-more-means",
    "href": "content/courses/Basics-of-Modeling/Modules/40-Means/index.html#testing-for-two-or-more-means",
    "title": "üÉè Permutation Test for Two Means",
    "section": "\n4 Testing for Two or More Means",
    "text": "4 Testing for Two or More Means\n\n4.1 Case Study-1: Hot Wings Orders vs Gender (From Chihara and Hesterberg)\nA student conducted a study of hot wings and beer consumption at a Bar. She asked patrons at the bar to record their consumption of hot wings and beer over the course of several hours. She wanted to know if people who ate more hot wings would then drink more beer. In addition, she investigated whether or not gender had an impact on hot wings or beer consumption. Is the mean order value related to the gender of the person who is ordering?\n\n\n\n\n\n\n\ncategorical variables:  \n    name  class levels  n missing                                  distribution\n1 Gender factor      2 30       0 F (50%), M (50%)                             \n\nquantitative variables:  \n      name   class min    Q1 median    Q3 max     mean        sd  n missing\n1       ID integer   1  8.25   15.5 22.75  30 15.50000  8.803408 30       0\n2 Hotwings integer   4  8.00   12.5 15.50  21 11.93333  4.784554 30       0\n3     Beer integer   0 24.00   30.0 36.00  48 26.20000 11.842064 30       0\n\n\nLet us calculate the observed difference in Hotwings consumption between Males and Females ( Gender): (using the mosaic package)\n\n\n        F         M \n 9.333333 14.533333 \n\n\ndiffmean \n     5.2 \n\n\n\n\n\n\n\n\n\n\nThe observed difference in mean consumption of Hotwings between Males and Females is 5.2. There is also a ‚Äúvisible‚Äù difference in medians as seen from the pair of box plots above.\nCould this have occurred by chance? Here is our formulation of the Hypotheses:\n\\[\nH_0: \\mu_M\\ = \\mu_F\\\\\nH_a: \\mu_M\\ \\ne \\mu_F\\\\\n\\]Note that we have a two-sided test: we want to check for differences in mean order value, either way. So we perform a Permutation Test to check: we create a null distribution of the differences in mean by a shuffle operation on gender:\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  prop_TRUE \n0.000999001 \n\n\nThe \\(\\color{red}{red\\ line}\\) shows the actual measured mean difference in Hot Wings consumption. The probability that our Permutation distribution is able to equal or exceed that number is \\(0.001998002\\) and we have to reject the Null Hypothesis that the means are identical.\n\n4.2 Matched Pairs: Results from a diving championship.\nSometimes the data is collected on the same set of individual categories, e.g.¬†scores by sport persons in two separate tournaments, or sales of identical items in two separate locations of a chain store. Here we have swimming records across a Semi-Final and a Final:\n\n\n\n\n  \n\n\n\n\ncategorical variables:  \n     name  class levels  n missing\n1    Name factor     12 12       0\n2 Country factor      8 12       0\n                                   distribution\n1  SI Yajie (8.3%) ...                         \n2 Canada (16.7%), China (16.7%) ...            \n\nquantitative variables:  \n       name   class    min       Q1  median      Q3   max    mean       sd  n\n1 Semifinal numeric 313.70 322.2000 325.625 356.575 382.8 338.500 22.94946 12\n2     Final numeric 283.35 318.5875 358.925 387.150 397.5 350.475 40.02204 12\n  missing\n1       0\n2       0\n\n\nThe data is made up of paired observations per swimmer. So we need to take the difference between the two swim records for each swimmer and then shuffle the differences to either polarity. Another way to look at this is to shuffle the records between Semifinal and Final on a per Swimmer basis. There are 12 swimmers and therefore 12 paired records.\nIn order to ensure that the records are paired, we use the argument only.2=FALSE in the diffmean function:\n\n\n\n\n  \n\n\n\n  318.7-313.7  320.55-318.7 322.75-320.55  325.5-322.75  325.75-325.5 \n       12.350       -63.050         5.225        85.125      -114.150 \n   346-325.75    355.15-346 360.85-355.15  367.5-360.85   382.8-367.5 \n      102.200       -54.150        28.600        31.950         4.050 \n\n\n[1] 11.975\n\n\nHow would we formulate our Hypothesis?\n\\[\nH_0: \\mu_{semifinal} = \\mu_{final}\\\\\nH_a: \\mu_{semifinal} \\ne \\mu_{final}\\\n\\]\n\n\n [1]  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprop_TRUE \n0.1294787 \n\n\nHmm‚Ä¶so by generating 100000 shufflings of score differences, with polarities, it does appear that we can not only obtain the current observed difference but even surpass it frequently. So it does seem that there is no difference in means between SemiFinal and Final swimming scores.\n\n4.3 Walmart vs Target\nIs there a difference in the price of Groceries sold by the two retailers Target and Walmart? The data set Groceries contains a sample of grocery items and their prices advertised on their respective web sites on one specific day.\n\nInspect the data set, then explain why this is an example of matched pairs data.\nCompute summary statistics of the prices for each store.\nConduct a permutation test to determine whether or not there is a difference in the mean prices.\nCreate a histogram bar-chart of the difference in prices. What is unusual about Quaker Oats Life cereal?\nRedo the hypothesis test without this observation. Do you reach the same conclusion?\n\n\n\n\n\n  \n\n\n\n\ncategorical variables:  \n      name     class levels  n missing\n1  Product character     30 30       0\n2     Size character     24 30       0\n3    Units character     16 30       0\n4 UnitType character      3 30       0\n                                   distribution\n1 Annie's Macaroni & Cheese (3.3%) ...         \n2 18oz (10%), 12oz (6.7%) ...                  \n3 10 (10%), 15 (10%), 16 (10%) ...             \n4 oz (93.3%), bars (3.3%) ...                  \n\nquantitative variables:  \n     name   class  min     Q1 median    Q3  max     mean       sd  n missing\n1  Target numeric 0.99 1.8275  2.545 3.140 7.99 2.762333 1.582128 30       0\n2 Walmart numeric 1.00 1.7600  2.340 2.955 6.98 2.705667 1.560211 30       0\n\n\nWe see that the comparison is to be made between two prices for the same product, and hence this is one more example of paired data, as in Case Study #4. Let us plot the prices for the products:\n\n\n\n\n\n\n\n\nWe see that the price difference between Walmart and Target prices is highest for the Product named Quaker Oats Life Cereal Original. Let us check the mean difference in prices:\n\n\n   1-0.99    1.22-1 1.42-1.22 1.49-1.42 1.59-1.49 1.62-1.59 1.79-1.62 1.94-1.79 \n-0.580000  0.170000  0.210000 -0.100000  0.190000  0.070000  0.180000  0.160000 \n1.99-1.94 2.12-1.99 2.39-2.12  2.5-2.39  2.59-2.5 2.64-2.59 2.79-2.64 2.82-2.79 \n 0.090000  0.010000  0.200000  0.600000 -0.200000 -0.600000  0.660000  0.040000 \n2.99-2.82 3.19-2.99 3.49-3.19 3.99-3.49 4.79-3.99 7.19-4.79 7.99-7.19 \n 0.220000  1.263333 -1.183333 -0.480000  2.290000  2.190000  0.000000 \n\n\n[1] -0.05666667\n\n\nLet us perform the pair-wise permutation test on prices, by shuffling the two store names:\n\n\n [1]  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n[26] -1 -1 -1 -1 -1\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n[1] 0\n\n\nDoes not seem to be any significant difference in prices‚Ä¶\nSuppose we knock off the Quaker Cereal data item‚Ä¶\n\n\n[1] 2\n\n\n\n\n  \n\n\n\n[1] -0.1558621\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n[1] 0.01551"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/40-Means/index.html#conclusion",
    "href": "content/courses/Basics-of-Modeling/Modules/40-Means/index.html#conclusion",
    "title": "üÉè Permutation Test for Two Means",
    "section": "\n5 Conclusion",
    "text": "5 Conclusion\nTBD"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/40-Means/index.html#references",
    "href": "content/courses/Basics-of-Modeling/Modules/40-Means/index.html#references",
    "title": "üÉè Permutation Test for Two Means",
    "section": "\n6 References",
    "text": "6 References\n\nRandall Pruim, Nicholas J. HortonDaniel T. Kaplan, Start Teaching with R"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/50-Proportions/index.html",
    "href": "content/courses/Basics-of-Modeling/Modules/50-Proportions/index.html",
    "title": "üÉè Permutation Test for Two Proportions",
    "section": "",
    "text": "R Tutorial¬†¬†\n  Orange Tutorial\n\n  Radiant Tutorial¬†\n  Datasets"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/50-Proportions/index.html#sec-table-plots",
    "href": "content/courses/Basics-of-Modeling/Modules/50-Proportions/index.html#sec-table-plots",
    "title": "üÉè Permutation Test for Two Proportions",
    "section": "\n3.1 Table Plots",
    "text": "3.1 Table Plots\nWe can plot a heatmap-like mosaic chart for this table.\n\n3.1.1 Using ggplot\n\n\n\n\n\n\n\n\n\n\n3.1.2 Using ggmosaic"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/50-Proportions/index.html#section",
    "href": "content/courses/Basics-of-Modeling/Modules/50-Proportions/index.html#section",
    "title": "üÉè Permutation Test for Two Proportions",
    "section": "",
    "text": "3.1.3 Observed Statistic: the \\(X^2\\) metric\nWhen there are multiple proportions involved, the \\(X^2\\) test is what is used.\nLet us now perform the base chisq test: We need a table and then the chisq test:\n\n\n            Education\nDeathPenalty Left HS  HS Jr Col Bachelors Graduate\n      Favor      117 511     71       135       64\n      Oppose      72 200     16        71       50\n\n\nX.squared \n 23.45093 \n\n\n\n    Pearson's Chi-squared test\n\ndata:  tally(DeathPenalty ~ Education, data = gss2002)\nX-squared = 23.451, df = 4, p-value = 0.0001029\n\n\nWhat would our Hypotheses be?\n\\[\nH_0: \\text{Education does not affect Votes on Death Penalty} \\\\\nH_a: \\text{Education affects votes on Death Penalty}\n\\]\nWe should now repeat the test with permutations on Education:\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n prop_TRUE \n0.00029997 \n\n\nThe p-value is well below our threshold of $0.05%, so we would conclude that Education has a significant effect on DeathPenalty opinion!"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/50-Proportions/index.html#conclusion",
    "href": "content/courses/Basics-of-Modeling/Modules/50-Proportions/index.html#conclusion",
    "title": "üÉè Permutation Test for Two Proportions",
    "section": "\n3.2 Conclusion",
    "text": "3.2 Conclusion\nWhy would a permutation test be a good idea here?\nIn our basic X^2 test, we calculate the test statistic of X^2 and look up a theoretical null distribution for that statistic, and see how unlikely our observed value is.\nWith a permutation test, there are no assumptions of the null distribution: this is computed based on real data. We note in passing that, in this case, since the number of cases in each cell of the Contingency Table are fairly high ( >= 5) the resulting NULL distribution is of the \\(X^2\\) variety."
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/40-Means/files/two-means.html",
    "href": "content/courses/Basics-of-Modeling/Modules/40-Means/files/two-means.html",
    "title": "Permutation Tests for Two Means",
    "section": "",
    "text": "Does Verizon create a difference in Repair Times between ILEC and CLEC systems?\n\nShow the Codedata(\"Verizon\")\ninspect(Verizon)\n\n\ncategorical variables:  \n   name  class levels    n missing\n1 Group factor      2 1687       0\n                                   distribution\n1 ILEC (98.6%), CLEC (1.4%)                    \n\nquantitative variables:  \n  name   class min   Q1 median   Q3   max     mean       sd    n missing\n1 Time numeric   0 0.75   3.63 7.35 191.6 8.522009 14.78848 1687       0\n\n\nDescribe the Variables!\n\n\n\n\nWrite the Null and Alternate hypotheses here."
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/40-Means/files/two-means.html#case-story-2-recidivism",
    "href": "content/courses/Basics-of-Modeling/Modules/40-Means/files/two-means.html#case-story-2-recidivism",
    "title": "Permutation Tests for Two Means",
    "section": "\n2 Case Story-2: Recidivism",
    "text": "2 Case Story-2: Recidivism\nDo criminals released after a jail term commit crimes again? Does recidivism depend upon age?\n\nShow the Codedata(\"Recidivism\")\ninspect(Recidivism)\n\n\ncategorical variables:  \n     name  class levels     n missing\n1  Gender factor      2 17019       3\n2     Age factor      5 17019       3\n3   Age25 factor      2 17019       3\n4    Race factor     10 16988      34\n5 Offense factor      2 17022       0\n6   Recid factor      2 17022       0\n7    Type factor      3 17022       0\n                                   distribution\n1 M (87.7%), F (12.3%)                         \n2 25-34 (36.6%), 35-44 (23.7%) ...             \n3 Over 25 (81.9%), Under 25 (18.1%)            \n4 White-NonHispanic (67%) ...                  \n5 Felony (80.6%), Misdemeanor (19.4%)          \n6 No (68.4%), Yes (31.6%)                      \n7 No Recidivism (68.4%), New (20.2%) ...       \n\nquantitative variables:  \n  name   class min  Q1 median  Q3  max     mean       sd    n missing\n1 Days integer   0 241    418 687 1095 473.3275 283.1393 5386   11636\n\n\nDescribe the variables!\n\n2.1 Hypothesis Specification\nLet us see if the indidence of recidivism is dependent upon whether a person is aged less than or more than 25 years. Write the Null and Alternate hypotheses here.\n\\[\nH_0 = \\mu_{recid-age-25-minus}\\ = \\mu_{recid-age-25-plus}\\\\\nH_a = \\mu_{recid-age-25-minus}\\ \\ne\\mu_{recid-age-25-plus}\\\\\n\\]\nH_0: _{recid_25_minus}\n\nShow the CodeRecidivism\n\n\n\n  \n\n\n\nAlso, the variable Recid is a factor variable coded ‚ÄúYes‚Äù or ‚ÄúNo‚Äù. We ought to convert it to a numeric variable of 1‚Äôs and 0‚Äôs. Why?\n\n\n\n\n2.2 Null Distribution for Recidivism\n\n\n\n\n2.3 Recidivism Conclusion\n\n2.4 Case Study #3: Flight Delays\nLaGuardia Airport (LGA) is one of three major airports that serves the New York City metropolitan area. In 2008, over 23 million passengers and over 375 000 planes flew in or out of LGA. United Airlines and America Airlines are two major airlines that schedule services at LGA. The data set FlightDelays contains information on all 4029 departures of these two airlines from LGA during May and June 2009.\n\nShow the Codedata(\"FlightDelays\")\ninspect(FlightDelays)\n\n\ncategorical variables:  \n         name  class levels    n missing\n1     Carrier factor      2 4029       0\n2 Destination factor      7 4029       0\n3  DepartTime factor      5 4029       0\n4         Day factor      7 4029       0\n5       Month factor      2 4029       0\n6   Delayed30 factor      2 4029       0\n                                   distribution\n1 AA (72.1%), UA (27.9%)                       \n2 ORD (44.3%), DFW (22.8%), MIA (15.1%) ...    \n3 8-Noon (26.1%), Noon-4pm (26%) ...           \n4 Fri (15.8%), Mon (15.6%), Tue (15.6%) ...    \n5 June (50.4%), May (49.6%)                    \n6 No (85.2%), Yes (14.8%)                      \n\nquantitative variables:  \n          name   class min   Q1 median   Q3  max      mean         sd    n\n1           ID integer   1 1008   2015 3022 4029 2015.0000 1163.21645 4029\n2     FlightNo integer  71  371    691  787 2255  827.1035  551.30939 4029\n3 FlightLength integer  68  155    163  228  295  185.3011   41.78783 4029\n4        Delay integer -19   -6     -3    5  693   11.7379   41.63050 4029\n  missing\n1       0\n2       0\n3       0\n4       0\n\n\nThe variables in the FlightDelays dataset are:\n\n2.5 Hypothesis Specification\nLet us compute the proportion of times that each carrier‚Äôs flights was delayed more than 20 min. We will conduct a two-sided test to see if the difference in these proportions is statistically significant.\n\n\n\n\n2.6 Null Distribution for FlightDelays\n\n\n\n\n\n\n\nwhich is very small. Hence we reject the null Hypothesis that there is no difference between carriers on delay times."
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/50-Proportions/index.html#table-plots",
    "href": "content/courses/Basics-of-Modeling/Modules/50-Proportions/index.html#table-plots",
    "title": "üÉè Permutation Test for Two Proportions",
    "section": "\n3.1 Table Plots",
    "text": "3.1 Table Plots\n\n\nUsing ggplot\nUsing ggmosaic\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.1.1 Observed Statistic: the \\(X^2\\) metric\nWhen there are multiple proportions involved, the \\(X^2\\) test is what is used.\nLet us now perform the base chisq test: We need a table and then the chisq test:\n\n\n            Education\nDeathPenalty Left HS  HS Jr Col Bachelors Graduate\n      Favor      117 511     71       135       64\n      Oppose      72 200     16        71       50\n\n\nX.squared \n 23.45093 \n\n\n\n    Pearson's Chi-squared test\n\ndata:  tally(DeathPenalty ~ Education, data = gss2002)\nX-squared = 23.451, df = 4, p-value = 0.0001029\n\n\nWhat would our Hypotheses be?\n\\[\nH_0: \\text{Education does not affect Votes on Death Penalty} \\\\\nH_a: \\text{Education affects votes on Death Penalty}\n\\]\nWe should now repeat the test with permutations on Education:\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\nprop_TRUE \n9.999e-05 \n\n\nThe p-value is well below our threshold of $0.05%, so we would conclude that Education has a significant effect on DeathPenalty opinion!"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/50-Proportions/index.html#contingency-table-plots",
    "href": "content/courses/Basics-of-Modeling/Modules/50-Proportions/index.html#contingency-table-plots",
    "title": "üÉè Permutation Test for Two Proportions",
    "section": "\n3.1 Contingency Table Plots",
    "text": "3.1 Contingency Table Plots\nThe Contingency Table can be plotted, as we have seen, using a mosaic plot using several packages:\n\n\nUsing vcd\nUsing ggplot\nUsing ggmosaic\n\n\n\n\nShow the Codevcd::mosaic(gss_table, gp = shading_hsv)\n\n\n\n\n\n\n\n\n\nNeed a little more work, to convert the Contigency Table into a tibble:\n\nShow the Code# https://stackoverflow.com/questions/19233365/how-to-create-a-marimekko-mosaic-plot-in-ggplot2\n\ngss_summary <- gss2002 %>%\n  mutate(\n    Education = factor(\n      Education,\n      levels = c(\"Bachelors\", \"Graduate\", \"Jr Col\", \"HS\", \"Left HS\"),\n      labels = c(\"Bachelors\", \"Graduate\", \"Jr Col\", \"HS\", \"Left HS\")\n    ),\n    DeathPenalty = as.factor(DeathPenalty)\n  ) %>%\n  group_by(Education, DeathPenalty) %>%\n  summarise(count = n()) %>% # This is good for a chisq test\n  \n  # Add two more columns to facilitate mosaic/Marrimekko Plot\n  # \n  mutate(edu_count = sum(count), \n         edu_prop = count / sum(count)) %>%\n  ungroup() \n\n###################################\n\n\nggplot(data = gss_summary, aes( x = Education, y = edu_prop)) +\n  \n  geom_bar(aes(width = edu_count, fill = DeathPenalty), \n           stat = \"identity\", \n           position = \"fill\", \n           colour = \"black\") +\n  \n  geom_text(aes(label = scales::percent(edu_prop)), \n            position = position_stack(vjust = 0.5)) +\n\n\n# if labels are desired\n facet_grid(~ Education, scales = \"free_x\", space = \"free_x\") + \n  theme(scale_fill_brewer(palette = \"RdYlGn\")) + \n  # theme(panel.spacing.x = unit(0, \"npc\")) + # if no spacing preferred between bars\n  theme_void() \n\n\n\n\n\n\n\n\n\n\nShow the Code#library(ggmosaic)\n\nggplot(data = gss2002) +\n  geom_mosaic(aes(x = product(DeathPenalty, Education), fill = DeathPenalty))\n\n\n\n\n\n\n\n\n\n\n\n3.1.1 Observed Statistic: the \\(X^2\\) metric\nWhen there are multiple proportions involved, the \\(X^2\\) test is what is used.\n\n\nCode\nIntuitive Explanation\n\n\n\nLet us now perform the base chisq test: We need a table and then the chisq test: We see that our observed \\(X^2 = 23.45\\):\n\nShow the Code# gss_table <- tally(DeathPenalty ~ Education, data = gss2002)\n# gss_table\n\n# Get the observed chi-square statistic\nobservedChi2 <- mosaic::chisq(tally(DeathPenalty ~ Education, data = gss2002))\nobservedChi2\n\nX.squared \n       23 \n\nShow the Code# Actual chi-square test\nstats::chisq.test(tally(DeathPenalty ~ Education, data = gss2002))\n\n\n    Pearson's Chi-squared test\n\ndata:  tally(DeathPenalty ~ Education, data = gss2002)\nX-squared = 23, df = 4, p-value = 0.0001\n\n\n\n\nLet us look at the Contingency Table that we have:\n\n\n            Education\nDeathPenalty Left HS   HS Jr Col Bachelors Graduate  Sum\n      Favor      117  511     71       135       64  898\n      Oppose      72  200     16        71       50  409\n      Sum        189  711     87       206      114 1307\n\n\nIn the chi-square test, we check whether the two ( or more ) categorical variables are independent. To do this we perform a simple check on the Contingency Table. We first re-compute the totals in each row and column, based on what we could expect if there was independence (NULL Hypothesis). If the two variables were independent, then there should be no difference between real and expected scores.\nHow do we know what scores to expect?\nConsider the entry in location (1,1): 117.\nThe number of expected entries there is probability of an entry landing in that square times the total number of entries:\n$$\n\\[\\begin{align}\n\\text{Expected Value at [1,1]} &=\\\\\n&= prob_{row_1} * prob_{col_1} * \\text{Overall Total}\\\\\n&= \\frac{\\text{row_1 total}}{\\text{Overall Total}} *\\frac{\\text{col_1 total}}{\\text{Overall Total}} * \\text{Overall Total}\\\\\n&= \\frac{898}{1307} * \\frac{189}{1307} * 1307\\\\\n&= 129.65\n\\end{align}\\]\n$$\nProceeding in this way for all the 15 entries in the Contingency Table, we get the ‚ÄúExpected‚Äù Contingency Table:\n\n\n            Education\nDeathPenalty Left HS   HS Jr Col Bachelors Graduate  Sum\n      Favor      117  511     71       135       64  898\n      Oppose      72  200     16        71       50  409\n      Sum        189  711     87       206      114 1307\n\n\n            Education\nDeathPenalty Left HS   HS Jr Col Bachelors Graduate  Sum\n      Favor      130  489     60       142       78  898\n      Oppose      59  222     27        64       36  409\n      Sum        189  711     87       206      114 1307\n\n\nThe \\(X^2\\) statistic is sum of squared differences between Observed and Expected scores, scaled by the Expected Scores. For location [1,1] this would be: \\((117-130)^2/189\\). Do try to compute all of these and the \\(X^2\\) statistic by hand !!\n\n\n\n\n3.1.2 Hypotheses Definition\nWhat would our Hypotheses be?\n$$\nH_0: \\\nH_a: \\\n$$\n\n3.1.3 Permutation Test for Education\n\nWe should now repeat the test with permutations on Education:\n\nShow the Codenull_chisq <- do(10000) * chisq.test(tally(DeathPenalty ~ shuffle(Education), data = gss2002))\n\nhead(null_chisq)\n\n\n\n  \n\n\nShow the Codegf_histogram( ~ X.squared, data = null_chisq) %>% \n  gf_vline(xintercept = observedChi2, color = \"red\")\n\n\n\n\n\n\nShow the Codeprop1(~ X.squared >= observedChi2, data = null_chisq)\n\nprop_TRUE \n   0.0003 \n\n\nThe p-value is well below our threshold of \\(0.05\\%\\), so we would conclude that Education has a significant effect on DeathPenalty opinion!"
  },
  {
    "objectID": "content/courses/Basics-of-Modeling/Modules/50-Proportions/index.html#references",
    "href": "content/courses/Basics-of-Modeling/Modules/50-Proportions/index.html#references",
    "title": "üÉè Permutation Test for Two Proportions",
    "section": "\n3.3 References",
    "text": "3.3 References"
  }
]