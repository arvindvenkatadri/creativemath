[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Applied Metaphorics",
    "section": "",
    "text": "Hi, I‚Äôm Arvind Venkatadri.\nI‚Äôm an Adjunct Professor at DSU, Bangalore, INDIA, with a passion for R, Data Visualization, Complexity Science, Literature, and Creative Thinking and Problem Solving with TRIZ. On this blog, I share and teach what I learn.\nTo get started, you can check out my courses. You can find me on Twitter or GitHub and YouTube. Feel free to reach out to me via mail !"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/listing.html",
    "href": "content/courses/Analytics/Descriptive/listing.html",
    "title": "Descriptive Analytics",
    "section": "",
    "text": "üï∂ Science, Human Experience, Experiments, and Data\n\n\nWhy do we visualize data\n\n\n\n\nScientific Inquiry\n\n\nExperiments\n\n\nObservations\n\n\nNature of Data\n\n\nExperience\n\n\nMeasurement\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2021\n\n\nArvind Venkatadri\n\n\n\n\n\n\n \n\n\n\n\nüìä Distributions\n\n\nDistributions\n\n\n\n\nQual Variables\n\n\nQuant Variables\n\n\nBar Charts\n\n\nColumn Charts\n\n\nHistograms\n\n\nDensity Plots\n\n\nBox Plots\n\n\n\n\nQuant Variable Graphs and their Siblings\n\n\n\n\n\n\nNov 15, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\n \n\n\n\n\nüìé Correlations\n\n\nCorrelations\n\n\n\n\nCorrelations\n\n\nScatter Plots\n\n\nBubble Plots\n\n\n2D Density Plots\n\n\nHeatmaps\n\n\nRegression Lines\n\n\n\n\nHow one variable changes with another\n\n\n\n\n\n\nNov 22, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\n \n\n\n\n\nüï∏ Evolution and Flow\n\n\nEvolution and Flow\n\n\n\n\nLine and Area Plots\n\n\nStream Charts\n\n\nAlluvial Plots\n\n\nSankey Diagrams\n\n\nChord Diagrams\n\n\nWaterfall Plots\n\n\nBump Charts\n\n\n\n\nChanges in Information over space and time\n\n\n\n\n\n\nNov 22, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\n \n\n\n\n\nüçï Parts of a Whole\n\n\nParts of a Whole\n\n\n\n\nPie Charts\n\n\nFan Charts\n\n\nDonut Charts\n\n\nGrouping\n\n\nStaacking\n\n\nCircular Bar Charts\n\n\nDot Plots\n\n\nMosaic Charts\n\n\nParliament Charts\n\n\nWaffle Charts\n\n\n\n\nSlices, Portions, Counts, and Aggregates of Data\n\n\n\n\n\n\nNov 25, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\n \n\n\n\n\nüñè Ratings and Rankings\n\n\nRanking\n\n\n\n\nBar Charts\n\n\nLollipop Charts\n\n\nRadar Charts\n\n\nWord Clouds\n\n\nBump Charts\n\n\n\n\nComparisons between observations and between variables\n\n\n\n\n\n\nFeb 10, 2023\n\n\nArvind Venkatadri\n\n\n\n\n\n\n \n\n\n\n\nüó∫ Maps\n\n\nMaps\n\n\n\n\nSpatial Data\n\n\nMaps\n\n\nStatic\n\n\nInteractive\n\n\nChoropleth Maps\n\n\nBubble Plots\n\n\nCartograms\n\n\n\n\nGeospatial Data and how to use it with intent\n\n\n\n\n\n\nAug 15, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\n \n\n\n\n\nüïî Time Series\n\n\nTime Series\n\n\n\n\nCandleStick Graphs\n\n\nHeatmap Graphs (over time)\n\n\nLine Graphs\n\n\nTime Series\n\n\n\n\nEvents, Trends, Seasons, and Changes over Time\n\n\n\n\n\n\nDec 15, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\n \n\n\n\n\nüï∏ Networks\n\n\nHow things are connected\n\n\nNetworks and Connections and what happens over them\n\n\n\n\n\n\nNov 21, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\n \n\n\n\n\nüêâ Visualizing Categorical Data\n\n\n\n\n\n\n\nProportions\n\n\nFrequency Tables\n\n\nContingency Tables\n\n\nNumerical Data in Groups\n\n\nMargins\n\n\nLikert Scale data\n\n\nBar Plots (for Contingency Tables)\n\n\nMosaic Plots\n\n\nBalloon Plots\n\n\nPie Charts\n\n\nCorrespondence Analysis\n\n\n\n\nTypes, Categories, and Counts\n\n\n\n\n\n\nDec 27, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\n \n\n\n\n\nüìö Miscellaneous Graphing Tools, and References\n\n\n\n\n\n\n\nOnline Tools\n\n\nNo Code\n\n\nData Viz Guides\n\n\n\n\nMiscellaneous Graphs and Tools\n\n\n\n\n\n\nNov 11, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-NatureData/nature-data.html#fa-envelope-titlean-envelope-where-does-data-come-from",
    "href": "content/courses/Analytics/Descriptive/Modules/10-NatureData/nature-data.html#fa-envelope-titlean-envelope-where-does-data-come-from",
    "title": "üï∂ Science, Human Experience, Experiments, and Data",
    "section": "\n Where does Data come from?",
    "text": "Where does Data come from?\nWe will need to form a basic understanding of basic scientific enterprise. Let us look at the slides."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-NatureData/nature-data.html#fa-chart-simple-why-visualize",
    "href": "content/courses/Analytics/Descriptive/Modules/10-NatureData/nature-data.html#fa-chart-simple-why-visualize",
    "title": "üï∂ Science, Human Experience, Experiments, and Data",
    "section": "\n Why Visualize?",
    "text": "Why Visualize?\n\nWe can digest information more easily when it is pictorial\nOur Working Memories are both short-term and limited in capacity. So a picture abstracts the details and presents us with an overall summary, an insight, or a story that is both easy to recall and easy on retention.\n\nData Viz includes shapes that carry strong cultural memories and impressions for us. These cultural memories help us to use data viz in a universal way to appeal to a wide variety of audiences. (Do humans have a gene for geometry?)\nIt helps sift facts and mere statements: for example:\n\n\n\nRape Capital\n\n\n\n\nWhat does Data Reveal?"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-NatureData/nature-data.html#iconify-mdi-category-plus-what-are-data-types",
    "href": "content/courses/Analytics/Descriptive/Modules/10-NatureData/nature-data.html#iconify-mdi-category-plus-what-are-data-types",
    "title": "üï∂ Science, Human Experience, Experiments, and Data",
    "section": "\n What are Data Types?",
    "text": "What are Data Types?\n\n\n\nIn more detail:"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-NatureData/nature-data.html#sec-data-types",
    "href": "content/courses/Analytics/Descriptive/Modules/10-NatureData/nature-data.html#sec-data-types",
    "title": "üï∂ Science, Human Experience, Experiments, and Data",
    "section": "\n How do we Spot Data Variable Types?",
    "text": "How do we Spot Data Variable Types?\nBy asking questions!\n\n\n\n\n\n\n No \n    Pronoun \n    Answer \n    Variable/Scale \n    Example \n    What Operations? \n  \n\n\n 1 \n    How Many / Much / Heavy? Few? Seldom? Often? When? \n    Quantities, with Scale and a Zero Value.Differences and Ratios /Products are meaningful. \n    Quantitative/Ratio \n    Length,Height,Temperature in Kelvin,Activity,Dose Amount,Reaction Rate,Flow Rate,Concentration,Pulse,Survival Rate \n    Correlation \n  \n\n 2 \n    How Many / Much / Heavy? Few? Seldom? Often? When? \n    Quantities with Scale.\nDifferences are meaningful, but not products or ratios \n    Quantitative/Interval \n    pH,SAT score(200-800),Credit score(300-850),SAT score(200-800),Year of Starting College \n    Mean,Standard Deviation \n  \n\n 3 \n    How, What Kind, What Sort \n    A Manner / Method, Type or Attribute from a list, with list items in some \" order\" ( e.g. good, better, improved, best..) \n    Qualitative/Ordinal \n    Socioeconomic status (Low income, Middle income, High income),Education level (HighSchool, BS, MS, PhD),Satisfaction rating(Very much Dislike, Dislike, Neutral, Like, Very Much Like) \n    Median,Percentile \n  \n\n 4 \n    What, Who, Where, Whom, Which \n    Name, Place, Animal, Thing \n    Qualitative/Nominal \n    Name \n    Count no. of cases,Mode \n  \n\n\n\n\n\nAs you go from Qualitative to Quantitative data types in the table, I hope you can detect a movement from fuzzy groups/categories to more and more crystallized numbers.\n\n\nType of Variables\n\n\nEach variable/scale can be subjected to the operations of the previous group. In the words of S.S. Stevens\n\nthe basic operations needed to create each type of scale is cumulative: to an operation listed opposite a particular scale must be added all those operations preceding it."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-NatureData/nature-data.html#sec-data-viz",
    "href": "content/courses/Analytics/Descriptive/Modules/10-NatureData/nature-data.html#sec-data-viz",
    "title": "üï∂ Science, Human Experience, Experiments, and Data",
    "section": "What Are the Parts of a Data Viz?",
    "text": "What Are the Parts of a Data Viz?"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-NatureData/nature-data.html#how-to-pick-a-data-viz",
    "href": "content/courses/Analytics/Descriptive/Modules/10-NatureData/nature-data.html#how-to-pick-a-data-viz",
    "title": "üï∂ Science, Human Experience, Experiments, and Data",
    "section": "How to pick a Data Viz?",
    "text": "How to pick a Data Viz?\nMost Data Visualizations use one or more of the following geometric attributes or aesthetics. These geometric aesthetics are used to represent qualitative or quantitative variables from your data.\n\n\nCommon Geometric Aesthetics in Charts\n\n\nWhat does this mean? We can think of simple visualizations as combinations of these aesthetics. Some examples:\n\n\nGeometries , Combinations, and Graphs\n\n\n\n\n\n\nAesthetic #1\nAesthetic #2\nShape\n\n\n\nPosition X = Quant Variable\nPosition Y = Quant Variable\nP o i n ts/Circles with Fixed Size\n\n\nPosition X = Qual Variable\nPosition Y = Count of Qual Variable\nColumns\n\n\nPosition X = Qual Variable\nPosition Y = Qual Variable\nR ectangles, with area p r oportional to joint(X,Y) count\n\n\n\nPosition X = Qual Variable\n\nPosition Y\nRa nk-Ordered Quant Variable\n\nBox + Whisker, Box length p r oportional to I n t e r-Quartile Range, w h i s ker-length p r oportional to upper and lower quartile resp.\n\n\nPosition X = Quant Variable\nPosition Y = Quant Variable + Qual Variable\n\nLine and Area\nColour for Area"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-NatureData/nature-data.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/10-NatureData/nature-data.html#references",
    "title": "üï∂ Science, Human Experience, Experiments, and Data",
    "section": "References",
    "text": "References\n\nOpen Intro Stats: Types of Variables\nClaus Wilke: Fundamentals of Data Visualization"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html",
    "title": "The Grammar of Networks",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE, warning = FALSE)\n\n# Fonts\n# Run these next few commands IN YOUR CONSOLE once. \n#install.packages(\"extrafontdb\")\n#library(extrafont)\n#extrafont::font_import(paths = NULL, recursive = TRUE, prompt = TRUE,pattern = NULL)\n\n########################################\n# For General Data Manipulation\nlibrary(tidyverse)\n\n########################################\n# Network Analysis Library (Handle data and Viz)\nlibrary(igraph)\n\n########################################\n# For Network \"Manipulation\"\nlibrary(tidygraph)\n\n# For Network Visualization\nlibrary(ggraph)\nlibrary(graphlayouts)\nlibrary(visNetwork)\n\n# For \"Network\" Datasets\nlibrary(igraphdata)\n\nset_graph_style(family = \"Arial\") # Plot theme for all graphs"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "title": "The Grammar of Networks",
    "section": "\n Introduction",
    "text": "Introduction\nThis Quarto document is part of my workshop course on R . The material is based on A Layered Grammar of Graphics by Hadley Wickham. The intent of this Course is to build Skill in coding in R, and also appreciate R as a way to metaphorically visualize information of various kinds, using predominantly geometric figures and structures.\nAll Quarto document files combine code, text, web-images, and figures developed using code. Everything is text; code chunks are enclosed in fences (```)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#goals",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#goals",
    "title": "The Grammar of Networks",
    "section": "Goals",
    "text": "Goals\nAt the end of this Lab session, we should:\n\nknow the types and structures of network data and be able to work with them\nunderstand the basics of modern network packages in R\nbe able to create network visualizations using tidygraph, ggraph( static visualizations ) and visNetwork (interactive visualizations)\nsee directions for how the network metaphor applies in a variety of domains (e.g.¬†biology/ecology, ideas/influence, technology, transportation, to name a few)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#pedagogical-note",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#pedagogical-note",
    "title": "The Grammar of Networks",
    "section": "Pedagogical Note",
    "text": "Pedagogical Note\nThe method followed will be based on PRIMM:\n\n\nPREDICT Inspect the code and guess at what the code might do, write predictions\n\n\nRUN the code provided and check what happens\n\nINFER what the parameters of the code do and write comments to explain. What bells and whistles can you see?\n\nMODIFY the parameters code provided to understand the options available. Write comments to show what you have aimed for and achieved.\n\nMAKE : take an idea/concept of your own, and graph it."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#graph-metaphors",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#graph-metaphors",
    "title": "The Grammar of Networks",
    "section": "Graph Metaphors",
    "text": "Graph Metaphors\nNetwork graphs are characterized by two key terms: nodes and edges\n\n\nNodes : Entities\n\nMetaphors: Individual People? Things? Ideas? Places? to be connected in the network.\nSynonyms: vertices. Nodes have IDs.\n\n\n\nEdges: Connections\n\nMetaphors: Interactions? Relationships? Influence? Letters sent and received? Dependence? between the entities.\nSynonyms: links, ties.\n\n\n\nIn R, we create network representations using node and edge information. One way in which these could be organized are:\n\n\nNode list: a data frame with a single column listing the node IDs found in the edge list. You can also add attribute columns to the data frame such as the names of the nodes or grouping variables. ( Type? Class? Family? Country? Subject? Race? )\n\n\nNode Table\n\n\n\n\n\n\n\nID\nNode Name\nAttribute? Qualities?Categories? Family? Country?Planet?\n\n\n1\nNed\nNursery School Teacher\n\n\n2\nJaguar Paw\nMain Character, Apocalypto\n\n\n3\nJohn Snow\nEpidemiologist\n\n\n\n\n\nEdge list: data frame containing two columns: source node and destination node of an edge. Source and Destination have node IDs.\n\nWeighted network graph: An edge list can also contain additional columns describing attributes of the edges such as a magnitude aspect for an edge. If the edges have a magnitude attribute the graph is considered weighted.\n\n\nEdges Table\n\nFrom\nTo\nRelationship\nWeightage\n\n\n\n1\n3\nFinancial Dealings\n6\n\n\n2\n1\nHistory Lessons\n2\n\n\n2\n3\nVaccination\n15\n\n\n\n\n\nLayout: A geometric arrangement of nodes and edges.\n\nMetaphors: Location? Spacing? Distance? Coordinates? Colour? Shape? Size? Provides visual insight due to the arrangement.\n\n\n\nLayout Algorithms : Method to arranges nodes and edges with the aim of optimizing some metric .\n\nMetaphors: Nodes are masses and edges are springs. The Layout algorithm minimizes the stretching and compressing of all springs.(BTW, are the Spring Constants K the same for all springs?‚Ä¶)\n\n\nDirected and undirected network graph: If the distinction between source and target is meaningful, the network is directed. If the distinction is not meaningful, the network is undirected. Directed edges represent an ordering of nodes, like a relationship extending from one node to another, where switching the direction would change the structure of the network. Undirected edges are simply links between nodes where order does not matter.\n\nExamples:\n - The World Wide Web is an example of a directed network because\n hyperlinks connect one Web page to another, but not necessarily \n the other way around.\n\n - Co-authorship networks represent examples of un-directed networks,\nwhere nodes are authors and they are connected by an edge if they\nhave written a publication together\n\n - When people send e-mail to each other, the distinction between the\nsender (source) and the recipient (target) is clearly meaningful,\ntherefore the network is directed.\n\n\nConnected and Disconnected graphs: If there is some path from any node to any other node, the Networks is said to be Connected. Else, Disconnected."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#predictruninfer--1",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#predictruninfer--1",
    "title": "The Grammar of Networks",
    "section": "Predict/Run/Infer -1",
    "text": "Predict/Run/Infer -1\nUsing tidygraph and ggraph\n\ntidygraph and ggraph are modern R packages for network data. Graph Data setup and manipulation is done in tidygraph and graph visualization with ggraph.\n\n\ntidygraph Data -> ‚ÄúNetwork Object‚Äù in R.\n\nggraph Network Object -> Plots using a chosen layout/algo.\n\nBoth leverage the power of igraph, which is the Big Daddy of all network packages. We will be using the Grey‚Äôs Anatomy dataset in our first foray into networks.\nStep1. Read the data\nDownload these two datasets into your current project-> data folder.\n Grey‚Äôs Anatomy Nodes \n Grey‚Äôs Anatomy Edges \n\ngrey_nodes <- read_csv(\"data/grey_nodes.csv\")\ngrey_edges <- read_csv(\"data/grey_edges.csv\")\n\ngrey_nodes\ngrey_edges\n\n\n\n\n\n  \n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #1\n\n\n\nLook at the console output thumbnail. What does for example name = col_character mean? What attributes (i.e.¬†extra information) are seen for Nodes and Edges? Understand the data in both nodes and edges as shown in the second and third thumbnails. Write some comments and inferences here.\n\n\nStep 2.Create a network object using tidygraph:\nKey function:\n\n\ntbl_graph(): (aka ‚Äútibble graph‚Äù). Key arguments: nodes, edges and directed. Note this is a very versatile command and can take many input forms, such as data structures that result from other packages. Type ?tbl_graph in the Console and see the Usage section.\n\n\nga <- tbl_graph(nodes = grey_nodes, \n                edges = grey_edges, \n                directed = FALSE)\nga\n\n# A tbl_graph: 54 nodes and 57 edges\n#\n# An undirected simple graph with 4 components\n#\n# A tibble: 54 √ó 7\n  name               sex   race  birthyear position  season sign  \n  <chr>              <chr> <chr>     <dbl> <chr>      <dbl> <chr> \n1 Addison Montgomery F     White      1967 Attending      1 Libra \n2 Adele Webber       F     Black      1949 Non-Staff      2 Leo   \n3 Teddy Altman       F     White      1969 Attending      6 Pisces\n4 Amelia Shepherd    F     White      1981 Attending      7 Libra \n5 Arizona Robbins    F     White      1976 Attending      5 Leo   \n6 Rebecca Pope       F     White      1975 Non-Staff      3 Gemini\n# ‚Ñπ 48 more rows\n#\n# A tibble: 57 √ó 4\n   from    to weight type    \n  <int> <int>  <dbl> <chr>   \n1     5    47      2 friends \n2    21    47      4 benefits\n3     5    46      1 friends \n# ‚Ñπ 54 more rows\n\n\n\n\n\n\n\n\nQuestions and Inferences #2\n\n\n\nWhat information does the graph object contain? What attributes do the nodes have? What about the edges?\n\n\nStep 3. Plot using ggraph\n\n3a. Quick Plot: autograph() This is to check quickly is the data is imported properly and to decide upon going on to a more elaborate plotting.\n\nautograph(ga)\n\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #3\n\n\n\nDescribe this graph, in simple words here. Try to use some of the new domain words we have just acquired: nodes/edges, connected/disconnected, directed/undirected.\n\n\n3b. More elaborate plot\nKey functions:\n\n\nggraph(layout = \"......\"): Create classic node-edge diagrams; i.e. Sets up the graph. Rather like ggplot for networks!\n\nTwo kinds of geom: one set for nodes, and another for edges\n\ngeom_node_point(aes(.....)): Draws node as ‚Äúpoints‚Äù. Alternatives are circle / arc_bar / tile / voronoi. Remember the geoms that we have seen before in Grammar of Graphics!\ngeom_edge_link(aes(.....)): Draws edges as ‚Äúlinks‚Äù. Alternatives are arc / bend / elbow / hive / loop / parallel / diagonal / point / span /tile.\ngeom_node_text(aes(label = ......), repel = TRUE): Adds text labels (non-overlapping). Alternatives are label /...\nlabs(title = \"....\", subtitle = \"....\", caption = \"....\"): Change main titles, axis labels and legend titles. We know this from our work with ggplot.\n\n\n# Write Comments next to each line \n# About what that line does for the overall graph\n\nggraph(graph = ga,  layout = \"kk\") +\n  #\n  \n  geom_edge_link(width = 2, color = \"pink\") +\n  #\n  \n  geom_node_point(\n    shape = 21,\n    size = 8,\n    fill = \"blue\",\n    color = \"green\",\n    stroke = 2\n  ) +\n  \n  labs(title = \"Whoo Hoo! My first silly Grey's Anatomy graph in R!\",\n       subtitle = \"Why did I ever get in this course...\",\n       caption = \"Bro, they are doing **cool** things in the other classes...\") \n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` in the `default_aes` field and elsewhere instead.\n\n\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #4:\n\n\n\nWhat parameters have been changed here, compared to the earlier graph? Where do you see these changes in the code above?\n\n\nLet us Play with this graph and see if we can make some small changes. Colour? Fill? Width? Size? Stroke? Labs? Of course!\n\n# Change the parameters in each of the commands here to new ones\n# Use fixed values for colours or sizes...etc. \n\nggraph(graph = ga,  layout = \"kk\") + \n  geom_edge_link(width = 2) + \n  geom_node_point(shape = 21, size = 8, \n                  fill = \"blue\", \n                  color = \"green\", \n                  stroke = 2) +\n  labs(title = \"Whoo Hoo! My next silly Grey's Anatomy graph in R!\",\n       subtitle = \"Why did I ever get in this course...\",\n       caption = \"Bro, they are doing cool things in the other classes...\") \n\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #5\n\n\n\nWhat did the shape parameter achieve? What are the possibilities with shape? How about including alpha?\n\n\n3c. Aesthetic Mapping from Node and Edge attribute columns\nUp to now, we have assigned specific numbers to geometric aesthetics such as shape and size. Now we are ready ( maybe ?) change the meaning and significance of the entire graph and each element within it, and use aesthetics / metaphoric mappings to achieve new meanings or insights. Let us try using aes() inside each geom to map a variable to a geometric aspect.\nDon‚Äôt try to use more than 2 aesthetic mappings simultaneously!!\nThe node elements we can tweak are:\n\nTypes of Nodes: geom_node_****()\n\nNode Parameters: inside geom_node_****(aes(...............))\n-aes(alpha  = node-variable) : opacity; a value between 0 and 1\n-aes(shape  = node-variable) : node shape\n-aes(colour = node-variable) : node colour\n-aes(fill   = node-variable) : fill colour for node\n-aes(size   = node-variable) : size of node\n\nThe edge elements we can tweak are:\n\nType of Edges‚Äù geom_edge_****()\n\nEdge Parameters: inside geom_edge_****(aes(...............))\n-aes(colour = edge-variable) : colour of the edge\n-aes(width  = edge-variable) : width of the edge\n-aes(label  = some_variable) : labels for the edge\n\nType ?geom_node_point and ?geom-edge_link in your Console for more information.\n\nggraph(graph = ga, layout = \"fr\") +\n  geom_edge_link0(aes(width = weight)) + # change variable here\n  \n  geom_node_point(aes(color = race), size = 6) + # change variable here\n\n\n  labs(title = \"Whoo Hoo! Yet another Grey's Anatomy graph in R!\") +\n  \n  set_graph_style() # Why is the previously set graph style not work here???\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #6\n\n\n\nDescribe some of the changes here. What types of edges worked? Which variables were you able to use for nodes and edges and how? What did not work with either of the two?"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#predictreuseinfer-2",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#predictreuseinfer-2",
    "title": "The Grammar of Networks",
    "section": "Predict/Reuse/Infer-2",
    "text": "Predict/Reuse/Infer-2\n\n# Arc diagram\n\nggraph(ga, layout = \"linear\") +\n  geom_edge_arc(aes(width = weight), alpha = 0.8) +\n  scale_edge_width(range = c(0.2, 2)) +\n  geom_node_point(size = 2, colour = \"red\") +\n  labs(edge_width = \"Weight\") +\n  theme_graph()\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #7\n\n\n\nHow does this graph look ‚Äúmetaphorically‚Äù different? Do you see a difference in the relationships between people here? Why?\n\n\n\n# Coord diagram, circular\nggraph(ga, layout = \"linear\", circular = TRUE) + \n  geom_edge_arc(aes(width = weight), alpha = 0.8) + \n  scale_edge_width(range = c(0.2, 2)) +\n  geom_node_point(size = 4,colour = \"red\") + \n  geom_node_text(aes(label = name),\n                 repel = TRUE, size = 3,\n                 max.overlaps = 15) +\n  labs(edge_width = \"Weight\") \n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #8\n\n\n\nHow does this graph look ‚Äúmetaphorically‚Äù different? Do you see a difference in the relationships between people here? Why?"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#hierarchical-layouts",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#hierarchical-layouts",
    "title": "The Grammar of Networks",
    "section": "Hierarchical layouts",
    "text": "Hierarchical layouts\nThese provide for some alternative metaphorical views of networks. Note that not all layouts are possible for all datasets!!\n\nset_graph_style()\n\n# This dataset contains the graph that describes the class \n# hierarchy for the Flare visualization library.\n# Type ?flare in your Console\nhead(flare$vertices)\n\n\n\n  \n\n\nhead(flare$edges)\n\n\n\n  \n\n\n# flare class hierarchy\ngraph = tbl_graph(edges = flare$edges, nodes = flare$vertices)\n\n# dendrogram\nggraph(graph, layout = \"dendrogram\") + \n  geom_edge_diagonal() + \n  labs(title = \"Dendrogram\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n# circular dendrogram\nggraph(graph, layout = \"dendrogram\", circular = TRUE) + \n  geom_edge_diagonal() + \n  geom_node_point(aes(filter = leaf)) + \n  coord_fixed()+ \n  labs(title = \"Circular Dendrogram\")\n\n\n\n# rectangular tree map\nggraph(graph, layout = \"treemap\", weight = size) + \n  geom_node_tile(aes(fill = depth), size = 0.25) + \n  labs(title = \"Rectangular Tree Map\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n# circular tree map\nggraph(graph, layout = \"circlepack\", weight = size) + \n  geom_node_circle(aes(fill = depth), size = 0.25, n = 50) + \n  coord_fixed() + \n  labs(title = \"Circular Tree Map\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n# icicle\nggraph(graph, layout = \"partition\") + \n  geom_node_tile(aes(y = -y, fill = depth))\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n# sunburst (circular icicle)\nggraph(graph, layout = \"partition\", circular = TRUE) +\n  geom_node_arc_bar(aes(fill = depth)) +\n  coord_fixed() + \n  labs(title = \"Circular Icicle\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuestions and Inferences #9 How do graphs look ‚Äúmetaphorically‚Äù different? Do they reveal different aspects of the group? How?"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#faceting",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#faceting",
    "title": "The Grammar of Networks",
    "section": "Faceting",
    "text": "Faceting\nFaceting allows to create sub-plots according to the values of a qualitative attribute on nodes or edges.\n\n# setting theme_graph \nset_graph_style()\n\n\n# facet edges by type\nggraph(ga,layout = \"linear\", circular = TRUE) + \n  geom_edge_link(aes(color = type)) + \n  geom_node_point() +\n  facet_edges(~ type) +\n  th_foreground(border = TRUE) +\n  theme(aspect.ratio = 1)\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n# facet nodes by sex\nggraph(ga,layout = \"linear\", circular = TRUE) + \n  geom_edge_link() + \n  geom_node_point() +\n  facet_nodes(~race) +\n  th_foreground(border = TRUE) +\n  theme(aspect.ratio = 1)\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n# facet both nodes and edges\nggraph(ga,layout = \"linear\", circular = TRUE) + \n  geom_edge_link(aes(color = type)) + \n  geom_node_point() +\n  facet_graph(type ~ race) + \n  th_foreground(border = TRUE) +\n  theme(aspect.ratio = 1, legend.position = \"top\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuestions and Inferences #10 Does splitting up the main graph into sub-networks give you more insight? Describe some of these."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#network-analysis-with-tidygraph",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#network-analysis-with-tidygraph",
    "title": "The Grammar of Networks",
    "section": "Network analysis with tidygraph",
    "text": "Network analysis with tidygraph\nThe data frame graph representation can be easily augmented with metrics or statistics computed on the graph. Remember how we computed counts with the penguin dataset in Grammar of Graphics.\nBefore computing a metric on nodes or edges use the activate() function to activate either node or edge data frames. Use dplyr verbs (filter, arrange, mutate) to achieve your computation in the proper way.\nNetwork Centrality\nCentrality is a an ‚Äúill-defined‚Äù metric of node and edge importance in a network. It is therefore calculated in many ways. Type ?centrality in your Console.\n\n\nStandards\n\n\nLet‚Äôs add a few columns to the nodes and edges based on network centrality measures:\n\nga %>% \n  activate(nodes) %>% \n  \n  # Node with  the most connections?\n  mutate(degree = centrality_degree(mode = c(\"in\"))) %>% \n  filter(degree > 0) %>% \n  \n  activate(edges) %>% \n  # \"Busiest\" edge?\n  mutate(betweenness = centrality_edge_betweenness())\n\n# A tbl_graph: 54 nodes and 57 edges\n#\n# An undirected simple graph with 4 components\n#\n# A tibble: 57 √ó 5\n   from    to weight type     betweenness\n  <int> <int>  <dbl> <chr>          <dbl>\n1     5    47      2 friends         20.3\n2    21    47      4 benefits        44.7\n3     5    46      1 friends         39  \n4     5    41      1 friends         66.3\n5    18    41      6 friends         39  \n6    21    41     12 benefits        91.5\n# ‚Ñπ 51 more rows\n#\n# A tibble: 54 √ó 8\n  name               sex   race  birthyear position  season sign   degree\n  <chr>              <chr> <chr>     <dbl> <chr>      <dbl> <chr>   <dbl>\n1 Addison Montgomery F     White      1967 Attending      1 Libra       3\n2 Adele Webber       F     Black      1949 Non-Staff      2 Leo         1\n3 Teddy Altman       F     White      1969 Attending      6 Pisces      4\n# ‚Ñπ 51 more rows\n\n\nPackages tidygraph and ggraph can be pipe-lined to perform analysis and visualization tasks in one go.\n\n# setting theme_graph \nset_graph_style()\n\nga %>% \n  activate(nodes) %>% \n  \n  # Who has the most connections?\n  mutate(degree = centrality_degree()) %>% \n  \n  activate(edges) %>% \n  # Who is the go-through person?\n  mutate(betweenness = centrality_edge_betweenness()) %>%\n  \n  # Now to continue with plotting\n  ggraph(layout = \"nicely\") +\n  geom_edge_link(aes(alpha = betweenness)) +\n  geom_node_point(aes(size = degree, colour = degree)) + \n  \n  # discrete colour legend\n  scale_color_gradient(guide = \"legend\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n# or even less typing\n  ggraph(ga,layout = \"nicely\") +\n    \n  geom_edge_link(aes(alpha = centrality_edge_betweenness())) +\n    \n  geom_node_point(aes(colour = centrality_degree(), \n                      size = centrality_degree())) + \n    \n  scale_color_gradient(guide = \"legend\",\n                       low = \"blue\",\n                       high = \"red\") \n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuestions and Inferences #11 How do the Centrality Measures show up in the graph? Would you ‚Äúagree‚Äù with the way we have done it? Try to modify the aesthetics by copy-pasting this chunk below and see how you can make an alternative representation.\n\n\nAnalysis and Visualizing Network Communities\nWho is close to whom? Which are the groups you can see?\n\n# setting theme_graph \nset_graph_style()\n\n\n# visualize communities of nodes\nga %>% \n  activate(nodes) %>%\n  mutate(community = as.factor(group_louvain())) %>% \n  ggraph(layout = \"graphopt\") + \n  geom_edge_link() + \n  geom_node_point(aes(color = community), size = 5) \n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #12\n\n\n\nIs the Community depiction clear? How would you do it, with which aesthetic? Copy Paste this chunk below and try.\n\n\nInteractive Graphs with visNetwork\n\nExploring the VisNetwork package. Make graphs wiggle and shake using tidy commands! The package implements interactivity using the physical metaphor of weights and springs we discussed earlier.\nThe visNetwork() function uses a nodes list and edges list to create an interactive graph. The nodes list must include an ‚Äúid‚Äù column, and the edge list must have ‚Äúfrom‚Äù and ‚Äúto‚Äù columns. The function also plots the labels for the nodes, using the names of the cities from the ‚Äúlabel‚Äù column in the node list.\n\nlibrary(visNetwork)\n\n# Prepare the data for plotting by visNetwork\ngrey_nodes\n\n\n\n  \n\n\ngrey_edges\n\n\n\n  \n\n\n# Relabel greys anatomy nodes and edges for VisNetwork\ngrey_nodes_vis <- grey_nodes %>% \n  rowid_to_column(var = \"id\") %>% \n  rename(\"label\" = name) %>% \n  mutate(sex = case_when(sex == \"F\" ~ \"Female\",\n                         sex == \"M\" ~ \"Male\")) %>% \n  replace_na(., list(sex = \"Transgender?\")) %>% \n  rename(\"group\" = sex)\ngrey_nodes_vis\n\n\n\n  \n\n\ngrey_edges_vis <- grey_edges %>% \n  select(from, to) %>% \n  left_join(., grey_nodes_vis, \n            by = c(\"from\" = \"label\")) %>% \n  left_join(., grey_nodes_vis, \n            by = c(\"to\" = \"label\")) %>%\n  select(\"from\"= id.x, \"to\" = id.y)\ngrey_edges_vis\n\n\n\n  \n\n\n\nUsing fontawesome icons\n\ngrey_nodes_vis %>%\n\n  visNetwork(nodes = ., edges = grey_edges_vis) %>% \n  visNodes(font = list(size = 40)) %>% \n  \n  # Colour and icons for each of the gender-groups\n  visGroups(groupname = \"Female\", shape = \"icon\", \n            icon = list(code = \"f182\", size = 75, color = \"tomato\"),\n            shadow = list(enabled = TRUE)) %>% \n  \n  visGroups(groupname = \"Male\", shape = \"icon\", \n            icon = list(code = \"f183\", size = 75, color = \"slateblue\"), \n            shadow = list(enabled = TRUE)) %>% \n  \n  visGroups(groupname = \"Transgender?\", shape = \"icon\", \n            icon = list(code = \"f22c\", size = 75, color = \"fuchsia\"), \n            shadow = list(enabled = TRUE)) %>% \n  \n  #visLegend() %>%\n  #Add the fontawesome icons!!\n  addFontAwesome() %>% \n  \n  # Add Interaction Controls\n  visInteraction(navigationButtons = TRUE,\n                 hover = TRUE,\n                 selectConnectedEdges = TRUE,\n                 hoverConnectedEdges = TRUE,\n                 zoomView = TRUE)\n\n\n\n\n\nThere is another family of icons available in visNetwork, called ionicons. Let‚Äôs see how they look:\n\ngrey_nodes_vis %>%\n\n  visNetwork(nodes = ., edges = grey_edges_vis,) %>%\n  visLayout(randomSeed = 12345) %>%\n  visNodes(font = list(size = 50)) %>%\n  visEdges(color = \"green\") %>%\n  visGroups(\n    groupname = \"Female\",\n    shape = \"icon\",\n    icon = list(\n      face = 'Ionicons',\n      code = \"f25d\",\n      color = \"fuchsia\",\n      size = 125\n    )\n  ) %>%\n  \n  visGroups(\n    groupname = \"Male\",\n    shape = \"icon\",\n    icon = list(\n      face = 'Ionicons',\n      code = \"f202\",\n      color = \"green\",\n      size = 125\n    )\n  ) %>%\n  \n  visGroups(\n    groupname = \"Transgender?\",\n    shape = \"icon\",\n    icon = list(\n      face = 'Ionicons',\n      code = \"f233\",\n      color = \"dodgerblue\",\n      size = 125\n    )\n  ) %>%\n  visLegend() %>%\n  addIonicons() %>%\n  visInteraction(\n    navigationButtons = TRUE,\n    hover = TRUE,\n    selectConnectedEdges = TRUE,\n    hoverConnectedEdges = TRUE,\n    zoomView = TRUE\n  )\n\n\n\n\n\nSome idea of interactivity and controls with visNetwork:\n Star Wars Nodes \n Star Wars Edges \n\n# let's look again at the data\nstarwars_nodes <- read_csv(\"data/star-wars-network-nodes.csv\")\n\nRows: 22 Columns: 2\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (1): name\ndbl (1): id\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nstarwars_edges <- read_csv(\"data/star-wars-network-edges.csv\")\n\nRows: 60 Columns: 3\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (2): source, target\ndbl (1): weight\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n# We need to rename starwars nodes dataframe and edge dataframe columns for visNetwork\nstarwars_nodes_vis <- \n  starwars_nodes %>% \n  rename(\"label\" = name)\n\n# Convert from and to columns to **node ids**\nstarwars_edges_vis <- \n  starwars_edges %>% \n  \n  # Matching Source <- Source Node id (\"id.x\")\n  left_join(., starwars_nodes_vis, by = c(\"source\" = \"label\")) %>% \n  \n  # Matching Target <- Target Node id (\"id.y\")\n  left_join(., starwars_nodes_vis, by = c(\"target\" = \"label\")) %>% \n  \n# Select \"id.x\" and \"id.y\" ONLY\n# Rename them as \"from\" and \"to\"\n# keep \"weight\" column for aesthetics of edges\n  select(\"from\" = id.x, \"to\" = id.y, \"value\" = weight)\n\n# Check everything once\nstarwars_nodes_vis\n\n\n\n  \n\n\nstarwars_edges_vis\n\n\n\n  \n\n\n\nOk, let‚Äôs make things move and shake!!\n\nvisNetwork(nodes = starwars_nodes_vis,\n           edges = starwars_edges_vis) %>% \n  visNodes(font = list(size = 30), shape = \"icon\", \n           icon = list(code = \"f1e3\", size = 75)) %>% \n  addFontAwesome() %>% \n  visEdges(color = \"red\")\n\n\n\n\n\n\nvisNetwork(nodes = starwars_nodes_vis,\n           edges = starwars_edges_vis) %>% \n  visNodes(font = list(size = 30)) %>% \n  visEdges(color = \"red\")"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#your-assignments",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#your-assignments",
    "title": "The Grammar of Networks",
    "section": "Your Assignments:",
    "text": "Your Assignments:\nMake-1 : With a ready made dataset\nStep 0. Fire up a New Project! Always!\nStep 1. Fire up a new Quarto document. Fill in the YAML header.\nStep 2. Take any one of the ‚ÄúMake1-Datasets‚Äù datasets described below.\nStep 3. Document contents:\n\nIntroduce / Inspect in R your data and describe\nIntroduce your Purpose\nCreate graph objects\nTry different layouts\nWrite comments in the code\nWrite narrative in text with sections, bold ,italic etc.\n\nStep 4. Knit before you submit. Submit only your renderable .qmd file.\nMake1 - Datasets:\n\nAirline Data:\n\n Airlines Nodes \n Airlines Edges \n\nStart with this bit of code in your second chunk, after set up\n\n\n\nairline_nodes <-\n  read_csv(\"./mydatafolder/AIRLINES-NODES.csv\") %>% \n  mutate(Id = Id + 1)\n\nairline_edges <-\n  read_csv(\"./mydatafolder/AIRLINES-EDGES.csv\") %>%\n  mutate(Source = Source + 1, Target = Target + 1)\n\n\nThe Famous Zachary Karate Club dataset\n\n\n\nStart with pulling this data into your Rmarkdown:\ndata(‚Äúkarate‚Äù,package= ‚Äúigraphdata‚Äù) karate\n\nTry ?karate in the console\nNote that this is not a set of nodes, nor edges, but already a graph-object!\nSo no need to create a graph object using tbl_graph.\nYou will need to just go ahead and plot using ggraph.\n\n\nGame of Thrones:\n\n GoT Networks \n\nStart with pulling this data into your Rmarkdown:\n\n\nGoT <- read_rds(\"data/GoT.RDS\")\n\n\nNote that this is a list of 7 graphs from Game of Thrones.\nSelect one using GoT[[index]] where index = 1‚Ä¶7 and then plot directly.\nTry to access the nodes and edges and modify them using any attribute data\n\n\nAny other graph dataset from igraphdata (type ?igraphdata in console)\n\n\nAsk me for help if you need any\nMake-2: Literary Network with TV Show / Book / Story / Play\nThis is in groups. Groups of 4. To be announced\nYou need to create a Network Graph for your favourite Book, play, TV serial or Show. (E.g. Friends, BBT, or LB or HIMYM, B99, TGP, JTV‚Ä¶or Hamlet, Little Women , Pride and Prejudice, or LoTR)\nStep 1. Go to: Literary Networks for instructions.\nStep 2. Make your data using the instructions.\n\nIn the nodes excel, use id and names as your columns. Any other details in other columns to the right.\nIn your edges excel, use from and to are your first columns. Entries in these columns can be names or ids but be consistent and don‚Äôt mix.\n\nStep 3. Decide on 3 answers that you to seek and plan to make graphs for.\nStep 4. Create graph objects. Say 3 visualizations.\nStep 5. Write comments/answers in the code and narrative text. Add pictures from the web using Markdown syntax.\nStep 6. Write Reflection ( ok, a short one!) inside your Quarto document. Make sure it renders !!\nStep 7. Group Submission: Submit the render-able .qmd file AND the data. Quarto Markdown with joint authorship. Each person submits on their Assignments. All get the same grade on this one.\nAsk me for clarifications on what to do after you have read the Instructions in your group."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#read-more",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#read-more",
    "title": "The Grammar of Networks",
    "section": "Read more",
    "text": "Read more\n\nThomas Lin Pedersen - 1 giraffe, 2 giraffe,GO!\nIgraph: Network Analysis and Visualization. https://CRAN.R-project.org/package=igraph.\nPedersen, Thomas Lin. 2017a. Ggraph: An Implementation of Grammar of Graphics for Graphs and Networks. https://CRAN.R-project.org/package=ggraph.\n‚Äî‚Äî‚Äî. 2017b. Tidygraph: A Tidy Api for Graph Manipulation. https://CRAN.R-project.org/package=tidygraph.\nTyner, Sam, Fran√ßois Briatte, and Heike Hofmann. 2017. ‚ÄúNetwork Visualization with ggplot2.‚Äù The R Journal 9 (1): 27‚Äì59. https://journal.r-project.org/archive/2017/RJ-2017-023/index.html.\nNetwork Datasets https://icon.colorado.edu/#!/networks\nYunran Chen, Introduction to Network Analysis Using R"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html",
    "title": "üï∏ Networks",
    "section": "",
    "text": "Orange Tutorial\n\n Networks in R"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html#iconify-noto-v1-package-setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html#iconify-noto-v1-package-setting-up-r-packages",
    "title": "üï∏ Networks",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nknitr::opts_chunk$set(echo = TRUE)\nlibrary(tidyverse)\nlibrary(tidygraph)\nlibrary(ggraph)\nlibrary(graphlayouts)\nlibrary(ggraph)\nlibrary(graphlayouts)\nlibrary(igraphdata)\nlibrary(sand)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "title": "üï∏ Networks",
    "section": "\n Introduction",
    "text": "Introduction\nNetwork graphs show relationships between entities: what sort they are, how strong they are, and even of they change over time.\nWe will examine data structures pertaining both to the entities and the relationships between them and look at the data object that can combine these aspects together. Then we will see how these are plotted, what the structure of the plot looks like. There are also metrics that we can calculate for the network, based on its structure. We will of course examine geometric metaphors that can represent various classes of entities and their relationships.\nNetwork graphs can be rendered both as static and interactive and we will examine R packages that render both kinds of plots.\nThere is a another kind of structure: one that combines spatial and network data in one. We will defer that for a future module !"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html#what-kind-network-graphs-will-we-make",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html#what-kind-network-graphs-will-we-make",
    "title": "üï∏ Networks",
    "section": "What kind Network graphs will we make?",
    "text": "What kind Network graphs will we make?\nHere is a network map of the characters in Victor Hugo‚Äôs Les Miserables:\n\n\nAnd this: the well known Zachary‚Äôs Karate Club dataset visualized as a network"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html#references",
    "title": "üï∏ Networks",
    "section": "References",
    "text": "References\n\nHadley Wickham, Danielle Navarro, and Thomas Lin Pedersen, ggplot2: Elegant Graphics for Data Analysis. https://ggplot2-book.org/networks.html\nOmar Lizardo and Isaac Jilbert, Social Networks: An Introduction. https://bookdown.org/omarlizardo/_main/\nMark Hoffman, Methods for Network Analysis. https://bookdown.org/markhoff/social_network_analysis/\nStatistical Analysis of Network Data with R, 2nd Edition.https://github.com/kolaczyk/sand"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/120-CatData/index.html",
    "href": "content/courses/Analytics/Descriptive/Modules/120-CatData/index.html",
    "title": "üêâ Visualizing Categorical Data",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE)\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(vcd) # Michael Friendly's package, Visualizing Categorical Data\nlibrary(vcdExtra) # Categorical Data Sets\nlibrary(sjPlot) # Likert Scale Plots\nlibrary(ggpubr) # Colours, Themes and geometries in ggplot\nlibrary(ca) # Correspondence Analysis, for use some day\n\nlibrary(mosaic) # Our trusted friend\n\n## Making Tables\nlibrary(kableExtra) # html styled tables\nlibrary(gt) # Making Cool Tables\nlibrary(patchwork) # To arrange plots on a grid and other things"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/120-CatData/index.html#what-sort-of-plots-can-we-make-for-categorical-data",
    "href": "content/courses/Analytics/Descriptive/Modules/120-CatData/index.html#what-sort-of-plots-can-we-make-for-categorical-data",
    "title": "üêâ Visualizing Categorical Data",
    "section": "What sort of Plots can we make for Categorical Data?",
    "text": "What sort of Plots can we make for Categorical Data?\nWe have already seen bar plots, which allow us to plot counts of categorical data. However, if there are a large number of categorical variables or if the categorical variables have many levels, the bar plot is not adequate.\nFrom Michael Friendly:\n\nThe familiar techniques for displaying raw data are often disappointing when applied to categorical data. The simple scatterplot, for example, widely used to show the relation between quantitative response and predictors, when applied to discrete variables, gives a display of the category combinations, with all identical values overplotted, and no representation of their frequency.\n\n\nInstead, frequencies of categorical variables are often best represented graphically using areas rather than as position along a scale. Using the visual attribute:\n\n\\[\\pmb{area \\sim frequency}\\]\n\nallows creating novel graphical displays of frequency data for special circumstances.\n\nLet us not look at some sample plots that embody this ‚Äúarea-frequency* principle.\nMosaic Plots\nA mosaic plot is basically an area-proportional visualization of (typically observed) frequencies, consisting of tiles (corresponding to the cells) created by vertically and horizontally splitting a rectangle recursively. Thus, the area of each tile is proportional to the corresponding cell entry given the dimensions of previous splits.\nThe vcd::mosaic() function needs the data in contingency table form. We will use our vcd::structable() function to construct one:\n\nart <- vcd::structable(~ Treatment + Improved, data = Arthritis)\nart\n\n          Improved None Some Marked\nTreatment                          \nPlacebo              29    7      7\nTreated              13    7     21\n\nvcd::mosaic(art, gp = shading_max)\n\n\n\n### Or\n### vcd::mosaic(structable(~ Treatment + Improved, data = Arthritis), gp = shading_max, split_vertical = TRUE)\n\nBalloon Plots\n\nhousetasks <- read.delim(\n  system.file(\"demo-data/housetasks.txt\", package = \"ggpubr\"),\n  row.names = 1\n  )\nhead(housetasks, 4)\n\n\n\n  \n\n\nggballoonplot(housetasks, fill = \"value\")+\n  scale_fill_viridis_c(option = \"C\")\n\n\n\n\n\ndf <- as.data.frame(HairEyeColor)\nggballoonplot(df, x = \"Hair\", y = \"Eye\", size = \"Freq\",\n              fill = \"Freq\", facet.by = \"Sex\",\n              ggtheme = theme_bw()) +\n  scale_fill_viridis_c(option = \"C\")\n\n\n\n\nPlots for Likert Data\nIn many business situations, we perform surveys to get Likert Scale data, where several respondents rate a product or a service on a scale of Very much like, somewhat like, neutral, Dislike and Very much dislike, for example. Such data may look for example as follows:\n\ndata(efc)\nhead(efc, 20)\n\n\n\n  \n\n\n\nefc is a German data set from a European study on family care of older people. Following a common protocol, data were collected from national samples of approximately 1,000 family carers (i.e.¬†caregivers) per country and clustered into comparable subgroups to facilitate cross-national analysis. One of the research questions in this EUROFAM study was:\n\nWhat are the main difficulties carers experience accessing the services used? What prevents carers from accessing unused supports that they need? What causes them to stop using still-needed services?\n\nWe will select the variables from the efc data set that related to coping (on part of care-givers) and plot their responses after inspecting them:\n\nefc %>% select(dplyr::contains(\"cop\")) %>% str()\n\n'data.frame':   908 obs. of  9 variables:\n $ c82cop1: num  3 3 2 4 3 2 4 3 3 3 ...\n  ..- attr(*, \"label\")= chr \"do you feel you cope well as caregiver?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"never\" \"sometimes\" \"often\" \"always\"\n $ c83cop2: num  2 3 2 1 2 2 2 2 2 2 ...\n  ..- attr(*, \"label\")= chr \"do you find caregiving too demanding?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"Never\" \"Sometimes\" \"Often\" \"Always\"\n $ c84cop3: num  2 3 1 3 1 3 4 2 3 1 ...\n  ..- attr(*, \"label\")= chr \"does caregiving cause difficulties in your relationship with your friends?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"Never\" \"Sometimes\" \"Often\" \"Always\"\n $ c85cop4: num  2 3 4 1 2 3 1 1 2 2 ...\n  ..- attr(*, \"label\")= chr \"does caregiving have negative effect on your physical health?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"Never\" \"Sometimes\" \"Often\" \"Always\"\n $ c86cop5: num  1 4 1 1 2 3 1 1 2 1 ...\n  ..- attr(*, \"label\")= chr \"does caregiving cause difficulties in your relationship with your family?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"Never\" \"Sometimes\" \"Often\" \"Always\"\n $ c87cop6: num  1 1 1 1 2 2 2 1 1 1 ...\n  ..- attr(*, \"label\")= chr \"does caregiving cause financial difficulties?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"Never\" \"Sometimes\" \"Often\" \"Always\"\n $ c88cop7: num  2 3 1 1 1 2 4 2 3 1 ...\n  ..- attr(*, \"label\")= chr \"do you feel trapped in your role as caregiver?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"Never\" \"Sometimes\" \"Often\" \"Always\"\n $ c89cop8: num  3 2 4 2 4 1 1 3 1 1 ...\n  ..- attr(*, \"label\")= chr \"do you feel supported by friends/neighbours?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"never\" \"sometimes\" \"often\" \"always\"\n $ c90cop9: num  3 2 3 4 4 1 4 3 3 3 ...\n  ..- attr(*, \"label\")= chr \"do you feel caregiving worthwhile?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"never\" \"sometimes\" \"often\" \"always\"\n\n\nThe coping related variables have responses on the Likert Scale (1,2,3,4) which correspond to (never, sometimes, often, always), and each variable also has a label defining each variable. We can plot this data using the plot_likert function from package sjPlot:\n\nefc %>% select(dplyr::contains(\"cop\")) %>% \n  sjPlot::plot_likert(title = \"Caregiver Survey from EUROFAM\")\n\n\n\n\nSo there we are with Categorical data ! There are a few other plots with this type of data, which are useful in very specialized circumstances. One example of this is the agreement plot which captures the agreement between two (sets) of evaluators, on ratings given on a shared ordinal scale to a set of items. An example from the field of medical diagnosis is the opinions of two specialists on a common set of patients."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/120-CatData/index.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/120-CatData/index.html#conclusion",
    "title": "üêâ Visualizing Categorical Data",
    "section": "Conclusion",
    "text": "Conclusion\nHow are these bar plots different from histograms? Why don‚Äôt ‚Äúregular‚Äù plots simply work for Categorical data? Discuss!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/120-CatData/index.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/120-CatData/index.html#your-turn",
    "title": "üêâ Visualizing Categorical Data",
    "section": "Your Turn",
    "text": "Your Turn\n\nTake some of the categorical datasets from the vcd and vcdExtra packages and recreate the plots from this module."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/120-CatData/index.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/120-CatData/index.html#references",
    "title": "üêâ Visualizing Categorical Data",
    "section": "References",
    "text": "References\n\nUsing the strcplot command from vcd, https://cran.r-project.org/web/packages/vcd/vignettes/strucplot.pdf\nCreating Frequency Tables with vcd, https://cran.r-project.org/web/packages/vcdExtra/vignettes/A_creating.html\nCreating mosaic plots with vcd, https://cran.r-project.org/web/packages/vcdExtra/vignettes/D_mosaics.html\nMichael Friendly, Corrgrams: Exploratory displays for correlation matrices. The American Statistician August 19, 2002 (v1.5). https://www.datavis.ca/papers/corrgram.pdf"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/180-RefMat/other-tools.html",
    "href": "content/courses/Analytics/Descriptive/Modules/180-RefMat/other-tools.html",
    "title": "üìö Miscellaneous Graphing Tools, and References",
    "section": "",
    "text": "https://rawgraphs.io\n\nhttps://datawrapper.de\n\nhttps://hdlab.stanford.edu/palladio/\n\nhttps://infogram.com/\n\nhttps://www.visme.co/chart-maker/\n\nhttps://flourish.studio/ https://www.figma.com/"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/180-RefMat/other-tools.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/180-RefMat/other-tools.html#references",
    "title": "üìö Miscellaneous Graphing Tools, and References",
    "section": "References",
    "text": "References\n\nGetting started with Flourish & Figma to create beautiful custom charts, https://inside.mediahack.co.za/getting-started-with-flourish-figma-to-create-beautiful-custom-charts-34e4efb8fd3d\nFlowing Data Chart Types https://flowingdata.com/chart-types/\nGeeks for Geeks: Chart Types https://www.geeksforgeeks.org/r-charts-and-graphs/\nFinancial Times Visual Vocabulary (Interactive) https://ft-interactive.github.io/visual-vocabulary/\nFinancial Times Visual Vocabulary (PDF) https://github.com/Financial-Times/chart-doctor/blob/main/visual-vocabulary/FT4schools_RGS.pdf\nFinancial Times Data Journalism Visuals https://www.ft.com/visual-and-data-journalism\nSeverino Ribecca and John Schwabish , The Graphic Continuum https://www.severinoribecca.one/portfolio-item/the-graphic-continuum/\nWeb based tools for Dataviz https://policyviz.com/resources/data-viz-tools/\nNightingale Data Visualization Society Blog: How to visualize categorical data: https://nightingaledvs.com/endless-river-an-overview-of-dataviz-for-categorical-data/\nJohn Schwabish‚Äôs policyviz Data Viz catalogue: https://datastudio.google.com/s/quUUlgosF4U"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/180-RefMat/other-tools.html#papers",
    "href": "content/courses/Analytics/Descriptive/Modules/180-RefMat/other-tools.html#papers",
    "title": "üìö Miscellaneous Graphing Tools, and References",
    "section": "Papers",
    "text": "Papers\n1.Christopher G. Healey Department of Computer Science, North Carolina State University. Perception in Visualization"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html",
    "title": "Distributions in R",
    "section": "",
    "text": "knitr::opts_chunk$set(collapse = T, comment = \"#>\", echo = TRUE)\noptions(tibble.print_min = 4L, tibble.print_max = 4L)\nlibrary(mosaic) # package for stats, simulations, and basic plots\nlibrary(mosaicData) # package containing datasets\nlibrary(ggformula) # package for professional looking plots, that use the formula interface from mosaic\nlibrary(NHANES) # survey data collected by the US National Center for Health Statistics (NCHS)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html#introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html#introduction",
    "title": "Distributions in R",
    "section": "\n Introduction",
    "text": "Introduction\nWe will create Distributions for data in R. As always, we will consistently use the Project Mosaic ecosystem of packages in R (mosaic, mosaicData and ggformula).\n\n\n\n\n\n\nTip\n\n\n\n\nNote the standard method for all commands from the mosaic package:\ngoal( y ~ x | z, data = mydata, ‚Ä¶)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html#case-study--1-dataset-from-mosaicdata",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html#case-study--1-dataset-from-mosaicdata",
    "title": "Distributions in R",
    "section": "Case Study -1: Dataset from mosaicData\n",
    "text": "Case Study -1: Dataset from mosaicData\n\nLet us inspect what datasets are available in the package mosaicData. Type data(package = \"mosaicData\") in your Console to see what datasets are available.\nLet us choose the famous Galton dataset:\n\ndata(\"Galton\")\ninspect(Galton)\n\n\ncategorical variables:  \n    name  class levels   n missing\n1 family factor    197 898       0\n2    sex factor      2 898       0\n                                   distribution\n1 185 (1.7%), 166 (1.2%), 66 (1.2%) ...        \n2 M (51.8%), F (48.2%)                         \n\nquantitative variables:  \n    name   class min Q1 median   Q3  max      mean       sd   n missing\n1 father numeric  62 68   69.0 71.0 78.5 69.232851 2.470256 898       0\n2 mother numeric  58 63   64.0 65.5 70.5 64.084410 2.307025 898       0\n3 height numeric  56 64   66.5 69.7 79.0 66.760690 3.582918 898       0\n4  nkids integer   1  4    6.0  8.0 15.0  6.135857 2.685156 898       0\n\n\nThe data is described as:\n\nA data frame with 898 observations on the following variables.\n\n\nfamily a factor with levels for each family\n\nfather the father‚Äôs height (in inches)\n\nmother the mother‚Äôs height (in inches)\n\nsex the child‚Äôs sex: F or M\n\nheight the child‚Äôs height as an adult (in inches)\n\nnkids the number of adult children in the family, or, at least, the number whose heights Galton recorded.\n\n\nThere is a lot of Description generated by the mosaic::inspect() command ! What can we say about the dataset and its variables? How big is the dataset? How many variables? What types are they, Quant or Qual? If they are Qual, what are the levels? Are they ordered levels? Discuss!\nStat Summaries\nAs Stigler said, summaries are the first thing to look at in data. Let us tabulate some quick stat summaries of the important variables in Galton:\n\n\nfavstats(~ father, data = Galton)\n\n\n\n  \n\n\nfavstats(~ height | sex, data = Galton)\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQ.1 How many families in the data for each value of nkids?\n\n\n\ntally(~ nkids, data = Galton)\n\nnkids\n  1   2   3   4   5   6   7   8   9  10  11  15 \n 32  40  66 116 140 114 112 128  63  40  32  15 \n\n\n\n\n\n\n\n\nNote\n\n\n\nQ.2. What is the break-up by sex of the child?\n\n\n\ntally(~ nkids | sex, data = Galton)\n\n     sex\nnkids  F  M\n   1  15 17\n   2  18 22\n   3  31 35\n   4  48 68\n   5  61 79\n   6  57 57\n   7  61 51\n   8  61 67\n   9  32 31\n   10 24 16\n   11 17 15\n   15  8  7\n\n\nDistribution Plots\nWhat Questions might we have, that we could answer with a Distribution?\n\n\n\n\n\n\nNote\n\n\n\nQ.1 How many families based on the number of children?\n\n\n\n# Convert the tally into a dataframe. See the difference!\nfamily_count <- tally( ~ nkids | sex, data = Galton) %>% \n  as_tibble() %>% \n  \n  # Convert nkids from char to int\n  mutate( nkids = as.integer(nkids))\nfamily_count\n\n\n\n  \n\n\ngf_col(n ~ nkids | sex, data = family_count, fill = ~ sex, ylab = \"Number of Families\", xlab = \"Number of Kids / Family\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQ.2: How are the children‚Äôs heights distributed?\n\n\n\ngf_histogram(~ height, data = Galton) %>% \n  gf_vline(xintercept = mean(Galton$height))\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQ.3: Is there a difference in height distributions between Male and Female children?\n\n\n\nmeasures <- favstats(~ height | sex, data = Galton)\nmeasures\n\n\n\n  \n\n\ngf_histogram(~ height | sex, data = Galton) %>% \n  gf_vline(xintercept = ~ mean, data = measures)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQ.4: Are Mothers generally shorter than fathers?\n\n\n\ngf_density(~ father, data = Galton, fill = \"blue\", alpha = 0.3) %>% \n  gf_density( ~ mother, data = Galton, fill = \"red\", alpha = 0.3, xlab = \"Heights\")\n\nWarning: `stat(density)` was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQ.5: Are heights of children different based on the number of kids in the family? For Male and Female children?\n\n\n\ngf_boxplot(height ~ sex | nkids, data = Galton)\n\n\n\n\nBox plots are used to show distributions of numeric data values, especially when you want to compare them between multiple groups.\n\n\n\n\n\n\nNote\n\n\n\nQ.6: Does the mean height of children in a family vary with the number of children in the family? ( family size)\n\n\n\nmean( height ~ sex | nkids, data = Galton) %>% as_tibble() # not very inspiring!\n\n\n\n  \n\n\nby_sex_nkids <- favstats( height ~ sex + nkids, data = Galton)\nby_sex_nkids # much better!\n\n\n\n  \n\n\n\n\ngf_col(mean ~ sex.nkids, data = by_sex_nkids)\n\n\n\n\nHmm‚Ä¶not a very informative plot‚Ä¶"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html#case-study-2-dataset-from-nhanes",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html#case-study-2-dataset-from-nhanes",
    "title": "Distributions in R",
    "section": "Case Study-2: Dataset from NHANES\n",
    "text": "Case Study-2: Dataset from NHANES\n\nLet us try the NHANES dataset. Try help(NHANES) in your Console.\n\ndata(\"NHANES\")\nnames(NHANES)\n\n [1] \"ID\"               \"SurveyYr\"         \"Gender\"           \"Age\"             \n [5] \"AgeDecade\"        \"AgeMonths\"        \"Race1\"            \"Race3\"           \n [9] \"Education\"        \"MaritalStatus\"    \"HHIncome\"         \"HHIncomeMid\"     \n[13] \"Poverty\"          \"HomeRooms\"        \"HomeOwn\"          \"Work\"            \n[17] \"Weight\"           \"Length\"           \"HeadCirc\"         \"Height\"          \n[21] \"BMI\"              \"BMICatUnder20yrs\" \"BMI_WHO\"          \"Pulse\"           \n[25] \"BPSysAve\"         \"BPDiaAve\"         \"BPSys1\"           \"BPDia1\"          \n[29] \"BPSys2\"           \"BPDia2\"           \"BPSys3\"           \"BPDia3\"          \n[33] \"Testosterone\"     \"DirectChol\"       \"TotChol\"          \"UrineVol1\"       \n[37] \"UrineFlow1\"       \"UrineVol2\"        \"UrineFlow2\"       \"Diabetes\"        \n[41] \"DiabetesAge\"      \"HealthGen\"        \"DaysPhysHlthBad\"  \"DaysMentHlthBad\" \n[45] \"LittleInterest\"   \"Depressed\"        \"nPregnancies\"     \"nBabies\"         \n[49] \"Age1stBaby\"       \"SleepHrsNight\"    \"SleepTrouble\"     \"PhysActive\"      \n[53] \"PhysActiveDays\"   \"TVHrsDay\"         \"CompHrsDay\"       \"TVHrsDayChild\"   \n[57] \"CompHrsDayChild\"  \"Alcohol12PlusYr\"  \"AlcoholDay\"       \"AlcoholYear\"     \n[61] \"SmokeNow\"         \"Smoke100\"         \"Smoke100n\"        \"SmokeAge\"        \n[65] \"Marijuana\"        \"AgeFirstMarij\"    \"RegularMarij\"     \"AgeRegMarij\"     \n[69] \"HardDrugs\"        \"SexEver\"          \"SexAge\"           \"SexNumPartnLife\" \n[73] \"SexNumPartYear\"   \"SameSex\"          \"SexOrientation\"   \"PregnantNow\"     \n\n\nStat Summaries\n\nmosaic::inspect(NHANES)\n\n\ncategorical variables:  \n               name  class levels     n missing\n1          SurveyYr factor      2 10000       0\n2            Gender factor      2 10000       0\n3         AgeDecade factor      8  9667     333\n4             Race1 factor      5 10000       0\n5             Race3 factor      6  5000    5000\n6         Education factor      5  7221    2779\n7     MaritalStatus factor      6  7231    2769\n8          HHIncome factor     12  9189     811\n9           HomeOwn factor      3  9937      63\n10             Work factor      3  7771    2229\n11 BMICatUnder20yrs factor      4  1274    8726\n12          BMI_WHO factor      4  9603     397\n13         Diabetes factor      2  9858     142\n14        HealthGen factor      5  7539    2461\n15   LittleInterest factor      3  6667    3333\n16        Depressed factor      3  6673    3327\n17     SleepTrouble factor      2  7772    2228\n18       PhysActive factor      2  8326    1674\n19         TVHrsDay factor      7  4859    5141\n20       CompHrsDay factor      7  4863    5137\n21  Alcohol12PlusYr factor      2  6580    3420\n22         SmokeNow factor      2  3211    6789\n23         Smoke100 factor      2  7235    2765\n24        Smoke100n factor      2  7235    2765\n25        Marijuana factor      2  4941    5059\n26     RegularMarij factor      2  4941    5059\n27        HardDrugs factor      2  5765    4235\n28          SexEver factor      2  5767    4233\n29          SameSex factor      2  5768    4232\n30   SexOrientation factor      3  4842    5158\n31      PregnantNow factor      3  1696    8304\n                                    distribution\n1  2009_10 (50%), 2011_12 (50%)                 \n2  female (50.2%), male (49.8%)                 \n3   40-49 (14.5%),  0-9 (14.4%) ...             \n4  White (63.7%), Black (12%) ...               \n5  White (62.7%), Black (11.8%) ...             \n6  Some College (31.4%) ...                     \n7  Married (54.6%), NeverMarried (19.1%) ...    \n8  more 99999 (24.2%) ...                       \n9  Own (64.7%), Rent (33.1%) ...                \n10 Working (59.4%), NotWorking (36.6%) ...      \n11 NormWeight (63.2%), Obese (17.3%) ...        \n12 18.5_to_24.9 (30.3%) ...                     \n13 No (92.3%), Yes (7.7%)                       \n14 Good (39.2%), Vgood (33.3%) ...              \n15 None (76.5%), Several (16.9%) ...            \n16 None (78.6%), Several (15.1%) ...            \n17 No (74.6%), Yes (25.4%)                      \n18 Yes (55.8%), No (44.2%)                      \n19 2_hr (26.2%), 1_hr (18.2%) ...               \n20 0_to_1_hr (29%), 0_hrs (22.1%) ...           \n21 Yes (79.2%), No (20.8%)                      \n22 No (54.3%), Yes (45.7%)                      \n23 No (55.6%), Yes (44.4%)                      \n24 Non-Smoker (55.6%), Smoker (44.4%)           \n25 Yes (58.5%), No (41.5%)                      \n26 No (72.4%), Yes (27.6%)                      \n27 No (81.5%), Yes (18.5%)                      \n28 Yes (96.1%), No (3.9%)                       \n29 No (92.8%), Yes (7.2%)                       \n30 Heterosexual (95.8%), Bisexual (2.5%) ...    \n31 No (92.7%), Yes (4.2%) ...                   \n\nquantitative variables:  \n              name   class      min        Q1    median        Q3        max\n1               ID integer 51624.00 56904.500 62159.500 67039.000  71915.000\n2              Age integer     0.00    17.000    36.000    54.000     80.000\n3        AgeMonths integer     0.00   199.000   418.000   624.000    959.000\n4      HHIncomeMid integer  2500.00 30000.000 50000.000 87500.000 100000.000\n5          Poverty numeric     0.00     1.240     2.700     4.710      5.000\n6        HomeRooms integer     1.00     5.000     6.000     8.000     13.000\n7           Weight numeric     2.80    56.100    72.700    88.900    230.700\n8           Length numeric    47.10    75.700    87.000    96.100    112.200\n9         HeadCirc numeric    34.20    39.575    41.450    42.925     45.400\n10          Height numeric    83.60   156.800   166.000   174.500    200.400\n11             BMI numeric    12.88    21.580    25.980    30.890     81.250\n12           Pulse integer    40.00    64.000    72.000    82.000    136.000\n13        BPSysAve integer    76.00   106.000   116.000   127.000    226.000\n14        BPDiaAve integer     0.00    61.000    69.000    76.000    116.000\n15          BPSys1 integer    72.00   106.000   116.000   128.000    232.000\n16          BPDia1 integer     0.00    62.000    70.000    76.000    118.000\n17          BPSys2 integer    76.00   106.000   116.000   128.000    226.000\n18          BPDia2 integer     0.00    60.000    68.000    76.000    118.000\n19          BPSys3 integer    76.00   106.000   116.000   126.000    226.000\n20          BPDia3 integer     0.00    60.000    68.000    76.000    116.000\n21    Testosterone numeric     0.25    17.700    43.820   362.410   1795.600\n22      DirectChol numeric     0.39     1.090     1.290     1.580      4.030\n23         TotChol numeric     1.53     4.110     4.780     5.530     13.650\n24       UrineVol1 integer     0.00    50.000    94.000   164.000    510.000\n25      UrineFlow1 numeric     0.00     0.403     0.699     1.221     17.167\n26       UrineVol2 integer     0.00    52.000    95.000   171.750    409.000\n27      UrineFlow2 numeric     0.00     0.475     0.760     1.513     13.692\n28     DiabetesAge integer     1.00    40.000    50.000    58.000     80.000\n29 DaysPhysHlthBad integer     0.00     0.000     0.000     3.000     30.000\n30 DaysMentHlthBad integer     0.00     0.000     0.000     4.000     30.000\n31    nPregnancies integer     1.00     2.000     3.000     4.000     32.000\n32         nBabies integer     0.00     2.000     2.000     3.000     12.000\n33      Age1stBaby integer    14.00    19.000    22.000    26.000     39.000\n34   SleepHrsNight integer     2.00     6.000     7.000     8.000     12.000\n35  PhysActiveDays integer     1.00     2.000     3.000     5.000      7.000\n36   TVHrsDayChild integer     0.00     1.000     2.000     3.000      6.000\n37 CompHrsDayChild integer     0.00     0.000     1.000     6.000      6.000\n38      AlcoholDay integer     1.00     1.000     2.000     3.000     82.000\n39     AlcoholYear integer     0.00     3.000    24.000   104.000    364.000\n40        SmokeAge integer     6.00    15.000    17.000    19.000     72.000\n41   AgeFirstMarij integer     1.00    15.000    16.000    19.000     48.000\n42     AgeRegMarij integer     5.00    15.000    17.000    19.000     52.000\n43          SexAge integer     9.00    15.000    17.000    19.000     50.000\n44 SexNumPartnLife integer     0.00     2.000     5.000    12.000   2000.000\n45  SexNumPartYear integer     0.00     1.000     1.000     1.000     69.000\n           mean           sd     n missing\n1  6.194464e+04 5.871167e+03 10000       0\n2  3.674210e+01 2.239757e+01 10000       0\n3  4.201239e+02 2.590431e+02  4962    5038\n4  5.720617e+04 3.302028e+04  9189     811\n5  2.801844e+00 1.677909e+00  9274     726\n6  6.248918e+00 2.277538e+00  9931      69\n7  7.098180e+01 2.912536e+01  9922      78\n8  8.501602e+01 1.370503e+01   543    9457\n9  4.118068e+01 2.311483e+00    88    9912\n10 1.618778e+02 2.018657e+01  9647     353\n11 2.666014e+01 7.376579e+00  9634     366\n12 7.355973e+01 1.215542e+01  8563    1437\n13 1.181550e+02 1.724817e+01  8551    1449\n14 6.748006e+01 1.435480e+01  8551    1449\n15 1.190902e+02 1.749636e+01  8237    1763\n16 6.827826e+01 1.378078e+01  8237    1763\n17 1.184758e+02 1.749133e+01  8353    1647\n18 6.766455e+01 1.441978e+01  8353    1647\n19 1.179292e+02 1.717719e+01  8365    1635\n20 6.729874e+01 1.495839e+01  8365    1635\n21 1.978980e+02 2.265045e+02  4126    5874\n22 1.364865e+00 3.992581e-01  8474    1526\n23 4.879220e+00 1.075583e+00  8474    1526\n24 1.185161e+02 9.033648e+01  9013     987\n25 9.792946e-01 9.495143e-01  8397    1603\n26 1.196759e+02 9.016005e+01  1478    8522\n27 1.149372e+00 1.072948e+00  1476    8524\n28 4.842289e+01 1.568050e+01   629    9371\n29 3.334838e+00 7.400700e+00  7532    2468\n30 4.126493e+00 7.832971e+00  7534    2466\n31 3.026882e+00 1.795341e+00  2604    7396\n32 2.456954e+00 1.315227e+00  2416    7584\n33 2.264968e+01 4.772509e+00  1884    8116\n34 6.927531e+00 1.346729e+00  7755    2245\n35 3.743513e+00 1.836358e+00  4663    5337\n36 1.938744e+00 1.434431e+00   653    9347\n37 2.197550e+00 2.516667e+00   653    9347\n38 2.914123e+00 3.182672e+00  4914    5086\n39 7.510165e+01 1.030337e+02  5922    4078\n40 1.782662e+01 5.326660e+00  3080    6920\n41 1.702283e+01 3.895010e+00  2891    7109\n42 1.769107e+01 4.806103e+00  1366    8634\n43 1.742870e+01 3.716551e+00  5540    4460\n44 1.508507e+01 5.784643e+01  5725    4275\n45 1.342330e+00 2.782688e+00  4928    5072\n\n\nAgain, lots of data from inspect, about the Quant and Qual variables. Spend a little time looking through the output of inspect. Which variables could have been data given by each respondent, and which ones could have been measured data variables? Why do you think so?\nWhy is there so much missing data? Which variable are the most affected by this?\n\ntally(Education ~ Work, data = NHANES)\n\n                Work\nEducation        Looking NotWorking Working <NA>\n  8th Grade           13        249     188    1\n  9 - 11th Grade      39        438     411    0\n  High School         52        579     886    0\n  Some College        88        792    1387    0\n  College Grad        72        474    1552    0\n  <NA>                47        315     189 2228\n\n\nDistribution Plots\n\n\n\n\n\n\nNote\n\n\n\nQ.1. What is the distribution of Physical Activity Days, across Gender? Across Education?\n\n\n\ngf_histogram(data = NHANES, ~ PhysActiveDays | Gender)\n\nWarning: Removed 5337 rows containing non-finite values (`stat_bin()`).\n\n\n\n\ngf_histogram(data = NHANES, ~ PhysActiveDays | Education)\n\nWarning: Removed 5337 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQ.2. How are people Ages distributed across levels of Education?\n\n\n\ngf_boxplot(Age ~ Education, \n           fill = ~ Education, # Always a good idea\n           data = NHANES)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQ.3. How is Education distributed over Race?\n\n\n\ntally(Education ~ Race1 + Race3, data = NHANES)\n\n, , Race3 = Asian\n\n                Race1\nEducation        Black Hispanic Mexican White Other\n  8th Grade          0        0       0     0    15\n  9 - 11th Grade     0        0       0     0    13\n  High School        0        0       0     0    15\n  Some College       0        0       0     0    56\n  College Grad       0        0       0     0   110\n  <NA>               0        0       0     0    79\n\n, , Race3 = Black\n\n                Race1\nEducation        Black Hispanic Mexican White Other\n  8th Grade         11        0       0     0     0\n  9 - 11th Grade    63        0       0     0     0\n  High School       94        0       0     0     0\n  Some College     152        0       0     0     0\n  College Grad      76        0       0     0     0\n  <NA>             193        0       0     0     0\n\n, , Race3 = Hispanic\n\n                Race1\nEducation        Black Hispanic Mexican White Other\n  8th Grade          0       43       0     0     0\n  9 - 11th Grade     0       45       0     0     0\n  High School        0       57       0     0     0\n  Some College       0       53       0     0     0\n  College Grad       0       34       0     0     0\n  <NA>               0      118       0     0     0\n\n, , Race3 = Mexican\n\n                Race1\nEducation        Black Hispanic Mexican White Other\n  8th Grade          0        0      65     0     0\n  9 - 11th Grade     0        0      68     0     0\n  High School        0        0      61     0     0\n  Some College       0        0      54     0     0\n  College Grad       0        0      23     0     0\n  <NA>               0        0     209     0     0\n\n, , Race3 = White\n\n                Race1\nEducation        Black Hispanic Mexican White Other\n  8th Grade          0        0       0    75     0\n  9 - 11th Grade     0        0       0   207     0\n  High School        0        0       0   436     0\n  Some College       0        0       0   799     0\n  College Grad       0        0       0   870     0\n  <NA>               0        0       0   748     0\n\n, , Race3 = Other\n\n                Race1\nEducation        Black Hispanic Mexican White Other\n  8th Grade          0        0       0     0     3\n  9 - 11th Grade     0        0       0     0     9\n  High School        0        0       0     0    16\n  Some College       0        0       0     0    46\n  College Grad       0        0       0     0    15\n  <NA>               0        0       0     0    69\n\n, , Race3 = NA\n\n                Race1\nEducation        Black Hispanic Mexican White Other\n  8th Grade         11       29     112    58    29\n  9 - 11th Grade    93       22      70   282    16\n  High School      125       29      61   597    26\n  Some College     140       59      60   776    72\n  College Grad      54       42      27   743   104\n  <NA>             185       79     205   781   113\n\n\n\n\n\n\n\n\nNote\n\n\n\nQ.4. What is the distribution of people‚Äôs BMI, split by Gender? By Race1?\n\n\n\ngf_histogram(~ BMI | Gender, data = NHANES)\n\nWarning: Removed 366 rows containing non-finite values (`stat_bin()`).\n\n\n\n\ngf_histogram(~ BMI | Race1 + Race3, data = NHANES)\n\nWarning: Removed 366 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nOne can also plot both histograms and densities in an overlay fashion, if desired:\n\n# Histogram with kernel density\ngf_dhistogram( ~ BMI, data = NHANES) %>% \n  gf_fitdistr(dist = \"dnorm\")\n\nWarning: `stat(density)` was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `after_stat(density)` instead.\n\n\nWarning: Removed 366 rows containing non-finite values (`stat_bin()`).\n\n\nWarning: Removed 366 rows containing non-finite values (`stat_fitdistr()`)."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html#case-study-3-a-complete-example",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html#case-study-3-a-complete-example",
    "title": "Distributions in R",
    "section": "Case Study-3: A complete example",
    "text": "Case Study-3: A complete example\nHere is a dataset from Jeremy Singer-Vine‚Äôs blog, Data Is Plural. This is a list of all books banned in schools across the US.\n\nbanned <- readxl::read_xlsx(path = \"data/banned.xlsx\",\n                            sheet = \"Sorted by Author & Title\")\nbanned\n\n\n\n  \n\n\nnames(banned)\n\n [1] \"Author\"                    \"Title\"                    \n [3] \"Type of Ban\"               \"Secondary Author(s)\"      \n [5] \"Illustrator(s)\"            \"Translator(s)\"            \n [7] \"State\"                     \"District\"                 \n [9] \"Date of Challenge/Removal\" \"Origin of Challenge\"      \n\n\nClearly the variables are all Qualitative, except perhaps for Date of Challenge/Removal, (which in this case has been badly mangled by Excel) So we need to make counts based on the levels* of the Qual variables and plot Bar/Column charts.\nLet us quickly make some Stat Summaries:\n\nmosaic::inspect(banned)\n\n\ncategorical variables:  \n                        name     class levels    n missing\n1                     Author character    797 1586       0\n2                      Title character   1145 1586       0\n3                Type of Ban character      4 1586       0\n4        Secondary Author(s) character     61   98    1488\n5             Illustrator(s) character    192  364    1222\n6              Translator(s) character      9   10    1576\n7                      State character     26 1586       0\n8                   District character     86 1586       0\n9  Date of Challenge/Removal character     15 1586       0\n10       Origin of Challenge character      2 1586       0\n                                    distribution\n1  Kobabe, Maia (1.9%) ...                      \n2  Gender Queer: A Memoir (1.9%) ...            \n3  Banned Pending Investigation (46.1%) ...     \n4  Cast, Kristin (12.2%) ...                    \n5  Aly, Hatem (4.7%) ...                        \n6  Mlawer, Teresa (20%) ...                     \n7  Texas (45%), Pennsylvania (28.8%) ...        \n8  Central York (27.8%) ...                     \n9  44440 (28.8%), 44531 (28.3%) ...             \n10 Administrator (95.6%) ...                    \n\n\nLet us try to answer this question:\n\n\n\n\n\n\nImportant\n\n\n\nWhat is the count of banned books by type and by US state?\n\n\n\nbanned_by_state <- banned %>% group_by(State) %>% summarise(total = n()) %>% ungroup()\nbanned_by_state\n\n\n\n  \n\n\nbanned %>% group_by(State, `Type of Ban`) %>% summarise(count = n()) %>% ungroup() %>% left_join(., banned_by_state, by = c(\"State\" = \"State\")) %>% \n #  pivot_wider(.,id_cols = State,\n #              names_from = `Type of Ban`,\n #              values_from = count) %>% janitor::clean_names() %>% \n #  replace_na(list(banned_from_libraries_and_classrooms = 0,\n #                  banned_from_libraries = 0,\n #                  banned_pending_investigation = 0,\n #                  banned_from_classrooms = 0)) %>% \n # mutate(total = sum(across(where(is.integer)))) %>%\nggplot(aes(x = reorder(State, total), y = count, fill = `Type of Ban`)) + geom_col() + coord_flip()\n\n`summarise()` has grouped output by 'State'. You can override using the\n`.groups` argument."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html",
    "title": "üìä Distributions",
    "section": "",
    "text": "R Tutorial¬†¬†\n  Orange Tutorial\n\n  Radiant Tutorial¬†\n  Datasets"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#iconify-noto-v1-package-setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#iconify-noto-v1-package-setting-up-r-packages",
    "title": "üìä Distributions",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(fontawesome)\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(palmerpenguins)\nlibrary(patchwork)\n\n\n# #remotes::install_github(\"R-CoderDotCom/ridgeline@main\")\nlibrary(ridgeline)\nlibrary(ggridges)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#what-graphs-will-we-see-today",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#what-graphs-will-we-see-today",
    "title": "üìä Distributions",
    "section": "What graphs will we see today?",
    "text": "What graphs will we see today?\nSome of the very basic and commonly used plots for data are:\n\nBar and Column Charts\nHistograms and Frequency Distributions\nBox Plots\n2D Hexbins Plots and 2D Frequency Distributions\nRidge Plots ( Quant + Qual variables)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#histograms-and-frequency-distributions",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#histograms-and-frequency-distributions",
    "title": "üìä Distributions",
    "section": "Histograms and Frequency Distributions",
    "text": "Histograms and Frequency Distributions\nHistograms are best to show the distribution of raw quantitative data,by displaying the number of values that fall within defined ranges, often called buckets or bins.\nAlthough histograms may look similar to column charts, the two are different. First, histograms show continuous data, and usually you can adjust the bucket ranges to explore frequency patterns. For example, you can shift histogram buckets from 0-1, 1-2, 2-3, etc. to 0-2, 2-4, etc. By contrast, column charts show categorical data, such as the number of apples, bananas, carrots, etc. Second, histograms do not usually show spaces between buckets because these are continuous values, while column charts show spaces to separate each category.\nLet us listen to the late great Hans Rosling from the Gapminder Project, which aims at telling stories of the world with data, to remove systemic biases about poverty, income and gender related issues.\n\nExamine the Data\nLet us look at the popular mpg dataset ( from R ) using mosaic::inspect(). We get two new pieces of output (i.e.¬†two new dataframes), describing the Qual and Quant variables separately:\n\ninspect(mpg)\n\n\ncategorical variables:  \n          name     class levels   n missing\n1 manufacturer character     15 234       0\n2        model character     38 234       0\n3        trans character     10 234       0\n4          drv character      3 234       0\n5           fl character      5 234       0\n6        class character      7 234       0\n                                   distribution\n1 dodge (15.8%), toyota (14.5%) ...            \n2 caravan 2wd (4.7%) ...                       \n3 auto(l4) (35.5%), manual(m5) (24.8%) ...     \n4 f (45.3%), 4 (44%), r (10.7%)                \n5 r (71.8%), p (22.2%), e (3.4%) ...           \n6 suv (26.5%), compact (20.1%) ...             \n\nquantitative variables:  \n   name   class    min     Q1 median     Q3  max        mean       sd   n\n1 displ numeric    1.6    2.4    3.3    4.6    7    3.471795 1.291959 234\n2  year integer 1999.0 1999.0 2003.5 2008.0 2008 2003.500000 4.509646 234\n3   cyl integer    4.0    4.0    6.0    8.0    8    5.888889 1.611534 234\n4   cty integer    9.0   14.0   17.0   19.0   35   16.858974 4.255946 234\n5   hwy integer   12.0   18.0   24.0   27.0   44   23.440171 5.954643 234\n  missing\n1       0\n2       0\n3       0\n4       0\n5       0\n\n\n\n\nWe can save and see the outputs separately:\n\nmpg_describe <- inspect(mpg)\n\nmpg_describe$categorical\nmpg_describe$quantitative\n\nThere is a lot of Description generated by the mosaic::inspect() command ! What can we say about the dataset and its variables? How big is the dataset? How many variables? What types are they, Quant or Qual? If they are Qual, what are the levels? Are they ordered levels? Discuss!\nSome Sample Charts for Quant Data Distributions\n\n# library(tidyverse)\n# library(patchwork)\n# library(ggridges)\n\nmpg <- mpg %>% mutate(drv= as_factor(drv))\np <- ggplot(mpg, alpha = 0.3) \np1 <- p + geom_histogram(aes(x = hwy, fill = drv)) + labs(title = \"Histogram\")\np2 <- p + geom_density(aes(x = hwy, fill = drv))+ labs(title = \"Frequency Density\")\np3 <- p + geom_boxplot(aes(x = drv, y = hwy, fill = drv))+ labs(title = \"Boxplot\")\np4 <- p + geom_violin(aes(x = drv, y = hwy, fill = drv))+ labs(title = \"Violin\")\n\n(p1+p3)/(p2+p4) + plot_layout(nrow = 3) + plot_annotation(title = \"Single Quant Variable\",subtitle = \"And one Qual Variable too\", tag_levels = 'A') & theme_minimal()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nmpg <- mpg %>% mutate(drv= as_factor(drv))\n\np5 <- p + geom_density_ridges(aes(x = hwy, y = drv, fill = drv), alpha = 0.3, rel_min_height = 0.005) +\n  scale_y_discrete(expand = c(0.01, 0)) +\n  scale_x_continuous(expand = c(0.01, 0)) +\n  theme_ridges() + labs(title = \"Ridge Plot\")\n\np7 <- p + geom_hex(aes(x = hwy, y = cty, fill = drv))+ labs(title = \"Hex Bin Plot\")\np8 <- p + geom_density_2d(aes(x = hwy, y = cty))+ labs(title = \"2D Density Plot\")\n\n(p7 + p8) / p5 + plot_layout(guides = 'keep') + \n  plot_annotation(title = \"Two Quant Variables\",subtitle = \"And one Qual Variable too\", tag_levels = 'A') & theme_minimal()\n\nPicking joint bandwidth of 1.28"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#conclusion",
    "title": "üìä Distributions",
    "section": "Conclusion",
    "text": "Conclusion\n\nHistograms, Frequency Distributions, and Box Plots are used for Quantitative data variables\nHistograms ‚Äúdwell upon‚Äù counts, ranges, means and standard deviations\n\nFrequency Density plots ‚Äúdwell upon‚Äù probabilities and densities\n\nBox Plots ‚Äúdwell upon‚Äù medians and Quartiles\n\nQualitative data variables can be plotted as counts, using Bar Charts, or using Heat Maps\n2D density plots are used for describing two quant variables\n\nRidge Plots are density plots used for describing one Quant and one Qual variable (by inherent splitting)\nWe can split all these plots on the basis of another Qualitative variable.(Ridge Plots are already split)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#your-turn",
    "title": "üìä Distributions",
    "section": "Your Turn",
    "text": "Your Turn\n  Datasets\n\nClick on the Dataset Icon above, and unzip that archive. Try to make distribution plots with each of the three tools.\nA dataset from calmcode.io https://calmcode.io/datasets.html\nOld Faithful Data in R ( Find it!)\n\ninspect the dataset in each case and develop a set of Questions, that can be answered by appropriate stat measures, or by using a chart to show the distribution."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#references",
    "title": "üìä Distributions",
    "section": "References",
    "text": "References\n\nSee the scrolly animation for a histogram at this website: Exploring Histograms, an essay by Aran Lunzer and Amelia McNamara https://tinlizzie.org/histograms/?s=09\nMinimal R using mosaic. https://cran.r-project.org/web/packages/mosaic/vignettes/MinimalRgg.pdf"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html",
    "title": "Correlations in R",
    "section": "",
    "text": "We will create Tables for Correlations, and graphs for Correlations in R. As always, we will consistently use the Project Mosaic ecosystem of packages in R (mosaic, mosaicData and ggformula).\n\nlibrary(mosaic) # package for stats, simulations, and basic plots\nlibrary(mosaicData) # package containing datasets\nlibrary(ggformula) # package for professional looking plots, that use the formula interface from mosaic\nlibrary(NHANES) # survey data collected by the US National Center for Health Statistics (NCHS)\nlibrary(corrplot) # For Correlogram plots\n\n\n\n\n\n\n\nImportant\n\n\n\nNote the standard method for all commands from the mosaic/ggformula packages:\ngoal( y ~ x | z, data = mydata, ‚Ä¶)\n\n\n\nLet us inspect what datasets are available in the package mosaicData. Run this command in your Console:\n\n# Run in Console\ndata(package = \"mosaicData\")\n\nThe popup tab shows a lot of datasets we could use. Let us continue to use the famous Galton dataset and inspect it:\n\ndata(\"Galton\")\ninspect(Galton)\n\n\ncategorical variables:  \n    name  class levels   n missing\n1 family factor    197 898       0\n2    sex factor      2 898       0\n                                   distribution\n1 185 (1.7%), 166 (1.2%), 66 (1.2%) ...        \n2 M (51.8%), F (48.2%)                         \n\nquantitative variables:  \n    name   class min Q1 median   Q3  max      mean       sd   n missing\n1 father numeric  62 68   69.0 71.0 78.5 69.232851 2.470256 898       0\n2 mother numeric  58 63   64.0 65.5 70.5 64.084410 2.307025 898       0\n3 height numeric  56 64   66.5 69.7 79.0 66.760690 3.582918 898       0\n4  nkids integer   1  4    6.0  8.0 15.0  6.135857 2.685156 898       0\n\n\nThe inspect command already gives us a series of statistical measures of different variables of interest. As discussed previously, we can retain the output of inspect and use it in our reports: (there are ways of dressing up these tables too)\n\ngalton_describe <- inspect(Galton)\n\ngalton_describe$categorical\n\n\n\n  \n\n\ngalton_describe$quantitative\n\n\n\n  \n\n\n\nTry help(\"Galton\") in your Console. The dataset is described as:\n\nA data frame with 898 observations on the following variables.\n- family a factor with levels for each family\n- father the father‚Äôs height (in inches)\n- mother the mother‚Äôs height (in inches)\n- sex the child‚Äôs sex: F or M\n- height the child‚Äôs height as an adult (in inches)\n- nkids the number of adult children in the family, or, at least, the number whose heights Galton recorded.\n\nThere is a lot of Description generated by the mosaic::inspect() command ! What can we say about the dataset and its variables? How big is the dataset? How many variables? What types are they, Quant or Qual? If they are Qual, what are the levels? Are they ordered levels? Discuss!\n\nWhat Questions might we have, that we could answer with a Statistical Measure, or Correlation chart?\nQ.1 Which are the variables that have significant pair-wise correlations? What polarity are these correlations?\n\n# Pulling out the list of Quant variables from NHANES\ngalton_quant <- galton_describe$quantitative\ngalton_quant$name\n\n[1] \"father\" \"mother\" \"height\" \"nkids\" \n\nGGally::ggpairs(\n  Galton,\n  columns = c(\"father\", \"mother\", \"height\", \"nkids\"),\n  diag = list(\"densityDiag\"),\n  title = \"Galton Data Correlations Plot\"\n)\n\n\n\n\nCan we plot a Correlogram for this dataset?\n\n#library(corrplot)\n\ngalton_num_var <- Galton %>% select(father, mother, height, nkids)\ngalton_cor <- cor(galton_num_var)\ngalton_cor %>%\n  corrplot(method = \"ellipse\",\n           type = \"lower\",\n           main = \"Correlogram for Galton dataset\")\n\n\n\n\nClearly height is positively correlated to father and mother; interestingly, height is negatively correlated ( slightly) with nkids.\nQ.2: Let us confirm with a test:\n\nmosaic::cor_test(height ~ father, data = Galton)\n\n\n    Pearson's product-moment correlation\n\ndata:  height and father\nt = 8.5737, df = 896, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2137851 0.3347455\nsample estimates:\n      cor \n0.2753548 \n\n\n\nmosaic::cor_test(height ~ mother, data = Galton)\n\n\n    Pearson's product-moment correlation\n\ndata:  height and mother\nt = 6.1628, df = 896, p-value = 1.079e-09\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.1380554 0.2635982\nsample estimates:\n      cor \n0.2016549 \n\n\nQ.3 What does this correlation look when split by sex of Child?\nWe will use the mosaic function cor_test to get these results:\n\n# For the sons\nmosaic::cor_test(height ~ father, data = Galton %>% filter(sex == \"M\"))\n\n\n    Pearson's product-moment correlation\n\ndata:  height and father\nt = 9.1498, df = 463, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3114667 0.4656805\nsample estimates:\n      cor \n0.3913174 \n\ncor_test(height ~ mother, data = Galton %>% filter(sex == \"M\"))\n\n\n    Pearson's product-moment correlation\n\ndata:  height and mother\nt = 7.628, df = 463, p-value = 1.367e-13\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2508178 0.4125305\nsample estimates:\n      cor \n0.3341309 \n\n# For the daughters\ncor_test(height ~ father, data = Galton %>% filter(sex == \"F\"))\n\n\n    Pearson's product-moment correlation\n\ndata:  height and father\nt = 10.719, df = 431, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3809944 0.5300812\nsample estimates:\n      cor \n0.4587605 \n\ncor_test(height ~ mother, data = Galton %>% filter(sex == \"F\"))\n\n\n    Pearson's product-moment correlation\n\ndata:  height and mother\nt = 6.8588, df = 431, p-value = 2.421e-11\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2261463 0.3962226\nsample estimates:\n      cor \n0.3136984 \n\n\nQ.4. How can we show this correlation in a set of Scatter Plots + Regression Lines? Can we recreate Galton‚Äôs famous diagram?\n\n# For the sons\ngf_point(height ~ father, data = Galton %>% filter(sex == \"M\")) %>% gf_smooth(method = \"lm\")\n\n\n\ngf_point(height ~ mother, data = Galton %>% filter(sex == \"M\")) %>% gf_smooth(method = \"lm\")\n\n\n\n# For the daughters\ngf_point(height ~ father, data = Galton %>% filter(sex == \"F\")) %>% gf_smooth(method = \"lm\")\n\n\n\ngf_point(height ~ mother, data = Galton %>% filter(sex == \"F\")) %>% gf_smooth(method = \"lm\")\n\n\n\n\nAn approximation to Galton‚Äôs famous plot (see Wikipedia):\n\ngf_point(height ~ (father + mother)/2, data = Galton) %>% \n  gf_smooth(method = \"lm\") %>% \n  gf_density_2d(n = 8) %>% \n  gf_abline(slope = 1)\n\n\n\n\nHow would you interpret this plot?\n\nWe will ‚Äúlive code‚Äù this in class!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html",
    "title": "üìé Correlations",
    "section": "",
    "text": "R Tutorial¬†¬†\n  Orange Tutorial\n\n  Radiant Tutorial¬†\n  Datasets"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#iconify-noto-v1-package-setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#iconify-noto-v1-package-setting-up-r-packages",
    "title": "üìé Correlations",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(tidyverse) # Tidy data processing and plotting\nlibrary(palmerpenguins) # Our favourite dataset\nlibrary(patchwork) # Arranging plots side by side\nlibrary(ggformula) # Formula based plots\nlibrary(mosaic) # Our go to package\nlibrary(GGally) # Corr plots\nlibrary(corrplot) # More corrplots\n\n#library(grid)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#what-is-correlation",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#what-is-correlation",
    "title": "üìé Correlations",
    "section": "What is Correlation?",
    "text": "What is Correlation?\nOne of the basic Questions we would have of our data is: Does some variable depend upon another in some way? Does \\(y\\) depend upon \\(x\\)? A Correlation Test is designed to answer exactly this question.\nThe word correlation is used in everyday life to denote some form of association. We might say that we have noticed a correlation between rainy days and reduced sales at supermarkets. However, in statistical terms we use correlation to denote association between two quantitative variables. We also assume that the association is linear, that one variable increases or decreases a fixed amount for a unit increase or decrease in the other. The other technique that is often used in these circumstances is regression, which involves estimating the best straight line to summarise the association."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#correlation-coefficient",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#correlation-coefficient",
    "title": "üìé Correlations",
    "section": "Correlation coefficient",
    "text": "Correlation coefficient\nThe degree of association is measured by a correlation coefficient, denoted by r. It is sometimes called Pearson‚Äôs correlation coefficient after its originator and is a measure of linear association. (If a curved line is needed to express the relationship, other and more complicated measures of the correlation must be used.)\nThe correlation coefficient is measured on a scale that varies from + 1 through 0 to ‚Äì 1. Complete correlation between two variables is expressed by either + 1 or -1. When one variable increases as the other increases the correlation is positive; when one decreases as the other increases it is negative.\nIn formal terms, the correlation between two variables \\(x\\) and \\(y\\) is defined as\n\\[\n\\rho = E\\left[\\frac{(x - \\mu_{x}) * (y - \\mu_{y})}{(\\sigma_x)*(\\sigma_y)}\\right]\n\\]\nwhere \\(E\\) is the expectation operator ( i.e taking mean ). Think of this as the average of the products of two scaled variables. (We can see \\((x-\\mu_x)/\\sigma_x\\) is a centering and scaling of the variable \\(x\\). It is called the z-score of x.)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#what-graphs-will-we-see-today",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#what-graphs-will-we-see-today",
    "title": "üìé Correlations",
    "section": "What graphs will we see today?",
    "text": "What graphs will we see today?\nSome of the very basic and commonly used plots for data are:\n\nScatter Plot\nContour Plot\nCorrelogram\nHeatmap"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#what-correlation-charts-can-we-plot-with-numerical-quantitative-data",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#what-correlation-charts-can-we-plot-with-numerical-quantitative-data",
    "title": "üìé Correlations",
    "section": "What Correlation Charts can we plot with Numerical / Quantitative Data?",
    "text": "What Correlation Charts can we plot with Numerical / Quantitative Data?\nScatter Plot\n\nmpg <- mpg %>% mutate(drv= as_factor(drv))\n\np1 <- gf_point(cty ~ hwy, fill = ~drv, \n               colour = ~drv, \n               data = mpg) %>% \n  gf_smooth(method = \"lm\", \n            title = \"Scatter Plot\",\n            subtitle = \"Correlation/Regression/Trend Line\")\n\np2 <- gf_density_2d(cty ~ hwy, \n                    data = mpg, \n                    title = \"2D Density Plot\")\np1 + p2\n\n\n\n\nPairwise Correlation Plot\n\np2 <- GGally::ggpairs(mpg, \n                      columns = c(\"hwy\", \"cty\", \"displ\",\"cyl\"), \n                      diag = list(\"densityDiag\"), \n                      title = \"Pairwise Correlations Plot\")\np2\n\n\n\n\nPairwise Correlogram\nIn this chart, the correlation between pairs of variables is shown symbolically as coloured ellipses. The direction of the semi-major axis + the colour of the ellipse indicate whether the correlation score is positive or negative. And the more eccentric the ellipse, the higher is the correlation score in value.\n\nlibrary(corrplot)\n\nmpg_num <- mpg %>% select(hwy, cty, displ, cyl)\nmydata_cor <- cor(mpg_num)\n\ncorrplot(mydata_cor,method = \"ellipse\",\n         type = \"lower\",\n         main = \"Correlogram\")\n\n\n\n# Heatmap with numbers\ncorrplot.mixed(mydata_cor, \n               lower = \"color\",\n               upper = \"number\",\n               bg = \"wheat\",\n               tl.pos = \"d\",\n               main = \"Heatmap?\")"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#references",
    "title": "üìé Correlations",
    "section": "References",
    "text": "References\n\nMinimal R using mosaic. https://cran.r-project.org/web/packages/mosaic/vignettes/MinimalRgg.pdf"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-EvolutionFlow/evol-flow.html",
    "href": "content/courses/Analytics/Descriptive/Modules/40-EvolutionFlow/evol-flow.html",
    "title": "üï∏ Evolution and Flow",
    "section": "",
    "text": "R Tutorial\n  Orange Tutorial\n  Radiant Tutorial \n   Datasets"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-EvolutionFlow/evol-flow.html#iconify-noto-v1-package-setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/40-EvolutionFlow/evol-flow.html#iconify-noto-v1-package-setting-up-r-packages",
    "title": "üï∏ Evolution and Flow",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(ggstream)\n\n# remotes::install_github(\"corybrunson/ggalluvial@main\", build_vignettes = TRUE)\nlibrary(ggalluvial)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-EvolutionFlow/evol-flow.html#what-time-evolution-charts-can-we-plot",
    "href": "content/courses/Analytics/Descriptive/Modules/40-EvolutionFlow/evol-flow.html#what-time-evolution-charts-can-we-plot",
    "title": "üï∏ Evolution and Flow",
    "section": "What Time Evolution Charts can we plot?",
    "text": "What Time Evolution Charts can we plot?\nIn these cases, the x-axis is typically time‚Ä¶and we chart the variable of another Quant variable with respect to time, using a line geometry.\nLet is take a healthcare budget dataset from Our World in Data: We will plot graphs for 5 countries (India, China, Brazil, Russia, Canada ). Download this data by clicking on the button below:\n Download the Health data \n\nhealth <-\n  read_csv(\"data/public-health-expenditure-share-GDP-OWID.csv\")\n\nRows: 2177 Columns: 4\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (2): Entity, Code\ndbl (2): Year, public_health_expenditure_pc_gdp\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhealth_filtered <- health %>%\n  filter(Entity %in% c(\n    \"India\",\n    \"China\",\n    \"United States\",\n    \"United Kingdom\",\n    \"Russia\",\n    \"Sweden\"\n  ))\np1 <- ggplot(health_filtered,\n         aes(x = Year, y = public_health_expenditure_pc_gdp, colour = Entity)) + geom_line() + labs(y = \"Healthcare Budget\\n as % of GDP\")\n\np2 <-  \n  ggplot(health_filtered,\n         aes(x = Year, y = public_health_expenditure_pc_gdp, fill = Entity)) + geom_area()+ labs(y = \"Healthcare Budget\\n as % of GDP\")\n\n\np1 / p2 + plot_layout(nrow = 2) + plot_annotation(title = \"Line and Area Charts to show Evolution (over Time )\",subtitle = \"\", tag_levels = 'A') & theme_minimal()"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-EvolutionFlow/evol-flow.html#what-space-evolution-charts-can-we-plot",
    "href": "content/courses/Analytics/Descriptive/Modules/40-EvolutionFlow/evol-flow.html#what-space-evolution-charts-can-we-plot",
    "title": "üï∏ Evolution and Flow",
    "section": "What Space Evolution Charts can we plot?",
    "text": "What Space Evolution Charts can we plot?\nHere, the space can be any Qual variable, and we can chart another Quant or Qual variable move across levels of the first chosen Qual variable.\nFor instance we can contemplate Enrollment at a University, and show how students move from course to course in a University. Or how customers drift from one category of products or brands to another‚Ä¶.or the movement of cricket players from one IPL Team to another !!\n\nA Sankey diagram is a visualization used to depict a flow from one set of values to another. The things being connected are called stages/axes, nodes / strata and the connections are called flows / links / alluvia. Sankeys are best used when you want to show a many-to-many mapping between two domains or multiple paths through a set of stages E.g Students going through multiple courses during a semester of study.\n\n\n\n\n\n\nlibrary(ggalluvial)\ndata(\"Titanic\")\nTitanic %>% as_tibble() %>% \nggplot(data = .,\n       aes(axis1 = Class, axis2 = Sex, axis3 = Age,\n           y = n)) +\n  geom_alluvium(aes(fill = Survived),size = .25) +\n  geom_stratum() + \n  geom_text(stat = \"stratum\", aes(label = after_stat(stratum))) +\n  scale_x_discrete(limits = c(\"Class\", \"Sex\", \"Age\"), \n                   expand = c(.2, .05)) +\n  scale_fill_manual(values = c(\"darkgoldenrod\", \"purple\")) + \n  xlab(\"Demographic\") +\n  theme_minimal() +\n  ggtitle(\"passengers on the maiden voyage of the Titanic\",\n          \"stratified by demographics and survival\")\n\n\n\n\nHere is another example of a Sankey Diagram: This diagram show how energy is converted or transmitted before being consumed or lost: supplies are on the left, and demands are on the right. Data: Department of Energy & Climate Change via Tom Counsell:"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-EvolutionFlow/evol-flow.html#chord-diagram",
    "href": "content/courses/Analytics/Descriptive/Modules/40-EvolutionFlow/evol-flow.html#chord-diagram",
    "title": "üï∏ Evolution and Flow",
    "section": "Chord Diagram",
    "text": "Chord Diagram\nWe will explore this diagram when we explore network graphs with the tidygraph and ggraph packages."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-EvolutionFlow/evol-flow.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/40-EvolutionFlow/evol-flow.html#conclusion",
    "title": "üï∏ Evolution and Flow",
    "section": "Conclusion",
    "text": "Conclusion\nWe see that we can visualize ‚Äôevolutions‚Äù over time and space. The evolutions can represent changes in the quantities of things, or their categorical affiliations or groups.\nWhat business data would you depict in this way? Revenue streams? Employment? Expenditures over time and market? There are many possibilities!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-EvolutionFlow/evol-flow.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/40-EvolutionFlow/evol-flow.html#your-turn",
    "title": "üï∏ Evolution and Flow",
    "section": "Your Turn",
    "text": "Your Turn\n\nWithin the ggalluvial package are two datasets, majors and vaccinations. Plot alluvial charts for both of these.\nGo to the American Life Panel Website where you will find many public datasets. Try to take one and make charts from it that we have learned in this Module."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-EvolutionFlow/evol-flow.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/40-EvolutionFlow/evol-flow.html#references",
    "title": "üï∏ Evolution and Flow",
    "section": "References",
    "text": "References\n\nGlobal Migration, https://download.gsb.bund.de/BIB/global_flow/ A good example of the use of a Chord Diagram.\nCory Brunson, ggalluvial, https://corybrunson.github.io/ggalluvial/\n\nOther packages: Sankey plot | the R Graph Gallery (r-graph-gallery.com)\n\nAnother package: Sankey diagrams in ggplot2 with ggsankey | R CHARTS (r-charts.com)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-EvolutionFlow/files/evolutions.html",
    "href": "content/courses/Analytics/Descriptive/Modules/40-EvolutionFlow/files/evolutions.html",
    "title": "Evolutions",
    "section": "",
    "text": "Tutorial Content to be written up when Arvind has time !!!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-PartWhole/files/parts.html",
    "href": "content/courses/Analytics/Descriptive/Modules/50-PartWhole/files/parts.html",
    "title": "Part of a Whole in R",
    "section": "",
    "text": "Introduction\nWe will create Data Visualizations in R to show Parts ofa Whole. As always, we will consistently use the Project Mosaic ecosystem of packages in R (mosaic, mosaicData and ggformula). Some specialized plots ( e.g.¬†Fan Plots) may require us to load other R Packages. These will be introduced appropriately.\n\n\n\n\nRecall the standard method for all commands from the mosaic package:\ngoal( y ~ x | z, data = mydata, ‚Ä¶)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-PartWhole/part-whole.html",
    "href": "content/courses/Analytics/Descriptive/Modules/50-PartWhole/part-whole.html",
    "title": "üçï Parts of a Whole",
    "section": "",
    "text": "library(tidyverse)\nlibrary(plotrix) # Fan, Pyramid Chart\nlibrary(ggparliament) # Parliament Chart\nlibrary(ggpol) # Parliament, Arc-Bar and other intersting charts\n\n# library(waffle) # What it says! See note below: need github version\n# install.packages(\"waffle\", repos = \"https://cinc.rud.is\")\n\nlibrary(tidygraph) # Trees, Dendros, and Circle Packings\nlibrary(ggraph) # Trees, Dendros, and Circle Packings\n\nlibrary(patchwork) # Arrange your plots"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-PartWhole/part-whole.html#what-graphs-will-we-see-today",
    "href": "content/courses/Analytics/Descriptive/Modules/50-PartWhole/part-whole.html#what-graphs-will-we-see-today",
    "title": "üçï Parts of a Whole",
    "section": "What Graphs will we see today?",
    "text": "What Graphs will we see today?\nThere are a good few charts available to depict things that constitute other bigger things. We will discuss a few of these: Pie, Fan, and Donuts; Waffle and Parliament charts; Trees, Dendrograms, and Circle Packings. (The last three visuals we will explore along with network diagrams in a later module.)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-PartWhole/part-whole.html#pies-and-fans",
    "href": "content/courses/Analytics/Descriptive/Modules/50-PartWhole/part-whole.html#pies-and-fans",
    "title": "üçï Parts of a Whole",
    "section": "Pies and Fans",
    "text": "Pies and Fans\nSo let us start with ‚Äúeating humble pie‚Äù: discussing a Pie chart first.\nA pie chart is a circle divided into sectors that each represent a proportion of the whole. It is often used to show percentage, where the sum of the sectors equals 100%.\nThe problem is that humans are pretty bad at reading angles. This ubiquitous chart is much vilified in the industry and bar charts that we have seen earlier, are viewed as better options. However do read this spirited defense of pie charts here. https://speakingppt.com/why-tufte-is-flat-out-wrong-about-pie-charts/\nOn the other hand, pie charts are ubiquitous in business circles, and are very much accepted ! So there is an attractive, and similar-looking alternative, called a fan chart which we will explore here.\n(Base) R has a simple pie command that does the job.\n\npie.sales <- c(0.12, 0.3, 0.26, 0.16, 0.04, 0.12)\nlabels <- c(\"Blueberry\", \"Cherry\",\n    \"Apple\", \"Boston Cream\", \"Other\", \"Vanilla Cream\")\npie(x = pie.sales, labels = labels,col =  grDevices::hcl.colors(palette= \"Plasma\", n = 6)) # default colours\n\n\n\n\nThe fan plot displays numerical values as arcs of overlapping sectors. This allows for more effective comparison:\n\nplotrix::fan.plot(x = pie.sales, labels = labels,\n                  col = grDevices::hcl.colors(palette= \"Plasma\", n = 6),\n                  shrink = 0.03, # How much to shrink each successive sector\n                  label.radius = 1.15,\n                  main = \"Fan Plot of Ice Cream Flavours\",\n                  # ticks = 360, \n                  # if we want tick marks on the circumference\n                  \n                  max.span = pi)\n\n\n\n\nThe donut chart suffers from the same defects as the pie, so should be used with discretion. The donut chart is essentially a geom_rect from ggplot, plotted on a polar coordinate set of of axes:\n\n# Data\ndf <- data.frame(group = LETTERS[1:3],\n                 value = c(25, 20, 35))\n\ndf <-\n  df %>% dplyr::mutate(\n    fraction = value / sum(value), # percentages\n    ymax = cumsum(fraction), # cumulative percentages\n    ymin = lag(ymax, 1, default = 0),\n    # bottom edge of each\n    label = paste0(group, \"\\n value: \", value),\n    labelPosition = (ymax + ymin) / 2 # labels midway on arcs\n  )\n\ndf\n\n\n\n  \n\n\n\n\nggplot(df) + \n# `geom_rect()` requires aesthetics: xmin, xmax, ymin, and ymax\n  geom_rect(aes(xmin = 2, xmax = 4, ymin = ymin, ymax = ymax, fill = group),colour = \"black\") + \n  geom_label( x=3.5, aes(y=labelPosition, label= label), size=4) +\n  coord_polar(theta = \"y\",direction = 1) + # Upto here will give us a pie chart\n\n# When switching to polar coords:\n# x maps to r\n# y maps to theta\n# so we create a \"hole\" in the radius, in in \nxlim(c(0,4)) + # try to play with the \"0\"\ntheme_void() +\ntheme(legend.position = \"none\")"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-PartWhole/part-whole.html#waffle-and-parliament-charts",
    "href": "content/courses/Analytics/Descriptive/Modules/50-PartWhole/part-whole.html#waffle-and-parliament-charts",
    "title": "üçï Parts of a Whole",
    "section": "Waffle and Parliament Charts",
    "text": "Waffle and Parliament Charts\nWaffle charts are often called ‚Äúsquare pie charts‚Äù !\n\n# install.packages(\"waffle\", repos = \"https://cinc.rud.is\")\nlibrary(waffle)\n\n# Data\ndf <- data.frame(group = LETTERS[1:3],\n                 value = c(25, 20, 35))\n\n# Waffle plot\nggplot(df, aes(fill = group, values = value)) +\n  geom_waffle(n_rows = 8, size = 0.33, colour = \"white\") +\n  scale_fill_manual(name = NULL,\n                    values = c(\"#BA182A\", \"#FF8288\", \"#FFDBDD\"),\n                    labels = c(\"A\", \"B\", \"C\")) +\n  coord_equal() +\n  theme_void() \n\n\n\n\nThe package ggpol offers an interesting visualization in the shape of a array of ‚Äúseats‚Äù in a parliament. ( There is also a package called ggparliament which in my opinion is a bit cumbersome, having a two step procedure to convert data into ‚Äúparliament form‚Äù etc. )\n\ndf <- data.frame(group = LETTERS[1:3],\n                 value = c(25, 20, 35))\n\n# Parliament Plot\nggplot(df) +\n  ggpol::geom_parliament(aes(seats = value, \n                             fill = group),\n                         r0 = 2, # inner radius\n                         r1 = 4 # Outer radius\n  ) + \n  scale_fill_manual(name = NULL,\n                    values = c(\"#BA182A\", \"#FF8288\", \"#FFDBDD\"),\n                    labels = c(\"A\", \"B\", \"C\")) +\n  coord_equal() +\n  theme_void() \n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` in the `default_aes` field and elsewhere instead."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-PartWhole/part-whole.html#trees-dendrograms-and-circle-packings",
    "href": "content/courses/Analytics/Descriptive/Modules/50-PartWhole/part-whole.html#trees-dendrograms-and-circle-packings",
    "title": "üçï Parts of a Whole",
    "section": "Trees, Dendrograms, and Circle Packings",
    "text": "Trees, Dendrograms, and Circle Packings\nThere are still more esoteric plots to explore, if you are hell-bent on startling people ! There is an R package called ggraph, that can do these charts, and many more:\n\nggraph is an extension of ggplot2 aimed at supporting relational data structures such as networks, graphs, and trees. While it builds upon the foundation of ggplot2 and its API it comes with its own self-contained set of geoms, facets, etc., as well as adding the concept of layouts to the grammar.\n\nWe will explore these charts when we examine network diagrams. For now, we can quickly see what these diagrams look like. Although the R-code is visible to you, it may not make sense at the moment!\nDendrograms\nFrom the R Graph Gallery Website :\n\nDendrograms can be built from:\n\nHierarchical dataset: think about a CEO managing team leads managing employees and so on.\nClustering result: clustering divides a set of individuals in group according to their similarity. Its result can be visualized as a tree.\n\n\n\n# create an edge list data frame giving the hierarchical structure of your individuals\nd1 <- data.frame(from=\"origin\", to=paste(\"group\", seq(1,5), sep=\"\"))\nd2 <- data.frame(from=rep(d1$to, each=5), to=paste(\"subgroup\", seq(1,25), sep=\"_\"))\nedges <- rbind(d1, d2)\n \n# Create a graph object \nmygraph1 <- tidygraph::as_tbl_graph( edges )\n \n# Basic tree\np1 <- ggraph(mygraph1, layout = 'dendrogram', circular = TRUE) + \n  geom_edge_diagonal() +\n  geom_node_point() +\n  theme_void()\n\n\n# create a data frame \ndata <- data.frame(\n  level1=\"CEO\",\n  level2=c( rep(\"boss1\",4), rep(\"boss2\",4)),\n  level3=paste0(\"mister_\", letters[1:8])\n)\n \n# transform it to a edge list!\nedges_level1_2 <- data %>% select(level1, level2) %>% unique %>% rename(from=level1, to=level2)\nedges_level2_3 <- data %>% select(level2, level3) %>% unique %>% rename(from=level2, to=level3)\nedge_list=rbind(edges_level1_2, edges_level2_3)\n \n# Now we can plot that\nmygraph2 <- as_tbl_graph( edge_list )\np2 <- ggraph(mygraph2, layout = 'dendrogram', circular = FALSE) + \n  geom_edge_diagonal() +\n  geom_node_point() +\n  theme_void()\n\np1 + p2 + theme(aspect.ratio = 1)\n\n\n\n\nCircle Packing\n\nlibrary(tidygraph)\nlibrary(ggraph)\ngraph <- tbl_graph(flare$vertices, flare$edges)\nset.seed(1)\nggraph(graph, 'circlepack', weight = size) + \n  geom_node_circle(aes(fill = as_factor(depth)), size = 0.25, n = 50) + \n  coord_fixed() +\n  scale_fill_discrete(name = \"Depth\") +\n  theme_void()"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-Ranking/ranking.html",
    "href": "content/courses/Analytics/Descriptive/Modules/60-Ranking/ranking.html",
    "title": "üñè Ratings and Rankings",
    "section": "",
    "text": "TBD."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-Ranking/ranking.html#iconify-noto-v1-package-setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/60-Ranking/ranking.html#iconify-noto-v1-package-setting-up-r-packages",
    "title": "üñè Ratings and Rankings",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nknitr::opts_chunk$set(echo = TRUE)\nlibrary(tidyverse) # includes ggplot for plotting\n\nlibrary(ggbump) # Bump Charts\n\nlibrary(ggiraphExtra) # Radar, Spine, Donut and Donut-Pie combo charts !!\n\n# install.packages(\"devtools\")\n# devtools::install_github(\"ricardo-bion/ggradar\")\nlibrary(ggradar) # Radar Plots"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-Ranking/ranking.html#what-graphs-are-we-going-to-see-today",
    "href": "content/courses/Analytics/Descriptive/Modules/60-Ranking/ranking.html#what-graphs-are-we-going-to-see-today",
    "title": "üñè Ratings and Rankings",
    "section": "What graphs are we going to see today?",
    "text": "What graphs are we going to see today?\nWhen we wish to compare the size of things and rank them, there are quite a few ways to do it.\nBar Charts and Lollipop Charts are immediately obvious when we wish to rank things on one aspect or parameter.\nWhen we wish to rank the same set of objects against multiple aspects or parameters, then we can use Bump Charts and Radar Charts."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-Ranking/ranking.html#lollipop-charts",
    "href": "content/courses/Analytics/Descriptive/Modules/60-Ranking/ranking.html#lollipop-charts",
    "title": "üñè Ratings and Rankings",
    "section": "Lollipop Charts",
    "text": "Lollipop Charts\n\n# Sample data set\nset.seed(1)\ndf1 <- tibble(x = LETTERS[1:10],\n                 y = sample(20:35, 10, replace = TRUE))\ndf1\n\n\n\n  \n\n\nggplot(df1) + \n  geom_segment(aes(x = x, xend = x, y = 0, yend = y)) + \n  geom_point(aes(x = x, y = y, colour = x), size = 5)\n\n\n\n\nWe can flip this horizontally and reorder the \\(x\\) categories in order of decreasing ( or increasing ) \\(y\\), using forcats::fct_reorder:\n\nggplot(df1) + \n  geom_segment(aes(x = fct_reorder(x, -y), # in decreasing order of y\n                   xend = fct_reorder(x, -y), \n                   y = 0, \n                   yend = y)) + \n  geom_point(aes(x = x, y = y, colour = x), size = 5) +\n  coord_flip() + \n  xlab(\"Group\") +\n  ylab(\"\") +\n  theme_minimal()"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-Ranking/ranking.html#bump-charts",
    "href": "content/courses/Analytics/Descriptive/Modules/60-Ranking/ranking.html#bump-charts",
    "title": "üñè Ratings and Rankings",
    "section": "Bump Charts",
    "text": "Bump Charts\nBump Charts track the ranking of several objects based on other parameters, such as time/month or even category. For instance, what is the opinion score of a set of products across various categories of users:\n\nyear <- rep(2019:2021, 4)\nposition <- c(4, 2, 2, 3, 1, 4, 2, 3, 1, 1, 4, 3)\nproduct <- c(\"A\", \"A\", \"A\",\n            \"B\", \"B\", \"B\", \n            \"C\", \"C\", \"C\",\n            \"D\", \"D\", \"D\")\n\ndf2 <- tibble(x = year,\n                 y = position,\n                 group = product)\n\ndf2\n\n\n\n  \n\n\n\nWe need to use a new package called, what else, ggbump to create our Bump Charts:\n\nlibrary(ggbump)\n\nggplot(df2) +\n  geom_bump(aes(x = x, y = y, color = group)) +\n  geom_point(aes(x = x, y = y, color = group), size = 6) +\n  theme_minimal() +\n  xlab(\"Time\") +\n  ylab(\"Product Rank\") +\n  scale_color_brewer(palette = \"RdBu\") # Change Colour Scale\n\n\n\n\nWe can add labels along the ‚Äúbump lines‚Äù and remove the legend altogether:\n\nggplot(df2) +\n  geom_bump(aes(x = x, y = y, color = group)) +\n  geom_point(aes(x = x, y = y, color = group), size = 6) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"RdBu\") + # Change Colour Scale\n# Same as before up to her\n# Add the labels at start and finish\n\n  geom_text(data = df2 %>% filter(x == min(x)),\n            aes(x = x - 0.1, label = group, y = y),\n            size = 5, hjust = 1) +\n  geom_text(data = df2 %>% filter(x == max(x)),\n            aes(x = x + 0.1, label = group, y = y),\n            size = 5, hjust = 0) +\n  xlab(\"Time\") +\n  ylab(\"Product Rank\") +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-Ranking/ranking.html#radar-charts",
    "href": "content/courses/Analytics/Descriptive/Modules/60-Ranking/ranking.html#radar-charts",
    "title": "üñè Ratings and Rankings",
    "section": "Radar Charts",
    "text": "Radar Charts\nThe same data can be plotted on a roughly circular set of axes, with the radial distance defining the rank against each axes.\nOf course, we can use ggradar, which is at this time (Feb 2023) a development version and not yet part of CRAN. We will still try it, and another package ggiraphExtra which IS a part of CRAN.\n\n#library(ggradar)\n\nset.seed(4)\ndf3 <- tibble(Product = c(\"G1\", \"G2\", \"G3\"),\n              Power = runif(3), \n              Cost = runif(3),\n              Harmony = runif(3),\n              Style = runif(3),\n              Size = runif(3),\n              Manufacturability = runif(3),\n              Durability = runif(3),\n              Universality = runif(3))\ndf3\n\n\n\n  \n\n\nggradar::ggradar(plot.data = df3,\n                 axis.label.size = 3, # Titles of Params\n                 grid.label.size = 4, # Score Values/Circles\n                 group.point.size = 3,# Product Points Sizes\n                 group.line.width = 1, # Product Line Widths\n                 fill = TRUE, # fill the radar polygons\n                 fill.alpha = 0.3, # Not too dark, Arvind\n                 legend.title = \"Product\") +\n  theme_void()\n\n\n\n\nFrom the ggiraphExtra website:\n\nPackage ggiraphExtra contains many useful functions for exploratory plots. These functions are made by both ‚Äòggplot2‚Äô and ‚Äòggiraph‚Äô packages. You can make a static ggplot or an interactive ggplot by setting the parameter interactive=TRUE.\n\n\n# library(ggiraphExtra)\n\nggiraphExtra::ggRadar(data = df3,\n        aes(colour = Product),\n        rescale = FALSE,\n          )  + # recale = TRUE makes it look different...try!!\n  theme_minimal()\n\n\n\n\nBoth render very similar-looking radar charts and the syntax is not too intimidating!!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-Ranking/ranking.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/60-Ranking/ranking.html#your-turn",
    "title": "üñè Ratings and Rankings",
    "section": "Your Turn",
    "text": "Your Turn\n\nTake the HELPrct dataset from our well used mosaicData package. Plot ranking charts using each of the public health issues that you can see in that dataset. What choice will you make for the the axes?\nTry the SaratogaHouses dataset also from mosaicData."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-Space/spatial.html",
    "href": "content/courses/Analytics/Descriptive/Modules/70-Space/spatial.html",
    "title": "üó∫ Maps",
    "section": "",
    "text": "Intro to Spatial Data in R¬†¬†\n R Tutorial\n\nStatic Maps¬†\n Interactive Maps"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-Space/spatial.html#iconify-noto-v1-package-setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/70-Space/spatial.html#iconify-noto-v1-package-setting-up-r-packages",
    "title": "üó∫ Maps",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nknitr::opts_chunk$set(echo = TRUE)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(osmdata)\nlibrary(osmplotr)\nlibrary(leaflet)\nlibrary(mapview)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-Space/spatial.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/70-Space/spatial.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "title": "üó∫ Maps",
    "section": "\n Introduction",
    "text": "Introduction\nFirst; let us watch a short, noisy video on maps:"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-Space/spatial.html#what-kind-of-maps-will-we-make",
    "href": "content/courses/Analytics/Descriptive/Modules/70-Space/spatial.html#what-kind-of-maps-will-we-make",
    "title": "üó∫ Maps",
    "section": "What kind of maps will we make?",
    "text": "What kind of maps will we make?\nWe will first understand the structure of spatial data and where to find it. For now, we will deal with vector spatial data; the discussion on raster data will be dealt with in another future module.\nWe will get hands-on with making maps, both static and interactive.\nChoropleth Map\nWhat does choropleth1 mean? And what kind of information could this map represent?1¬†Etymology. From Ancient Greek œáœéœÅŒ± (kh·πìra, ‚Äúlocation‚Äù) + œÄŒª·øÜŒ∏ŒøœÇ (pl√™thos, ‚Äúa great number‚Äù) + English map. First proposed in 1938 by American geographer John Kirtland Wright to mean ‚Äúquantity in area,‚Äù although maps of the type have been used since the early 19th century.\n\n\nBubble Map\nWhat information could this map below represent?\n\n\nLet us now look at the slides. Then we will understand how the R packages sf, tmap work to create maps, using data downloadable into R using osmdata and osmplotr. We will also make interactive maps with leaflet and mapview; tmap is also capable of creating interactive maps."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-Space/spatial.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/70-Space/spatial.html#your-turn",
    "title": "üó∫ Maps",
    "section": "Your Turn",
    "text": "Your Turn\nAnimal and Bird Migration\n\nHead off to movebank.org. Look at a few species of interest and choose one.\nDownload the data ( ESRI Shapefile). Note: You will get a .zip file with a good many files in it. Save all of them, but read only the .shp file into R and Orange.\nImport that into R using sf_read()\n\nSee how you can plot locations, tracks and colour by species‚Ä¶.based on the data you download.\nFor tutorial info: https://movebankworkshopraleighnc.netlify.app/\n\nUFO Sightings\nHere is a UFO Sighting dataset, containing location and text descriptions. https://github.com/planetsig/ufo-reports/blob/master/csv-data/ufo-scrubbed-geocoded-time-standardized.csv\nSales Data from kaggle\nHead off to Kaggle and search for Geographical Sales related data. Make both static and interactive maps with this data. Justify your decisions for type of map."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Time/files/timeseries.html",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Time/files/timeseries.html",
    "title": "Analysis of Time Series in R",
    "section": "",
    "text": "library(tidyverse) # For tidy data processing and plotting\nlibrary(lubridate)  # Deal with dates\n\nlibrary(mosaic) # Out go to package for everything\n\nlibrary(fpp3) # Robert Hyndman's time series analysis package\nlibrary(timetk)  # Convert data frames to time series-specific objects\nlibrary(forecast)  # Make forecasts and decompose time series\n\n# devtools::install_github(\"FinYang/tsdl\")\nlibrary(tsdl) # Time Series Data Library from Rob Hyndman"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Time/files/timeseries.html#introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Time/files/timeseries.html#introduction",
    "title": "Analysis of Time Series in R",
    "section": "Introduction",
    "text": "Introduction\nWe will see how a time series can be broken down to its components so as to systematically understand, analyze, model and forecast it. We have to begin by answering fundamental questions such as:\n\nWhat are the types of time series?\nHow does one process and analyze time series data?\nHow does one plot time series?\nHow to decompose it? How to extract a level, a trend, and seasonal components from a time series?\nWhat is auto correlation etc.\n\nWhat is a stationary time series?"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Time/files/timeseries.html#case-study--1-walmart-sales-dataset-from-timetk",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Time/files/timeseries.html#case-study--1-walmart-sales-dataset-from-timetk",
    "title": "Analysis of Time Series in R",
    "section": "Case Study -1: Walmart Sales Dataset from timetk\n",
    "text": "Case Study -1: Walmart Sales Dataset from timetk\n\nLet us inspect what datasets are available in the package timetk. Type data(package = \"timetk\") in your Console to see what datasets are available.\nLet us choose the Walmart Sales dataset. See here for more details: Walmart Recruiting - Store Sales Forecasting |Kaggle\n\ndata(\"walmart_sales_weekly\")\nwalmart_sales_weekly\n\n\n\n  \n\n\ninspect(walmart_sales_weekly)\n\n\ncategorical variables:  \n       name     class levels    n missing\n1        id    factor   3331 1001       0\n2 IsHoliday   logical      2 1001       0\n3      Type character      1 1001       0\n                                   distribution\n1 1_1 (14.3%), 1_3 (14.3%), 1_8 (14.3%) ...    \n2 FALSE (93%), TRUE (7%)                       \n3 A (100%)                                     \n\nDate variables:  \n  name class      first       last min_diff max_diff    n missing\n1 Date  Date 2010-02-05 2012-10-26   0 days   7 days 1001       0\n\nquantitative variables:  \n           name   class         min          Q1      median          Q3\n1         Store numeric      1.0000      1.0000      1.0000      1.0000\n2          Dept numeric      1.0000      3.0000     13.0000     93.0000\n3  Weekly_Sales numeric   6165.7300  28257.3000  39886.0600  77943.5700\n4          Size numeric 151315.0000 151315.0000 151315.0000 151315.0000\n5   Temperature numeric     35.4000     57.7900     69.6400     80.4900\n6    Fuel_Price numeric      2.5140      2.7590      3.2900      3.5940\n7     MarkDown1 numeric    410.3100   4039.3900   6154.1400  10121.9700\n8     MarkDown2 numeric      0.5000     40.4800    144.8700   1569.0000\n9     MarkDown3 numeric      0.2500      6.0000     25.9650    101.6400\n10    MarkDown4 numeric      8.0000    577.1400   1822.5500   3750.5900\n11    MarkDown5 numeric    554.9200   3127.8800   4325.1900   6222.2500\n12          CPI numeric    210.3374    211.5312    215.4599    220.6369\n13 Unemployment numeric      6.5730      7.3480      7.7870      7.8380\n           max         mean           sd    n missing\n1       1.0000 1.000000e+00 0.000000e+00 1001       0\n2      95.0000 3.585714e+01 3.849159e+01 1001       0\n3  148798.0500 5.464634e+04 3.627627e+04 1001       0\n4  151315.0000 1.513150e+05 0.000000e+00 1001       0\n5      91.6500 6.830678e+01 1.420767e+01 1001       0\n6       3.9070 3.219699e+00 4.260286e-01 1001       0\n7   34577.0600 8.090766e+03 6.550983e+03  357     644\n8   46011.3800 2.941315e+03 7.873661e+03  294     707\n9   55805.5100 1.225400e+03 7.811934e+03  350     651\n10  32403.8700 3.746085e+03 5.948867e+03  357     644\n11  20475.3200 5.018655e+03 3.254071e+03  357     644\n12    223.4443 2.159969e+02 4.337818e+00 1001       0\n13      8.1060 7.610420e+00 3.825958e-01 1001       0\n\nglimpse(walmart_sales_weekly)\n\nRows: 1,001\nColumns: 17\n$ id           <fct> 1_1, 1_1, 1_1, 1_1, 1_1, 1_1, 1_1, 1_1, 1_1, 1_1, 1_1, 1_‚Ä¶\n$ Store        <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ Dept         <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ Date         <date> 2010-02-05, 2010-02-12, 2010-02-19, 2010-02-26, 2010-03-‚Ä¶\n$ Weekly_Sales <dbl> 24924.50, 46039.49, 41595.55, 19403.54, 21827.90, 21043.3‚Ä¶\n$ IsHoliday    <lgl> FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA‚Ä¶\n$ Type         <chr> \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A‚Ä¶\n$ Size         <dbl> 151315, 151315, 151315, 151315, 151315, 151315, 151315, 1‚Ä¶\n$ Temperature  <dbl> 42.31, 38.51, 39.93, 46.63, 46.50, 57.79, 54.58, 51.45, 6‚Ä¶\n$ Fuel_Price   <dbl> 2.572, 2.548, 2.514, 2.561, 2.625, 2.667, 2.720, 2.732, 2‚Ä¶\n$ MarkDown1    <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ MarkDown2    <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ MarkDown3    <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ MarkDown4    <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ MarkDown5    <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ CPI          <dbl> 211.0964, 211.2422, 211.2891, 211.3196, 211.3501, 211.380‚Ä¶\n$ Unemployment <dbl> 8.106, 8.106, 8.106, 8.106, 8.106, 8.106, 8.106, 8.106, 7‚Ä¶\n\n# Try this in your Console\n# help(\"walmart_sales_weekly\")\n\nThe data is described as:\n\nA tibble: 9,743 x 3\n\n\nid Factor. Unique series identifier (4 total)\n\nStore Numeric. Store ID.\n\nDept Numeric. Department ID.\n\nDate Date. Weekly timestamp.\n\nWeekly_Sales Numeric. Sales for the given department in the given store.\n\nIsHoliday Logical. Whether the week is a ‚Äúspecial‚Äù holiday for the store.\n\nType Character. Type identifier of the store.\n\nSize Numeric. Store square-footage\n\nTemperature Numeric. Average temperature in the region.\n\nFuel_Price Numeric. Cost of fuel in the region.\n\nMarkDown1, MarkDown2, MarkDown3, MarkDown4, MarkDown5 Numeric. Anonymized data related to promotional markdowns that Walmart is running. MarkDown data is only available after Nov 2011, and is not available for all stores all the time. Any missing value is marked with an NA.\n\nCPI Numeric. The consumer price index.\n\nUnemployment Numeric. The unemployment rate in the region.\n\n\nVery cool to know that mosaic::inspect() identifies date variables separately!\n\n\n\n\n\n\nNote\n\n\n\nNOTE:\nThis is still a tibble, with a time-oriented variable of course, but not yet a time-series object. The data frame has the YMD columns repeated for each Dept, giving us what is called ‚Äúlong‚Äù form data. To deal with this repetition, we will always need to split the Weekly_Sales by the Dept column before we plot or analyze.\n\n\nSince our sales are weekly, we will convert Date to yearweek format:\n\n#|label: walmart sales tsibble\nwalmart_time <- walmart_sales_weekly %>% \n  # mutate(Date = as.Date(Date)) %>% \n  as_tsibble(index = Date, # Time Variable\n             key = Dept)\n             \n  #  Identifies unique \"subject\" who are measures\n  #  All other variables such as Weekly_sales become \"measured variable\"\n  #  Each observation should be uniquely identified by index and key\n\nwalmart_time\n\n\n\n  \n\n\n\nBasic Time Series Plots\nThe easiest way is to use autoplot from the feasts package. You may need to specify the actual measured variable, if there is more than one numerical column:\n\nautoplot(walmart_time,\n         .vars = Weekly_Sales)\n\n\n\n\ntimetk gives us interactive plots that may be more evocative than the static plot above. The basic plot function with timetk is plot_time_series. There are arguments for the date variable, the value you want to plot, colours, groupings etc.\nLet us explore this dataset using timetk, using our trusted method of asking Questions:\nQ.1 How are the weekly sales different for each Department?\nThere are 7 number of Departments. So we should be fine plotting them and also facetting with them, as we will see in a bit:\n\nwalmart_time %>% timetk::plot_time_series(.date_var = Date, \n                                            .value = Weekly_Sales,\n                   .color_var = Dept, \n                   .legend_show = TRUE,\n                   .title = \"Walmart Sales Data by Department\",\n                   .smooth = FALSE)\n\n\n\n\n\nQ.2. What do the sales per Dept look like during the month of December (Christmas time) in 2012? Show the individual Depts as facets.\nWe can of course zoom into the interactive plot above, but if we were to plot it anyway:\n\n# Only include rows from  1 to December 31, 2011\n# Data goes only up to Oct 2012\n\nwalmart_time %>% \n  # Each side of the time_formula is specified as the character 'YYYY-MM-DD HH:MM:SS',\n  timetk::filter_by_time(.date_var = Date,\n                         .start_date = \"2011-12-01\",\n                         .end_date = \"2011-12-31\") %>%\n\n  plot_time_series(.date_var = Date, \n                   .value = Weekly_Sales, \n                   .color_var = Dept, \n                   .facet_vars = Dept, \n                   .facet_ncol = 2,\n                   .smooth = FALSE) # Only 4 points per graph\n\n\n\n\n\nClearly the ‚Äúunfortunate‚Äù Dept#13 has seen something of a Christmas drop in sales, as has Dept#38 ! The rest, all is well, it seems‚Ä¶\nToo much noise? How about some averaging?\nQ.3 How do we smooth out some of the variations in the time series to be able to understand it better?\nSometimes there is too much noise in the time series observations and we want to take what is called a rolling average. For this we will use the function timetk::slidify to create an averaging function of our choice, and then apply it to the time series using regular dplyr::mutate\n\n# Let's take the average of Sales for each month in each Department.\n# Our **function** will be named \"rolling_avg_month\": \n\nrolling_avg_month = slidify(.period = 4, # every 4 weeks\n                            .f = mean, # The funtion to average\n                            .align = \"center\", # Aligned with middle of month\n                            .partial = TRUE) # TO catch any leftover half weeks\nrolling_avg_month\n\nfunction (...) \n{\n    slider_2(..., .slider_fun = slider::pslide, .f = .f, .period = .period, \n        .align = .align, .partial = .partial, .unlist = .unlist)\n}\n<bytecode: 0x0000021e4bee3958>\n<environment: 0x0000021e4bee03a0>\n\n\nOK, slidify creates a function! Let‚Äôs apply it to the Walmart Sales time series‚Ä¶\n\nwalmart_time %>% \n  # group_by(Dept) %>% \n  mutate(avg_monthly_sales = rolling_avg_month(Weekly_Sales)) %>% \n  # ungroup() %>% \n  timetk::plot_time_series(Date, avg_monthly_sales,.color_var = Dept, .smooth = FALSE)\n\n\n\n\n\nCurves are smoother now. Need to check whether the averaging was done on a per-Dept basis‚Ä¶should we have had a group_by(Dept) before the averaging, and ungroup() before plotting? Try it !!\nDecomposing Time Series: Trends, Seasonal Patterns, and Cycles\nEach data point (\\(Y_t\\)) at time \\(t\\) in a Time Series can be expressed as either a sum or a product of 4 components, namely, Seasonality(\\(S_t\\)), Trend(\\(T_t\\)), Cyclic, and Error(\\(e_t\\)) (a.k.a White Noise).\n\nTrend: pattern exists when there is a long-term increase or decrease in the data.\nSeasonal: pattern exists when a series is influenced by seasonal factors (e.g., the quarter of the year, the month, or day of the week).\nCyclic: pattern exists when data exhibit rises and falls that are not of fixed period (duration usually of at least 2 years). Often combined with Trend into ‚ÄúTrend-Cycle‚Äù.\nError or Noise: Random component\n\nDecomposing non-seasonal datas means breaking it up into trend and irregular components. To estimate the trend component of a non-seasonal time series that can be described using an additive model, it is common to use a smoothing method, such as calculating the simple moving average of the time series.\ntimetk has the ability to achieve this: Let us plot the trend, seasonal, cyclic and irregular aspects of Weekly_Sales for Dept 38:\n\nwalmart_time %>% \n  filter(Dept == \"38\") %>% \n  timetk::plot_stl_diagnostics(.data = .,\n                               .date_var = Date, \n                               .value = Weekly_Sales)\n\nfrequency = 13 observations per 1 quarter\n\n\ntrend = 52 observations per 1 year\n\n\n\n\n\n\nWe can do this for all Dept using fable and fabletools:\n\nwalmart_decomposed <- \n  walmart_time %>% \n  \n  # If we want to filter, we do it here\n  filter(Dept == \"38\") %>% \n  # \n\nfabletools::model(stl = STL(Weekly_Sales))\n\nfabletools::components(walmart_decomposed)\n\n\n\n  \n\n\nautoplot(components((walmart_decomposed)))"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Time/files/timeseries.html#case-study-2-dataset-from-nycflights13",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Time/files/timeseries.html#case-study-2-dataset-from-nycflights13",
    "title": "Analysis of Time Series in R",
    "section": "Case Study-2: Dataset from nycflights13\n",
    "text": "Case Study-2: Dataset from nycflights13\n\nLet us try the flights dataset from the package nycflights13. Try data(package = \"nycflights13\") in your Console.\nWe have the following datasets innycflights13:\nData sets in package nycflights13:\n\n\nairlines Airline names.\n\nairports Airport metadata\n\nflights Flights data\n\nplanes Plane metadata.\n\nweather Hourly weather data\n\nLet us analyze the flights data:\n\ndata(\"flights\", package = \"nycflights13\")\nmosaic::inspect(flights)\n\n\ncategorical variables:  \n     name     class levels      n missing\n1 carrier character     16 336776       0\n2 tailnum character   4043 334264    2512\n3  origin character      3 336776       0\n4    dest character    105 336776       0\n                                   distribution\n1 UA (17.4%), B6 (16.2%), EV (16.1%) ...       \n2 N725MQ (0.2%), N722MQ (0.2%) ...             \n3 EWR (35.9%), JFK (33%), LGA (31.1%)          \n4 ORD (5.1%), ATL (5.1%), LAX (4.8%) ...       \n\nquantitative variables:  \n             name   class  min   Q1 median   Q3  max        mean          sd\n1            year integer 2013 2013   2013 2013 2013 2013.000000    0.000000\n2           month integer    1    4      7   10   12    6.548510    3.414457\n3             day integer    1    8     16   23   31   15.710787    8.768607\n4        dep_time integer    1  907   1401 1744 2400 1349.109947  488.281791\n5  sched_dep_time integer  106  906   1359 1729 2359 1344.254840  467.335756\n6       dep_delay numeric  -43   -5     -2   11 1301   12.639070   40.210061\n7        arr_time integer    1 1104   1535 1940 2400 1502.054999  533.264132\n8  sched_arr_time integer    1 1124   1556 1945 2359 1536.380220  497.457142\n9       arr_delay numeric  -86  -17     -5   14 1272    6.895377   44.633292\n10         flight integer    1  553   1496 3465 8500 1971.923620 1632.471938\n11       air_time numeric   20   82    129  192  695  150.686460   93.688305\n12       distance numeric   17  502    872 1389 4983 1039.912604  733.233033\n13           hour numeric    1    9     13   17   23   13.180247    4.661316\n14         minute numeric    0    8     29   44   59   26.230100   19.300846\n        n missing\n1  336776       0\n2  336776       0\n3  336776       0\n4  328521    8255\n5  336776       0\n6  328521    8255\n7  328063    8713\n8  336776       0\n9  327346    9430\n10 336776       0\n11 327346    9430\n12 336776       0\n13 336776       0\n14 336776       0\n\ntime variables:  \n       name   class               first                last min_diff   max_diff\n1 time_hour POSIXct 2013-01-01 05:00:00 2013-12-31 23:00:00   0 secs 25200 secs\n       n missing\n1 336776       0\n\n\nWe have time-related columns; Apart from year, month, day we have time_hour; and time-event numerical data such as arr_delay (arrival delay) and dep_delay (departure delay). We also have categorical data such as carrier, origin, dest, flight and tailnum of the aircraft. It is also a large dataset containing 330K entries. Enough to play with!!\nLet us replace the NAs in arr_delay and dep_delay with zeroes for now, and convert it into a time-series object with tsibble:\n\nflights_delay_ts <- flights %>% \n  \n  mutate(arr_delay = replace_na(arr_delay, 0), \n         dep_delay = replace_na(dep_delay, 0)) %>% \n  \n  select(time_hour, arr_delay, dep_delay, \n         carrier, origin, dest, \n         flight, tailnum) %>% \n  \n  tsibble::as_tsibble(index = time_hour, \n                      # All the remaining identify unique entries\n                      # Along with index\n                      # Many of these variables are common\n                      # Need *all* to make unique entries!\n                      key = c(carrier, origin, dest,flight, tailnum), \n                      validate = TRUE) # Making sure each entry is unique\n\n\nflights_delay_ts\n\n\n\n  \n\n\n\nQ.1. Plot the monthly average arrival delay by carrier\n\nmean_arr_delays_by_carrier <- \n  flights_delay_ts %>%\n  group_by(carrier) %>% \n  \n  index_by(month = ~ yearmonth(.)) %>% \n  # index_by uses (year, yearquarter, yearmonth, yearweek, as.Date)\n  # to create a new column to show the time-grouping\n  # year / quarter / month/ week, or day...\n  # which IS different from traditional dplyr\n  \n  summarise(mean_arr_delay = \n              mean(arr_delay, na.rm = TRUE)\n  )\n\nmean_arr_delays_by_carrier\n\n\n\n  \n\n\nmean_arr_delays_by_carrier %>%\n  timetk::plot_time_series(\n    .data = .,\n    .date_var = month,\n    .value = mean_arr_delay,\n    .facet_vars = carrier,\n    .smooth = FALSE,\n    # .smooth_degree = 1,\n    \n    # keep .smooth off since it throws warnings if there are too few points\n    # Like if we do quarterly or even yearly summaries\n    # Use only for smaller values of .smooth_degree (0,1)\n    #\n    .facet_ncol = 4,\n    .title = \"Average Monthly Arrival Delays by Carrier\"\n  )\n\n\n\n\n\nQ.2. Plot a candlestick chart for total flight delays by month for each carrier\n\nflights_delay_ts %>% \n  mutate(total_delay = arr_delay + dep_delay) %>%\n  timetk::plot_time_series_boxplot(\n    .date_var = time_hour,\n    .value = total_delay,\n    .color_var = origin,\n    .facet_vars = origin,\n    .period = \"month\",\n  # same warning again\n    .smooth = FALSE\n  )\n\n\n\n\n\nQ.2. Plot a heatmap chart for total flight delays by origin, aggregated by month\n\navg_delays_month <- flights_delay_ts %>% \n  group_by(origin) %>% \n  mutate(total_delay = arr_delay + dep_delay) %>% \n  index_by(month = ~ yearmonth(.)) %>% \n  # index_by uses (year, yearquarter, yearmonth, yearweek, as.Date)\n  # to create a new column to show the time-grouping\n  # year / quarter / month/ week, or day...\n  # which IS different from traditional dplyr\n    summarise(mean_monthly_delay = mean(total_delay, na.rm = TRUE)\n  )\n\navg_delays_month \n\n\n\n  \n\n\n# three origins 12 months therefore 36 rows\n# Tsibble index_by + summarise also gives us a  month` column \n\n\n\nggformula::gf_tile(origin ~ month, fill = ~ mean_monthly_delay, \n                   color = \"black\", data = avg_delays_month,\n                   title = \"Mean Flight Delays from NY Airports in 2013\") %>% \n  gf_theme(theme_classic()) %>% \n  gf_theme(scale_fill_viridis_c(option = \"A\")) %>% \n  \n# \"magma\" (or \"A\") inferno\" (or \"B\") \"plasma\" (or \"C\") \n# \"viridis\" (or \"D\") \"cividis\" (or \"E\") \n# \"rocket\" (or \"F\") \"mako\" (or \"G\") \"turbo\" (or \"H\")\n\n  gf_theme(scale_x_time(breaks = 1:12, labels = month.abb))"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Time/time.html",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Time/time.html",
    "title": "üïî Time Series",
    "section": "",
    "text": "library(tidyverse)\nlibrary(lubridate)  # Deal with dates\n\nlibrary(mosaic)\nlibrary(fpp3) # Robert Hyndman's textbook package\n# Loads all the core timeseries packages, see messages\n\n# devtools::install_github(\"FinYang/tsdl\")\nlibrary(tsdl) # Time Series Data Library from Rob Hyndman\n\nlibrary(tsbox) # \"new kid on the block\"\nlibrary(TSstudio) # Each Plots, Decompositions, and Modelling with Time Series"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Time/time.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Time/time.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "title": "üïî Time Series",
    "section": "\n Introduction",
    "text": "Introduction\nAny metric that is measured over regular time intervals forms a time series. Analysis of Time Series is commercially important because of industrial need and relevance, especially with respect to Forecasting (Weather data, sports scores, population growth figures, stock prices, demand, sales, supply‚Ä¶). For example, in the graph shown below are the temperatures over time in two US cities:\n\n\nWhat can we do with Time Series? A time series can be broken down to its components so as to systematically understand, analyze, model and forecast it. As with other datasets, we have to begin by answering fundamental questions, such as:\n\nWhat are the types of time series?\nHow do we visualize time series?\nHow do we decompose the time series into level,trend, and seasonal components?\nHoe might we make a model of the underlying process that creates these time series?\nHow do we make useful forecasts with the data we have?\n\nWe will first look at the multiple data formats for time series in R. Alongside we will look at the R packages that work with these formats and create graphs and measures using those objects. We will then look at obtaining the components of the time series and try our hand at modelling and forecasting."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Time/time.html#time-series-data-formats",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Time/time.html#time-series-data-formats",
    "title": "üïî Time Series",
    "section": "Time Series Data Formats",
    "text": "Time Series Data Formats\nThere are multiple formats for time series data. The ones that we are likely to encounter most are\n\ntibble format: the simplest is of course the standard tibble/ dataframe, with a time column/variable to indicate that the other variables vary with time. The standard tibble object is used by many packages, e.g.¬†timetk & modeltime\nThe ts format: We may simply have a single series of measurements that are made over time, stored as a numerical vector. The stats::ts() function will convert a numeric vector into an R time series ts object, which is the most basic time series object in R. The base ts object is used by established packages forecast and is also supported by newer packages such as tsbox.\nThe modern tsibble format: this is a new modern format for time series analysis. The special tsibble object (‚Äútime series tibble‚Äù) is used by fable, feasts and others from the tidyverts set of packages.\n\nThere are other time-oriented data formats too‚Ä¶probably too many, such a tibbletime and TimeSeries objects. For now the best way to deal with these, should you encounter them, is to use tsbox to convert them to a tibble or tsibble and work with these."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Time/time.html#creating-and-plotting-time-series",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Time/time.html#creating-and-plotting-time-series",
    "title": "üïî Time Series",
    "section": "Creating and Plotting Time Series",
    "text": "Creating and Plotting Time Series\nIn this first example, we will use simple ts data first, and then do another with tibble format that we can plot as is and then do more after conversion to tsibble format, and then a third example with a ground-up tsibble dataset.\n\nts format data\nThere are a few datasets in base R that are in ts format already.\n\nAirPassengers\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1949 112 118 132 129 121 135 148 148 136 119 104 118\n1950 115 126 141 135 125 149 170 170 158 133 114 140\n1951 145 150 178 163 172 178 199 199 184 162 146 166\n1952 171 180 193 181 183 218 230 242 209 191 172 194\n1953 196 196 236 235 229 243 264 272 237 211 180 201\n1954 204 188 235 227 234 264 302 293 259 229 203 229\n1955 242 233 267 269 270 315 364 347 312 274 237 278\n1956 284 277 317 313 318 374 413 405 355 306 271 306\n1957 315 301 356 348 355 422 465 467 404 347 305 336\n1958 340 318 362 348 363 435 491 505 404 359 310 337\n1959 360 342 406 396 420 472 548 559 463 407 362 405\n1960 417 391 419 461 472 535 622 606 508 461 390 432\n\nstr(AirPassengers)\n\n Time-Series [1:144] from 1949 to 1961: 112 118 132 129 121 135 148 148 136 119 ...\n\n\nThis can be easily plotted using base R and other more recent packages:\n\nplot(AirPassengers) # Base R\n\n\n\ntsbox::ts_plot(AirPassengers) # tsbox static plot\n\n\n\nTSstudio::ts_plot(AirPassengers) # TSstudio interactive plot\n\n\n\n\n\nOne can see that there is an upward trend and also seasonal variations that also increase over time.\nLet us take data that is ‚Äútime oriented‚Äù but not in ts format. We use the command ts to convert a numeric vector to ts format: the syntax of ts() is:\nSyntax: objectName <- ts(data, start, end, frequency), where,\n\n\ndata : represents the data vector\n\nstart : represents the first observation in time series\n\nend : represents the last observation in time series\n\nfrequency : represents number of observations per unit time. For example 1=annual, 4=quarterly, 12=monthly, 7=weekly, etc.\n\nWe will pick simple numerical vector data ( i.e.¬†not a time series ) ChickWeight:\n\nChickWeight %>% head()\n\n\n\n  \n\n\n# Filter for Chick #1 and for Diet #1\nChickWeight_ts <- ChickWeight %>% \n  filter(Chick == 1, Diet ==1) %>% \n  select(weight, Time)\n\nChickWeight_ts <- stats::ts(ChickWeight_ts$weight, frequency = 2) \nChickWeight_ts\n\nTime Series:\nStart = c(1, 1) \nEnd = c(6, 2) \nFrequency = 2 \n [1]  42  51  59  64  76  93 106 125 149 171 199 205\n\n\n\nplot(ChickWeight_ts)\n\n\n\n#ts_boxable(ChickWeight_ts)\ntsbox::ts_plot(ChickWeight_ts,\n               ylab = \"Weight of Chick#1\")\n\n\n\n\n\nTSstudio::ts_plot(ts.obj = ChickWeight_ts,\n                  Xtitle = \"Time\", \n                  Ytitle = \"Weight of Chick #1\")\n\n\n\n\n\n\ntibble data\nUsing the familiar tibble structure opens up new possibilities. We can have multiple time series within a tibble (think GDP, Population, Imports, Exports for multiple countries as with the gapminder data we saw earlier). It also allows for data processing with dplyr such as filtering and summarizing.\nLet us read and inspect in the US births data from 2000 to 2014. Download this data by clicking on the icon below, and saving the downloaded file in a sub-folder called data inside your project:\n\n\n\n Download US Births Data\n\n\n\nRead this data in:\n\nbirths_2000_2014 <- read_csv(\"data/US_births_2000-2014_SSA.csv\")\nbirths_2000_2014\n\n\n\n  \n\n\n\n\n\nUsing ggformula\nUsing tsbox and TSstudio\n\n\n\nWe will now plot this using ggformula.\nWith the separate year/month/week and day_of_week / day_of_month columns, we can plot births over time, colouring by day_of_week, for example:\n\nbirths_2000_2014 %>% \n  \n# So that we can have discrete colours for each month\n  # mutate(month = as_factor(month)) %>% \n  # \n  # group_by(year, month) %>% \n  # summarise(mean_monthly_births = mean(births, na.rm = TRUE)) %>% \n  \n  gf_line(births ~ year, group = ~ day_of_week, color = ~ day_of_week) %>% \n  gf_point() %>% \n  gf_theme(scale_colour_distiller(palette = \"Paired\")) %>% \n  gf_theme(theme_classic())\n\n\n\n\nNot particularly illuminating. This is because the data is daily and we have considerable variation over time.\nWe should calculate the the mean births on a month basis in each year and plot that:\n\nbirths_2000_2014 %>% \n         \n         # So that we can have discrete colours for each month\n         mutate(month = as_factor(month)) %>% \n  \n  group_by(year, month) %>% \n  summarise(mean_monthly_births = mean(births, na.rm = TRUE)) %>% \n  \n  gf_line(mean_monthly_births ~ year, \n          group = ~ month, \n          colour = ~month) %>% \n  gf_point() %>% \n  gf_theme(scale_colour_brewer(palette = \"Paired\")) %>% \n  gf_theme(theme_classic())\n\n\n\n\nSo‚Ä¶average births per month were higher in 2005 to 2007 and have dropped since. We can do similar graphs using day_of_week as our basis for grouping, instead of month:\n\nbirths_2000_2014 %>% \n  mutate(\n         # So that we can have discrete colours for each week day\n         day_of_week = base::factor(day_of_week,\n                                    levels = c(1,2,3,4,5,6,7), \n                                    labels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"))) %>% \n  group_by(year, day_of_week) %>% \n  summarise(mean_weekly_births = mean(births, \n                                      na.rm = TRUE)) %>% \n  gf_line(mean_weekly_births ~ year, \n             group = ~ day_of_week, \n             colour = ~ day_of_week, data = .) %>% \n  gf_point() %>% \n  \n  # palette for 12 colours\n  gf_theme(scale_colour_brewer(palette = \"Paired\")) %>% \n\n  gf_theme(theme_classic())\n\n\n\n\nLooks like an interesting story here‚Ä¶there are significantly fewer births on average on Sat and Sun, over the years! Why? Should we watch Grey‚Äôs Anatomy ?\n\n\nSo far we are simply treating the year/month/day variables are simple numerical variables. We have not created an explicit time or date variable. Let us do that now:\nSo there are several numerical variables for year, month, and day_of_month, day_of_week, and of course the births on a daily basis. tsbox::ts_plot needs just the data and the births column to plot with and not be confused by the other numerical columns, so let us create a time column from these three, but retain them for now.\n\nbirths_timeseries <- \n  births_2000_2014 %>% \n  mutate(date = lubridate::make_date(year = year,\n                                     month = month,\n                                     day = date_of_month)) %>% \n  select(date, births, year, month,date_of_month, day_of_week)\n\nbirths_timeseries\n\n\n\n  \n\n\n\nPlotting this directly:\n\nbirths_timeseries %>% \n  select(date, births) %>% \n  tsbox::ts_plot()\n\n[time]: 'date' [value]: 'births' \n\n\n\n\nbirths_timeseries %>% \n  select(date, births) %>% \n  TSstudio::ts_plot()\n\n\n\n\n\nIf we need setup average monthly and weekly births as before, we need to understand more of data processing with time series, similar to what dplyr does for tibble.\n\n\n\n\ntsibble data\nFinally, tsibble ( ‚Äútime series tibble‚Äù) format data contains three main components:\n\nan index variable that defines time;\na set of key variables, usually categorical, that define sets of observations, over time. This allows for each combination of the categorical variables to define a separate time series.\na set of quantitative variables, that represent the quantities that vary over time ( i.e index)\n\nHere is Robert Hyndman‚Äôs video introducing tsibbles:\n\nThe package tsibbledata contains several ready made tsibble format data. Run data(package = \"tsibbledata\") in your Console to find out about these. Let us try PBS which is a dataset containing Monthly Medicare prescription data in Australia.\n\ndata(\"PBS\")\nPBS\n\n\n\n  \n\n\n\nThis is a large dataset, with 67K observations, for 336 combinations of key variables (Concession, Type, ATC1, ATC2) which are categorical, as foreseen. Data appears to be monthly, as indicated by the 1M. Note that there are multiple Quantitative variables (Scripts,Cost), a feature which is not supported in the ts format, but is supported in a tsibble. The Qualitative Variables are described below. (Type help(\"PBS\") in your Console)\n\nThe data is dis-aggregated/grouped using four keys:\n\n\n\n\nConcession: Concessional scripts are given to pensioners, unemployed, dependents, and other card holders\n\nType: Co-payments are made until an individual‚Äôs script expenditure hits a threshold ($290.00 for concession, $1141.80 otherwise). Safety net subsidies are provided to individuals exceeding this amount.\n\nATC1: Anatomical Therapeutic Chemical index (level 1). 15 types\n\nATC2: Anatomical Therapeutic Chemical index (level 2). 84 types, nested inside ATC1\n\n\n\nLet us simply plot Cost over time:\n\nPBS %>% \n  gf_point(Cost ~ Month, data = .) %>% \n  gf_line() %>% \n  gf_theme(theme_classic())\n\n\n\n\nThis basic plot is quite messy. We can use dplyr functions such as mutate(), filter(), select() and summarise() to work with tsibble objects.\n\n\n\n\n\n\ntsibble package\n\n\n\nThere are specialized functions in the tsibble package to do with the index ( i.e time) variable similar things to what dplyr does.\n\n\nLet us first see how many observations there are for each combo of keys:\n\nPBS %>% \n  tsibble::group_by_key(ATC1, ATC2, Concession, Type) %>% \n  count()\n\n\n\n  \n\n\n\nWe have 336 combinations of Qualitative variables, each containing 204 observations: so let us filter for a few such combinations and plot:\n\nPBS %>% dplyr::filter(Concession == \"General\", \n                      ATC1 == \"A\",\n                      ATC2 == \"A10\") %>% \n  gf_line(Cost ~ Month, \n          colour = ~ Type, \n          data = .) %>% \n  gf_point() %>% \n  gf_theme(theme_classic())\n\n\n\n\nAs can be seen, very different time patterns based on the two Types of payment methods. Strongly seasonal for both, with seasonal variation increasing over the years, but there is an upward trend with the Co-payments method of payment.\nFinally, it may be a good idea to convert some tibble into a tsibble to leverage some of functions that tsibble offers:\n\nbirths_tsibble <- births_2000_2014 %>% \n  mutate(date = lubridate::make_date(year = year,\n                                     month = month,\n                                     day = date_of_month)) %>%\n  tsibble::as_tsibble(index = date) # Time Variable\n\nbirths_tsibble\n\n\n\n  \n\n\n\nThis is DAILY data of course. Let us say we want to group by month and plot mean monthly births as before, but now using tsibble and the index variable:\n\n\nBasic Plot\nGrouped Plot version 1\nGrouped Plot version 2\nErrors\n\n\n\n\nbirths_tsibble %>%\n  gf_line(births ~ date, data = .) %>% \n  gf_theme(theme_classic())\n\n\n\n# Very busy plot\n# Try to group by month and take average as before\n# this time with tsibble\n# \n\n\n\n\nbirths_tsibble %>% \n  tsibble::index_by(month_index = ~ yearmonth(.)) %>% \n  \n  # Monthly Birth Averages \n  summarise(mean_births = mean(births, na.rm = TRUE)) %>% \n  \n  gf_point(mean_births ~ month_index, data = .) %>% \n  gf_line() %>% \n  gf_smooth(se = FALSE, method = \"loess\") %>% \n  gf_theme(theme_minimal())\n\n\n\n\nApart from the bump during in 2006-2007, there are also seasonal trends that repeat each year, which we glimpsed earlier.\n\n\n\nbirths_tsibble %>% \n  tsibble::index_by(year_index = ~ year(.)) %>% \n\n  # Annual Birth Averages now\n  summarise(mean_births = mean(births, na.rm = TRUE)) %>%\n  \n  gf_point(mean_births ~ year_index, data = .) %>% \n  gf_line() %>% \n  gf_smooth(se = FALSE, method = \"loess\") %>% \n  gf_theme(theme_minimal())\n\n\n\n\n\n\n\n#|label: Why not use dplyr group_by?\nbirths_tsibble %>% \n  dplyr::group_by(year) %>% \n# This grouping does not give a proper result\n# The grouping by `index` is different\n# Annual Birth Average as before\n  summarise(mean_births = mean(births, na.rm = TRUE)) \n\n\n\n  \n\n\n# Should give 15 rows but does not!\n# The original dataset does, however."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Time/time.html#candle--stick-plots",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Time/time.html#candle--stick-plots",
    "title": "üïî Time Series",
    "section": "Candle- Stick Plots",
    "text": "Candle- Stick Plots\nHmm‚Ä¶can we try to plot box plots over time (Candle-Stick Plots)? Over month / quarter or year?\nMonthly Box Plots\n\n# Monthly box plots\nbirths_tsibble %>%\n  index_by(month_index = ~ yearmonth(.)) %>% \n  # 15 years\n  # No need to summarise, since we want boxplots per year / month\n  gf_boxplot(births ~ date, \n             group =  ~ month_index, \n             fill = ~ month_index, data = .) %>%  \n  # plot the groups\n  # 180 plots!!\n  gf_theme(theme_minimal())\n\n\n\n\nQuarterly boxplots\n\nbirths_tsibble %>%\n  index_by(qrtr_index = ~ yearquarter(.)) %>% # 60 quarters over 15 years\n  # No need to summarise, since we want boxplots per year / month\n  gf_boxplot(births ~ date, \n             group = ~ qrtr_index,\n             fill = ~ qrtr_index,\n             data = .) %>%  # 60 plots!!\n  gf_theme(theme_minimal())\n\n\n\n\nYearwise boxplots\n\nbirths_tsibble %>% \n  index_by(year_index = ~ lubridate::year(.)) %>% # 15 years, 15 groups\n    # No need to summarise, since we want boxplots per year / month\n\n  gf_boxplot(births ~ date, \n              group = ~ year_index, \n              fill = ~ year_index, \n             data = .) %>%  # plot the groups 15 plots\n  gf_theme(scale_fill_distiller(palette = \"Spectral\")) %>% \n  gf_theme(theme_minimal())\n\n\n\n\nAlthough the graphs are very busy, they do reveal seasonality trends at different periods. :::"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Time/time.html#seasons-trends-cycles-and-random-changes",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Time/time.html#seasons-trends-cycles-and-random-changes",
    "title": "üïî Time Series",
    "section": "Seasons, Trends, Cycles, and Random Changes",
    "text": "Seasons, Trends, Cycles, and Random Changes\nHere are how the different types of patterns in time series are as follows:\n\nTrend: A trend exists when there is a long-term increase or decrease in the data. It does not have to be linear. Sometimes we will refer to a trend as ‚Äúchanging direction‚Äù, when it might go from an increasing trend to a decreasing trend.\n\n\nSeasonal: A seasonal pattern occurs when a time series is affected by seasonal factors such as the time of the year or the day of the week. Seasonality is always of a fixed and known period. The monthly sales of drugs ( with the PBD data ) shows seasonality which is induced partly by the change in the cost of the drugs at the end of the calendar year.\n\n\nCyclic: A cycle occurs when the data exhibit rises and falls that are not of a fixed frequency. These fluctuations are usually due to economic conditions, and are often related to the ‚Äúbusiness cycle‚Äù. The duration of these fluctuations is usually at least 2 years.\n\nThe function feasts::STL allows us to create these decompositions\nLet us try to find and plot these patterns in Time Series.\n\nbirths_STL_yearly <- births_tsibble %>% \n  fabletools::model(STL(births ~ season(period = \"year\")))\n\nfabletools::components(births_STL_yearly)\n\n\n\n  \n\n\nfeasts::autoplot(components(births_STL_yearly))\n\n\n\n\nHow about a heatmap? We can cook up a categorical variable based on the number of births (low, fine, high) and use that to create a heatmap:\n\nlibrary(ggformula)\n\nbirths_2000_2014 %>%  \n  mutate(birthrate = case_when(births >=10000 ~ \"high\", births <= 8000 ~ \"low\", TRUE ~ \"fine\")) %>% \n  gf_tile(data = ., year ~ month, fill = ~ birthrate, color = \"black\") %>%\n  \n  gf_theme(scale_x_time(breaks = 1:12, labels = c(\"Jan\", \"Feb\", \"Mar\",\"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"))) %>% \n  \n  gf_theme(theme_classic())"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Time/time.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Time/time.html#conclusion",
    "title": "üïî Time Series",
    "section": "Conclusion",
    "text": "Conclusion\nTBW"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Time/time.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Time/time.html#your-turn",
    "title": "üïî Time Series",
    "section": "Your Turn",
    "text": "Your Turn\n\nChoose some of the datasets in the tsdl and in the tsibbledata packages. Plot basic, filtered and model-based graphs for these and interpret."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Time/time.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Time/time.html#references",
    "title": "üïî Time Series",
    "section": "References",
    "text": "References\n\nRobert Hyndman, Forecasting: Principles and Practice ( Third Edition). https://otexts.com/fpp3/\n\nTime Series Analysis at Our Coding Club"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Time/time.html#readings",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Time/time.html#readings",
    "title": "üïî Time Series",
    "section": "Readings",
    "text": "Readings\n\nThe Nuclear Threat‚ÄîThe Shadow Peace, part 1\n11 Ways to Visualize Changes Over Time ‚Äì A Guide\nWhat is seasonal adjustment and why is it used?\nThe start-at-zero rule\nKeeping one‚Äôs appetite after touring the sausage factory\nHow Common is Your Birthday? This Visualization Might Surprise You\nThe Fallen of World War II\nVisualizing Statistical Mix Effects and Simpson‚Äôs Paradox\nHow To Fix a Toilet (And Other Things We Couldn‚Äôt Do Without Search)"
  },
  {
    "objectID": "content/courses/Analytics/listing.html",
    "href": "content/courses/Analytics/listing.html",
    "title": "Data Analytics",
    "section": "",
    "text": "This Course takes Business Practitioners on a journey of Business Analytics: using data to derive insights, make predictions, and decide on plans of action that can be communicated and actualized in a Business context.\n\n‚ÄúBusiness analytics, or simply analytics, is the use of data, information technology, statistical analysis, quantitative methods, and mathematical or computer-based models to help managers gain improved insight about their business operations and make better, fact-based decisions. Business analytics is‚Äùa process of transforming data into actions through analysis and insights in the context of organizational decision making and problem solving.‚Äù\n\nLibertore and Luo, 2010\n\n\n The Course starts with Descriptive Analytics: Datasets from various domains of Business enterprise and activity are introduced. The datasets are motivated from the point of view of the types of information they contain: students will relate the Data Variables (Qualitative and Quantitative) to various types of Data/Information Visualizations.\nStatistical Concepts such as Sampling, Hypothesis Tests, Simulation / Modelling, and Uncertainty will be introduced.\nPredictive Analytics will take us into looking at Data and training standard ML algorithms to make predictions with new Data. Regression, Clustering, and Classification will be covered.\nPrescriptive Analytics will deal with coming to terms with the uncertainty in Predictions, and using tools such as both ML, Linear/non-Linear Programming, and Decision-Making to make Business Decisions, with an assessment of the Risks involved.\nThe Course will culminate in a full Business Analytics Workflow that includes Data Gathering and Cleaning, Descriptive and Predictive Analytics, Prescriptive Analytics and Decision Making, and Communication resulting in a publication-worthy documents.(HTML / PDF/ Word)"
  },
  {
    "objectID": "content/courses/Analytics/listing.html#what-you-will-learn",
    "href": "content/courses/Analytics/listing.html#what-you-will-learn",
    "title": "Data Analytics",
    "section": "What you will learn",
    "text": "What you will learn\n\n\nData Basics: What does data look like and why should we care?\nRapidly and intuitively creating Graphs and Data Visualizations to explore data for insights\nUse Statistical Tests, Procedures, Models, and Simulations and to answer Business Questions\nUsing ML algorithms such Regression, Classification, and Clustering to develop Business Insights\nUse Linear Programming to make Business Decisions\nCreate crisp and readable Reports that can be shared in a Business Context"
  },
  {
    "objectID": "content/courses/Analytics/listing.html#texts",
    "href": "content/courses/Analytics/listing.html#texts",
    "title": "Data Analytics",
    "section": "Texts",
    "text": "Texts\n\nJames R Evans, Business Analytics: Methods, Models, and Decisions, Pearson Education, 2021."
  },
  {
    "objectID": "content/courses/Analytics/listing.html#references",
    "href": "content/courses/Analytics/listing.html#references",
    "title": "Data Analytics",
    "section": "References",
    "text": "References\n\nDimitris Bertsimas, Robert Freund, Data, Models, and Decisions: the Fundamentals of Management Science, Dynamic Ideas Press, 2004.\nCliff T. Ragsdale, Spreadsheet Modeling & Decision Analysis: A Practical Introduction to Management Science, South Western, Cengage Learning, Mason, OH, 2012.\nJack Dougherty and Ilya Ilyankou, Hands-On Data Visualization: Interactive Storytelling from Spreadsheets to Code, https://handsondataviz.org/. Available free Online.\nClaus O. Wilke, Fundamentals of Data Visualization, https://clauswilke.com/dataviz/. Available free Online.\nJonathan Schwabish, Better Data Visualizations: A Guide for Scholars, Researchers, and Wonks, Columbia University Press, 2021.\nAlberto Cairo, The Functional Art:An introduction to information graphics and visualization, New Riders. 2013. ISBN-9780133041361.\nCole Nussbaumer Knaflic, Storytelling With Data: A Data Visualization Guide for Business Professionals, Wiley 2015. ISBN-9781119002253."
  },
  {
    "objectID": "content/courses/Analytics/listing.html#our-tools",
    "href": "content/courses/Analytics/listing.html#our-tools",
    "title": "Data Analytics",
    "section": "Our Tools",
    "text": "Our Tools\n\n\nOrange Data Mining https://orangedatamining.com/ Orange is a FOSS visual point-and-click software for Data Mining and ML, developed at the University of Slovenia, Ljubljana.\n\n\n\n\n\n\nRadiant ‚Äì Business analytics using R and Shiny https://radiant-rstats.github.io/docs/index.html\n\n\nRadiant is a FOSS platform-independent browser-based interface for business analytics in R, developed at the University of San Diego. The application is based on the Shiny package and can be run using R, or in your browser with no installation required.\n\n\n\n\n\nR https://cran.r-project.org/ and RStudio https://posit.co/\n\n\nR is a freely available language and environment for statistical computing and graphics which provides a wide variety of statistical and graphical techniques: linear and nonlinear modelling, statistical tests, time series analysis, classification, clustering, etc. RStudio is an integrated development environment (IDE) for R and Python."
  },
  {
    "objectID": "content/courses/Analytics/listing.html#modules",
    "href": "content/courses/Analytics/listing.html#modules",
    "title": "Data Analytics",
    "section": "Modules",
    "text": "Modules"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/listing.html",
    "href": "content/courses/Analytics/Modelling/listing.html",
    "title": "Basics of Modelling",
    "section": "",
    "text": "üß≠ Basics of Statistical Modeling\n\n\n\n\n\n\n\n\n\n\n\n\nNov 9, 2021\n\n\n\n\n\n\n \n\n\n\n\nüé≤ Samples, Populations, Statistics and Inference\n\n\n\n\n\n\n\nSampling\n\n\nCentral Limit Theorem\n\n\nStandard Error\n\n\nConfidence Intervals\n\n\n\n\nHow much Land Data does a Man need?\n\n\n\n\n\n\nNov 25, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\n \n\n\n\n\nBasics of Simulation Tests\n\n\n\n\n\nWhat is meant by a Simulation Test?\n\n\n\n\n\n\nNov 27, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\n \n\n\n\n\nPermutation Tests\n\n\n\n\n\n\n\nPermutation\n\n\nMonte Carlo Simulation\n\n\nRandom Number Generation\n\n\nNull Distributions\n\n\n\n\nGenerating Parallel Worlds\n\n\n\n\n\n\nNov 28, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nüÉè Permutation Test for Two Proportions\n\n\n\n\n\n\n\nPermutation\n\n\nMonte Carlo Simulation\n\n\nRandom Number Generation\n\n\nDistributions\n\n\nGenerating Parallel Worlds\n\n\n\n\nUsing Permutation Tests to check the equivalence of two proportions\n\n\n\n\n\n\nNov 10, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nüÉè Permutation Test for Two Means\n\n\n\n\n\n\n\n\n\n\n\n\nNov 10, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\n \n\n\n\n\nBootstrap\n\n\n\n\n\nSampling with Replacement\n\n\n\n\n\n\nNov 28, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\n \n\n\n\n\nComparing Multiple Means with ANOVA\n\n\n\n\n\nANOVA to investigate how frogspawn hatching time varies with temperature.\n\n\n\n\n\n\nMar 28, 2023\n\n\nArvind Venkatadri\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/10-Intro/index.html#introduction",
    "href": "content/courses/Analytics/Modelling/Modules/10-Intro/index.html#introduction",
    "title": "üß≠ Basics of Statistical Modeling",
    "section": "Introduction",
    "text": "Introduction\nIn this set of modules we will explore Data, understand what types of data variables there are, and the kinds of statistical tests and visualizations we can create with them."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/10-Intro/index.html#the-big-ideas-in-stats",
    "href": "content/courses/Analytics/Modelling/Modules/10-Intro/index.html#the-big-ideas-in-stats",
    "title": "üß≠ Basics of Statistical Modeling",
    "section": "The Big Ideas in Stats",
    "text": "The Big Ideas in Stats\nSteven Stigler is the author of the book ‚ÄúThe Seven Pillars of Statistical Wisdom‚Äù. The Big Ideas in Statistics from that book are:\n\n\nAggregation\n\nThe first pillar I will call Aggregation, although it could just as well be given the nineteenth-century name, ‚ÄúThe Combination of Observations,‚Äù or even reduced to the simplest example, taking a mean. Those simple names are misleading, in that I refer to an idea that is now old but was truly revolutionary in an earlier day‚Äîand it still is so today, whenever it reaches into a new area of application. How is it revolutionary? By stipulating that, given a number of observations, you can actually gain information by throwing information away! In taking a simple arithmetic mean, we discard the individuality of the measures, subsuming them to one summary.\n\n\n\nInformation\n\nIn the early eighteenth century it was discovered that in many situations the amount of information in a set of data was only proportional to the square root of the number n of observations, not the number n itself.\n\n\n\nLikelihood\n\nBy the name I give to the third pillar, Likelihood, I mean the calibration of inferences with the use of probability. The simplest form for this is in significance testing and the common P-value, but as the name ‚ÄúLikelihood‚Äù hints, there is a wealth of associated methods, many related to parametric families or to Fisherian or Bayesian inference.\n\n\n\nIntercomparison\n\nIt represents what was also once a radical idea and is now commonplace: that statistical comparisons do not need to be made with respect to an exterior standard but can often be made in terms interior to the data themselves. The most commonly encountered examples of intercomparisons are Student‚Äôs t-tests and the tests of the analysis of variance.\n\n\n\nRegression\n\nI call the fifth pillar Regression, after Galton‚Äôs revelation of 1885, explained in terms of the bivariate normal distribution. Galton arrived at this by attempting to devise a mathematical framework for Charles Darwin‚Äôs theory of natural selection, overcoming what appeared to Galton to be an intrinsic contradiction in the theory: selection required increasing diversity, in contradiction to the appearance of the population stability needed for the definition of species.\n\n\n\nDesign of Experiments and Observations\n\nThe sixth pillar is Design, as in ‚ÄúDesign of Experiments,‚Äù but conceived of more broadly, as an ideal that can discipline our thinking in even observational settings.Starting in the late nineteenth century, a new understanding of the topic appeared, as Charles S. Peirce and then Fisher discovered the extraordinary role randomization could play in inference.\n\n\n\nResiduals\n\nThe most common appearances in Statistics are our model diagnostics (plotting residuals), but more important is the way we explore high-dimensional spaces by fitting and comparing nested models.\n\n\n\nIn our work with Statistical Models, we will be working with all except Idea 6 above."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/10-Intro/index.html#what-is-a-statistical-model",
    "href": "content/courses/Analytics/Modelling/Modules/10-Intro/index.html#what-is-a-statistical-model",
    "title": "üß≠ Basics of Statistical Modeling",
    "section": "What is a Statistical Model?",
    "text": "What is a Statistical Model?\nFrom Daniel Kaplan‚Äôs book:\n‚ÄúModeling‚Äù is a process of asking questions. ‚ÄúStatistical‚Äù refers in part to data ‚Äì the statistical models you will construct will be rooted in data. But it refers also to a distinctively modern idea: that you can measure what you don‚Äôt know and that doing so contributes to your understanding.\nThe conclusions you reach from data depend on the specific questions you ask.\nThe word ‚Äúmodeling‚Äù highlights that your goals, your beliefs, and your current state of knowledge all influence your analysis of data.\nSimilarly, in statistical modeling, you examine your data to see whether they are consistent with the hypotheses that frame your understanding of the system under study."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/10-Intro/index.html#uses-and-types-of-statistical-models",
    "href": "content/courses/Analytics/Modelling/Modules/10-Intro/index.html#uses-and-types-of-statistical-models",
    "title": "üß≠ Basics of Statistical Modeling",
    "section": "Uses and Types of Statistical Models",
    "text": "Uses and Types of Statistical Models\nThere are three main uses for statistical models. They are closely related, but distinct enough to be worth enumerating.\n\nDescription. Sometimes you want to describe the range or typical values of a quantity. For example, what‚Äôs a ‚Äúnormal‚Äù white blood cell count? Sometimes you want to describe the relationship between things. Example: What‚Äôs the relationship between the price of gasoline and consumption by automobiles?\nClassification or prediction. You often have information about some observable traits, qualities, or attributes of a system you observe and want to draw conclusions about other things that you can‚Äôt directly observe. For instance, you know a patient‚Äôs white blood-cell count and other laboratory measurements and want to diagnose the patient‚Äôs illness.\nAnticipating the consequences of interventions. Here, you intend to do something: you are not merely an observer but an active participant in the system. For example, people involved in setting or debating public policy have to deal with questions like these: To what extent will increasing the tax on gasoline reduce consumption? To what extent will paying teachers more increase student performance?\n\nThe appropriate form of a model depends on the purpose. For example, a model that diagnoses a patient as ill based on an observation of a high number of white blood cells can be sensible and useful. But that same model could give absurd predictions about intervention: Do you really think that lowering the white blood cell count by bleeding a patient will make the patient better?\nTo anticipate correctly the effects of an intervention you need to get the direction of cause and effect correct in your models. But for a model used for classification or prediction, it may be unnecessary to represent causation correctly. Instead, other issues, e.g., the reliability of data, can be the most important. One of the thorniest issues in statistical modeling ‚Äì with tremendous consequences for science, medicine, government, and commerce ‚Äì is how you can legitimately draw conclusions about interventions from models based on data collected without performing these interventions.\nThe Intent of Modelling\nFrom Daniel T. Kaplan‚Äôs book:\n\nStatistics is about variation. Describing and interpreting variation is a major goal of statistics.\nYou can create empirical, mathematical descriptions not only of a single trait or variable but also of the relationships between two or more traits. (Empirical means based on measurements, data, observations.)\n\nModels let you split variation into components: ‚Äúexplained‚Äù versus ‚Äúunexplained.‚Äù How to measure the size of these components and how to compare them to one another is a central aspect of statistical methodology. Indeed, this provides a definition of statistics:\n\nStatistics is the explanation of variation in the context of what remains unexplained.\n\n\nBy collecting data in ways that require care but are quite feasible, you can estimate how reliable your descriptions are, e.g., whether it‚Äôs plausible that you should see similar relationships if you collected new data. This notion of reliability is very narrow and there are some issues that depend critically on the context in which the data were collected and the correctness of assumptions that you make about how the world works.\nRelationships between pairs of traits can be studied in isolation only in special circumstances. In general, to get valid results it is necessary to study entire systems of traits simultaneously. Failure to do so can easily lead to conclusions that are grossly misleading.\nDescriptions of relationships are often subjective ‚Äì they depend on choices that you, the modeler, make. These choices are generally rooted in your own beliefs about how the world works, or the theories accepted as plausible within some community of inquiry.\nIf data are collected properly, you can get an indication of whether the data are consistent or inconsistent with your subjective beliefs or ‚Äì and this is important ‚Äì whether you don‚Äôt have enough data to tell either way.\nModels can be used to check out the sensitivity of your conclusions to different beliefs. People who disagree in their views of how the world works often may not be able to reconcile their differences based on data, but they will be able to decide objectively whether their own or the other party‚Äôs beliefs are reasonable given the data.\nNotwithstanding everything said above about the strong link between your prior, subjective beliefs and the conclusions you draw from data, by collecting data in a certain context ‚Äì experiments ‚Äì you can dramatically simplify the interpretation of the results. It‚Äôs actually possible to remove the dependence on identified subjective beliefs by intervening in the system under study experimentally.\nTypes of Models\nLet us look at the famous dataset pertaining to Francis Galton‚Äôs work on the heights of children and the heights of their parents. We can create 4 kinds of models based on the types of variables in that dataset.\n\n\nVariables and Models\n\n\nOur method in this set of modules is to take the modern view that all these models can be viewed from a standpoint of the Linear Model, also called Linear Regression \\(y = \\beta_1 *x + \\beta_0\\) . For example, it is relatively straightforward to imagine Plot B (Quant vs Quant ) as an example of a Linear Model, with the dependent variable modelled as \\(y\\) and the independent one as \\(x\\). We will try to work up to the intuition that this model can be used to understand all the models in the Figure."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/10-Intro/index.html#degrees-of-freedom",
    "href": "content/courses/Analytics/Modelling/Modules/10-Intro/index.html#degrees-of-freedom",
    "title": "üß≠ Basics of Statistical Modeling",
    "section": "Degrees of Freedom",
    "text": "Degrees of Freedom\nTBD"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/10-Intro/index.html#effect-size",
    "href": "content/courses/Analytics/Modelling/Modules/10-Intro/index.html#effect-size",
    "title": "üß≠ Basics of Statistical Modeling",
    "section": "Effect Size",
    "text": "Effect Size\nAn effect size tells how the output of a model changes when a simple change is made to the input.\nEffect sizes always involve two variables: a response variable and a single explanatory variable. Effect size is always about a model. The model might have one explanatory variable or many explanatory variables. Each explanatory variable will have its own effect size, so a model with multiple explanatory variables will have multiple effect sizes."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/100-ThreeMeansOrMore/ANOVA.html#iconify-noto-v1-package-setting-up-r-packages",
    "href": "content/courses/Analytics/Modelling/Modules/100-ThreeMeansOrMore/ANOVA.html#iconify-noto-v1-package-setting-up-r-packages",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n Setting up R packages",
    "text": "Setting up R packages\n\nShow the Codeknitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(ggformula)\nlibrary(mosaic)\nlibrary(infer)"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/100-ThreeMeansOrMore/ANOVA.html#introduction",
    "href": "content/courses/Analytics/Modelling/Modules/100-ThreeMeansOrMore/ANOVA.html#introduction",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "Introduction",
    "text": "Introduction\nSuppose we have three sales strategies on our website, to sell a certain product, say men‚Äôs shirts. We have website interaction over several months. How do we know which strategy makes people buy the fastest ?\nIf there is a University course that is offered in parallel in three different classrooms, is there a difference between the average marks obtained by students in each of the classrooms?\nIn each case we have a set of observations in each category: Interaction Time vs Sales Strategy in the first example, and Student Marks vs Classroom in the second. We can take mean scores in each category and decide to compare them. How do we make the comparisons? One way would be to compare them pair-wise. But with this rapidly becomes intractable and also dangerous: with increasing number of groups, the number of mean-comparisons becomes very large \\(N\\choose 2\\) and with each comparison the possibility of some difference showing up, just by chance, increases! And we end up making the wrong inference and perhaps the wrong decision.\nThe trick is of course to make comparisons all at once and ANOVA is the technique that allows us to do just that. In this tutorial, we will compare the Hatching Time of frog spawn1, at three different lab temperatures. Our research question is:1¬†The ANOVA tutorial at Our Coding Club.\n\n\n\n\n\n\nResearch Question\n\n\n\nHow does frogspawn hatching time vary with temperature?\n\n\nRead the Data\nDownload the data by clicking the button below:\n Download the frogs data \n\nShow the Codefrogs_orig <- read_csv(\"data/frogs.csv\")\n\nRows: 60 Columns: 4\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\ndbl (4): Frogspawn sample id, Temperature13, Temperature18, Temperature25\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nShow the Codefrogs_orig\n\n\n\n  \n\n\n\nOur response variable is the hatching Time. Our explanatory variable is a factor, Temperature, with 3 levels: 13¬∞C, 18¬∞C and 25¬∞C. Different samples of spawn were subject to each of these temperatures respectively. The data is badly organized, with a separate column for each Temperature, with NA entries since not all samples of spawn could be subject to all temperatures. Hence, we should pivot_longer() this data, so all Time readings are in one column, and also convert the Temperature into a factor:\n\nShow the Codefrogs_long <- frogs_orig %>% \n  pivot_longer(., cols = starts_with(\"Temperature\"),\n               cols_vary = \"fastest\", # new in pivot_longer\n               names_to = \"Temp\",\n               values_to = \"Time\") %>% \n  drop_na() %>% \n  \n  # knock off the unnecessary \"Temperature\" word everywhere\n  separate_wider_regex(cols = Temp,\n                       patterns = c(\"Temperature\", \n                                     TempFac = \"\\\\d+\"), \n                       cols_remove = TRUE) %>% \n  \n  # Convert Temp into TempFac, a 3-level factor\n  mutate(TempFac = factor(x = TempFac,\n                              levels = c(13,18,25), \n                              labels = c(\"13\", \"18\", \"25\"))) %>% \n  rename(\"Id\" = `Frogspawn sample id`)\n\nfrogs_long\n\n\n\n  \n\n\n\n\nShow the Codefrogs_long %>% count(TempFac)\n\n\n\n  \n\n\n\nSo we have 20 samples for Hatching Time per TempFac setting."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/100-ThreeMeansOrMore/ANOVA.html#eda",
    "href": "content/courses/Analytics/Modelling/Modules/100-ThreeMeansOrMore/ANOVA.html#eda",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "EDA",
    "text": "EDA\nLet us set a plot theme:\n\nShow the Code# Data visualisation\n\ntheme_frogs <- function(){  # Creating a function\n  theme_classic() +  # Using pre-defined theme as base\n  theme(axis.text.x = element_text(size = 12, face = \"bold\"),  # Customizing axes text      \n        axis.text.y = element_text(size = 12, face = \"bold\"),\n        axis.title = element_text(size = 14, face = \"bold\"),  # Customizing axis title\n        panel.grid = element_blank(),  # Taking off the default grid\n        plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), units = , \"cm\"),\n        legend.text = element_text(size = 12, face = \"italic\"),  # Customizing legend text\n        legend.title = element_text(size = 12, face = \"bold\"),  # Customizing legend title\n        legend.position = \"right\",  # Customizing legend position\n        plot.caption = element_text(size = 12))  # Customizing plot caption\n}                                                                              \n\n\nLet us plot some histograms of Hatching Time:\n\nShow the Codegf_histogram(data = frogs_long, \n             ~ Time, \n             fill = ~ TempFac,\n             stat = \"count\") %>% \n  gf_vline(xintercept = ~ mean(Time)) %>% \n  gf_labs(x = \"Hatching Time\") %>% \n  gf_theme(theme = theme_frogs()) %>% \n  gf_theme(guides(fill = guide_legend(title = \"Temperature level (¬∞C)\")))\n\n\n\n\nWe should also look at boxplots:\n\nShow the Codegf_boxplot(data = frogs_long, \n             Time ~ TempFac, \n             fill = ~ TempFac) %>% \n  gf_vline(xintercept = ~ mean(Time)) %>% \n  gf_labs(x = \"Hatching Time\") %>% \n  gf_theme(theme = theme_frogs()) %>% \n  gf_theme(guides(fill = guide_legend(title = \"Temperature level (¬∞C)\")))"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/100-ThreeMeansOrMore/ANOVA.html#anova",
    "href": "content/courses/Analytics/Modelling/Modules/100-ThreeMeansOrMore/ANOVA.html#anova",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "ANOVA",
    "text": "ANOVA\nWe will first execute the ANOVA test with code and evaluate the results. Then we will do an intuitive walk through of the process and finally, hand-calculate entire analysis for clear understanding.\n\n\nANOVA Test with Code\nANOVA Intuitive\nANOVA Manually Demonstrated (Apologies to Spinoza)\n\n\n\nR offers a very simple command to execute an ANOVA test: Note the familiar formula of stating the variables:\n\nShow the Codefrogs_anova <- aov(Time ~ TempFac, data = frogs_long)\nfrogs_anova %>% broom::tidy()\n\n\n\n  \n\n\nShow the Codesummary(frogs_anova)\n\n            Df Sum Sq Mean Sq F value Pr(>F)    \nTempFac      2 1020.9   510.5   385.9 <2e-16 ***\nResiduals   57   75.4     1.3                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe effect of Temperature on Hatching time is significant, with a p-value of \\(<2e-16\\). The F-statistic for the ANOVA test is given by \\(385.9\\), which is very high.\nClearly Temperature has a very significant effect on the hatching Time.\nTo find which specific value of TempFac has the most effect will require pairwise comparison of the group means, using a standard t-test. The confidence level for such repeated comparisons will need what is called Bonferroni correction2 to prevent us from detecting a significant (pair-wise) difference simply by chance. To do this we take \\(\\alpha\\) the confidence level used and divide it by \\(K\\), the number of pair-wise comparisons we intend to make. So using an \\(\\alpha = 0.05\\) for ANOVA, the pairwise comparisons in our current data will have to use \\(\\alpha/3 = 0.0166\\) as the confidence level. We will discuss this more in the section titled ‚ÄúANOVA Intuitive‚Äù.2¬†https://www.openintro.org/go/?id=anova-supplement&referrer=/book/ahss/index.php\n\n\nAll that is very well, but what is happening under the hood of the aov() command?\nConsider a data set with a single Quant and a single Qual variable. The Qual variable has two levels, the Quant data has 20 observations per Qual level.\n\nShow the Codelibrary(patchwork)\nggplot2::theme_set(theme_classic())\ndata = tibble(index = 1:40,\n              qual = c(rep(x = \"A\", 20),rep(x = \"B\", 20)),\n              quant = c(rnorm(n = 20, mean = 0, sd = 2), \n                        rnorm(n = 20, mean = 10, sd = 2)))\ndata\n\n\n\n  \n\n\nShow the Codeoverall_mean <- data %>%\n             summarise(overall_mean = mean(quant))\n#overall_mean\n\ngrouped_means <- data %>%\n  group_by(qual) %>% \n             summarise(grouped_means = mean(quant))\n#grouped_means\n\np1 <- gf_point(quant ~ index, \n               color = ~ qual,\n         data = data) %>% \n  gf_hline(yintercept =  ~ overall_mean,\n             data = overall_mean) %>% \n  gf_segment(data = data, \n             color = \"black\",\n             overall_mean$overall_mean + quant ~ index + index)\n\np2 <- gf_point(quant ~ index, \n         group = ~ qual, \n         colour = ~ qual, \n         data = data) %>% \n  \n  gf_hline(yintercept = ~ mean, \n           colour = ~ qual,\n           data = data %>% \n             group_by(qual) %>% \n             summarise(mean = mean(quant))) %>% \n  gf_segment(data = data %>% filter(qual == \"A\"),\n             grouped_means$grouped_means[1] + quant ~ index + index\n             ) %>% \n    gf_segment(data = data %>% filter(qual == \"B\"),\n             grouped_means$grouped_means[2] + quant ~ index + index\n             ) \n\np1 + p2 + patchwork::plot_annotation(tag_levels = c(\"A\", \"B\"))\n\n\n\n\nIn Fig A, the horizontal black line is the overall mean of quant, denoted as \\(\\mu_{tot}\\). The vertical black lines to the points show the departures of each point from this overall mean. The sum of squares of these vertical black lines in Fig A is called the Total Sum of Squares (SST).\n\\[\nSST = \\Sigma (y - \\mu_{tot})^2\n\\tag{1}\\]\nIf there are \\(k\\) levels in qual and \\(n\\) observations \\(y_ n\\) for each level, we can also write:\n\\[\nSST =\n\\sum_{i=1}^{kn}y_i^2 - \\frac{ \\left( \\sum_{i=1}^{kn}\ny_i \\right)^2}{kn}\n\\]\nIn Fig B, the horizontal green and red lines are the means of the individual groups, respectively \\(\\mu_A\\) and \\(\\mu_B\\). The green and red vertical lines are the departures, or errors, of each point from its own group-mean. The sum of the squares of the green and red lines is called the Total Error Sum of Squares (SSE).\n\\[\nSSE = \\Sigma [(y - \\mu_i)^2 +... (y - \\mu_k)^2]\n\\tag{2}\\]\nIf the \\(\\mu_A\\) and \\(\\mu_B\\) are different from \\(\\mu_{tot}\\), then what would be the relationship between \\(SSA\\) and \\(SSE\\)? Clearly if the all means are identical then the \\(SST\\) and \\(SSE\\) are equal, since the two coloured lines would be in the same place as the black line. It should be clear that if \\(\\mu_A\\) and \\(\\mu_B\\) are different from the overall mean, then \\(SSE < SST\\).\nSo, when we desire to detect if the two groups are different in their means, we take the difference:\n\\[\nSSA = SST - SSE\n\\tag{3}\\]\n\\(SSA\\) is called the Treatment Sum of Squares and is a measure the differences in means of observations at different levels of the factor.\n\\(SSA\\) can also directly be re-written in a very symmetric fashion to Equation¬†1 as:\n\\[\nSSA = \\frac{\\sum_{i=1}^{k} \\left( \\sum_{j=1}^{n}y_{ij}\\right)^2 }{n} - \\frac{\\left( \\sum_{i=1}^{kn}y_i \\right)^2}{kn}\n\\]\nNote that in the first term, we are calculating sums of observations within each group in the inner summation, which is like a per-group mean(without the division), and squaring these (square means). The outer summation takes the sum of square-means of these undivided summations and divides by \\(n\\), giving us an average squared mean! Whew!\nComparing \\(SSA\\) and \\(SSE\\) now provides us with a method that helps us decide whether these means are different. The comparison is of course in the form of a ratio, the F-statistic.\nSince each of these measures uses a different sets of observations, the comparison is done after scaling each of \\(SSA\\) and \\(SSE\\) by the number of observations influencing them. This means that we need to divide each of \\(SSA\\) and \\(SSE\\) by their degrees of freedom, which gives us a ratio of variances:\n\\[\nF_{stat} = \\frac{SSA / df_{SSA}}{SSE / df_{SSE}}\n\\]\nwhere \\(df_{SSA}\\) and \\(df_{SSE}\\) are respectively the degrees of freedom in \\(SSA\\) and \\(SSE\\). And so we are in effect deciding if means are significantly different by analyzing (a ratio of) variances! Hence AN-alysis O-f VA-riance, ANOVA.\nIn order to find which of the means is significantly different from others, we need to make a pair-wise comparison of the means, applying the Bonferroni correction as stated before.\n\n\nLet us also hand-calculate the numbers so we know what the ANOVA test is doing.\nHere is the SST:\n\nShow the Code# Calculate overall sum squares SST\n\nfrogs_overall <- frogs_long %>% \n  summarise(mean_time = mean(Time), \n            # Overall mean across all readings\n            # The Black Line\n            \n            SST = sum((Time - mean_time)^2),\n            \n            # always use n = n() at the end of a summarise!!\n            n = n())\nfrogs_overall\n\n\n\n  \n\n\nShow the CodeSST <- frogs_overall$SST\nSST\n\n[1] 1096.333\n\n\nAnd here is our plot to understand the SST:\n\nShow the Codefrogs_plot <- frogs_long %>% \n  arrange(TempFac) %>% \n           rowid_to_column(var = \"index\")\n\nfrogs_mean <- frogs_long %>%\n             summarise(overall_mean = mean(Time))\n\nfrogs_grouped_means <- frogs_long %>%\n  group_by(TempFac) %>% \n             summarise(grouped_means = mean(Time))\n\ngf_point(Time ~ index, \n         color = ~ TempFac,\n         data = frogs_plot) %>% \n  \n gf_hline(yintercept =  ~ overall_mean,\n             data = frogs_mean) %>% \n  \n  gf_segment(data = frogs_plot, \n             color = \"black\",\n             frogs_mean$overall_mean + Time ~ index + index) %>% \n  gf_theme(theme = theme_frogs)\n\n\n\n\nHere is the SSE:\n\nShow the Code# First calculate sums of square errors *within* each group\n# with respect to individual group means\n# Then sum them together\n# Be careful of unbalanced groups!\n# Not all groups may have the same number of observations\n\nfrogs_within_groups <- frogs_long %>% \n  group_by(TempFac) %>% \n   summarise(mean_time = mean(Time),\n            group_error_squares = sum((Time - mean_time)^2),\n            n = n())\nfrogs_within_groups\n\n\n\n  \n\n\nShow the Codefrogs_SSE <- frogs_within_groups %>% \n  summarise(SSE = sum(group_error_squares))\n\nSSE <- frogs_SSE$SSE\nSSE\n\n[1] 75.4\n\n\nWhat does the SSE look like in a graph?\n\nShow the Codefrogs_plot <- frogs_long %>% \n  arrange(TempFac) %>% \n           rowid_to_column(var = \"index\")\n\nfrogs_mean <- frogs_long %>%\n             summarise(overall_mean = mean(Time))\n\nfrogs_grouped_means <- frogs_long %>%\n  group_by(TempFac) %>% \n             summarise(grouped_means = mean(Time))\n\ngf_point(Time ~ index, \n         group = ~ TempFac, \n         colour = ~ TempFac, \n         data = frogs_plot) %>% \n  \n  gf_hline(yintercept = ~ grouped_means, \n           colour = ~ TempFac,\n           data = frogs_grouped_means) %>% \n  \n  gf_segment(data = frogs_plot %>% filter(TempFac == 13 ),\n             frogs_grouped_means$grouped_means[1] + Time ~ index + index\n             ) %>% \n  \n  gf_segment(data = frogs_plot %>% filter(TempFac == 18),\n             frogs_grouped_means$grouped_means[2] + Time ~ index + index\n             ) %>% \n  \n  gf_segment(data = frogs_plot %>% filter(TempFac == 25),\n             frogs_grouped_means$grouped_means[3] + Time ~ index + index\n             ) %>% \n  \n  gf_theme(theme = theme_frogs)\n\n\n\n\nOK, we have \\(SST\\) and \\(SSE\\), so let‚Äôs get \\(SSA\\):\n\nShow the CodeSST\n\n[1] 1096.333\n\nShow the CodeSSE\n\n[1] 75.4\n\nShow the CodeSSA <- SST - SSE\nSSA\n\n[1] 1020.933\n\n\nWe have \\(SST = 1096\\), \\(SSE = 75.4\\) and therefore \\(SSA = 1020.9\\).\nIn order to calculate the F-Statistic, we need to compute the variances, using these sum of squares. We obtain variances from these Sums of Squares by dividing by their Degrees of Freedom:\n\\[\nF_{stat} = \\frac{SSA / df_{SSA}}{SSE / df_{SSE}}\n\\]\nwhere \\(df_{SSA}\\) and \\(df_{SSE}\\) are respectively the degrees of freedom in SSA and SSE.\nLet us calculate these Degrees of Freedom. With \\(k = 3\\) levels in the factor TempFac, and \\(n = 20\\) points per level, \\(SST\\) clearly has degree of freedom \\(kn-1 = 59\\), since it uses all observations but loses one degree to calculate the global mean. (If each level did not have the same number of points \\(n\\), we simply take all observations less one as the degrees of freedom for \\(SST\\)).\n\\(SSE\\) has \\(k*(n-1)\\) as degrees of freedom, since each of the \\(k\\) groups there are \\(n\\) observations and each group loses one degree to calculate its own group mean.\nAnd therefore, \\(SSA\\) being their difference, has \\(k-1\\) degrees of freedom.\nWe can still calculate these in R, for the sake of method and clarity:\n\nShow the Code# Error Sum of Squares SSE\ndf_SSE <- frogs_long %>% \n  \n# Takes into account \"unbalanced\" situations\n  group_by(TempFac) %>% \n  summarise(per_group_df_SSE = n() - 1) %>% \n  summarise(df_SSE = sum(per_group_df_SSE)) %>% \n  \n# Convert df_SSE from tibble to a scalar\n  as.integer()\n\n\n## Overall Sum of Squares SST\ndf_SST <- frogs_long %>% \n  summarise(df_SST = n() - 1) %>% \n  as.integer()\n\n\n# Treatment Sum of Squares SSA\nk <- length(unique(frogs_long$TempFac))\ndf_SSA <- k - 1\n\n\nThe degrees of freedom for the quantities are:\n\nShow the Codedf_SST\n\n[1] 59\n\nShow the Codedf_SSE\n\n[1] 57\n\nShow the Codedf_SSA\n\n[1] 2\n\n\nNow we are ready to compute the F-statistic:\n\nShow the Code# Finally F_Stat!\n## Combine the sum-square_error for each level of the factor\n# weighted by degrees of freedom per level\n# Which are of course equal here ;-D\n\nMSE <- frogs_within_groups %>% \n  summarise(mean_square_error = sum(group_error_squares/df_SSE)) %>% \n  as.numeric()\nMSE\n\n[1] 1.322807\n\nShow the CodeMSA <- SSA/df_SSA # This is OK\nMSA\n\n[1] 510.4667\n\nShow the CodeF_stat <- MSA/MSE\nF_stat\n\n[1] 385.8966\n\n\nThe F-stat is compared with a critical value of the F-statistic, which is computes using the formula for the f-distribution in R. As with our hypothesis tests, we set the significance level to 0.95, and quote the two relevant degrees of freedom as parameters to qf() which computes the critical F value as a quartile:\n\nShow the CodeF_crit <-  qf(p = 0.95,     # Significance level is 5%\n              df1 = df_SSA, # Numerator degrees of freedom \n              df2 = df_SSE) # Denominator degrees of freedom\nF_crit\n\n[1] 3.158843\n\nShow the CodeF_stat\n\n[1] 385.8966\n\n\nThe F_crit value can also be seen in a plot3:3¬†https://rpubs.com/kerkhoffa/ANOVAbasic\n\nShow the Codemosaic::pdist(dist = \"f\",\n              q = 3.158843, \n              df1=df_SSA, df2=df_SSE)\n\n\n\n\n[1] 0.95\n\n\nAny value of F more than the F_crit occurs with smaller probability than 0,05. Our F_stat is much higher than F_crit, by orders of magnitude! And so we can say with confidence that Temperature has a significant effect on spawn Time.\nAnd that is how ANOVA computes!"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/100-ThreeMeansOrMore/ANOVA.html#checking-anova-assumptions",
    "href": "content/courses/Analytics/Modelling/Modules/100-ThreeMeansOrMore/ANOVA.html#checking-anova-assumptions",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "Checking ANOVA Assumptions",
    "text": "Checking ANOVA Assumptions\nANOVA makes 3 fundamental assumptions:\n\nData are normally distributed.\nVariances are homogeneous.\nObservations are independent.\n\nWe can check these using checks and graphs:\nChecks for Normality\nThe shapiro.wilk test tests if a vector of numeric data is normally distributed and rejects the hypothesis of normality when the p-value is less than or equal to 0.05.¬†\n\nShow the Codeshapiro.test(x = frogs_long$Time)\n\n\n    Shapiro-Wilk normality test\n\ndata:  frogs_long$Time\nW = 0.92752, p-value = 0.001561\n\n\nThe p-value is very low and we cannot reject the (alternative) hypothesis that the data is not normal. Clearly however, the overall data cannot be normally distributed,especially if there is a strong effect of the TempFac factor! We therefore need to look for normality at each level of the factor:\n\nShow the Codefrogs_grouped <- frogs_long %>% \n  group_by(TempFac) %>% \n  nest(.key = \"list\") \n\nfrogs_grouped %>% \n  pluck(\"list\", 1) %>% # naming the nested column \"list\"\n  select(Time) %>% \n  as_vector() %>% \n  shapiro.test(.)\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.88954, p-value = 0.02638\n\nShow the Code# OK now we are set for group-wise Shapiro-Wilk testing:\n\nfrogs_grouped %>% \n  mutate(shaptest = \n           purrr::map(.x = list, # Column name is \"list\"\n                      .f = \\(.x) select(.data = .x, \n                                        Time) %>% \n                                 as_vector() %>% \n                                 shapiro.test(.)),\n         \n         params = map(.x = shaptest,\n                      .f = \\(.x) broom::tidy(.x))) %>% \n  \n  select(TempFac, params) %>% \n  unnest(cols = params)\n\n\n\n  \n\n\n\nThe shapiro.wilk test makes a NULL Hypothesis that the data are normally distributed and estimates the probability that this could have happened by chance. Except for TempFac = 18 the p-values are less than 0.05 and we can reject the NULL hypothesis that each of these is normally distributed. Perhaps this is a sign that we need more than 20 samples per factor level.\nWe can also check the residuals post-model:\n\nShow the Codefrogs_anova$residuals %>% \n  as_tibble() %>% \n  gf_histogram(~ value,data = .) %>% \n  gf_theme(theme = theme_frogs())\n\n\n\nShow the Codefrogs_anova$residuals %>%\n  as_tibble() %>% \n  gf_qq(~ value, data = .) %>% \n  gf_qqstep() %>% \n  gf_qqline() %>% \n  gf_theme(theme = theme_frogs())\n\nWarning: The following aesthetics were dropped during statistical transformation: sample\n‚Ñπ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\n‚Ñπ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\nShow the Codeshapiro.test(frogs_anova$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  frogs_anova$residuals\nW = 0.94814, p-value = 0.01275\n\n\nUnsurprisingly, the residuals are also not normally distributed either.\nCheck for Similar Variance\nResponse data with different variances at different levels of an explanatory variable are said to exhibit heteroscedasticity. This violates one of the assumptions of ANOVA.\nTo check if the Time readings are similar in variance across levels of TempFac, we can use the Levene Test, or since our per-group observations are not normally distributed, a non-parametric rank-based Fligner-Killeen test. The NULL hypothesis is that the data are with similar variances. The tests assess how probable this is with the given data assuming this NULL hypothesis:\n\nShow the Codefrogs_long %>% \n  group_by(TempFac) %>% \n  summarise(variance = var(Time))\n\n\n\n  \n\n\nShow the Code# Not too different...OK on with the test\nfligner.test(Time ~ TempFac, data = frogs_long)\n\n\n    Fligner-Killeen test of homogeneity of variances\n\ndata:  Time by TempFac\nFligner-Killeen:med chi-squared = 0.53898, df = 2, p-value = 0.7638\n\nShow the CodeDescTools::LeveneTest(Time ~ TempFac, data = frogs_long)\n\n\n\n  \n\n\n\nIt seems that there is no cause for concern here; the p-value is quite high and therefore we do not reject the NULL hypothesis that the data have similar variances.\nIndependent Observations\nThis is an experiment design concern; the way the data is gathered must be specified such that data for each level of the factors ( factor combinations if there are more than one) should be independent."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/100-ThreeMeansOrMore/ANOVA.html#effect-size",
    "href": "content/courses/Analytics/Modelling/Modules/100-ThreeMeansOrMore/ANOVA.html#effect-size",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "Effect Size",
    "text": "Effect Size\nThe simplest way to find the actual effect sizes detected by an ANOVA test is to use (paradoxically) the summary.lm() command:\n\nShow the Codesummary.lm(frogs_anova) %>% broom::tidy()\n\n\n\n  \n\n\n\nIt may take a bit of effort to understand this. First the TempFac is arranged in order of levels, and the mean at the \\(`TempFac` = 13\\) is titled intercept. That is \\(26.3\\). The other two means for levels \\(18\\) and \\(25\\) are stated as differences from this intercept, \\(-5.3\\) and \\(-10.1\\) respectively. The p.value for all these effect sizes is well below the desired confidence level of \\(0.05\\).\nWe can easily plot bar-chart with error bars for the effect size:\n\nShow the Codetidy_anova <- frogs_anova %>% \n  summary.lm() %>% \n  broom::tidy()\ntidy_anova\n\n\n\n  \n\n\nShow the Code# Merging group averages with `std.error`\nfrogs_long %>% \n  group_by(TempFac) %>% \n  summarise(mean = mean(Time)) %>% \n  cbind(std.error = tidy_anova$std.error) %>% \n  mutate(hi = mean + std.error,\n         lo = mean - std.error) %>% \n  gf_col(data = ., mean ~ TempFac, \n         fill = \"grey\", \n         color = \"black\") %>% \n  gf_point(mean ~ TempFac, \n           color = \"red\", \n           size = 4) %>% \n    gf_errorbar(hi + lo ~ TempFac,\n                color = \"blue\",\n                linetype = \"dashed\")"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/100-ThreeMeansOrMore/ANOVA.html#anova-using-permutation-tests",
    "href": "content/courses/Analytics/Modelling/Modules/100-ThreeMeansOrMore/ANOVA.html#anova-using-permutation-tests",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "ANOVA using Permutation Tests",
    "text": "ANOVA using Permutation Tests\nWe wish to establish the significance of the effect size due to each of the levels in TempFac. From the normality tests conducted earlier we see that except at one level of TempFac, the times are are not normally distributed. Hence we opt for a Permutation Test to check for significance of effect.\nAs remarked in Ernst4, the non-parametric permutation test can be both exact and also intuitively easier for students to grasp. Permutations are easily executed in R, using packages such as mosaic5.4¬†Ernst, Michael D. 2004. ‚ÄúPermutation Methods: A Basis for Exact Inference.‚Äù Statistical Science 19 (4): 676‚Äì85. doi:10.1214/088342304000000396.5¬†Pruim R, Kaplan DT, Horton NJ (2017). ‚ÄúThe mosaic Package: Helping Students to ‚ÄòThink with Data‚Äô Using R.‚Äù The R Journal, 9(1), 77‚Äì102. https://journal.r-project.org/archive/2017/RJ-2017-024/index.html.\nWe proceed with a Permutation Test for TempFac. We shuffle the levels (13, 18, 25) randomly between the Times and repeat the ANOVA test each time and calculate the F-statistic. The Null distribution is the distribution of the F-statistic over the many permutations and the p-value is given by the proportion of times the F-statistic equals or exceeds that observed.\nWe will use mosaic and also try with infer.\n\n\nUsing mosaic\nUsing infer\n\n\n\n\nShow the Codeobs_F_stat <- frogs_anova %>% \n  broom::tidy() %>% \n  select(statistic)\nobserved_mosaic <- obs_F_stat$statistic[1]\n\nnull_dist_mosaic <- do(10000) * aov(Time ~ shuffle(TempFac), data = frogs_long)\nnull_dist_mosaic %>% head()\n\n\n\n  \n\n\nShow the Codenull_dist_mosaic %>% drop_na() %>% \n  select(F) %>% \n  gf_histogram(data = ., ~ F, \n               fill = ~ F >= observed_mosaic,\n               title = \"Null Distribution of ANOVA F-statistic\" )\n\n\n\n\nThe Null distribution of the F_statistic under permutation shows it never crosses the real-world observed value, testifying the strength of the effect of TempFac on hatching Time. And the p-value is:\n\nShow the Codep_value <- mean(null_dist_mosaic$F >= observed_mosaic, na.rm = TRUE)\np_value\n\n[1] 0\n\n\n\n\nWe calculate the observed F-stat with infer:\n\nShow the Codeobserved_infer <- frogs_long %>% \n  specify(Time ~ TempFac) %>% \n  hypothesise(null = \"independence\") %>% \n  calculate(stat = \"F\")\nobserved_infer\n\n\n\n  \n\n\n\nWe see that the observed F-Statistic is of course \\(385.8966\\) as before. Now we use infer to generate a NULL distribution using permutation of the factor TempFac:\n\nShow the Codenull_dist_infer <- frogs_long %>% \n  specify(Time ~ TempFac) %>% \n  hypothesise(null = \"independence\") %>% \n  generate(reps = 1000, type = \"permute\") %>% \n  calculate(stat = \"F\")\n\nhead(null_dist_infer)\n\n\n\n  \n\n\nShow the Codenull_dist_infer %>% \n  visualise()\n\n\n\n\nAs seen, the infer based permutation test also shows that the permutationally generated F-statistics are nowhere near that which was observed. The effect of TempFac is very strong."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/100-ThreeMeansOrMore/ANOVA.html#references",
    "href": "content/courses/Analytics/Modelling/Modules/100-ThreeMeansOrMore/ANOVA.html#references",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "References",
    "text": "References\n\nMichael Crawley, The R Book, second edition, 2013. Chapter 11.\nOpen Intro Statistics. ANOVA Calculations\nDavid C Howell, Permutation Tests for Factorial ANOVA Designs\nMarti Anderson, Permutation tests for univariate or multivariate analysis of variance and regression"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/150-Bootstrap/files/bootstrap.html",
    "href": "content/courses/Analytics/Modelling/Modules/150-Bootstrap/files/bootstrap.html",
    "title": "Bootstrap Case Studies",
    "section": "",
    "text": "Example 5.2\n\nShow the Codemy.sample <- rgamma(16, 1, 1/2)\n\nN <- 10^5\nmy.boot <- numeric(N)\nfor (i in 1:N)\n {\n  x <- sample(my.sample, 16, replace = TRUE)  #draw resample\n  my.boot[i] <- mean(x)                     #compute mean, store in my.boot\n  }\n\nggplot() + geom_histogram(aes(my.boot), bins=15)\n\n\n\nShow the Codemean(my.boot)  #mean\n\n[1] 2.679718\n\nShow the Codesd(my.boot)    #bootstrap SE\n\n[1] 0.7833044\n\n\nExample 5.3\nArsenic in wells in Bangladesh\n\nShow the CodeBangladesh <- read.csv(\"../../../../../../materials/data/resampling/Bangladesh.csv\")\n\nggplot(Bangladesh, aes(Arsenic)) + geom_histogram(bins = 15)\n\n\n\nShow the Codeggplot(Bangladesh, aes(sample = Arsenic)) + stat_qq() + stat_qq_line()\n\n\n\nShow the CodeArsenic <- pull(Bangladesh, Arsenic)\n#Alternatively\n#Arsenic <- Bangladesh$Arsenic\n\nn <- length(Arsenic)\nN <- 10^4\n\narsenic.mean <- numeric(N)\n\nfor (i in 1:N)\n{\n   x <- sample(Arsenic, n, replace = TRUE)\n   arsenic.mean[i] <- mean(x)\n}\n\nggplot() + geom_histogram(aes(arsenic.mean), bins = 15) + \n  labs(title=\"Bootstrap distribution of means\") + \n  geom_vline(xintercept = mean(Arsenic), colour = \"blue\")\n\n\n\nShow the Codedf <- data.frame(x = arsenic.mean)\nggplot(df, aes(sample = x)) + stat_qq() + stat_qq_line()\n\n\n\nShow the Codemean(arsenic.mean)                 #bootstrap mean\n\n[1] 125.4128\n\nShow the Codemean(arsenic.mean) - mean(Arsenic) #bias\n\n[1] 0.09286144\n\nShow the Codesd(arsenic.mean)                   #bootstrap SE\n\n[1] 18.20276\n\nShow the Codesum(arsenic.mean > 161.3224)/N\n\n[1] 0.0333\n\nShow the Codesum(arsenic.mean < 89.75262)/N\n\n[1] 0.016\n\n\nExample 5.4 Skateboard\n\nShow the CodeSkateboard <- read.csv(\"../../../../../../materials/data/resampling/Skateboard.csv\")\n\ntestF <- Skateboard %>% filter(Experimenter == \"Female\") %>% pull(Testosterone)\ntestM <- Skateboard %>% filter(Experimenter == \"Male\") %>% pull(Testosterone)\n\nobserved <- mean(testF) - mean(testM)\n\nnf <- length(testF)\nnm <- length(testM)\n\nN <- 10^4\n\nTestMean <- numeric(N)\n\nfor (i in 1:N)\n{\n  sampleF <- sample(testF, nf, replace = TRUE)\n  sampleM <- sample(testM, nm, replace = TRUE)\n  TestMean[i] <- mean(sampleF) - mean(sampleM)\n}\n\ndf <- data.frame(TestMean)\nggplot(df) + geom_histogram(aes(TestMean), bins = 15) + \n  labs(title = \"Bootstrap distribution of difference in means\", xlab = \"means\") +\n  geom_vline(xintercept = observed, colour = \"blue\")\n\n\n\nShow the Codeggplot(df, aes(sample = TestMean))  + stat_qq() + stat_qq_line()\n\n\n\nShow the Codemean(testF) - mean(testM)\n\n[1] 83.0692\n\nShow the Codemean(TestMean)\n\n[1] 83.12008\n\nShow the Codesd(TestMean)\n\n[1] 29.13951\n\nShow the Codequantile(TestMean,c(0.025,0.975))\n\n     2.5%     97.5% \n 25.12621 139.64168 \n\nShow the Codemean(TestMean)- observed  #bias\n\n[1] 0.05088193\n\n\nPermutation test for Skateboard means\n\nShow the CodetestAll <- pull(Skateboard, Testosterone)\n#testAll <- Skateboard$Testosterone\n\nN <- 10^4 - 1  #set number of times to repeat this process\n\n#set.seed(99)\nresult <- numeric(N) # space to save the random differences\nfor(i in 1:N)\n  {\n  index <- sample(71, size = nf, replace = FALSE) #sample of numbers from 1:71\n  result[i] <- mean(testAll[index]) - mean(testAll[-index])\n}\n\n(sum(result >= observed)+1)/(N + 1)  #P-value\n\n[1] 0.0062\n\nShow the Codeggplot() + geom_histogram(aes(result), bins = 15) + \n  labs(x = \"xbar1-xbar2\", title=\"Permutation distribution for testosterone levels\") +\n  geom_vline(xintercept = observed, colour = \"blue\")\n\n\n\nShow the Codedf <- data.frame(result)\nggplot(df, aes(sample = result)) + stat_qq() + stat_qq_line()\n\n\n\n\nSection 5.4.1 Matched pairs for Diving data\n\nShow the CodeDiving2017 <- read.csv(\"../../../../../../materials/data/resampling/Diving2017.csv\")\nDiff <- Diving2017 %>% mutate(Diff = Final - Semifinal) %>% pull(Diff)\n#alternatively\n#Diff <- Diving2017$Final - Diving2017$Semifinal\nn <- length(Diff)\n\nN <- 10^5\nresult <- numeric(N)\n\nfor (i in 1:N)\n{\n  dive.sample <- sample(Diff, n, replace = TRUE)\n  result[i] <- mean(dive.sample)\n}\n\nggplot() + geom_histogram(aes(result), bins = 15)\n\n\n\nShow the Codequantile(result, c(0.025, 0.975))\n\n     2.5%     97.5% \n-6.495937 31.016771 \n\n\nExample 5.5 Verizon cont. Bootstrap means for the ILEC data and for the CLEC data\nBootstrap difference of means.\n\nShow the CodeVerizon <- read.csv(\"../../../../../../materials/data/resampling/Verizon.csv\")\n\nTime.ILEC <- Verizon %>% filter(Group == \"ILEC\") %>% pull(Time)\nTime.CLEC <- Verizon %>% filter(Group == \"CLEC\") %>% pull(Time)\n\nobserved <- mean(Time.ILEC) - mean(Time.CLEC)\n\nn.ILEC <- length(Time.ILEC)\nn.CLEC <- length(Time.CLEC)\n\nN <- 10^4\n\ntime.ILEC.boot <- numeric(N)\ntime.CLEC.boot <- numeric(N)\ntime.diff.mean <- numeric(N)\n\nset.seed(100)\nfor (i in 1:N)\n {\n  ILEC.sample <- sample(Time.ILEC, n.ILEC, replace = TRUE)\n  CLEC.sample <- sample(Time.CLEC, n.CLEC, replace = TRUE)\n  time.ILEC.boot[i] <- mean(ILEC.sample)\n  time.CLEC.boot[i] <- mean(CLEC.sample)\n  time.diff.mean[i] <- mean(ILEC.sample) - mean(CLEC.sample)\n}\n\n#bootstrap for ILEC\nggplot() + geom_histogram(aes(time.ILEC.boot), bins = 15) + \n  labs(title = \"Bootstrap distribution of ILEC means\", x = \"means\") + \n  geom_vline(xintercept = mean(Time.ILEC), colour = \"blue\") + \n  geom_vline(xintercept = mean(time.ILEC.boot), colour = \"red\", lty=2)\n\n\n\nShow the Codesummary(time.ILEC.boot)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  7.036   8.156   8.400   8.406   8.642   9.832 \n\nShow the Codedf <- data.frame(x = time.ILEC.boot)\nggplot(df, aes(sample = x)) + stat_qq() + stat_qq_line()\n\n\n\nShow the Code#bootstrap for CLEC\nggplot() + geom_histogram(aes(time.CLEC.boot), bins = 15) + \n  labs(title = \"Bootstrap distribution of CLEC means\", x = \"means\") + \n  geom_vline(xintercept = mean(Time.CLEC), colour = \"blue\") + \n  geom_vline(xintercept = mean(time.CLEC.boot), colour = \"red\", lty = 2)\n\n\n\nShow the Codedf <- data.frame(x = time.CLEC.boot)\nggplot(df, aes(sample = x)) + stat_qq() + stat_qq_line()\n\n\n\nShow the Code#Different in means\nggplot() + geom_histogram(aes(time.diff.mean), bins = 15) + \n  labs(title = \"Bootstrap distribution of difference in means\", x = \"means\") +\n  geom_vline(xintercept = mean(time.diff.mean), colour = \"blue\") + \n  geom_vline(xintercept = mean(observed), colour = \"red\", lty = 2)\n\n\n\nShow the Codedf <- data.frame(x = time.diff.mean)\nggplot(df, aes(sample = x)) + stat_qq() + stat_qq_line()\n\n\n\nShow the Codemean(time.diff.mean)\n\n[1] -8.096489\n\nShow the Codequantile(time.diff.mean, c(0.025, 0.975))\n\n      2.5%      97.5% \n-16.970068  -1.690859 \n\n\nSection 5.5 Verizon cont.\nBootstrap difference in trimmed means\n\nShow the CodeTime.ILEC <- Verizon %>% filter(Group == \"ILEC\") %>% pull(Time)\nTime.CLEC <- Verizon %>% filter(Group == \"CLEC\") %>% pull(Time)\nn.ILEC <- length(Time.ILEC)\nn.CLEC <- length(Time.CLEC)\n\nN <- 10^4\ntime.diff.trim <- numeric(N)\n\n#set.seed(100)\nfor (i in 1:N)\n{\n  x.ILEC <- sample(Time.ILEC, n.ILEC, replace = TRUE)\n  x.CLEC <- sample(Time.CLEC, n.CLEC, replace = TRUE)\n  time.diff.trim[i] <- mean(x.ILEC, trim = .25) - mean(x.CLEC, trim = .25)\n}\n\nggplot() + geom_histogram(aes(time.diff.trim), bins = 15) + \n  labs(x = \"difference in trimmed means\") + \n  geom_vline(xintercept = mean(time.diff.trim),colour = \"blue\") + \n  geom_vline(xintercept = mean(Time.ILEC, trim = .25) - mean(Time.CLEC, trim = .25), colour = \"red\", lty = 2)\n\n\n\nShow the Codedf <- data.frame(x = time.diff.trim)\nggplot(df, aes(sample = x)) + stat_qq() + stat_qq_line()\n\n\n\nShow the Codemean(time.diff.trim)\n\n[1] -10.32079\n\nShow the Codequantile(time.diff.trim, c(0.025,0.975))\n\n     2.5%     97.5% \n-15.47049  -4.97130 \n\n\nSection 5.5 Other statistics Verizon cont:\nBootstrap of the ratio of means\nTime.ILEC and Time.CLEC created above.\nn.ILEC, n.CLEC created above\n\nShow the CodeN <- 10^4\ntime.ratio.mean <- numeric(N)\n\n#set.seed(100)\nfor (i in 1:N)\n{\n  ILEC.sample <- sample(Time.ILEC, n.ILEC, replace = TRUE)\n  CLEC.sample <- sample(Time.CLEC, n.CLEC, replace = TRUE)\n  time.ratio.mean[i] <- mean(ILEC.sample)/mean(CLEC.sample)\n}\n\nggplot() + geom_histogram(aes(time.ratio.mean), bins = 12) + \n  labs(title = \"bootstrap distribution of ratio of means\", x = \"ratio of means\") +\n  geom_vline(xintercept = mean(time.ratio.mean), colour = \"red\", lty = 2) + \n  geom_vline(xintercept  = mean(Time.ILEC)/mean(Time.CLEC), col = \"blue\")\n\n\n\nShow the Codedf <- data.frame(x = time.ratio.mean)\nggplot(df, aes(sample = x)) + stat_qq() + stat_qq_line()\n\n\n\nShow the Codemean(time.ratio.mean)\n\n[1] 0.5429164\n\nShow the Codesd(time.ratio.mean)\n\n[1] 0.1354238\n\nShow the Codequantile(time.ratio.mean, c(0.025, 0.975))\n\n     2.5%     97.5% \n0.3283862 0.8517156 \n\n\nExample 5.7 Relative risk example\n\nShow the Codehighbp <- rep(c(1,0),c(55,3283))   #high blood pressure\nlowbp <- rep(c(1,0),c(21,2655))    #low blood pressure\n\nN <- 10^4\nboot.rr <- numeric(N)\nhigh.prop <- numeric(N)\nlow.prop <- numeric(N)\n\nfor (i in 1:N)\n{\n   x.high <- sample(highbp,3338, replace = TRUE)\n   x.low  <- sample(lowbp, 2676, replace = TRUE)\n   high.prop[i] <- sum(x.high)/3338\n   low.prop[i]  <- sum(x.low)/2676\n   boot.rr[i] <- high.prop[i]/low.prop[i]\n}\n\nci <- quantile(boot.rr, c(0.025, 0.975))\n\nggplot() + geom_histogram(aes(boot.rr), bins = 15) + \n  labs(title = \"Bootstrap distribution of relative risk\", x = \"relative risk\") +\n  geom_vline(aes(xintercept = mean(boot.rr), colour = \"mean of bootstrap\")) +\n  geom_vline(aes(xintercept = 2.12, colour=\"observed rr\"), lty = 2) + \n  scale_colour_manual(name=\"\", values = c(\"mean of bootstrap\"=\"blue\", \"observed rr\" = \"red\"))\n\n\n\nShow the Codetemp <- ifelse(high.prop < 1.31775*low.prop, 1, 0)\ntemp2 <- ifelse(high.prop > 3.687*low.prop, 1, 0)\ntemp3 <- temp + temp2\n\ndf <- data.frame(y=high.prop, x=low.prop, temp, temp2, temp3)\ndf1 <- df %>% filter(temp == 1)\ndf2 <- df %>% filter (temp2 == 1)\ndf3 <- df %>% filter(temp3 == 0)\n\nggplot(df, aes(x=x, y = y)) + \n  geom_point(data =df1, aes(x= x, y = y), colour = \"green\") + \n  geom_point(data = df2, aes(x = x, y = y), colour = \"green\") + \n  geom_point(data = df3, aes(x = x, y = y), colour = \"red\") + \n  geom_vline(aes(xintercept = mean(low.prop)), colour = \"red\") +\n  geom_hline(yintercept = mean(high.prop), colour = \"red\") + \n  geom_abline(aes(intercept = 0, slope = 2.12, colour = \"observed rr\"), lty = 2, lwd = 1) + \n  geom_abline(aes(intercept = 0, slope = ci[1], colour = \"bootstrap CI\"), lty = 2, lwd = 1) + \n  geom_abline(intercept = 0, slope = ci[2], colour = \"blue\", lty = 2, lwd = 1) +\n  scale_colour_manual(name=\"\", values=c(\"observed rr\"=\"black\", \"bootstrap CI\" = \"blue\")) +\n  labs(x = \"Proportion in low blood pressure group\", y = \"Proportion in high blood pressure group\")"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/150-Bootstrap/index.html#introduction",
    "href": "content/courses/Analytics/Modelling/Modules/150-Bootstrap/index.html#introduction",
    "title": "Bootstrap",
    "section": "Introduction",
    "text": "Introduction\n\n\nBootstrap Sampling"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/150-Bootstrap/index.html#datasets",
    "href": "content/courses/Analytics/Modelling/Modules/150-Bootstrap/index.html#datasets",
    "title": "Bootstrap",
    "section": "Datasets",
    "text": "Datasets"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/150-Bootstrap/index.html#workflow-in-orange",
    "href": "content/courses/Analytics/Modelling/Modules/150-Bootstrap/index.html#workflow-in-orange",
    "title": "Bootstrap",
    "section": "Workflow in Orange",
    "text": "Workflow in Orange"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/150-Bootstrap/index.html#workflow-in-radiant",
    "href": "content/courses/Analytics/Modelling/Modules/150-Bootstrap/index.html#workflow-in-radiant",
    "title": "Bootstrap",
    "section": "Workflow in Radiant",
    "text": "Workflow in Radiant"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/150-Bootstrap/index.html#workflow-in-r",
    "href": "content/courses/Analytics/Modelling/Modules/150-Bootstrap/index.html#workflow-in-r",
    "title": "Bootstrap",
    "section": "Workflow in R",
    "text": "Workflow in R"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/150-Bootstrap/index.html#conclusion",
    "href": "content/courses/Analytics/Modelling/Modules/150-Bootstrap/index.html#conclusion",
    "title": "Bootstrap",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/150-Bootstrap/index.html#references",
    "href": "content/courses/Analytics/Modelling/Modules/150-Bootstrap/index.html#references",
    "title": "Bootstrap",
    "section": "References",
    "text": "References\n\nhttps://openintro-ims.netlify.app/foundations-bootstrapping.html#foundations-bootstrapping\ns"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/20-SampProb/files/sampling.html",
    "href": "content/courses/Analytics/Modelling/Modules/20-SampProb/files/sampling.html",
    "title": "Sampling",
    "section": "",
    "text": "Continuing to treat the NHANES dataset as a population, We will try to replicate the process of sampling and CLT for another variable in the NHANES variable, AlcoholYear."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/20-SampProb/index.html",
    "href": "content/courses/Analytics/Modelling/Modules/20-SampProb/index.html",
    "title": "üé≤ Samples, Populations, Statistics and Inference",
    "section": "",
    "text": "R Tutorial¬†¬†\n  Orange Tutorial\n\n  Radiant Tutorial¬†\n  Datasets"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/20-SampProb/index.html#iconify-noto-v1-package-setting-up-r-packages",
    "href": "content/courses/Analytics/Modelling/Modules/20-SampProb/index.html#iconify-noto-v1-package-setting-up-r-packages",
    "title": "üé≤ Samples, Populations, Statistics and Inference",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nset.seed(123456)\n\nlibrary(tidyverse) # Data Processing in R\nlibrary(mosaic) # Our workhorse for stats, sampling\nlibrary(ggformula) # Formula interface for graphs\n\n# load the NHANES data library\nlibrary(NHANES)\n\nlibrary(infer) # tidy workflow for statistical inference\nlibrary(gt) # Create tidy tables to report data\nlibrary(cowplot) # ggplot themes and stacking of plots"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/20-SampProb/index.html#what-is-a-population",
    "href": "content/courses/Analytics/Modelling/Modules/20-SampProb/index.html#what-is-a-population",
    "title": "üé≤ Samples, Populations, Statistics and Inference",
    "section": "What is a Population?",
    "text": "What is a Population?\nA population is a collection of individuals or observations we are interested in. This is also commonly denoted as a study population. We mathematically denote the population‚Äôs size using upper-case N.\nA population parameter is some numerical summary about the population that is unknown but you wish you knew. For example, when this quantity is a mean like the average height of all Bangaloreans, the population parameter of interest is the population mean.\n\n\n\n\n\n\nImportant\n\n\n\nPopulations Parameters are usually indicated by Greek Letters.\n\n\nA census is an exhaustive enumeration or counting of all N individuals in the population. We do this in order to compute the population parameter‚Äôs value exactly. Of note is that as the number N of individuals in our population increases, conducting a census gets more expensive (in terms of time, energy, and money)."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/20-SampProb/index.html#what-is-a-sample",
    "href": "content/courses/Analytics/Modelling/Modules/20-SampProb/index.html#what-is-a-sample",
    "title": "üé≤ Samples, Populations, Statistics and Inference",
    "section": "What is a Sample?",
    "text": "What is a Sample?\nSampling is the act of collecting a sample from the population, which we generally do when we can‚Äôt perform a census. We mathematically denote the sample size using lower case n, as opposed to upper case N which denotes the population‚Äôs size. Typically the sample size n is much smaller than the population size N. Thus sampling is a much cheaper alternative than performing a census.\nA sample statistic, also known as a point estimate, is a summary statistic like a mean or standard deviation that is computed from a sample."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/20-SampProb/index.html#why-do-we-sample",
    "href": "content/courses/Analytics/Modelling/Modules/20-SampProb/index.html#why-do-we-sample",
    "title": "üé≤ Samples, Populations, Statistics and Inference",
    "section": "Why do we sample?",
    "text": "Why do we sample?\nBecause we cannot conduct a census ( not always ) ‚Äî and sometimes we won‚Äôt even know how big the population is ‚Äî we take samples. And we still want to do useful work for/with the population, after estimating its parameters, an act of generalizing from sample to population. So the question is, can we estimate useful parameters of the population, using just samples? Can point estimates serve as useful guides to population parameters?\nThis act of generalizing from sample to population is at the heart of statistical inference.\n\n\n\n\n\n\nImportant\n\n\n\nNOTE: there is an alliterative mnemonic here:\nSamples have Statistics; Populations have Parameters."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/20-SampProb/index.html#sampling",
    "href": "content/courses/Analytics/Modelling/Modules/20-SampProb/index.html#sampling",
    "title": "üé≤ Samples, Populations, Statistics and Inference",
    "section": "Sampling",
    "text": "Sampling\nWe will first execute some samples from a known dataset. We load up the NHANES dataset and inspect it.\n\ndata(\"NHANES\")\nmosaic::inspect(NHANES)\n\n\ncategorical variables:  \n               name  class levels     n missing\n1          SurveyYr factor      2 10000       0\n2            Gender factor      2 10000       0\n3         AgeDecade factor      8  9667     333\n4             Race1 factor      5 10000       0\n5             Race3 factor      6  5000    5000\n6         Education factor      5  7221    2779\n7     MaritalStatus factor      6  7231    2769\n8          HHIncome factor     12  9189     811\n9           HomeOwn factor      3  9937      63\n10             Work factor      3  7771    2229\n11 BMICatUnder20yrs factor      4  1274    8726\n12          BMI_WHO factor      4  9603     397\n13         Diabetes factor      2  9858     142\n14        HealthGen factor      5  7539    2461\n15   LittleInterest factor      3  6667    3333\n16        Depressed factor      3  6673    3327\n17     SleepTrouble factor      2  7772    2228\n18       PhysActive factor      2  8326    1674\n19         TVHrsDay factor      7  4859    5141\n20       CompHrsDay factor      7  4863    5137\n21  Alcohol12PlusYr factor      2  6580    3420\n22         SmokeNow factor      2  3211    6789\n23         Smoke100 factor      2  7235    2765\n24        Smoke100n factor      2  7235    2765\n25        Marijuana factor      2  4941    5059\n26     RegularMarij factor      2  4941    5059\n27        HardDrugs factor      2  5765    4235\n28          SexEver factor      2  5767    4233\n29          SameSex factor      2  5768    4232\n30   SexOrientation factor      3  4842    5158\n31      PregnantNow factor      3  1696    8304\n                                    distribution\n1  2009_10 (50%), 2011_12 (50%)                 \n2  female (50.2%), male (49.8%)                 \n3   40-49 (14.5%),  0-9 (14.4%) ...             \n4  White (63.7%), Black (12%) ...               \n5  White (62.7%), Black (11.8%) ...             \n6  Some College (31.4%) ...                     \n7  Married (54.6%), NeverMarried (19.1%) ...    \n8  more 99999 (24.2%) ...                       \n9  Own (64.7%), Rent (33.1%) ...                \n10 Working (59.4%), NotWorking (36.6%) ...      \n11 NormWeight (63.2%), Obese (17.3%) ...        \n12 18.5_to_24.9 (30.3%) ...                     \n13 No (92.3%), Yes (7.7%)                       \n14 Good (39.2%), Vgood (33.3%) ...              \n15 None (76.5%), Several (16.9%) ...            \n16 None (78.6%), Several (15.1%) ...            \n17 No (74.6%), Yes (25.4%)                      \n18 Yes (55.8%), No (44.2%)                      \n19 2_hr (26.2%), 1_hr (18.2%) ...               \n20 0_to_1_hr (29%), 0_hrs (22.1%) ...           \n21 Yes (79.2%), No (20.8%)                      \n22 No (54.3%), Yes (45.7%)                      \n23 No (55.6%), Yes (44.4%)                      \n24 Non-Smoker (55.6%), Smoker (44.4%)           \n25 Yes (58.5%), No (41.5%)                      \n26 No (72.4%), Yes (27.6%)                      \n27 No (81.5%), Yes (18.5%)                      \n28 Yes (96.1%), No (3.9%)                       \n29 No (92.8%), Yes (7.2%)                       \n30 Heterosexual (95.8%), Bisexual (2.5%) ...    \n31 No (92.7%), Yes (4.2%) ...                   \n\nquantitative variables:  \n              name   class      min        Q1    median        Q3        max\n1               ID integer 51624.00 56904.500 62159.500 67039.000  71915.000\n2              Age integer     0.00    17.000    36.000    54.000     80.000\n3        AgeMonths integer     0.00   199.000   418.000   624.000    959.000\n4      HHIncomeMid integer  2500.00 30000.000 50000.000 87500.000 100000.000\n5          Poverty numeric     0.00     1.240     2.700     4.710      5.000\n6        HomeRooms integer     1.00     5.000     6.000     8.000     13.000\n7           Weight numeric     2.80    56.100    72.700    88.900    230.700\n8           Length numeric    47.10    75.700    87.000    96.100    112.200\n9         HeadCirc numeric    34.20    39.575    41.450    42.925     45.400\n10          Height numeric    83.60   156.800   166.000   174.500    200.400\n11             BMI numeric    12.88    21.580    25.980    30.890     81.250\n12           Pulse integer    40.00    64.000    72.000    82.000    136.000\n13        BPSysAve integer    76.00   106.000   116.000   127.000    226.000\n14        BPDiaAve integer     0.00    61.000    69.000    76.000    116.000\n15          BPSys1 integer    72.00   106.000   116.000   128.000    232.000\n16          BPDia1 integer     0.00    62.000    70.000    76.000    118.000\n17          BPSys2 integer    76.00   106.000   116.000   128.000    226.000\n18          BPDia2 integer     0.00    60.000    68.000    76.000    118.000\n19          BPSys3 integer    76.00   106.000   116.000   126.000    226.000\n20          BPDia3 integer     0.00    60.000    68.000    76.000    116.000\n21    Testosterone numeric     0.25    17.700    43.820   362.410   1795.600\n22      DirectChol numeric     0.39     1.090     1.290     1.580      4.030\n23         TotChol numeric     1.53     4.110     4.780     5.530     13.650\n24       UrineVol1 integer     0.00    50.000    94.000   164.000    510.000\n25      UrineFlow1 numeric     0.00     0.403     0.699     1.221     17.167\n26       UrineVol2 integer     0.00    52.000    95.000   171.750    409.000\n27      UrineFlow2 numeric     0.00     0.475     0.760     1.513     13.692\n28     DiabetesAge integer     1.00    40.000    50.000    58.000     80.000\n29 DaysPhysHlthBad integer     0.00     0.000     0.000     3.000     30.000\n30 DaysMentHlthBad integer     0.00     0.000     0.000     4.000     30.000\n31    nPregnancies integer     1.00     2.000     3.000     4.000     32.000\n32         nBabies integer     0.00     2.000     2.000     3.000     12.000\n33      Age1stBaby integer    14.00    19.000    22.000    26.000     39.000\n34   SleepHrsNight integer     2.00     6.000     7.000     8.000     12.000\n35  PhysActiveDays integer     1.00     2.000     3.000     5.000      7.000\n36   TVHrsDayChild integer     0.00     1.000     2.000     3.000      6.000\n37 CompHrsDayChild integer     0.00     0.000     1.000     6.000      6.000\n38      AlcoholDay integer     1.00     1.000     2.000     3.000     82.000\n39     AlcoholYear integer     0.00     3.000    24.000   104.000    364.000\n40        SmokeAge integer     6.00    15.000    17.000    19.000     72.000\n41   AgeFirstMarij integer     1.00    15.000    16.000    19.000     48.000\n42     AgeRegMarij integer     5.00    15.000    17.000    19.000     52.000\n43          SexAge integer     9.00    15.000    17.000    19.000     50.000\n44 SexNumPartnLife integer     0.00     2.000     5.000    12.000   2000.000\n45  SexNumPartYear integer     0.00     1.000     1.000     1.000     69.000\n           mean           sd     n missing\n1  6.194464e+04 5.871167e+03 10000       0\n2  3.674210e+01 2.239757e+01 10000       0\n3  4.201239e+02 2.590431e+02  4962    5038\n4  5.720617e+04 3.302028e+04  9189     811\n5  2.801844e+00 1.677909e+00  9274     726\n6  6.248918e+00 2.277538e+00  9931      69\n7  7.098180e+01 2.912536e+01  9922      78\n8  8.501602e+01 1.370503e+01   543    9457\n9  4.118068e+01 2.311483e+00    88    9912\n10 1.618778e+02 2.018657e+01  9647     353\n11 2.666014e+01 7.376579e+00  9634     366\n12 7.355973e+01 1.215542e+01  8563    1437\n13 1.181550e+02 1.724817e+01  8551    1449\n14 6.748006e+01 1.435480e+01  8551    1449\n15 1.190902e+02 1.749636e+01  8237    1763\n16 6.827826e+01 1.378078e+01  8237    1763\n17 1.184758e+02 1.749133e+01  8353    1647\n18 6.766455e+01 1.441978e+01  8353    1647\n19 1.179292e+02 1.717719e+01  8365    1635\n20 6.729874e+01 1.495839e+01  8365    1635\n21 1.978980e+02 2.265045e+02  4126    5874\n22 1.364865e+00 3.992581e-01  8474    1526\n23 4.879220e+00 1.075583e+00  8474    1526\n24 1.185161e+02 9.033648e+01  9013     987\n25 9.792946e-01 9.495143e-01  8397    1603\n26 1.196759e+02 9.016005e+01  1478    8522\n27 1.149372e+00 1.072948e+00  1476    8524\n28 4.842289e+01 1.568050e+01   629    9371\n29 3.334838e+00 7.400700e+00  7532    2468\n30 4.126493e+00 7.832971e+00  7534    2466\n31 3.026882e+00 1.795341e+00  2604    7396\n32 2.456954e+00 1.315227e+00  2416    7584\n33 2.264968e+01 4.772509e+00  1884    8116\n34 6.927531e+00 1.346729e+00  7755    2245\n35 3.743513e+00 1.836358e+00  4663    5337\n36 1.938744e+00 1.434431e+00   653    9347\n37 2.197550e+00 2.516667e+00   653    9347\n38 2.914123e+00 3.182672e+00  4914    5086\n39 7.510165e+01 1.030337e+02  5922    4078\n40 1.782662e+01 5.326660e+00  3080    6920\n41 1.702283e+01 3.895010e+00  2891    7109\n42 1.769107e+01 4.806103e+00  1366    8634\n43 1.742870e+01 3.716551e+00  5540    4460\n44 1.508507e+01 5.784643e+01  5725    4275\n45 1.342330e+00 2.782688e+00  4928    5072\n\n\nLet us create a NHANES dataset without duplicated IDs and only adults:\n\nNHANES <-\n  NHANES %>%\n  distinct(ID, .keep_all = TRUE) \n\n#create a dataset of only adults\nNHANES_adult <-  NHANES %>%\n  filter(Age >= 18) %>%\n  drop_na(Height)\n\nglimpse(NHANES_adult)\n\nRows: 4,790\nColumns: 76\n$ ID               <int> 51624, 51630, 51647, 51654, 51656, 51657, 51666, 5166‚Ä¶\n$ SurveyYr         <fct> 2009_10, 2009_10, 2009_10, 2009_10, 2009_10, 2009_10,‚Ä¶\n$ Gender           <fct> male, female, female, male, male, male, female, male,‚Ä¶\n$ Age              <int> 34, 49, 45, 66, 58, 54, 58, 50, 33, 60, 56, 57, 54, 3‚Ä¶\n$ AgeDecade        <fct>  30-39,  40-49,  40-49,  60-69,  50-59,  50-59,  50-5‚Ä¶\n$ AgeMonths        <int> 409, 596, 541, 795, 707, 654, 700, 603, 404, 721, 677‚Ä¶\n$ Race1            <fct> White, White, White, White, White, White, Mexican, Wh‚Ä¶\n$ Race3            <fct> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ Education        <fct> High School, Some College, College Grad, Some College‚Ä¶\n$ MaritalStatus    <fct> Married, LivePartner, Married, Married, Divorced, Mar‚Ä¶\n$ HHIncome         <fct> 25000-34999, 35000-44999, 75000-99999, 25000-34999, m‚Ä¶\n$ HHIncomeMid      <int> 30000, 40000, 87500, 30000, 100000, 70000, 87500, 175‚Ä¶\n$ Poverty          <dbl> 1.36, 1.91, 5.00, 2.20, 5.00, 2.20, 2.03, 1.24, 1.27,‚Ä¶\n$ HomeRooms        <int> 6, 5, 6, 5, 10, 6, 10, 4, 11, 5, 10, 9, 3, 6, 6, 10, ‚Ä¶\n$ HomeOwn          <fct> Own, Rent, Own, Own, Rent, Rent, Rent, Rent, Own, Own‚Ä¶\n$ Work             <fct> NotWorking, NotWorking, Working, NotWorking, Working,‚Ä¶\n$ Weight           <dbl> 87.4, 86.7, 75.7, 68.0, 78.4, 74.7, 57.5, 84.1, 93.8,‚Ä¶\n$ Length           <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ HeadCirc         <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ Height           <dbl> 164.7, 168.4, 166.7, 169.5, 181.9, 169.4, 148.1, 177.‚Ä¶\n$ BMI              <dbl> 32.22, 30.57, 27.24, 23.67, 23.69, 26.03, 26.22, 26.6‚Ä¶\n$ BMICatUnder20yrs <fct> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ BMI_WHO          <fct> 30.0_plus, 30.0_plus, 25.0_to_29.9, 18.5_to_24.9, 18.‚Ä¶\n$ Pulse            <int> 70, 86, 62, 60, 62, 76, 94, 74, 96, 84, 64, 70, 64, 6‚Ä¶\n$ BPSysAve         <int> 113, 112, 118, 111, 104, 134, 127, 142, 128, 152, 95,‚Ä¶\n$ BPDiaAve         <int> 85, 75, 64, 63, 74, 85, 83, 68, 74, 100, 69, 89, 41, ‚Ä¶\n$ BPSys1           <int> 114, 118, 106, 124, 108, 136, NA, 138, 126, 154, 94, ‚Ä¶\n$ BPDia1           <int> 88, 82, 62, 64, 76, 86, NA, 66, 80, 98, 74, 82, 48, 8‚Ä¶\n$ BPSys2           <int> 114, 108, 118, 108, 104, 132, 134, 142, 128, 150, 94,‚Ä¶\n$ BPDia2           <int> 88, 74, 68, 62, 72, 88, 82, 74, 74, 98, 70, 88, 42, 8‚Ä¶\n$ BPSys3           <int> 112, 116, 118, 114, 104, 136, 120, 142, NA, 154, 96, ‚Ä¶\n$ BPDia3           <int> 82, 76, 60, 64, 76, 82, 84, 62, NA, 102, 68, 90, 40, ‚Ä¶\n$ Testosterone     <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ DirectChol       <dbl> 1.29, 1.16, 2.12, 0.67, 0.96, 1.16, 1.14, 1.06, 0.91,‚Ä¶\n$ TotChol          <dbl> 3.49, 6.70, 5.82, 4.99, 4.24, 6.41, 4.78, 5.22, 5.59,‚Ä¶\n$ UrineVol1        <int> 352, 77, 106, 113, 163, 215, 29, 64, 155, 238, 26, 13‚Ä¶\n$ UrineFlow1       <dbl> NA, 0.094, 1.116, 0.489, NA, 0.903, 0.299, 0.190, 0.5‚Ä¶\n$ UrineVol2        <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 86, NA, NA, N‚Ä¶\n$ UrineFlow2       <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 0.43, NA, NA,‚Ä¶\n$ Diabetes         <fct> No, No, No, No, No, No, No, No, No, No, No, No, No, N‚Ä¶\n$ DiabetesAge      <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ HealthGen        <fct> Good, Good, Vgood, Vgood, Vgood, Fair, NA, Good, Fair‚Ä¶\n$ DaysPhysHlthBad  <int> 0, 0, 0, 10, 0, 4, NA, 0, 3, 7, 3, 0, 0, 3, 0, 2, 0, ‚Ä¶\n$ DaysMentHlthBad  <int> 15, 10, 3, 0, 0, 0, NA, 0, 7, 0, 0, 0, 0, 4, 0, 30, 0‚Ä¶\n$ LittleInterest   <fct> Most, Several, None, None, None, None, NA, None, Seve‚Ä¶\n$ Depressed        <fct> Several, Several, None, None, None, None, NA, None, N‚Ä¶\n$ nPregnancies     <int> NA, 2, 1, NA, NA, NA, NA, NA, NA, NA, 4, 2, NA, NA, N‚Ä¶\n$ nBabies          <int> NA, 2, NA, NA, NA, NA, NA, NA, NA, NA, 3, 2, NA, NA, ‚Ä¶\n$ Age1stBaby       <int> NA, 27, NA, NA, NA, NA, NA, NA, NA, NA, 26, 32, NA, N‚Ä¶\n$ SleepHrsNight    <int> 4, 8, 8, 7, 5, 4, 5, 7, 6, 6, 7, 8, 6, 5, 6, 4, 5, 7,‚Ä¶\n$ SleepTrouble     <fct> Yes, Yes, No, No, No, Yes, No, No, No, Yes, No, No, Y‚Ä¶\n$ PhysActive       <fct> No, No, Yes, Yes, Yes, Yes, Yes, Yes, No, No, Yes, Ye‚Ä¶\n$ PhysActiveDays   <int> NA, NA, 5, 7, 5, 1, 2, 7, NA, NA, 7, 3, 3, NA, 2, NA,‚Ä¶\n$ TVHrsDay         <fct> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ CompHrsDay       <fct> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ TVHrsDayChild    <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ CompHrsDayChild  <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ Alcohol12PlusYr  <fct> Yes, Yes, Yes, Yes, Yes, Yes, NA, No, Yes, Yes, Yes, ‚Ä¶\n$ AlcoholDay       <int> NA, 2, 3, 1, 2, 6, NA, NA, 3, 6, 1, 1, 2, NA, 12, NA,‚Ä¶\n$ AlcoholYear      <int> 0, 20, 52, 100, 104, 364, NA, 0, 104, 36, 12, 312, 15‚Ä¶\n$ SmokeNow         <fct> No, Yes, NA, No, NA, NA, Yes, NA, No, No, NA, No, NA,‚Ä¶\n$ Smoke100         <fct> Yes, Yes, No, Yes, No, No, Yes, No, Yes, Yes, No, Yes‚Ä¶\n$ Smoke100n        <fct> Smoker, Smoker, Non-Smoker, Smoker, Non-Smoker, Non-S‚Ä¶\n$ SmokeAge         <int> 18, 38, NA, 13, NA, NA, 17, NA, NA, 16, NA, 18, NA, N‚Ä¶\n$ Marijuana        <fct> Yes, Yes, Yes, NA, Yes, Yes, NA, No, No, NA, No, Yes,‚Ä¶\n$ AgeFirstMarij    <int> 17, 18, 13, NA, 19, 15, NA, NA, NA, NA, NA, 18, NA, N‚Ä¶\n$ RegularMarij     <fct> No, No, No, NA, Yes, Yes, NA, No, No, NA, No, No, No,‚Ä¶\n$ AgeRegMarij      <int> NA, NA, NA, NA, 20, 15, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ HardDrugs        <fct> Yes, Yes, No, No, Yes, Yes, NA, No, No, No, No, No, N‚Ä¶\n$ SexEver          <fct> Yes, Yes, Yes, Yes, Yes, Yes, NA, Yes, Yes, Yes, Yes,‚Ä¶\n$ SexAge           <int> 16, 12, 13, 17, 22, 12, NA, NA, 27, 20, 20, 18, 14, 2‚Ä¶\n$ SexNumPartnLife  <int> 8, 10, 20, 15, 7, 100, NA, 9, 1, 1, 2, 5, 20, 1, 20, ‚Ä¶\n$ SexNumPartYear   <int> 1, 1, 0, NA, 1, 1, NA, 1, 1, NA, 1, 1, 2, 1, 3, 1, NA‚Ä¶\n$ SameSex          <fct> No, Yes, Yes, No, No, No, NA, No, No, No, No, No, No,‚Ä¶\n$ SexOrientation   <fct> Heterosexual, Heterosexual, Bisexual, NA, Heterosexua‚Ä¶\n$ PregnantNow      <fct> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n\n\n\n\n\n\n\n\nPopulation\n\n\n\nFor now, we will treat this dataset as our Population. So each variable in the dataset is a population for that particular quantity/category, with appropriate population parameters such as means, sd-s, and proportions.\n\n\nLet us calculate the population parameters for the Height data:\n\npop_mean_height <- mean(~ Height, data = NHANES_adult)\npop_sd_height <- sd(~ Height, data = NHANES_adult)\n\npop_mean_height\n\n[1] 168.3497\n\npop_sd_height\n\n[1] 10.15705\n\n\nNow, we will sample ONCE from the NHANES Height variable. Let us take a sample of sample size 50. We will compare sample statistics with population parameters on the basis of this ONE sample of 50:\n\nsample_height <- sample(NHANES_adult, size = 50) %>% \n  select(Height)\nsample_height\n\n\n\n\n\nSingle-Sample Mean and Population Mean\n\nsample_mean_height <- mean(~ Height, data = sample_height)\nsample_mean_height\n\n[1] 165.866\n\n# Plotting the histogram of this sample\nsample_height %>% \n  gf_histogram(~ Height) %>% \n  \n  gf_vline(xintercept = sample_mean_height, \n           color = \"red\") %>% \n  \n  gf_vline(xintercept = pop_mean_height, \n           colour = \"blue\") %>% \n  \n  gf_text(1 ~ (pop_mean_height + 5), \n          label = \"Population Mean Height\", \n          color = \"blue\") %>% \n  \n  gf_text(2 ~ (sample_mean_height-5), \n          label = \"Sample Mean Height\", color = \"red\")  %>% \n  \n  gf_theme(theme = theme_minimal)\n\n\n\nSingle-Sample Mean and Population Mean\n\n\n\n\nOK, so the sample_mean_height is not too far from the pop_mean_height. Is this always true? Let us check: we will create 500 samples each of size 50. And calculate their mean as the sample statistic, giving us a dataframe containing 5000 sample means. We will then compare if these 500 means are close to the pop_mean_height:\n\nsample_height_500 <- do(500) * {\n  sample(NHANES_adult, size = 50) %>%\n    select(Height) %>%\n    summarise(\n      sample_mean_500 = mean(Height),\n      sample_min_500 = min(Height),\n      sample_max_500 = max(Height))\n}\n\nhead(sample_height_500)\n\n\n\n\n\nMultiple Sample-Means and Population Mean\n\ndim(sample_height_500)\n\n[1] 500   5\n\nsample_height_500 %>%\n  gf_point(.index ~ sample_mean_500, color = \"red\") %>%\n  \n  gf_segment(\n    .index + .index ~ sample_min_500 + sample_max_500,\n    color = \"red\",\n    size = 0.3,\n    alpha = 0.3,\n    ylab = \"Sample Index (1-500)\",\n    xlab = \"Sample Means\"\n  ) %>%\n  \n  gf_vline(xintercept = ~ pop_mean_height, \n           color = \"blue\") %>%\n  \n  gf_label(-15 ~ pop_mean_height, label = \"Population Mean\", \n           color = \"blue\") %>% \n  \n  gf_theme(theme = theme_minimal)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\n\n\n\nMultiple Sample-Means and Population Mean\n\n\n\n\nThe sample_means (red dots), are themselves random because the samples are random, of course. It appears that they are generally in the vicinity of the pop_mean (blue line).\nDistribution of Sample-Means\nSince the sample-means are themselves random variables, let‚Äôs plot the distribution of these 5000 sample-means themselves, called a a distribution of sample-means.\n\nNOTE: this a distribution of sample-means will itself have a mean and standard deviation. Do not get confused ;-D\n\nWe will also plot the position of the population mean pop_mean_height parameter, the means of the Height variable.\n\nsample_height_500 %>% \n  gf_dhistogram(~ sample_mean_500) %>% \n  \n  gf_vline(xintercept = pop_mean_height, \n           color = \"blue\") %>% \n  \n   gf_label(0.01 ~ pop_mean_height, \n            label = \"Population Mean\", \n            color = \"blue\")\n\nWarning: `stat(density)` was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `after_stat(density)` instead.\n\n\n\n\nSampling Mean Distribution\n\n\n\n\nHow does this distribution of sample-means compare with that of the overall distribution of the population?\n\nsample_height_500 %>% \n  gf_dhistogram(~ sample_mean_500) %>% \n  \n  gf_vline(xintercept = pop_mean_height, \n           color = \"blue\") %>% \n  \n   gf_label(0.01 ~ pop_mean_height, \n            label = \"Population Mean\", \n            color = \"blue\") %>% \n\n  ## Add the population histogram\n  gf_histogram(~ Height, data = NHANES_adult, \n               alpha = 0.2, fill = \"blue\", \n               bins = 50) %>% \n  \n  gf_label(0.025 ~ (pop_mean_height + 20), \n           label = \"Population Distribution\", color = \"blue\")\n\n\n\nSampling Means and Population Distributions\n\n\n\n\nCentral limit theorem\nWe see in the Figure above that\n\nthe distribution of sample-means is centered around mean = pop_mean.\nThat the standard deviation of the distribution of sample means is less than that of the original population. But exactly what is it?\nAnd what is the kind of distribution?\n\nOne more experiment.\nNow let‚Äôs repeatedly sample Height and compute the sample mean, and look at the resulting histograms and Q-Q plots. ( Q-Q plots check whether a certain distribution is close to being normal or not.)\nWe will use sample sizes of c(16, 32, 64, 128) and generate 1000 samples each time, take the means and plot these 1000 means:\n\nset.seed(12345)\n\n\nsamples_height_16 <- do(1000) * mean(resample(NHANES_adult$Height, size = 16))\n\nsamples_height_32 <- do(1000) * mean(resample(NHANES_adult$Height, size = 32))\n\nsamples_height_64 <- do(1000) * mean(resample(NHANES_adult$Height, size = 64))\n\nsamples_height_128 <- do(1000) * mean(resample(NHANES_adult$Height, size = 128))\n\n# Quick Check\nhead(samples_height_16)\n\n\n\n  \n\n\n### do(1000,) * mean(resample(NHANES_adult$Height, size = 16)) produces a data frame with a variable named mean.\n###\n\nNow let‚Äôs create separate Q-Q plots for the different sample sizes.\n\n# Now let's create separate Q-Q plots for the different sample sizes.\n#\np1 <- gf_qq( ~ mean,data = samples_height_16,\n             title = \"N = 16\", \n             color = \"cornsilk\") %>%\n  gf_qqline()\n\np2 <- gf_qq( ~ mean,data = samples_height_32,\n            title = \"N = 32\", \n            color = \"sienna\") %>%\n  gf_qqline()\n\np3 <- gf_qq( ~ mean,data = samples_height_32,\n            title = \"N = 64\", \n            color = \"tomato2\") %>%\n  gf_qqline()\n\np4 <- gf_qq( ~ mean,data = samples_height_128,\n            title = \"N = 128\", \n            color = \"violetred\") %>%\n  gf_qqline()\n\ncowplot::plot_grid(p1, p2, p3, p4)\n\nWarning: The following aesthetics were dropped during statistical transformation: sample\n‚Ñπ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\n‚Ñπ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\nThe following aesthetics were dropped during statistical transformation: sample\n‚Ñπ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\n‚Ñπ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\nThe following aesthetics were dropped during statistical transformation: sample\n‚Ñπ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\n‚Ñπ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\nThe following aesthetics were dropped during statistical transformation: sample\n‚Ñπ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\n‚Ñπ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\nLet us plot their individual histograms to compare them:\n\n\n\n\n\nAnd if we overlay the histograms:\n\n\n\n\n\nThis shows that the results become more normally distributed (i.e. following the straight line) as the samples get larger. Hence we learn that:\n\nthe sample-means are normally distributed around the population mean. This is because when we sample from the population, many values will be close to the population mean, and values far away from the mean will be increasingly scarce.\n\n\nmean(~ mean, data  = samples_height_16)\n\n[1] 168.306\n\nmean(~ mean, data  = samples_height_32)\n\n[1] 168.4349\n\nmean(~ mean, data  = samples_height_64)\n\n[1] 168.3184\n\nmean(~ mean, data  = samples_height_128)\n\n[1] 168.366\n\npop_mean_height\n\n[1] 168.3497\n\n\n\n\n\n\n\n\nCentral Limit Theorem\n\n\n\nThis is the Central Limit Theorem (CLT)\n\nthe sample-means become ‚Äúmore normally distributed‚Äù with sample length, as shown by the (small but definite) improvements in the Q-Q plots with sample-size.\nthe sample-mean distributions narrow with sample length.\nThis is regardless of the distribution of the population itself.\n\n\n\nThe Height variable seems to be normally distributed at population level. We will try other non-normal population variables as an exercise).\nAs we saw above, the standard deviations of the sample-mean distributions reduce with sample size. In fact their SDs are defined by:\nsd = pop_sd/sqrt(sample_size) where sample-size here is one of c(16,32,64,128)\n\nsd(~ mean, data  = samples_height_16)\n\n[1] 2.578355\n\nsd(~ mean, data  = samples_height_32)\n\n[1] 1.834979\n\nsd(~ mean, data  = samples_height_64)\n\n[1] 1.280014\n\nsd(~ mean, data  = samples_height_128)\n\n[1] 0.9096318\n\n\nThe standard deviation of the sample-mean distribution is called the Standard Error. This statistic derived from the sample, will help us infer our population parameters with a precise estimate of the uncertainty involved.\n\\[\nStandard\\ Error\\ \\pmb {se} = \\frac{population\\ sd}{\\sqrt[]{sample\\ size}} \\\\\\\n\\pmb {se} = \\frac{\\sigma}{\\sqrt[]{n}}\n\\]\nIn our sampling experiments, the Standard Errors evaluate to:\n\npop_sd_height <- sd(~ Height, data = NHANES_adult)\n\npop_sd_height/sqrt(16)\n\n[1] 2.539262\n\npop_sd_height/sqrt(32)\n\n[1] 1.795529\n\npop_sd_height/sqrt(64)\n\n[1] 1.269631\n\npop_sd_height/sqrt(128)\n\n[1] 0.8977646\n\n\nAs seen, these are identical to the Standard Deviations of the individual sample-mean distributions."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/20-SampProb/index.html#confidence-intervals",
    "href": "content/courses/Analytics/Modelling/Modules/20-SampProb/index.html#confidence-intervals",
    "title": "üé≤ Samples, Populations, Statistics and Inference",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nWhen we work with samples, we want to be able to speak with a certain degree of confidence about the population mean, based on the evaluation of one sample mean,not a whole large number of them. Give that sample-means are normally distributed around the population means, we can say that \\(68\\%\\) of all possible sample-mean lie within \\(\\pm SE\\)of the population mean; and further that \\(95 \\%\\) of all possible sample-mean lie within \\(\\pm 1.5*SE\\) of the population mean.\nThese two intervals \\(sample.mean \\pm SE\\) and \\(sample.mean \\pm 1.5*SE\\) are called the confidence intervals for the population mean, at levels \\(68\\%\\) and \\(95 \\%\\) probability respectively.\nThus if we want to estimate a population mean:\n\nwe take one sample from the population\nwe calculate the sample-mean\nwe calculate the sample-sd\nwe calculate the Standard Error as \\(\\frac{sample-sd}{\\sqrt[]{n}}\\)\n\nwe calculate 95% confidence intervals for the population mean based on the formula above."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/20-SampProb/index.html#references",
    "href": "content/courses/Analytics/Modelling/Modules/20-SampProb/index.html#references",
    "title": "üé≤ Samples, Populations, Statistics and Inference",
    "section": "References",
    "text": "References\n\nDiez, David M & Barr, Christopher D & √áetinkaya-Rundel, Mine, OpenIntro Statistics. https://www.openintro.org/book/os/\nStats Test Wizard. https://www.socscistatistics.com/tests/what_stats_test_wizard.aspx\nDiez, David M & Barr, Christopher D & √áetinkaya-Rundel, Mine: OpenIntro Statistics. Available online https://www.openintro.org/book/os/\nM√•ns Thulin, Modern Statistics with R: From wrangling and exploring data to inference and predictive modelling http://www.modernstatisticswithr.com/\nJonas Kristoffer Lindel√∏v, Common statistical tests are linear models (or: how to teach stats) https://lindeloev.github.io/tests-as-linear/\nCheatSheet https://lindeloev.github.io/tests-as-linear/linear_tests_cheat_sheet.pdf\nCommon statistical tests are linear models: a work through by Steve Doogue https://steverxd.github.io/Stat_tests/\nJeffrey Walker ‚ÄúElements of Statistical Modeling for Experimental Biology‚Äù. https://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/60-SimTest/files/simulation.html",
    "href": "content/courses/Analytics/Modelling/Modules/60-SimTest/files/simulation.html",
    "title": "simulation",
    "section": "",
    "text": "In this module we will use simulation to solve several problems in Business Decision Making."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/60-SimTest/index.html#fa-folder-open-slides-and-tutorials",
    "href": "content/courses/Analytics/Modelling/Modules/60-SimTest/index.html#fa-folder-open-slides-and-tutorials",
    "title": "Basics of Simulation Tests",
    "section": "\n Slides and Tutorials",
    "text": "Slides and Tutorials\n\n\n\n\n\n\n\n\n\n R Tutorial¬†\n  Orange Tutorial\n Radiant Tutorial\n Datasets"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/60-SimTest/index.html#introduction-the-lady-who-drank-tea",
    "href": "content/courses/Analytics/Modelling/Modules/60-SimTest/index.html#introduction-the-lady-who-drank-tea",
    "title": "Basics of Simulation Tests",
    "section": "Introduction: The Lady who Drank Tea",
    "text": "Introduction: The Lady who Drank Tea\n\nThere is a famous story about a lady who claimed that tea with milk tasted different depending on whether the milk was added to the tea or the tea added to the milk. The story is famous because of the setting in which she made this claim. She was attending a party in Cambridge, England, in the 1920s. Also in attendance were a number of university dons and their wives. The scientists in attendance scoffed at the woman and her claim. What, after all, could be the difference?\nAll the scientists but one, that is. Rather than simply dismiss the woman‚Äôs claim, he proposed that they decide how one should test the claim. The tenor of the conversation changed at this suggestion, and the scientists began to discuss how the claim should be tested. Within a few minutes cups of tea with milk had been prepared and presented to the woman for tasting.\nAt this point, you may be wondering who the innovative scientist was and what the results of the experiment were. The scientist was R. A. Fisher, who first described this situation as a pedagogical example in his 1925 book on statistical methodology[^1]. Fisher developed statistical methods that are among the most important and widely used methods to this day, and most of his applications were biological.\n\nGame\nLet‚Äôs try an experiment. I‚Äôll flip 10 coins. You guess which are heads and which are tails, and we‚Äôll see how you do. Please write down a sequence of ‚ÄúH‚Äù or ‚ÄúT‚Äù. Comparing with your classmates, we will undoubtedly see that some of you did better and others worse.\nWhat would be your impression of one of you got 9 guesses correct? Is that SKILL or is that something else? What would be your immediate reaction and next move?\nAnalysis\nBack to the Lady who drank Tea !!\n\nLet‚Äôs suppose we decide to test the lady with ten cups of tea. We‚Äôll flip a coin to decide which way to prepare the cups. If we flip a head, we will pour the milk in first; if tails, we put the tea in first. Then we present the ten cups to the lady and have her state which ones she thinks were prepared each way.\nIt is easy to give her a score (9 out of 10, or 7 out of 10, or whatever it happens to be). It is trickier to figure out what to do with her score. Even if she is just guessing and has no idea, she could get lucky and get quite a few correct ‚Äì maybe even all 10. But how likely is that?\nNow let‚Äôs suppose the lady gets 9 out of 10 correct. That‚Äôs not perfect, but it is better than we would expect for someone who was just guessing. On the other hand, it is not impossible to get 9 out of 10 just by guessing.\nSo here is Fisher‚Äôs great idea: Let‚Äôs figure out how hard it is to get 9 out of 10 by guessing. If it‚Äôs not so hard to do, then perhaps that‚Äôs just what happened ( that she was guessing ), so we won‚Äôt be too impressed with the lady‚Äôs tea tasting ability. On the other hand, if it is really unusual to get 9 out of 10 correct by guessing, then we will have some evidence that she must be able to tell something ( and has an unusual Skill).\nBut how do we figure out how unusual it is to get 9 out of 10 just by guessing? Let‚Äôs just flip a bunch of coins and keep track. If the lady is just guessing, she might as well be flipping a coin.\nSo here‚Äôs the plan. We‚Äôll flip 10 coins, and repeat that experiment 10000 times. We‚Äôll call the heads correct guesses and the tails incorrect guesses.\n\n\n\nheads\n   0    1    2    3    4    5    6    7    8    9   10 \n   9  110  413 1145 2096 2446 2066 1141  469   97    8 \n\n\n\n\n\nSo what do we conclude? It is possible that the lady could get 9 or 10 correct just by guessing, but it is not very likely (it only happened in about \\(\\frac{97+8}{10000} = 1.05\\%\\) of our simulations). So one of two things must be true:\n‚Ä¢ The lady got unusually ‚Äúlucky‚Äù, by chance; OR\n‚Ä¢ The lady is not just guessing and really has some ability in this regard."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/60-SimTest/index.html#commentary",
    "href": "content/courses/Analytics/Modelling/Modules/60-SimTest/index.html#commentary",
    "title": "Basics of Simulation Tests",
    "section": "Commentary",
    "text": "Commentary\nFirst we realize something is surprising, and that we have a question or doubt. This is based on something we see, or measure, a test statistic. In our story, it is the score of \\(10/10\\) that the Lady was able to achieve about how the Tea was made.\nWe then assume the Lady is guessing and somehow by chance able to guess correctly. This would be our‚Ä¶.NULL Hypothesis. This is our (conservative) belief about the Real World.\nWe then randomly generate many Parallel Counterfactual Worlds, where we repeat the experiment many many times, each time calculating the test statistic, under the assumption of the NULL Hypothesis is TRUE.\nWe see how often our Parallel Worlds can mimic or exceed Real World measurement of the the test statistic by comparison. If this is common (i.e.¬†probability is high) we say we cannot reject the NULL Hypothesis (and the Lady is lucky). If the occurrence is rare, as in our case, we say we have reason to reject the NULL Hypothesis and reason to believe an underlying pattern (and Lady‚Äôs ability is beyond Question !)\nThis is the essence of the Simulation Method in statistical modelling. Take one more look at the picture from Allen Downey‚Äôs blog, below:"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/60-SimTest/index.html#references",
    "href": "content/courses/Analytics/Modelling/Modules/60-SimTest/index.html#references",
    "title": "Basics of Simulation Tests",
    "section": "References",
    "text": "References\n\nAllen Downey, There is still only one test\nR.A. Fisher. Statistical Methods for Research Workers. Oliver & Boyd, 1925\nhttps://timesofindia.indiatimes.com/sports/cricket/icc-mens-t20-world-cup/in-numbers-virat-kohli-and-his-strange-luck-with-the-coin-toss/articleshow/87538443.cms\nLaura Chihara, Tim Hesterberg, Mathematical Statistics with Resampling and R, Wiley, 2019.\nD. Salsburg. The Lady Tasting Tea: How statistics revolutionized science in the twentieth century. W.H. Freeman, New York, 2001\nDaniel Kaplan, Nicholas J. Horton, and Randall Pruim, Simulation-based inference with mosaic https://www.mosaic-web.org/mosaic/articles/Resampling.html"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/70-PermTest/files/permutation.html",
    "href": "content/courses/Analytics/Modelling/Modules/70-PermTest/files/permutation.html",
    "title": "Permutation Tests",
    "section": "",
    "text": "A student conducted a study of hot wings and beer consumption at a Bar. She asked patrons at the bar to record their consumption of hot wings and beer over the course of several hours. She wanted to know if people who ate more hot wings would then drink more beer. In addition, she investigated whether or not gender had an impact on hot wings or beer consumption.\n\nShow the CodeBeerwings <- read.csv(\"../../../../../../materials/data/resampling/Beerwings.csv\")\ninspect(Beerwings)\n\n\ncategorical variables:  \n    name     class levels  n missing\n1 Gender character      2 30       0\n                                   distribution\n1 F (50%), M (50%)                             \n\nquantitative variables:  \n      name   class min    Q1 median    Q3 max     mean        sd  n missing\n1       ID integer   1  8.25   15.5 22.75  30 15.50000  8.803408 30       0\n2 Hotwings integer   4  8.00   12.5 15.50  21 11.93333  4.784554 30       0\n3     Beer integer   0 24.00   30.0 36.00  48 26.20000 11.842064 30       0\n\n\nLet us calculate the observed difference in Hotwings consumption between Males and Females ( Gender)\n\nShow the Codemean(Hotwings ~ Gender, data = Beerwings)\n\n        F         M \n 9.333333 14.533333 \n\nShow the Codeobs_diff_wings <- mosaic::diffmean(data = Beerwings, Hotwings ~ Gender)\nobs_diff_wings \n\ndiffmean \n     5.2 \n\n\n\nShow the Codegf_boxplot(data = Beerwings, Hotwings ~ Gender, title = \"Hotwings Consumption by Gender\")\n\n\n\n\nThe observed difference in mean consumption of Hotwings between Males and Females is 5.2. Could this have occurred by chance? Here is our formulation of the Hypotheses:\n\\[\nNULL\\ Hypothesis\\ H_0 => No\\¬†difference\\ between\\ means\\ across\\ groups\\\\\nAlternative\\ Hypothesis\\\nH_a =>Significant\\ difference\\ between\\ the\\ means\\\n\\]\nSo we perform a Permutation Test to check:\n\nShow the Codenull_dist_wings <- do(1000) * diffmean(Hotwings ~ shuffle(Gender), data = Beerwings)\nnull_dist_wings %>% head()\n\n\n\n  \n\n\nShow the Codegf_histogram(data = null_dist_wings, ~ diffmean) %>% \n  gf_vline(xintercept = obs_diff_wings, colour = \"red\")\n\n\n\nShow the Codeprop1(~ diffmean >= obs_diff_wings, data = null_dist_wings)\n\n  prop_TRUE \n0.001998002 \n\n\nThe \\(\\color{red}{red\\ line}\\) shows the actual measured mean difference in Hot Wings consumption. The probability that our Permutation distribution is able to equal or exceed that number is \\(0.001998002\\) and we have to reject the Null Hypothesis that the means are identical.\n\n\nShow the Codeverizon <- read.csv(\"../../../../../../materials/data/resampling/Verizon.csv\")\ninspect(verizon)\n\n\ncategorical variables:  \n   name     class levels    n missing\n1 Group character      2 1687       0\n                                   distribution\n1 ILEC (98.6%), CLEC (1.4%)                    \n\nquantitative variables:  \n  name   class min   Q1 median   Q3   max     mean       sd    n missing\n1 Time numeric   0 0.75   3.63 7.35 191.6 8.522009 14.78848 1687       0\n\n\n\nShow the Codemean(Time ~ Group, data = verizon)\n\n     CLEC      ILEC \n16.509130  8.411611 \n\nShow the Codeobs_diff_verizon <- diffmean(Time ~ Group, data = verizon)\nobs_diff_verizon\n\ndiffmean \n-8.09752 \n\n\n\nShow the Codenull_dist_verizon <- do(1000) * diffmean(Time ~ shuffle(Group), data = verizon)\ngf_histogram(data = null_dist_verizon, ~ diffmean) %>% \n  gf_vline(xintercept = obs_diff_wings, colour = \"red\")\n\n\n\nShow the Codeprop1(~ diffmean >= obs_diff_wings, data = null_dist_verizon)\n\n prop_TRUE \n0.01298701 \n\n\n\nDo criminals released after a jail term commit crimes again?\n\nShow the Coderecidivism <- read.csv(\"../../../../../../materials/data/resampling/Recidivism.csv\")\ninspect(recidivism)\n\n\ncategorical variables:  \n     name     class levels     n missing\n1  Gender character      2 17019       3\n2     Age character      5 17019       3\n3   Age25 character      2 17019       3\n4 Offense character      2 17022       0\n5   Recid character      2 17022       0\n6    Type character      3 17022       0\n                                   distribution\n1 M (87.7%), F (12.3%)                         \n2 25-34 (36.6%), 35-44 (23.7%) ...             \n3 Over 25 (81.9%), Under 25 (18.1%)            \n4 Felony (80.6%), Misdemeanor (19.4%)          \n5 No (68.4%), Yes (31.6%)                      \n6 No Recidivism (68.4%), New (20.2%) ...       \n\nquantitative variables:  \n  name   class min  Q1 median  Q3  max     mean       sd    n missing\n1 Days integer   0 241    418 687 1095 473.3275 283.1393 5386   11636\n\n\nThere are some missing values in the variable  Age25. The  complete.cases command gives the row numbers where values are not missing. We create a new data frame omitting the rows where there is a missing value in the  ‚ÄòAge25‚Äô  variable.\n\nShow the Coderecidivism_na <- recidivism %>% tidyr::drop_na(Age25)\n\n\nAlso, the variable Recid is a factor variable coded ‚ÄúYes‚Äù or ‚ÄúNo‚Äù. We convert it to a numeric variable of 1‚Äôs and 0‚Äôs.\n\nShow the Coderecidivism_na <- recidivism_na %>% mutate(Recid2 = ifelse(Recid==\"Yes\", 1, 0))\n\nobs_diff_recid <- diffmean( Recid2 ~ Age25, data = recidivism_na)\nobs_diff_recid\n\n  diffmean \n0.05919913 \n\nShow the Codenull_dist_recid <- do(1000) * diffmean( Recid2 ~ shuffle(Age25), data = recidivism_na)\n\ngf_histogram( ~ diffmean, data = null_dist_recid) %>% \n  gf_vline(xintercept = obs_diff_recid, colour = \"red\")\n\n\n\n\n\n\nShow the CodeDiving2017 <- read.csv(\"../../../../../../materials/data/resampling/Diving2017.csv\")\nhead(Diving2017)\n\n\n\n  \n\n\nShow the Codeinspect(Diving2017)\n\n\ncategorical variables:  \n     name     class levels  n missing\n1    Name character     12 12       0\n2 Country character      8 12       0\n                                   distribution\n1  SI Yajie (8.3%) ...                         \n2 Canada (16.7%), China (16.7%) ...            \n\nquantitative variables:  \n       name   class    min       Q1  median      Q3   max    mean       sd  n\n1 Semifinal numeric 313.70 322.2000 325.625 356.575 382.8 338.500 22.94946 12\n2     Final numeric 283.35 318.5875 358.925 387.150 397.5 350.475 40.02204 12\n  missing\n1       0\n2       0\n\n\nThe data is made up of paired observations per swimmer. So we need to take the difference between the two swim records for each swimmer and then shuffle the differences to either polarity. Another way to look at this is to shuffle the records between Semifinal and Final on a per Swimmer basis.\n\nShow the CodeDiving2017\n\n\n\n  \n\n\nShow the CodeDiving2017 %>% diffmean(data = ., Final ~ Semifinal, only.2 = FALSE)\n\n  318.7-313.7  320.55-318.7 322.75-320.55  325.5-322.75  325.75-325.5 \n       12.350       -63.050         5.225        85.125      -114.150 \n   346-325.75    355.15-346 360.85-355.15  367.5-360.85   382.8-367.5 \n      102.200       -54.150        28.600        31.950         4.050 \n\nShow the Codeobs_diff_swim <- mean(~ Final - Semifinal, data = Diving2017)\nobs_diff_swim\n\n[1] 11.975\n\n\n\nShow the Codepolarity <- c(rep(1, 6), rep(-1,6))\npolarity\n\n [1]  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1\n\nShow the Codenull_dist_swim <- do(100000) * mean(data = Diving2017, \n                                    ~(Final - Semifinal) * resample(polarity,\n                                                    replace = TRUE))\nnull_dist_swim %>% head()\n\n\n\n  \n\n\nShow the Codegf_histogram(data = null_dist_swim, ~mean) %>% \n  gf_vline(xintercept = obs_diff_swim, colour = \"red\")\n\n\n\n\n\nLaGuardia Airport (LGA) is one of three major airports that serves the New York City metropolitan area. In 2008, over 23 million passengers and over 375 000 planes flew in or out of LGA. United Airlines and America Airlines are two major airlines that schedule services at LGA. The data set FlightDelays contains information on all 4029 departures of these two airlines from LGA during May and June 2009.\n\nShow the CodeflightDelays <- read.csv(\"../../../../../../materials/data/resampling/FlightDelays.csv\")\n\ninspect(flightDelays)\n\n\ncategorical variables:  \n         name     class levels    n missing\n1     Carrier character      2 4029       0\n2 Destination character      7 4029       0\n3  DepartTime character      5 4029       0\n4         Day character      7 4029       0\n5       Month character      2 4029       0\n6   Delayed30 character      2 4029       0\n                                   distribution\n1 AA (72.1%), UA (27.9%)                       \n2 ORD (44.3%), DFW (22.8%), MIA (15.1%) ...    \n3 8-Noon (26.1%), Noon-4pm (26%) ...           \n4 Fri (15.8%), Mon (15.6%), Tue (15.6%) ...    \n5 June (50.4%), May (49.6%)                    \n6 No (85.2%), Yes (14.8%)                      \n\nquantitative variables:  \n          name   class min   Q1 median   Q3  max      mean         sd    n\n1           ID integer   1 1008   2015 3022 4029 2015.0000 1163.21645 4029\n2     FlightNo integer  71  371    691  787 2255  827.1035  551.30939 4029\n3 FlightLength integer  68  155    163  228  295  185.3011   41.78783 4029\n4        Delay integer -19   -6     -3    5  693   11.7379   41.63050 4029\n  missing\n1       0\n2       0\n3       0\n4       0\n\n\nThe variables in the flightDelays dataset are:\n\nflightDelay dataset variables\n\n\n\n\n\nVariable\nDescription\n\n\n\nCarrier\nUA=United Airlines, AA=American Airlines\n\n\nFlightNo\nFlight number\n\n\nDestination\nAirport code\n\n\nDepartTime\nScheduled departure time in 4 h intervals\n\n\nDay\nDay of the Week\n\n\nMonth\nMay or June\n\n\nDelay\nMinutes flight delayed (negative indicates early departure)\n\n\nDelayed30\nDeparture delayed more than 30 min? Yes or No\n\n\nFlightLength\nLength of time of flight (minutes)\n\n\n\n\nLet us compute the proportion of times that each carrier‚Äôs flights was delayed more than 20 min. We will conduct a two-sided test to see if the difference in these proportions is statistically significant.\n\n\nShow the Codeprop(data = flightDelays, Delay >= 20 ~ Carrier)\n\nprop_TRUE.AA prop_TRUE.UA \n   0.1713696    0.2226180 \n\nShow the Codeobs_diff_delay <- diffprop(data = flightDelays, Delay >= 20 ~ Carrier)\nobs_diff_delay\n\n  diffprop \n0.05124841 \n\n\nWe see carrier AA has a 17.13% chance of delays>= 20, while UA has 22.26% chance. The difference is 5.12%. Is this statistically significant? We take the Delays for both Carriers and perform a permutation test by shuffle on the carrier variable:\n\nShow the Codenull_dist_delay <- do(10000) * diffprop(data = flightDelays, Delay >= 20 ~ shuffle(Carrier))\nnull_dist_delay %>% head()\n\n\n\n  \n\n\nShow the Codegf_histogram(data = null_dist_delay, ~ diffprop) %>% gf_vline(xintercept = obs_diff_delay, color = \"red\")\n\n\n\n\nIt appears that the difference indelay times is significant. We can compute the p-value based on this test:\n\nShow the Code2* mean(null_dist_delay >= obs_diff_delay)\n\n[1] 2e-04\n\n\nwhich is very small. Hence we reject the null Hypothesis that there is no difference between carriers on delay times.\n\nCompute the variance in the flight delay lengths for each carrier. Conduct a test to see if the variance for United Airlines differs from that of American Airlines.\n\n\nShow the Codevar(data = flightDelays, Delay ~ Carrier)\n\n      AA       UA \n1606.457 2037.525 \n\nShow the Code# There is no readymade function in mosaic called `diffvar`...so...we construct one\nobs_diff_var <- diff(var(data = flightDelays, Delay ~ Carrier))\nobs_diff_var\n\n      UA \n431.0677 \n\n\nThe difference in variances in Delay between the two carriers is \\(-431.0677\\). In our Permutation Test, we shuffle the Carrier variable:\n\nShow the Codeobs_diff_var <- diff(var(data = flightDelays, Delay ~ Carrier))\nnull_dist_var <-\n  do(10000) * diff(var(data = flightDelays, Delay ~ shuffle(Carrier)))\nnull_dist_var %>% head()\n\n\n\n  \n\n\nShow the Code# The null distribution variable is called `UA`\ngf_histogram(data = null_dist_var, ~ UA) %>% gf_vline(xintercept = obs_diff_delay, color = \"red\")\n\n\n\nShow the Code2 * mean(null_dist_var >= obs_diff_var)\n\n[1] 0.2958\n\n\nClearly there is no case for a significant difference in variances!\n\nIs there a difference in the price of groceries sold by the two retailers Target and Walmart? The data set Groceries contains a sample of grocery items and their prices advertised on their respective web sites on one specific day.\n\nInspect the data set, then explain why this is an example of matched pairs data.\nCompute summary statistics of the prices for each store.\nConduct a permutation test to determine whether or not there is a difference in the mean prices.\nCreate a histogram bar-chart of the difference in prices. What is unusual about Quaker Oats Life cereal?\nRedo the hypothesis test without this observation. Do you reach the same conclusion?\n\n\nShow the Codegroceries <- read.csv(\"../../../../../../materials/data/resampling/Groceries.csv\") %>% mutate(Product = stringr::str_squish(Product))\nhead(groceries)\n\n\n\n  \n\n\nShow the Codeinspect(groceries)\n\n\ncategorical variables:  \n     name     class levels  n missing\n1 Product character     30 30       0\n2    Size character     24 30       0\n                                   distribution\n1 Annie's Macaroni & Cheese (3.3%) ...         \n2 18oz (10%), 12oz (6.7%) ...                  \n\nquantitative variables:  \n     name   class  min     Q1 median    Q3  max     mean       sd  n missing\n1  Target numeric 0.99 1.8275  2.545 3.140 7.99 2.762333 1.582128 30       0\n2 Walmart numeric 1.00 1.7600  2.340 2.955 6.98 2.705667 1.560211 30       0\n\n\nWe see that the comparison is to be made between two prices for the same product, and hence this is one more example of paired data, as in Case Study #4. Let us plot the prices for the products:\n\nShow the Codegf_col(data = groceries,\n       Target ~ Product,\n       fill = \"#0073C299\",\n       width = 0.5 ) %>% \n  gf_col(data = groceries,\n         -Walmart ~ Product,\n         fill = \"#EFC00099\",\n         ylab = \"Prices\",\n         width = 0.5\n       ) %>% \n  gf_col(data = groceries %>% filter(Product == \"Quaker Oats Life Cereal Original\"), \n         -Walmart ~ Product,\n         fill = \"red\", \n         width = 0.5) %>% \n  gf_theme(theme_classic()) %>%\n  gf_theme(ggplot2::theme(axis.text.x = element_text(\n    size = 8,\n    face = \"bold\",\n    vjust = 0,\n    hjust = 1\n  ))) %>% gf_theme(ggplot2::coord_flip())\n\n\n\n\nWe see that the price difference between Walmart and Target prices is highest for the Product named Quaker Oats Life Cereal Original. Let us check the mean difference in prices:\n\nShow the Codediffmean(data = groceries, Walmart ~ Target, only.2 = FALSE)\n\n   1-0.99    1.22-1 1.42-1.22 1.49-1.42 1.59-1.49 1.62-1.59 1.79-1.62 1.94-1.79 \n-0.580000  0.170000  0.210000 -0.100000  0.190000  0.070000  0.180000  0.160000 \n1.99-1.94 2.12-1.99 2.39-2.12  2.5-2.39  2.59-2.5 2.64-2.59 2.79-2.64 2.82-2.79 \n 0.090000  0.010000  0.200000  0.600000 -0.200000 -0.600000  0.660000  0.040000 \n2.99-2.82 3.19-2.99 3.49-3.19 3.99-3.49 4.79-3.99 7.19-4.79 7.99-7.19 \n 0.220000  1.263333 -1.183333 -0.480000  2.290000  2.190000  0.000000 \n\nShow the Codeobs_diff_price = mean( ~ Walmart - Target, data = groceries)\nobs_diff_price\n\n[1] -0.05666667\n\n\nLet us perform the pair-wise permutation test on prices, by shuffling the two store names:\n\nShow the Codepolarity <- c(rep(1, 15), rep(-1,15))\npolarity\n\n [1]  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n[26] -1 -1 -1 -1 -1\n\nShow the Codenull_dist_price <- do(100000) * mean(data = groceries, \n                                    ~(Walmart-Target) * resample(polarity,\n                                                    replace = TRUE))\nnull_dist_price %>% head()\n\n\n\n  \n\n\nShow the Codegf_histogram(data = null_dist_price, ~mean) %>% \n  gf_vline(xintercept = obs_diff_price, colour = \"red\")\n\n\n\nShow the Code2*(sum(null_dist_price >= obs_diff_price + 1)/(100000+1)) #P-value\n\n[1] 0\n\n\nDoes not seem to be aby significant difference in prices‚Ä¶\nSuppose we knock off the Quaker Cereal data item‚Ä¶\n\nShow the Codewhich(groceries$Product == \"Quaker Oats Life Cereal Original\")\n\n[1] 2\n\nShow the Codegroceries_less <- groceries[-2,]\ngroceries_less\n\n\n\n  \n\n\nShow the Codeobs_diff_price_less = mean( ~ Walmart - Target, data = groceries_less)\nobs_diff_price_less\n\n[1] -0.1558621\n\nShow the Codepolarity_less <- c(rep(1, 15), rep(-1,14)) # Due to resampling this small bias makes no difference\nnull_dist_price_less <- do(100000) * mean(data = groceries_less, \n                                    ~(Walmart-Target) * resample(polarity_less,\n                                                    replace = TRUE))\nnull_dist_price_less %>% head()\n\n\n\n  \n\n\nShow the Codegf_histogram(data = null_dist_price_less, ~mean) %>% \n  gf_vline(xintercept = obs_diff_price_less, colour = \"red\")\n\n\n\nShow the Code1- mean(null_dist_price_less >= obs_diff_price_less) #P-value\n\n[1] 0.01489\n\n\n\nLet us try a dataset with Qualitative / Categorical data. This is a General Social Survey dataset, and we have people with different levels of Education stating their opinion on the Death Penalty. We want to know if these two Categorical variables have a correlation, i.e.¬†can the opinions in favour of the Death Penalty be explained by the Education level?\nSince data is Categorical, we need to take counts in a table, and then implement a chi-square test. In the test, we will permute the Education variable to see if we can see how significant its effect size is.\n\nShow the CodeGSS2002 <- read.csv(\"../../../../../../materials/data/resampling/GSS2002.csv\")\ninspect(GSS2002)\n\n\ncategorical variables:  \n            name     class levels    n missing\n1         Region character      7 2765       0\n2         Gender character      2 2765       0\n3           Race character      3 2765       0\n4      Education character      5 2760       5\n5        Marital character      5 2765       0\n6       Religion character     13 2746      19\n7          Happy character      3 1369    1396\n8         Income character     24 1875     890\n9       PolParty character      8 2729      36\n10      Politics character      7 1331    1434\n11     Marijuana character      2  851    1914\n12  DeathPenalty character      2 1308    1457\n13        OwnGun character      3  924    1841\n14        GunLaw character      2  916    1849\n15 SpendMilitary character      3 1324    1441\n16     SpendEduc character      3 1343    1422\n17      SpendEnv character      3 1322    1443\n18      SpendSci character      3 1266    1499\n19        Pres00 character      5 1749    1016\n20      Postlife character      2 1211    1554\n                                    distribution\n1  North Central (24.7%) ...                    \n2  Female (55.6%), Male (44.4%)                 \n3  White (79.1%), Black (14.8%) ...             \n4  HS (53.8%), Bachelors (16.1%) ...            \n5  Married (45.9%), Never Married (25.6%) ...   \n6  Protestant (53.2%), Catholic (24.5%) ...     \n7  Pretty happy (57.3%) ...                     \n8  40000-49999 (9.1%) ...                       \n9  Ind (19.3%), Not Str Dem (18.9%) ...         \n10 Moderate (39.2%), Conservative (15.8%) ...   \n11 Not legal (64%), Legal (36%)                 \n12 Favor (68.7%), Oppose (31.3%)                \n13 No (65.5%), Yes (33.5%) ...                  \n14 Favor (80.5%), Oppose (19.5%)                \n15 About right (46.5%) ...                      \n16 Too little (73.9%) ...                       \n17 Too little (60%) ...                         \n18 About right (49.7%) ...                      \n19 Bush (50.6%), Gore (44.7%) ...               \n20 Yes (80.5%), No (19.5%)                      \n\nquantitative variables:  \n  name   class min  Q1 median   Q3  max mean       sd    n missing\n1   ID integer   1 692   1383 2074 2765 1383 798.3311 2765       0\n\n\nNote how all variables are Categorical !! Education has five levels:\n\nShow the CodeGSS2002 %>% count(Education)\n\n\n\n  \n\n\nShow the CodeGSS2002 %>% count(DeathPenalty)\n\n\n\n  \n\n\n\nLet us drop NA entries in Education and Death Penalty. And set up a table for the chi-square test.\n\nShow the Codegss2002 <- GSS2002 %>% \n  dplyr::select(Education, DeathPenalty) %>% \n  tidyr::drop_na(., c(Education, DeathPenalty))\ndim(gss2002)\n\n[1] 1307    2\n\nShow the Codegss_summary <- gss2002 %>%\n  mutate(\n    Education = factor(\n      Education,\n      levels = c(\"Bachelors\", \"Graduate\", \"Jr Col\", \"HS\", \"Left HS\"),\n      labels = c(\"Bachelors\", \"Graduate\", \"Jr Col\", \"HS\", \"Left HS\")\n    ),\n    DeathPenalty = as.factor(DeathPenalty)\n  ) %>%\n  group_by(Education, DeathPenalty) %>%\n  summarise(count = n()) %>% # This is good for a chisq test\n  \n  # Add two more columns to faciltate mosaic/Marrimekko Plot\n  # \n  mutate(edu_count = sum(count), \n         edu_prop = count / sum(count)) %>%\n  ungroup() \n\ngss_summary\n\n\n\n  \n\n\nShow the Code# We can plot a heatmap-like `mosaic chart` for this table, using `ggplot`:\n# https://stackoverflow.com/questions/19233365/how-to-create-a-marimekko-mosaic-plot-in-ggplot2\n\nggplot(data = gss_summary, aes( x = Education, y = edu_prop)) +\n  geom_bar(aes(width = edu_count, fill = DeathPenalty), stat = \"identity\", position = \"fill\", colour = \"black\") +\n  geom_text(aes(label = scales::percent(edu_prop)), position = position_stack(vjust = 0.5)) +\n\n\n# if labels are desired\n facet_grid(~ Education, scales = \"free_x\", space = \"free_x\") + \n  theme(scale_fill_brewer(palette = \"RdYlGn\")) + \n  # theme(panel.spacing.x = unit(0, \"npc\")) + # if no spacing preferred between bars\n  theme_void() \n\n\n\n\nLet us now perform the base chisq test: We need a table and then the chisq test:\n\nShow the Codegss_table <- tally(DeathPenalty ~ Education, data = gss2002)\ngss_table\n\n            Education\nDeathPenalty Bachelors Graduate  HS Jr Col Left HS\n      Favor        135       64 511     71     117\n      Oppose        71       50 200     16      72\n\nShow the Code# Get the observed chi-square statistic\nobservedChi2 <- mosaic::chisq(tally(DeathPenalty ~ Education, data = gss2002))\nobservedChi2\n\nX.squared \n 23.45093 \n\nShow the Code# Actual chi-square test\nstats::chisq.test(tally(DeathPenalty ~ Education, data = gss2002))\n\n\n    Pearson's Chi-squared test\n\ndata:  tally(DeathPenalty ~ Education, data = gss2002)\nX-squared = 23.451, df = 4, p-value = 0.0001029\n\n\nWe should now repeat the test with permutations on Education:\n\nShow the Codenull_chisq <- do(10000) * chisq.test(tally(DeathPenalty ~ shuffle(Education), data = gss2002))\n\nhead(null_chisq)\n\n\n\n  \n\n\nShow the Codegf_histogram( ~ X.squared, data = null_chisq) %>% \n  gf_vline(xintercept = observedChi2, color = \"red\")\n\n\n\nShow the Codegf_histogram( ~ p.value, data = null_chisq, binwidth = 0.1, center = 0.05)\n\n\n\n\nSo we would conclude that Education has a significant effect on DeathPenalty opinion!"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/70-PermTest/index.html#introduction",
    "href": "content/courses/Analytics/Modelling/Modules/70-PermTest/index.html#introduction",
    "title": "Permutation Tests",
    "section": "Introduction",
    "text": "Introduction\nWe saw from the diagram created by Allen Downey that there is only one test! We will now use this philosophy to develop a technique that allows us to mechanize several Statistical Models in that way, with nearly identical code.\nWe will use two packages in R, mosaic and the relatively new infer package, to develop our intuition for what are called permutation based statistical tests."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/70-PermTest/index.html#hypothesis-testing-using-permutation",
    "href": "content/courses/Analytics/Modelling/Modules/70-PermTest/index.html#hypothesis-testing-using-permutation",
    "title": "Permutation Tests",
    "section": "Hypothesis Testing using Permutation",
    "text": "Hypothesis Testing using Permutation\nFrom Reference #1:\n\nHypothesis testing can be thought of as a 4-step process:\n\nState the null and alternative hypotheses.\nCompute a test statistic.\nDetermine the p-value.\n\nDraw a conclusion.\nIn a traditional introductory statistics course, once this general framework has been mastered, the main work is in applying the correct formula to compute the standard test statistics in step 2 and using a table or computer to determine the p-value based on the known (usually approximate) theoretical distribution of the test statistic under the null hypothesis.\nIn a simulation-based approach, steps 2 and 3 change. In Step 2, it is no longer required that the test statistic be normalized to conform with a known, named distribution. Instead, natural test statistics, like the difference between two sample means \\(y1 ‚àí y2\\) can be used.\nIn Step 3, we use randomization to approximate the sampling distribution of the test statistic. Our lady tasting tea example demonstrates how this can be done from first principles. More typically, we will use randomization to create new simulated data sets ( ‚ÄúParallel Worlds‚Äù) that are like our original data in some ways, but make the null hypothesis true. For each simulated data set, we calculate our test statistic, just as we did for the original sample. Together, this collection of test statistics computed from the simulated samples constitute our randomization distribution.\nWhen creating a randomization distribution, we will attempt to satisfy 3 guiding principles.\n\nBe consistent with the null hypothesis. We need to simulate a world in which the null hypothesis is true. If we don‚Äôt do this, we won‚Äôt be testing our null hypothesis.\nUse the data in the original sample. The original data should shed light on some aspects of the distribution that are not determined by null hypothesis. For example, a null hypothesis about a mean doesn‚Äôt tell us about the shape of the population distribution, but the data give us some indication.\nReflect the way the original data were collected.\n\n\nFrom Chihara and Hesterberg:\n\nThis is the core idea of statistical significance or classical hypothesis testing ‚Äì to calculate how often pure random chance would give an effect as large as that observed in the data, in the absence of any real effect. If that probability is small enough, we conclude that the data provide convincing evidence of a real effect.\n\nPermutations tests using mosaic::shuffle()\n\nThe mosaic package provides the shuffle() function as a synonym for sample(). When used without additional arguments, this will permute its first argument.\n\n# library(mosaic)\nshuffle(1:10)\n\n [1]  5  3  2  9  6 10  4  8  1  7\n\n\nApplying shuffle() to an explanatory variable in a model allows us to test the null hypothesis that the explanatory variable has, in fact, no explanatory power. This idea can be used to test\n\nthe equivalence of two or more means,\nthe equivalence of two or more proportions,\nwhether a regression parameter is 0. (Correlations between two variables)\n\nWe will now see examples of each of these models using Permutations."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/80-Prop/files/cat.html",
    "href": "content/courses/Analytics/Modelling/Modules/80-Prop/files/cat.html",
    "title": "üÉè Permutation Test for Two Proportions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(ggmosaic) # plotting mosaic plots for Categorical Data\n\n### Dataset from Chihara and Hesterberg's book (Second Edition)\nlibrary(resampledata)"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/80-Prop/files/cat.html#introduction",
    "href": "content/courses/Analytics/Modelling/Modules/80-Prop/files/cat.html#introduction",
    "title": "üÉè Permutation Test for Two Proportions",
    "section": "Introduction",
    "text": "Introduction\nWe saw from the diagram created by Allen Downey that there is only one test! We will now use this philosophy to develop a technique that allows us to mechanize several Statistical Models in that way, with nearly identical code.\nWe will use two packages in R, mosaic and the relatively new infer package, to develop our intuition for what are called permutation based statistical tests."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/80-Prop/files/cat.html#testing-for-two-or-more-proportions",
    "href": "content/courses/Analytics/Modelling/Modules/80-Prop/files/cat.html#testing-for-two-or-more-proportions",
    "title": "üÉè Permutation Test for Two Proportions",
    "section": "Testing for Two or More Proportions",
    "text": "Testing for Two or More Proportions\nLet us try a dataset with Qualitative / Categorical data. This is the General Social Survey GSS dataset, and we have people with different levels of Education stating their opinion on the Death Penalty. We want to know if these two Categorical variables have a correlation, i.e. can the opinions in favour of the Death Penalty be explained by the Education level?\nSince data is Categorical ( both variables ), we need to take counts in a table, and then implement a chi-square test. In the test, we will permute the Education variable to see if we can see how significant its effect size is.\n\ndata(GSS2002)\ninspect(GSS2002)\n\n\ncategorical variables:  \n            name  class levels    n missing\n1         Region factor      7 2765       0\n2         Gender factor      2 2765       0\n3           Race factor      3 2765       0\n4      Education factor      5 2760       5\n5        Marital factor      5 2765       0\n6       Religion factor     13 2746      19\n7          Happy factor      3 1369    1396\n8         Income factor     24 1875     890\n9       PolParty factor      8 2729      36\n10      Politics factor      7 1331    1434\n11     Marijuana factor      2  851    1914\n12  DeathPenalty factor      2 1308    1457\n13        OwnGun factor      3  924    1841\n14        GunLaw factor      2  916    1849\n15 SpendMilitary factor      3 1324    1441\n16     SpendEduc factor      3 1343    1422\n17      SpendEnv factor      3 1322    1443\n18      SpendSci factor      3 1266    1499\n19        Pres00 factor      5 1749    1016\n20      Postlife factor      2 1211    1554\n                                    distribution\n1  North Central (24.7%) ...                    \n2  Female (55.6%), Male (44.4%)                 \n3  White (79.1%), Black (14.8%) ...             \n4  HS (53.8%), Bachelors (16.1%) ...            \n5  Married (45.9%), Never Married (25.6%) ...   \n6  Protestant (53.2%), Catholic (24.5%) ...     \n7  Pretty happy (57.3%) ...                     \n8  40000-49999 (9.1%) ...                       \n9  Ind (19.3%), Not Str Dem (18.9%) ...         \n10 Moderate (39.2%), Conservative (15.8%) ...   \n11 Not legal (64%), Legal (36%)                 \n12 Favor (68.7%), Oppose (31.3%)                \n13 No (65.5%), Yes (33.5%) ...                  \n14 Favor (80.5%), Oppose (19.5%)                \n15 About right (46.5%) ...                      \n16 Too little (73.9%) ...                       \n17 Too little (60%) ...                         \n18 About right (49.7%) ...                      \n19 Bush (50.6%), Gore (44.7%) ...               \n20 Yes (80.5%), No (19.5%)                      \n\nquantitative variables:  \n  name   class min  Q1 median   Q3  max mean       sd    n missing\n1   ID integer   1 692   1383 2074 2765 1383 798.3311 2765       0\n\n\nNote how all variables are Categorical !! Education has five levels:\n\nGSS2002 %>% count(Education)\n\n\n\n  \n\n\nGSS2002 %>% count(DeathPenalty)\n\n\n\n  \n\n\n\nLet us drop NA entries in Education and Death Penalty. And set up a table for the chi-square test.\n\ngss2002 <- GSS2002 %>% \n  dplyr::select(Education, DeathPenalty) %>% \n  tidyr::drop_na(., c(Education, DeathPenalty))\ndim(gss2002)\n\n[1] 1307    2\n\ngss_summary <- gss2002 %>%\n  mutate(\n    Education = factor(\n      Education,\n      levels = c(\"Bachelors\", \"Graduate\", \"Jr Col\", \"HS\", \"Left HS\"),\n      labels = c(\"Bachelors\", \"Graduate\", \"Jr Col\", \"HS\", \"Left HS\")\n    ),\n    DeathPenalty = as.factor(DeathPenalty)\n  ) %>%\n  group_by(Education, DeathPenalty) %>%\n  summarise(count = n()) %>% # This is good for a chisq test\n  \n  # Add two more columns to facilitate mosaic/Marrimekko Plot\n  # \n  mutate(edu_count = sum(count), \n         edu_prop = count / sum(count)) %>%\n  ungroup() \n\ngss_summary"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/80-Prop/files/cat.html#sec-table-plots",
    "href": "content/courses/Analytics/Modelling/Modules/80-Prop/files/cat.html#sec-table-plots",
    "title": "üÉè Permutation Test for Two Proportions",
    "section": "Table Plots",
    "text": "Table Plots\nWe can plot a heatmap-like mosaic chart for this table.\nUsing ggplot\n\n\n# https://stackoverflow.com/questions/19233365/how-to-create-a-marimekko-mosaic-plot-in-ggplot2\n\nggplot(data = gss_summary, aes( x = Education, y = edu_prop)) +\n  \n  geom_bar(aes(width = edu_count, fill = DeathPenalty), \n           stat = \"identity\", \n           position = \"fill\", \n           colour = \"black\") +\n  \n  geom_text(aes(label = scales::percent(edu_prop)), \n            position = position_stack(vjust = 0.5)) +\n\n\n# if labels are desired\n facet_grid(~ Education, scales = \"free_x\", space = \"free_x\") + \n  theme(scale_fill_brewer(palette = \"RdYlGn\")) + \n  # theme(panel.spacing.x = unit(0, \"npc\")) + # if no spacing preferred between bars\n  theme_void() \n\n\n\n\nUsing ggmosaic\n\n\n#library(ggmosaic)\n\nggplot(data = gss2002) +\n  geom_mosaic(aes(x = product(DeathPenalty, Education), fill = DeathPenalty))"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/80-Prop/files/cat.html#section",
    "href": "content/courses/Analytics/Modelling/Modules/80-Prop/files/cat.html#section",
    "title": "üÉè Permutation Test for Two Proportions",
    "section": "",
    "text": "Observed Statistic: the X^2 metric\nWhen there are multiple proportions involved, the X^2 test is what is used.\nLet us now perform the base chisq test: We need a table and then the chisq test:\n\ngss_table <- tally(DeathPenalty ~ Education, data = gss2002)\ngss_table\n\n            Education\nDeathPenalty Left HS  HS Jr Col Bachelors Graduate\n      Favor      117 511     71       135       64\n      Oppose      72 200     16        71       50\n\n# Get the observed chi-square statistic\nobservedChi2 <- mosaic::chisq(tally(DeathPenalty ~ Education, data = gss2002))\nobservedChi2\n\nX.squared \n 23.45093 \n\n# Actual chi-square test\nstats::chisq.test(tally(DeathPenalty ~ Education, data = gss2002))\n\n\n    Pearson's Chi-squared test\n\ndata:  tally(DeathPenalty ~ Education, data = gss2002)\nX-squared = 23.451, df = 4, p-value = 0.0001029\n\n\nWhat would our Hypotheses be?\n$$ H_0: Education¬†Does¬†Not¬†affect¬†Votes¬†on¬†Death¬†Penalty\\\nH_a: Education¬†affects¬†Votes¬†on¬†Death¬†Penalty\n$$\nWe should now repeat the test with permutations on Education:\n\nnull_chisq <- do(10000) * chisq.test(tally(DeathPenalty ~ shuffle(Education), data = gss2002))\n\nhead(null_chisq)\n\n\n\n  \n\n\ngf_histogram( ~ X.squared, data = null_chisq) %>% \n  gf_vline(xintercept = observedChi2, color = \"red\")\n\n\n\nprop1(~ X.squared >= observedChi2, data = null_chisq)\n\n prop_TRUE \n0.00029997 \n\n\nThe p-value is well below our threshold of \\(0.05%\\), so we would conclude that Education has a significant effect on DeathPenalty opinion!"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/80-Prop/files/cat.html#conclusion",
    "href": "content/courses/Analytics/Modelling/Modules/80-Prop/files/cat.html#conclusion",
    "title": "üÉè Permutation Test for Two Proportions",
    "section": "Conclusion",
    "text": "Conclusion\nSo, what do you think?"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/80-Prop/index.html",
    "href": "content/courses/Analytics/Modelling/Modules/80-Prop/index.html",
    "title": "üÉè Permutation Test for Two Proportions",
    "section": "",
    "text": "R Tutorial¬†¬†\n  Orange Tutorial\n\n  Radiant Tutorial¬†\n  Datasets"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/80-Prop/index.html#iconify-noto-v1-package-setting-up-r-packages",
    "href": "content/courses/Analytics/Modelling/Modules/80-Prop/index.html#iconify-noto-v1-package-setting-up-r-packages",
    "title": "üÉè Permutation Test for Two Proportions",
    "section": "\n Setting up R packages",
    "text": "Setting up R packages\n\nknitr::opts_chunk$set(echo = TRUE,message = TRUE,warning = TRUE, fig.align = \"center\")\noptions(digits=2)\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(ggmosaic) # plotting mosaic plots for Categorical Data\nlibrary(vcd) # Creating Tables and plotting mosaic charts\n\n### Dataset from Chihara and Hesterberg's book (Second Edition)\nlibrary(resampledata)"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/80-Prop/index.html#introduction",
    "href": "content/courses/Analytics/Modelling/Modules/80-Prop/index.html#introduction",
    "title": "üÉè Permutation Test for Two Proportions",
    "section": "Introduction",
    "text": "Introduction\nWe saw from the diagram created by Allen Downey that there is only one test! We will now use this philosophy to develop a technique that allows us to mechanize several Statistical Models in that way, with nearly identical code.\nWe will use two packages in R, mosaic to develop our intuition for what are called permutation based statistical tests. (There is also a more recent package called infer in R which can do pretty much all of this, including visualization. In my opinion, the code is a little too high-level and does not offer quite the detailed insight that the mosaic package does)."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/80-Prop/index.html#testing-for-two-or-more-proportions",
    "href": "content/courses/Analytics/Modelling/Modules/80-Prop/index.html#testing-for-two-or-more-proportions",
    "title": "üÉè Permutation Test for Two Proportions",
    "section": "Testing for Two or More Proportions",
    "text": "Testing for Two or More Proportions\nLet us try a dataset with Qualitative / Categorical data. This is the General Social Survey GSS dataset, and we have people with different levels of Education stating their opinion on the Death Penalty. We want to know if these two Categorical variables have a correlation, i.e. can the opinions in favour of the Death Penalty be explained by the Education level?\nSince data is Categorical ( both variables ), we need to take counts in a table, and then implement a chi-square test. In the test, we will permute the Education variable to see if we can see how significant its effect size is.\n\ndata(GSS2002)\ninspect(GSS2002)\n\n\ncategorical variables:  \n            name  class levels    n missing\n1         Region factor      7 2765       0\n2         Gender factor      2 2765       0\n3           Race factor      3 2765       0\n4      Education factor      5 2760       5\n5        Marital factor      5 2765       0\n6       Religion factor     13 2746      19\n7          Happy factor      3 1369    1396\n8         Income factor     24 1875     890\n9       PolParty factor      8 2729      36\n10      Politics factor      7 1331    1434\n11     Marijuana factor      2  851    1914\n12  DeathPenalty factor      2 1308    1457\n13        OwnGun factor      3  924    1841\n14        GunLaw factor      2  916    1849\n15 SpendMilitary factor      3 1324    1441\n16     SpendEduc factor      3 1343    1422\n17      SpendEnv factor      3 1322    1443\n18      SpendSci factor      3 1266    1499\n19        Pres00 factor      5 1749    1016\n20      Postlife factor      2 1211    1554\n                                    distribution\n1  North Central (24.7%) ...                    \n2  Female (55.6%), Male (44.4%)                 \n3  White (79.1%), Black (14.8%) ...             \n4  HS (53.8%), Bachelors (16.1%) ...            \n5  Married (45.9%), Never Married (25.6%) ...   \n6  Protestant (53.2%), Catholic (24.5%) ...     \n7  Pretty happy (57.3%) ...                     \n8  40000-49999 (9.1%) ...                       \n9  Ind (19.3%), Not Str Dem (18.9%) ...         \n10 Moderate (39.2%), Conservative (15.8%) ...   \n11 Not legal (64%), Legal (36%)                 \n12 Favor (68.7%), Oppose (31.3%)                \n13 No (65.5%), Yes (33.5%) ...                  \n14 Favor (80.5%), Oppose (19.5%)                \n15 About right (46.5%) ...                      \n16 Too little (73.9%) ...                       \n17 Too little (60%) ...                         \n18 About right (49.7%) ...                      \n19 Bush (50.6%), Gore (44.7%) ...               \n20 Yes (80.5%), No (19.5%)                      \n\nquantitative variables:  \n  name   class min  Q1 median   Q3  max mean       sd    n missing\n1   ID integer   1 692   1383 2074 2765 1383 798.3311 2765       0\n\n\nNote how all variables are Categorical !! Education has five levels, and of course DeathPenalty has three:\n\nGSS2002 %>% count(Education)\n\n\n\n  \n\n\nGSS2002 %>% count(DeathPenalty)\n\n\n\n  \n\n\n\nLet us drop NA entries in Education and Death Penalty and set up a Contingency Table.\n\ngss2002 <- GSS2002 %>% \n  dplyr::select(Education, DeathPenalty) %>% \n  tidyr::drop_na(., c(Education, DeathPenalty))\n\n\n\ngss_table <- tally(DeathPenalty ~ Education, data = gss2002)\ngss_table %>% \n  addmargins()\n\n            Education\nDeathPenalty Left HS   HS Jr Col Bachelors Graduate  Sum\n      Favor      117  511     71       135       64  898\n      Oppose      72  200     16        71       50  409\n      Sum        189  711     87       206      114 1307"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/80-Prop/index.html#contingency-table-plots",
    "href": "content/courses/Analytics/Modelling/Modules/80-Prop/index.html#contingency-table-plots",
    "title": "üÉè Permutation Test for Two Proportions",
    "section": "Contingency Table Plots",
    "text": "Contingency Table Plots\nThe Contingency Table can be plotted, as we have seen, using a mosaic plot using several packages:\n\n\nUsing ggformula\nUsing vcd\nUsing ggmosaic\n\n\n\nNeed a little more work, to convert the Contigency Table into a tibble:\n\n# https://stackoverflow.com/questions/19233365/how-to-create-a-marimekko-mosaic-plot-in-ggplot2\n\ngss_summary <- gss2002 %>%\n  mutate(\n    Education = factor(\n      Education,\n      levels = c(\"Bachelors\", \"Graduate\", \"Jr Col\", \"HS\", \"Left HS\"),\n      labels = c(\"Bachelors\", \"Graduate\", \"Jr Col\", \"HS\", \"Left HS\")\n    ),\n    DeathPenalty = as.factor(DeathPenalty)\n  ) %>%\n  group_by(Education, DeathPenalty) %>%\n  summarise(count = n()) %>% # This is good for a chisq test\n  \n  # Add two more columns to facilitate mosaic/Marrimekko Plot\n  # \n  mutate(edu_count = sum(count), \n         edu_prop = count / sum(count)) %>%\n  ungroup() \n\ngf_col(edu_prop ~ Education, data = gss_summary,\n       width = ~ edu_count, \n       fill = ~ DeathPenalty,\n       stat = \"identity\", \n       position = \"fill\", \n       color = \"black\") %>% \n  \n  gf_text(edu_prop ~ Education, \n          label = ~ scales::percent(edu_prop),\n          position = position_stack(vjust = 0.5)) %>% \n  \n  gf_facet_grid(~ Education, \n                scales = \"free_x\", \n                space = \"free_x\") %>% \n  \n  gf_theme(scale_fill_manual(values = c(\"orangered\", \"palegreen3\"))) %>% \n  gf_theme(theme_void())\n\n\n\n\n\n\n\nvcd::mosaic(gss_table, gp = shading_hsv)\n\n\n\n\n\n\n\n#library(ggmosaic)\n\nggplot(data = gss2002) +\n  geom_mosaic(aes(x = product(DeathPenalty, Education), \n                  fill = DeathPenalty))"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/80-Prop/index.html#observed-statistic-the-x2-metric",
    "href": "content/courses/Analytics/Modelling/Modules/80-Prop/index.html#observed-statistic-the-x2-metric",
    "title": "üÉè Permutation Test for Two Proportions",
    "section": "Observed Statistic: the \\(X^2\\) metric",
    "text": "Observed Statistic: the \\(X^2\\) metric\nWhen there are multiple proportions involved, the \\(X^2\\) test is what is used.\n\n\nIntuitive Explanation\nCode\n\n\n\nLet us look at the Contingency Table that we have:\n\n\n\n\n\n \n   \n    Left HS \n    HS \n    Jr Col \n    Bachelors \n    Graduate \n    Sum \n  \n\n\n Favor \n    117 \n    511 \n    71 \n    135 \n    64 \n    898 \n  \n\n Oppose \n    72 \n    200 \n    16 \n    71 \n    50 \n    409 \n  \n\n Sum \n    189 \n    711 \n    87 \n    206 \n    114 \n    1307 \n  \n\n\nContigency Table\n\nIn the chi-square test, we check whether the two ( or more ) categorical variables are independent. To do this we perform a simple check on the Contingency Table. We first re-compute the totals in each row and column, based on what we could expect if there was independence (NULL Hypothesis). If the two variables were independent, then there should be no difference between real and expected scores.\nHow do we know what scores to expect?\nConsider the entry in location (1,1): 117. The number of expected entries there is probability of an entry landing in that square times the total number of entries:\n\n\\[\\begin{align}\n\n\\text{Expected Value at location[1,1]}\n&= p_{row_1} * p_{col_1} * \\text{Total Scores}\\\\\\\n&= \\frac{\\text{Row-1-Total}}{\\text{Total Scores}} * \\frac{\\text{Col-1-Total}}{\\text{Total Scores}} * \\text{Total Scores}\\\\\\\n&= \\frac{898}{1307} * \\frac{189}{1307} * 1307\\\\\\\n&= 130\n\n\n\\end{align}\\]\n\nProceeding in this way for all the 15 entries in the Contingency Table, we get the ‚ÄúExpected‚Äù Contingency Table. Here are both tables for comparison:\n\n\n\n\n\n\n\n\n \n   \n    Left HS \n    HS \n    Jr Col \n    Bachelors \n    Graduate \n    Sum \n  \n\n\n Favor \n    130 \n    489 \n    60 \n    142 \n    78 \n    898 \n  \n\n Oppose \n    59 \n    222 \n    27 \n    64 \n    36 \n    409 \n  \n\n Sum \n    189 \n    711 \n    87 \n    206 \n    114 \n    1307 \n  \n\n\nExpected Contigency Table\n\n\n\n\n\n\n \n   \n    Left HS \n    HS \n    Jr Col \n    Bachelors \n    Graduate \n    Sum \n  \n\n\n Favor \n    117 \n    511 \n    71 \n    135 \n    64 \n    898 \n  \n\n Oppose \n    72 \n    200 \n    16 \n    71 \n    50 \n    409 \n  \n\n Sum \n    189 \n    711 \n    87 \n    206 \n    114 \n    1307 \n  \n\n\nActual Contigency Table\n\nThe \\(X^2\\) statistic is sum of squared differences between Observed and Expected scores, scaled by the Expected Scores. For location [1,1] this would be: \\((117-130)^2/189\\). Do try to compute all of these and the \\(X^2\\) statistic by hand !!\n\n\nLet us now perform the base chisq test: We need a table and then the chisq test:\n\n# gss_table <- tally(DeathPenalty ~ Education, data = gss2002)\n# gss_table\n\n# Get the observed chi-square statistic\nobservedChi2 <- mosaic::chisq(tally(DeathPenalty ~ Education, data = gss2002))\nobservedChi2\n\nX.squared \n 23.45093 \n\n# Actual chi-square test\nstats::chisq.test(tally(DeathPenalty ~ Education, data = gss2002))\n\n\n    Pearson's Chi-squared test\n\ndata:  tally(DeathPenalty ~ Education, data = gss2002)\nX-squared = 23.451, df = 4, p-value = 0.0001029\n\n\nWe see that our observed \\(X^2 = 23.45\\).\n\n\n\nHypotheses Definition\nWhat would our Hypotheses be?\n\\(H_0: \\text{Education does not affect votes for Death Penalty}\\) \\(H_a: \\text{Education affects votes for Death Penalty}\\)\nPermutation Test for Education\n\nWe should now repeat the test with permutations on Education:\n\nnull_chisq <- do(10000) * \n  chisq.test(tally(DeathPenalty ~ shuffle(Education), \n                   data = gss2002))\n\nhead(null_chisq)\n\n\n\n  \n\n\ngf_histogram( ~ X.squared, data = null_chisq) %>% \n  \n  gf_vline(xintercept = observedChi2, \n           color = \"red\") %>% \n  gf_theme(theme = theme_classic())\n\n\n\nprop1(~ X.squared >= observedChi2, data = null_chisq)\n\n prop_TRUE \n0.00019998 \n\n\nThe p-value is well below our threshold of \\(0.05\\), so we would conclude that Education has a significant effect on DeathPenalty opinion!"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/80-Prop/index.html#conclusion",
    "href": "content/courses/Analytics/Modelling/Modules/80-Prop/index.html#conclusion",
    "title": "üÉè Permutation Test for Two Proportions",
    "section": "Conclusion",
    "text": "Conclusion\nWhy would a permutation test be a good idea here?\nIn our basic \\(X^2\\) test, we calculate the test statistic of \\(X^2\\) and look up a theoretical null distribution for that statistic, and see how unlikely our observed value is.\nWith a permutation test, there are no assumptions of the null distribution: this is computed based on real data. We note in passing that, in this case, since the number of cases in each cell of the Contingency Table are fairly high ( >= 5) the resulting NULL distribution is of the \\(X^2\\) variety."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/80-Prop/index.html#references",
    "href": "content/courses/Analytics/Modelling/Modules/80-Prop/index.html#references",
    "title": "üÉè Permutation Test for Two Proportions",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/90-Means/files/inf_for_numerical_data.html",
    "href": "content/courses/Analytics/Modelling/Modules/90-Means/files/inf_for_numerical_data.html",
    "title": "Inference for numerical data",
    "section": "",
    "text": "In this lab, we will explore and visualize the data using the tidyverse suite of packages, and perform statistical inference using infer. The data can be found in the companion package for OpenIntro resources, openintro.\nLet‚Äôs load the packages.\n\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(openintro)\nlibrary(infer)\nlibrary(skimr)\n\n\nTo create your new lab report, in RStudio, go to New File -> R Markdown‚Ä¶ Then, choose From Template and then choose Lab Report for OpenIntro Statistics Labs from the list of templates.\n\nEvery two years, the Centers for Disease Control and Prevention conduct the Youth Risk Behavior Surveillance System (YRBSS) survey, where it takes data from high schoolers (9th through 12th grade), to analyze health patterns. You will work with a selected group of variables from a random sample of observations during one of the years the YRBSS was conducted.\nLoad the yrbss data set into your workspace.\n\ndata(yrbss)"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/90-Means/files/inf_for_numerical_data.html#exploratory-data-analysis",
    "href": "content/courses/Analytics/Modelling/Modules/90-Means/files/inf_for_numerical_data.html#exploratory-data-analysis",
    "title": "Inference for numerical data",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nThere are observations on 13 different variables, some categorical and some numerical. The meaning of each variable can be found by bringing up the help file: type this in your console\n\n\n\n\n\n\nNote\n\n\n\nhelp(yrbss)\n\n\n\n\n\n\n\n\nNote\n\n\n\n1 . What are the cases in this data set? How many cases are there in our sample?\n\n\nYou will first start with analyzing the weight of the participants in kilograms: weight.\nUsing visualization and summary statistics, describe the distribution of weights. The inspect() function from the mosaic package produces nice summaries of the variables in the dataset, separating categorical (character) variables from quantitative variables.\n\nmosaic::inspect(yrbss)\n\n\ncategorical variables:  \n                      name     class levels     n missing\n1                   gender character      2 13571      12\n2                    grade character      5 13504      79\n3                 hispanic character      2 13352     231\n4                     race character      5 10778    2805\n5               helmet_12m character      6 13272     311\n6   text_while_driving_30d character      8 12665     918\n7  hours_tv_per_school_day character      7 13245     338\n8 school_night_hours_sleep character      7 12335    1248\n                                   distribution\n1 male (51.2%), female (48.8%)                 \n2 9 (26.6%), 12 (26.3%), 11 (23.6%) ...        \n3 not (74.4%), hispanic (25.6%)                \n4 White (59.5%) ...                            \n5 never (52.6%), did not ride (34.3%) ...      \n6 0 (37.8%), did not drive (36.7%) ...         \n7 2 (20.4%), <1 (16.4%), 3 (16.1%) ...         \n8 7 (28.1%), 8 (21.8%), 6 (21.5%) ...          \n\nquantitative variables:  \n                  name   class   min    Q1 median    Q3    max      mean\n1                  age integer 12.00 15.00  16.00 17.00  18.00 16.157041\n2               height numeric  1.27  1.60   1.68  1.78   2.11  1.691241\n3               weight numeric 29.94 56.25  64.41 76.20 180.99 67.906503\n4 physically_active_7d integer  0.00  2.00   4.00  7.00   7.00  3.903005\n5 strength_training_7d integer  0.00  0.00   3.00  5.00   7.00  2.949948\n          sd     n missing\n1  1.2637373 13506      77\n2  0.1046973 12579    1004\n3 16.8982128 12579    1004\n4  2.5641046 13310     273\n5  2.5768522 12407    1176\n\n\nNext, consider the possible relationship between a high schooler‚Äôs weight and their physical activity. Plotting the data is a useful first step because it helps us quickly visualize trends, identify strong associations, and develop research questions.\nFirst, let‚Äôs create a new variable physical_3plus, which will be coded as either ‚Äúyes‚Äù if the student is physically active for at least 3 days a week, and ‚Äúno‚Äù if not. Recall that we have several missing data in that column, so we will (sadly) drop these before generating the new variable:\n\nyrbss <- yrbss %>% \n  drop_na() %>% \n  mutate(physical_3plus = if_else(physically_active_7d >= 2, \"yes\", \"no\"),\n         physical_3plus = factor(physical_3plus, \n                                 labels = c(\"yes\", \"no\"),\n                                 levels = c(\"yes\", \"no\")))\n# Let us check\nyrbss %>% count(physical_3plus)\n\n\n\n  \n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nMake a side-by-side violin box plots of physical_3plus and weight.\nIs there a relationship between these two variables? What did you expect and why?\n\n\n\n\ngf_boxplot(weight ~ physical_3plus, \n          fill = ~ physical_3plus,\n          data = yrbss,\n          draw_quantiles = TRUE)\n\nWarning: Removed 946 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nThe box plots show how the medians of the two distributions compare, but we can also compare the means of the distributions using the following to first group the data by the physical_3plus variable, and then calculate the mean weight in these groups using the mean function while ignoring missing values by setting the na.rm argument to TRUE.\n\nyrbss %>%\n  group_by(physical_3plus) %>%\n  summarise(mean_weight = mean(weight, na.rm = TRUE))\n\n\n\n  \n\n\n\nThere is an observed difference, but is this difference large enough to deem it ‚Äústatistically significant‚Äù? In order to answer this question we will conduct a hypothesis test."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/90-Means/files/inf_for_numerical_data.html#inference",
    "href": "content/courses/Analytics/Modelling/Modules/90-Means/files/inf_for_numerical_data.html#inference",
    "title": "Inference for numerical data",
    "section": "Inference",
    "text": "Inference\n\n\n\n\n\n\nImportant\n\n\n\nAre all conditions necessary for inference satisfied? Comment on each. You can compute the group sizes with the summarize command above by defining a new variable with the definition n().\n\n\n\n\n\n\n\n\nNote\n\n\n\nWrite the hypotheses for testing if the average weights are different for those who exercise at least times a week and those who don‚Äôt.\nWrite here !\n\n\nWe will do this in two ways, just for fun: one using mosaic and the other using infer.\nBut first, we need to initialize the test, which we will save as obs_diff.\n\nobs_diff_infer <- yrbss %>%\n  specify(weight ~ physical_3plus) %>%\n  calculate(stat = \"diff in means\", order = c(\"yes\", \"no\"))\nobs_diff_infer\n\n\n\n  \n\n\nobs_diff_mosaic <- diffmean(~ weight | physical_3plus, data = yrbss)\nobs_diff_mosaic\n\ndiffmean \n1.694383 \n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that obs_diff_infer is a 1 X 1 dataframe; obs_diff_mosaic is a scalar!!\n\n\n\n\nInference Using `infer`\nInference Using `mosaic`\n\n\n\nNext, we will work through creating a permutation distribution using tools from the infer package.\nRecall that the specify() function is used to specify the variables you are considering (notated y ~x), and you can use the calculate() function to specify the statistic you want to calculate and the order of subtraction you want to use. For this hypothesis, the statistic you are searching for is the difference in means, with the order being yes - no.\nAfter you have calculated your observed statistic, you need to create a permutation distribution. This is the distribution that is created by shuffling the observed weights into new physical_3plus groups, labeled ‚Äúyes‚Äù and ‚Äúno‚Äù.\nWe will save the permutation distribution as null_dist.\n\nnull_dist <- yrbss %>%\n  specify(weight ~ physical_3plus) %>%\n  hypothesize(null = \"independence\") %>%\n  generate(reps = 1000, type = \"permute\") %>%\n  calculate(stat = \"diff in means\", order = c(\"yes\", \"no\"))\n\nThe hypothesize() function is used to declare what the null hypothesis is. Here, we are assuming that student‚Äôs weight is independent of whether they exercise at least 3 days or not.\nWe should also note that the type argument within generate() is set to \"permute\". This ensures that the statistics calculated by the calculate() function come from a reshuffling of the data (not a resampling of the data)! Finally, the specify() and calculate() steps should look familiar, since they are the same as what we used to find the observed difference in means!\nWe can visualize this null distribution with the following code:\n\ngf_histogram(data = null_dist, ~ stat)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nAdd a vertical red line to the plot above, demonstrating where the observed difference in means (obs_diff_mosaic) falls on the distribution.\nHow many of these null_dist permutations have a difference at least as large (or larger) as obs_diff_mosaic?\n\n\n\n\n\n\nNow that you have calculated the observed statistic and generated a permutation distribution, you can calculate the p-value for your hypothesis test using the function get_p_value() from the infer package.\n\nnull_dist %>%\n  get_p_value(obs_stat = obs_diff_infer, direction = \"two_sided\")\n\nWarning: Please be cautious in reporting a p-value of 0. This result is an\napproximation based on the number of `reps` chosen in the `generate()` step.\nSee `?get_p_value()` for more information.\n\n\n\n\n  \n\n\n\n\n\n\n\nWhat warning message do you get? Why do you think you get this warning message?\nConstruct and record a confidence interval for the difference between the weights of those who exercise at least three times a week and those who don‚Äôt, and interpret this interval in context of the data.\n\n\n\n\nWe already have the observed difference, obs_diff_mosaic. Now we generate the null distribution using permutation, with mosaic:\n\nnull_dist_mosaic <- do(1000) * diffmean(~ weight | shuffle(physical_3plus), data = yrbss)\n\nWe can also generate the histogram of the null distribution, compare that with the observed diffrence and compute the p-value and confidence intervals:\n\ngf_histogram(~ diffmean, data = null_dist_mosaic) %>% \n  gf_vline(xintercept = obs_diff_mosaic, colour = \"red\")\n\n\n\n# p-value\nprop(~ diffmean >= obs_diff_mosaic, data = null_dist_mosaic)\n\nprop_TRUE \n        0 \n\n# Confidence Intervals for p = 0.95\nmosaic::cdata(~ diffmean, p = 0.95, data = null_dist_mosaic)"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/90-Means/files/inf_for_numerical_data.html#more-practice",
    "href": "content/courses/Analytics/Modelling/Modules/90-Means/files/inf_for_numerical_data.html#more-practice",
    "title": "Inference for numerical data",
    "section": "More Practice",
    "text": "More Practice\n\nCalculate a 95% confidence interval for the average height in meters (height) and interpret it in context.\nCalculate a new confidence interval for the same parameter at the 90% confidence level. Comment on the width of this interval versus the one obtained in the previous exercise.\nConduct a hypothesis test evaluating whether the average height is different for those who exercise at least three times a week and those who don‚Äôt.\nNow, a non-inference task: Determine the number of different options there are in the dataset for the hours_tv_per_school_day there are.\nCome up with a research question evaluating the relationship between height or weight and sleep. Formulate the question in a way that it can be answered using a hypothesis test and/or a confidence interval. Report the statistical results, and also provide an explanation in plain language. Be sure to check all assumptions, state your \\(\\alpha\\) level, and conclude in context."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/90-Means/files/two-means.html",
    "href": "content/courses/Analytics/Modelling/Modules/90-Means/files/two-means.html",
    "title": "Permutation Tests for Two Means",
    "section": "",
    "text": "library(ggplot2)\nlibrary(dplyr)\nlibrary(mosaic)\n\nlibrary(resampledata)"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/90-Means/files/two-means.html#case-study-1-verizon",
    "href": "content/courses/Analytics/Modelling/Modules/90-Means/files/two-means.html#case-study-1-verizon",
    "title": "Permutation Tests for Two Means",
    "section": "Case Study-1: Verizon",
    "text": "Case Study-1: Verizon\nDoes Verizon create a difference in Repair Times between ILEC and CLEC systems?\n\ndata(\"Verizon\")\ninspect(Verizon)\n\n\ncategorical variables:  \n   name  class levels    n missing\n1 Group factor      2 1687       0\n                                   distribution\n1 ILEC (98.6%), CLEC (1.4%)                    \n\nquantitative variables:  \n  name   class min   Q1 median   Q3   max     mean       sd    n missing\n1 Time numeric   0 0.75   3.63 7.35 191.6 8.522009 14.78848 1687       0\n\n\nDescribe the Variables!\n\n\n\nHypothesis Specification\nWrite the Null and Alternate hypotheses here.\nNull Distribution Computation\n\n\n\nVerizon Conclusion"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/90-Means/files/two-means.html#case-story-2-recidivism",
    "href": "content/courses/Analytics/Modelling/Modules/90-Means/files/two-means.html#case-story-2-recidivism",
    "title": "Permutation Tests for Two Means",
    "section": "Case Story-2: Recidivism",
    "text": "Case Story-2: Recidivism\nDo criminals released after a jail term commit crimes again? Does recidivism depend upon age?\n\ndata(\"Recidivism\")\ninspect(Recidivism)\n\n\ncategorical variables:  \n     name  class levels     n missing\n1  Gender factor      2 17019       3\n2     Age factor      5 17019       3\n3   Age25 factor      2 17019       3\n4    Race factor     10 16988      34\n5 Offense factor      2 17022       0\n6   Recid factor      2 17022       0\n7    Type factor      3 17022       0\n                                   distribution\n1 M (87.7%), F (12.3%)                         \n2 25-34 (36.6%), 35-44 (23.7%) ...             \n3 Over 25 (81.9%), Under 25 (18.1%)            \n4 White-NonHispanic (67%) ...                  \n5 Felony (80.6%), Misdemeanor (19.4%)          \n6 No (68.4%), Yes (31.6%)                      \n7 No Recidivism (68.4%), New (20.2%) ...       \n\nquantitative variables:  \n  name   class min  Q1 median  Q3  max     mean       sd    n missing\n1 Days integer   0 241    418 687 1095 473.3275 283.1393 5386   11636\n\n\nDescribe the variables!\nHypothesis Specification\nLet us see if the indidence of recidivism is dependent upon whether a person is aged less than or more than 25 years. Write the Null and Alternate hypotheses here.\n\\[\nH_0 = \\mu_{recid-age-25-minus}\\ = \\mu_{recid-age-25-plus}\\\\\nH_a = \\mu_{recid-age-25-minus}\\ \\ne\\mu_{recid-age-25-plus}\\\\\n\\]\nH_0: _{recid_25_minus}\n\nRecidivism\n\n\n\n  \n\n\n\nAlso, the variable Recid is a factor variable coded ‚ÄúYes‚Äù or ‚ÄúNo‚Äù. We ought to convert it to a numeric variable of 1‚Äôs and 0‚Äôs. Why?\n\n\n\nNull Distribution for Recidivism\n\n\n\nRecidivism Conclusion\nCase Study #3: Flight Delays\nLaGuardia Airport (LGA) is one of three major airports that serves the New York City metropolitan area. In 2008, over 23 million passengers and over 375 000 planes flew in or out of LGA. United Airlines and America Airlines are two major airlines that schedule services at LGA. The data set FlightDelays contains information on all 4029 departures of these two airlines from LGA during May and June 2009.\n\ndata(\"FlightDelays\")\ninspect(FlightDelays)\n\n\ncategorical variables:  \n         name  class levels    n missing\n1     Carrier factor      2 4029       0\n2 Destination factor      7 4029       0\n3  DepartTime factor      5 4029       0\n4         Day factor      7 4029       0\n5       Month factor      2 4029       0\n6   Delayed30 factor      2 4029       0\n                                   distribution\n1 AA (72.1%), UA (27.9%)                       \n2 ORD (44.3%), DFW (22.8%), MIA (15.1%) ...    \n3 8-Noon (26.1%), Noon-4pm (26%) ...           \n4 Fri (15.8%), Mon (15.6%), Tue (15.6%) ...    \n5 June (50.4%), May (49.6%)                    \n6 No (85.2%), Yes (14.8%)                      \n\nquantitative variables:  \n          name   class min   Q1 median   Q3  max      mean         sd    n\n1           ID integer   1 1008   2015 3022 4029 2015.0000 1163.21645 4029\n2     FlightNo integer  71  371    691  787 2255  827.1035  551.30939 4029\n3 FlightLength integer  68  155    163  228  295  185.3011   41.78783 4029\n4        Delay integer -19   -6     -3    5  693   11.7379   41.63050 4029\n  missing\n1       0\n2       0\n3       0\n4       0\n\n\nThe variables in the FlightDelays dataset are:\nHypothesis Specification\nLet us compute the proportion of times that each carrier‚Äôs flights was delayed more than 20 min. We will conduct a two-sided test to see if the difference in these proportions is statistically significant.\n\n\n\nNull Distribution for FlightDelays\n\n\n\n\n\n\n\nwhich is very small. Hence we reject the null Hypothesis that there is no difference between carriers on delay times."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/90-Means/index.html",
    "href": "content/courses/Analytics/Modelling/Modules/90-Means/index.html",
    "title": "üÉè Permutation Test for Two Means",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = FALSE,message = TRUE,\n                      warning = TRUE, \n                      fig.align = \"center\")\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.0     ‚úî readr     2.1.4\n‚úî forcats   1.0.0     ‚úî stringr   1.5.0\n‚úî ggplot2   3.4.1     ‚úî tibble    3.2.0\n‚úî lubridate 1.9.2     ‚úî tidyr     1.3.0\n‚úî purrr     1.0.1     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\n### Datasets from Chihara and Hesterberg's book (Second Edition)\nlibrary(resampledata)\n\n\nAttaching package: 'resampledata'\n\nThe following object is masked from 'package:datasets':\n\n    Titanic"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/90-Means/index.html#fa-folder-open-slides-and-tutorials",
    "href": "content/courses/Analytics/Modelling/Modules/90-Means/index.html#fa-folder-open-slides-and-tutorials",
    "title": "üÉè Permutation Test for Two Means",
    "section": "\n Slides and Tutorials",
    "text": "Slides and Tutorials\n\n\n\n\n\n\n\n\n\n R Tutorial¬†¬†\n  Orange Tutorial\n\n  Radiant Tutorial¬†\n  Datasets"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/90-Means/index.html#introduction",
    "href": "content/courses/Analytics/Modelling/Modules/90-Means/index.html#introduction",
    "title": "üÉè Permutation Test for Two Means",
    "section": "Introduction",
    "text": "Introduction\nWe saw from the diagram created by Allen Downey that there is only one test! We will now use this philosophy to develop a technique that allows us to mechanize several Statistical Models in that way, with nearly identical code.\nWe will use two packages in R, mosaic and the relatively new infer package, to develop our intuition for what are called permutation based statistical tests."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/90-Means/index.html#hypothesis-testing-using-permutation",
    "href": "content/courses/Analytics/Modelling/Modules/90-Means/index.html#hypothesis-testing-using-permutation",
    "title": "üÉè Permutation Test for Two Means",
    "section": "Hypothesis Testing using Permutation",
    "text": "Hypothesis Testing using Permutation\nFrom Reference #1:\n\nHypothesis testing can be thought of as a 4-step process:\n\nState the null and alternative hypotheses.\nCompute a test statistic.\nDetermine the p-value.\n\nDraw a conclusion.\nIn a traditional introductory statistics course, once this general framework has been mastered, the main work is in applying the correct formula to compute the standard test statistics in step 2 and using a table or computer to determine the p-value based on the known (usually approximate) theoretical distribution of the test statistic under the null hypothesis.\nIn a simulation-based approach, steps 2 and 3 change. In Step 2, it is no longer required that the test statistic be normalized to conform with a known, named distribution. Instead, natural test statistics, like the difference between two sample means \\(y1 ‚àí y2\\) can be used.\nIn Step 3, we use randomization to approximate the sampling distribution of the test statistic. Our lady tasting tea example demonstrates how this can be done from first principles. More typically, we will use randomization to create new simulated data sets ( ‚ÄúParallel Worlds‚Äù) that are like our original data in some ways, but make the null hypothesis true. For each simulated data set, we calculate our test statistic, just as we did for the original sample. Together, this collection of test statistics computed from the simulated samples constitute our randomization distribution.\nWhen creating a randomization distribution, we will attempt to satisfy 3 guiding principles.\n\nBe consistent with the null hypothesis. We need to simulate a world in which the null hypothesis is true. If we don‚Äôt do this, we won‚Äôt be testing our null hypothesis.\nUse the data in the original sample. The original data should shed light on some aspects of the distribution that are not determined by null hypothesis. For example, a null hypothesis about a mean doesn‚Äôt tell us about the shape of the population distribution, but the data give us some indication.\nReflect the way the original data were collected.\n\n\nFrom Chihara and Hesterberg:\n\nThis is the core idea of statistical significance or classical hypothesis testing ‚Äì to calculate how often pure random chance would give an effect as large as that observed in the data, in the absence of any real effect. If that probability is small enough, we conclude that the data provide convincing evidence of a real effect.\n\nPermutations tests using mosaic::shuffle()\n\nThe mosaic package provides the shuffle() function as a synonym for sample(). When used without additional arguments, this will permute its first argument.\n\nshuffle(1:10)\n\n [1]  8  7  5  6  1 10  4  2  3  9\n\n\nApplying shuffle() to an explanatory variable in a model allows us to test the null hypothesis that the explanatory variable has, in fact, no explanatory power. This idea can be used to test\n\nthe equivalence of two or more means,\nthe equivalence of two or more proportions,\nwhether a regression parameter is 0.\n\nWe will now see examples of each of these models using Permutations."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/90-Means/index.html#testing-for-two-or-more-means",
    "href": "content/courses/Analytics/Modelling/Modules/90-Means/index.html#testing-for-two-or-more-means",
    "title": "üÉè Permutation Test for Two Means",
    "section": "Testing for Two or More Means",
    "text": "Testing for Two or More Means\nCase Study-1: Hot Wings Orders vs Gender (From Chihara and Hesterberg)\nA student conducted a study of hot wings and beer consumption at a Bar. She asked patrons at the bar to record their consumption of hot wings and beer over the course of several hours. She wanted to know if people who ate more hot wings would then drink more beer. In addition, she investigated whether or not gender had an impact on hot wings or beer consumption. Is the mean order value related to the gender of the person who is ordering?\n\n\n\n\n\ndata(\"Beerwings\")\ninspect(Beerwings)\n\n\ncategorical variables:  \n    name  class levels  n missing                                  distribution\n1 Gender factor      2 30       0 F (50%), M (50%)                             \n\nquantitative variables:  \n      name   class min    Q1 median    Q3 max     mean        sd  n missing\n1       ID integer   1  8.25   15.5 22.75  30 15.50000  8.803408 30       0\n2 Hotwings integer   4  8.00   12.5 15.50  21 11.93333  4.784554 30       0\n3     Beer integer   0 24.00   30.0 36.00  48 26.20000 11.842064 30       0\n\n\nLet us calculate the observed difference in Hotwings consumption between Males and Females ( Gender): (using the mosaic package)\n\nmosaic::mean(Hotwings ~ Gender, data = Beerwings)\n\n        F         M \n 9.333333 14.533333 \n\nobs_diff_wings <- mosaic::diffmean(data = Beerwings, Hotwings ~ Gender)\nobs_diff_wings \n\ndiffmean \n     5.2 \n\n\n\ngf_boxplot(data = Beerwings, Hotwings ~ Gender, title = \"Hotwings Consumption by Gender\") %>%  gf_theme(theme_classic)\n\n\n\n\nThe observed difference in mean consumption of Hotwings between Males and Females is 5.2. There is also a ‚Äúvisible‚Äù difference in medians as seen from the pair of box plots above.\nCould this have occurred by chance? Here is our formulation of the Hypotheses:\n\\[\nH_0: \\mu_M\\ = \\mu_F\\\\\nH_a: \\mu_M\\ \\ne \\mu_F\\\\\n\\]Note that we have a two-sided test: we want to check for differences in mean order value, either way. So we perform a Permutation Test to check: we create a null distribution of the differences in mean by a shuffle operation on gender:\n\nnull_dist_wings <- do(1000) * diffmean(Hotwings ~ shuffle(Gender), data = Beerwings)\nnull_dist_wings %>% head()\n\n\n\n  \n\n\ngf_histogram(data = null_dist_wings, ~ diffmean) %>% \n  gf_vline(xintercept = obs_diff_wings, colour = \"red\") %>% gf_theme(theme_classic())\n\n\n\ngf_ecdf(data = null_dist_wings, ~ diffmean) %>% \n  gf_vline(xintercept = obs_diff_wings, colour = \"red\") %>% gf_theme(theme_classic())\n\n\n\nprop1(~ diffmean >= obs_diff_wings, data = null_dist_wings)\n\n  prop_TRUE \n0.001998002 \n\n\nThe \\(\\color{red}{red\\ line}\\) shows the actual measured mean difference in Hot Wings consumption. The probability that our Permutation distribution is able to equal or exceed that number is \\(0.001998002\\) and we have to reject the Null Hypothesis that the means are identical.\nMatched Pairs: Results from a diving championship.\nSometimes the data is collected on the same set of individual categories, e.g.¬†scores by sport persons in two separate tournaments, or sales of identical items in two separate locations of a chain store. Here we have swimming records across a Semi-Final and a Final:\n\ndata(\"Diving2017\")\nhead(Diving2017)\n\n\n\n  \n\n\ninspect(Diving2017)\n\n\ncategorical variables:  \n     name  class levels  n missing\n1    Name factor     12 12       0\n2 Country factor      8 12       0\n                                   distribution\n1  SI Yajie (8.3%) ...                         \n2 Canada (16.7%), China (16.7%) ...            \n\nquantitative variables:  \n       name   class    min       Q1  median      Q3   max    mean       sd  n\n1 Semifinal numeric 313.70 322.2000 325.625 356.575 382.8 338.500 22.94946 12\n2     Final numeric 283.35 318.5875 358.925 387.150 397.5 350.475 40.02204 12\n  missing\n1       0\n2       0\n\n\nThe data is made up of paired observations per swimmer. So we need to take the difference between the two swim records for each swimmer and then shuffle the differences to either polarity. Another way to look at this is to shuffle the records between Semifinal and Final on a per Swimmer basis. There are 12 swimmers and therefore 12 paired records.\nIn order to ensure that the records are paired, we use the argument only.2=FALSE in the diffmean function:\n\nDiving2017\n\n\n\n  \n\n\nDiving2017 %>% diffmean(data = ., Final ~ Semifinal, only.2 = FALSE)\n\n  318.7-313.7  320.55-318.7 322.75-320.55  325.5-322.75  325.75-325.5 \n       12.350       -63.050         5.225        85.125      -114.150 \n   346-325.75    355.15-346 360.85-355.15  367.5-360.85   382.8-367.5 \n      102.200       -54.150        28.600        31.950         4.050 \n\nobs_diff_swim <- mean(~ Final - Semifinal, data = Diving2017)\nobs_diff_swim\n\n[1] 11.975\n\n\nHow would we formulate our Hypothesis?\n\\[\nH_0: \\mu_{semifinal} = \\mu_{final}\\\\\nH_a: \\mu_{semifinal} \\ne \\mu_{final}\\\n\\]\n\npolarity <- c(rep(1, 6), rep(-1, 6)) \n# 12 +/- 1s, \n# 6 each to make sure there is equal probability\npolarity\n\n [1]  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1\n\nnull_dist_swim <- do(100000) *\n  mean(data = Diving2017,\n       ~ (Final - Semifinal) * resample(polarity, replace = TRUE))\n\nnull_dist_swim %>% head()\n\n\n\n  \n\n\ngf_histogram(data = null_dist_swim, ~ mean) %>%\n  gf_vline(xintercept = obs_diff_swim, colour = \"red\")\n\n\n\ngf_ecdf(data = null_dist_swim, ~ mean) %>%\n  gf_vline(xintercept = obs_diff_swim, colour = \"red\")\n\n\n\nprop1(~ mean >= obs_diff_swim, data = null_dist_swim)\n\nprop_TRUE \n0.1277987 \n\n\nHmm‚Ä¶so by generating 100000 shufflings of score differences, with polarities, it does appear that we can not only obtain the current observed difference but even surpass it frequently. So it does seem that there is no difference in means between Semi-Final and Final swimming scores.\nWalmart vs Target\nIs there a difference in the price of Groceries sold by the two retailers Target and Walmart? The data set Groceries contains a sample of grocery items and their prices advertised on their respective web sites on one specific day.\n\nInspect the data set, then explain why this is an example of matched pairs data.\nCompute summary statistics of the prices for each store.\nConduct a permutation test to determine whether or not there is a difference in the mean prices.\nCreate a histogram bar-chart of the difference in prices. What is unusual about Quaker Oats Life cereal?\nRedo the hypothesis test without this observation. Do you reach the same conclusion?\n\n\ndata(\"Groceries\")\nGroceries <- Groceries %>% mutate(Product = stringr::str_squish(Product))\nhead(Groceries)\n\n\n\n  \n\n\ninspect(Groceries)\n\n\ncategorical variables:  \n      name     class levels  n missing\n1  Product character     30 30       0\n2     Size character     24 30       0\n3    Units character     16 30       0\n4 UnitType character      3 30       0\n                                   distribution\n1 Annie's Macaroni & Cheese (3.3%) ...         \n2 18oz (10%), 12oz (6.7%) ...                  \n3 10 (10%), 15 (10%), 16 (10%) ...             \n4 oz (93.3%), bars (3.3%) ...                  \n\nquantitative variables:  \n     name   class  min     Q1 median    Q3  max     mean       sd  n missing\n1  Target numeric 0.99 1.8275  2.545 3.140 7.99 2.762333 1.582128 30       0\n2 Walmart numeric 1.00 1.7600  2.340 2.955 6.98 2.705667 1.560211 30       0\n\n\nWe see that the comparison is to be made between two prices for the same product, and hence this is one more example of paired data, as in Case Study #4. Let us plot the prices for the products:\n\ngf_col(data = Groceries,\n       Target ~ Product,\n       fill = \"#0073C299\",\n       width = 0.5 ) %>% \n  gf_col(data = Groceries,\n         -Walmart ~ Product,\n         fill = \"#EFC00099\",\n         ylab = \"Prices\",\n         width = 0.5\n       ) %>% \n  gf_col(data = Groceries %>% filter(Product == \"Quaker Oats Life Cereal Original\"), \n         -Walmart ~ Product,\n         fill = \"red\", \n         width = 0.5) %>% \n  gf_theme(theme_classic()) %>%\n  gf_theme(ggplot2::theme(axis.text.x = element_text(\n    size = 8,\n    face = \"bold\",\n    vjust = 0,\n    hjust = 1\n  ))) %>% gf_theme(ggplot2::coord_flip())\n\n\n\n\nWe see that the price difference between Walmart and Target prices is highest for the Product named Quaker Oats Life Cereal Original. Let us check the mean difference in prices:\n\ndiffmean(data = Groceries, Walmart ~ Target, only.2 = FALSE)\n\n   1-0.99    1.22-1 1.42-1.22 1.49-1.42 1.59-1.49 1.62-1.59 1.79-1.62 1.94-1.79 \n-0.580000  0.170000  0.210000 -0.100000  0.190000  0.070000  0.180000  0.160000 \n1.99-1.94 2.12-1.99 2.39-2.12  2.5-2.39  2.59-2.5 2.64-2.59 2.79-2.64 2.82-2.79 \n 0.090000  0.010000  0.200000  0.600000 -0.200000 -0.600000  0.660000  0.040000 \n2.99-2.82 3.19-2.99 3.49-3.19 3.99-3.49 4.79-3.99 7.19-4.79 7.99-7.19 \n 0.220000  1.263333 -1.183333 -0.480000  2.290000  2.190000  0.000000 \n\nobs_diff_price = mean( ~ Walmart - Target, data = Groceries)\nobs_diff_price\n\n[1] -0.05666667\n\n\nLet us perform the pair-wise permutation test on prices, by shuffling the two store names:\n\npolarity <- c(rep(1, 15), rep(-1,15))\npolarity\n\n [1]  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n[26] -1 -1 -1 -1 -1\n\nnull_dist_price <- do(100000) * mean(data = Groceries, \n                                    ~(Walmart-Target) * resample(polarity,\n                                                    replace = TRUE))\nnull_dist_price %>% head()\n\n\n\n  \n\n\ngf_histogram(data = null_dist_price, ~mean) %>% \n  gf_vline(xintercept = obs_diff_price, colour = \"red\")\n\n\n\n2*(sum(null_dist_price >= obs_diff_price + 1)/(100000+1)) #P-value\n\n[1] 0\n\n\nDoes not seem to be any significant difference in prices‚Ä¶\nSuppose we knock off the Quaker Cereal data item‚Ä¶\n\nwhich(Groceries$Product == \"Quaker Oats Life Cereal Original\")\n\n[1] 2\n\nGroceries_less <- Groceries[-2,]\nGroceries_less\n\n\n\n  \n\n\nobs_diff_price_less = mean( ~ Walmart - Target, data = Groceries_less)\nobs_diff_price_less\n\n[1] -0.1558621\n\npolarity_less <- c(rep(1, 15), rep(-1,14)) # Due to resampling this small bias makes no difference\nnull_dist_price_less <- do(100000) * mean(data = Groceries_less, \n                                    ~(Walmart-Target) * resample(polarity_less,\n                                                    replace = TRUE))\nnull_dist_price_less %>% head()\n\n\n\n  \n\n\ngf_histogram(data = null_dist_price_less, ~mean) %>% \n  gf_vline(xintercept = obs_diff_price_less, colour = \"red\")\n\n\n\n1- mean(null_dist_price_less >= obs_diff_price_less) #P-value\n\n[1] 0.016"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/90-Means/index.html#conclusion",
    "href": "content/courses/Analytics/Modelling/Modules/90-Means/index.html#conclusion",
    "title": "üÉè Permutation Test for Two Means",
    "section": "Conclusion",
    "text": "Conclusion\nIt should be fairly clear now that we can test for the equivalence of two means, using a very simple permutation tests. Given computing power, we can always mechanize this test very quickly to get our results. And that performing this test yields reliable results without having to rely on any assumption relating to underlying distributions and so on."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/90-Means/index.html#references",
    "href": "content/courses/Analytics/Modelling/Modules/90-Means/index.html#references",
    "title": "üÉè Permutation Test for Two Means",
    "section": "References",
    "text": "References\n\nRandall Pruim, Nicholas J. Horton, Daniel T. Kaplan, Start Teaching with R\n\nhttps://bcs.wiley.com/he-bcs/Books?action=index&itemId=111941654X&bcsId=11307"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/listing.html",
    "href": "content/courses/Analytics/Predictive/listing.html",
    "title": "Predictive Analytics",
    "section": "",
    "text": "üêâ Intro to Orange\n\n\n\n\n\nUsing A Visual drag and drop tool called Orange\n\n\n\n\n\n\nOct 17, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\n \n\n\n\n\nML - Regression\n\n\n\n\n\n\n\nLinear Regression\n\n\nTrend Line\n\n\nFrancis Galton\n\n\n\n\nUsing Linear Regression to Predict Numerical Data\n\n\n\n\n\n\nAug 16, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\n \n\n\n\n\nML - Classification\n\n\n\n\n\nWe will look at the basic models for Classification of Data\n\n\n\n\n\n\nJul 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nML - Clustering\n\n\n\n\n\nWe will look at the basic models for Clustering of Data.\n\n\n\n\n\n\nJul 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nüïî Modelling Time Series\n\n\n\n\n\n\n\nSmoothing\n\n\nExponential Models\n\n\nARIMA\n\n\nForecasting\n\n\nProphet\n\n\n\n\nWe will look at the basic models for Time Series\n\n\n\n\n\n\nNov 19, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html",
    "href": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html",
    "title": "üêâ Intro to Orange",
    "section": "",
    "text": "Orange is a visual drag-and-drop tool for\n\nData visualization\n\nMachine Learning\n\nData mining\n\nand much more. Here is what it looks like:\n\nAll operations are done using a visual menu-driven interface. The visuals can be exported to PNG/SVG/PDF and the entire workflow can be exported into a Report Form and edited for presentation and sharing."
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#installing-orange",
    "href": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#installing-orange",
    "title": "üêâ Intro to Orange",
    "section": "Installing Orange",
    "text": "Installing Orange\nYou can download and install Orange from here:\nhttps://orangedatamining.com/download/"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#basic-usage-of-orange",
    "href": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#basic-usage-of-orange",
    "title": "üêâ Intro to Orange",
    "section": "Basic Usage of Orange",
    "text": "Basic Usage of Orange\n{{% youtube \"HXjnDIgGDuI\" %}}"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#orange-workflows",
    "href": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#orange-workflows",
    "title": "üêâ Intro to Orange",
    "section": "Orange Workflows",
    "text": "Orange Workflows\n{{% youtube \"lb-x36xqJ-E\" %}}"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#widgets-and-channels",
    "href": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#widgets-and-channels",
    "title": "üêâ Intro to Orange",
    "section": "Widgets and Channels",
    "text": "Widgets and Channels\n{{% youtube \"2xS6QjnG714\" %}}"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#loading-data-into-orange",
    "href": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#loading-data-into-orange",
    "title": "üêâ Intro to Orange",
    "section": "Loading data into Orange",
    "text": "Loading data into Orange\n{{% youtube \"MHcGdQeYCMg\" %}} \nWe are good to get started with Orange!!"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#simple-visuals-using-orange",
    "href": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#simple-visuals-using-orange",
    "title": "üêâ Intro to Orange",
    "section": "Simple Visuals using Orange",
    "text": "Simple Visuals using Orange\nLet us create some simple visualizations using Orange.\n\nUse the File Widget to import the iris dataset into your session\nUse the Data Table Widget to look at the data, and note its variable names\nUse the Visualization Widgets ( Scatter Plot, Bar Plot, and Distributions) to look at the properties of the variables, and examine relationships between them."
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#reference",
    "href": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#reference",
    "title": "üêâ Intro to Orange",
    "section": "Reference",
    "text": "Reference\n\nIntroduction to Data Mining-Working notes for the hands-on course with Orange Data Mining. (Download file)"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/20-Regression/index.html",
    "href": "content/courses/Analytics/Predictive/Modules/20-Regression/index.html",
    "title": "ML - Regression",
    "section": "",
    "text": "Interpolation:\n\nbetween TWO colours, both colours inclusive using a straight line between them\nbetween several different colours?\n\nby mixing ‚Äúequal proportions‚Äù of each\nProportions based on ‚Äúdistance‚Äù from each colour\nOn a ‚Äúplane‚Äù with these points"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/20-Regression/index.html#workflow-in-orange",
    "href": "content/courses/Analytics/Predictive/Modules/20-Regression/index.html#workflow-in-orange",
    "title": "ML - Regression",
    "section": "Workflow in Orange",
    "text": "Workflow in Orange\nLet us ‚Äúdraw inspiration‚Äù from the picture above, and see if we can replicate it. We will fire up Orange, paint some data and see if we can fit a linear regression ML model to it.\nHere is the Orange file for you to download. Open this file in Orange."
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/20-Regression/index.html#workflow-in-radiant",
    "href": "content/courses/Analytics/Predictive/Modules/20-Regression/index.html#workflow-in-radiant",
    "title": "ML - Regression",
    "section": "Workflow in Radiant",
    "text": "Workflow in Radiant"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/20-Regression/index.html#workflow-in-r",
    "href": "content/courses/Analytics/Predictive/Modules/20-Regression/index.html#workflow-in-r",
    "title": "ML - Regression",
    "section": "Workflow in R",
    "text": "Workflow in R"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/20-Regression/index.html#conclusion",
    "href": "content/courses/Analytics/Predictive/Modules/20-Regression/index.html#conclusion",
    "title": "ML - Regression",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/files/Random-Forests.html",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/files/Random-Forests.html",
    "title": "Random Forests",
    "section": "",
    "text": "Machine Learning Basics - Random Forest at Shirin‚Äôs Playground"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/files/Random-Forests.html#penguin-random-forest-model-withrandomforest",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/files/Random-Forests.html#penguin-random-forest-model-withrandomforest",
    "title": "Random Forests",
    "section": "Penguin Random Forest Model withrandomForest\n",
    "text": "Penguin Random Forest Model withrandomForest\n\nUsing the penguins dataset and Random Forest Classification.\n\npenguins\n\n\n\n  \n\n\nsummary(penguins)\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2                                 \n\npenguins %>% skimr::skim()\n\n\n\n\n\nName\nPiped data\n\n\nNumber of rows\n344\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nData summaryVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nspecies\n0\n1.00\nFALSE\n3\nAde: 152, Gen: 124, Chi: 68\n\n\nisland\n0\n1.00\nFALSE\n3\nBis: 168, Dre: 124, Tor: 52\n\n\nsex\n11\n0.97\nFALSE\n2\nmal: 168, fem: 165\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nbill_length_mm\n2\n0.99\n43.92\n5.46\n32.1\n39.23\n44.45\n48.5\n59.6\n‚ñÉ‚ñá‚ñá‚ñÜ‚ñÅ\n\n\nbill_depth_mm\n2\n0.99\n17.15\n1.97\n13.1\n15.60\n17.30\n18.7\n21.5\n‚ñÖ‚ñÖ‚ñá‚ñá‚ñÇ\n\n\nflipper_length_mm\n2\n0.99\n200.92\n14.06\n172.0\n190.00\n197.00\n213.0\n231.0\n‚ñÇ‚ñá‚ñÉ‚ñÖ‚ñÇ\n\n\nbody_mass_g\n2\n0.99\n4201.75\n801.95\n2700.0\n3550.00\n4050.00\n4750.0\n6300.0\n‚ñÉ‚ñá‚ñÜ‚ñÉ‚ñÇ\n\n\nyear\n0\n1.00\n2008.03\n0.82\n2007.0\n2007.00\n2008.00\n2009.0\n2009.0\n‚ñá‚ñÅ‚ñá‚ñÅ‚ñá\n\n\n\n\npenguins <- penguins %>% tidyr::drop_na()\n# Spent one hour trying to find `drop-na()` ( 14 June 2020)\n\n\n# library(corrplot)\ncor <- penguins %>% select(is.numeric) %>% cor() \n\nWarning: Use of bare predicate functions was deprecated in tidyselect 1.1.0.\n‚Ñπ Please use wrap predicates in `where()` instead.\n  # Was:\n  data %>% select(is.numeric)\n\n  # Now:\n  data %>% select(where(is.numeric))\n\ncor %>% corrplot(., method = \"ellipse\", order = \"hclust\",tl.cex = 0.5)\n\n\n\n# try these too:\n# cor %>% corrplot(., method = \"square\", order = \"hclust\",tl.cex = 0.5)\n# cor %>% corrplot(., method = \"color\", order = \"hclust\",tl.cex = 0.5)\n# cor %>% corrplot(., method = \"shade\", order = \"hclust\",tl.cex = 0.5)\n\nNotes: - flipper_length_mm and culmen_depth_mm are negtively correlated at approx (-0.7) - flipper_length_mm and body_mass_g are positively correlated at approx 0.8\nSo we will use steps in the recipe to remove correlated variables.\nPenguin Data Sampling and Recipe\n\n# Data Split\npenguin_split <- initial_split(penguins, prop = 0.6)\npenguin_train <- training(penguin_split)\npenguin_test <- testing(penguin_split)\npenguin_split\n\n<Training/Testing/Total>\n<199/134/333>\n\nhead(penguin_train)\n\n\n\n  \n\n\n# Recipe\npenguin_recipe <- penguins %>% \n  recipe(species ~ .) %>% \n  step_normalize(all_numeric()) %>% # Scaling and Centering\n  step_corr(all_numeric()) %>%  # Handling correlated variables\n  prep()\n\n# Baking the data\npenguin_train_baked <-  penguin_train %>% \n  bake(object = penguin_recipe, new_data = .)\n\npenguin_test_baked <-  penguin_test %>% \n  bake(object = penguin_recipe, new_data = .)\n\nhead(penguin_train_baked)\n\n\n\n  \n\n\n\nPenguin Random Forest Model\n\npenguin_model <- \n  rand_forest(trees = 100) %>% \n  set_engine(\"randomForest\") %>% \n  set_mode(\"classification\")\npenguin_model\n\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  trees = 100\n\nComputational engine: randomForest \n\npenguin_fit <- \n  penguin_model %>% \n  fit(species ~ .,penguin_train_baked)\npenguin_fit\n\nparsnip model object\n\n\nCall:\n randomForest(x = maybe_data_frame(x), y = y, ntree = ~100) \n               Type of random forest: classification\n                     Number of trees: 100\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 0%\nConfusion matrix:\n          Adelie Chinstrap Gentoo class.error\nAdelie        96         0      0           0\nChinstrap      0        37      0           0\nGentoo         0         0     66           0\n\n# iris_ranger <- \n#   rand_forest(trees = 100) %>% \n#   set_mode(\"classification\") %>% \n#   set_engine(\"ranger\") %>% \n#   fit(Species ~ ., data = iris_training_baked)\n\nMetrics for the Penguin Random Forest Model\n\n# Predictions\npredict(object = penguin_fit, new_data = penguin_test_baked) %>%  \n  dplyr::bind_cols(penguin_test_baked) %>% \n  glimpse()\n\nRows: 134\nColumns: 9\n$ .pred_class       <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel‚Ä¶\n$ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen, Torgerse‚Ä¶\n$ bill_length_mm    <dbl> -0.6752636, -1.3335592, -0.9861254, -1.3518452, -0.2‚Ä¶\n$ bill_depth_mm     <dbl> 0.42409105, 1.08424573, 2.04908718, 0.32252879, 1.79‚Ä¶\n$ flipper_length_mm <dbl> -0.4257325, -0.5684290, -0.7111254, -1.1392148, -0.2‚Ä¶\n$ body_mass_g       <dbl> -1.18857213, -0.94019151, -0.50552542, -0.62971573, ‚Ä¶\n$ sex               <fct> female, female, male, female, male, male, female, fe‚Ä¶\n$ year              <dbl> -1.2818130, -1.2818130, -1.2818130, -1.2818130, -1.2‚Ä¶\n$ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel‚Ä¶\n\n# Prediction Accuracy Metrics\npredict(object = penguin_fit, new_data = penguin_test_baked) %>%  \n  dplyr::bind_cols(penguin_test_baked) %>% \n  yardstick::metrics(truth = species, estimate = .pred_class)\n\n\n\n  \n\n\n# Prediction Probabilities\npenguin_fit_probs <- \n  predict(penguin_fit, penguin_test_baked, type = \"prob\") %>%\n  dplyr::bind_cols(penguin_test_baked)\nglimpse(penguin_fit_probs)\n\nRows: 134\nColumns: 11\n$ .pred_Adelie      <dbl> 0.99, 0.98, 1.00, 1.00, 0.95, 1.00, 0.98, 1.00, 0.93‚Ä¶\n$ .pred_Chinstrap   <dbl> 0.01, 0.02, 0.00, 0.00, 0.03, 0.00, 0.01, 0.00, 0.07‚Ä¶\n$ .pred_Gentoo      <dbl> 0.00, 0.00, 0.00, 0.00, 0.02, 0.00, 0.01, 0.00, 0.00‚Ä¶\n$ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen, Torgerse‚Ä¶\n$ bill_length_mm    <dbl> -0.6752636, -1.3335592, -0.9861254, -1.3518452, -0.2‚Ä¶\n$ bill_depth_mm     <dbl> 0.42409105, 1.08424573, 2.04908718, 0.32252879, 1.79‚Ä¶\n$ flipper_length_mm <dbl> -0.4257325, -0.5684290, -0.7111254, -1.1392148, -0.2‚Ä¶\n$ body_mass_g       <dbl> -1.18857213, -0.94019151, -0.50552542, -0.62971573, ‚Ä¶\n$ sex               <fct> female, female, male, female, male, male, female, fe‚Ä¶\n$ year              <dbl> -1.2818130, -1.2818130, -1.2818130, -1.2818130, -1.2‚Ä¶\n$ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel‚Ä¶\n\n# Confusion Matrix\npenguin_fit$fit$confusion %>% tidy()\n\nWarning: 'tidy.numeric' is deprecated.\nSee help(\"Deprecated\")\n\n\n\n\n  \n\n\n# Gain Curves\npenguin_fit_probs %>% \n  yardstick::gain_curve(species, .pred_Adelie:.pred_Gentoo) %>%\n  autoplot()\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\n‚Ñπ Please use `reframe()` instead.\n‚Ñπ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n‚Ñπ The deprecated feature was likely used in the yardstick package.\n  Please report the issue at <https://github.com/tidymodels/yardstick/issues>.\n\n\n\n\n# ROC Plot\npenguin_fit_probs%>%\n  roc_curve(species, .pred_Adelie:.pred_Gentoo) %>%\n  autoplot()\n\n\n\n\nUsing broom on the penguin model\n\npenguin_split\n\n<Training/Testing/Total>\n<199/134/333>\n\npenguin_split %>% broom::tidy()\n\n\n\n  \n\n\npenguin_recipe %>% broom::tidy()\n\n\n\n  \n\n\n# Following do not work for `random forest models` !! ;-()\n#penguin_model %>% tidy()\n#penguin_fit %>% tidy() \npenguin_model %>% str()\n\nList of 7\n $ args                 :List of 3\n  ..$ mtry : language ~NULL\n  .. ..- attr(*, \".Environment\")=<environment: R_EmptyEnv> \n  ..$ trees: language ~100\n  .. ..- attr(*, \".Environment\")=<environment: R_EmptyEnv> \n  ..$ min_n: language ~NULL\n  .. ..- attr(*, \".Environment\")=<environment: R_EmptyEnv> \n $ eng_args             : Named list()\n  ..- attr(*, \"class\")= chr [1:2] \"quosures\" \"list\"\n $ mode                 : chr \"classification\"\n $ user_specified_mode  : logi TRUE\n $ method               : NULL\n $ engine               : chr \"randomForest\"\n $ user_specified_engine: logi TRUE\n - attr(*, \"class\")= chr [1:2] \"rand_forest\" \"model_spec\"\n\npenguin_test_baked"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/files/Random-Forests.html#iris-random-forest-model-with-ranger",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/files/Random-Forests.html#iris-random-forest-model-with-ranger",
    "title": "Random Forests",
    "section": "Iris Random Forest Model with ranger\n",
    "text": "Iris Random Forest Model with ranger\n\nUsing the iris dataset and Random Forest Classification. This part uses rsample to split the data and the recipes to prep the data for model making.\n\n#set.seed(100)\niris_split <- rsample::initial_split(iris, prop = 0.6)\niris_split\n\n<Training/Testing/Total>\n<90/60/150>\n\niris_split %>% training() %>% glimpse()\n\nRows: 90\nColumns: 5\n$ Sepal.Length <dbl> 5.6, 5.7, 4.8, 5.0, 6.0, 7.2, 4.9, 6.1, 7.1, 5.7, 6.1, 6.‚Ä¶\n$ Sepal.Width  <dbl> 2.5, 3.0, 3.4, 3.0, 2.2, 3.2, 3.1, 3.0, 3.0, 3.8, 2.9, 2.‚Ä¶\n$ Petal.Length <dbl> 3.9, 4.2, 1.6, 1.6, 5.0, 6.0, 1.5, 4.6, 5.9, 1.7, 4.7, 4.‚Ä¶\n$ Petal.Width  <dbl> 1.1, 1.2, 0.2, 0.2, 1.5, 1.8, 0.2, 1.4, 2.1, 0.3, 1.4, 1.‚Ä¶\n$ Species      <fct> versicolor, versicolor, setosa, setosa, virginica, virgin‚Ä¶\n\niris_split %>% testing() %>% glimpse()\n\nRows: 60\nColumns: 5\n$ Sepal.Length <dbl> 5.1, 5.0, 4.4, 5.4, 4.8, 5.4, 5.1, 5.4, 4.8, 5.0, 5.2, 5.‚Ä¶\n$ Sepal.Width  <dbl> 3.5, 3.4, 2.9, 3.7, 3.0, 3.9, 3.8, 3.4, 3.4, 3.4, 3.5, 3.‚Ä¶\n$ Petal.Length <dbl> 1.4, 1.5, 1.4, 1.5, 1.4, 1.3, 1.5, 1.7, 1.9, 1.6, 1.5, 1.‚Ä¶\n$ Petal.Width  <dbl> 0.2, 0.2, 0.2, 0.2, 0.1, 0.4, 0.3, 0.2, 0.2, 0.4, 0.2, 0.‚Ä¶\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setosa, s‚Ä¶\n\n\nIris Data Pre-Processing: Creating the Recipe\nThe recipes package provides an interface that specializes in data pre-processing. Within the package, the functions that start, or execute, the data transformations are named after cooking actions. That makes the interface more user-friendly. For example:\n\nrecipe() - Starts a new set of transformations to be applied, similar to the ggplot() command. Its main argument is the model‚Äôs formula.\nprep() - Executes the transformations on top of the data that is supplied (typically, the training data). Each data transformation is a step() function. ( Recall what we did with the caret package: Centering, Scaling, Removing Correlated variables‚Ä¶)\n\nNote that in order to avoid data leakage (e.g: transferring information from the train set into the test set), data should be ‚Äúprepped‚Äù using the train_tbl only. https://towardsdatascience.com/modelling-with-tidymodels-and-parsnip-bae2c01c131c CRAN: The idea is that the preprocessing operations will all be created using the training set and then these steps will be applied to both the training and test set.\n\n# Pre Processing the Training Data\n\niris_recipe <- \n  training(iris_split) %>% # Note: Using TRAINING data !!\n  recipe(Species ~ .)      # Note: Outcomes ~ Predictors !!\n\n# The data contained in the `data` argument need not be the training set; this data is only used to catalog the names of the variables and their types (e.g. numeric, etc.).\n\nQ: How does the recipe ‚Äúfigure‚Äù out which are the outcomes and which are the predictors? A.The recipe command defines Outcomes and Predictors using the formula interface. Not clear how this recipe ‚Äúfigures‚Äù out which are the outcomes and which are the predictors, when we have not yet specified them‚Ä¶\nQ. Why is the recipe not agnostic to data set? Is that a meaningful question? A. The use of the training set in the recipe command is just to declare the variables and specify the roles of the data, nothing else. Roles are open-ended and extensible. From https://cran.r-project.org/web/packages/recipes/vignettes/Simple_Example.html :\n\nThis document demonstrates some basic uses of recipes. First, some definitions are required: - variables are the original (raw) data columns in a data frame or tibble. For example, in a traditional formula Y ~ A + B + A:B, the variables are A, B, and Y. - roles define how variables will be used in the model. Examples are: predictor (independent variables), response, and case weight. This is meant to be open-ended and extensible. - terms are columns in a design matrix such as A, B, and A:B. These can be other derived entities that are grouped, such as a set of principal components or a set of columns, that define a basis function for a variable. These are synonymous with features in machine learning. Variables that have predictor roles would automatically be main effect terms.\n\n\n# Apply the transformation steps\niris_recipe <- iris_recipe %>% \n  step_corr(all_predictors()) %>% \n  step_center(all_predictors(), -all_outcomes()) %>% \n  step_scale(all_predictors(), -all_outcomes()) %>% \n  prep()\n\nThis has created the recipe() and prepped it too. We now need to apply it to our datasets:\n\nTake training data and bake() it to prepare it for modelling.\nDo the same for the testing set.\n\n\niris_training_baked <- \n  iris_split %>% \n  training() %>% \n  bake(iris_recipe,.)\niris_training_baked\n\n\n\n  \n\n\niris_testing_baked <- \n  iris_split %>% \n  testing() %>% \n  bake(iris_recipe,.)\niris_testing_baked \n\n\n\n  \n\n\n\nIris Model Training using parsnip\n\nDifferent ML packages provide different interfaces (APIs ) to do the same thing (e.g random forests). The tidymodels package provides a consistent interface to invoke a wide variety of packages supporting a wide variety of models.\nThe parsnip package is a successor to caret.\nTo model with parsnip: 1. Pick a model : 2. Set the engine 3. Set the mode (if needed): Classification or Regression\nCheck here for models available in parsnip.\n\nMode: classification and regression in parsnip, each using a variety of models. ( Which Way). This defines the form of the output.\nEngine: The engine is the R package that is invoked by parsnip to execute the model. E.g glm, glmnet,keras.( How ) parsnip provides wrappers for models from these packages.\nModel: is the specific technique used for the modelling task. E.g linear_reg(), logistic_reg(), mars, decision_tree, nearest_neighbour‚Ä¶(What model).\n\nand models have: - hyperparameters: that are numerical or factor variables that tune the model ( Like the alpha beta parameters for Bayesian priors)\nWe can use the random forest model to classify the iris into species. Here Species is the Outcome variable and the rest are predictor variables. The random forest model is provided by the ranger package, to which tidymodels/parsnip provides a simple and consistent interface.\n\nlibrary(ranger)\niris_ranger <- \n  rand_forest(trees = 100) %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"ranger\") %>% \n  fit(Species ~ ., data = iris_training_baked)\n\nranger can generate random forest models for classification, regression, survival( time series, time to event stuff). Extreme Forests are also supported, wherein all points in the dataset are used ( instead of bootstrap samples) along with feature bagging. We can also run the same model using the randomForest package:\n\nlibrary(randomForest,quietly = TRUE)\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:ranger':\n\n    importance\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\niris_rf <- \n  rand_forest(trees = 100) %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"randomForest\") %>% \n  fit(Species ~ ., data = iris_training_baked)\n\nIris Predictions\nThe predict() function run against a parsnip model returns a prediction tibble. By default, the prediction variable is called .pred_class.\n\npredict(object = iris_ranger, new_data = iris_testing_baked) %>%  \n  dplyr::bind_cols(iris_testing_baked) %>% \n  glimpse()\n\nRows: 60\nColumns: 5\n$ .pred_class  <fct> setosa, setosa, setosa, setosa, setosa, setosa, setosa, s‚Ä¶\n$ Sepal.Length <dbl> -0.8384977, -0.9624136, -1.7059090, -0.4667499, -1.210245‚Ä¶\n$ Sepal.Width  <dbl> 0.9083583, 0.6954619, -0.3690206, 1.3341513, -0.1561241, ‚Ä¶\n$ Petal.Width  <dbl> -1.2726177, -1.2726177, -1.2726177, -1.2726177, -1.409132‚Ä¶\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setosa, s‚Ä¶\n\n\nIris Classification Model Validation\nWe use metrics() function from the yardstick package to evaluate how good the model is.\n\npredict(iris_ranger, iris_testing_baked) %>%\n  dplyr::bind_cols(iris_testing_baked) %>% \n  yardstick::metrics(truth = Species, estimate = .pred_class)\n\n\n\n  \n\n\n\nWe can also check the metrics for randomForest model:\n\npredict(iris_rf, iris_testing_baked) %>%\n  dplyr::bind_cols(iris_testing_baked) %>% \n  yardstick::metrics(truth = Species, estimate = .pred_class)\n\n\n\n  \n\n\n\nIris Per-Classifier Metrics\nWe can use the parameter type = \"prob\" in the predict() function to obtain a probability score on each prediction. TBD: How is this prob calculated? Possible answer: the Random Forest model outputs its answer by majority voting across n trees. Each of the possible answers( i.e.¬†predictions) for a particular test datum gets a share of the vote, that represents its probability. Hence each dataum in the test vector can show a probability for the ‚Äúwinning‚Äù answer. ( Quite possibly we can get the probabilities for all possible outcomes for each test datum)\n\niris_ranger_probs <- \n  predict(iris_ranger, iris_testing_baked, type = \"prob\") %>%\n  dplyr::bind_cols(iris_testing_baked)\nglimpse(iris_ranger_probs)\n\nRows: 60\nColumns: 7\n$ .pred_setosa     <dbl> 0.989500000, 0.973833333, 0.870750000, 0.979083333, 0‚Ä¶\n$ .pred_versicolor <dbl> 0.007500000, 0.018500000, 0.128000000, 0.015416667, 0‚Ä¶\n$ .pred_virginica  <dbl> 0.003000000, 0.007666667, 0.001250000, 0.005500000, 0‚Ä¶\n$ Sepal.Length     <dbl> -0.8384977, -0.9624136, -1.7059090, -0.4667499, -1.21‚Ä¶\n$ Sepal.Width      <dbl> 0.9083583, 0.6954619, -0.3690206, 1.3341513, -0.15612‚Ä¶\n$ Petal.Width      <dbl> -1.2726177, -1.2726177, -1.2726177, -1.2726177, -1.40‚Ä¶\n$ Species          <fct> setosa, setosa, setosa, setosa, setosa, setosa, setos‚Ä¶\n\niris_rf_probs <- \n  predict(iris_rf, iris_testing_baked, type = \"prob\") %>%\n  dplyr::bind_cols(iris_testing_baked)\nglimpse(iris_rf_probs)\n\nRows: 60\nColumns: 7\n$ .pred_setosa     <dbl> 1.00, 1.00, 0.88, 0.99, 0.97, 0.99, 0.99, 0.91, 1.00,‚Ä¶\n$ .pred_versicolor <dbl> 0.00, 0.00, 0.12, 0.01, 0.03, 0.00, 0.00, 0.09, 0.00,‚Ä¶\n$ .pred_virginica  <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.01, 0.01, 0.00, 0.00,‚Ä¶\n$ Sepal.Length     <dbl> -0.8384977, -0.9624136, -1.7059090, -0.4667499, -1.21‚Ä¶\n$ Sepal.Width      <dbl> 0.9083583, 0.6954619, -0.3690206, 1.3341513, -0.15612‚Ä¶\n$ Petal.Width      <dbl> -1.2726177, -1.2726177, -1.2726177, -1.2726177, -1.40‚Ä¶\n$ Species          <fct> setosa, setosa, setosa, setosa, setosa, setosa, setos‚Ä¶\n\n# Tabulating the probabilities\nftable(iris_rf_probs$.pred_versicolor)\n\n  0 0.01 0.02 0.03 0.07 0.08 0.09 0.1 0.11 0.12 0.14 0.16 0.17 0.18 0.21 0.32 0.34 0.41 0.43 0.44 0.48 0.49 0.53 0.58 0.65 0.68 0.72 0.8 0.83 0.87 0.88 0.93 0.95 0.97 0.99  1\n                                                                                                                                                                              \n 11    4    2    2    2    1    5   1    1    2    1    1    1    1    1    1    1    1    2    1    1    1    1    1    1    1    1   1    1    2    1    1    1    1    2  1\n\nftable(iris_rf_probs$.pred_virginica)\n\n  0 0.01 0.03 0.04 0.07 0.1 0.13 0.14 0.17 0.27 0.3 0.34 0.42 0.47 0.5 0.51 0.55 0.57 0.58 0.64 0.68 0.79 0.82 0.83 0.86 0.89 0.9 0.91 0.92 0.93 0.97\n                                                                                                                                                     \n 18    4    1    2    1   1    1    1    1    1   1    1    1    1   1    1    1    2    1    1    1    2    1    1    2    3   1    2    2    1    2\n\nftable(iris_rf_probs$.pred_setosa)\n\n  0 0.01 0.02 0.03 0.06 0.07 0.09 0.88 0.91 0.97 0.99  1\n                                                        \n 19   10    8    1    1    1    1    1    2    2    6  8\n\n\n\n### Iris Classifier: Gain and ROC Curves\n\nWe can plot gain and ROC curves for each of these models\n\n::: {.cell hash='Random-Forests_cache/html/Gain and ROC Curves `ranger`_4d61a502168be25c4ca5da6162ae106e'}\n\n```{.r .cell-code}\niris_ranger_probs %>% \n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %>% \n  glimpse()\n\nRows: 133\nColumns: 5\n$ .level          <chr> \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"set‚Ä¶\n$ .n              <dbl> 0, 2, 3, 4, 5, 7, 8, 9, 10, 12, 13, 14, 16, 18, 19, 20‚Ä¶\n$ .n_events       <dbl> 0, 2, 3, 4, 5, 7, 8, 9, 10, 12, 13, 14, 16, 18, 19, 19‚Ä¶\n$ .percent_tested <dbl> 0.000000, 3.333333, 5.000000, 6.666667, 8.333333, 11.6‚Ä¶\n$ .percent_found  <dbl> 0.00000, 10.52632, 15.78947, 21.05263, 26.31579, 36.84‚Ä¶\n\niris_ranger_probs %>% \n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %>% \n  autoplot()\n\n\n\niris_ranger_probs %>% \n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %>% \n  glimpse()\n\nRows: 136\nColumns: 4\n$ .level      <chr> \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\"‚Ä¶\n$ .threshold  <dbl> -Inf, 0.000000000, 0.005888889, 0.006000000, 0.006666667, ‚Ä¶\n$ specificity <dbl> 0.0000000, 0.0000000, 0.2926829, 0.3414634, 0.4390244, 0.4‚Ä¶\n$ sensitivity <dbl> 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0‚Ä¶\n\niris_ranger_probs %>% \n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %>% \n  autoplot()\n\n\n\n:::\n\niris_rf_probs %>% \n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %>% \n  glimpse()\n\nRows: 82\nColumns: 5\n$ .level          <chr> \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"set‚Ä¶\n$ .n              <dbl> 0, 8, 14, 16, 18, 19, 20, 21, 22, 23, 31, 41, 60, 0, 1‚Ä¶\n$ .n_events       <dbl> 0, 8, 14, 16, 18, 19, 19, 19, 19, 19, 19, 19, 19, 0, 1‚Ä¶\n$ .percent_tested <dbl> 0.000000, 13.333333, 23.333333, 26.666667, 30.000000, ‚Ä¶\n$ .percent_found  <dbl> 0.000000, 42.105263, 73.684211, 84.210526, 94.736842, ‚Ä¶\n\niris_rf_probs %>% \n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %>% \n  autoplot()\n\n\n\niris_rf_probs %>% \n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %>% \n  glimpse()\n\nRows: 85\nColumns: 4\n$ .level      <chr> \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\"‚Ä¶\n$ .threshold  <dbl> -Inf, 0.00, 0.01, 0.02, 0.03, 0.06, 0.07, 0.09, 0.88, 0.91‚Ä¶\n$ specificity <dbl> 0.0000000, 0.0000000, 0.4634146, 0.7073171, 0.9024390, 0.9‚Ä¶\n$ sensitivity <dbl> 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0‚Ä¶\n\niris_rf_probs %>% \n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %>% \n  autoplot()\n\n\n\n\nIris Classifier: Metrics\n\npredict(iris_ranger, iris_testing_baked, type = \"prob\") %>% \n  bind_cols(predict(iris_ranger,iris_testing_baked)) %>% \n  bind_cols(select(iris_testing_baked,Species)) %>% \n  glimpse()\n\nRows: 60\nColumns: 5\n$ .pred_setosa     <dbl> 0.989500000, 0.973833333, 0.870750000, 0.979083333, 0‚Ä¶\n$ .pred_versicolor <dbl> 0.007500000, 0.018500000, 0.128000000, 0.015416667, 0‚Ä¶\n$ .pred_virginica  <dbl> 0.003000000, 0.007666667, 0.001250000, 0.005500000, 0‚Ä¶\n$ .pred_class      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setos‚Ä¶\n$ Species          <fct> setosa, setosa, setosa, setosa, setosa, setosa, setos‚Ä¶\n\npredict(iris_ranger, iris_testing_baked, type = \"prob\") %>% \n  bind_cols(predict(iris_ranger,iris_testing_baked)) %>% \n  bind_cols(select(iris_testing_baked,Species)) %>% \n  yardstick::metrics(data = ., truth = Species, estimate = .pred_class, ... = .pred_setosa:.pred_virginica)\n\n\n\n  \n\n\n# And for the `randomForest`method\n\npredict(iris_rf, iris_testing_baked, type = \"prob\") %>% \n  bind_cols(predict(iris_ranger,iris_testing_baked)) %>% \n  bind_cols(select(iris_testing_baked,Species)) %>% \n  glimpse()\n\nRows: 60\nColumns: 5\n$ .pred_setosa     <dbl> 1.00, 1.00, 0.88, 0.99, 0.97, 0.99, 0.99, 0.91, 1.00,‚Ä¶\n$ .pred_versicolor <dbl> 0.00, 0.00, 0.12, 0.01, 0.03, 0.00, 0.00, 0.09, 0.00,‚Ä¶\n$ .pred_virginica  <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.01, 0.01, 0.00, 0.00,‚Ä¶\n$ .pred_class      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setos‚Ä¶\n$ Species          <fct> setosa, setosa, setosa, setosa, setosa, setosa, setos‚Ä¶\n\npredict(iris_rf, iris_testing_baked, type = \"prob\") %>% \n  bind_cols(predict(iris_ranger,iris_testing_baked)) %>% \n  bind_cols(select(iris_testing_baked,Species)) %>% \n  yardstick::metrics(data = ., truth = Species, estimate = .pred_class, ... = .pred_setosa:.pred_virginica)"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html",
    "title": "ML - Classification",
    "section": "",
    "text": "Have you played a Childhood Game called 20 Questions? Someone has a ‚Äútarget‚Äù entity in mind ( a person or a thing or a literary character) and the others need to discover that entity by asking 20 questions.\n\nHow does one create questions in the game?\n\nCategories?\nNumbers? How?\nComparisons?\n\n\nWhat sort of answers can you expect for each question?"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#twenty-questions-game-as-a-play-with-data",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#twenty-questions-game-as-a-play-with-data",
    "title": "ML - Classification",
    "section": "Twenty Questions Game as a Play with Data‚Ä¶",
    "text": "Twenty Questions Game as a Play with Data‚Ä¶\nAssuming we think of a 20Q Target as say, celebrity singer like Taylor Swift, or a cartoon character like Thomas the Tank Engine, what would an underlying ‚Äúdata structure‚Äù look like? We would ask Questions for instance in the following order to find the target of Taylor Swift:\n\nHuman?(Yes)\nLiving?(Yes)\nMale?(No)\nCelebrity?(Yes)\nMusic?(Yes)\nUSA?(Yes)\n\nOh‚Ä¶Taylor Swift!!!\nLet us try to construct the ‚Äúdatasets‚Äù underlying this game!\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nOccupation\nSex\nLiving\nNationality\ngenre\npet\n\n\nTaylor Swift\nSinger\nF\nTRUE\nUSA\ncountry/rock\nScottish Fold Cats\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nType\nLiving\nhuman\nNationality\ncolour\nmaterial\n\n\nThomas, the Tank Engine\nCartoon Character\nFALSE\nFALSE\nUK\nblue\nmetal\n\n\n\n\nIt should be fairly clear that the Questions we ask are based on the COLUMNs in the respective 1-row datasets! The TARGET Column in both cases is the name column."
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#what-is-a-decision-tree",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#what-is-a-decision-tree",
    "title": "ML - Classification",
    "section": "What is a Decision Tree?",
    "text": "What is a Decision Tree?\nCan you imagine how the 20 Questions Game can be shown as a tree?\n\n\n\n\n\n\nEach Question we ask, based on one of the Feature columns, begets a Yes/NO answer and we turn the left or right accordingly. When we arrive at the leaf, we should be in a position to guess the answer !"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#twenty-times-20-questions",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#twenty-times-20-questions",
    "title": "ML - Classification",
    "section": "Twenty times 20 Questions !!",
    "text": "Twenty times 20 Questions !!\nWhat if the dataset we had contained many rows, instead of just one row? How would we play the 20Q Game in this situation? Here is a sample of the famous penguins dataset:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\nChinstrap\nDream\n46.8\n16.5\n189\n3650\nfemale\n2009\n\n\nGentoo\nBiscoe\n49.6\n15.0\n216\n4750\nmale\n2008\n\n\nGentoo\nBiscoe\n47.2\n13.7\n214\n4925\nfemale\n2009\n\n\nGentoo\nBiscoe\n48.4\n16.3\n220\n5400\nmale\n2008\n\n\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nfemale\n2007\n\n\nAdelie\nBiscoe\n38.2\n18.1\n185\n3950\nmale\n2007\n\n\nChinstrap\nDream\n50.5\n19.6\n201\n4050\nmale\n2007\n\n\nGentoo\nBiscoe\n51.3\n14.2\n218\n5300\nmale\n2009\n\n\nAdelie\nDream\n40.7\n17.0\n190\n3725\nmale\n2009\n\n\nAdelie\nBiscoe\n39.7\n18.9\n184\n3550\nmale\n2009\n\n\nChinstrap\nDream\n52.8\n20.0\n205\n4550\nmale\n2008\n\n\nAdelie\nDream\n32.1\n15.5\n188\n3050\nfemale\n2009\n\n\n\n\n\nAs before, we would need to look at the dataset as containing a TARGET column which we want to predict using several other FEATURE columns. Let us choose species.\nWhen we look at the FEATURE columns, We would need to formulate questions based on entire columns at a time. For instance:\n\n\n‚ÄúIs the bill_length_mm* greater than 45mm?‚Äù considers the entire bill_length_mm* FEATURE column\nIs the sex female? considers the entire sex column\n\nIf the specific FEATURE column is a Numerical (N) variable, the question would use some ‚Äúthresholding‚Äù as shown in the question above, to convert the Numerical Variable into a Categorical variable.\nIf a specific FEATURE column is a Categorical (C) variable, the question would be like a filter operation in Excel.\nEither way, we end up answering with a smaller and smaller subset of rows in the dataset, to which the questions are answered with a Yes. It is as if we played many 20 Questions games in parallel, since there are so many simultaneous ‚Äúanswers‚Äù!\nOnce we exhaust all the FEATURE columns, then what remains is a subset (i.e.¬†rows) of the original dataset and we read off the TARGET column, which should now contain a set of identical entries, e.g. ‚ÄúAdelie‚Äù. Thus we can extend a single-target 20Q game to a multiple-target one using a larger dataset. ( Note how the multiple targets are all the same: ‚ÄúAdelie‚Äù, or ‚ÄúGentoo‚Äù, or ‚ÄúChinstrap‚Äù)\nThis forms the basic intuition for a Machine Learning Algorithm called a Decision Tree.\nDecision Tree in Orange\nLet us visualize this Decision Tree in Orange. Look at the now famous penguins dataset, available here:\nhttps://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\nWe see that there are three species of penguins, that live on three islands. The measurements for each penguin are flipper_length_mm, bill_length_mm, bill_depth_mm, and body_mass_g.\n\nTask 1: Create a few data visualizations for the variables, and pairs of variables from this dataset.\nTask 2: Can you inspect the visualizations and imagine how each of this dataset can be used in a 20 Questions Game, to create a Decision Tree for this dataset as shown below?\n\n\n\nPenguins Decision Tree!\n\n\nWhat did we learn?\n\nThe 20Q Game can be viewed as a ‚ÄúDecision Tree‚Äù of Questions and Answers,\nEach fork in the game is a Question.\nDepending upon whether the current answer is yes or no, we turn in one direction or the other.\nEach of our questions is based on the information available in one or other of the columns!!\nWe arrive at a final ‚Äúanswer‚Äù or ‚Äútarget‚Äù after a particular sequence of yes/no answers. This is the one of the leaf nodes in the Tree.\nThe island and the species columns are categories and are especially suited to being the targets for a 20 Questions Game.\nWe can therefore use an entire column of data as our 20Questions target, rather than just one entity, person.\n\nThis is how we will use this Game as a Model for our first ML algorithm, classification using Decision Trees."
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#how-do-we-make-predictions-using-our-decision-tree",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#how-do-we-make-predictions-using-our-decision-tree",
    "title": "ML - Classification",
    "section": "How do we Make Predictions using our Decision Tree",
    "text": "How do we Make Predictions using our Decision Tree\nOur aim is to make predictions. Predictions of what? When we are given new unseen data in the same format, we should be able to predict TARGET variable using the same FEATURE columns.\nNOTE: This that is usually a class/category (We CAN also predict a numerical value with a Decision Tree; but we will deal with that later.)\nIn order to make predictions with completely unseen data, we need to first check if the algorithm is working well with known data. The way to do this is to use a large portion of data to design the tree, and then use the tree to predict some aspect of the remaining, but similar, data. Let us split the penguins dataset into two pieces: a training set to design our tree, and a test set to check how it is working.\nDownload this penguin tree file and open it in Orange.\nHow good are the Predictions? What is the Classification Error Rate?"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#how-many-trees-do-we-need-enter-the-random-forest",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#how-many-trees-do-we-need-enter-the-random-forest",
    "title": "ML - Classification",
    "section": "How Many Trees do we Need? Enter the Random Forest!",
    "text": "How Many Trees do we Need? Enter the Random Forest!\nCheck all your individual Decision Trees: do they ask the same Questions? Do they fork in the same way? Yes, they all seem to use the same set of parameters to reach the target. So they are capable of being ‚Äúbiased‚Äù and make the same mistakes. So we ask: Does it help to use more than one tree, if all the questions/forks in the Trees are similar?\nNo‚Ä¶we need different Trees to be able to ask different questions, based on different variables or features in the data. That will make the Trees as different as possible and so‚Ä¶unbiased. This is what we also saw when we played 20Q: offbeat questions opened up some avenues for predicting the answer/target.\nA forest of such trees is called the Wild Wood a Random Forest !"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#an-introduction-to-random-forests",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#an-introduction-to-random-forests",
    "title": "ML - Classification",
    "section": "An Introduction to Random Forests",
    "text": "An Introduction to Random Forests\nIn the Random Forest method, we do as follows:\n\nSplit the dataset into training and test subsets (70::30 proportion is very common). Keep aside the testing dataset for final testing.\nDecide on a number of trees, say 100-500 in the forest.\nTake the training dataset and repeatedly sample some of the rows in it. Rows can be repeated too; this is called bootstrap sampling.\nGive this sampled training set to each tree. Each tree develops a question from this dataset, in a random fashion, using a randomly chosen variable. E.g. with penguins, if our target is species, then some trees will will use island, some others will use body_mass_g and some others may use bill_length_mm.\nEach tree will ‚Äúgrow its questions‚Äù in a unique way !! Since the questions are possibly based on a different variable at each time, the trees will grow in very different ways.\nStop when the required accuracy has been achieved (the sets contain observations/rows from only one species predominantly)\nWith the test set let each tree vote on which species it has decided upon. Take the majority vote.\n\nPhew!!\nLet‚Äôs get a visual sense of how all this works:\nhttps://waternova.github.io/random-forest-viz/"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#random-forest-classification-for-heart-patients",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#random-forest-classification-for-heart-patients",
    "title": "ML - Classification",
    "section": "Random Forest Classification for Heart Patients",
    "text": "Random Forest Classification for Heart Patients\nDo you want to develop an ML model for heart patients? We have a dataset of heart patients at the University of California, Arvind Irvine ML Dataset Repository\nHeart Patient Data. Import into Orange !!\nWhat are the variables?\n\n(age): age in years\n(sex): 1 = male; 0 = female\n(cp): chest-pain type( 4 types, 1/2/3/4)\n(trestbps): resting blood pressure (in mm Hg on admission to the hospital)\n(chol) : serum cholesterol in mg/dl\n(fbs): (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n(restecg): resting electrocardiograph results (0 = normal; 1= ST-T wave abnormality; 3 = LV hypertrophy)\n(thalach): maximum heart rate achieved\n(exang): exercise induced angina (1 = yes; 0 = no) (remember Puneet Rajkumar)\n(oldpeak): ST depression induced by exercise relative to rest\n(slope): the slope of the peak exercise ST segment\n\nValue 1: upsloping\nValue 2: flat\nValue 3: downsloping\n\n\n(ca): number of major vessels (0-3) colored by fluoroscopy\n(thal): 3 = normal; 6 = fixed defect; 7 = reversible defect\n(num) : the target attribute, diagnosis of heart disease (angiographic disease status)\n\nValue 0: < 50% diameter narrowing\nValue 1: > 50% diameter narrowing\n(in any major vessel: attributes 59 through 68 are vessels)\n\n\n\nWe will create a Random Forest Model for this dataset, and compare with the Desision Tree for the same dataset."
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#how-good-is-my-random-forest",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#how-good-is-my-random-forest",
    "title": "ML - Classification",
    "section": "How good is my Random Forest?",
    "text": "How good is my Random Forest?\n\nClassification Error\n\nGini Impurity\n\nCross Entropy"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#references",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#references",
    "title": "ML - Classification",
    "section": "References",
    "text": "References\n\nhttps://towardsdatascience.com/data-science-made-easy-data-modeling-and-prediction-using-orange-f451f17061fa\nThe beauty of Random Forests: https://orangedatamining.com/blog/2016/12/22/the-beauty-of-random-forest/\nPythagorean Trees for Random Forests: https://orangedatamining.com/blog/2016/07/29/pythagorean-trees-and-forests/\ndata.tree sample applications, Christoph Glur, 2020-07-31. https://cran.r-project.org/web/packages/data.tree/vignettes/applications.html\nhttps://ryjohnson09.netlify.app/post/caret-and-tidymodels/"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html",
    "href": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html",
    "title": "ML - Clustering",
    "section": "",
    "text": "Quoting from http://baoqiang.org/?p=579\n\nThese two are arguably the two commonly used cluster methods. One of the reasons is that they are easy to use and also somehow straightforward. So how do they work?\nk-Nearest-Neighbour: Provide N n-dimension entries with known associated classes for each entry, the number of classes is k, that is, \\[\n\\{\\vec{x_i}, y_i\\} ,\\ \\vec{x_i} \\in\\ {\\Re^{n}}\\ , y_i\\ = \\{c_1,...c_k\\},\ni = 1...N\n\\]\nFor a new entry \\(\\vec{v_j}\\), to which class should it belong? We need use a distance measure to get the k closest entries of the new entry , the final decision is simple majority vote based the closest k neighbors. The distance metric could be euclidean or other similar ones.\n\n\nK-means: Given N n-dimension entries and classify them in k classes. At first, we randomly choose k entries and assign them to k clusters. They are the seed classes. Then we calculate the distance between each entry and each class. Each entry will be assigned into one class in terms of the its distance to each class, i.e., assign the entry to its closest class. After the assignment is complete, we then calculate the centroid of each class based on their new members. After the centroid calculation, we go back to the distance calculation and therefore new round classification. We stop the iteration when there is convergence,i.e,, no new centroid and classification.\nThe two methods are all semi-supervised learning algorithms because they do need we provide the number of clusters prior the clustering."
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#workflow-using-orange",
    "href": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#workflow-using-orange",
    "title": "ML - Clustering",
    "section": "Workflow using Orange",
    "text": "Workflow using Orange"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#workflow-using-radiant",
    "href": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#workflow-using-radiant",
    "title": "ML - Clustering",
    "section": "Workflow using Radiant",
    "text": "Workflow using Radiant"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#workflow-using-r",
    "href": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#workflow-using-r",
    "title": "ML - Clustering",
    "section": "Workflow using R",
    "text": "Workflow using R"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#conclusion",
    "href": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#conclusion",
    "title": "ML - Clustering",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#references",
    "href": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#references",
    "title": "ML - Clustering",
    "section": "References",
    "text": "References\n\nK-means Cluster Analysis. UC Business Analytics R Programming Guide https://uc-r.github.io/kmeans_clustering#optimal\nThean C Lim. Clustering: k-means, k-means ++ and gganimate. https://theanlim.rbind.io/post/clustering-k-means-k-means-and-gganimate/"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/50-ModelTimeSeries/index.html",
    "href": "content/courses/Analytics/Predictive/Modules/50-ModelTimeSeries/index.html",
    "title": "üïî Modelling Time Series",
    "section": "",
    "text": "#|label:  setup\n#|include: TRUE\n\n# knitr::opts_chunk$set(echo = TRUE,message = FALSE, warning = FALSE)\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(mosaic)\n\nlibrary(tsibble)\nlibrary(feasts)\nlibrary(fable)\nlibrary(sweep) # Tidy forecast Model objects\nlibrary(timetk)\nlibrary(forecast)\nlibrary(prophet)\n\nlibrary(fontawesome)"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/50-ModelTimeSeries/index.html#introduction",
    "href": "content/courses/Analytics/Predictive/Modules/50-ModelTimeSeries/index.html#introduction",
    "title": "üïî Modelling Time Series",
    "section": "\n Introduction",
    "text": "Introduction\nIn this module we will look at modelling of time series. We will start with the simplest of exponential models and go all the way through ARIMA and forecasting with Prophet.\nFirst, some terminology!"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/50-ModelTimeSeries/index.html#additive-and-multiplicative-time-series-models",
    "href": "content/courses/Analytics/Predictive/Modules/50-ModelTimeSeries/index.html#additive-and-multiplicative-time-series-models",
    "title": "üïî Modelling Time Series",
    "section": "Additive and Multiplicative Time Series Models",
    "text": "Additive and Multiplicative Time Series Models\nAdditive Time Series can be represented as:\n\\[\nY_t‚ÄÑ=‚ÄÑS_t‚ÄÖ+‚ÄÖT_t‚ÄÖ+‚ÄÖœµ_t\n\\]\nMultiplicative Time Series can be described as:\n\\[\nY_t‚ÄÑ=‚ÄÑS_t‚ÄÖ√ó‚ÄÖT_t‚ÄÖ√ó‚ÄÖœµ_t\n\\]\nLet us consider a Multiplicative Time Series, pertaining to sales of souvenirs at beaches in Australia: The time series looks like this:\n\n\n\n\n\n\nNote that along with the trend, the amplitude of both seasonal and noise components are also increasing in a multiplicative here !! A multiplicative time series can be converted to additive by taking a log of the time series."
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/50-ModelTimeSeries/index.html#stationarity",
    "href": "content/courses/Analytics/Predictive/Modules/50-ModelTimeSeries/index.html#stationarity",
    "title": "üïî Modelling Time Series",
    "section": "Stationarity",
    "text": "Stationarity\nA time series is said to be stationary if it holds the following conditions true:\n\nThe mean value of time-series is constant over time, which implies,the trend component is nullified/constant.\nThe variance does not increase over time.\nSeasonality effect is minimal.\n\nThis means it is devoid of trend or seasonal patterns, which makes it looks like a random white noise irrespective of the observed time interval.( i.e.¬†self-similar and fractal)"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/50-ModelTimeSeries/index.html#a-bit-of-forecasting",
    "href": "content/courses/Analytics/Predictive/Modules/50-ModelTimeSeries/index.html#a-bit-of-forecasting",
    "title": "üïî Modelling Time Series",
    "section": "A Bit of Forecasting?",
    "text": "A Bit of Forecasting?\nWe are always interested in the future. We will do this in three ways:\n\nuse Simple Exponential Smoothing\nuse a package called forecast to fit an ARIMA (Autoregressive Moving Average Integrated Model) model to the data and make predictions for weekly sales;\nAnd do the same using a package called prophet.\n\nForecasting using Exponential Smoothing\nFor example, the file contains total annual rainfall in inches for London, from 1813-1912 (original data from Hipel and McLeod, 1994).\n\nrain <- scan(\"https://robjhyndman.com/tsdldata/hurst/precip1.dat\", skip = 2)\nrainseries <- ts(rain, start = c(1813))\nplot(rainseries)\n\n\n\n\nThere is a nearly constant value of about 25 around which there are random fluctuations and it seems to be an additive model. How can we make forecasts with this time series?\nA deliberate detour:\nLet‚Äôs see some quick notation to aid understanding: Much of smoothing is based on the high school concept of a straight line, \\(y = m*x + c\\).\nIn the following, we choose to describe the models with:\n\n\n\\(y\\) : the actual values in the time series\n\n\\(\\hat y\\) : our predictions from whichever model we create\n\n\\(l\\) : a level or mean as forecast;\n\n\\(b\\) : a trend variable; akin to the slope in the straight line equation;\n\n\\(s\\) : seasonal component of the time series. Note that this is a set of values that stretch over one cycle of the time series.\n\nIn Exponential Smoothing and Forecasting, we make three models of increasing complexity:\n\nSimple Exponential Model: Here we deal only with the mean or level aspect of the (decomposed) time series and make predictions with that.\nHolt Model: Here we use the level and the trend from the decomposed time series for predictions\nHolt-Winters Model: Here we use the level, the trend, and the seasonal component from the decomposed time series for predictions.\n\n\n\n\n\n[<start>st]->[<input>input]\n[<input> input]->[<package> Time  Series|Decomposition]\n[<package> Time  Series|Decomposition]->[<component> Mean/Level]\n[<package> Time  Series|Decomposition]->[<component> Slope/Trend]\n[<package> Time  Series|Decomposition]->[<component> Seasonal]\n\n//Simple Exponential Smoothing\n[<component> Mean/Level]->[Delay A1]\n[Delay A1]->[Delay A2]\n[Delay A2]->[Delay A3]\n[Delay A3]...->...[Delay AN]\n[Delay A1]->[<state> A1]\n[Delay A2]->[<state> A2]\n[Delay A3]->[<state> A3]\n[Delay AN]->[<state> AN]\n[<state> AN]---([<note> $$alpha(1-alpha)^i$$]\n\n[<state> A1]->[<state> Add1]\n[<state> A2]->[<state> Add1]\n[<state> A3]->[<state> Add1]\n[<state> AN]->[<state> Add1]\n[<state> Add1]->[<end> Output]\n\n//Holt \n[<component> Slope/Trend]->[Delay B1]\n[Delay B1]->[Delay B2]\n[Delay B2]->[Delay B3]\n[Delay B3]...->...[Delay BN]\n[Delay B1]->[<state> B1]\n[Delay B2]->[<state> B2]\n[Delay B3]->[<state> B3]\n[Delay BN]->[<state> BN]\n[<state> BN]---([<note> $$beta(1-beta)^i$$]\n[<state> B1]->[<state> Add2]\n[<state> B2]->[<state> Add2]\n[<state> B3]->[<state> Add2]\n[<state> BN]->[<state> Add2]\n[<state> Add2]->[<end> Output]\n\n// Holt Winters\n[<component> Seasonal]->[Delay C1]\n[Delay C1]->[Delay C2]\n[Delay C2]->[Delay C3]\n[Delay C3]...->...[Delay CN]\n[Delay C1]->[<state> C1]\n[Delay C2]->[<state> C2]\n[Delay C3]->[<state> C3]\n[Delay CN]->[<state> CN]\n[<state> CN]---([<note> $$gamma(1-gamma)^i$$]\n[<state> C1]->[<state> Add3]\n[<state> C2]->[<state> Add3]\n[<state> C3]->[<state> Add3]\n[<state> CN]->[<state> Add3]\n[<state> Add3]->[<end> Output]\n\n// Final Output\n[<end> Output]->[<receiver> Forecast]\n\n\n\n\n\nSimple Smoothing is smoothing based forecasting using just the level ( i.e.¬†mean) of the Time Series to make forecasts.\nDouble exponential smoothing, or Holt Smoothing Model, is just exponential smoothing applied to both level and trend.\nThe idea behind triple exponential smoothing, or the Holt-Winters Smoothing Model, is to apply exponential smoothing to the seasonal components in addition to level and trend.\nWhat does ‚ÄúExponential‚Äù mean?\nAll three models use memory: at each time instant in the Time Series, a set of past values, along with the present sample is used to make a prediction of the relevant parameter ( level / slope / seasonal). These are then added together to make the forecast.\nThe memory in each case controlled by a parameter: alpha for the estimate of the level beta for the slope estimate, and gamma for the seasonal component estimate at the current time point. All these parameters are between 0 and 1. The model takes a weighted average of past values of each parameter. The weights are derived in the form of \\(\\alpha(1-\\alpha)^i\\), where \\(i\\) defines how old the sample is compared to the present one, thus forming a set of weights that decrease exponentially with delay. Values of \\(\\alpha, \\beta. \\gamma\\) that are close to 0 mean that significant weightage is placed on observations in the past.(Memory is ‚Äústronger‚Äù). To express this in mathematical notation we now need three equations: one for level, one for the trend and one to combine the level and trend to get the expected \\(\\hat y\\).\nTo make forecasts using simple exponential smoothing in R, we can use the HoltWinters() function in R, or the forecast::ets() function from forecasts. This latter function is more powerful.\n\nargs(HoltWinters)\n\nfunction (x, alpha = NULL, beta = NULL, gamma = NULL, seasonal = c(\"additive\", \n    \"multiplicative\"), start.periods = 2, l.start = NULL, b.start = NULL, \n    s.start = NULL, optim.start = c(alpha = 0.3, beta = 0.1, \n        gamma = 0.1), optim.control = list()) \nNULL\n\nargs(forecast::ets)\n\nfunction (y, model = \"ZZZ\", damped = NULL, alpha = NULL, beta = NULL, \n    gamma = NULL, phi = NULL, additive.only = FALSE, lambda = NULL, \n    biasadj = FALSE, lower = c(rep(1e-04, 3), 0.8), upper = c(rep(0.9999, \n        3), 0.98), opt.crit = c(\"lik\", \"amse\", \"mse\", \"sigma\", \n        \"mae\"), nmse = 3, bounds = c(\"both\", \"usual\", \"admissible\"), \n    ic = c(\"aicc\", \"aic\", \"bic\"), restrict = TRUE, allow.multiplicative.trend = FALSE, \n    use.initial.values = FALSE, na.action = c(\"na.contiguous\", \n        \"na.interp\", \"na.fail\"), ...) \nNULL\n\n\nTo use HoltWinters() for simple exponential smoothing, we need to set the parameters beta=FALSE and gamma=FALSE in the HoltWinters() function (the beta and gamma parameters are used for double exponential smoothing, or triple exponential smoothing.\nTo use forecast::ets, we set the model argument to ‚ÄúANN‚Äù, ‚ÄúAAN‚Äù, and ‚ÄúAAA‚Äù respectively for each of the three smoothing models.\nNote: The HoltWinters() function returns a list variable, that contains several named elements.\n\nrainseriesforecasts <- forecast::ets(rainseries, model = \"ANN\")\n# class(rainseriesforecasts)\n# str(rainseriesforecasts)\nplot(rainseriesforecasts)\n\n\n\nplot(forecast(rainseriesforecasts, 10))\n\n\n\n\nARIMA\nWe can also use past trends and seasonality in the data to make predictions about the future using the forecast package. Here we use an auto ARIMA model to guess at the trend in the time series. Then we use that model to forecast a few periods into the future.\nMathematically an ARIMA model can be shown as follows:\n\n\n\n\n\n\nWe will use the familiar Walmart Sales dataset, and try to predict weekly sales for one of the Departments.\n\ndata(\"walmart_sales_weekly\")\nwalmart_wide <- walmart_sales_weekly %>% \n  pivot_wider(., id_cols = c(Date), \n              names_from = Dept, \n              values_from = Weekly_Sales,\n              names_prefix = \"Sales_\")\n\n## forecast::auto.arima needs a SINGLE time series, so we pick one, Dept95\nsales_95_ts <- walmart_wide %>% \n  select(Sales_95) %>% \n  ts(start = c(2010,1), end = c(2012,52),frequency = 52)\nsales_95_ts\n\nTime Series:\nStart = c(2010, 1) \nEnd = c(2012, 52) \nFrequency = 52 \n  [1] 106690.06 111390.36 107952.07 103652.58 112807.75 112048.41 117716.13\n  [8] 113117.35 111466.37 116770.82 126341.84 110204.77 107648.14 125592.28\n [15] 120247.90 120036.99 121902.19 133056.97 131995.00 134118.05 120172.47\n [22] 124821.44 126241.20 121386.73 116256.35 108781.57 131128.96 131288.83\n [29] 124601.48 117929.58 124220.10 125027.49 124372.90 114702.69 113009.41\n [36] 120764.22 123510.99 110052.15 105793.40 110332.92 110209.31 107544.02\n [43] 106015.41 100834.31 111384.36 116521.67 121695.13  93676.95 107317.32\n [50] 109955.90 103724.16  99043.34 114270.08 117548.75 112165.80 107742.95\n [57] 116225.68 120621.32 123405.41 122280.13 112905.09 126746.25 126834.30\n [64] 118632.26 111764.31 120882.84 124953.94 112581.20 119815.67 135260.49\n [71] 136364.46 135197.63 121814.84 128054.88 133213.04 127906.50 121483.11\n [78] 117284.94 138538.47 138567.10 133260.84 122721.92 130446.34 133762.77\n [85] 133939.40 116165.28 115663.78 132805.42 125954.30 116931.34 108018.21\n [92] 114793.92 115047.16 113966.34 112688.97 102798.99 119053.80 120721.07\n [99] 125041.39  93358.91 116427.93 118685.12 113021.23 102202.04 115507.25\n[106] 125038.09 119807.63 110870.94 118406.27 125840.82 132318.50 117030.73\n[113] 127706.00 137958.76 129438.22 123172.79 118589.44 130920.36 131341.85\n[120] 129031.19 127603.00 130573.37 139857.10 140806.36 124594.40 131935.56\n[127] 148798.05 129724.74 126861.49 121030.79 134832.22 137408.20 136264.68\n[134] 118845.34 124741.33 140657.40 128542.73 119121.35 115326.47 127009.22\n[141] 124559.93 123346.24 117375.38 106690.06 111390.36 107952.07 103652.58\n[148] 112807.75 112048.41 117716.13 113117.35 111466.37 116770.82 126341.84\n[155] 110204.77 107648.14\n\narima_dept_95 <- forecast::auto.arima(y = sales_95_ts)\narima_dept_95\n\nSeries: sales_95_ts \nARIMA(0,1,1)(0,1,0)[52] \n\nCoefficients:\n          ma1\n      -0.8842\ns.e.   0.0530\n\nsigma^2 = 29974424:  log likelihood = -1033.02\nAIC=2070.03   AICc=2070.15   BIC=2075.3\n\nplot(arima_dept_95)\n\n\n\n# Use the model to forecast 12 weeks into the future\nsales95_forecast <- forecast(arima_dept_95, h = 12)\n\n# Plot the forecast. Again, we can use autoplot.\nautoplot(sales95_forecast) +\n  theme_minimal()\n\n\n\n\nWe‚Äôre fairly limited in what we can actually tweak when using autoplot(), so instead we can convert the forecast object to a data frame and use ggplot() like normal:\n\n# Get data out of this weird sales95_forecast object\nsales95_forecast\n\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n2013.000       116571.1 109554.8 123587.5 105840.6 127301.7\n2013.019       126102.0 119038.7 133165.2 115299.7 136904.3\n2013.038       120871.5 113761.7 127981.4 109998.0 131745.1\n2013.058       111934.8 104778.7 119091.0 100990.5 122879.2\n2013.077       119470.2 112268.0 126672.3 108455.5 130484.9\n2013.096       126904.7 119656.9 134152.5 115820.1 137989.3\n2013.115       133382.4 126089.2 140675.6 122228.3 144536.5\n2013.135       118094.6 110756.3 125433.0 106871.6 129317.7\n2013.154       128769.9 121386.7 136153.1 117478.2 140061.6\n2013.173       139022.7 131594.8 146450.5 127662.8 150382.5\n2013.192       130502.1 123030.0 137974.3 119074.5 141929.8\n2013.212       124236.7 116720.5 131752.9 112741.7 135731.7\n\nsales95_forecast_tidy <- sweep::sw_sweep(sales95_forecast, \n                                         fitted = TRUE, \n                                         timetk_idx = TRUE)\n\nWarning in sw_sweep.forecast(sales95_forecast, fitted = TRUE, timetk_idx =\nTRUE): Object has no timetk index. Using default index.\n\nsales95_forecast_tidy\n\n\n\n  \n\n\n# For whatever reason, the date column here is a special type of variable called\n# \"yearmon\", which ggplot doesn't know how to deal with (like, we can't zoom in\n# on the plot with coord_cartesian). We use zoo::as.Date() to convert the\n# yearmon variable into a regular date\nsales95_forecast_tidy_real_date <- sales95_forecast_tidy %>% \n  mutate(actual_date = zoo::as.Date(index, frac = 1))\nsales95_forecast_tidy_real_date\n\n\n\n  \n\n\n## Stuck here!! 1975 !!! What the hell!!!\n## \n## \n# Plot this puppy!\nggplot(sales95_forecast_tidy_real_date, aes(x = actual_date, y = value, color = key)) +\n  geom_ribbon(aes(ymin = lo.95, ymax = hi.95), \n              fill = \"#3182bd\", color = NA) +\n  geom_ribbon(aes(ymin = lo.80, ymax = hi.80, fill = key), \n              fill = \"#deebf7\", color = NA, alpha = 0.8) +\n  geom_line(size = 1) + \n  geom_point(size = 0.5) +\n  labs(x = NULL, y = \"sales95\") +\n  scale_y_continuous(labels = scales::comma) +\n  # Zoom in on 2012-2016\n  coord_cartesian(xlim = ymd(c(\"2012-01-01\", \"2016-01-01\"))) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\n\n\n\n\nA Bit of Forecasting?\nWe are always interested in the future. We will do this in three ways:\n\nuse Simple Exponential Smoothing\nuse a package called forecast to fit an ARIMA (Auto-regressive Moving Average Integrated Model) model to the data and make predictions for weekly sales;\nAnd do the same using a package called ‚Äôprophet`."
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/50-ModelTimeSeries/index.html#workflow-using-orange",
    "href": "content/courses/Analytics/Predictive/Modules/50-ModelTimeSeries/index.html#workflow-using-orange",
    "title": "üïî Modelling Time Series",
    "section": "Workflow using Orange",
    "text": "Workflow using Orange"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/50-ModelTimeSeries/index.html#workflow-using-radiant",
    "href": "content/courses/Analytics/Predictive/Modules/50-ModelTimeSeries/index.html#workflow-using-radiant",
    "title": "üïî Modelling Time Series",
    "section": "Workflow using Radiant",
    "text": "Workflow using Radiant"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/50-ModelTimeSeries/index.html#workflow-using-r",
    "href": "content/courses/Analytics/Predictive/Modules/50-ModelTimeSeries/index.html#workflow-using-r",
    "title": "üïî Modelling Time Series",
    "section": "Workflow using R",
    "text": "Workflow using R\n fred_raw.csv"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/50-ModelTimeSeries/index.html#conclusion",
    "href": "content/courses/Analytics/Predictive/Modules/50-ModelTimeSeries/index.html#conclusion",
    "title": "üïî Modelling Time Series",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/50-ModelTimeSeries/index.html#references",
    "href": "content/courses/Analytics/Predictive/Modules/50-ModelTimeSeries/index.html#references",
    "title": "üïî Modelling Time Series",
    "section": "References",
    "text": "References\n1, Shampoo Dataset Brownlee: https://raw.githubusercontent.com/jbrownlee/Datasets/master/shampoo.csv"
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/listing.html",
    "href": "content/courses/Analytics/Prescriptive/listing.html",
    "title": "Prescriptive Analytics",
    "section": "",
    "text": "üìê Intro to Linear Programming\n\n\n\n\n\n\n\n\n\n\n\n\nNov 10, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\n\n\nüí≠ The Simplex Method - Intuitively\n\n\n\n\n\nWe will look at developing an intuitive understanding of the Simplex Method for Linear Programming.\n\n\n\n\n\n\nNov 14, 2022\n\n\n\n\n\n\n\n\nüìÖ The Simplex Method - In Excel\n\n\n\n\n\nWe will look at mechanizing the Simplex Method in Excel\n\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html",
    "href": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html",
    "title": "üìê Intro to Linear Programming",
    "section": "",
    "text": "What is Linear Programming?"
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#demonstration-of-level-curve",
    "href": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#demonstration-of-level-curve",
    "title": "üìê Intro to Linear Programming",
    "section": "Demonstration of Level Curve",
    "text": "Demonstration of Level Curve"
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#linear-programming-solver",
    "href": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#linear-programming-solver",
    "title": "üìê Intro to Linear Programming",
    "section": "Linear Programming Solver",
    "text": "Linear Programming Solver"
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#linear-programming-in-3d-view",
    "href": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#linear-programming-in-3d-view",
    "title": "üìê Intro to Linear Programming",
    "section": "Linear Programming in 3D view",
    "text": "Linear Programming in 3D view"
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#linear-programming-interactive",
    "href": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#linear-programming-interactive",
    "title": "üìê Intro to Linear Programming",
    "section": "Linear Programming Interactive",
    "text": "Linear Programming Interactive\nLet us say we have a Linear Programming problem with 3 variables: We define the model:\n\\[\nMaximise : 20x_1 + 10x_2 + 15x_3\\\\\nSubject \\ to \\\\\n\\\\\nx_1 + x_2 + x_3 <= 10\\\\\n3x_1 + x_3 <= 24\n\\]\n\n\n\n\n\n\nHere is the interactive LP Polytope:"
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#references",
    "href": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#references",
    "title": "üìê Intro to Linear Programming",
    "section": "References",
    "text": "References\n\nVirginia Postrel, Operations Everything, Boston Globe, Hune 27, 2004. http://archive.boston.com/news/globe/ideas/articles/2004/06/27/operation_everything?pg=full"
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/20-SimplexMethodIntuitive/index.html",
    "href": "content/courses/Analytics/Prescriptive/Modules/20-SimplexMethodIntuitive/index.html",
    "title": "üí≠ The Simplex Method - Intuitively",
    "section": "",
    "text": "To be written up."
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/20-SimplexMethodIntuitive/index.html#a-random-walk-with-the-simplex-method",
    "href": "content/courses/Analytics/Prescriptive/Modules/20-SimplexMethodIntuitive/index.html#a-random-walk-with-the-simplex-method",
    "title": "üí≠ The Simplex Method - Intuitively",
    "section": "A Random Walk with the Simplex Method",
    "text": "A Random Walk with the Simplex Method\nLet us try to form a geometric intuition for the Simplex method.\nWe will define an LP problem, and geometrically traverse the steps the Simplex method might take to solve for the optimum solution.\nLet us define a problem:\n\\[\nMaximise\\ 7.75x_1 + 10x_2\\\\\n\\] \\[\nSubject\\ to\\\\\n  \\begin{cases}\n    C1: -3x_1 + 2x_2 &<= 3\\\\\n    C2: 2x_1 + 4x_2 &<= 27\\\\\n    C3: 9x_1 + 10x_2 &<= 90\\\\\n    x_1, x_2 >= 0\n  \\end{cases}\n\\]\nThe Objective function is: \\(7.75x_1 + 10x_2\\)\nThe Constraints are defined by the three inequalities \\(C1::C3\\). In order to plot these, we convert the inequalities to equalities and plot these as lines. Each line splits the \\(x_1:x_2\\) plane into two half-planes. The inequality part is then taken into account by choosing the appropriate half-plane created by the equation. The intersection of all the half-planes defined by the constraints is the Feasibility Region.\n\n\n\nThe Feasibility region for this LP problem is plotted below:\n\n\n\n\n\n\n\n\nThe corner points of the Feasibility Region are:\n\n\n\n\n  \n\n\n\nRecall that:\n\nThe optimum in an LP problem is found on the boundary, at one of the vertices\nAt each of these vertices one or more constraints (\\(C1::C_n\\)) is tight, i.e.¬†there is no slack."
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/20-SimplexMethodIntuitive/index.html#procedure",
    "href": "content/courses/Analytics/Prescriptive/Modules/20-SimplexMethodIntuitive/index.html#procedure",
    "title": "üí≠ The Simplex Method - Intuitively",
    "section": "Procedure",
    "text": "Procedure\n\nWe start with an arbitrary point on the edge of the Feasibility Region. \\(A = (0,0)\\) is a common choice. At this point, since all variables are \\(0\\), the objective function is also \\(0\\).\nWe (arbitrarily) decide to move along the boundary of the Feasibility Region, to another FSP. We arbitrarily chose the \\(x_1\\) axis, and set/keep \\(x_2 = 0\\). We now wish to find out the \\(x_1\\) coordinate of the next FSP point. This would be at the intersection of the \\(x_1\\) axis and one of the Constraint lines.\nAll the three Constraint Lines would possibly intersect the \\(x_1\\) axis. We need to choose that intercept point that has the smallest, non-negative \\(x_1\\) intercept value. (Why?)\nSo, which Constraint Line intersects the \\(x_1\\) axis at the smallest value? Is it point B, or point F?\nTo find out, we substitute \\(x_2 = 0\\) in each of the Constraint equations, and solve for the \\(x_1\\):\\[\n\\begin{cases}\nC1: -3x_1 + 2 \\times 0 = 3 \\ => x_1 = \\color{red}{-1}\\\\\nC2: 2x_1 + 4\\times0 = 27 \\ => x_1 = 13.5\\ Point\\ F\\\\\n   {\\mathbf{ \\color{lightgreen}{C3}: 9x_1 + 10\\times0 = 90 \\ => x_1 = 10\\  \\color{lightgreen}{Point\\ B}}}\n\\end{cases}\n\\]\nNegative values for any variable are not permitted. So the smallest value of intercept is \\(x_1 = 10\\) for Constraint \\(C3\\). We therefore move to point \\(B(10,0)\\). At this point the objective function has improved to:\n\n\\[\nObjective = 7.75\\times 10 + 10\\times0 = 77.5\\ at\\ Point\\ B\n\\]\n\nWe now start from Point B, and move to the next nearest point. In identical fashion to Step2, we ‚Äúimagine‚Äù that we move along a new axis defined by:\\[\nIntercept = Point\\¬†B(10,0)\\\\\n\\] \\[\nEquation = Constraint\\¬†C3:¬†9x_1 + 10x_2 = 90\\\\\n\\] We express \\(x_1\\) in terms of \\(x_2\\) with \\(C3\\): \\[\n\\hat C3:¬†x_1 = \\frac{90 - 10x_2}{9}\n\\] As in Step 2, we substitute this equation \\(\\hat C3\\) into the other two constraints, \\(C1\\) and \\(C2\\): \\[\n\\begin{cases}\nC1: -3\\times \\frac{90 - 10x_2}{9} + 2x_2 = 3 \\ => x_2 = 6.18\\ Point\\ K\\\\\n{\\mathbf{ \\color{lightgreen}{C2}: 2\\times \\frac{90 - 10x_2}{9}+ 4x_2 = 27 => x_2 = 3.93\\ \\color{lightgreen}{Point\\ C}}}\n\\end{cases}\n\\] As before we choose the smaller of the two intercepts, so \\(x_2 = 3.93\\). Calculating for \\(x_1\\), we get point \\(C(5.63, 3.93)\\). At this point the objective function has improved to:\n\n\\[\n7.75\\times 5.63 + 10\\times 3.93 = 82.9\\ at\\ Point\\ C\n\\]\n\nWe now proceed along the constraint line \\(C2\\) towards the next point. In identical fashion to Step 2 and 3, we ‚Äúimagine‚Äù that we move along a new axis defined by: \\[\nIntercept = Point\\¬†C(5.63,3.93)\n\\] \\[\nEquation = Constraint\\¬†C2:¬†2x_1 + 4x_2 = 27 \\\\\n\\] Again, We express \\(x_1\\) in terms of \\(x_2\\) with \\(C2\\) this time: \\[\n\\hat C2:¬†x_1 = \\frac{27 - 4x_2}{2}\n\\] As in Step 2 and, we substitute this equation \\(\\hat C2\\) into the other constraint, the only remaining \\(C1\\): \\[\n{\\mathbf C1: -3\\times \\frac{27 - 4x_2}{2} + 2x_2 = 3 \\ => x_2 = 5.44\\ Point\\ D\\\\}\n\\] Calculating for \\(x_1\\), we get point \\(C(2.63, 5.44)\\). At this point the objective function has improved decreased to: \\[\n7.75\\times 2.63 + 10\\times 5.44 = 74.8\\¬†at\\¬†Point\\¬†D\n\\] Since this value for the Objective function is smaller than that at the previous point, our search terminates and we decide that Point \\(C(5.63,3.93)\\) is the optimal point.\nSo the final result is: \\[\n   x_1(max) = 5.63\\\\\n\\] \\[\n   x_2(max) = 3.93\\\\\n\\] \\[\n   Maximum\\¬†Objective\\¬†Function\\¬†Value = 82.9\n\\] The final result is plotted below:"
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/20-SimplexMethodIntuitive/index.html#summary",
    "href": "content/courses/Analytics/Prescriptive/Modules/20-SimplexMethodIntuitive/index.html#summary",
    "title": "üí≠ The Simplex Method - Intuitively",
    "section": "Summary",
    "text": "Summary\nThe essence of this ‚Äúintuitive method‚Äù can be captured as follows:\n\nStart from a known simple point on the edge of Feasibility Region, e.g.¬†(0,0), since the two coordinate axes frequently form two edges to the Feasibility Region.\n\nMove along one of the axis to find a first adjacent edge point. This adjacent point corresponds to the ‚Äútightening‚Äù of one or other of the Constraint equations(i.e.¬†slack = 0 for that Constraint)\n\nCalculate the Objective function at that point.\n\nUse this new point as the next starting point and move along the Constraint line from the previous step.\n\nRepeat step 2 and 3, calculating the Objective function each time.\n\nKeep the solution point where the objective function hits a maximum, i.e.¬†when moving to the next point reduces the value of the Objective function."
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/30-SimplexMethodExcel/index.html",
    "href": "content/courses/Analytics/Prescriptive/Modules/30-SimplexMethodExcel/index.html",
    "title": "üìÖ The Simplex Method - In Excel",
    "section": "",
    "text": "Let us take the same problem as before:\n\\[\nMaximise\\ 7.75x_1 + 10x_2 \\\\\n\\] \\[\nSubject\\ to \\\\\n  \\begin{cases}\n    C1: -3x_1 + 2x_2 &<= 3 \\\\\n    C2: 2x_1 + 4x_2 &<= 27 \\\\\n    C3: 9x_1 + 10x_2 &<= 90 \\\\\n    x_1, x_2 >= 0\n  \\end{cases}\n\\]"
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/30-SimplexMethodExcel/index.html#procedure",
    "href": "content/courses/Analytics/Prescriptive/Modules/30-SimplexMethodExcel/index.html#procedure",
    "title": "üìÖ The Simplex Method - In Excel",
    "section": "Procedure",
    "text": "Procedure\n\nSet up an Excel sheet as shown in the picture below. We enter in the objective function and the constraints in tabular form as shown:\n\n\n\n\n\n\n\n\n\n\nNext we invoke the Solver Add-in: (Data -> Solver):\n\n\n\n\n\n\n\n\n\n\nWe set up the Solver for our problem as follows: Hit the SOLVE button.\n\n\n\n\n\n\n\n\n\n\nChoose to have all the three kinds of Reports from Solver (Answers, Sensitivity, and Limits).\n\n\n\n\n\n\n\n\n\nThis will create three new tabs which give additional information on:\n- How ‚Äúcentered‚Äù the solution is, or is it sensitive to variations of some parameters\n- How much slack do the individual constraints still have, at the end\nWe will discuss this in class!\nThe complete Excel file is here for your reference."
  },
  {
    "objectID": "content/courses/Analytics/Tools/05-Intro-to-Orange/orange.html",
    "href": "content/courses/Analytics/Tools/05-Intro-to-Orange/orange.html",
    "title": "üêâ Introduction to Orange",
    "section": "",
    "text": "Orange is a visual drag-and-drop tool for\n\nData visualization\nMachine Learning\nData mining\n\nand much more. Here is what it looks like:\n\nAll operations are done using a visual menu-driven interface. The visuals can be exported to PNG/SVG/PDF and the entire workflow can be exported into a Report Form and edited for presentation and sharing."
  },
  {
    "objectID": "content/courses/Analytics/Tools/05-Intro-to-Orange/orange.html#installing-orange",
    "href": "content/courses/Analytics/Tools/05-Intro-to-Orange/orange.html#installing-orange",
    "title": "üêâ Introduction to Orange",
    "section": "Installing Orange",
    "text": "Installing Orange\nYou can download and install Orange from here:\nhttps://orangedatamining.com/download/"
  },
  {
    "objectID": "content/courses/Analytics/Tools/05-Intro-to-Orange/orange.html#basic-usage-of-orange",
    "href": "content/courses/Analytics/Tools/05-Intro-to-Orange/orange.html#basic-usage-of-orange",
    "title": "üêâ Introduction to Orange",
    "section": "Basic Usage of Orange",
    "text": "Basic Usage of Orange"
  },
  {
    "objectID": "content/courses/Analytics/Tools/05-Intro-to-Orange/orange.html#orange-workflows",
    "href": "content/courses/Analytics/Tools/05-Intro-to-Orange/orange.html#orange-workflows",
    "title": "üêâ Introduction to Orange",
    "section": "Orange Workflows",
    "text": "Orange Workflows"
  },
  {
    "objectID": "content/courses/Analytics/Tools/05-Intro-to-Orange/orange.html#widgets-and-channels",
    "href": "content/courses/Analytics/Tools/05-Intro-to-Orange/orange.html#widgets-and-channels",
    "title": "üêâ Introduction to Orange",
    "section": "Widgets and Channels",
    "text": "Widgets and Channels"
  },
  {
    "objectID": "content/courses/Analytics/Tools/05-Intro-to-Orange/orange.html#loading-data-into-orange",
    "href": "content/courses/Analytics/Tools/05-Intro-to-Orange/orange.html#loading-data-into-orange",
    "title": "üêâ Introduction to Orange",
    "section": "Loading data into Orange",
    "text": "Loading data into Orange\n\n\n\nWe are good to get started with Orange!!"
  },
  {
    "objectID": "content/courses/Analytics/Tools/05-Intro-to-Orange/orange.html#simple-visuals-using-orange",
    "href": "content/courses/Analytics/Tools/05-Intro-to-Orange/orange.html#simple-visuals-using-orange",
    "title": "üêâ Introduction to Orange",
    "section": "Simple Visuals using Orange",
    "text": "Simple Visuals using Orange\nLet us create some simple visualizations using Orange.\n\nUse the File Widget to import the iris dataset into your session\nUse the Data Table Widget to look at the data, and note its variable names\nUse the Visualization Widgets ( Scatter Plot, Bar Plot, and Distributions) to look at the properties of the variables, and examine relationships between them."
  },
  {
    "objectID": "content/courses/Analytics/Tools/05-Intro-to-Orange/orange.html#reference",
    "href": "content/courses/Analytics/Tools/05-Intro-to-Orange/orange.html#reference",
    "title": "üêâ Introduction to Orange",
    "section": "Reference",
    "text": "Reference\n\nIntroduction to Data Mining-Working notes for the hands-on course with Orange Data Mining.(Download file)\nOrange Data Mining Widget Catalog: Look here for help and guidance! https://orangedatamining.com/widget-catalog/"
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-Radiant/radiant.html",
    "href": "content/courses/Analytics/Tools/10-Intro-to-Radiant/radiant.html",
    "title": "üêâ Introduction to Radiant",
    "section": "",
    "text": "Radiant is an open-source platform-independent browser-based interface for business analytics in R.\nKey features\n\nExplore: Quickly and easily summarize, visualize, and analyze your data\nCross-platform: It runs in a browser on Windows, Mac, and Linux\nReproducible: Recreate results and share work with others as a state file or an Rmarkdown report\nProgramming: Integrate Radiant‚Äôs analysis functions with your own R-code\nContext: Data and examples focus on business applications\n\nRadiant can be used for a variety of tasks\n\nProbability and Stats\nData visualization\nMachine Learning\nData mining\nReport Generation.\n\nRadiant Workflows can also be exported to R/RStudio easily."
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-Radiant/radiant.html#installing-radiant",
    "href": "content/courses/Analytics/Tools/10-Intro-to-Radiant/radiant.html#installing-radiant",
    "title": "üêâ Introduction to Radiant",
    "section": "Installing Radiant",
    "text": "Installing Radiant\nYou can download and install Radiant from here:\nhttps://radiant-rstats.github.io/docs/install.html\n\n\n\n\n\n\nImportant\n\n\n\nNOTE: This automatically installs R, RStudio, and Radiant on your machine. This is going to be convenient when we start working in R too!\nIt also installs Latex, which allows us to create crisp PDF reports of our analyses.\nThe version of R may not be the latest one, though‚Ä¶"
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-Radiant/radiant.html#basic-tutorials-with-radiant",
    "href": "content/courses/Analytics/Tools/10-Intro-to-Radiant/radiant.html#basic-tutorials-with-radiant",
    "title": "üêâ Introduction to Radiant",
    "section": "Basic Tutorials with Radiant",
    "text": "Basic Tutorials with Radiant\nAll the Tutorials are available on Youtube; the links to individual videos are on the page below\nhttps://radiant-rstats.github.io/docs/radiant-tutorial-series.html"
  },
  {
    "objectID": "content/courses/Analytics/Tools/15-Intro-to-R/r-intro.html#introduction-to-r",
    "href": "content/courses/Analytics/Tools/15-Intro-to-R/r-intro.html#introduction-to-r",
    "title": "üêâ Introduction to R and RStudio",
    "section": "Introduction to R",
    "text": "Introduction to R\nWe have already installed R and RStudio when we installed Radiant! We will now get acquainted with data and its representations in R! We will also form a view of how English Grammar ( pronouns, verbs, adjectives,figures of speech..) get ‚Äúmetaphorized‚Äù into the R World!!"
  },
  {
    "objectID": "content/courses/Analytics/Tools/15-Intro-to-R/r-intro.html#readings",
    "href": "content/courses/Analytics/Tools/15-Intro-to-R/r-intro.html#readings",
    "title": "üêâ Introduction to R and RStudio",
    "section": "Readings",
    "text": "Readings\n\nR for Data Science, Workflow: Basics Chapter: http://r4ds.had.co.nz/workflow-basics.html\nModern Dive, Getting Started Chapter: http://moderndive.com/2-getting-started.html\nR & RStudio Basics: https://bookdown.org/chesterismay/rbasics/3-rstudiobasics.html\nRStudio IDE Cheatsheet: https://github.com/rstudio/cheatsheets/blob/master/rstudio-ide.pdf"
  },
  {
    "objectID": "content/courses/Analytics/Tools/listing.html",
    "href": "content/courses/Analytics/Tools/listing.html",
    "title": "Tools and Software",
    "section": "",
    "text": "üêâ Introduction to Orange\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\n\n\n\n \n\n\n\n\nüêâ Introduction to Radiant\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\n\n\n\n \n\n\n\n\nüêâ Introduction to R and RStudio\n\n\n\n\n\n\n\n\n\n0 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/courses/Analytics/Workflow/listing.html",
    "href": "content/courses/Analytics/Workflow/listing.html",
    "title": "Workflow",
    "section": "",
    "text": "Using FlexDashboard in R\n\n\nMaking Business Presentations using Flexdashboards in R\n\n\n\n\nflexdashboard\n\n\nDashboard Layouts\n\n\n\n\nMaking Business Presentations using Flexdashboards in R\n\n\n\n\n\n\nMar 10, 2023\n\n\nArvind Venkatadri\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/10-Business-Report-using-Flextable-in-R/index.html",
    "href": "content/courses/Analytics/Workflow/Modules/10-Business-Report-using-Flextable-in-R/index.html",
    "title": "Using FlexDashboard in R",
    "section": "",
    "text": "R Tutorial"
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/10-Business-Report-using-Flextable-in-R/index.html#references",
    "href": "content/courses/Analytics/Workflow/Modules/10-Business-Report-using-Flextable-in-R/index.html#references",
    "title": "Using FlexDashboard in R",
    "section": "References",
    "text": "References\n\nFlexdashboard Basics https://rstudio.github.io/flexdashboard/articles/flexdashboard.html\nFlexdashboard Examples https://rstudio.github.io/flexdashboard/articles/examples.html\nShannon Haymond,Create laboratory business intelligence dashboards for free using R: A tutorial using the flexdashboard package, Journal of Mass Spectrometry and Advances in the Clinical Lab, Volume 23, 2022,Pages 39-43, ISSN 2667-145X, https://doi.org/10.1016/j.jmsacl.2021.12.002.\nhttps://posit.co/blog/flexdashboard-easy-interactive-dashboards-for-r/"
  },
  {
    "objectID": "content/courses/ISTW/listing.html",
    "href": "content/courses/ISTW/listing.html",
    "title": "In Short, the World",
    "section": "",
    "text": "Title\n\n\nReading Time\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/courses/MathModelsDesign/listing.html",
    "href": "content/courses/MathModelsDesign/listing.html",
    "title": "Math Models in Design",
    "section": "",
    "text": "Title\n\n\nReading Time\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/courses/ML4Artists/1-IntroOrange/index.html",
    "href": "content/courses/ML4Artists/1-IntroOrange/index.html",
    "title": "üêâ Intro to Orange",
    "section": "",
    "text": "Orange is a visual drag-and-drop tool for\n\nData visualization\n\nMachine Learning\n\nData mining\n\nand much more. Here is what it looks like:\n\nAll operations are done using a visual menu-driven interface. The visuals can be exported to PNG/SVG/PDF and the entire workflow can be exported into a Report Form and edited for presentation and sharing."
  },
  {
    "objectID": "content/courses/ML4Artists/1-IntroOrange/index.html#installing-orange",
    "href": "content/courses/ML4Artists/1-IntroOrange/index.html#installing-orange",
    "title": "üêâ Intro to Orange",
    "section": "Installing Orange",
    "text": "Installing Orange\nYou can download and install Orange from here:\nhttps://orangedatamining.com/download/"
  },
  {
    "objectID": "content/courses/ML4Artists/1-IntroOrange/index.html#basic-usage-of-orange",
    "href": "content/courses/ML4Artists/1-IntroOrange/index.html#basic-usage-of-orange",
    "title": "üêâ Intro to Orange",
    "section": "Basic Usage of Orange",
    "text": "Basic Usage of Orange\n{{% youtube \"HXjnDIgGDuI\" %}}"
  },
  {
    "objectID": "content/courses/ML4Artists/1-IntroOrange/index.html#orange-workflows",
    "href": "content/courses/ML4Artists/1-IntroOrange/index.html#orange-workflows",
    "title": "üêâ Intro to Orange",
    "section": "Orange Workflows",
    "text": "Orange Workflows\n{{% youtube \"lb-x36xqJ-E\" %}}"
  },
  {
    "objectID": "content/courses/ML4Artists/1-IntroOrange/index.html#widgets-and-channels",
    "href": "content/courses/ML4Artists/1-IntroOrange/index.html#widgets-and-channels",
    "title": "üêâ Intro to Orange",
    "section": "Widgets and Channels",
    "text": "Widgets and Channels\n{{% youtube \"2xS6QjnG714\" %}}"
  },
  {
    "objectID": "content/courses/ML4Artists/1-IntroOrange/index.html#loading-data-into-orange",
    "href": "content/courses/ML4Artists/1-IntroOrange/index.html#loading-data-into-orange",
    "title": "üêâ Intro to Orange",
    "section": "Loading data into Orange",
    "text": "Loading data into Orange\n{{% youtube \"MHcGdQeYCMg\" %}} \nWe are good to get started with Orange!!"
  },
  {
    "objectID": "content/courses/ML4Artists/1-IntroOrange/index.html#simple-visuals-using-orange",
    "href": "content/courses/ML4Artists/1-IntroOrange/index.html#simple-visuals-using-orange",
    "title": "üêâ Intro to Orange",
    "section": "Simple Visuals using Orange",
    "text": "Simple Visuals using Orange\nLet us create some simple visualizations using Orange.\n\nUse the File Widget to import the iris dataset into your session\nUse the Data Table Widget to look at the data, and note its variable names\nUse the Visualization Widgets ( Scatter Plot, Bar Plot, and Distributions) to look at the properties of the variables, and examine relationships between them."
  },
  {
    "objectID": "content/courses/ML4Artists/1-IntroOrange/index.html#reference",
    "href": "content/courses/ML4Artists/1-IntroOrange/index.html#reference",
    "title": "üêâ Intro to Orange",
    "section": "Reference",
    "text": "Reference\n\nIntroduction to Data Mining-Working notes for the hands-on course with Orange Data Mining. (Download file)"
  },
  {
    "objectID": "content/courses/ML4Artists/2-Regression/index.html",
    "href": "content/courses/ML4Artists/2-Regression/index.html",
    "title": "Basics of Machine Learning - Regression",
    "section": "",
    "text": "Introduction: Mixing Colours\nInterpolation:\n\nbetween TWO colours, both colours inclusive using a straight line between them\n\nbetween several different colours?\n\nby mixing ‚Äúequal proportions‚Äù of each\n\nProportions based on ‚Äúdistance‚Äù from each colour\n\nOn a ‚Äúplane‚Äù with these points\n\n\nSome Examples from Drama\n\nLegally Blonde:\n\n{{% youtube \"aVRUfPRUKtU\" %}}\n\nGreek Chorus Explained:\n\n{{% youtube \"orXPMdCU-6s\" %}}\n\nSutradhar in Indian Drama\n\nhttps://www.britannica.com/art/theater-building/Developments-in-Asia#ref463835\nDiscussion\n\nInterpolation\nExtrapolation\nCalculating the optimum values for m and c, given x and y**, for \\(y = mx + c\\)\n\nPlaying with Orange: Paint My Data\n\n\n\n\n\nLet us ‚Äúdraw inspiration‚Äù from the picture above, and see if we can replicate it. We will fire up Orange, paint some data and see if we can fit a linear regression ML model to it.\nHere is the Orange file for you to download. Open this file in Orange.\nRegression Plane\n\n\n\n\n\nInteractive Regression Plane"
  },
  {
    "objectID": "content/courses/ML4Artists/3-Classification/index.html",
    "href": "content/courses/ML4Artists/3-Classification/index.html",
    "title": "Basics of Machine Learning - Classification",
    "section": "",
    "text": "Have you played a Childhood Game called 20 Questions? Someone has a ‚Äútarget‚Äù entity in mind ( a person or a thing or a literary character) and the others need to discover that entity by asking 20 questions.\n\nHow does one create questions in the game?\n\nCategories?\nNumbers? How?\nComparisons?\n\n\nWhat sort of answers can you expect for each question?"
  },
  {
    "objectID": "content/courses/ML4Artists/3-Classification/index.html#twenty-questions-game-as-a-play-with-data",
    "href": "content/courses/ML4Artists/3-Classification/index.html#twenty-questions-game-as-a-play-with-data",
    "title": "Basics of Machine Learning - Classification",
    "section": "Twenty Questions Game as a Play with Data‚Ä¶",
    "text": "Twenty Questions Game as a Play with Data‚Ä¶\nAssuming we think of a 20Q Target as say, celebrity singer like Taylor Swift, or a cartoon character like Thomas the Tank Engine, what would an underlying ‚Äúdata structure‚Äù look like? We would ask Questions for instance in the following order to find the target of Taylor Swift:\n- Human?(Yes)\n- Living?(Yes)\n- Male?(No)\n- Celebrity?(Yes)\n- Music?(Yes)\n- USA?(Yes)\nOh‚Ä¶Taylor Swift!!!\nLet us try to construct the ‚Äúdatasets‚Äù underlying this game!\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nOccupation\nSex\nLiving\nNationality\ngenre\npet\n\n\nTaylor Swift\nSinger\nF\nTRUE\nUSA\ncountry/rock\nScottish Fold Cats\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nType\nLiving\nhuman\nNationality\ncolour\nmaterial\n\n\nThomas, the Tank Engine\nCartoon Character\nFALSE\nFALSE\nUK\nblue\nmetal\n\n\n\n\nIt should be fairly clear that the Questions we ask are based on the COLUMNs in the respective 1-row datasets! The TARGET Column in both cases is the name column."
  },
  {
    "objectID": "content/courses/ML4Artists/3-Classification/index.html#what-is-a-decision-tree",
    "href": "content/courses/ML4Artists/3-Classification/index.html#what-is-a-decision-tree",
    "title": "Basics of Machine Learning - Classification",
    "section": "What is a Decision Tree?",
    "text": "What is a Decision Tree?\nCan you imagine how the 20 Questions Game can be shown as a tree?\n\n\n\n\n\n\nEach Question we ask, based on one of the Feature columns, begets a Yes/NO answer and we turn the left or right accordingly. When we arrive at the leaf, we should be in a position to guess the answer !"
  },
  {
    "objectID": "content/courses/ML4Artists/3-Classification/index.html#twenty-times-20-questions",
    "href": "content/courses/ML4Artists/3-Classification/index.html#twenty-times-20-questions",
    "title": "Basics of Machine Learning - Classification",
    "section": "Twenty times 20 Questions !!",
    "text": "Twenty times 20 Questions !!\nWhat if the dataset we had contained many rows, instead of just one row? How would we play the 20Q Game in this situation? Here is a sample of the famous penguins dataset:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\nAdelie\nDream\n41.1\n18.1\n205\n4300\nmale\n2008\n\n\nAdelie\nDream\n40.8\n18.4\n195\n3900\nmale\n2007\n\n\nAdelie\nTorgersen\n42.1\n19.1\n195\n4000\nmale\n2008\n\n\nAdelie\nDream\n41.1\n19.0\n182\n3425\nmale\n2007\n\n\nAdelie\nBiscoe\n38.2\n18.1\n185\n3950\nmale\n2007\n\n\nAdelie\nDream\n39.5\n16.7\n178\n3250\nfemale\n2007\n\n\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n2007\n\n\nGentoo\nBiscoe\n46.2\n14.4\n214\n4650\nNA\n2008\n\n\nAdelie\nDream\n39.7\n17.9\n193\n4250\nmale\n2009\n\n\nChinstrap\nDream\n42.4\n17.3\n181\n3600\nfemale\n2007\n\n\nGentoo\nBiscoe\n44.5\n14.3\n216\n4100\nNA\n2007\n\n\nAdelie\nTorgersen\n45.8\n18.9\n197\n4150\nmale\n2008\n\n\n\n\n\nAs before, we would need to look at the dataset as containing a TARGET column which we want to predict using several other FEATURE columns. Let us choose species.\nWhen we look at the FEATURE columns, We would need to formulate questions based on entire columns at a time. For instance:\n\n\n‚ÄúIs the bill_length_mm* greater than 45mm?‚Äù considers the entire bill_length_mm* FEATURE column\nIs the sex female? considers the entire sex column\n\nIf the specific FEATURE column is a Numerical (N) variable, the question would use some ‚Äúthresholding‚Äù as shown in the question above, to convert the Numerical Variable into a Categorical variable.\nIf a specific FEATURE column is a Categorical (C) variable, the question would be like a filter operation in Excel.\nEither way, we end up answering with a smaller and smaller subset of rows in the dataset, to which the questions are answered with a Yes. It is as if we played many 20 Questions games in parallel, since there are so many simultaneous ‚Äúanswers‚Äù!\nOnce we exhaust all the FEATURE columns, then what remains is a subset (i.e.¬†rows) of the original dataset and we read off the TARGET column, which should now contain a set of identical entries, e.g. ‚ÄúAdelie‚Äù. Thus we can extend a single-target 20Q game to a multiple-target one using a larger dataset. ( Note how the multiple targets are all the same: ‚ÄúAdelie‚Äù, or ‚ÄúGentoo‚Äù, or ‚ÄúChinstrap‚Äù)\nThis forms the basic intuition for a Machine Learning Algorithm called a Decision Tree.\nDecision Tree in Orange\nLet us visualize this Decision Tree in Orange. Look at the now famous penguins dataset, available here:\nhttps://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\nWe see that there are three species of penguins, that live on three islands. The measurements for each penguin are flipper_length_mm, bill_length_mm, bill_depth_mm, and body_mass_g.\n\nTask 1: Create a few data visualizations for the variables, and pairs of variables from this dataset.\nTask 2: Can you inspect the visualizations and imagine how each of this dataset can be used in a 20 Questions Game, to create a Decision Tree for this dataset as shown below?\n\n\n\nPenguins Decision Tree!\n\n\nWhat did we learn?\n\nThe 20Q Game can be viewed as a ‚ÄúDecision Tree‚Äù of Questions and Answers,\nEach fork in the game is a Question.\nDepending upon whether the current answer is yes or no, we turn in one direction or the other.\nEach of our questions is based on the information available in one or other of the columns!!\nWe arrive at a final ‚Äúanswer‚Äù or ‚Äútarget‚Äù after a particular sequence of yes/no answers. This is the one of the leaf nodes in the Tree.\nThe island and the species columns are categories and are especially suited to being the targets for a 20 Questions Game.\nWe can therefore use an entire column of data as our 20Questions target, rather than just one entity, person.\n\nThis is how we will use this Game as a Model for our first ML algorithm, classification using Decision Trees."
  },
  {
    "objectID": "content/courses/ML4Artists/3-Classification/index.html#how-do-we-make-predictions-using-our-decision-tree",
    "href": "content/courses/ML4Artists/3-Classification/index.html#how-do-we-make-predictions-using-our-decision-tree",
    "title": "Basics of Machine Learning - Classification",
    "section": "How do we Make Predictions using our Decision Tree",
    "text": "How do we Make Predictions using our Decision Tree\nOur aim is to make predictions. Predictions of what? When we are given new unseen data in the same format, we should be able to predict TARGET variable using the same FEATURE columns.\nNOTE: This that is usually a class/category (We CAN also predict a numerical value with a Decision Tree; but we will deal with that later.)\nIn order to make predictions with completely unseen data, we need to first check if the algorithm is working well with known data. The way to do this is to use a large portion of data to design the tree, and then use the tree to predict some aspect of the remaining, but similar, data. Let us split the penguins dataset into two pieces: a training set to design our tree, and a test set to check how it is working.\nDownload this penguin tree file and open it in Orange.\nHow good are the Predictions? What is the Classification Error Rate?"
  },
  {
    "objectID": "content/courses/ML4Artists/3-Classification/index.html#how-many-trees-do-we-need-enter-the-random-forest",
    "href": "content/courses/ML4Artists/3-Classification/index.html#how-many-trees-do-we-need-enter-the-random-forest",
    "title": "Basics of Machine Learning - Classification",
    "section": "How Many Trees do we Need? Enter the Random Forest!",
    "text": "How Many Trees do we Need? Enter the Random Forest!\nCheck all your individual Decision Trees: do they ask the same Questions? Do they fork in the same way? Yes, they all seem to use the same set of parameters to reach the target. So they are capable of being ‚Äúbiased‚Äù and make the same mistakes. So we ask: Does it help to use more than one tree, if all the questions/forks in the Trees are similar?\nNo‚Ä¶we need different Trees to be able to ask different questions, based on different variables or features in the data. That will make the Trees as different as possible and so‚Ä¶unbiased. This is what we also saw when we played 20Q: offbeat questions opened up some avenues for predicting the answer/target.\nA forest of such trees is called the Wild Wood a Random Forest !"
  },
  {
    "objectID": "content/courses/ML4Artists/3-Classification/index.html#an-introduction-to-random-forests",
    "href": "content/courses/ML4Artists/3-Classification/index.html#an-introduction-to-random-forests",
    "title": "Basics of Machine Learning - Classification",
    "section": "An Introduction to Random Forests",
    "text": "An Introduction to Random Forests\nIn the Random Forest method, we do as follows:\n\nSplit the dataset into training and test subsets (70::30 proportion is very common). Keep aside the testing dataset for final testing.\nDecide on a number of trees, say 100-500 in the forest.\nTake the training dataset and repeatedly sample some of the rows in it. Rows can be repeated too; this is called bootstrap sampling.\nGive this sampled training set to each tree. Each tree develops a question from this dataset, in a random fashion, using a randomly chosen variable. E.g. with penguins, if our target is species, then some trees will will use island, some others will use body_mass_g and some others may use bill_length_mm.\nEach tree will ‚Äúgrow its questions‚Äù in a unique way !! Since the questions are possibly based on a different variable at each time, the trees will grow in very different ways.\nStop when the required accuracy has been achieved (the sets contain observations/rows from only one species predominantly)\nWith the test set let each tree vote on which species it has decided upon. Take the majority vote.\n\nPhew!!\nLet‚Äôs get a visual sense of how all this works:\nhttps://waternova.github.io/random-forest-viz/"
  },
  {
    "objectID": "content/courses/ML4Artists/3-Classification/index.html#random-forest-classification-for-heart-patients",
    "href": "content/courses/ML4Artists/3-Classification/index.html#random-forest-classification-for-heart-patients",
    "title": "Basics of Machine Learning - Classification",
    "section": "Random Forest Classification for Heart Patients",
    "text": "Random Forest Classification for Heart Patients\nDo you want to develop an ML model for heart patients? We have a dataset of heart patients at the University of California, Arvind Irvine ML Dataset Repository\nHeart Patient Data. Import into Orange !!\nWhat are the variables?\n\n(age): age in years\n\n(sex): 1 = male; 0 = female\n\n(cp): chest-pain type( 4 types, 1/2/3/4)\n\n(trestbps): resting blood pressure (in mm Hg on admission to the hospital)\n(chol) : serum cholesterol in mg/dl\n(fbs): (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n(restecg): resting electrocardiograph results (0 = normal; 1= ST-T wave abnormality; 3 = LV hypertrophy)\n(thalach): maximum heart rate achieved\n(exang): exercise induced angina (1 = yes; 0 = no) (remember Puneet Rajkumar)\n\n(oldpeak): ST depression induced by exercise relative to rest\n(slope): the slope of the peak exercise ST segment\nValue 1: upsloping\n\nValue 2: flat\n\nValue 3: downsloping\n\n\n\n(ca): number of major vessels (0-3) colored by fluoroscopy\n\n(thal): 3 = normal; 6 = fixed defect; 7 = reversible defect\n\n(num) : the target attribute, diagnosis of heart disease (angiographic disease status)\nValue 0: < 50% diameter narrowing\n\nValue 1: > 50% diameter narrowing\n(in any major vessel: attributes 59 through 68 are vessels)\n\n\n\nWe will create a Random Forest Model for this dataset, and compare with the Desision Tree for the same dataset."
  },
  {
    "objectID": "content/courses/ML4Artists/3-Classification/index.html#how-good-is-my-random-forest",
    "href": "content/courses/ML4Artists/3-Classification/index.html#how-good-is-my-random-forest",
    "title": "Basics of Machine Learning - Classification",
    "section": "How good is my Random Forest?",
    "text": "How good is my Random Forest?\n\nClassification Error\n\nGini Impurity\n\nCross Entropy"
  },
  {
    "objectID": "content/courses/ML4Artists/3-Classification/index.html#references",
    "href": "content/courses/ML4Artists/3-Classification/index.html#references",
    "title": "Basics of Machine Learning - Classification",
    "section": "References",
    "text": "References\n\nhttps://towardsdatascience.com/data-science-made-easy-data-modeling-and-prediction-using-orange-f451f17061fa\nThe beauty of Random Forests: https://orangedatamining.com/blog/2016/12/22/the-beauty-of-random-forest/\nPythagorean Trees for Random Forests: https://orangedatamining.com/blog/2016/07/29/pythagorean-trees-and-forests/\ndata.tree sample applications, Christoph Glur, 2020-07-31. https://cran.r-project.org/web/packages/data.tree/vignettes/applications.html"
  },
  {
    "objectID": "content/courses/ML4Artists/3-Classification/Random-Forests.html",
    "href": "content/courses/ML4Artists/3-Classification/Random-Forests.html",
    "title": "Random Forests",
    "section": "",
    "text": "Machine Learning Basics - Random Forest at Shirin‚Äôs Playground"
  },
  {
    "objectID": "content/courses/ML4Artists/3-Classification/Random-Forests.html#penguin-random-forest-model-withrandomforest",
    "href": "content/courses/ML4Artists/3-Classification/Random-Forests.html#penguin-random-forest-model-withrandomforest",
    "title": "Random Forests",
    "section": "Penguin Random Forest Model withrandomForest\n",
    "text": "Penguin Random Forest Model withrandomForest\n\nUsing the penguins dataset and Random Forest Classification.\n\npenguins\n\n# A tibble: 344 √ó 8\n   species island    bill_length_mm bill_depth_mm flipper_‚Ä¶¬π body_‚Ä¶¬≤ sex    year\n   <fct>   <fct>              <dbl>         <dbl>      <int>   <int> <fct> <int>\n 1 Adelie  Torgersen           39.1          18.7        181    3750 male   2007\n 2 Adelie  Torgersen           39.5          17.4        186    3800 fema‚Ä¶  2007\n 3 Adelie  Torgersen           40.3          18          195    3250 fema‚Ä¶  2007\n 4 Adelie  Torgersen           NA            NA           NA      NA <NA>   2007\n 5 Adelie  Torgersen           36.7          19.3        193    3450 fema‚Ä¶  2007\n 6 Adelie  Torgersen           39.3          20.6        190    3650 male   2007\n 7 Adelie  Torgersen           38.9          17.8        181    3625 fema‚Ä¶  2007\n 8 Adelie  Torgersen           39.2          19.6        195    4675 male   2007\n 9 Adelie  Torgersen           34.1          18.1        193    3475 <NA>   2007\n10 Adelie  Torgersen           42            20.2        190    4250 <NA>   2007\n# ‚Ä¶ with 334 more rows, and abbreviated variable names ¬π‚Äãflipper_length_mm,\n#   ¬≤‚Äãbody_mass_g\n\nsummary(penguins)\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2                                 \n\npenguins %>% skimr::skim()\n\n\n\n\n\nName\nPiped data\n\n\nNumber of rows\n344\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nData summaryVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nspecies\n0\n1.00\nFALSE\n3\nAde: 152, Gen: 124, Chi: 68\n\n\nisland\n0\n1.00\nFALSE\n3\nBis: 168, Dre: 124, Tor: 52\n\n\nsex\n11\n0.97\nFALSE\n2\nmal: 168, fem: 165\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nbill_length_mm\n2\n0.99\n43.92\n5.46\n32.1\n39.23\n44.45\n48.5\n59.6\n‚ñÉ‚ñá‚ñá‚ñÜ‚ñÅ\n\n\nbill_depth_mm\n2\n0.99\n17.15\n1.97\n13.1\n15.60\n17.30\n18.7\n21.5\n‚ñÖ‚ñÖ‚ñá‚ñá‚ñÇ\n\n\nflipper_length_mm\n2\n0.99\n200.92\n14.06\n172.0\n190.00\n197.00\n213.0\n231.0\n‚ñÇ‚ñá‚ñÉ‚ñÖ‚ñÇ\n\n\nbody_mass_g\n2\n0.99\n4201.75\n801.95\n2700.0\n3550.00\n4050.00\n4750.0\n6300.0\n‚ñÉ‚ñá‚ñÜ‚ñÉ‚ñÇ\n\n\nyear\n0\n1.00\n2008.03\n0.82\n2007.0\n2007.00\n2008.00\n2009.0\n2009.0\n‚ñá‚ñÅ‚ñá‚ñÅ‚ñá\n\n\n\n\npenguins <- penguins %>% tidyr::drop_na()\n# Spent one hour trying to find `drop-na()` ( 14 June 2020)\n\n\n# library(corrplot)\ncor <- penguins %>% select(is.numeric) %>% cor() \n\nWarning: Use of bare predicate functions was deprecated in tidyselect 1.1.0.\n‚Ñπ Please use wrap predicates in `where()` instead.\n  # Was:\n  data %>% select(is.numeric)\n\n  # Now:\n  data %>% select(where(is.numeric))\n\ncor %>% corrplot(., method = \"ellipse\", order = \"hclust\",tl.cex = 0.5)\n\n\n\n# try these too:\n# cor %>% corrplot(., method = \"square\", order = \"hclust\",tl.cex = 0.5)\n# cor %>% corrplot(., method = \"color\", order = \"hclust\",tl.cex = 0.5)\n# cor %>% corrplot(., method = \"shade\", order = \"hclust\",tl.cex = 0.5)\n\nNotes: - flipper_length_mm and culmen_depth_mm are negtively correlated at approx (-0.7) - flipper_length_mm and body_mass_g are positively correlated at approx 0.8\nSo we will use steps in the recipe to remove correlated variables.\nPenguin Data Sampling and Recipe\n\n# Data Split\npenguin_split <- initial_split(penguins, prop = 0.6)\npenguin_train <- training(penguin_split)\npenguin_test <- testing(penguin_split)\npenguin_split\n\n<Training/Testing/Total>\n<199/134/333>\n\nhead(penguin_train)\n\n# A tibble: 6 √ó 8\n  species   island bill_length_mm bill_depth_mm flipper_le‚Ä¶¬π body_‚Ä¶¬≤ sex    year\n  <fct>     <fct>           <dbl>         <dbl>        <int>   <int> <fct> <int>\n1 Adelie    Dream            41.1          19            182    3425 male   2007\n2 Adelie    Biscoe           41.6          18            192    3950 male   2008\n3 Gentoo    Biscoe           40.9          13.7          214    4650 fema‚Ä¶  2007\n4 Adelie    Dream            42.2          18.5          180    3550 fema‚Ä¶  2007\n5 Chinstrap Dream            43.2          16.6          187    2900 fema‚Ä¶  2007\n6 Adelie    Dream            34            17.1          185    3400 fema‚Ä¶  2008\n# ‚Ä¶ with abbreviated variable names ¬π‚Äãflipper_length_mm, ¬≤‚Äãbody_mass_g\n\n# Recipe\npenguin_recipe <- penguins %>% \n  recipe(species ~ .) %>% \n  step_normalize(all_numeric()) %>% # Scaling and Centering\n  step_corr(all_numeric()) %>%  # Handling correlated variables\n  prep()\n\n# Baking the data\npenguin_train_baked <-  penguin_train %>% \n  bake(object = penguin_recipe, new_data = .)\n\npenguin_test_baked <-  penguin_test %>% \n  bake(object = penguin_recipe, new_data = .)\n\nhead(penguin_train_baked)\n\n# A tibble: 6 √ó 8\n  island bill_length_mm bill_depth_mm flipper_le‚Ä¶¬π body_‚Ä¶¬≤ sex      year species\n  <fct>           <dbl>         <dbl>        <dbl>   <dbl> <fct>   <dbl> <fct>  \n1 Dream          -0.529        0.932        -1.35   -0.971 male  -1.28   Adelie \n2 Biscoe         -0.438        0.424        -0.640  -0.319 male  -0.0517 Adelie \n3 Biscoe         -0.566       -1.76          0.930   0.550 fema‚Ä¶ -1.28   Gentoo \n4 Dream          -0.328        0.678        -1.50   -0.816 fema‚Ä¶ -1.28   Adelie \n5 Dream          -0.145       -0.287        -0.997  -1.62  fema‚Ä¶ -1.28   Chinst‚Ä¶\n6 Dream          -1.83        -0.0329       -1.14   -1.00  fema‚Ä¶ -0.0517 Adelie \n# ‚Ä¶ with abbreviated variable names ¬π‚Äãflipper_length_mm, ¬≤‚Äãbody_mass_g\n\n\nPenguin Random Forest Model\n\npenguin_model <- \n  rand_forest(trees = 100) %>% \n  set_engine(\"randomForest\") %>% \n  set_mode(\"classification\")\npenguin_model\n\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  trees = 100\n\nComputational engine: randomForest \n\npenguin_fit <- \n  penguin_model %>% \n  fit(species ~ .,penguin_train_baked)\npenguin_fit\n\nparsnip model object\n\n\nCall:\n randomForest(x = maybe_data_frame(x), y = y, ntree = ~100) \n               Type of random forest: classification\n                     Number of trees: 100\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 0.5%\nConfusion matrix:\n          Adelie Chinstrap Gentoo class.error\nAdelie        85         0      0  0.00000000\nChinstrap      1        40      0  0.02439024\nGentoo         0         0     73  0.00000000\n\n# iris_ranger <- \n#   rand_forest(trees = 100) %>% \n#   set_mode(\"classification\") %>% \n#   set_engine(\"ranger\") %>% \n#   fit(Species ~ ., data = iris_training_baked)\n\nMetrics for the Penguin Random Forest Model\n\n# Predictions\npredict(object = penguin_fit, new_data = penguin_test_baked) %>%  \n  dplyr::bind_cols(penguin_test_baked) %>% \n  glimpse()\n\nRows: 134\nColumns: 9\n$ .pred_class       <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel‚Ä¶\n$ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen, Biscoe, ‚Ä¶\n$ bill_length_mm    <dbl> -0.6752636, -0.9312674, -0.5289757, -1.7541369, -1.4‚Ä¶\n$ bill_depth_mm     <dbl> 0.42409105, 0.32252879, 0.22096653, 0.62721557, 1.03‚Ä¶\n$ flipper_length_mm <dbl> -0.42573251, -1.42460769, -1.35325946, -1.21056301, ‚Ä¶\n$ body_mass_g       <dbl> -1.18857213, -0.72285846, -1.25066728, -1.09542939, ‚Ä¶\n$ sex               <fct> female, female, female, female, female, male, male, ‚Ä¶\n$ year              <dbl> -1.2818130, -1.2818130, -1.2818130, -1.2818130, -1.2‚Ä¶\n$ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel‚Ä¶\n\n# Prediction Accuracy Metrics\npredict(object = penguin_fit, new_data = penguin_test_baked) %>%  \n  dplyr::bind_cols(penguin_test_baked) %>% \n  yardstick::metrics(truth = species, estimate = .pred_class)\n\n# A tibble: 2 √ó 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy multiclass     0.985\n2 kap      multiclass     0.976\n\n# Prediction Probabilities\npenguin_fit_probs <- \n  predict(penguin_fit, penguin_test_baked, type = \"prob\") %>%\n  dplyr::bind_cols(penguin_test_baked)\nglimpse(penguin_fit_probs)\n\nRows: 134\nColumns: 11\n$ .pred_Adelie      <dbl> 0.99, 0.98, 0.98, 1.00, 0.98, 0.99, 0.96, 0.99, 0.99‚Ä¶\n$ .pred_Chinstrap   <dbl> 0.01, 0.02, 0.01, 0.00, 0.00, 0.00, 0.01, 0.00, 0.00‚Ä¶\n$ .pred_Gentoo      <dbl> 0.00, 0.00, 0.01, 0.00, 0.02, 0.01, 0.03, 0.01, 0.01‚Ä¶\n$ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen, Biscoe, ‚Ä¶\n$ bill_length_mm    <dbl> -0.6752636, -0.9312674, -0.5289757, -1.7541369, -1.4‚Ä¶\n$ bill_depth_mm     <dbl> 0.42409105, 0.32252879, 0.22096653, 0.62721557, 1.03‚Ä¶\n$ flipper_length_mm <dbl> -0.42573251, -1.42460769, -1.35325946, -1.21056301, ‚Ä¶\n$ body_mass_g       <dbl> -1.18857213, -0.72285846, -1.25066728, -1.09542939, ‚Ä¶\n$ sex               <fct> female, female, female, female, female, male, male, ‚Ä¶\n$ year              <dbl> -1.2818130, -1.2818130, -1.2818130, -1.2818130, -1.2‚Ä¶\n$ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel‚Ä¶\n\n# Confusion Matrix\npenguin_fit$fit$confusion %>% tidy()\n\nWarning: 'tidy.numeric' is deprecated.\nSee help(\"Deprecated\")\n\n\n# A tibble: 3 √ó 1\n  x[,\"Adelie\"] [,\"Chinstrap\"] [,\"Gentoo\"] [,\"class.error\"]\n         <dbl>          <dbl>       <dbl>            <dbl>\n1           85              0           0           0     \n2            1             40           0           0.0244\n3            0              0          73           0     \n\n# Gain Curves\npenguin_fit_probs %>% \n  yardstick::gain_curve(species, .pred_Adelie:.pred_Gentoo) %>%\n  autoplot()\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\n‚Ñπ Please use `reframe()` instead.\n‚Ñπ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n‚Ñπ The deprecated feature was likely used in the yardstick package.\n  Please report the issue at <https://github.com/tidymodels/yardstick/issues>.\n\n\n\n\n# ROC Plot\npenguin_fit_probs%>%\n  roc_curve(species, .pred_Adelie:.pred_Gentoo) %>%\n  autoplot()\n\n\n\n\nUsing broom on the penguin model\n\npenguin_split\n\n<Training/Testing/Total>\n<199/134/333>\n\npenguin_split %>% broom::tidy()\n\n# A tibble: 333 √ó 2\n     Row Data    \n   <int> <chr>   \n 1     1 Analysis\n 2     2 Analysis\n 3     4 Analysis\n 4     5 Analysis\n 5     7 Analysis\n 6     9 Analysis\n 7    10 Analysis\n 8    11 Analysis\n 9    12 Analysis\n10    13 Analysis\n# ‚Ä¶ with 323 more rows\n\npenguin_recipe %>% broom::tidy()\n\n# A tibble: 2 √ó 6\n  number operation type      trained skip  id             \n   <int> <chr>     <chr>     <lgl>   <lgl> <chr>          \n1      1 step      normalize TRUE    FALSE normalize_zCUDv\n2      2 step      corr      TRUE    FALSE corr_WPWs7     \n\n# Following do not work for `random forest models` !! ;-()\n#penguin_model %>% tidy()\n#penguin_fit %>% tidy() \npenguin_model %>% str()\n\nList of 7\n $ args                 :List of 3\n  ..$ mtry : language ~NULL\n  .. ..- attr(*, \".Environment\")=<environment: R_EmptyEnv> \n  ..$ trees: language ~100\n  .. ..- attr(*, \".Environment\")=<environment: R_EmptyEnv> \n  ..$ min_n: language ~NULL\n  .. ..- attr(*, \".Environment\")=<environment: R_EmptyEnv> \n $ eng_args             : Named list()\n  ..- attr(*, \"class\")= chr [1:2] \"quosures\" \"list\"\n $ mode                 : chr \"classification\"\n $ user_specified_mode  : logi TRUE\n $ method               : NULL\n $ engine               : chr \"randomForest\"\n $ user_specified_engine: logi TRUE\n - attr(*, \"class\")= chr [1:2] \"rand_forest\" \"model_spec\"\n\npenguin_test_baked\n\n# A tibble: 134 √ó 8\n   island    bill_length_mm bill_depth_mm flipper_‚Ä¶¬π body_‚Ä¶¬≤ sex    year species\n   <fct>              <dbl>         <dbl>      <dbl>   <dbl> <fct> <dbl> <fct>  \n 1 Torgersen         -0.675        0.424      -0.426  -1.19  fema‚Ä¶ -1.28 Adelie \n 2 Torgersen         -0.931        0.323      -1.42   -0.723 fema‚Ä¶ -1.28 Adelie \n 3 Torgersen         -0.529        0.221      -1.35   -1.25  fema‚Ä¶ -1.28 Adelie \n 4 Torgersen         -1.75         0.627      -1.21   -1.10  fema‚Ä¶ -1.28 Adelie \n 5 Biscoe            -1.48         1.03       -0.854  -0.506 fema‚Ä¶ -1.28 Adelie \n 6 Biscoe            -1.06         0.475      -1.14   -0.319 male  -1.28 Adelie \n 7 Biscoe            -0.950        0.0178     -1.50   -0.506 male  -1.28 Adelie \n 8 Biscoe            -0.620        0.729      -1.28   -0.816 male  -1.28 Adelie \n 9 Biscoe            -0.639        0.881      -1.50   -0.319 male  -1.28 Adelie \n10 Dream             -0.822       -0.236      -1.64   -1.19  fema‚Ä¶ -1.28 Adelie \n# ‚Ä¶ with 124 more rows, and abbreviated variable names ¬π‚Äãflipper_length_mm,\n#   ¬≤‚Äãbody_mass_g"
  },
  {
    "objectID": "content/courses/ML4Artists/3-Classification/Random-Forests.html#iris-random-forest-model-with-ranger",
    "href": "content/courses/ML4Artists/3-Classification/Random-Forests.html#iris-random-forest-model-with-ranger",
    "title": "Random Forests",
    "section": "Iris Random Forest Model with ranger\n",
    "text": "Iris Random Forest Model with ranger\n\nUsing the iris dataset and Random Forest Classification. This part uses rsample to split the data and the recipes to prep the data for model making.\n\n#set.seed(100)\niris_split <- rsample::initial_split(iris, prop = 0.6)\niris_split\n\n<Training/Testing/Total>\n<90/60/150>\n\niris_split %>% training() %>% glimpse()\n\nRows: 90\nColumns: 5\n$ Sepal.Length <dbl> 5.2, 5.1, 6.1, 5.2, 6.8, 4.6, 5.4, 5.8, 7.7, 6.5, 5.1, 6.‚Ä¶\n$ Sepal.Width  <dbl> 4.1, 3.5, 2.6, 3.5, 2.8, 3.4, 3.9, 2.6, 3.0, 3.0, 3.3, 2.‚Ä¶\n$ Petal.Length <dbl> 1.5, 1.4, 5.6, 1.5, 4.8, 1.4, 1.7, 4.0, 6.1, 5.8, 1.7, 4.‚Ä¶\n$ Petal.Width  <dbl> 0.1, 0.2, 1.4, 0.2, 1.4, 0.3, 0.4, 1.2, 2.3, 2.2, 0.5, 1.‚Ä¶\n$ Species      <fct> setosa, setosa, virginica, setosa, versicolor, setosa, se‚Ä¶\n\niris_split %>% testing() %>% glimpse()\n\nRows: 60\nColumns: 5\n$ Sepal.Length <dbl> 4.7, 5.4, 4.8, 4.3, 5.8, 5.7, 5.1, 5.4, 5.1, 4.7, 5.5, 5.‚Ä¶\n$ Sepal.Width  <dbl> 3.2, 3.7, 3.0, 3.0, 4.0, 3.8, 3.8, 3.4, 3.7, 3.2, 4.2, 3.‚Ä¶\n$ Petal.Length <dbl> 1.3, 1.5, 1.4, 1.1, 1.2, 1.7, 1.5, 1.7, 1.5, 1.6, 1.4, 1.‚Ä¶\n$ Petal.Width  <dbl> 0.2, 0.2, 0.1, 0.1, 0.2, 0.3, 0.3, 0.2, 0.4, 0.2, 0.2, 0.‚Ä¶\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setosa, s‚Ä¶\n\n\nIris Data Pre-Processing: Creating the Recipe\nThe recipes package provides an interface that specializes in data pre-processing. Within the package, the functions that start, or execute, the data transformations are named after cooking actions. That makes the interface more user-friendly. For example:\n\nrecipe() - Starts a new set of transformations to be applied, similar to the ggplot() command. Its main argument is the model‚Äôs formula.\nprep() - Executes the transformations on top of the data that is supplied (typically, the training data). Each data transformation is a step() function. ( Recall what we did with the caret package: Centering, Scaling, Removing Correlated variables‚Ä¶)\n\nNote that in order to avoid data leakage (e.g: transferring information from the train set into the test set), data should be ‚Äúprepped‚Äù using the train_tbl only. https://towardsdatascience.com/modelling-with-tidymodels-and-parsnip-bae2c01c131c CRAN: The idea is that the preprocessing operations will all be created using the training set and then these steps will be applied to both the training and test set.\n\n# Pre Processing the Training Data\n\niris_recipe <- \n  training(iris_split) %>% # Note: Using TRAINING data !!\n  recipe(Species ~ .)      # Note: Outcomes ~ Predictors !!\n\n# The data contained in the `data` argument need not be the training set; this data is only used to catalog the names of the variables and their types (e.g. numeric, etc.).\n\nQ: How does the recipe ‚Äúfigure‚Äù out which are the outcomes and which are the predictors? A.The recipe command defines Outcomes and Predictors using the formula interface. Not clear how this recipe ‚Äúfigures‚Äù out which are the outcomes and which are the predictors, when we have not yet specified them‚Ä¶\nQ. Why is the recipe not agnostic to data set? Is that a meaningful question? A. The use of the training set in the recipe command is just to declare the variables and specify the roles of the data, nothing else. Roles are open-ended and extensible. From https://cran.r-project.org/web/packages/recipes/vignettes/Simple_Example.html :\n\nThis document demonstrates some basic uses of recipes. First, some definitions are required: - variables are the original (raw) data columns in a data frame or tibble. For example, in a traditional formula Y ~ A + B + A:B, the variables are A, B, and Y. - roles define how variables will be used in the model. Examples are: predictor (independent variables), response, and case weight. This is meant to be open-ended and extensible. - terms are columns in a design matrix such as A, B, and A:B. These can be other derived entities that are grouped, such as a set of principal components or a set of columns, that define a basis function for a variable. These are synonymous with features in machine learning. Variables that have predictor roles would automatically be main effect terms.\n\n\n# Apply the transformation steps\niris_recipe <- iris_recipe %>% \n  step_corr(all_predictors()) %>% \n  step_center(all_predictors(), -all_outcomes()) %>% \n  step_scale(all_predictors(), -all_outcomes()) %>% \n  prep()\n\nThis has created the recipe() and prepped it too. We now need to apply it to our datasets:\n\nTake training data and bake() it to prepare it for modelling.\nDo the same for the testing set.\n\n\niris_training_baked <- \n  iris_split %>% \n  training() %>% \n  bake(iris_recipe,.)\niris_training_baked\n\n# A tibble: 90 √ó 4\n   Sepal.Length Sepal.Width Petal.Width Species   \n          <dbl>       <dbl>       <dbl> <fct>     \n 1      -0.759        2.41      -1.48   setosa    \n 2      -0.878        1.02      -1.35   setosa    \n 3       0.307       -1.08       0.193  virginica \n 4      -0.759        1.02      -1.35   setosa    \n 5       1.14        -0.616      0.193  versicolor\n 6      -1.47         0.782     -1.22   setosa    \n 7      -0.522        1.95      -1.10   setosa    \n 8      -0.0487      -1.08      -0.0644 versicolor\n 9       2.20        -0.150      1.35   virginica \n10       0.780       -0.150      1.22   virginica \n# ‚Ä¶ with 80 more rows\n\niris_testing_baked <- \n  iris_split %>% \n  testing() %>% \n  bake(iris_recipe,.)\niris_testing_baked \n\n# A tibble: 60 √ó 4\n   Sepal.Length Sepal.Width Petal.Width Species\n          <dbl>       <dbl>       <dbl> <fct>  \n 1      -1.35         0.316       -1.35 setosa \n 2      -0.522        1.48        -1.35 setosa \n 3      -1.23        -0.150       -1.48 setosa \n 4      -1.83        -0.150       -1.48 setosa \n 5      -0.0487       2.18        -1.35 setosa \n 6      -0.167        1.71        -1.22 setosa \n 7      -0.878        1.71        -1.22 setosa \n 8      -0.522        0.782       -1.35 setosa \n 9      -0.878        1.48        -1.10 setosa \n10      -1.35         0.316       -1.35 setosa \n# ‚Ä¶ with 50 more rows\n\n\nIris Model Training using parsnip\n\nDifferent ML packages provide different interfaces (APIs ) to do the same thing (e.g random forests). The tidymodels package provides a consistent interface to invoke a wide variety of packages supporting a wide variety of models.\nThe parsnip package is a successor to caret.\nTo model with parsnip: 1. Pick a model : 2. Set the engine 3. Set the mode (if needed): Classification or Regression\nCheck here for models available in parsnip.\n\nMode: classification and regression in parsnip, each using a variety of models. ( Which Way). This defines the form of the output.\nEngine: The engine is the R package that is invoked by parsnip to execute the model. E.g glm, glmnet,keras.( How ) parsnip provides wrappers for models from these packages.\nModel: is the specific technique used for the modelling task. E.g linear_reg(), logistic_reg(), mars, decision_tree, nearest_neighbour‚Ä¶(What model).\n\nand models have: - hyperparameters: that are numerical or factor variables that tune the model ( Like the alpha beta parameters for Bayesian priors)\nWe can use the random forest model to classify the iris into species. Here Species is the Outcome variable and the rest are predictor variables. The random forest model is provided by the ranger package, to which tidymodels/parsnip provides a simple and consistent interface.\n\nlibrary(ranger)\niris_ranger <- \n  rand_forest(trees = 100) %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"ranger\") %>% \n  fit(Species ~ ., data = iris_training_baked)\n\nranger can generate random forest models for classification, regression, survival( time series, time to event stuff). Extreme Forests are also supported, wherein all points in the dataset are used ( instead of bootstrap samples) along with feature bagging. We can also run the same model using the randomForest package:\n\nlibrary(randomForest,quietly = TRUE)\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:ranger':\n\n    importance\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\niris_rf <- \n  rand_forest(trees = 100) %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"randomForest\") %>% \n  fit(Species ~ ., data = iris_training_baked)\n\nIris Predictions\nThe predict() function run against a parsnip model returns a prediction tibble. By default, the prediction variable is called .pred_class.\n\npredict(object = iris_ranger, new_data = iris_testing_baked) %>%  \n  dplyr::bind_cols(iris_testing_baked) %>% \n  glimpse()\n\nRows: 60\nColumns: 5\n$ .pred_class  <fct> setosa, setosa, setosa, setosa, setosa, setosa, setosa, s‚Ä¶\n$ Sepal.Length <dbl> -1.35138822, -0.52239642, -1.23296082, -1.82509781, -0.04‚Ä¶\n$ Sepal.Width  <dbl> 0.31591693, 1.48118431, -0.15019002, -0.15019002, 2.18034‚Ä¶\n$ Petal.Width  <dbl> -1.3532022, -1.3532022, -1.4820786, -1.4820786, -1.353202‚Ä¶\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setosa, s‚Ä¶\n\n\nIris Classification Model Validation\nWe use metrics() function from the yardstick package to evaluate how good the model is.\n\npredict(iris_ranger, iris_testing_baked) %>%\n  dplyr::bind_cols(iris_testing_baked) %>% \n  yardstick::metrics(truth = Species, estimate = .pred_class)\n\n# A tibble: 2 √ó 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy multiclass     0.967\n2 kap      multiclass     0.950\n\n\nWe can also check the metrics for randomForest model:\n\npredict(iris_rf, iris_testing_baked) %>%\n  dplyr::bind_cols(iris_testing_baked) %>% \n  yardstick::metrics(truth = Species, estimate = .pred_class)\n\n# A tibble: 2 √ó 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy multiclass     0.95 \n2 kap      multiclass     0.925\n\n\nIris Per-Classifier Metrics\nWe can use the parameter type = \"prob\" in the predict() function to obtain a probability score on each prediction. TBD: How is this prob calculated? Possible answer: the Random Forest model outputs its answer by majority voting across n trees. Each of the possible answers( i.e.¬†predictions) for a particular test datum gets a share of the vote, that represents its probability. Hence each dataum in the test vector can show a probability for the ‚Äúwinning‚Äù answer. ( Quite possibly we can get the probabilities for all possible outcomes for each test datum)\n\niris_ranger_probs <- \n  predict(iris_ranger, iris_testing_baked, type = \"prob\") %>%\n  dplyr::bind_cols(iris_testing_baked)\nglimpse(iris_ranger_probs)\n\nRows: 60\nColumns: 7\n$ .pred_setosa     <dbl> 0.99147619, 0.96175794, 0.94710119, 0.94710119, 0.730‚Ä¶\n$ .pred_versicolor <dbl> 0.005666667, 0.020416667, 0.031291667, 0.031291667, 0‚Ä¶\n$ .pred_virginica  <dbl> 0.002857143, 0.017825397, 0.021607143, 0.021607143, 0‚Ä¶\n$ Sepal.Length     <dbl> -1.35138822, -0.52239642, -1.23296082, -1.82509781, -‚Ä¶\n$ Sepal.Width      <dbl> 0.31591693, 1.48118431, -0.15019002, -0.15019002, 2.1‚Ä¶\n$ Petal.Width      <dbl> -1.3532022, -1.3532022, -1.4820786, -1.4820786, -1.35‚Ä¶\n$ Species          <fct> setosa, setosa, setosa, setosa, setosa, setosa, setos‚Ä¶\n\niris_rf_probs <- \n  predict(iris_rf, iris_testing_baked, type = \"prob\") %>%\n  dplyr::bind_cols(iris_testing_baked)\nglimpse(iris_rf_probs)\n\nRows: 60\nColumns: 7\n$ .pred_setosa     <dbl> 0.99, 0.97, 0.95, 0.95, 0.73, 0.81, 0.98, 0.98, 0.98,‚Ä¶\n$ .pred_versicolor <dbl> 0.01, 0.00, 0.02, 0.02, 0.22, 0.14, 0.00, 0.02, 0.00,‚Ä¶\n$ .pred_virginica  <dbl> 0.00, 0.03, 0.03, 0.03, 0.05, 0.05, 0.02, 0.00, 0.02,‚Ä¶\n$ Sepal.Length     <dbl> -1.35138822, -0.52239642, -1.23296082, -1.82509781, -‚Ä¶\n$ Sepal.Width      <dbl> 0.31591693, 1.48118431, -0.15019002, -0.15019002, 2.1‚Ä¶\n$ Petal.Width      <dbl> -1.3532022, -1.3532022, -1.4820786, -1.4820786, -1.35‚Ä¶\n$ Species          <fct> setosa, setosa, setosa, setosa, setosa, setosa, setos‚Ä¶\n\n# Tabulating the probabilities\nftable(iris_rf_probs$.pred_versicolor)\n\n 0 0.01 0.02 0.03 0.05 0.06 0.08 0.12 0.13 0.14 0.15 0.17 0.22 0.24 0.26 0.28 0.29 0.3 0.34 0.5 0.62 0.64 0.68 0.72 0.74 0.75 0.76 0.77 0.78 0.79 0.81 0.84 0.88 0.9 0.94 0.95 0.96 0.97 1\n                                                                                                                                                                                          \n 8    4    8    1    2    1    1    1    1    1    1    1    1    1    1    1    1   1    1   1    1    1    1    2    1    1    1    1    1    1    2    1    1   1    1    1    1    2 1\n\nftable(iris_rf_probs$.pred_virginica)\n\n 0 0.02 0.03 0.05 0.06 0.09 0.11 0.12 0.14 0.16 0.18 0.21 0.24 0.26 0.28 0.31 0.36 0.38 0.5 0.66 0.7 0.71 0.72 0.76 0.83 0.85 0.87 0.88 0.92 0.94 0.95 0.97 0.98 0.99\n                                                                                                                                                                     \n 8    6    8    4    2    1    1    1    1    1    2    1    1    2    1    1    1    1   1    1   1    1    1    1    1    1    1    1    1    1    2    1    1    1\n\nftable(iris_rf_probs$.pred_setosa)\n\n  0 0.01 0.02 0.05 0.11 0.12 0.68 0.73 0.81 0.95 0.97 0.98 0.99  1\n                                                                  \n 28    6    1    1    1    2    1    1    1    5    1    7    3  2\n\n\n\n### Iris Classifier: Gain and ROC Curves\n\nWe can plot gain and ROC curves for each of these models\n\n::: {.cell hash='Random-Forests_cache/html/Gain and ROC Curves `ranger`_4d61a502168be25c4ca5da6162ae106e'}\n\n```{.r .cell-code}\niris_ranger_probs %>% \n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %>% \n  glimpse()\n\nRows: 137\nColumns: 5\n$ .level          <chr> \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"set‚Ä¶\n$ .n              <dbl> 0, 2, 4, 5, 9, 10, 11, 12, 13, 17, 18, 19, 20, 21, 22,‚Ä¶\n$ .n_events       <dbl> 0, 2, 4, 5, 9, 10, 11, 12, 13, 17, 18, 19, 20, 21, 21,‚Ä¶\n$ .percent_tested <dbl> 0.000000, 3.333333, 6.666667, 8.333333, 15.000000, 16.‚Ä¶\n$ .percent_found  <dbl> 0.00000, 9.52381, 19.04762, 23.80952, 42.85714, 47.619‚Ä¶\n\niris_ranger_probs %>% \n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %>% \n  autoplot()\n\n\n\niris_ranger_probs %>% \n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %>% \n  glimpse()\n\nRows: 140\nColumns: 4\n$ .level      <chr> \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\"‚Ä¶\n$ .threshold  <dbl> -Inf, 0.000000000, 0.000625000, 0.001000000, 0.003958333, ‚Ä¶\n$ specificity <dbl> 0.0000000, 0.0000000, 0.3589744, 0.4102564, 0.4615385, 0.4‚Ä¶\n$ sensitivity <dbl> 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0‚Ä¶\n\niris_ranger_probs %>% \n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %>% \n  autoplot()\n\n\n\n:::\n\niris_rf_probs %>% \n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %>% \n  glimpse()\n\nRows: 90\nColumns: 5\n$ .level          <chr> \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"set‚Ä¶\n$ .n              <dbl> 0, 2, 5, 12, 13, 18, 19, 20, 21, 23, 24, 25, 26, 32, 6‚Ä¶\n$ .n_events       <dbl> 0, 2, 5, 12, 13, 18, 19, 20, 21, 21, 21, 21, 21, 21, 2‚Ä¶\n$ .percent_tested <dbl> 0.000000, 3.333333, 8.333333, 20.000000, 21.666667, 30‚Ä¶\n$ .percent_found  <dbl> 0.000000, 9.523810, 23.809524, 57.142857, 61.904762, 8‚Ä¶\n\niris_rf_probs %>% \n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %>% \n  autoplot()\n\n\n\niris_rf_probs %>% \n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %>% \n  glimpse()\n\nRows: 93\nColumns: 4\n$ .level      <chr> \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\"‚Ä¶\n$ .threshold  <dbl> -Inf, 0.00, 0.01, 0.02, 0.05, 0.11, 0.12, 0.68, 0.73, 0.81‚Ä¶\n$ specificity <dbl> 0.0000000, 0.0000000, 0.7179487, 0.8717949, 0.8974359, 0.9‚Ä¶\n$ sensitivity <dbl> 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0‚Ä¶\n\niris_rf_probs %>% \n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %>% \n  autoplot()\n\n\n\n\nIris Classifier: Metrics\n\npredict(iris_ranger, iris_testing_baked, type = \"prob\") %>% \n  bind_cols(predict(iris_ranger,iris_testing_baked)) %>% \n  bind_cols(select(iris_testing_baked,Species)) %>% \n  glimpse()\n\nRows: 60\nColumns: 5\n$ .pred_setosa     <dbl> 0.99147619, 0.96175794, 0.94710119, 0.94710119, 0.730‚Ä¶\n$ .pred_versicolor <dbl> 0.005666667, 0.020416667, 0.031291667, 0.031291667, 0‚Ä¶\n$ .pred_virginica  <dbl> 0.002857143, 0.017825397, 0.021607143, 0.021607143, 0‚Ä¶\n$ .pred_class      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setos‚Ä¶\n$ Species          <fct> setosa, setosa, setosa, setosa, setosa, setosa, setos‚Ä¶\n\npredict(iris_ranger, iris_testing_baked, type = \"prob\") %>% \n  bind_cols(predict(iris_ranger,iris_testing_baked)) %>% \n  bind_cols(select(iris_testing_baked,Species)) %>% \n  yardstick::metrics(data = ., truth = Species, estimate = .pred_class, ... = .pred_setosa:.pred_virginica)\n\n# A tibble: 4 √ó 3\n  .metric     .estimator .estimate\n  <chr>       <chr>          <dbl>\n1 accuracy    multiclass     0.967\n2 kap         multiclass     0.950\n3 mn_log_loss multiclass     0.221\n4 roc_auc     hand_till      0.987\n\n# And for the `randomForest`method\n\npredict(iris_rf, iris_testing_baked, type = \"prob\") %>% \n  bind_cols(predict(iris_ranger,iris_testing_baked)) %>% \n  bind_cols(select(iris_testing_baked,Species)) %>% \n  glimpse()\n\nRows: 60\nColumns: 5\n$ .pred_setosa     <dbl> 0.99, 0.97, 0.95, 0.95, 0.73, 0.81, 0.98, 0.98, 0.98,‚Ä¶\n$ .pred_versicolor <dbl> 0.01, 0.00, 0.02, 0.02, 0.22, 0.14, 0.00, 0.02, 0.00,‚Ä¶\n$ .pred_virginica  <dbl> 0.00, 0.03, 0.03, 0.03, 0.05, 0.05, 0.02, 0.00, 0.02,‚Ä¶\n$ .pred_class      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setos‚Ä¶\n$ Species          <fct> setosa, setosa, setosa, setosa, setosa, setosa, setos‚Ä¶\n\npredict(iris_rf, iris_testing_baked, type = \"prob\") %>% \n  bind_cols(predict(iris_ranger,iris_testing_baked)) %>% \n  bind_cols(select(iris_testing_baked,Species)) %>% \n  yardstick::metrics(data = ., truth = Species, estimate = .pred_class, ... = .pred_setosa:.pred_virginica)\n\n# A tibble: 4 √ó 3\n  .metric     .estimator .estimate\n  <chr>       <chr>          <dbl>\n1 accuracy    multiclass     0.967\n2 kap         multiclass     0.950\n3 mn_log_loss multiclass     0.197\n4 roc_auc     hand_till      0.988"
  },
  {
    "objectID": "content/courses/ML4Artists/4-Clustering/index.html",
    "href": "content/courses/ML4Artists/4-Clustering/index.html",
    "title": "Basics of Machine Learning - Clustering",
    "section": "",
    "text": "Introduction"
  },
  {
    "objectID": "content/courses/ML4Artists/listing.html",
    "href": "content/courses/ML4Artists/listing.html",
    "title": "Machine Learning for Artists",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "content/courses/NoCode/listing.html",
    "href": "content/courses/NoCode/listing.html",
    "title": "Data Vis with No Code",
    "section": "",
    "text": "Title\n\n\nReading Time\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/courses/R4Artists/listing.html",
    "href": "content/courses/R4Artists/listing.html",
    "title": "R for Artists",
    "section": "",
    "text": "üï∂ Science, Human Experience, Experiments, and Data\n\n\nWhy do we visualize data\n\n\n\n\n\n\nNov 1, 2021\n\n\n9 min\n\n\n\n\n\n\n \n\n\n\n\nLab-2: Down the R-abbit Hole‚Ä¶\n\n\nWelcome ! Introduce Yourself to R, RStudio, and to all of Us!\n\n\n\n\n\n\nJul 9, 2021\n\n\n0 min\n\n\n\n\n\n\n \n\n\n\n\nLab-3: Drink Me!\n\n\nWorking with Quarto\n\n\n\n\n\n\nMar 10, 2023\n\n\n1 min\n\n\n\n\n\n\n \n\n\n\n\nLab-4: I say what I mean and I mean what I say\n\n\nGetting started with Data in R\n\n\n\n\n\n\nJul 12, 2022\n\n\n0 min\n\n\n\n\n\n\n \n\n\n\n\nLab-5: Twas brillig, and the slithy toves‚Ä¶\n\n\nTidy Data at the wabe MoMA\n\n\n\n\n\n\nNov 22, 2022\n\n\n0 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLab-6: These Roses have been Painted !!\n\n\nThe Grammar of Graphics in R\n\n\n\n\n\n\nAug 21, 2022\n\n\n1 min\n\n\n\n\n\n\n \n\n\n\n\nLab-9: If you please sir‚Ä¶which way to the Secret Garden?\n\n\nThe Grammar of Maps\n\n\n\n\n\n\nJul 10, 2022\n\n\n1 min\n\n\n\n\n\n\n \n\n\n\n\nLab-10: An Invitation from the Queen‚Ä¶to play Croquet\n\n\nThe Grammar of Networks\n\n\n\n\n\n\nJun 6, 2022\n\n\n0 min\n\n\n\n\n\n\n \n\n\n\n\nLab-11: The Queen of Hearts, She Made some Tarts\n\n\nThe Grammar of Diagrams\n\n\n\n\n\n\nInvalid Date\n\n\n0 min\n\n\n\n\n\n\n \n\n\n\n\nLab-12: Time is a Him!!\n\n\nTime Series in R\n\n\n\n\n\n\nFeb 14, 2022\n\n\n6 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/10-Basics/basics.html",
    "href": "content/courses/R4Artists/Modules/10-Basics/basics.html",
    "title": "üï∂ Science, Human Experience, Experiments, and Data",
    "section": "",
    "text": "< iconify-icon icon=‚Äúicon-park:ppt‚Äù>< /iconify-icon>"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/10-Basics/basics.html#fa-envelope-titlean-envelope-where-does-data-come-from",
    "href": "content/courses/R4Artists/Modules/10-Basics/basics.html#fa-envelope-titlean-envelope-where-does-data-come-from",
    "title": "üï∂ Science, Human Experience, Experiments, and Data",
    "section": "\n Where does Data come from?",
    "text": "Where does Data come from?\nWe will need to form a basic understanding of basic scientific enterprise. Let us look at the slides."
  },
  {
    "objectID": "content/courses/R4Artists/Modules/10-Basics/basics.html#fa-chart-simple-why-visualize",
    "href": "content/courses/R4Artists/Modules/10-Basics/basics.html#fa-chart-simple-why-visualize",
    "title": "üï∂ Science, Human Experience, Experiments, and Data",
    "section": "\n Why Visualize?",
    "text": "Why Visualize?\n\nWe can digest information more easily when it is pictorial\nOur Working Memories are both short-term and limited in capacity. So a picture abstracts the details and presents us with an overall summary, an insight, or a story that is both easy to recall and easy on retention.\n\nData Viz includes shapes that carry strong cultural memories and impressions for us. These cultural memories help us to use data viz in a universal way to appeal to a wide variety of audiences. (Do humans have a gene for geometry?)\nIt helps sift facts and mere statements: for example:\n\n\n\nRape Capital\n\n\n\n\nWhat does Data Reveal?"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/10-Basics/basics.html#iconify-mdi-category-plus-what-are-data-types",
    "href": "content/courses/R4Artists/Modules/10-Basics/basics.html#iconify-mdi-category-plus-what-are-data-types",
    "title": "üï∂ Science, Human Experience, Experiments, and Data",
    "section": "\n What are Data Types?",
    "text": "What are Data Types?\n\n\n\nIn more detail:"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/10-Basics/basics.html#sec-data-types",
    "href": "content/courses/R4Artists/Modules/10-Basics/basics.html#sec-data-types",
    "title": "üï∂ Science, Human Experience, Experiments, and Data",
    "section": "\n How do we Spot Data Variable Types?",
    "text": "How do we Spot Data Variable Types?\nBy asking questions!\n\n\n\n\n\n\n No \n    Pronoun \n    Answer \n    Variable/Scale \n    Example \n    What Operations? \n  \n\n\n 1 \n    How Many / Much / Heavy? Few? Seldom? Often? When? \n    Quantities, with Scale and a Zero Value.Differences and Ratios /Products are meaningful. \n    Quantitative/Ratio \n    Length,Height,Temperature in Kelvin,Activity,Dose Amount,Reaction Rate,Flow Rate,Concentration,Pulse,Survival Rate \n    Correlation \n  \n\n 2 \n    How Many / Much / Heavy? Few? Seldom? Often? When? \n    Quantities with Scale.\nDifferences are meaningful, but not products or ratios \n    Quantitative/Interval \n    pH,SAT score(200-800),Credit score(300-850),SAT score(200-800),Year of Starting College \n    Mean,Standard Deviation \n  \n\n 3 \n    How, What Kind, What Sort \n    A Manner / Method, Type or Attribute from a list, with list items in some \" order\" ( e.g. good, better, improved, best..) \n    Qualitative/Ordinal \n    Socioeconomic status (Low income, Middle income, High income),Education level (HighSchool, BS, MS, PhD),Satisfaction rating(Very much Dislike, Dislike, Neutral, Like, Very Much Like) \n    Median,Percentile \n  \n\n 4 \n    What, Who, Where, Whom, Which \n    Name, Place, Animal, Thing \n    Qualitative/Nominal \n    Name \n    Count no. of cases,Mode \n  \n\n\n\n\n\nAs you go from Qualitative to Quantitative data types in the table, I hope you can detect a movement from fuzzy groups/categories to more and more crystallized numbers.\n\n\nType of Variables\n\n\nEach variable/scale can be subjected to the operations of the previous group. In the words of S.S. Stevens\n\nthe basic operations needed to create each type of scale is cumulative: to an operation listed opposite a particular scale must be added all those operations preceding it."
  },
  {
    "objectID": "content/courses/R4Artists/Modules/10-Basics/basics.html#sec-data-viz",
    "href": "content/courses/R4Artists/Modules/10-Basics/basics.html#sec-data-viz",
    "title": "üï∂ Science, Human Experience, Experiments, and Data",
    "section": "What Are the Parts of a Data Viz?",
    "text": "What Are the Parts of a Data Viz?"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/10-Basics/basics.html#how-to-pick-a-data-viz",
    "href": "content/courses/R4Artists/Modules/10-Basics/basics.html#how-to-pick-a-data-viz",
    "title": "üï∂ Science, Human Experience, Experiments, and Data",
    "section": "How to pick a Data Viz?",
    "text": "How to pick a Data Viz?\nMost Data Visualizations use one or more of the following geometric attributes or aesthetics. These geometric aesthetics are used to represent qualitative or quantitative variables from your data.\n\n\nCommon Geometric Aesthetics in Charts\n\n\nWhat does this mean? We can think of simple visualizations as combinations of these aesthetics. Some examples:\n\n\nGeometries , Combinations, and Graphs\n\n\n\n\n\n\nAesthetic #1\nAesthetic #2\nShape\n\n\n\nPosition X = Quant Variable\nPosition Y = Quant Variable\nP oints/Circles with Fixed Size\n\n\nPosition X = Qual Variable\nPosition Y = Count of Qual Variable\nColumns\n\n\nPosition X = Qual Variable\nPosition Y = Qual Variable\nRectangles, with area proportional to joint(X,Y) count\n\n\n\nPosition X = Qual Variable\nPosition Y = Rank-Ordered Quant Variable\nBox + Whisker, Box length proportional to I nter-Quartile Range, w hisker-length proportional to upper and lower quartile resp.\n\n\nPosition X = Quant Variable\nPosition Y = Quant Variable + Qual Variable\n\nLine and Area\nColour for Area"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/100-GoN/index.html#fa-folder-open-slides-and-tutorials",
    "href": "content/courses/R4Artists/Modules/100-GoN/index.html#fa-folder-open-slides-and-tutorials",
    "title": "Lab-10: An Invitation from the Queen‚Ä¶to play Croquet",
    "section": "\n Slides and Tutorials",
    "text": "Slides and Tutorials"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/100-GoN/index.html#fa-envelope-introduction",
    "href": "content/courses/R4Artists/Modules/100-GoN/index.html#fa-envelope-introduction",
    "title": "Lab-10: An Invitation from the Queen‚Ä¶to play Croquet",
    "section": "\n Introduction",
    "text": "Introduction\nNetwork Diagrams are important in data visualization to bring out relationships between diverse entities. They are used in ecology, biology, transportation, and even history!\nAnd hey, whom did Jon Snow marry?"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/100-GoN/index.html#fa-asterisk-references",
    "href": "content/courses/R4Artists/Modules/100-GoN/index.html#fa-asterisk-references",
    "title": "Lab-10: An Invitation from the Queen‚Ä¶to play Croquet",
    "section": "\n References",
    "text": "References\n\nMichael Gastner, Data Analysis and Visualisation with R, Chapter 23: Networks\nDavid Schoch, Network Visualizations in R using ggraph and graphlayouts\nKonrad M. Lawson, Toilers and Gangsters:Simple Network Visualization with R for Historians"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html",
    "title": "The Grammar of Diagrams",
    "section": "",
    "text": "There are many presentation and drawing tools out there. And these allow the user full control over the diagram so generally result in prettier diagrams that can convey more information to the audience at that point in time.\nBut that point in time passes, and pretty pictures can quickly become out-of-date and, ironically, misinforming if they don‚Äôt match the reality of the system they are describing. This is especially so if one team is drawing the pretty pictures, and another team is writing the software/implementing the system.\nHaving diagrams as code that can live beside the system design/code, that the stakeholders are equally comfortable editing and viewing,reduces the gap i.e.¬†‚ÄúWhere system diagrams meet system reality‚Äù.\nWe will ‚Äúexplore‚Äù two packages to do this: DiagrammeR and nomnoml. Each of these follows a specific grammar so that sets of ‚Äúsentences‚Äù will morph into very different kinds of diagrams."
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#sequence-diagram",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#sequence-diagram",
    "title": "The Grammar of Diagrams",
    "section": "Sequence Diagram",
    "text": "Sequence Diagram\nLook at the code below: What do you think it represents?\n\nCodeDiagrammeR(\"\nsequenceDiagram\nArvind ->> Anamika: Why are you late today?\nAnamika ->> Anamika: Ulp...\nAnamika ->> Arvind: I am sorry... <br> may I come in please?\n\nArvind ->> Komal: And you? What kept you?\nKomal ->> Anamika: (Quietly) He's having a bad day, dude...\nAnamika ->> Komal: (Whisper) Boomer...\n\")\n\n\n\n\n\nThis is a simple Sequence Diagram! Shows a strictly imaginary exchange between a pair of students and an unknown Faculty Member.\nLet us now see how we can embellish this kind of diagram. Can we have a Garden of Forking Paths?\n\nCodeDiagrammeR(\"\n  graph LR\n    A-->B\n    A-->C\n    C-->E\n    B-->D\n    C-->D\n    D-->F\n    E-->F\n\")\n\n\n\n\n\n\nCodeDiagrammeR(\"\n        sequenceDiagram\n        \n        alt Anamika is always punctual\n        Arvind ->> Anamika: Why haven't you put up your Daily Reflection?\n        Anamika ->> Anamika: Ulp...\n        Note right of Anamika : I have had it today..\n        Anamika ->> Arvind: I am sorry... \n        Arvind ->> Anamika: Ok write it today\n        \n        else Anamika is usually tardy\n        Arvind ->> Anamika: Why haven't you put up your Daily Reflection?\n        Anamika ->> Anamika: Ulp...\n        Anamika ->> Arvind: I am sorry... \n        Arvind ->> Anamika: This is not acceptable and will reflect in your grade\n        end\n        \n        Arvind ->> Komal: And you? What kept you?\n        Komal ->> Anamika: (Quietly) He's having a bad day, dude...\n        Anamika ->> Komal: (Whisper) Boomer...\n        Note over Anamika,Komal: Giggle...\n\")\n\n\n\n\n\n\nFrom here: https://cyberhelp.sesync.org/blog/visualization-with-diagrammeR.html\n\nCodegrViz(\"digraph{\n\n      graph[rankdir = LR]\n  \n      node[shape = rectangle, style = filled]\n  \n      node[fillcolor = Coral, margin = 0.2]\n      A[label = 'Figure 1: Map']\n      B[label = 'Figure 2: Metrics']\n  \n      node[fillcolor = Cyan, margin = 0.2]\n      C[label = 'Figures.Rmd']\n  \n      node[fillcolor = Violet, margin = 0.2]\n      D[label = 'Analysis_1.R']\n      E[label = 'Analysis_2.R']\n  \n      subgraph cluster_0 {\n        graph[shape = rectangle]\n        style = rounded\n        bgcolor = Gold\n    \n        label = 'Data Source 1'\n        node[shape = rectangle, fillcolor = LemonChiffon, margin = 0.25]\n        F[label = 'my_dataframe_1.csv']\n        G[label = 'my_dataframe_2.csv']\n      }\n  \n      subgraph cluster_1 {\n         graph[shape = rectangle]\n         style = rounded\n         bgcolor = Gold\n    \n         label = 'Data Source 2'\n         node[shape = rectangle, fillcolor = LemonChiffon, margin = 0.25]\n         H[label = 'my_dataframe_3.csv']\n         I[label = 'my_dataframe_4.csv']\n      }\n  \n      edge[color = black, arrowhead = vee, arrowsize = 1.25]\n      C -> {A B}\n      D -> C\n      E -> C\n      F -> D\n      G -> D\n      H -> E\n      I -> E\n      \n      }\")\n\n\n\n\n\n\nCodemermaid(\"\n        graph BT\n        A((Salinity))\n        A-->B(Barnacles)\n        B-.->|-0.10|B1{Mussels}\n        A-- 0.30 -->B1\n\n        C[Air Temp]\n        C-->B\n        C-.->E(Macroalgae)\n        E-->B1\n        C== 0.89 ==>B1\n\n        style A fill:#FFF, stroke:#333, stroke-width:4px\n        style B fill:#9AA, stroke:#9AA, stroke-width:2px\n        style B1 fill:#879, stroke:#333, stroke-width:1px\n        style C fill:#ADF, stroke:#333, stroke-width:2px\n        style E fill:#9C2, stroke:#9C2, stroke-width:2px\n\n        \")\n\n\n\n\n\n\nCodeDiagrammeR(\"\nsequenceDiagram\n  Arvind ->>ticket seller: ask ticket\n  ticket seller->>database: seats\n  alt tickets available\n    database->>ticket seller: ok\n    ticket seller->>customer: confirm\n    Arvind ->>ticket seller: ok\n    ticket seller->>database: book a seat\n    ticket seller->>printer: print ticket\n  else sold out\n    database->>ticket seller: none left\n    ticket seller->>customer: sorry\n  end\n\")\n\n\n\n\n\n\nCodeDiagrammeR(\n\"graph TB;\nA(Rounded)-->B[Squared];\nB-->C{A Decision};\nC-->D[Square One];\nC-->E[Square Two];\n\n%% Now styling these blocks\nstyle A fill:#E5E25F;  \nstyle B fill:#87AB51; \nstyle C fill:#3C8937;\nstyle D fill:#23772C;  \nstyle E fill:#B6E6E6;\n\"\n)\n\n\n\n\n\n\nCode  grViz(\"\ndigraph boxes_and_circles {\n\n  # a 'graph' statement\n  graph [overlap = true, fontsize = 10,forcelabels = true]\n\n  # several 'node' statements\n  node [shape = box,fontname = Helvetica, color = red, style = filled]\n  A[label = 'This is \\\\n an internal \\\\n label', xlabel = 'This is \\\\nan external \\\\nlabel']; B; C; D; E; F\n\n  node [shape = circle, fixedsize = true, color = palegreen, width = 0.9] // sets as circles\n  1; 2; 3; 4; 5; 6; 7; 8\n\n  # several 'edge' statements\n  A->{1,2,3,4} B->2 B->3 B->4 C->A\n  1->D E->A 2->4 1->5 1->F\n  E->6 4->6 5->7 6->7 3->8 3->1\n}\n\")"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#sequence-diagram-2",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#sequence-diagram-2",
    "title": "The Grammar of Diagrams",
    "section": "Sequence Diagram-2",
    "text": "Sequence Diagram-2"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#sequence-diagram-3",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#sequence-diagram-3",
    "title": "The Grammar of Diagrams",
    "section": "Sequence Diagram 3",
    "text": "Sequence Diagram 3"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#mindmap",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#mindmap",
    "title": "The Grammar of Diagrams",
    "section": "Mindmap",
    "text": "Mindmap"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#gantt-chart",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#gantt-chart",
    "title": "The Grammar of Diagrams",
    "section": "Gantt Chart",
    "text": "Gantt Chart"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#flow-chart",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#flow-chart",
    "title": "The Grammar of Diagrams",
    "section": "Flow chart",
    "text": "Flow chart"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#some-definitions-on-the-grammar-of-shapes-in-nomnoml",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#some-definitions-on-the-grammar-of-shapes-in-nomnoml",
    "title": "The Grammar of Diagrams",
    "section": "Some definitions on the ‚Äúgrammar of shapes‚Äù in nomnoml\n",
    "text": "Some definitions on the ‚Äúgrammar of shapes‚Äù in nomnoml\n\n\nAssociation Types: Connectors between blocks( i.e.¬†Classifiers)\nClassifier Types: Kinds of blocks.\nDirective Types: Directives change the nature of the diagram rendered, by affective parameters like colour, direction and margins. ( Ha! VC people!!)\n\nCSS colours https://www.w3schools.com/cssref/css_colors.asp Only these colours are permitted, so use either the names or these specific colour hash codes. Any general hash code will not render.\n\nCode//association-1\n[a] - [b] \n\n//association-2\n[b] -> [c] \n\n//association_3\n[c] <-> [a]\n\n//dependency-1\n[a] <-->[d]\n\n//dependency-2\n#.ell: visual=ellipse fill=#fbfb09 bold\n#.arvind: visual=rhomb fill=#ff2234 bold\n[<ell>e]-->[a]\n//generalization-1\n[c]-:>[<arvind>k]\n\n//implementation --:>\n[k]--:>[d]\n\n\n\n\n\n\nCode//composition +-\n[a]+-[b]\n//composition +->\n[b]-+[c]\n//aggregation o-\n[c]o->[d]\n//aggregation o->\n[d]o->[a]\n//note --\n[d]--[everything happens;here]\n//hidden -/-\n[d]-/-[f]\n////////////////////////\n//weightless edge _>\n//[k]_>[d] //not working\n//weightless dashed__\n//[d]__[j] //not working\n\n\n\n\n\nClassifier Types\nThese are different kinds of blocks.\n\nCode[class]->[<abstract> abstract]\n[<abstract> abstract]-:>[<instance> instance]\n[<instance> instance]-:>[<note> note]\n[<note> note]-->[<reference> reference]\n\n\n\n\n\n\nCode[<package> package|components]-->[<frame> frame|]\n[<database> database]-->[<start> start]\n[<end> end]-o>[<state> state]\n\n\n\n\n\n\nCode[<choice> choice]--->[<sync> sync]\n[<input> input]->[<sender> sender]\n[<receiver> receiver]o-[<transceiver> transceiver]\n\n\n\n\n\n\nCode#direction:down\n#background:lightgrey\n#fill: fuchsia; green; purple\n#fillArrows: false\n#font: Courier\n[class]->[<abstract> abstract]\n[<abstract> abstract]-:>[<instance> instance]\n[<instance> instance]-:>[<note> note]\n[<note> note]-->[<reference> reference]\n\n\n\n\n\n\nCode#font: CenturySchoolbook\n#fill: lightyellow\n#stroke: green\n\n[<actor> actor]---[<usecase> usecase]\n[<usecase> usecase]<-->[<label> label]\n[<usecase> usecase]-/-[<hidden> hidden]\n\n\n\n\n\n\nCode[<table> table| a | 5 || b | 7]\n\n\n\n\n\n\n\nCode[<table> table| c | 9 ]"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#directives",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#directives",
    "title": "The Grammar of Diagrams",
    "section": "Directives",
    "text": "Directives\nDirectives change the nature of the diagram rendered, by affective parameters like colour, direction and margins."
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#custom-classifier-styles",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#custom-classifier-styles",
    "title": "The Grammar of Diagrams",
    "section": "Custom classifier styles",
    "text": "Custom classifier styles\nA directive that starts with ‚Äú.‚Äù define a classifier‚Äôs style. The style is written as a space separated list of modifiers and key/value pairs.\n\nCode#.box: fill=#8f8 dashed\n#.blob: visual=ellipse title=bold\n#.arvind: visual=rhomb title=bold dashed fill=CornFlowerBlue\n[<box> GreenBox]\n[<blob> Blobby]\n[<arvind> Someone]"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#nomnoml-keyvalue-pairs",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#nomnoml-keyvalue-pairs",
    "title": "The Grammar of Diagrams",
    "section": "\nnomnoml Key/value pairs",
    "text": "nomnoml Key/value pairs\n\nfill=(any css color)\nstroke=(any css color)\nalign=center align=left\ndirection=right direction=down\nvisual=actor\nvisual=class\nvisual=database\nvisual=ellipse\nvisual=end\nvisual=frame\nvisual=hidden\nvisual=input\nvisual=none\nvisual=note\nvisual=package\nvisual=receiver\nvisual=rhomb\nvisual=roundrect\nvisual=sender\nvisual=start\nvisual=sync\nvisual=table\nvisual=transceiver"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#text-modifiers",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#text-modifiers",
    "title": "The Grammar of Diagrams",
    "section": "Text modifiers",
    "text": "Text modifiers\nbold center italic left underline\n\nCode# .box: fill=#8f8 dashed\n# .blob: visual=rhomb title=bold fill=#8f8 dashed\n\n[A]-[B]\n[B]--[<usecase>C]\n[C]-[<box> D]\n[B]--[<blob> Jabba;TheHut]\n\n\n\n\n\n\nCode[a] ->[b]\n[b] -:> [c]\n[c]o->[d]\n[d]-/-[e]\n\n\n\n\n\n\nCode#fill: lightgreen; lightblue; lightyellow; grey; white\n\n[<table> table | c | 9 ]\n\n[R | [<table> Packages |\n         Base R |\n         [ <table> tidyverse| ggplot | tidyr | readr |\n             [<table> dplyr|\n                 magrittr | Others]]]]\n\n\n\n\n\n\n\nCode#fill: lightgreen; lightblue; lightyellow; pink; white\n\n[RStudio | [R | [<table> Packages |\n                   Base R | [ tidyverse |\n                               ggplot | tidyr | readr |\n                               [dplyr]--[magrittr]\n                               [dplyr]--[Others]\n                             | tibble\n                             ]\n                 | lubridate | DiagrammeR | Lattice]]]\n\n\n\n\n\n\n\nCode[Linux]+-[Ubuntu]\n[Linux]+-[Mint]\n[Ubuntu]--[Mint]\n[Linux]+-[Rosa Linux]\n[Linux]+-[Mx Linux]\n[Debian]-+[Linux]\n\n\n[Fedora]-+[Linux]\n[Puppy Linux]-+[Linux]\n[Personal Pups]-+[Puppy Linux]"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/index.html#introduction",
    "href": "content/courses/R4Artists/Modules/110-GoD/index.html#introduction",
    "title": "Lab-11: The Queen of Hearts, She Made some Tarts",
    "section": "Introduction",
    "text": "Introduction\nWe can use R to create complicated diagrams too ! Flow charts, Gantt charts, Org charts‚Ä¶all with R. We will use packages such as nomnoml and DiagrammeR to achieve these ends."
  },
  {
    "objectID": "content/courses/R4Artists/Modules/120-time/index.html#introduction",
    "href": "content/courses/R4Artists/Modules/120-time/index.html#introduction",
    "title": "Lab-12: Time is a Him!!",
    "section": "Introduction",
    "text": "Introduction\nTime Series data are important in data visualization where events have a temporal dimension, such as with finance, transportation, music, telecommunications for example.\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(ggformula)\n##########################################\n# Install core TimeSeries Packages\n# library(ctv)\n# ctv::install.views(\"TimeSeries\", coreOnly = TRUE)\n# To update core TimeSeries packages\n# ctv::update.views(\"TimeSeries\")\n# Time Series Core Packages\n##########################################\nlibrary(tsibble)\nlibrary(feasts) # Feature Extraction and Statistics for Time Series\nlibrary(fable) # Forecasting Models for Tidy Time Series\nlibrary(tseries) # Time Series Analysis and Computational Finance\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n\n\nAttaching package: 'tseries'\n\n\nThe following object is masked from 'package:mosaic':\n\n    value\n\nlibrary(forecast)\n\n\nAttaching package: 'forecast'\n\n\nThe following objects are masked from 'package:fabletools':\n\n    accuracy, forecast\n\nlibrary(zoo)\n\n\nAttaching package: 'zoo'\n\n\nThe following object is masked from 'package:tsibble':\n\n    index\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n##########################################\nlibrary(tsibbledata) # Time Series Demo Datasets"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/120-time/index.html#creating-time-series",
    "href": "content/courses/R4Artists/Modules/120-time/index.html#creating-time-series",
    "title": "Lab-12: Time is a Him!!",
    "section": "Creating time series",
    "text": "Creating time series\nIn this first example, we will use simple ts data, and then do another with a tibble dataset, and then a third example with an tsibble formatted dataset.\n\nts format data\nThere are a few datasets in base R that are in ts format already.\n\nAirPassengers\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1949 112 118 132 129 121 135 148 148 136 119 104 118\n1950 115 126 141 135 125 149 170 170 158 133 114 140\n1951 145 150 178 163 172 178 199 199 184 162 146 166\n1952 171 180 193 181 183 218 230 242 209 191 172 194\n1953 196 196 236 235 229 243 264 272 237 211 180 201\n1954 204 188 235 227 234 264 302 293 259 229 203 229\n1955 242 233 267 269 270 315 364 347 312 274 237 278\n1956 284 277 317 313 318 374 413 405 355 306 271 306\n1957 315 301 356 348 355 422 465 467 404 347 305 336\n1958 340 318 362 348 363 435 491 505 404 359 310 337\n1959 360 342 406 396 420 472 548 559 463 407 362 405\n1960 417 391 419 461 472 535 622 606 508 461 390 432\n\nstr(AirPassengers)\n\n Time-Series [1:144] from 1949 to 1961: 112 118 132 129 121 135 148 148 136 119 ...\n\n\nThis can be easily plotted using base R:\n\nplot(AirPassengers)\n\n\n\n\nLet us take data that is ‚Äútime oriented‚Äù but not in ts format, and convert it to ts: the syntax of ts() is:\nSyntax: objectName <- ts(data, start, end, frequency) where, - data represents the data vector - start represents the first observation in time series - end represents the last observation in time series - frequency represents number of observations per unit time. For example, frequency=1 for monthly data.\nWe will pick simple numerical vector data ( i.e.¬†not a timeseries ) ChickWeight:\n\nChickWeight %>% head()\n\n\n\n  \n\n\nChickWeight_ts <- ts(ChickWeight$weight, frequency = 2)\nplot(ChickWeight_ts)\n\n\n\n\n\n\n\n\n\n\nThe `ts` format\n\n\n\nThe ts format is not recommended for new analysis since it does not permit inclusion of multiple time series in one dataset, nor other categorical variables for grouping etc.\n\n\n\ntibble format data\nSome ‚Äútime-oriented‚Äù datasets are available in tibble form. Let us try to plot one, the walmart_sales_weekly dataset from the timetk package:\n\ndata(walmart_sales_weekly, package = \"timetk\")\nwalmart_sales_weekly\n\n\n\n  \n\n\n\nThis dataset is a tibble with a Date column. The Dept column is clearly a categorical column that allows us to distinguish separate time series, i.e.¬†one for each value of Dept. We will convert that to a factor( it is an double precision number ) and then plot the data using this column on the Date on the \\(x\\)-axis:\n\nwalmart_sales_weekly %>% \n  \n  # convert Dept number to a **categorical factor**\n  mutate(Dept = forcats::as_factor(Dept)) %>% \n  \n  gf_point(Weekly_Sales ~ Date, \n           group = ~ Dept, \n           colour = ~ Dept, data = .) %>% \n  gf_line() %>% \n  gf_theme(theme_minimal())\n\n\n\n\nFor more analysis and forecasting etc., it is useful to convert this tibble into a tsibble:\n\nwalmart_tsibble <- as_tsibble(walmart_sales_weekly,\n                         index = Date,\n                         key = c(id, Dept))\nwalmart_tsibble\n\n\n\n  \n\n\n\nThe 7D states the data is weekly. There is a Date column and all the other numerical variables are time-varying quantities. The categorical variables such as id, and Dept allow us to identify separate time series in the data, and these have 7 combinations hence are 7 time series in this data, as indicated.\n\n\nLet us plot Weekly_Sales, colouring the time series by Dept:\n\nwalmart_tsibble %>% \n  gf_line(Weekly_Sales ~ Date, \n          colour = ~ as_factor(Dept), data = .) %>% \n  gf_point() %>% \n  labs(title = \"Weekly Sales by Dept at Walmart\")\n\n[[1]]\n\n\n\n$title\n[1] \"Weekly Sales by Dept at Walmart\"\n\nattr(,\"class\")\n[1] \"labels\"\n\n\n\n\nFigure¬†1: Walmart Time Series\n\n\n\n\n\nWe can also do a quick autoplot that seems to offer less control and is also not interactive.\n\nwalmart_tsibble %>% \n  dplyr::group_by(Dept) %>% \n  autoplot(Weekly_Sales)\n\n`mutate_if()` ignored the following grouping variables:\n‚Ä¢ Column `Dept`\n\n\n\n\n\n\n\ntsibble format data\nIn the packages tsibbledata and fpp3 we have a good choice of tsibble format data. Let us pick one:\n\nhh_budget\n\n\n\n  \n\n\n\nThere are 4 keys ( id variables ) here, one for each country. Six other quantitative columns are the individual series. Let us plot the timeseries:"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/120-time/index.html#one-more-example",
    "href": "content/courses/R4Artists/Modules/120-time/index.html#one-more-example",
    "title": "Lab-12: Time is a Him!!",
    "section": "One more example",
    "text": "One more example\nOften we have data in table form, that is time-oriented, with a date like column, and we need to convert it into a tsibble for analysis:\n\nprison <- readr::read_csv(\"https://OTexts.com/fpp3/extrafiles/prison_population.csv\")\n\nRows: 3072 Columns: 6\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr  (4): State, Gender, Legal, Indigenous\ndbl  (1): Count\ndate (1): Date\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(prison)\n\nRows: 3,072\nColumns: 6\n$ Date       <date> 2005-03-01, 2005-03-01, 2005-03-01, 2005-03-01, 2005-03-01‚Ä¶\n$ State      <chr> \"ACT\", \"ACT\", \"ACT\", \"ACT\", \"ACT\", \"ACT\", \"ACT\", \"ACT\", \"NS‚Ä¶\n$ Gender     <chr> \"Female\", \"Female\", \"Female\", \"Female\", \"Male\", \"Male\", \"Ma‚Ä¶\n$ Legal      <chr> \"Remanded\", \"Remanded\", \"Sentenced\", \"Sentenced\", \"Remanded‚Ä¶\n$ Indigenous <chr> \"ATSI\", \"Non-ATSI\", \"ATSI\", \"Non-ATSI\", \"ATSI\", \"Non-ATSI\",‚Ä¶\n$ Count      <dbl> 0, 2, 0, 5, 7, 58, 5, 101, 51, 131, 145, 323, 355, 1617, 12‚Ä¶\n\n\nWe have a Date column for the time index, and we have unique key variables like State, Gender, Legal and Indigenous. Count is the value that is variable over time. It also appears that the data is quarterly, since mosaic::inspect reports the max_diff in the Date column as \\(92\\). (Run mosaic::inspect(prison) in your Console).\n\nprison_tsibble <- prison %>% \n  mutate(quarter = yearquarter(Date)) %>% \n  select(-Date) %>% # Remove the Date column now that we have quarters\n  as_tsibble(index = quarter, key = c(State, Gender, Legal, Indigenous))\n\nprison_tsibble\n\n\n\n  \n\n\n\n(Here, ATSI stands for Aboriginal or Torres Strait Islander.). We have \\(64\\) time series here, organized quarterly.\nLet us examine the key variables:\n\nprison_tsibble %>% distinct(Indigenous)\n\n\n\n  \n\n\nprison_tsibble %>% distinct(State)\n\n\n\n  \n\n\n\nSo we can plot the time series, faceted by / coloured by State:\n\nprison_tsibble %>% \n  tsibble::index_by() %>% \n  group_by(Indigenous, State) %>% \n  #filter(State == \"NSW\") %>% \n  summarise(Total = sum(Count))  %>%\n  ggplot(aes(x = quarter, y = Total, colour = Indigenous, \n             shape = Indigenous)) + \n  geom_point() +\n  geom_line()  + \n  facet_wrap(~ State)\n\n\n\n\nHmm‚Ä¶looks like New South Wales(NSW) as something different going on compared to the rest of the states in Aus."
  },
  {
    "objectID": "content/courses/R4Artists/Modules/120-time/index.html#decomposing-time-series",
    "href": "content/courses/R4Artists/Modules/120-time/index.html#decomposing-time-series",
    "title": "Lab-12: Time is a Him!!",
    "section": "Decomposing Time Series",
    "text": "Decomposing Time Series\nWe can decompose the Weekly_Sales into components representing trends, seasonal events that repeat, and irregular noise. Since each Dept could have a different set of trends, we will do this first for one Dept, say Dept #95:\n\nwalmart_decomposed_season <- walmart_tsibble %>% \n  dplyr::filter(Dept == \"95\") %>% # filter for Dept 95\n  #\n  # feasts depends upon fabletools.\n  # \n  fabletools::model(\n    season = STL(Weekly_Sales ~ season(window = \"periodic\"))) \n\nwalmart_decomposed_ets <- walmart_tsibble %>% \n  dplyr::filter(Dept == \"95\") %>% # filter for Dept 95\n  #\n  # feasts depends upon fabletools.\n  # \n  fabletools::model(\n    ets = ETS(box_cox(Weekly_Sales, 0.3)))\n\nwalmart_decomposed_season %>% fabletools::components()\n\n\n\n  \n\n\nwalmart_decomposed_ets %>% fabletools::components()\n\n\n\n  \n\n\n# walmart_decomposed_arima <- walmart_tsibble %>% \n#   dplyr::filter(Dept == \"95\") %>% # filter for Dept 95\n#     arima = ARIMA(log(Weekly_Sales))\n\n\nwalmart_decomposed_season %>% \n  components() %>% \n  autoplot() + \n  labs( title = \"Seasonal Variations in Weekly Sales, Dept #95\")\n\n\n\nwalmart_decomposed_ets %>% \n  components() %>% \n  autoplot() + \n  labs( title = \"ETS Variations in Weekly Sales, Dept #95\")\n\nWarning: Removed 1 row containing missing values (`geom_line()`)."
  },
  {
    "objectID": "content/courses/R4Artists/Modules/120-time/index.html#references",
    "href": "content/courses/R4Artists/Modules/120-time/index.html#references",
    "title": "Lab-12: Time is a Him!!",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/140-website/index.html#introduction",
    "href": "content/courses/R4Artists/Modules/140-website/index.html#introduction",
    "title": "Lab-14: You‚Äôre are Nothing but a Pack of Cards!!",
    "section": "Introduction",
    "text": "Introduction\nLet‚Äôs make a website in RStudio to show off our data viz portfolio, and to share with friends, parents, prospective employers‚Ä¶\nWe will encounter a new package called blogdown and use workflows with github and a free web hosting service called Netlify to create a website where all our RMarkdowns become individual blog posts, complete with Titles, Sections, Text, Diagrams and Links!"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/140-website/index.html#references",
    "href": "content/courses/R4Artists/Modules/140-website/index.html#references",
    "title": "Lab-14: You‚Äôre are Nothing but a Pack of Cards!!",
    "section": "References",
    "text": "References\n\nAllison Hill\nSharon Macliss\nYihui Xie"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/20-intro/intro.html#introduction",
    "href": "content/courses/R4Artists/Modules/20-intro/intro.html#introduction",
    "title": "Lab-2: Down the R-abbit Hole‚Ä¶",
    "section": "Introduction",
    "text": "Introduction\nWelcome!\nLet‚Äôs start our journey to the Garden of Data Visualization, with this terrific presentation by the great ( and sadly late..) Hans Rosling.\nThe best stats you‚Äôve ever seen by Hans Rosling:\n\nWe will run some boiler-plate R code today! Nothing ( almost! ) to code! We will get used to the tools and words of the trade: R, RStudio, installation, packages, libraries‚Ä¶."
  },
  {
    "objectID": "content/courses/R4Artists/Modules/30-rmd/rmd.html",
    "href": "content/courses/R4Artists/Modules/30-rmd/rmd.html",
    "title": "Lab-3: Drink Me!",
    "section": "",
    "text": "We will get acquainted with the RMarkdown Quarto Document format, which allows us to mix text narrative, code, code-developed figures and items from the web in a seamless document.Then we can try to create something in the lines of what Hans Rosling did.\nSomething that can:\n\nprovide a visualization\nprovide insight\ntell a story\nis reproducible"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/30-rmd/rmd.html#setting-up-quarto",
    "href": "content/courses/R4Artists/Modules/30-rmd/rmd.html#setting-up-quarto",
    "title": "Lab-3: Drink Me!",
    "section": "Setting up Quarto",
    "text": "Setting up Quarto\nQuarto is installed along with RStudio. We can check if all is in order by running a check in the Terminal in RStudio. \nThe commands are:\n\nquarto check install\nquarto check knitr\n\nIf these come out with no errors then we are ready to fire up our first Quarto document."
  },
  {
    "objectID": "content/courses/R4Artists/Modules/30-rmd/rmd.html#practice",
    "href": "content/courses/R4Artists/Modules/30-rmd/rmd.html#practice",
    "title": "Lab-3: Drink Me!",
    "section": "Practice",
    "text": "Practice\nFire up a new Quarto document by going to: File -> New File -> Quarto Document.Switch to Visual mode, if it is not already there.\nUse the visual mode tool bar.\nTry to create Sections, code chunks, embedding images and tables.\nHit the Render button to see how the documents is converted into an html document."
  },
  {
    "objectID": "content/courses/R4Artists/Modules/30-rmd/rmd.html#references",
    "href": "content/courses/R4Artists/Modules/30-rmd/rmd.html#references",
    "title": "Lab-3: Drink Me!",
    "section": "References:",
    "text": "References:\n\nhttps://rmarkdown.rstudio.com/index.html\nhttps://ysc-rmarkdown.netlify.app/slides/01-basics.html Nice RMarkdown presentation and ‚Äúcode movies‚Äù !\nhttps://www.markdowntutorial.com\nhttps://quarto.org/docs/authoring/markdown-basics.html"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/30-rmd/rmd.html#assignments",
    "href": "content/courses/R4Artists/Modules/30-rmd/rmd.html#assignments",
    "title": "Lab-3: Drink Me!",
    "section": "Assignment(s)",
    "text": "Assignment(s)\n\nComplete the markdown tutorial in [reference 1]\nCreate a fresh Quarto document and use as many as possible of the RMarkdown constructs from the Cheatsheet [reference 4]"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/30-rmd/rmd.html#fun-stuff",
    "href": "content/courses/R4Artists/Modules/30-rmd/rmd.html#fun-stuff",
    "title": "Lab-3: Drink Me!",
    "section": "Fun Stuff",
    "text": "Fun Stuff\n\nDesir√©e De Leon, Alison Hill: rstudio4edu: A Handbook for Teaching and Learning with R and RStudio, https://rstudio4edu.github.io/rstudio4edu-book/"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/40-working-in-r/working.html",
    "href": "content/courses/R4Artists/Modules/40-working-in-r/working.html",
    "title": "Lab-4: I say what I mean and I mean what I say",
    "section": "",
    "text": "R Tutorial\n Slides"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/40-working-in-r/working.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "href": "content/courses/R4Artists/Modules/40-working-in-r/working.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "title": "Lab-4: I say what I mean and I mean what I say",
    "section": "\n Introduction",
    "text": "Introduction\nWe will get acquainted with data and its representations in R! We will also form a view of how English Grammar ( pronouns, verbs, adjectives, figures of speech..) get metaphorized into the R World!!"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/40-working-in-r/working.html#readings",
    "href": "content/courses/R4Artists/Modules/40-working-in-r/working.html#readings",
    "title": "Lab-4: I say what I mean and I mean what I say",
    "section": "Readings",
    "text": "Readings\n\nR for Data Science, Workflow: Basics Chapter: http://r4ds.had.co.nz/workflow-basics.html\nModern Dive, Getting Started Chapter: http://moderndive.com/2-getting-started.html\nR & RStudio Basics: https://bookdown.org/chesterismay/rbasics/3-rstudiobasics.html\nRStudio IDE Cheatsheet: https://github.com/rstudio/cheatsheets/blob/master/rstudio-ide.pdf"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/50-working-in-tidyverse/tidy.html",
    "href": "content/courses/R4Artists/Modules/50-working-in-tidyverse/tidy.html",
    "title": "Lab-5: Twas brillig, and the slithy toves‚Ä¶",
    "section": "",
    "text": "R Tutorial\n  Slides\ndplyr Tutorial"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/50-working-in-tidyverse/tidy.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "href": "content/courses/R4Artists/Modules/50-working-in-tidyverse/tidy.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "title": "Lab-5: Twas brillig, and the slithy toves‚Ä¶",
    "section": "\n Introduction",
    "text": "Introduction\nWe meet the most important idea in R: tidy data. Once data is tidy, there is a great deal of insight to be obtained from it, by way of tables, graphs and explorations!\nWe will get hands on with dplyr, the R-package that belongs in the tidyverse and is a terrific toolbox to clean, transform, reorder, and summarize your data. And we will be ready to ask Questions of our data and embark on anlayzing it."
  },
  {
    "objectID": "content/courses/R4Artists/Modules/50-working-in-tidyverse/tidy.html#readings",
    "href": "content/courses/R4Artists/Modules/50-working-in-tidyverse/tidy.html#readings",
    "title": "Lab-5: Twas brillig, and the slithy toves‚Ä¶",
    "section": "Readings",
    "text": "Readings\n\nR4DS dplyr chapter\nModernDive dplyr chapter\nRStudio dplyr Cheatsheet"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/60-GoG/gog.html",
    "href": "content/courses/R4Artists/Modules/60-GoG/gog.html",
    "title": "Lab-6: These Roses have been Painted !!",
    "section": "",
    "text": "R Tutorial¬†¬†\n  Slides \n\n  Colour in R¬†\nAdvanced Graphics"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/60-GoG/gog.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "href": "content/courses/R4Artists/Modules/60-GoG/gog.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "title": "Lab-6: These Roses have been Painted !!",
    "section": "\n Introduction",
    "text": "Introduction\nAh‚Ä¶ggplot ! All those wonderful pictures and graphs, that Alice might have relished!\nMetaphors, aesthetics, geometries‚Ä¶and pictures !! ggplot seems to equate ravens to writing desks in its syntax‚Ä¶and out come graphs!!\nAnd colours: Wes Anderson! Tim Burton! The Economist‚Ä¶ and many others!!"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/60-GoG/gog.html#fa-asterisk-references",
    "href": "content/courses/R4Artists/Modules/60-GoG/gog.html#fa-asterisk-references",
    "title": "Lab-6: These Roses have been Painted !!",
    "section": "\n References",
    "text": "References\n\nGeorge Lakoff and Mark Johnson, Metaphors We Live By, https://www.youtube.com/watch?v=lYcQcwUfo8c\nWickham and Grolemund, R for Data Science, ggplot chapter: https://r4ds.had.co.nz/data-visualisation.html\nCMDLineTips, 10 Tips to Customize Text Color, Font, Size in ggplot2 with element_text(), https://cmdlinetips.com/2021/05/tips-to-customize-text-color-font-size-in-ggplot2-with-element_text/\nCMDLineTips, How to write a simple custom ggplot theme from scratch, https://cmdlinetips.com/2022/05/how-to-write-a-simple-custom-ggplot-theme-from-scratch/\nAsha Hill @ mode.com, 12 Extensions to ggplot2 for More Powerful R Visualizations, https://mode.com/blog/r-ggplot-extension-packages/\nEmil Hvitfeldt, ggplot Trial and Error, https://www.emilhvitfeldt.com/post/2018-06-12-ggplot2-trial-and-error-us-trade-data/"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/60-GoG/gog.html#fa-icons-fun-stuff",
    "href": "content/courses/R4Artists/Modules/60-GoG/gog.html#fa-icons-fun-stuff",
    "title": "Lab-6: These Roses have been Painted !!",
    "section": "\n Fun Stuff",
    "text": "Fun Stuff\n\nYihan Wu, Mapping ggplot geoms and aesthetic parameters, ( An interactive view of which aesthetic parameters work with which ggplot geom!! ) https://www.yihanwu.ca/post/geoms-and-aesthetic-parameters/\nhttps://www.theartstory.org/artist/kandinsky-wassily/"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/index.html#introduction",
    "href": "content/courses/R4Artists/Modules/70-wizardy/index.html#introduction",
    "title": "Lab-7: The Lobster Quadrille",
    "section": "Introduction",
    "text": "Introduction\nWe will hear at icing and froth to our vanilla ggplots:\n\n\nfonts, annotations, highlights and even pictures!!"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/index.html#references",
    "href": "content/courses/R4Artists/Modules/70-wizardy/index.html#references",
    "title": "Lab-7: The Lobster Quadrille",
    "section": "References",
    "text": "References\n\nThomas Lin Pedersen, https://www.data-imaginist.com/. The creator of ggforce, and patchwork packages.\nClaus Wilke, cowplot ‚Äì Streamlined plot theme and plot annotations for ggplot2, https://wilkelab.org/cowplot/index.html\nClaus Wilke, Spruce up your ggplot2 visualizations with formatted text, https://clauswilke.com/talk/rstudio_conf_2020/. Slides, Code, and Video !\nRobert Kabacoff, ggplot theme cheatsheet, https://rkabacoff.github.io/datavis/modifyingthemes.pdf"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/index.html#fun-stuff",
    "href": "content/courses/R4Artists/Modules/70-wizardy/index.html#fun-stuff",
    "title": "Lab-7: The Lobster Quadrille",
    "section": "Fun Stuff",
    "text": "Fun Stuff\n\nZuguang Gu, Circular Visualization in R,\n\n\n\n\n\n\n\n\n\nhttps://jokergoo.github.io/circlize_book/book/"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/90-GoM/spatial.html",
    "href": "content/courses/R4Artists/Modules/90-GoM/spatial.html",
    "title": "Lab-9: If you please sir‚Ä¶which way to the Secret Garden?",
    "section": "",
    "text": "Alice asks the Catterpillar the Way"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/90-GoM/spatial.html#fa-folder-open-slides-and-tutorials",
    "href": "content/courses/R4Artists/Modules/90-GoM/spatial.html#fa-folder-open-slides-and-tutorials",
    "title": "Lab-9: If you please sir‚Ä¶which way to the Secret Garden?",
    "section": "\n Slides and Tutorials",
    "text": "Slides and Tutorials\n\n\n\n\n\n\n\n\n R Tutorial\n Slides"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/90-GoM/spatial.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "href": "content/courses/R4Artists/Modules/90-GoM/spatial.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "title": "Lab-9: If you please sir‚Ä¶which way to the Secret Garden?",
    "section": "\n Introduction",
    "text": "Introduction\nChoropleth Map\nWhat does choropleth1 mean? And what kind of information could this map represent?1¬†Etymology. From Ancient Greek œáœéœÅŒ± (kh·πìra, ‚Äúlocation‚Äù) + œÄŒª·øÜŒ∏ŒøœÇ (pl√™thos, ‚Äúa great number‚Äù) + English map. First proposed in 1938 by American geographer John Kirtland Wright to mean ‚Äúquantity in area,‚Äù although maps of the type have been used since the early 19th century.\n\n\nBubble Map\nWhat information could this map below represent?\n\n\nWhat is there to not like about maps!!! Let us now look at the slides. Then we will understand how the R packages sf, tmap work to create maps, using data downloadable into R using osmdata, osmplotr and rnaturalearth.\nWe will learn to make static and interactive maps and to show off different kinds of data on them, data that have an inherently ‚Äúspatial‚Äù spread or significance! sf + ggplot and tmap give us great static maps. Interactive maps we will make with leaflet and mapview; tmap is also capable of creating interactive maps.\nTrade Routes? Populations? Street Crime hotspots? Theatre and Food districts and popular Restaurants? Literary Paris, London and Barcelona?\nAll possible !!"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/90-GoM/spatial.html#references",
    "href": "content/courses/R4Artists/Modules/90-GoM/spatial.html#references",
    "title": "Lab-9: If you please sir‚Ä¶which way to the Secret Garden?",
    "section": "References",
    "text": "References\n\nOSM Basic Maps Vignette\nNikita Voevodin, R, Not the Best Practices\nNico Hahn, Making Maps with R\nEmine Fidan, Interactive Maps in R\nLovelace et al, Geocomputation in R"
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/listing.html",
    "href": "content/courses/TRIZ4ProbSolving/listing.html",
    "title": "TRIZ for Innovation",
    "section": "",
    "text": "Title\n\n\nReading Time\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/labs/doe/index.html",
    "href": "content/labs/doe/index.html",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "",
    "text": "This project is a quick analysis of the Design of Experiments class carried out in the Order and Chaos course, FSP-2021-2022, at SMI MAHE, Bangalore.\nThe methodology followed was that in A.J. Lawrance‚Äôs paper 1 describing a Statistics module based on the method of Design of Experiments. The inquiry relates to Short Term Memory (STM) among students.1¬†Lawrance, A. J. 1996. ‚ÄúA Design of Experiments Workshop as an Introduction to Statistics.‚Äù American Statistician 50 (2): 156‚Äì58. doi:10.1080/00031305.1996.10474364."
  },
  {
    "objectID": "content/labs/doe/index.html#structure",
    "href": "content/labs/doe/index.html#structure",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "\n2 Structure",
    "text": "2 Structure\nThe total number of students were 17. Eight Pairs of students were created randomly to create eight different Test tools for Short Term Memory testing.\nThe binary ( two - level ) variables/parameters that were used in the tests, were, following Lawrance:\n\nWL: Word List Length ( 7 and 15 words )\n\nSL: Syllables in the Words ( 2 and 5 syllables )\n\nST: Study Time allowed for the Respondents ( 15 and 30 seconds )\n\nOther parameters considered were a) Language b) Structure/Depiction of the Word Lists ( e.g.¬†word clouds, matrices, columns‚Ä¶), c) Whether the words would be shown or read aloud, and d) whether the respondents had to speak out, or write down, the recollected words. These parameters were discussed and abandoned as too complex to mechanize, though they could have had an impact on the STM scores.\nHence a total of 8 Tests were created by 8 pairs of students, and each team tested the remaining 15 students ( Due to COVID restrictions, this testing was carried out entirely online on MS Teams, using individual breakout rooms for the Test Teams. )\nThe data were entered into a Google Sheet and the STM scores were converted to percentages so as to be comparable across WL.\nThe data was then ‚Äúflattened‚Äù for each of the binary parameters; this was logical to do since for each parameter, the other two parameters were balanced out by the Test structure. For instance, for WL = 5, the SL and ST parameters used all the four combinations ( SL = 5, 15 ) and (ST = 15, 30 ). Hence the ‚Äúcommon sense‚Äù analysis could proceed for each of the parameters individually. Joint effects were not considered for this preliminary class."
  },
  {
    "objectID": "content/labs/doe/index.html#data",
    "href": "content/labs/doe/index.html#data",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "\n3 Data",
    "text": "3 Data\n\n\n\n\n  \n\n\n\nThe data has scores that have been combined into single columns for each setting for each of the parameters. For example, the column syllable_2 contains STM scores for all tests that used SL = 2-syllables in their tests. The Word Length WL and Study Time ST go through all their combinations in this column. The other columns are constructed similarly.\n\n3.1 Basic Plots\nWe will use Box Plots and Density Plots to compare the STM score distributions for each Parameter. To do this we need to pivot_longer the adjacent columns ( e.g.¬†syllable_2 and syllable_5) and use these names as categorical variables:\n\n3.1.1 Syllable Parameter SL\n\n\n\n\n  \n\n\n\n\n\n\n\n3.1.2 Study Time Parameter ST\n\n\n\n\n  \n\n\n\n\n\n\n\n3.1.3 Word List Length Parameter WL"
  },
  {
    "objectID": "content/labs/doe/index.html#preliminary-observations",
    "href": "content/labs/doe/index.html#preliminary-observations",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "\n4 Preliminary Observations",
    "text": "4 Preliminary Observations\nClearly, based on visual inspection of the Plots, the Word Count seems to have a large effect on STM Test Scores, with fewer words ( 7 ) being easier to recall. Study Time ( 15 and 30 seconds ) also seems to have a more modest positive effect on STM scores, while Syllable Count ( 2 or 5 syllables ) seems to have a modest negative effect on STM scores."
  },
  {
    "objectID": "content/labs/doe/index.html#analysis",
    "href": "content/labs/doe/index.html#analysis",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "\n5 Analysis",
    "text": "5 Analysis\nWe wish to establish the significance of the effect size due to each of the Parameters. Already from the Density Plots, we can see that none of the scores are normally distributed. A quick Shapiro-Wilkes Test for each of them confirms that the scores are not normally distributed.\nHence we go for a Permutation Test to check for significance of effect.\nOn the other hand, as remarked in Ernst2, the non-parametric permutation test can be both exact and also intuitively easier for students to grasp, as I can testify from direct observation in this class. There is no need to discuss sampling distributions and means, t-tests and the like. Permutations are easily executed in R, using packages such as mosaic3.2¬†Ernst, Michael D. 2004. ‚ÄúPermutation Methods: A Basis for Exact Inference.‚Äù Statistical Science 19 (4): 676‚Äì85. doi:10.1214/088342304000000396.3¬†Pruim R, Kaplan DT, Horton NJ (2017). ‚ÄúThe mosaic Package: Helping Students to ‚ÄòThink with Data‚Äô Using R.‚Äù The R Journal, 9(1), 77‚Äì102. https://journal.r-project.org/archive/2017/RJ-2017-024/index.html.\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  stm$syllable_2\nW = 0.95508, p-value = 0.02716\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  stm$syllable_5\nW = 0.95321, p-value = 0.02211\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  stm$study_time_15\nW = 0.9068, p-value = 0.0002348\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  stm$study_time_30\nW = 0.95539, p-value = 0.0281\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  stm$list_length_7\nW = 0.90542, p-value = 0.0002085\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  stm$list_length_15\nW = 0.92806, p-value = 0.001645"
  },
  {
    "objectID": "content/labs/doe/index.html#permutation-tests",
    "href": "content/labs/doe/index.html#permutation-tests",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "\n6 Permutation Tests",
    "text": "6 Permutation Tests\nWe proceed with a Permutation Test for each of the Parameters. We start with the Syllable Parameter SL. We shuffle the labels ( SL- = 2 and SL+ = 5) between the scores and determine the null distribution. This is then compared with the difference in mean scores between the unpermuted sets. We continue similarly for the other two parameters.\n\n\n[1] 0.0153731\n\n\n\n\n  \n\n\n\n[1] 0.08526183\n\n\n\n\n  \n\n\n\n[1] 0.2887539"
  },
  {
    "objectID": "content/labs/doe/index.html#conclusions",
    "href": "content/labs/doe/index.html#conclusions",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "\n7 Conclusions",
    "text": "7 Conclusions\nFrom the above null distribution plots obtained using Permutation tests, it is clear that both Study Time ( ST ) and List Word Length ( WL) have significant effects on the Short Term Memory Scores. The probability that the observed value is obtained or exceeded by any permutation of scores is very low in both cases.\nOn the other hand, Syllable Count (SL) does not seem to affect the STM scores significantly."
  },
  {
    "objectID": "content/labs/doe/index.html#references",
    "href": "content/labs/doe/index.html#references",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "\n8 References",
    "text": "8 References"
  },
  {
    "objectID": "content/labs/r-labs/dashboard/example.html",
    "href": "content/labs/r-labs/dashboard/example.html",
    "title": "Critical Result Callback Monitor",
    "section": "",
    "text": "mean_tat <- round(mean(cb_data$call_tat), 0)\nvalueBox(value = mean_tat, icon = \"fa-stopwatch\", \n         caption = \"Mean callback time\", color = \"#708090\")\n\n15\n\n\n\n\ncalls <- cb_data %>% nrow()\nvalueBox(value = calls, icon = \"fa-hashtag\", \n         caption = \"Total calls\", color = \"orange\")\n\n2167\n\n\n\n\nontime_n <- cb_data %>% filter(call_tat <= 30) %>% nrow()\nlate_n <- cb_data %>% filter(call_tat >30) %>% nrow()\npct_ontime <- round((ontime_n/calls)*100,0)\n\n\nif(pct_ontime >= 95){\n  valueBox(value = pct_ontime, icon = \"fa-percent\", \n           caption = \"Percent on time\", color = \"mediumseagreen\") #3CB371\n} else{\nvalueBox(value = pct_ontime, icon = \"fa-percent\", \n         caption = \"Percent on time\", color = \"indianred\")} #CD5C5C\n\n89\n\n\n\n\n\np1 <- ggplot(filter(cb_data, \n              call_wday %in% c(\"Mon\", \"Tue\", \"Wed\", \n                               \"Thu\", \"Fri\"))) +\n  geom_bar(aes(x = call_hour, fill = tech_location)) + \n  facet_grid(call_wday~call_week) +\n  labs(x = \"Hour\", y = \"Count\", fill = \"Type\") +\n  scale_fill_manual(values=c(\"steelblue3\", \"gray60\")) +\n  theme_bw() +\n  theme(legend.position = \"top\")\n\nggplotly(p1) %>% layout(legend = list(orientation = \"h\", \n                                      x = 0.35, y = 1.2)) \n\n\n\n\n# have to manually position the legend\n\n\n\ndaily_vol <- cb_data %>% \n              group_by(call_year, call_month, call_date) %>%\n              summarize(n = n()) %>%\n              ungroup() %>%\n              unite(dttm, call_year, call_month, call_date, \n                    sep = \"-\", remove = TRUE) %>%\n              mutate(dttm = as.Date(dttm))\n\n`summarise()` has grouped output by 'call_year', 'call_month'. You can override\nusing the `.groups` argument.\n\n#dygraph requires ts object\ndaily_vol_ts <- xts(daily_vol$n, order.by = daily_vol$dttm) \n\np2 <- dygraph(daily_vol_ts) %>% \n  dyOptions(colors = \"#4F94CD\", strokeWidth = 2) \np2"
  },
  {
    "objectID": "content/labs/r-labs/dashboard/example.html#row-2",
    "href": "content/labs/r-labs/dashboard/example.html#row-2",
    "title": "Critical Result Callback Monitor",
    "section": "\n2.1 Row",
    "text": "2.1 Row\n\n2.1.1 Mean callback time\n\ncc_calls <- cb_data %>% filter(tech_location == \"CallCenter\")\nmean_tat_cc <- round(mean(cc_calls$call_tat),0)\n \nvalueBox(value = mean_tat_cc, icon = \"fa-stopwatch\", \n         caption = \"Mean callback time\", color = \"#708090\")\n\n10\n\n\n\n2.1.2 Total calls\n\ncc_calls_n <- cb_data %>% filter(tech_location == \"CallCenter\") %>% nrow()\nvalueBox(value = cc_calls_n, icon = \"fa-hashtag\", \n         caption = \"Total calls\", color = \"orange\")\n\n864\n\n\n\n2.1.3 Percent on Time\n\ncc_ontime_n <- cb_data %>% \n  filter(tech_location == \"CallCenter\", call_tat <= 30) %>% nrow()\ncc_late_n <- cb_data %>% \n  filter(tech_location == \"CallCenter\", call_tat >30) %>% nrow()\ncc_pct_ontime <- round((cc_ontime_n/cc_calls_n)*100,0)\n\n\nif(cc_pct_ontime >= 95){\n  valueBox(value = cc_pct_ontime, icon = \"fa-percent\", \n           caption = \"Percent on time\", color = \"mediumseagreen\") #3CB371\n} else{\nvalueBox(value = cc_pct_ontime, icon = \"fa-percent\", \n         caption = \"Percent on time\", color = \"indianred\")} #CD5C5C\n\n99"
  },
  {
    "objectID": "content/labs/r-labs/dashboard/example.html#row-3",
    "href": "content/labs/r-labs/dashboard/example.html#row-3",
    "title": "Critical Result Callback Monitor",
    "section": "\n2.2 Row",
    "text": "2.2 Row\n\n2.2.1 Overdue Call Details for 2020-07-01 through 2020-07-31\n\ncc_late <- cb_data %>% \n  filter(tech_location == \"CallCenter\", call_tat >30) %>%\n  select(call_tat, accession, pt_type, pt_loc_code, test_code,\n         result_datetime, phoned_title, tech)\n\ndatatable(cc_late, options = list(pageLength = 20, autoWidth = TRUE))\n\n\n\n\n\n\n\n2.2.2 Tech Summary for 2020-07-01 through 2020-07-31\n\np3 <- hcboxplot(\n  outliers = FALSE,\n  x = cc_calls$call_tat,\n  var = cc_calls$tech,\n  name = \"TAT\", \n  color = \"#4F94CD\",\n  lineWidth = 2) %>%\n  hc_title(text = \"\") %>%\n  hc_xAxis(title = list(text = \"Tech Code\")) %>%\n  hc_yAxis(title = list(text = \"Call Time in Min\")) %>%\n  hc_chart(type = \"column\")\n\nWarning: 'hcboxplot' is deprecated.\nUse 'data_to_boxplot' instead.\nSee help(\"Deprecated\")\n\np3"
  },
  {
    "objectID": "content/labs/r-labs/dashboard/example.html#row-4",
    "href": "content/labs/r-labs/dashboard/example.html#row-4",
    "title": "Critical Result Callback Monitor",
    "section": "\n3.1 Row",
    "text": "3.1 Row\n\n3.1.1 Mean callback time\n\nnon_calls <- cb_data %>% filter(tech_location == \"Non-CC\")\nmean_tat_non <- round(mean(non_calls$call_tat),0)\n \nvalueBox(value = mean_tat_non, icon = \"fa-stopwatch\",\n         caption = \"Mean callback time\", color = \"#708090\")\n\n18\n\n\n\n3.1.2 Total calls\n\nnon_calls_n <- cb_data %>% filter(tech_location == \"Non-CC\") %>% nrow()\nvalueBox(value = non_calls_n, icon = \"fa-hashtag\",\n         caption = \"Total calls\", color = \"orange\")\n\n1303\n\n\n\n3.1.3 Percent on Time\n\nnon_ontime_n <- cb_data %>% \n  filter(tech_location == \"Non-CC\", call_tat <= 30) %>% nrow()\nnon_late_n <- cb_data %>% \n  filter(tech_location == \"Non-CC\", call_tat >30) %>% nrow()\nnon_pct_ontime <- round((non_ontime_n/non_calls_n)*100,0)\n\n\nif(non_pct_ontime >= 95){\n  valueBox(value = non_pct_ontime, icon = \"fa-percent\", \n           caption = \"Percent on time\", color = \"mediumseagreen\") #3CB371\n} else{\nvalueBox(value = non_pct_ontime, icon = \"fa-percent\", \n         caption = \"Percent on time\", color = \"indianred\")} #CD5C5C\n\n82"
  },
  {
    "objectID": "content/labs/r-labs/dashboard/example.html#row-5",
    "href": "content/labs/r-labs/dashboard/example.html#row-5",
    "title": "Critical Result Callback Monitor",
    "section": "\n3.2 Row",
    "text": "3.2 Row\n\n3.2.1 Overdue Call Details for 2020-07-01 through 2020-07-31\n\nnon_late <- cb_data %>% \n  filter(tech_location == \"Non-CC\", call_tat >30) %>%\n  select(call_tat, accession, pt_type, pt_loc_code, test_code,\n         result_datetime, phoned_title, tech)\n\ndatatable(non_late, options = list(pageLength = 20, autoWidth = TRUE))\n\n\n\n\n\n\n\n3.2.2 Tech Summary for 2020-07-01 through 2020-07-31\n\np4 <- hcboxplot(\n  outliers = FALSE,\n  x = non_calls$call_tat,\n  var = non_calls$tech,\n  name = \"TAT\", \n  color = \"#4F94CD\",\n  lineWidth = 2) %>%\n  hc_title(text = \"\") %>%\n  hc_xAxis(title = list(text = \"Tech Code\")) %>%\n  hc_yAxis(title = list(text = \"Call Time in Min\")) %>%\n  hc_chart(type = \"column\")\n\nWarning: 'hcboxplot' is deprecated.\nUse 'data_to_boxplot' instead.\nSee help(\"Deprecated\")\n\np4"
  },
  {
    "objectID": "content/labs/r-labs/dashboard/flexdashboard.html",
    "href": "content/labs/r-labs/dashboard/flexdashboard.html",
    "title": "ggplotly: various examples",
    "section": "",
    "text": "# This example modifies code from Hadley Wickham (https://gist.github.com/hadley/233134)\n# It also uses data from Nathan Yau's flowingdata site (http://flowingdata.com/)\nunemp <- read.csv(\"http://datasets.flowingdata.com/unemployment09.csv\")\nnames(unemp) <- c(\"id\", \"state_fips\", \"county_fips\", \"name\", \"year\", \n                  \"?\", \"?\", \"?\", \"rate\")\nunemp$county <- tolower(gsub(\" County, [A-Z]{2}\", \"\", unemp$name))\nunemp$state <- gsub(\"^.*([A-Z]{2}).*$\", \"\\\\1\", unemp$name)\ncounty_df <- map_data(\"county\")\nnames(county_df) <- c(\"long\", \"lat\", \"group\", \"order\", \"state_name\", \"county\")\ncounty_df$state <- state.abb[match(county_df$state_name, tolower(state.name))]\ncounty_df$state_name <- NULL\nstate_df <- map_data(\"state\")\nchoropleth <- merge(county_df, unemp, by = c(\"state\", \"county\"))\nchoropleth <- choropleth[order(choropleth$order), ]\nchoropleth$rate_d <- cut(choropleth$rate, breaks = c(seq(0, 10, by = 2), 35))\n\n# provide a custom tooltip to plotly with the county name and actual rate\nchoropleth$text <- with(choropleth, paste0(\"County: \", name, \"Rate: \", rate))\np <- ggplot(choropleth, aes(long, lat, group = group)) +\n  geom_polygon(aes(fill = rate_d, text = text), \n               colour = alpha(\"white\", 1/2), size = 0.2) + \n  geom_polygon(data = state_df, colour = \"white\", fill = NA) +\n  scale_fill_brewer(palette = \"PuRd\") + theme_void()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\n\nWarning in geom_polygon(aes(fill = rate_d, text = text), colour =\nalpha(\"white\", : Ignoring unknown aesthetics: text\n\n# just show the text aesthetic in the tooltip\nggplotly(p, tooltip = \"text\")\n\n\n\n\n\n\n\n\ncrimes <- data.frame(state = tolower(rownames(USArrests)), USArrests)\ncrimesm <- tidyr::gather(crimes, variable, value, -state)\nstates_map <- map_data(\"state\")\ng <- ggplot(crimesm, aes(map_id = state)) +\n  geom_map(aes(fill = value), map = states_map) +\n  expand_limits(x = states_map$long, y = states_map$lat) +\n  facet_wrap( ~ variable) + theme_void()\nggplotly(g)"
  },
  {
    "objectID": "content/labs/r-labs/dashboard/flexdashboard.html#row-1",
    "href": "content/labs/r-labs/dashboard/flexdashboard.html#row-1",
    "title": "ggplotly: various examples",
    "section": "\n2 Row",
    "text": "2 Row\n\n2.1 Faithful Eruptions\n\nm <- ggplot(faithful, aes(x = eruptions, y = waiting)) +\n  stat_density_2d() + xlim(0.5, 6) + ylim(40, 110)\nggplotly(m)\n\n\n\n\n\n\n2.2 Faithful Eruptions (polygon)\n\nm <- ggplot(faithful, aes(x = eruptions, y = waiting)) +\n  stat_density_2d(aes(fill = ..level..), geom = \"polygon\") + \n  xlim(0.5, 6) + ylim(40, 110)\nggplotly(m)\n\nWarning: The dot-dot notation (`..level..`) was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `after_stat(level)` instead.\n‚Ñπ The deprecated feature was likely used in the ggplot2 package.\n  Please report the issue at <https://github.com/tidyverse/ggplot2/issues>.\n\n\n\n\n\n\n\n2.3 Faithful Eruptions (hex)\n\nm <- ggplot(faithful, aes(x = eruptions, y = waiting)) + geom_hex() \nggplotly(m)"
  },
  {
    "objectID": "content/labs/r-labs/diagrams/index.html",
    "href": "content/labs/r-labs/diagrams/index.html",
    "title": "Lab 01 - Introduce Yourself",
    "section": "",
    "text": "At the end of this Lab, we will:\n\nhave installed R and RStudio on our machines\nunderstood how to add additional R-packages for specific features and graphic capability\nrun code within RStudio and interpret the results\nhave learnt to look for help within R and RStudio"
  },
  {
    "objectID": "content/labs/r-labs/diagrams/index.html#Check-In-R",
    "href": "content/labs/r-labs/diagrams/index.html#Check-In-R",
    "title": "Lab 01 - Introduce Yourself",
    "section": "\n3.1 Check in",
    "text": "3.1 Check in\nLaunch R by clicking this logo. You should see one console with a command line interpreter. Try typing 2 + 2 and check !\nClose R."
  },
  {
    "objectID": "content/labs/r-labs/diagrams/index.html#Check-In-RStudio",
    "href": "content/labs/r-labs/diagrams/index.html#Check-In-RStudio",
    "title": "Lab 01 - Introduce Yourself",
    "section": "\n4.1 Check in",
    "text": "4.1 Check in\nLaunch RStudio. You should get a window similar to the screenshot you see here, but yours will be empty. Look at the bottom left pane: this is the same console window you saw when you opened R in step @Check-In-R\n\nPlace your cursor where you see > and type x <- 2 + 2 again hit enter or return, then type x, and hit enter/return again.\nIf [1] 4 prints to the screen, you have successfully installed R and RStudio, and you can move onto installing packages."
  },
  {
    "objectID": "content/labs/r-labs/diagrams/index.html#save-and-share",
    "href": "content/labs/r-labs/diagrams/index.html#save-and-share",
    "title": "Lab 01 - Introduce Yourself",
    "section": "\n7.1 Save and share",
    "text": "7.1 Save and share\nSave your work so you can share your favorite plot with us. You will not like the looks of your plot if you mouse over to Export and save it. Instead, use ggplot2‚Äôs command for saving a plot with sensible defaults:\n\nShow the Codehelp(ggsave)\n\n\n\nShow the Codeggsave(\"file_name_here.pdf\", plot) # please make the filename unique!\n\n\nUpload this exported plot to Teams -> Assignments.\nBefore you do that, check that you can submit stuff/assignments on the MLS by uploading a cat picture üêà first, especially if you are an ailurophile like me. (Acceptable Breeds: Scottish Folds or Maine Coons)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html",
    "href": "content/labs/r-labs/graphics/colors.html",
    "title": "Lab 05: Colors with Penguins",
    "section": "",
    "text": "knitr::opts_chunk$set(error = TRUE, comment = NA, warning = FALSE, errors = FALSE, message = FALSE, tidy = FALSE, cache = FALSE, fig.path= \"03-figs/\")\n\nlibrary(tidyverse) # Manage data\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.0     ‚úî readr     2.1.4\n‚úî forcats   1.0.0     ‚úî stringr   1.5.0\n‚úî ggplot2   3.4.1     ‚úî tibble    3.2.0\n‚úî lubridate 1.9.2     ‚úî tidyr     1.3.0\n‚úî purrr     1.0.1     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(scales) # Create special ( % or $ ) scales\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n#\nlibrary(palmerpenguins) # source of our data\n#\nlibrary(RColorBrewer) # Colour Palettes\nlibrary(wesanderson) # Colour Palettes\n#library(gameofthrones) # You all know this!\n#\nlibrary(paletteer) # Colour Palettes\nlibrary(colorspace) # Colour Palettes\n#\nlibrary(patchwork) # arranges plots on Row-Col \nlibrary(ggthemes)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "href": "content/labs/r-labs/graphics/colors.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n2  Introduction",
    "text": "2  Introduction\nThis Quarto document is part of my Workshop in R. The material is based on A Layered Grammar of Graphics by Hadley Wickham. The course is meant for First Year students pursuing a Degree in Art and Design. The intent is to build Skill in coding in R, and also appreciate R as a way to metaphorically visualize information of various kinds, using predominantly geometric figures and structures.\nAll Quarto files combine code, text, web-images, and figures developed using code. Everything is text; code chunks are enclosed in fences (```)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#goals",
    "href": "content/labs/r-labs/graphics/colors.html#goals",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n3 Goals",
    "text": "3 Goals\n\n(Re)Understand different kinds of data variables\nAppreciate how they can be identified based on the Interrogative Pronouns they answer to\nUnderstand how each kind of variable lends itself to a specific choice of colour scale in the data visualization."
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#pedagogical-note",
    "href": "content/labs/r-labs/graphics/colors.html#pedagogical-note",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n4 Pedagogical Note",
    "text": "4 Pedagogical Note\nThe method followed will be based on PRIMM:\n\n\nPREDICT Inspect the code and guess at what the code might do, write predictions\n\n\nRUN the code provided and check what happens\n\nINFER what the parameters of the code do and write comments to explain. What bells and whistles can you see?\n\nMODIFY the parameters code provided to understand the options available. Write comments to show what you have aimed for and achieved.\n\nMAKE : take an idea/concept of your own, and graph it.\n\nIn the following, there is some boiler plate code demonstrating the use of colour palettes in R. There are places where YOUR TURN is mention; copy and play with the boiler plate code to see what happens !"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#data",
    "href": "content/labs/r-labs/graphics/colors.html#data",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n5 Data",
    "text": "5 Data\nWe will use the penguins dataset built into the palmerpenguins package. Your should try other datasets too!\nHere is a glimpse of the data:\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel‚Ä¶\n$ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen, Torgerse‚Ä¶\n$ bill_length_mm    <dbl> 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, ‚Ä¶\n$ bill_depth_mm     <dbl> 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, ‚Ä¶\n$ flipper_length_mm <int> 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186‚Ä¶\n$ body_mass_g       <int> 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, ‚Ä¶\n$ sex               <fct> male, female, female, NA, female, male, female, male‚Ä¶\n$ year              <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007‚Ä¶\n\n\nNote that the unit of observation here is one-row-per-penguin.\nVariables you need for this lab:"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#colour-vs-fill-aesthetic",
    "href": "content/labs/r-labs/graphics/colors.html#colour-vs-fill-aesthetic",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n6 Colour vs fill aesthetic",
    "text": "6 Colour vs fill aesthetic\nFill and colour scales in ggplot2 can use the same palettes. Some shapes such as lines only accept the colour aesthetic, while others, such as polygons, accept both colour and fill aesthetics. In the latter case, the colour refers to the border of the shape, and the fill to the interior.\n\n## A look at all 25 symbols\ndf <- data.frame(x = 1:5,\n                 y = rep(rev(seq(0, 24, by = 5)), each = 5),\n                 z = 1:25)\ns <- ggplot(df, aes(x = x, y = y)) +\n  geom_text(aes(label = z, y = y - 1)) +\n  theme_void()\ns + geom_point(aes(shape = z), size = 4) + scale_shape_identity()\n\n\n\n\nAll symbols have a foreground colour, so if we add color = \"navy\", they all are affected.\n\ns + geom_point(aes(shape = z), size = 4, colour = \"blue\")  + scale_shape_identity()\n\n\n\n\nWhile all symbols have a foreground colour, symbols 21-25 also take a background colour (fill). So if we add fill = \"orchid\", only the last row of symbols are affected.\n\ns + geom_point(aes(shape = z), size = 4, colour = \"blue\", fill = \"orchid\")  + scale_shape_identity()"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#discrete-vs-continuous-variables",
    "href": "content/labs/r-labs/graphics/colors.html#discrete-vs-continuous-variables",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n7 Discrete vs continuous variables",
    "text": "7 Discrete vs continuous variables\nWHAT IS THE DIFFERENCE BETWEEN CATEGORICAL, ORDINAL AND INTERVAL VARIABLES?\nIn order to use color with your data, most importantly, you need to know if you‚Äôre dealing with discrete or continuous variables."
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#some-colour-palette-packages-in-r",
    "href": "content/labs/r-labs/graphics/colors.html#some-colour-palette-packages-in-r",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n8 Some Colour Palette Packages in R",
    "text": "8 Some Colour Palette Packages in R\nWe have the following example packages that offer palettes in R:\n\nRColorBrewer\nwesanderson\npaletteer\ncolorspace\n\nSee Appendix for a detailed graphical analysis of these palette packages."
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#colour-palette-types",
    "href": "content/labs/r-labs/graphics/colors.html#colour-palette-types",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n9 Colour Palette Types",
    "text": "9 Colour Palette Types\nThese palettes can be:\n\nSequential (type = ‚Äúseq‚Äù) palettes are suited to ordered data that progress from low to high. Lightness steps dominate the look of these schemes, with light colors for low data values to dark colors for high data values. (for numerical data, that are ordered)\n\n\nDiverging (type = ‚Äúdiv‚Äù) palettes put equal emphasis on mid-range critical values and extremes at both ends of the data range. The critical class or break in the middle of the legend is emphasized with light colors and low and high extremes are emphasized with dark colors that have contrasting hues.(for numerical data that can be positive or negative, often representing deviations from some norm or baseline)\n\n\nQualitative (type = ‚Äúqual‚Äù) palettes do not imply magnitude differences between legend classes, and hues are used to create the primary visual differences between classes. Qualitative schemes are best suited to representing nominal or categorical data. (for qualitative unordered data)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#create-a-simple-set-of-scatter-plots",
    "href": "content/labs/r-labs/graphics/colors.html#create-a-simple-set-of-scatter-plots",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n10 Create a simple set of scatter plots",
    "text": "10 Create a simple set of scatter plots\nWe will create simple base plots in ggplot and see how we may alter the colour scales using palettes.\n\nnames(penguins)\n\n[1] \"species\"           \"island\"            \"bill_length_mm\"   \n[4] \"bill_depth_mm\"     \"flipper_length_mm\" \"body_mass_g\"      \n[7] \"sex\"               \"year\"             \n\n\n\np1 <- penguins %>% \n  drop_na() %>% \n  # pipe data into ggplot\n  # after removing data rows that have missing ( NA ) values\n  ggplot(aes(y = body_mass_g, x = flipper_length_mm, \n           color = species # COLOUR = DISCRETE/QUAL VARIABLE\n           )) +\n           geom_point() + \n           labs(title = \"Default Colours in ggplot\", \n                subtitle = \"P1: DISCRETE/QUAL Colour Palette\")\n\n\np2 <- \npenguins %>% \n  drop_na() %>% \n  # pipe the data into ggplot, \n  # after removing data rows that have missing ( NA ) values\n  ggplot(aes(y = body_mass_g, x = flipper_length_mm, \n           color = bill_length_mm # COLOUR = CONT/QUANT VARIABLE\n           )) +\n           geom_point() + \n           labs(title = \"Default Colours in ggplot\", \n                subtitle = \"P2: CONTINUOUS/QUANT Colour Palette\")\n\np1\n\n\n\np2\n\n\n\n\nNote that these use the default colours in R."
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#colours-for-discrete-qual-variables",
    "href": "content/labs/r-labs/graphics/colors.html#colours-for-discrete-qual-variables",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n11 Colours for Discrete (QUAL) Variables",
    "text": "11 Colours for Discrete (QUAL) Variables\nThe commands below are used to fill colours based on Qualitative Variables:\n\nscale_colour/fill_discrete\n\nscale_colour/fill_brewer # RColorBrewer\n‚Ä¶.\n\nNow to use these!"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#plotting-colours-based-on-discrete-variables",
    "href": "content/labs/r-labs/graphics/colors.html#plotting-colours-based-on-discrete-variables",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n12 Plotting Colours based on Discrete Variables",
    "text": "12 Plotting Colours based on Discrete Variables"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#discrete-n-colour-palettes-from-rcolorbrewer",
    "href": "content/labs/r-labs/graphics/colors.html#discrete-n-colour-palettes-from-rcolorbrewer",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n13 Discrete n-Colour palettes from RColorBrewer\n",
    "text": "13 Discrete n-Colour palettes from RColorBrewer\n\n\nRColorBrewer::brewer.pal.info\n\n\n\n  \n\n\nRColorBrewer::display.brewer.all()\n\n\n\n\n\np1 +\n  # default palette = \"Blues\"\n  scale_colour_brewer() +\n  labs(title = \"Brewer Palette = Blues\")\n\n\n\np1 +\n  scale_color_brewer(palette = \"Spectral\") +\n  labs(title = \"Brewer Palette = Spectral\")"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#discrete-colour-scales-using-wesanderson-palettes",
    "href": "content/labs/r-labs/graphics/colors.html#discrete-colour-scales-using-wesanderson-palettes",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n14 Discrete Colour scales using wesanderson palettes",
    "text": "14 Discrete Colour scales using wesanderson palettes\n\nwesanderson::wes_palettes %>% names()\n\n [1] \"BottleRocket1\"  \"BottleRocket2\"  \"Rushmore1\"      \"Rushmore\"      \n [5] \"Royal1\"         \"Royal2\"         \"Zissou1\"        \"Darjeeling1\"   \n [9] \"Darjeeling2\"    \"Chevalier1\"     \"FantasticFox1\"  \"Moonrise1\"     \n[13] \"Moonrise2\"      \"Moonrise3\"      \"Cavalcanti1\"    \"GrandBudapest1\"\n[17] \"GrandBudapest2\" \"IsleofDogs1\"    \"IsleofDogs2\"   \n\n\n\np1 +\n  scale_colour_discrete(type = wes_palette(name = \"GrandBudapest1\",\n                                           n = 3)) +\n  labs(title = \"Wes Anderson Palette: GrandBudapest\")\n\n\n\n# We can also specify colour codes ourselves with scale_x_discrete.\n# Use argument \"values\" instead of \"type\"\nmanual_colours <- c(\"#afc4b8\", \"#f1a4b2\", \"#ffb1e1\") \nmanual_colours\n\n[1] \"#afc4b8\" \"#f1a4b2\" \"#ffb1e1\"\n\np1 +\n  scale_colour_manual(values =  manual_colours) +\n  labs(title = \"Manual Colours\")"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#discrete-n-colour-palettes-from-rcolorbrewer-1",
    "href": "content/labs/r-labs/graphics/colors.html#discrete-n-colour-palettes-from-rcolorbrewer-1",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n15 Discrete n-Colour palettes from RColorBrewer\n",
    "text": "15 Discrete n-Colour palettes from RColorBrewer\n\n\n# scale_x_brewer() for DISCRETE data\np1 +\n  scale_colour_brewer(palette = \"Spectral\") +\n  \n  labs(title = \"RColorBrewer Palette = Spectral\")"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#discrete-colour-scales-using-paletteer-palettes",
    "href": "content/labs/r-labs/graphics/colors.html#discrete-colour-scales-using-paletteer-palettes",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n16 Discrete Colour scales using paletteer palettes",
    "text": "16 Discrete Colour scales using paletteer palettes\n\npalettes_d_names\n\n\n\n  \n\n\npalettes_dynamic_names\n\n\n\n  \n\n\npaletteer_d(\"dutchmasters::pearl_earring\")\n\n<colors>\n#A65141FF #E7CDC2FF #80A0C7FF #394165FF #FCF9F0FF #B1934AFF #DCA258FF #100F14FF #8B9DAFFF #EEDA9DFF #E8DCCFFF \n\npaletteer_dynamic(\"ggthemes_ptol::qualitative\", n = 3)\n\n<colors>\n#4477AAFF #DDCC77FF #CC6677FF \n\np1 +\n  scale_colour_paletteer_d(\"ggthemes_ptol::qualitative\", \n                           dynamic = TRUE) +\n  \n  labs(title = \"ggThemes Palette: Qualitative\", \n          subtitle = \"\")\n\n\n\n# I like Vermeer's \"Girl with the Pearl Earring\"!\np1 +\n  scale_colour_paletteer_d(\"dutchmasters::pearl_earring\",\n                           dynamic = FALSE) +\n  \n  labs(title = \"Palettes from `paletteer`\", \n          subtitle = \" Palette from Vermeer: Girl with Pearl Earring\")"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#colours-for-continuous-quant-variables",
    "href": "content/labs/r-labs/graphics/colors.html#colours-for-continuous-quant-variables",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n17 Colours for Continuous (QUANT) Variables",
    "text": "17 Colours for Continuous (QUANT) Variables\nThe commands below are used to fill colours based on Quantitative Variables:\n\n\nscale_colour/fill_gradient (Two colour gradient)\n\nscale_colour/fill_gradient2 (Three colour gradient)\n\nscale_colour/fill_gradientn (Specify Palette, from other packages also, like wesanderson )\n\nscale_colour/fill_distiller (Palettes from RColorBrewer)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#plotting-colours-based-on-continuous-variables",
    "href": "content/labs/r-labs/graphics/colors.html#plotting-colours-based-on-continuous-variables",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n18 Plotting Colours based on Continuous Variables",
    "text": "18 Plotting Colours based on Continuous Variables"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#continuous-two-colour-gradients",
    "href": "content/labs/r-labs/graphics/colors.html#continuous-two-colour-gradients",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n19 Continuous Two Colour Gradients",
    "text": "19 Continuous Two Colour Gradients\nCreates a pallete containing continuous shades between two colours:\n\np2 +\n    scale_color_gradient(\n      low = \"yellow\", # Play with this in the chunk below\n      high = \"purple\") + # Play with this in the chnk below\n  \n  labs(title = \"Two Colour Gradients\",\n          subtitle = \"P2: Continuous 2-Colour Pallete\")"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#continuous-three-colour-gradients",
    "href": "content/labs/r-labs/graphics/colors.html#continuous-three-colour-gradients",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n20 Continuous Three Colour Gradients",
    "text": "20 Continuous Three Colour Gradients\nSometimes we want a palette this way: a midpoint colour, and colours for the two extremes of a continuous variable:\n\ncolour_midpoint <- mean(penguins$bill_length_mm, \n                         na.rm = TRUE) # remove missing values\n# Struggled all morning on 22 Aug 2020 to get at this ;-D\n\n# Play with the function: 0/mean/median/mode/max/min\n\np2 +\n  scale_colour_gradient2(\n  low = \"brown\", # Play with this in the chunk below\n  mid = \"white\", # Play with this in the chunk below\n  high = \"purple\", # Play with this in the chunk below\n  midpoint = colour_midpoint, # see above\n  space = \"Lab\", # don't mess with this!\n  na.value = \"grey50\")  +\n  labs(title = \"Three colour continuous gradient\", \n          subtitle = \"Mid Colour mapped to midpoint of data variable\",\n          caption = \"Colours inspired by my favourite cocker spaniel, Lord Chestnut\") # Play with these"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#continuous-n-colour-gradients---grdevices-package",
    "href": "content/labs/r-labs/graphics/colors.html#continuous-n-colour-gradients---grdevices-package",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n21 Continuous n-Colour Gradients - grDevices package",
    "text": "21 Continuous n-Colour Gradients - grDevices package\n\n# grDevices Palettes\np2 +\n  scale_colour_gradientn(\n    colours = terrain.colors(10)) +\n  # Try these:\n  # heat.colors() / topo.colors() / cm.colors() / rainbow()\n  \n  labs(title = \"N-colour continuous gradients\", \n          subtitle = \"Palettes from grDevices\",\n          caption = \"Palette: terrain.colors\")"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#continuous-n-colour-gradients---wesanderson-palettes",
    "href": "content/labs/r-labs/graphics/colors.html#continuous-n-colour-gradients---wesanderson-palettes",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n22 Continuous n-Colour Gradients - wesanderson Palettes",
    "text": "22 Continuous n-Colour Gradients - wesanderson Palettes\n\nwes_palettes\n\n$BottleRocket1\n[1] \"#A42820\" \"#5F5647\" \"#9B110E\" \"#3F5151\" \"#4E2A1E\" \"#550307\" \"#0C1707\"\n\n$BottleRocket2\n[1] \"#FAD510\" \"#CB2314\" \"#273046\" \"#354823\" \"#1E1E1E\"\n\n$Rushmore1\n[1] \"#E1BD6D\" \"#EABE94\" \"#0B775E\" \"#35274A\" \"#F2300F\"\n\n$Rushmore\n[1] \"#E1BD6D\" \"#EABE94\" \"#0B775E\" \"#35274A\" \"#F2300F\"\n\n$Royal1\n[1] \"#899DA4\" \"#C93312\" \"#FAEFD1\" \"#DC863B\"\n\n$Royal2\n[1] \"#9A8822\" \"#F5CDB4\" \"#F8AFA8\" \"#FDDDA0\" \"#74A089\"\n\n$Zissou1\n[1] \"#3B9AB2\" \"#78B7C5\" \"#EBCC2A\" \"#E1AF00\" \"#F21A00\"\n\n$Darjeeling1\n[1] \"#FF0000\" \"#00A08A\" \"#F2AD00\" \"#F98400\" \"#5BBCD6\"\n\n$Darjeeling2\n[1] \"#ECCBAE\" \"#046C9A\" \"#D69C4E\" \"#ABDDDE\" \"#000000\"\n\n$Chevalier1\n[1] \"#446455\" \"#FDD262\" \"#D3DDDC\" \"#C7B19C\"\n\n$FantasticFox1\n[1] \"#DD8D29\" \"#E2D200\" \"#46ACC8\" \"#E58601\" \"#B40F20\"\n\n$Moonrise1\n[1] \"#F3DF6C\" \"#CEAB07\" \"#D5D5D3\" \"#24281A\"\n\n$Moonrise2\n[1] \"#798E87\" \"#C27D38\" \"#CCC591\" \"#29211F\"\n\n$Moonrise3\n[1] \"#85D4E3\" \"#F4B5BD\" \"#9C964A\" \"#CDC08C\" \"#FAD77B\"\n\n$Cavalcanti1\n[1] \"#D8B70A\" \"#02401B\" \"#A2A475\" \"#81A88D\" \"#972D15\"\n\n$GrandBudapest1\n[1] \"#F1BB7B\" \"#FD6467\" \"#5B1A18\" \"#D67236\"\n\n$GrandBudapest2\n[1] \"#E6A0C4\" \"#C6CDF7\" \"#D8A499\" \"#7294D4\"\n\n$IsleofDogs1\n[1] \"#9986A5\" \"#79402E\" \"#CCBA72\" \"#0F0D0E\" \"#D9D0D3\" \"#8D8680\"\n\n$IsleofDogs2\n[1] \"#EAD3BF\" \"#AA9486\" \"#B6854D\" \"#39312F\" \"#1C1718\"\n\nnames(wes_palettes)\n\n [1] \"BottleRocket1\"  \"BottleRocket2\"  \"Rushmore1\"      \"Rushmore\"      \n [5] \"Royal1\"         \"Royal2\"         \"Zissou1\"        \"Darjeeling1\"   \n [9] \"Darjeeling2\"    \"Chevalier1\"     \"FantasticFox1\"  \"Moonrise1\"     \n[13] \"Moonrise2\"      \"Moonrise3\"      \"Cavalcanti1\"    \"GrandBudapest1\"\n[17] \"GrandBudapest2\" \"IsleofDogs1\"    \"IsleofDogs2\"   \n\n\n\np2 +\n    scale_colour_gradientn(\n      colors = wes_palette(name = \"GrandBudapest1\", \n                           n = 4), # Keep an eye on \"n\".\n      na.value = \"grey\") +\n  # Try these:\n  # \"BottleRocket1\"  \"BottleRocket2\"  \"Rushmore1\"\n  # \"Rushmore\"       \"Royal1\"         \"Royal2\"\n  # \"Zissou1\"        \"Darjeeling1\"    \"Darjeeling2\"   \n  # \"Chevalier1\"     \"FantasticFox1\"  \"Moonrise1\"     \n  # \"Moonrise2\"      \"Moonrise3\"      \"Cavalcanti1\"   \n  # \"GrandBudapest1\" \"GrandBudapest2\" \"IsleofDogs1\"   \n  # \"IsleofDogs2\"   \n  # Keep an eye on \"n\".\n  \n  labs(title = \"N-colour continuous gradients\", \n       subtitle = \"Palettes from wesanderson\",\n       caption = \"Palette: GrandBudapest1\") # Change this caption based on palette choice"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#continuous-n-colour-palettes-from-rcolorbrewer",
    "href": "content/labs/r-labs/graphics/colors.html#continuous-n-colour-palettes-from-rcolorbrewer",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n23 Continuous n-Colour palettes from RColorBrewer\n",
    "text": "23 Continuous n-Colour palettes from RColorBrewer\n\nRecall Palette types\n\n\nseq for continuous data mapped to colour\n\nqual for categorical data mapped to colour ( discrete)\n\ndiv continuous data mapped to colour, that has pos and neg extremes from a middle value\n\n\nbrewer.pal.info\n\n\n\n  \n\n\n\n\n# scale_color_distiller() and scale_fill_distiller() \n# are used to apply the ColorBrewer colour scales \n# to continuous data.\n\np2 +\n  scale_colour_distiller(\n    palette = \"YlGnBu\") + # Play with this palette\n  \n  labs(title = \"RColorBrewer Palette\")"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#continuous-colour-scales-using-paletteer-palettes",
    "href": "content/labs/r-labs/graphics/colors.html#continuous-colour-scales-using-paletteer-palettes",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n24 Continuous Colour scales using paletteer palettes",
    "text": "24 Continuous Colour scales using paletteer palettes\nThis palette seems to have everything accessible in a simple way! NOTE: In order to access some palettes in paletteer, you may be asked to install other packages. E.g. harrypotter or scico. These need not be brought into your session using library() but are accessed directly by paletteer which is very convenient!!\n\n# What continuous palettes are there in paletteer?\npaletteer::palettes_c_names\n\n\n\n  \n\n\n\nOK, one of the Games of Thrones Palettes, and Harry Potter!\n\np2 +\n  scale_colour_paletteer_c(\"gameofthrones::jon_snow\") +\n  labs(title = \"Using Paletteer\",\n       subtitle = \"Continuous Palette-Game of Thrones: Jon Snow\",\n       caption = \"Oh you awful Srishti people...\") +\n  \n  # Harry Potter Gryffindor Palette.\n  # Will ask for `harrypotter` package to be installed. Say yes!\n  p2 +\n  scale_colour_paletteer_c(\"harrypotter::gryffindor\") +\n  labs(title = \"Using Paletteer\",\n       subtitle = \"Continuous Palette-Harry Potter:Gryffindor\")\n\nError in gen_fun(name = palette[2], n = n): could not find function \"gen_fun\""
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html",
    "href": "content/labs/r-labs/graphics/index.html",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "",
    "text": "This RMarkdown document is part of my course on R for Artists and Designers. The material is based on A Layered Grammar of Graphics by Hadley Wickham. The intent of this Course is to build Skill in coding in R, and also appreciate R as a way to metaphorically visualize information of various kinds, using predominantly geometric figures and structures.\nAll RMarkdown files combine code, text, web-images, and figures developed using code. Everything is text; code chunks are enclosed in fences (```)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#goals",
    "href": "content/labs/r-labs/graphics/index.html#goals",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "\n2 Goals",
    "text": "2 Goals\nAt the end of this Lab session, we should: - know the types and structures of tidy data and be able to work with them - be able to create data visualizations using ggplot - Understand aesthetics and scales in `ggplot"
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#pedagogical-note",
    "href": "content/labs/r-labs/graphics/index.html#pedagogical-note",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "\n3 Pedagogical Note",
    "text": "3 Pedagogical Note\nThe method followed will be based on PRIMM:\n\n\nPREDICT Inspect the code and guess at what the code might do, write predictions\n\n\nRUN the code provided and check what happens\n\nINFER what the parameters of the code do and write comments to explain. What bells and whistles can you see?\n\nMODIFY the parameters code provided to understand the options available. Write comments to show what you have aimed for and achieved.\n\nMAKE : take an idea/concept of your own, and graph it."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#set-up",
    "href": "content/labs/r-labs/graphics/index.html#set-up",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "\n4 Set Up",
    "text": "4 Set Up\nThe setup code chunk below brings into our coding session R packages that provide specific computational abilities and also datasets which we can use.\nTo reiterate: Packages and datasets are not the same thing !! Packages are (small) collections of programs. Datasets are just‚Ä¶.information."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#a-teaser-from-john-snow",
    "href": "content/labs/r-labs/graphics/index.html#a-teaser-from-john-snow",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "\n5 A Teaser from John Snow",
    "text": "5 A Teaser from John Snow"
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#review-of-tidy-data",
    "href": "content/labs/r-labs/graphics/index.html#review-of-tidy-data",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "\n6 Review of Tidy Data",
    "text": "6 Review of Tidy Data\n‚ÄúTidy Data‚Äù is an important way of thinking about what data typically look like in R. Let‚Äôs fetch a figure from the web to show the (preferred) structure of data in R. (The syntax to bring in a web-figure is ![caption](url))\n The three features described in the figure above define the nature of tidy data:\n\n\nVariables in Columns\n\n\nObservations in Rows and\n\n\nMeasurements in Cells.\n\nData are imagined to be resulting from an experiment. Each variable represents a parameter/aspect in the experiment. Each row represents an additional datum of measurement. A cell is a single measurement on a single parameter(column) in a single observation(row)."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#kinds-of-variables",
    "href": "content/labs/r-labs/graphics/index.html#kinds-of-variables",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "\n7 Kinds of Variables",
    "text": "7 Kinds of Variables\nKinds of Variable are defined by the kind of questions they answer to:\n\nWhat/Who/Where? -> Some kind of Name. Categorical variable\nWhat Kind? How? -> Some kind of ‚ÄúType‚Äù. Factor variable\nHow Many? How large? -> Some kind of Quantity. Numerical variable. Most Figures in R are computed with variables, and therefore, with columns."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#interrogations-and-graphs",
    "href": "content/labs/r-labs/graphics/index.html#interrogations-and-graphs",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "\n8 Interrogations and Graphs",
    "text": "8 Interrogations and Graphs\nCreating graphs from data is an act of asking questions and viewing answers in a geometric way. Let us write some simple English descriptions of measures and visuals and see what commands they use in R.\n\n8.1 Components of the layered grammar of graphics\nLayers are used to create the objects on a plot. They are defined by five basic parts:\n\nData (What dataset/spreadsheet am I using?)\nMapping (What does each column do in my graph?)\nStatistical transformation (stat) (Do I have count something first?)\nGeometric object (geom) (What shape, colour, size‚Ä¶do I want?)\nPosition adjustment (position) (Where do I want it on the graph?)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#data",
    "href": "content/labs/r-labs/graphics/index.html#data",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "\n9 Data",
    "text": "9 Data\nWe will use ‚Äúreal world‚Äù data. Let‚Äôs use the penguins dataset in the palmerpenguins package. Run ?penguins in the console to get more information about this dataset.\n\n9.1 Head\n\n\n\n\n  \n\n\n\n\n9.2 Tail\n\n\n\n\n  \n\n\n\n\n9.3 Dim\n\n\n[1] 344   8\n\n\nSo we know what our data looks like. We pass this data to ggplot use to plot as follows: in R this creates an empty graph sheet!! Because we have not (yet) declared the geometric shapes we want to use to plot our information."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#mapping",
    "href": "content/labs/r-labs/graphics/index.html#mapping",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "\n10 Mapping",
    "text": "10 Mapping\nNow that we have told R what data to use, we need to state what variables to plot and how.\nAesthetic Mapping defines how the variables are applied to the plot, i.e.¬†we take a variable from the data and ‚Äúmetaphorize‚Äù it into a geometric feature. We can map variables metaphorically to a variety of geometric things: coordinate, length, height, size, shape, colour, alpha(how dark?)‚Ä¶.\nThe syntax uses: aes(some_geometric_thing = some_variable)\nRemember variable = column.\nSo if we were graphing information from penguins, we might map a penguin‚Äôs flipper_length_mm column to the \\(x\\) position, and the body_mass_g column to the \\(y\\) position.\n\n10.1 Mapping Example-1\nWe can try another example of aesthetic mapping with the same dataset:\n\n10.1.1 Plot-1a\n\n\n\n\n\n\n10.1.2 Plot-1b\n\n10.1.3 Plot-1c\n\n10.2 Mapping Example-2\nWe can try another example of aesthetic mapping with the same dataset:\n\n10.2.1 Plot-2a\n\n\n\n\n\n\n10.2.2 Plot-2b\n\n10.2.3 Plot-2c\n\n10.3 Mapping Example-3\nWe can try another example of aesthetic mapping with the same dataset:\n\n10.3.1 Plot-3a\n\n\n\n\n\n\n10.3.2 Plot-3b\n\n10.3.3 Plot-3c\n\n10.4 Mapping Example-4\nWe can try another example of aesthetic mapping with the same dataset:\n\n10.4.1 Plot-4a\n\n\n\n\n\n\n10.4.2 Plot-4b\n\n10.4.3 Plot-4c"
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#geometric-objects",
    "href": "content/labs/r-labs/graphics/index.html#geometric-objects",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "\n11 Geometric objects",
    "text": "11 Geometric objects\nGeometric objects (geoms) control the type of plot you create. Geoms are classified by their dimensionality:\n\n0 dimensions - point, text\n1 dimension - path, line\n2 dimensions - polygon, interval\n\nEach geom can only display certain aesthetics or visual attributes of the geom. For example, a point geom has position, color, shape, and size aesthetics.\nWe can also stack up geoms on top of one another to add layers to the graph.\n\n11.1 Plot1\n\n\nWarning: Removed 2 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\n11.2 Plot2\n\n\nWarning: Removed 2 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\n11.3 Plot3\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPosition determines the starting location (origin) of each bar\nHeight determines how tall to draw the bar. Here the height is based on the number of observations in the dataset for each possible number of cylinders."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#position-adjustment",
    "href": "content/labs/r-labs/graphics/index.html#position-adjustment",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "\n12 Position adjustment",
    "text": "12 Position adjustment\nSometimes with dense data we need to adjust the position of elements on the plot, otherwise data points might obscure one another. Bar plots frequently stack or dodge the bars to avoid overlap:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSometimes scatterplots with few unique \\(x\\) and \\(y\\) values are jittered (random noise is added) to reduce overplotting.\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#statistical-transformation",
    "href": "content/labs/r-labs/graphics/index.html#statistical-transformation",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "\n13 Statistical transformation",
    "text": "13 Statistical transformation\nA statistical transformation (stat) pre-transforms the data, before plotting. For instance, in a bar graph you might summarize the data by counting the total number of observations within a set of categories, and then plotting the count.\n\n13.1 Count\n\n\n\n\n  \n\n\n\n\n13.2 Count and Bar Graph\n\n\n\n\n\n\n13.3 Tidy Count and Bar Graph\n\n\n\n\n\n\n13.4 Count inside the Plot\n\n\n\n\n\nSometimes you don‚Äôt need to make a statistical transformation. For example, in a scatterplot you use the raw values for the \\(x\\) and \\(y\\) variables to map onto the graph. In these situations, the statistical transformation is an identity transformation - the stat simply passes in the original dataset and exports the exact same dataset."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#scale",
    "href": "content/labs/r-labs/graphics/index.html#scale",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "\n14 Scale",
    "text": "14 Scale\nA scale controls how data is mapped to aesthetic attributes, so we need one scale for every aesthetic property employed in a layer. For example, this graph defines a scale for color:\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nThe scale can be changed to use a different color palette:\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nNow we are using a different palette, but the scale is still consistent: all Adelie penguins utilize the same color, whereas Chinstrap use a new color but each Adelie still uses the same, consistent color."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#coordinate-system",
    "href": "content/labs/r-labs/graphics/index.html#coordinate-system",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "\n15 Coordinate system",
    "text": "15 Coordinate system\nA coordinate system (coord) maps the position of objects onto the plane of the plot, and controls how the axes and grid lines are drawn. Plots typically use two coordinates (\\(x, y\\)), but could use any number of coordinates. Most plots are drawn using the Cartesian coordinate system:\n\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nThis system requires a fixed and equal spacing between values on the axes. That is, the graph draws the same distance between 1 and 2 as it does between 5 and 6. The graph could be drawn using a semi-log coordinate system which logarithmically compresses the distance on an axis:\n\n\n\n\n\nOr could even be drawn using polar coordinates:"
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#faceting",
    "href": "content/labs/r-labs/graphics/index.html#faceting",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "\n16 Faceting",
    "text": "16 Faceting\nFaceting can be used to split the data up into subsets of the entire dataset. This is a powerful tool when investigating whether patterns are the same or different across conditions, and allows the subsets to be visualized on the same plot (known as conditioned or trellis plots). The faceting specification describes which variables should be used to split up the data, and how they should be arranged.\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#defaults",
    "href": "content/labs/r-labs/graphics/index.html#defaults",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "\n17 Defaults",
    "text": "17 Defaults\nRather than explicitly declaring each component of a layered graphic (which will use more code and introduces opportunities for errors), we can establish intelligent defaults for specific geoms and scales. For instance, whenever we want to use a bar geom, we can default to using a stat that counts the number of observations in each group of our variable in the \\(x\\) position.\nConsider the following scenario: you wish to generate a scatterplot visualizing the relationship between penguins‚Äô bill_length and their body_mass. With no defaults, the code to generate this graph is:\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nThe above code:\n\nCreates a new plot object (ggplot)\n\nAdds a layer (layer)\n\nSpecifies the data (penguins)\nMaps engine bill length to the \\(x\\) position and body mass to the \\(y\\) position (mapping)\nUses the point geometric transformation (geom = \"point\")\nImplements an identity transformation and position (stat = \"identity\" and position = \"identity\")\n\n\nEstablishes two continuous position scales (scale_x_continuous and scale_y_continuous)\nDeclares a cartesian coordinate system (coord_cartesian)\n\nHow can we simplify this using intelligent defaults?\n\nWe only need to specify one geom and stat, since each geom has a default stat.\nCartesian coordinate systems are most commonly used, so it should be the default.\n\nDefault scales can be added based on the aesthetic and type of variables.\n\nContinuous values are transformed with a linear scaling.\nDiscrete values are mapped to integers.\nScales for aesthetics such as color, fill, and size can also be intelligently defaulted.\n\n\n\nUsing these defaults, we can rewrite the above code as:\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nThis generates the exact same plot, but uses fewer lines of code. Because multiple layers can use the same components (data, mapping, etc.), we can also specify that information in the ggplot() function rather than in the layer() function:\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nAnd as we will learn, function arguments in R use specific ordering, so we can omit the explicit call to data and mapping:\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html",
    "href": "content/labs/r-labs/graphics/wizardy.html",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "",
    "text": "This Quarto document is part of my Workshop in R. The material is based on A Layered Grammar of Graphics by Hadley Wickham, and more specifically on the tutorials by Cedric Scherer. The course is meant for First Year students pursuing a Degree in Art and Design. The intent is to build Skill in coding in R, and also appreciate R as a way to metaphorically visualize information of various kinds, using predominantly geometric figures and structures.\nAll Quarto files combine code, text, web-images, and figures developed using code. Everything is text; code chunks are enclosed in fences (```)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#goals",
    "href": "content/labs/r-labs/graphics/wizardy.html#goals",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "\n2 Goals",
    "text": "2 Goals\n\n(Re)Understand different kinds of data variables\nAppreciate how they can be identified based on the Interrogative Pronouns they answer to\nUnderstand how each kind of variable lends itself to a specific choice of colour scale in the data visualization."
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#pedagogical-note",
    "href": "content/labs/r-labs/graphics/wizardy.html#pedagogical-note",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "\n3 Pedagogical Note",
    "text": "3 Pedagogical Note\nThe method followed will be based on PRIMM:\n\n\nPREDICT Inspect the code and guess at what the code might do, write predictions\n\n\nRUN the code provided and check what happens\n\nINFER what the parameters of the code do and write comments to explain. What bells and whistles can you see?\n\nMODIFY the parameters code provided to understand the options available. Write comments to show what you have aimed for and achieved.\n\nMAKE : take an idea/concept of your own, and graph it.\n\nIn the following, there is some boiler plate code demonstrating the use of colour palettes in R. There are places where YOUR TURN is mention; copy and play with the boiler plate code to see what happens !"
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#iconify-noto-v1-package-setting-up-r-packages",
    "href": "content/labs/r-labs/graphics/wizardy.html#iconify-noto-v1-package-setting-up-r-packages",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "\n4  Setting up R Packages",
    "text": "4  Setting up R Packages\nLet‚Äôs load up a few packages that we need to start:\n\nlibrary(tidyverse)   ## data science package collection (incl. the ggplot2 package)\nlibrary(systemfonts) ## use custom fonts (need to be installed on your OS)  \nlibrary(scico)       ## scico color palettes(http://www.fabiocrameri.ch/colourmaps.php) in R \nlibrary(ggtext)      ## add improved text rendering to ggplot2\nlibrary(ggforce)     ## add missing functionality to ggplot2\nlibrary(ggdist)      ## add uncertainity visualizations to ggplot2\nlibrary(magick)      ## load images into R\nlibrary(patchwork)   ## combine outputs from ggplot2\nlibrary(kableExtra)  ## Produces attractive tables\nlibrary(palmerpenguins)\n\nlibrary(showtext)   ## add google fonts to plots\n\nWe will want to add a few new fonts to our graphs. The best way (currently) is to use the showtext package ( which we loaded above) to bring into our work fonts from Google. To view and select the fonts you might want to work with, spend some time looking over:\n\nGoogle Webfonts Helper App\nGoogle Fonts\n\n\nfont_add_google(\"Gochi Hand\", \"gochi\")\nfont_add_google(\"Schoolbell\", \"bell\")\nfont_add_google(\"Galada\", \"galada\")\nfont_add_google(\"Schoolbell\", \"bell\")\nfont_add_google(\"Roboto\", \"roboto\")\nfont_add_google(\"Noto Sans\", \"noto\")\nfont_add_google(\"Uchen\", \"uchen\")\nfont_add_google(\"Ibarra Real Nova\", \"ibarra\")\nfont_add_google(\"Open Sans\", \"open\")\nfont_add_google(\"Anton\", \"anton\")\nfont_add_google(\"Tangerine\", \"tangerine\")\n\nshowtext_auto() # set the google fonts as default\n\nWe will work with a familiar dataset, so that we can concentrate on the chart aesthetics, without having to spend time getting used to the data: the penguins dataset again, from the palmerpenguins package."
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#data",
    "href": "content/labs/r-labs/graphics/wizardy.html#data",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "\n5 Data",
    "text": "5 Data\nAlways start your work with a table of the data:\n\npenguins <- penguins %>% drop_na() # remove data containing missing data\n\n## Create a nicely formatted table\n## uses `kableExtra` package\n## \npenguins %>% \n  kableExtra::kbl() %>%\n  kableExtra::kable_paper(full_width = TRUE) %>%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"condensed\", \"responsive\")) %>%\n  kableExtra::scroll_box(width = \"700px\", height = \"500px\")\n\n\n\n\n species \n    island \n    bill_length_mm \n    bill_depth_mm \n    flipper_length_mm \n    body_mass_g \n    sex \n    year \n  \n\n\n Adelie \n    Torgersen \n    39.1 \n    18.7 \n    181 \n    3750 \n    male \n    2007 \n  \n\n Adelie \n    Torgersen \n    39.5 \n    17.4 \n    186 \n    3800 \n    female \n    2007 \n  \n\n Adelie \n    Torgersen \n    40.3 \n    18.0 \n    195 \n    3250 \n    female \n    2007 \n  \n\n Adelie \n    Torgersen \n    36.7 \n    19.3 \n    193 \n    3450 \n    female \n    2007 \n  \n\n Adelie \n    Torgersen \n    39.3 \n    20.6 \n    190 \n    3650 \n    male \n    2007 \n  \n\n Adelie \n    Torgersen \n    38.9 \n    17.8 \n    181 \n    3625 \n    female \n    2007 \n  \n\n Adelie \n    Torgersen \n    39.2 \n    19.6 \n    195 \n    4675 \n    male \n    2007 \n  \n\n Adelie \n    Torgersen \n    41.1 \n    17.6 \n    182 \n    3200 \n    female \n    2007 \n  \n\n Adelie \n    Torgersen \n    38.6 \n    21.2 \n    191 \n    3800 \n    male \n    2007 \n  \n\n Adelie \n    Torgersen \n    34.6 \n    21.1 \n    198 \n    4400 \n    male \n    2007 \n  \n\n Adelie \n    Torgersen \n    36.6 \n    17.8 \n    185 \n    3700 \n    female \n    2007 \n  \n\n Adelie \n    Torgersen \n    38.7 \n    19.0 \n    195 \n    3450 \n    female \n    2007 \n  \n\n Adelie \n    Torgersen \n    42.5 \n    20.7 \n    197 \n    4500 \n    male \n    2007 \n  \n\n Adelie \n    Torgersen \n    34.4 \n    18.4 \n    184 \n    3325 \n    female \n    2007 \n  \n\n Adelie \n    Torgersen \n    46.0 \n    21.5 \n    194 \n    4200 \n    male \n    2007 \n  \n\n Adelie \n    Biscoe \n    37.8 \n    18.3 \n    174 \n    3400 \n    female \n    2007 \n  \n\n Adelie \n    Biscoe \n    37.7 \n    18.7 \n    180 \n    3600 \n    male \n    2007 \n  \n\n Adelie \n    Biscoe \n    35.9 \n    19.2 \n    189 \n    3800 \n    female \n    2007 \n  \n\n Adelie \n    Biscoe \n    38.2 \n    18.1 \n    185 \n    3950 \n    male \n    2007 \n  \n\n Adelie \n    Biscoe \n    38.8 \n    17.2 \n    180 \n    3800 \n    male \n    2007 \n  \n\n Adelie \n    Biscoe \n    35.3 \n    18.9 \n    187 \n    3800 \n    female \n    2007 \n  \n\n Adelie \n    Biscoe \n    40.6 \n    18.6 \n    183 \n    3550 \n    male \n    2007 \n  \n\n Adelie \n    Biscoe \n    40.5 \n    17.9 \n    187 \n    3200 \n    female \n    2007 \n  \n\n Adelie \n    Biscoe \n    37.9 \n    18.6 \n    172 \n    3150 \n    female \n    2007 \n  \n\n Adelie \n    Biscoe \n    40.5 \n    18.9 \n    180 \n    3950 \n    male \n    2007 \n  \n\n Adelie \n    Dream \n    39.5 \n    16.7 \n    178 \n    3250 \n    female \n    2007 \n  \n\n Adelie \n    Dream \n    37.2 \n    18.1 \n    178 \n    3900 \n    male \n    2007 \n  \n\n Adelie \n    Dream \n    39.5 \n    17.8 \n    188 \n    3300 \n    female \n    2007 \n  \n\n Adelie \n    Dream \n    40.9 \n    18.9 \n    184 \n    3900 \n    male \n    2007 \n  \n\n Adelie \n    Dream \n    36.4 \n    17.0 \n    195 \n    3325 \n    female \n    2007 \n  \n\n Adelie \n    Dream \n    39.2 \n    21.1 \n    196 \n    4150 \n    male \n    2007 \n  \n\n Adelie \n    Dream \n    38.8 \n    20.0 \n    190 \n    3950 \n    male \n    2007 \n  \n\n Adelie \n    Dream \n    42.2 \n    18.5 \n    180 \n    3550 \n    female \n    2007 \n  \n\n Adelie \n    Dream \n    37.6 \n    19.3 \n    181 \n    3300 \n    female \n    2007 \n  \n\n Adelie \n    Dream \n    39.8 \n    19.1 \n    184 \n    4650 \n    male \n    2007 \n  \n\n Adelie \n    Dream \n    36.5 \n    18.0 \n    182 \n    3150 \n    female \n    2007 \n  \n\n Adelie \n    Dream \n    40.8 \n    18.4 \n    195 \n    3900 \n    male \n    2007 \n  \n\n Adelie \n    Dream \n    36.0 \n    18.5 \n    186 \n    3100 \n    female \n    2007 \n  \n\n Adelie \n    Dream \n    44.1 \n    19.7 \n    196 \n    4400 \n    male \n    2007 \n  \n\n Adelie \n    Dream \n    37.0 \n    16.9 \n    185 \n    3000 \n    female \n    2007 \n  \n\n Adelie \n    Dream \n    39.6 \n    18.8 \n    190 \n    4600 \n    male \n    2007 \n  \n\n Adelie \n    Dream \n    41.1 \n    19.0 \n    182 \n    3425 \n    male \n    2007 \n  \n\n Adelie \n    Dream \n    36.0 \n    17.9 \n    190 \n    3450 \n    female \n    2007 \n  \n\n Adelie \n    Dream \n    42.3 \n    21.2 \n    191 \n    4150 \n    male \n    2007 \n  \n\n Adelie \n    Biscoe \n    39.6 \n    17.7 \n    186 \n    3500 \n    female \n    2008 \n  \n\n Adelie \n    Biscoe \n    40.1 \n    18.9 \n    188 \n    4300 \n    male \n    2008 \n  \n\n Adelie \n    Biscoe \n    35.0 \n    17.9 \n    190 \n    3450 \n    female \n    2008 \n  \n\n Adelie \n    Biscoe \n    42.0 \n    19.5 \n    200 \n    4050 \n    male \n    2008 \n  \n\n Adelie \n    Biscoe \n    34.5 \n    18.1 \n    187 \n    2900 \n    female \n    2008 \n  \n\n Adelie \n    Biscoe \n    41.4 \n    18.6 \n    191 \n    3700 \n    male \n    2008 \n  \n\n Adelie \n    Biscoe \n    39.0 \n    17.5 \n    186 \n    3550 \n    female \n    2008 \n  \n\n Adelie \n    Biscoe \n    40.6 \n    18.8 \n    193 \n    3800 \n    male \n    2008 \n  \n\n Adelie \n    Biscoe \n    36.5 \n    16.6 \n    181 \n    2850 \n    female \n    2008 \n  \n\n Adelie \n    Biscoe \n    37.6 \n    19.1 \n    194 \n    3750 \n    male \n    2008 \n  \n\n Adelie \n    Biscoe \n    35.7 \n    16.9 \n    185 \n    3150 \n    female \n    2008 \n  \n\n Adelie \n    Biscoe \n    41.3 \n    21.1 \n    195 \n    4400 \n    male \n    2008 \n  \n\n Adelie \n    Biscoe \n    37.6 \n    17.0 \n    185 \n    3600 \n    female \n    2008 \n  \n\n Adelie \n    Biscoe \n    41.1 \n    18.2 \n    192 \n    4050 \n    male \n    2008 \n  \n\n Adelie \n    Biscoe \n    36.4 \n    17.1 \n    184 \n    2850 \n    female \n    2008 \n  \n\n Adelie \n    Biscoe \n    41.6 \n    18.0 \n    192 \n    3950 \n    male \n    2008 \n  \n\n Adelie \n    Biscoe \n    35.5 \n    16.2 \n    195 \n    3350 \n    female \n    2008 \n  \n\n Adelie \n    Biscoe \n    41.1 \n    19.1 \n    188 \n    4100 \n    male \n    2008 \n  \n\n Adelie \n    Torgersen \n    35.9 \n    16.6 \n    190 \n    3050 \n    female \n    2008 \n  \n\n Adelie \n    Torgersen \n    41.8 \n    19.4 \n    198 \n    4450 \n    male \n    2008 \n  \n\n Adelie \n    Torgersen \n    33.5 \n    19.0 \n    190 \n    3600 \n    female \n    2008 \n  \n\n Adelie \n    Torgersen \n    39.7 \n    18.4 \n    190 \n    3900 \n    male \n    2008 \n  \n\n Adelie \n    Torgersen \n    39.6 \n    17.2 \n    196 \n    3550 \n    female \n    2008 \n  \n\n Adelie \n    Torgersen \n    45.8 \n    18.9 \n    197 \n    4150 \n    male \n    2008 \n  \n\n Adelie \n    Torgersen \n    35.5 \n    17.5 \n    190 \n    3700 \n    female \n    2008 \n  \n\n Adelie \n    Torgersen \n    42.8 \n    18.5 \n    195 \n    4250 \n    male \n    2008 \n  \n\n Adelie \n    Torgersen \n    40.9 \n    16.8 \n    191 \n    3700 \n    female \n    2008 \n  \n\n Adelie \n    Torgersen \n    37.2 \n    19.4 \n    184 \n    3900 \n    male \n    2008 \n  \n\n Adelie \n    Torgersen \n    36.2 \n    16.1 \n    187 \n    3550 \n    female \n    2008 \n  \n\n Adelie \n    Torgersen \n    42.1 \n    19.1 \n    195 \n    4000 \n    male \n    2008 \n  \n\n Adelie \n    Torgersen \n    34.6 \n    17.2 \n    189 \n    3200 \n    female \n    2008 \n  \n\n Adelie \n    Torgersen \n    42.9 \n    17.6 \n    196 \n    4700 \n    male \n    2008 \n  \n\n Adelie \n    Torgersen \n    36.7 \n    18.8 \n    187 \n    3800 \n    female \n    2008 \n  \n\n Adelie \n    Torgersen \n    35.1 \n    19.4 \n    193 \n    4200 \n    male \n    2008 \n  \n\n Adelie \n    Dream \n    37.3 \n    17.8 \n    191 \n    3350 \n    female \n    2008 \n  \n\n Adelie \n    Dream \n    41.3 \n    20.3 \n    194 \n    3550 \n    male \n    2008 \n  \n\n Adelie \n    Dream \n    36.3 \n    19.5 \n    190 \n    3800 \n    male \n    2008 \n  \n\n Adelie \n    Dream \n    36.9 \n    18.6 \n    189 \n    3500 \n    female \n    2008 \n  \n\n Adelie \n    Dream \n    38.3 \n    19.2 \n    189 \n    3950 \n    male \n    2008 \n  \n\n Adelie \n    Dream \n    38.9 \n    18.8 \n    190 \n    3600 \n    female \n    2008 \n  \n\n Adelie \n    Dream \n    35.7 \n    18.0 \n    202 \n    3550 \n    female \n    2008 \n  \n\n Adelie \n    Dream \n    41.1 \n    18.1 \n    205 \n    4300 \n    male \n    2008 \n  \n\n Adelie \n    Dream \n    34.0 \n    17.1 \n    185 \n    3400 \n    female \n    2008 \n  \n\n Adelie \n    Dream \n    39.6 \n    18.1 \n    186 \n    4450 \n    male \n    2008 \n  \n\n Adelie \n    Dream \n    36.2 \n    17.3 \n    187 \n    3300 \n    female \n    2008 \n  \n\n Adelie \n    Dream \n    40.8 \n    18.9 \n    208 \n    4300 \n    male \n    2008 \n  \n\n Adelie \n    Dream \n    38.1 \n    18.6 \n    190 \n    3700 \n    female \n    2008 \n  \n\n Adelie \n    Dream \n    40.3 \n    18.5 \n    196 \n    4350 \n    male \n    2008 \n  \n\n Adelie \n    Dream \n    33.1 \n    16.1 \n    178 \n    2900 \n    female \n    2008 \n  \n\n Adelie \n    Dream \n    43.2 \n    18.5 \n    192 \n    4100 \n    male \n    2008 \n  \n\n Adelie \n    Biscoe \n    35.0 \n    17.9 \n    192 \n    3725 \n    female \n    2009 \n  \n\n Adelie \n    Biscoe \n    41.0 \n    20.0 \n    203 \n    4725 \n    male \n    2009 \n  \n\n Adelie \n    Biscoe \n    37.7 \n    16.0 \n    183 \n    3075 \n    female \n    2009 \n  \n\n Adelie \n    Biscoe \n    37.8 \n    20.0 \n    190 \n    4250 \n    male \n    2009 \n  \n\n Adelie \n    Biscoe \n    37.9 \n    18.6 \n    193 \n    2925 \n    female \n    2009 \n  \n\n Adelie \n    Biscoe \n    39.7 \n    18.9 \n    184 \n    3550 \n    male \n    2009 \n  \n\n Adelie \n    Biscoe \n    38.6 \n    17.2 \n    199 \n    3750 \n    female \n    2009 \n  \n\n Adelie \n    Biscoe \n    38.2 \n    20.0 \n    190 \n    3900 \n    male \n    2009 \n  \n\n Adelie \n    Biscoe \n    38.1 \n    17.0 \n    181 \n    3175 \n    female \n    2009 \n  \n\n Adelie \n    Biscoe \n    43.2 \n    19.0 \n    197 \n    4775 \n    male \n    2009 \n  \n\n Adelie \n    Biscoe \n    38.1 \n    16.5 \n    198 \n    3825 \n    female \n    2009 \n  \n\n Adelie \n    Biscoe \n    45.6 \n    20.3 \n    191 \n    4600 \n    male \n    2009 \n  \n\n Adelie \n    Biscoe \n    39.7 \n    17.7 \n    193 \n    3200 \n    female \n    2009 \n  \n\n Adelie \n    Biscoe \n    42.2 \n    19.5 \n    197 \n    4275 \n    male \n    2009 \n  \n\n Adelie \n    Biscoe \n    39.6 \n    20.7 \n    191 \n    3900 \n    female \n    2009 \n  \n\n Adelie \n    Biscoe \n    42.7 \n    18.3 \n    196 \n    4075 \n    male \n    2009 \n  \n\n Adelie \n    Torgersen \n    38.6 \n    17.0 \n    188 \n    2900 \n    female \n    2009 \n  \n\n Adelie \n    Torgersen \n    37.3 \n    20.5 \n    199 \n    3775 \n    male \n    2009 \n  \n\n Adelie \n    Torgersen \n    35.7 \n    17.0 \n    189 \n    3350 \n    female \n    2009 \n  \n\n Adelie \n    Torgersen \n    41.1 \n    18.6 \n    189 \n    3325 \n    male \n    2009 \n  \n\n Adelie \n    Torgersen \n    36.2 \n    17.2 \n    187 \n    3150 \n    female \n    2009 \n  \n\n Adelie \n    Torgersen \n    37.7 \n    19.8 \n    198 \n    3500 \n    male \n    2009 \n  \n\n Adelie \n    Torgersen \n    40.2 \n    17.0 \n    176 \n    3450 \n    female \n    2009 \n  \n\n Adelie \n    Torgersen \n    41.4 \n    18.5 \n    202 \n    3875 \n    male \n    2009 \n  \n\n Adelie \n    Torgersen \n    35.2 \n    15.9 \n    186 \n    3050 \n    female \n    2009 \n  \n\n Adelie \n    Torgersen \n    40.6 \n    19.0 \n    199 \n    4000 \n    male \n    2009 \n  \n\n Adelie \n    Torgersen \n    38.8 \n    17.6 \n    191 \n    3275 \n    female \n    2009 \n  \n\n Adelie \n    Torgersen \n    41.5 \n    18.3 \n    195 \n    4300 \n    male \n    2009 \n  \n\n Adelie \n    Torgersen \n    39.0 \n    17.1 \n    191 \n    3050 \n    female \n    2009 \n  \n\n Adelie \n    Torgersen \n    44.1 \n    18.0 \n    210 \n    4000 \n    male \n    2009 \n  \n\n Adelie \n    Torgersen \n    38.5 \n    17.9 \n    190 \n    3325 \n    female \n    2009 \n  \n\n Adelie \n    Torgersen \n    43.1 \n    19.2 \n    197 \n    3500 \n    male \n    2009 \n  \n\n Adelie \n    Dream \n    36.8 \n    18.5 \n    193 \n    3500 \n    female \n    2009 \n  \n\n Adelie \n    Dream \n    37.5 \n    18.5 \n    199 \n    4475 \n    male \n    2009 \n  \n\n Adelie \n    Dream \n    38.1 \n    17.6 \n    187 \n    3425 \n    female \n    2009 \n  \n\n Adelie \n    Dream \n    41.1 \n    17.5 \n    190 \n    3900 \n    male \n    2009 \n  \n\n Adelie \n    Dream \n    35.6 \n    17.5 \n    191 \n    3175 \n    female \n    2009 \n  \n\n Adelie \n    Dream \n    40.2 \n    20.1 \n    200 \n    3975 \n    male \n    2009 \n  \n\n Adelie \n    Dream \n    37.0 \n    16.5 \n    185 \n    3400 \n    female \n    2009 \n  \n\n Adelie \n    Dream \n    39.7 \n    17.9 \n    193 \n    4250 \n    male \n    2009 \n  \n\n Adelie \n    Dream \n    40.2 \n    17.1 \n    193 \n    3400 \n    female \n    2009 \n  \n\n Adelie \n    Dream \n    40.6 \n    17.2 \n    187 \n    3475 \n    male \n    2009 \n  \n\n Adelie \n    Dream \n    32.1 \n    15.5 \n    188 \n    3050 \n    female \n    2009 \n  \n\n Adelie \n    Dream \n    40.7 \n    17.0 \n    190 \n    3725 \n    male \n    2009 \n  \n\n Adelie \n    Dream \n    37.3 \n    16.8 \n    192 \n    3000 \n    female \n    2009 \n  \n\n Adelie \n    Dream \n    39.0 \n    18.7 \n    185 \n    3650 \n    male \n    2009 \n  \n\n Adelie \n    Dream \n    39.2 \n    18.6 \n    190 \n    4250 \n    male \n    2009 \n  \n\n Adelie \n    Dream \n    36.6 \n    18.4 \n    184 \n    3475 \n    female \n    2009 \n  \n\n Adelie \n    Dream \n    36.0 \n    17.8 \n    195 \n    3450 \n    female \n    2009 \n  \n\n Adelie \n    Dream \n    37.8 \n    18.1 \n    193 \n    3750 \n    male \n    2009 \n  \n\n Adelie \n    Dream \n    36.0 \n    17.1 \n    187 \n    3700 \n    female \n    2009 \n  \n\n Adelie \n    Dream \n    41.5 \n    18.5 \n    201 \n    4000 \n    male \n    2009 \n  \n\n Gentoo \n    Biscoe \n    46.1 \n    13.2 \n    211 \n    4500 \n    female \n    2007 \n  \n\n Gentoo \n    Biscoe \n    50.0 \n    16.3 \n    230 \n    5700 \n    male \n    2007 \n  \n\n Gentoo \n    Biscoe \n    48.7 \n    14.1 \n    210 \n    4450 \n    female \n    2007 \n  \n\n Gentoo \n    Biscoe \n    50.0 \n    15.2 \n    218 \n    5700 \n    male \n    2007 \n  \n\n Gentoo \n    Biscoe \n    47.6 \n    14.5 \n    215 \n    5400 \n    male \n    2007 \n  \n\n Gentoo \n    Biscoe \n    46.5 \n    13.5 \n    210 \n    4550 \n    female \n    2007 \n  \n\n Gentoo \n    Biscoe \n    45.4 \n    14.6 \n    211 \n    4800 \n    female \n    2007 \n  \n\n Gentoo \n    Biscoe \n    46.7 \n    15.3 \n    219 \n    5200 \n    male \n    2007 \n  \n\n Gentoo \n    Biscoe \n    43.3 \n    13.4 \n    209 \n    4400 \n    female \n    2007 \n  \n\n Gentoo \n    Biscoe \n    46.8 \n    15.4 \n    215 \n    5150 \n    male \n    2007 \n  \n\n Gentoo \n    Biscoe \n    40.9 \n    13.7 \n    214 \n    4650 \n    female \n    2007 \n  \n\n Gentoo \n    Biscoe \n    49.0 \n    16.1 \n    216 \n    5550 \n    male \n    2007 \n  \n\n Gentoo \n    Biscoe \n    45.5 \n    13.7 \n    214 \n    4650 \n    female \n    2007 \n  \n\n Gentoo \n    Biscoe \n    48.4 \n    14.6 \n    213 \n    5850 \n    male \n    2007 \n  \n\n Gentoo \n    Biscoe \n    45.8 \n    14.6 \n    210 \n    4200 \n    female \n    2007 \n  \n\n Gentoo \n    Biscoe \n    49.3 \n    15.7 \n    217 \n    5850 \n    male \n    2007 \n  \n\n Gentoo \n    Biscoe \n    42.0 \n    13.5 \n    210 \n    4150 \n    female \n    2007 \n  \n\n Gentoo \n    Biscoe \n    49.2 \n    15.2 \n    221 \n    6300 \n    male \n    2007 \n  \n\n Gentoo \n    Biscoe \n    46.2 \n    14.5 \n    209 \n    4800 \n    female \n    2007 \n  \n\n Gentoo \n    Biscoe \n    48.7 \n    15.1 \n    222 \n    5350 \n    male \n    2007 \n  \n\n Gentoo \n    Biscoe \n    50.2 \n    14.3 \n    218 \n    5700 \n    male \n    2007 \n  \n\n Gentoo \n    Biscoe \n    45.1 \n    14.5 \n    215 \n    5000 \n    female \n    2007 \n  \n\n Gentoo \n    Biscoe \n    46.5 \n    14.5 \n    213 \n    4400 \n    female \n    2007 \n  \n\n Gentoo \n    Biscoe \n    46.3 \n    15.8 \n    215 \n    5050 \n    male \n    2007 \n  \n\n Gentoo \n    Biscoe \n    42.9 \n    13.1 \n    215 \n    5000 \n    female \n    2007 \n  \n\n Gentoo \n    Biscoe \n    46.1 \n    15.1 \n    215 \n    5100 \n    male \n    2007 \n  \n\n Gentoo \n    Biscoe \n    47.8 \n    15.0 \n    215 \n    5650 \n    male \n    2007 \n  \n\n Gentoo \n    Biscoe \n    48.2 \n    14.3 \n    210 \n    4600 \n    female \n    2007 \n  \n\n Gentoo \n    Biscoe \n    50.0 \n    15.3 \n    220 \n    5550 \n    male \n    2007 \n  \n\n Gentoo \n    Biscoe \n    47.3 \n    15.3 \n    222 \n    5250 \n    male \n    2007 \n  \n\n Gentoo \n    Biscoe \n    42.8 \n    14.2 \n    209 \n    4700 \n    female \n    2007 \n  \n\n Gentoo \n    Biscoe \n    45.1 \n    14.5 \n    207 \n    5050 \n    female \n    2007 \n  \n\n Gentoo \n    Biscoe \n    59.6 \n    17.0 \n    230 \n    6050 \n    male \n    2007 \n  \n\n Gentoo \n    Biscoe \n    49.1 \n    14.8 \n    220 \n    5150 \n    female \n    2008 \n  \n\n Gentoo \n    Biscoe \n    48.4 \n    16.3 \n    220 \n    5400 \n    male \n    2008 \n  \n\n Gentoo \n    Biscoe \n    42.6 \n    13.7 \n    213 \n    4950 \n    female \n    2008 \n  \n\n Gentoo \n    Biscoe \n    44.4 \n    17.3 \n    219 \n    5250 \n    male \n    2008 \n  \n\n Gentoo \n    Biscoe \n    44.0 \n    13.6 \n    208 \n    4350 \n    female \n    2008 \n  \n\n Gentoo \n    Biscoe \n    48.7 \n    15.7 \n    208 \n    5350 \n    male \n    2008 \n  \n\n Gentoo \n    Biscoe \n    42.7 \n    13.7 \n    208 \n    3950 \n    female \n    2008 \n  \n\n Gentoo \n    Biscoe \n    49.6 \n    16.0 \n    225 \n    5700 \n    male \n    2008 \n  \n\n Gentoo \n    Biscoe \n    45.3 \n    13.7 \n    210 \n    4300 \n    female \n    2008 \n  \n\n Gentoo \n    Biscoe \n    49.6 \n    15.0 \n    216 \n    4750 \n    male \n    2008 \n  \n\n Gentoo \n    Biscoe \n    50.5 \n    15.9 \n    222 \n    5550 \n    male \n    2008 \n  \n\n Gentoo \n    Biscoe \n    43.6 \n    13.9 \n    217 \n    4900 \n    female \n    2008 \n  \n\n Gentoo \n    Biscoe \n    45.5 \n    13.9 \n    210 \n    4200 \n    female \n    2008 \n  \n\n Gentoo \n    Biscoe \n    50.5 \n    15.9 \n    225 \n    5400 \n    male \n    2008 \n  \n\n Gentoo \n    Biscoe \n    44.9 \n    13.3 \n    213 \n    5100 \n    female \n    2008 \n  \n\n Gentoo \n    Biscoe \n    45.2 \n    15.8 \n    215 \n    5300 \n    male \n    2008 \n  \n\n Gentoo \n    Biscoe \n    46.6 \n    14.2 \n    210 \n    4850 \n    female \n    2008 \n  \n\n Gentoo \n    Biscoe \n    48.5 \n    14.1 \n    220 \n    5300 \n    male \n    2008 \n  \n\n Gentoo \n    Biscoe \n    45.1 \n    14.4 \n    210 \n    4400 \n    female \n    2008 \n  \n\n Gentoo \n    Biscoe \n    50.1 \n    15.0 \n    225 \n    5000 \n    male \n    2008 \n  \n\n Gentoo \n    Biscoe \n    46.5 \n    14.4 \n    217 \n    4900 \n    female \n    2008 \n  \n\n Gentoo \n    Biscoe \n    45.0 \n    15.4 \n    220 \n    5050 \n    male \n    2008 \n  \n\n Gentoo \n    Biscoe \n    43.8 \n    13.9 \n    208 \n    4300 \n    female \n    2008 \n  \n\n Gentoo \n    Biscoe \n    45.5 \n    15.0 \n    220 \n    5000 \n    male \n    2008 \n  \n\n Gentoo \n    Biscoe \n    43.2 \n    14.5 \n    208 \n    4450 \n    female \n    2008 \n  \n\n Gentoo \n    Biscoe \n    50.4 \n    15.3 \n    224 \n    5550 \n    male \n    2008 \n  \n\n Gentoo \n    Biscoe \n    45.3 \n    13.8 \n    208 \n    4200 \n    female \n    2008 \n  \n\n Gentoo \n    Biscoe \n    46.2 \n    14.9 \n    221 \n    5300 \n    male \n    2008 \n  \n\n Gentoo \n    Biscoe \n    45.7 \n    13.9 \n    214 \n    4400 \n    female \n    2008 \n  \n\n Gentoo \n    Biscoe \n    54.3 \n    15.7 \n    231 \n    5650 \n    male \n    2008 \n  \n\n Gentoo \n    Biscoe \n    45.8 \n    14.2 \n    219 \n    4700 \n    female \n    2008 \n  \n\n Gentoo \n    Biscoe \n    49.8 \n    16.8 \n    230 \n    5700 \n    male \n    2008 \n  \n\n Gentoo \n    Biscoe \n    49.5 \n    16.2 \n    229 \n    5800 \n    male \n    2008 \n  \n\n Gentoo \n    Biscoe \n    43.5 \n    14.2 \n    220 \n    4700 \n    female \n    2008 \n  \n\n Gentoo \n    Biscoe \n    50.7 \n    15.0 \n    223 \n    5550 \n    male \n    2008 \n  \n\n Gentoo \n    Biscoe \n    47.7 \n    15.0 \n    216 \n    4750 \n    female \n    2008 \n  \n\n Gentoo \n    Biscoe \n    46.4 \n    15.6 \n    221 \n    5000 \n    male \n    2008 \n  \n\n Gentoo \n    Biscoe \n    48.2 \n    15.6 \n    221 \n    5100 \n    male \n    2008 \n  \n\n Gentoo \n    Biscoe \n    46.5 \n    14.8 \n    217 \n    5200 \n    female \n    2008 \n  \n\n Gentoo \n    Biscoe \n    46.4 \n    15.0 \n    216 \n    4700 \n    female \n    2008 \n  \n\n Gentoo \n    Biscoe \n    48.6 \n    16.0 \n    230 \n    5800 \n    male \n    2008 \n  \n\n Gentoo \n    Biscoe \n    47.5 \n    14.2 \n    209 \n    4600 \n    female \n    2008 \n  \n\n Gentoo \n    Biscoe \n    51.1 \n    16.3 \n    220 \n    6000 \n    male \n    2008 \n  \n\n Gentoo \n    Biscoe \n    45.2 \n    13.8 \n    215 \n    4750 \n    female \n    2008 \n  \n\n Gentoo \n    Biscoe \n    45.2 \n    16.4 \n    223 \n    5950 \n    male \n    2008 \n  \n\n Gentoo \n    Biscoe \n    49.1 \n    14.5 \n    212 \n    4625 \n    female \n    2009 \n  \n\n Gentoo \n    Biscoe \n    52.5 \n    15.6 \n    221 \n    5450 \n    male \n    2009 \n  \n\n Gentoo \n    Biscoe \n    47.4 \n    14.6 \n    212 \n    4725 \n    female \n    2009 \n  \n\n Gentoo \n    Biscoe \n    50.0 \n    15.9 \n    224 \n    5350 \n    male \n    2009 \n  \n\n Gentoo \n    Biscoe \n    44.9 \n    13.8 \n    212 \n    4750 \n    female \n    2009 \n  \n\n Gentoo \n    Biscoe \n    50.8 \n    17.3 \n    228 \n    5600 \n    male \n    2009 \n  \n\n Gentoo \n    Biscoe \n    43.4 \n    14.4 \n    218 \n    4600 \n    female \n    2009 \n  \n\n Gentoo \n    Biscoe \n    51.3 \n    14.2 \n    218 \n    5300 \n    male \n    2009 \n  \n\n Gentoo \n    Biscoe \n    47.5 \n    14.0 \n    212 \n    4875 \n    female \n    2009 \n  \n\n Gentoo \n    Biscoe \n    52.1 \n    17.0 \n    230 \n    5550 \n    male \n    2009 \n  \n\n Gentoo \n    Biscoe \n    47.5 \n    15.0 \n    218 \n    4950 \n    female \n    2009 \n  \n\n Gentoo \n    Biscoe \n    52.2 \n    17.1 \n    228 \n    5400 \n    male \n    2009 \n  \n\n Gentoo \n    Biscoe \n    45.5 \n    14.5 \n    212 \n    4750 \n    female \n    2009 \n  \n\n Gentoo \n    Biscoe \n    49.5 \n    16.1 \n    224 \n    5650 \n    male \n    2009 \n  \n\n Gentoo \n    Biscoe \n    44.5 \n    14.7 \n    214 \n    4850 \n    female \n    2009 \n  \n\n Gentoo \n    Biscoe \n    50.8 \n    15.7 \n    226 \n    5200 \n    male \n    2009 \n  \n\n Gentoo \n    Biscoe \n    49.4 \n    15.8 \n    216 \n    4925 \n    male \n    2009 \n  \n\n Gentoo \n    Biscoe \n    46.9 \n    14.6 \n    222 \n    4875 \n    female \n    2009 \n  \n\n Gentoo \n    Biscoe \n    48.4 \n    14.4 \n    203 \n    4625 \n    female \n    2009 \n  \n\n Gentoo \n    Biscoe \n    51.1 \n    16.5 \n    225 \n    5250 \n    male \n    2009 \n  \n\n Gentoo \n    Biscoe \n    48.5 \n    15.0 \n    219 \n    4850 \n    female \n    2009 \n  \n\n Gentoo \n    Biscoe \n    55.9 \n    17.0 \n    228 \n    5600 \n    male \n    2009 \n  \n\n Gentoo \n    Biscoe \n    47.2 \n    15.5 \n    215 \n    4975 \n    female \n    2009 \n  \n\n Gentoo \n    Biscoe \n    49.1 \n    15.0 \n    228 \n    5500 \n    male \n    2009 \n  \n\n Gentoo \n    Biscoe \n    46.8 \n    16.1 \n    215 \n    5500 \n    male \n    2009 \n  \n\n Gentoo \n    Biscoe \n    41.7 \n    14.7 \n    210 \n    4700 \n    female \n    2009 \n  \n\n Gentoo \n    Biscoe \n    53.4 \n    15.8 \n    219 \n    5500 \n    male \n    2009 \n  \n\n Gentoo \n    Biscoe \n    43.3 \n    14.0 \n    208 \n    4575 \n    female \n    2009 \n  \n\n Gentoo \n    Biscoe \n    48.1 \n    15.1 \n    209 \n    5500 \n    male \n    2009 \n  \n\n Gentoo \n    Biscoe \n    50.5 \n    15.2 \n    216 \n    5000 \n    female \n    2009 \n  \n\n Gentoo \n    Biscoe \n    49.8 \n    15.9 \n    229 \n    5950 \n    male \n    2009 \n  \n\n Gentoo \n    Biscoe \n    43.5 \n    15.2 \n    213 \n    4650 \n    female \n    2009 \n  \n\n Gentoo \n    Biscoe \n    51.5 \n    16.3 \n    230 \n    5500 \n    male \n    2009 \n  \n\n Gentoo \n    Biscoe \n    46.2 \n    14.1 \n    217 \n    4375 \n    female \n    2009 \n  \n\n Gentoo \n    Biscoe \n    55.1 \n    16.0 \n    230 \n    5850 \n    male \n    2009 \n  \n\n Gentoo \n    Biscoe \n    48.8 \n    16.2 \n    222 \n    6000 \n    male \n    2009 \n  \n\n Gentoo \n    Biscoe \n    47.2 \n    13.7 \n    214 \n    4925 \n    female \n    2009 \n  \n\n Gentoo \n    Biscoe \n    46.8 \n    14.3 \n    215 \n    4850 \n    female \n    2009 \n  \n\n Gentoo \n    Biscoe \n    50.4 \n    15.7 \n    222 \n    5750 \n    male \n    2009 \n  \n\n Gentoo \n    Biscoe \n    45.2 \n    14.8 \n    212 \n    5200 \n    female \n    2009 \n  \n\n Gentoo \n    Biscoe \n    49.9 \n    16.1 \n    213 \n    5400 \n    male \n    2009 \n  \n\n Chinstrap \n    Dream \n    46.5 \n    17.9 \n    192 \n    3500 \n    female \n    2007 \n  \n\n Chinstrap \n    Dream \n    50.0 \n    19.5 \n    196 \n    3900 \n    male \n    2007 \n  \n\n Chinstrap \n    Dream \n    51.3 \n    19.2 \n    193 \n    3650 \n    male \n    2007 \n  \n\n Chinstrap \n    Dream \n    45.4 \n    18.7 \n    188 \n    3525 \n    female \n    2007 \n  \n\n Chinstrap \n    Dream \n    52.7 \n    19.8 \n    197 \n    3725 \n    male \n    2007 \n  \n\n Chinstrap \n    Dream \n    45.2 \n    17.8 \n    198 \n    3950 \n    female \n    2007 \n  \n\n Chinstrap \n    Dream \n    46.1 \n    18.2 \n    178 \n    3250 \n    female \n    2007 \n  \n\n Chinstrap \n    Dream \n    51.3 \n    18.2 \n    197 \n    3750 \n    male \n    2007 \n  \n\n Chinstrap \n    Dream \n    46.0 \n    18.9 \n    195 \n    4150 \n    female \n    2007 \n  \n\n Chinstrap \n    Dream \n    51.3 \n    19.9 \n    198 \n    3700 \n    male \n    2007 \n  \n\n Chinstrap \n    Dream \n    46.6 \n    17.8 \n    193 \n    3800 \n    female \n    2007 \n  \n\n Chinstrap \n    Dream \n    51.7 \n    20.3 \n    194 \n    3775 \n    male \n    2007 \n  \n\n Chinstrap \n    Dream \n    47.0 \n    17.3 \n    185 \n    3700 \n    female \n    2007 \n  \n\n Chinstrap \n    Dream \n    52.0 \n    18.1 \n    201 \n    4050 \n    male \n    2007 \n  \n\n Chinstrap \n    Dream \n    45.9 \n    17.1 \n    190 \n    3575 \n    female \n    2007 \n  \n\n Chinstrap \n    Dream \n    50.5 \n    19.6 \n    201 \n    4050 \n    male \n    2007 \n  \n\n Chinstrap \n    Dream \n    50.3 \n    20.0 \n    197 \n    3300 \n    male \n    2007 \n  \n\n Chinstrap \n    Dream \n    58.0 \n    17.8 \n    181 \n    3700 \n    female \n    2007 \n  \n\n Chinstrap \n    Dream \n    46.4 \n    18.6 \n    190 \n    3450 \n    female \n    2007 \n  \n\n Chinstrap \n    Dream \n    49.2 \n    18.2 \n    195 \n    4400 \n    male \n    2007 \n  \n\n Chinstrap \n    Dream \n    42.4 \n    17.3 \n    181 \n    3600 \n    female \n    2007 \n  \n\n Chinstrap \n    Dream \n    48.5 \n    17.5 \n    191 \n    3400 \n    male \n    2007 \n  \n\n Chinstrap \n    Dream \n    43.2 \n    16.6 \n    187 \n    2900 \n    female \n    2007 \n  \n\n Chinstrap \n    Dream \n    50.6 \n    19.4 \n    193 \n    3800 \n    male \n    2007 \n  \n\n Chinstrap \n    Dream \n    46.7 \n    17.9 \n    195 \n    3300 \n    female \n    2007 \n  \n\n Chinstrap \n    Dream \n    52.0 \n    19.0 \n    197 \n    4150 \n    male \n    2007 \n  \n\n Chinstrap \n    Dream \n    50.5 \n    18.4 \n    200 \n    3400 \n    female \n    2008 \n  \n\n Chinstrap \n    Dream \n    49.5 \n    19.0 \n    200 \n    3800 \n    male \n    2008 \n  \n\n Chinstrap \n    Dream \n    46.4 \n    17.8 \n    191 \n    3700 \n    female \n    2008 \n  \n\n Chinstrap \n    Dream \n    52.8 \n    20.0 \n    205 \n    4550 \n    male \n    2008 \n  \n\n Chinstrap \n    Dream \n    40.9 \n    16.6 \n    187 \n    3200 \n    female \n    2008 \n  \n\n Chinstrap \n    Dream \n    54.2 \n    20.8 \n    201 \n    4300 \n    male \n    2008 \n  \n\n Chinstrap \n    Dream \n    42.5 \n    16.7 \n    187 \n    3350 \n    female \n    2008 \n  \n\n Chinstrap \n    Dream \n    51.0 \n    18.8 \n    203 \n    4100 \n    male \n    2008 \n  \n\n Chinstrap \n    Dream \n    49.7 \n    18.6 \n    195 \n    3600 \n    male \n    2008 \n  \n\n Chinstrap \n    Dream \n    47.5 \n    16.8 \n    199 \n    3900 \n    female \n    2008 \n  \n\n Chinstrap \n    Dream \n    47.6 \n    18.3 \n    195 \n    3850 \n    female \n    2008 \n  \n\n Chinstrap \n    Dream \n    52.0 \n    20.7 \n    210 \n    4800 \n    male \n    2008 \n  \n\n Chinstrap \n    Dream \n    46.9 \n    16.6 \n    192 \n    2700 \n    female \n    2008 \n  \n\n Chinstrap \n    Dream \n    53.5 \n    19.9 \n    205 \n    4500 \n    male \n    2008 \n  \n\n Chinstrap \n    Dream \n    49.0 \n    19.5 \n    210 \n    3950 \n    male \n    2008 \n  \n\n Chinstrap \n    Dream \n    46.2 \n    17.5 \n    187 \n    3650 \n    female \n    2008 \n  \n\n Chinstrap \n    Dream \n    50.9 \n    19.1 \n    196 \n    3550 \n    male \n    2008 \n  \n\n Chinstrap \n    Dream \n    45.5 \n    17.0 \n    196 \n    3500 \n    female \n    2008 \n  \n\n Chinstrap \n    Dream \n    50.9 \n    17.9 \n    196 \n    3675 \n    female \n    2009 \n  \n\n Chinstrap \n    Dream \n    50.8 \n    18.5 \n    201 \n    4450 \n    male \n    2009 \n  \n\n Chinstrap \n    Dream \n    50.1 \n    17.9 \n    190 \n    3400 \n    female \n    2009 \n  \n\n Chinstrap \n    Dream \n    49.0 \n    19.6 \n    212 \n    4300 \n    male \n    2009 \n  \n\n Chinstrap \n    Dream \n    51.5 \n    18.7 \n    187 \n    3250 \n    male \n    2009 \n  \n\n Chinstrap \n    Dream \n    49.8 \n    17.3 \n    198 \n    3675 \n    female \n    2009 \n  \n\n Chinstrap \n    Dream \n    48.1 \n    16.4 \n    199 \n    3325 \n    female \n    2009 \n  \n\n Chinstrap \n    Dream \n    51.4 \n    19.0 \n    201 \n    3950 \n    male \n    2009 \n  \n\n Chinstrap \n    Dream \n    45.7 \n    17.3 \n    193 \n    3600 \n    female \n    2009 \n  \n\n Chinstrap \n    Dream \n    50.7 \n    19.7 \n    203 \n    4050 \n    male \n    2009 \n  \n\n Chinstrap \n    Dream \n    42.5 \n    17.3 \n    187 \n    3350 \n    female \n    2009 \n  \n\n Chinstrap \n    Dream \n    52.2 \n    18.8 \n    197 \n    3450 \n    male \n    2009 \n  \n\n Chinstrap \n    Dream \n    45.2 \n    16.6 \n    191 \n    3250 \n    female \n    2009 \n  \n\n Chinstrap \n    Dream \n    49.3 \n    19.9 \n    203 \n    4050 \n    male \n    2009 \n  \n\n Chinstrap \n    Dream \n    50.2 \n    18.8 \n    202 \n    3800 \n    male \n    2009 \n  \n\n Chinstrap \n    Dream \n    45.6 \n    19.4 \n    194 \n    3525 \n    female \n    2009 \n  \n\n Chinstrap \n    Dream \n    51.9 \n    19.5 \n    206 \n    3950 \n    male \n    2009 \n  \n\n Chinstrap \n    Dream \n    46.8 \n    16.5 \n    189 \n    3650 \n    female \n    2009 \n  \n\n Chinstrap \n    Dream \n    45.7 \n    17.0 \n    195 \n    3650 \n    female \n    2009 \n  \n\n Chinstrap \n    Dream \n    55.8 \n    19.8 \n    207 \n    4000 \n    male \n    2009 \n  \n\n Chinstrap \n    Dream \n    43.5 \n    18.1 \n    202 \n    3400 \n    female \n    2009 \n  \n\n Chinstrap \n    Dream \n    49.6 \n    18.2 \n    193 \n    3775 \n    male \n    2009 \n  \n\n Chinstrap \n    Dream \n    50.8 \n    19.0 \n    210 \n    4100 \n    male \n    2009 \n  \n\n Chinstrap \n    Dream \n    50.2 \n    18.7 \n    198 \n    3775 \n    female \n    2009"
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#basic-plot",
    "href": "content/labs/r-labs/graphics/wizardy.html#basic-plot",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "\n6 Basic Plot",
    "text": "6 Basic Plot\nA basic scatter plot, which we will progressively dress up:\n\n## simple plot: data + mappings + geometry\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(alpha = .6, size = 3.5)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#customized-plot",
    "href": "content/labs/r-labs/graphics/wizardy.html#customized-plot",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "\n7 Customized Plot",
    "text": "7 Customized Plot\nLet us set some ggplot theme aspects now!! Here is a handy picture showing ( most of ) the theme-able aspects of a ggplot plot.\nFor more info, type ?theme in your console.\n\n\nggplot Theme Elements\n\n\n\n## change global theme settings (for all following plots)\ntheme_set(theme_minimal(base_size = 12, base_family = \"open\"))\n\n## modify plot elements globally (for all following plots)\ntheme_update(\n  axis.ticks = element_line(color = \"grey92\"),\n  axis.ticks.length = unit(.5, \"lines\"),\n  panel.grid.minor = element_blank(),\n  legend.title = element_text(size = 12),\n  legend.text = element_text(color = \"grey30\"),\n  plot.title = element_text(size = 18, face = \"bold\"),\n  plot.subtitle = element_text(size = 12, color = \"grey30\"),\n  plot.caption = element_text(size = 9, margin = margin(t = 15))\n)\n\nSince we know what the basic plot looks like, let‚Äôs add titles, labels and colours. We will also set limits and scales:\n\nggplot(penguins, aes(x = bill_length_mm, \n                     y = bill_depth_mm)) +\n  \n  geom_point(aes(color = body_mass_g), alpha = .6, \n             size = 3.5) + \n  \n  ## custom axes scaling\n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  \n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5), \n                     limits = c(12.5, 22.5)) +\n  \n  ## custom colors from the scico package\n  scico::scale_color_scico(palette = \"bamako\", direction = -1) +\n  \n  ## custom labels\n  labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (Pygoscelis)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) PLoS ONE',\n    x = 'Bill Length (mm)', \n    y = 'Bill Depth (mm)',\n    color = 'Body mass (g)'\n  )"
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#using-ggtext",
    "href": "content/labs/r-labs/graphics/wizardy.html#using-ggtext",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "\n8 Using {ggtext}\n",
    "text": "8 Using {ggtext}\n\nFrom Claus Wilke‚Äôs website (www.wilkelab.org/ggtext)\n\nThe ggtext package provides simple Markdown and HTML rendering for ggplot2. Under the hood, the package uses the gridtext package for the actual rendering, and consequently it is limited to the feature set provided by gridtext.\nSupport is provided for Markdown both in theme elements (plot titles, subtitles, captions, axis labels, legends, etc.) and in geoms (similar to geom_text()). In both cases, there are two alternatives, one for creating simple text labels and one for creating text boxes with word wrapping.\n\nNOTE: on some machines, the ggtext package may not work as expected. In this case, please do as follows, using your Console:\n\nremove gridtext: remove.packages(gridtext).\nInstall development version of gridtext: remotes::install_github(\"wilkelab/gridtext\")\n\n\n\n8.1 element_markdown()\n\nWe can use our familiar markdown syntax right inside the titles and captions of the plot. element_markdown() is a theming command made available by the ggtext package.\nelement_markdown() ‚Üí formatted text elements, e.g.¬†titles, caption, axis text, strip text\n\n## assign plot to `gt` - we can add new things to this plot later\n## (wrapped in parenthesis so it is assigned and plotted in one step)\n\n(gt <- ggplot(penguins, aes(x = bill_length_mm, \n                            y = bill_depth_mm)) +\n    \n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5) +\n    \n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n    \n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5), \n                     limits = c(12.5, 22.5)) +\n    \n  scico::scale_color_scico(palette = \"bamako\", direction = -1) +\n   \n# New code starts here: Two Step Procedure with ggtext\n# 1. Markdown formatting of labels and title, using asterisks\n# To create italics and bold text in titles\n    \n    \n  labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) *PLoS ONE*',\n    \n    x = '**Bill Length** (mm)', \n    y = '**Bill Depth** (mm)',\n    \n    color = 'Body mass (g)'\n  ) +\n   \n  ## 2. Add theme related commands from ggtext\n  ## render respective text elements\n    \n  theme(\n    plot.title = ggtext::element_markdown(),\n    plot.caption = ggtext::element_markdown(),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown()\n  )\n)\n\n\n\n\n\n8.2 element_markdown() in combination with HTML\nThis allows us to change fonts in titles, labels, and captions:\n\n## use HTML syntax to change text color\ngt_mar <- gt +\n  labs(title = 'Bill Dimensions of Brush-Tailed Penguins <i style=\"color:#28A87D;\">Pygoscelis</i>') +\n  theme(plot.margin = margin(t = 25))\n\n\n## use HTML syntax to change font and text size\ngt_mar +\n  labs(title = 'Bill Dimensions of Brush-Tailed Penguins <b style=\"font-size:32pt;font-family:tangerine;\">Pygoscelis</b>')\n\n\n\n\n\n8.3 Adding images to ggplot\nSave an image from the web in the same folder as your RMarkdown. Use html tags to include it, say as part of your plot title, as shown below.\n\n## use HTML syntax to add images to text elements\ngt_mar + \n  labs(title = 'Bill Dimensions of Brush-Tailed Penguins &nbsp;&nbsp;&nbsp; <img src=\"images/culmen_depth.png\"‚Äö width=\"480\"/>')\n\n\n\n\n\n8.4 Annotations with geom_richtext() and geom_textbox()\n\nFurther ggplot annotations can be achieved using geom_richtext() and geom_textbox().\ngeom_richtext() also allows formatted text labels with 360¬∞ rotation. One needs to pass a dataframe to geom_richtext() giving the location, colour, rotation etc of the label annotation.\n\ngt_rich <- ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(aes(color = species), alpha = .6, size = 3.5) + \n  \n  ## add text annotations for each species\n  ggtext::geom_richtext(\n    data = tibble(\n      \n      # Three rich text labels, so three sets of locations x and y, and angle of rotation\n      x = c(34, 56, 54), y = c(20, 18.5, 14.5),\n            angle = c(12, 20, 335),\n      species = c(\"Ad√©lie\", \"Chinstrap\", \"Gentoo\"),\n      lab = c(\"<b style='font-family:anton;font-size:24pt;'>Ad√©lie</b><br><i style='color:darkgrey;'>P. ad√©liae</i>\", \n              \"<b style='font-family:anton;font-size:24pt;'>Chinstrap</b><br><i style='color:darkgrey;'>P. antarctica</i>\", \n              \"<b style='font-family:anton;font-size:24pt;'>Gentoo</b><br><i style='color:darkgrey;'>P. papua</i>\"),\n\n    ),\n    \n    \n    # Now pass these data variables as aesthetics\n    aes(x, y, label = lab, color = species, angle = angle), \n    \n    size = 4, fill = NA, label.color = NA,\n    lineheight = .3\n  ) +\n  \n  \n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5), \n                     limits = c(12.5, 22.5)) +\n  \n  rcartocolor::scale_color_carto_d(palette = \"Bold\", \n                                   guide = \"none\") +\n  \n  labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) *PLoS ONE*',\n    x = '**Bill Length** (mm)', \n    y = '**Bill Depth** (mm)',\n    color = 'Body mass (g)'\n  ) +\n  \n  # Use theme and element_markdown() to format axes and titles as usual\n  theme(\n    plot.title = ggtext::element_markdown(),\n    plot.caption = ggtext::element_markdown(),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown(),\n    plot.margin = margin(25, 6, 15, 6)\n  )\n  \ngt_rich\n\n\n\n\n\n8.5 Formatted Text boxes on ggplots\nelement_textbox() and element_textbox_simple() ‚Üí formatted text boxes with word wrapping\n\ngt_box <- ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(aes(color = species), alpha = .6, size = 3.5) +\n  \n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  \n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5),\n                     limits = c(12.5, 22.5)) +\n  \n  rcartocolor::scale_color_carto_d(palette = \"Bold\", guide = \"none\") +\n  \n  ## add text annotations for each species\n  ## Creating a tibble for the labels!\n  ggtext::geom_richtext(\n    data = tibble(\n      # Three rich text labels\n      # So three sets of locations x and y, and angle of rotation\n      x = c(34, 56, 54),\n      y = c(20, 18.5, 14.5),\n      angle = c(12, 20, 335),\n      species = c(\"Ad√©lie\", \"Chinstrap\", \"Gentoo\"),\n      notes = c(\n        \"<b style='font-family:anton;font-size:24pt;'>Ad√©lie</b><br><i style='color:darkgrey;'>P. ad√©liae</i>\",\n        \"<b style='font-family:anton;font-size:24pt;'>Chinstrap</b><br><i style='color:darkgrey;'>P. antarctica</i>\",\n        \"<b style='font-family:anton;font-size:24pt;'>Gentoo</b><br><i style='color:darkgrey;'>P. papua</i>\"\n      )\n      ),\n    \n    \n    # Now pass these data variables as aesthetics\n    aes(\n      x,\n      y,\n      label = notes,\n      color = species,\n      angle = angle\n    ),\n    \n    size = 4,\n    fill = NA,\n    label.color = NA,\n    lineheight = .3\n  ) +\n  \n  \n# Now for the Plot Titles and Labels, as before  \n  labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) *PLoS ONE*',\n    x = '**Bill Length** (mm)',\n    y = '**Bill Depth** (mm)',\n    color = 'Body mass (g)'\n  ) +\n  \n# Add the ggtext theme related commands\n  theme(\n    ## turn title into filled textbox\n    plot.title = ggtext::element_textbox_simple(\n      color = \"white\",\n      fill = \"#28A78D\",\n      size = 32,\n      padding = margin(8, 4, 8, 4),\n      margin = margin(b = 5),\n      lineheight = .9\n    ),\n    ## add round outline to caption\n    plot.caption = ggtext::element_textbox_simple(\n      width = NULL,\n      linetype = 1,\n      padding = margin(4, 8, 4, 8),\n      margin = margin(t = 15),\n      r = grid::unit(8, \"pt\")\n    ),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown(),\n    plot.margin = margin(25, 6, 15, 6)\n  )\n\ngt_box\n\n\n\n\ngeom_textbox() ‚Üí formatted text boxes with word wrapping\n\ngt_box +\n  ## add textbox with long paragraphs\n  ggtext::geom_textbox(\n    data = tibble(x = 34, y = 13.7, label = \"<span style='font-size:12pt;font-family:anton;'>Lorem Ipsum Dolor Sit Amet</span><br><br>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\"),\n    aes(x, y, label = label),\n    size = 2.2, family = \"sans\",\n    fill = \"cornsilk\", box.color = \"cornsilk3\",\n    width = unit(11, \"lines\")\n  ) +\n  coord_cartesian(clip = \"off\") # ensure no clipping of labels near the edge"
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#using-ggforce",
    "href": "content/labs/r-labs/graphics/wizardy.html#using-ggforce",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "\n9 Using {ggforce}\n",
    "text": "9 Using {ggforce}\n\nFrom Thomas Lin Pedersen‚Äôs website ‚Üí www.ggforce.data-imaginist.com\n\nggforce is a package aimed at providing missing functionality to ggplot2 through the extension system introduced with ggplot2 v2.0.0. Broadly speaking ggplot2 has been aimed primarily at explorative data visualization in order to investigate the data at hand, and less at providing utilities for composing custom plots a la D3.js. ggforce is mainly an attempt to address these ‚Äúshortcoming‚Äù (design choices might be a better description). The goal is to provide a repository of geoms, stats, etc. that are as well documented and implemented as the official ones found in ggplot2.\n\nWe will start with the basic plot, with the ggtext related work done up to now:\n\n## use ggtext rendering for the following plots\ntheme_update(\n  plot.title = ggtext::element_markdown(),\n  plot.caption = ggtext::element_markdown(),\n  axis.title.x = ggtext::element_markdown(),\n  axis.title.y = ggtext::element_markdown()\n)\n\n\n## plot that we will annotate with ggforce afterwards\n(gf <- ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) + \n  scico::scale_color_scico(palette = \"bamako\", direction = -1) +\n  coord_cartesian(xlim = c(25, 65), ylim = c(10, 25)) +\n  rcartocolor::scale_fill_carto_d(palette = \"Bold\") +\n  labs(\n    title = \"Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)\",\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE*\",\n    x = \"**Bill Length** (mm)\", \n    y = \"**Bill Depth** (mm)\",\n    color = \"Body mass (g)\",\n    fill = \"Species\"\n  ))\n\n\n\n\n\n## ellipsoids for all groups\n(gf +\n  ggforce::geom_mark_ellipse(\n    aes(fill = species, label = species), \n    alpha = .15, show.legend = FALSE\n  ) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5)\n)\n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` in the `default_aes` field and elsewhere instead.\n\n\n\n\n\n\n## ellipsoids for specific subset\n(gf +\n  ggforce::geom_mark_ellipse(\n    aes(fill = species, label = species, filter = species == 'Gentoo'), \n    alpha = 0, show.legend = FALSE\n  ) +\n geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5)\n#   coord_cartesian(xlim = c(25, 65), ylim = c(10, 25))\n)\n\n\n\n\n\n## circles\n(gf +\n  ggforce::geom_mark_circle(\n    aes(fill = species, label = species, filter = species == 'Gentoo'), \n    alpha = 0, show.legend = FALSE\n  ) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5)\n)\n\n\n\n\n\n## rectangles\n(gf +\n  ggforce::geom_mark_rect(\n    aes(fill = species, label = species, filter = species == 'Gentoo'), \n    alpha = 0, show.legend = FALSE\n  ) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5) \n)\n\n\n\n\n\nlibrary(concaveman)\n## hull\n(gf +\n  ggforce::geom_mark_hull(\n    aes(fill = species, label = species, filter = species == 'Gentoo'), \n    alpha = 0, show.legend = FALSE\n  ) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5)\n)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#ggplot-tricks",
    "href": "content/labs/r-labs/graphics/wizardy.html#ggplot-tricks",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "\n10 ggplot tricks",
    "text": "10 ggplot tricks\n\n(gg0 <- \n  ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n    ggforce::geom_mark_ellipse(\n      aes(fill = species, label = species), \n      alpha = 0, show.legend = FALSE\n    ) +\n    geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5) + \n    scale_x_continuous(breaks = seq(25, 65, by = 5), limits = c(25, 65)) +\n    scale_y_continuous(breaks = seq(12, 24, by = 2), limits = c(12, 24)) +\n    scico::scale_color_scico(palette = \"bamako\", direction = -1) +\n    labs(\n      title = \"Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)\",\n      subtitle = 'A scatter plot of bill depth versus bill length.',\n      caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE*\",\n      x = \"Bill Length (mm)\", \n      y = \"Bill Depth (mm)\",\n      color = \"Body mass (g)\"\n    )\n)\n\n\n\n\n\n10.1 Left-Aligned Title\n\n(gg1 <- gg0 + theme(plot.title.position = \"plot\"))\n\n\n\n\n\n10.2 Right-Aligned Caption\n\n(gg1b <- gg1 +  theme(plot.caption.position = \"panel\"))\n\n\n\n\n\n10.3 Legend Design\n\n(gg2 <- gg1b + theme(legend.position = \"top\"))\n\n\n\nggsave(\"06a_legend_position.pdf\", width = 9, height = 8, device = cairo_pdf)\n\n(gg2b <- gg2 + \n  guides(color = guide_colorbar(title.position = \"top\", \n                                title.hjust = .5, \n                                barwidth = unit(20, \"lines\"), \n                                barheight = unit(.5, \"lines\"))))\n\n\n\n\n\n10.4 Add Images\n\n## read PNG file from web\npng <- magick::image_read(\"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/man/figures/culmen_depth.png\")\n## turn image into `rasterGrob`\nimg <- grid::rasterGrob(png, interpolate = TRUE)\n\ngg3 <- gg2b +\n  annotation_custom(img, ymin = 18.5, ymax = 30.5, xmin = 55, xmax = 65.5) +\n    labs(caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE* &bull; Illustration: Allison Horst\")\ngg3\n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` in the `default_aes` field and elsewhere instead."
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#using-patchwork",
    "href": "content/labs/r-labs/graphics/wizardy.html#using-patchwork",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "\n11 Using {patchwork}\n",
    "text": "11 Using {patchwork}\n\n\nThe goal of patchwork is to make it ridiculously simple to combine separate ggplots into the same graphic. As such it tries to solve the same problem as gridExtra::grid.arrange() and cowplot::plot_grid but using an API that incites exploration and iteration, and scales to arbitrily complex layouts.\n\n‚Üí https://patchwork.data-imaginist.com/\nLet us make two plots and combine them into a single patchwork plot.\n\n## calculate bill ratio\npenguins_stats <- penguins %>% \n  mutate(bill_ratio = bill_length_mm / bill_depth_mm) %>% \n  filter(!is.na(bill_ratio))\n\n## create a second chart\ngg4 <- ggplot(penguins_stats, aes(y = bill_ratio, x = species, fill = species, color = species)) + geom_violin() + \n  labs(\n    y = \"Bill ratio\",\n    x = \"Species\",\n    subtitle = \"\",\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE* &bull; Illustration: Allison Horst\"\n  ) +\n  theme(\n    panel.grid.major.x = element_line(size = .35),\n    panel.grid.major.y = element_blank(),\n    axis.text.y = element_text(size = 13),\n    axis.ticks.length = unit(0, \"lines\"),\n    plot.title.position = 'plot',\n    plot.subtitle = element_text(margin = margin(t = 5, b = 10)),\n    plot.margin = margin(10, 25, 10, 25)\n  )\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\n‚Ñπ Please use the `linewidth` argument instead.\n\n\nNow to combine both plots into one using simple operators:\n\nFor the special case of putting plots besides each other or on top of each other patchwork provides 2 shortcut operators. | will place plots next to each other while / will place them on top of each other.\n\nFirst we stack up the graphs side by side:\n\n## combine both plots\ngg3 | (gg4 + labs(title = \"Bill Ratios of Brush-Tailed Penguins\",\n                  subtitle = \"Violin Plots of Bill Ration versus species\"))\n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` in the `default_aes` field and elsewhere instead.\n\n\n\n\n\nWe can place them in one column:\n\ngg3 / (gg3 + labs(title = \"Bill Ratios of Brush-Tailed Penguins\",\n                  subtitle = \"Violin Plots of Bill Ration versus species\")) +\n  plot_layout(heights = c(0.4, 0.4))"
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#resources",
    "href": "content/labs/r-labs/graphics/wizardy.html#resources",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "\n12 Resources",
    "text": "12 Resources\n\n\nIntro to R (one of many good online tutorials)\n‚ÄúR for Data Science‚Äù book (open-access)\nggplot2 Book (open-access)\nR Graph Gallery\nSlides of Cedric Scherer‚Äôs talk\nExtensive ggplot2 tutorial\n‚ÄúEvolution of a ggplot‚Äù blog post by Cedric Scherer\n\n#TidyTuesday project (#TidyTuesday on Twitter)\n\n#TidyTuesday Contributions by Cedric Scherer incl.¬†all codes\n\nR4DS learning community (huge Slack community for people learning R incl.¬†a mentoring program)\n\nIllustrations by Allison Horst (more general about data and stats + R-related)\nR Packages:\n\nggplot2\nggtext\nggforce\nggdist\nggraph\nggstream\nggbump\ngggibous\nwaffle\ngeofacet\ncartogram\npatchwork\nsf"
  },
  {
    "objectID": "content/labs/r-labs/installation/installation.html",
    "href": "content/labs/r-labs/installation/installation.html",
    "title": "Lab 01 - Introduce Yourself",
    "section": "",
    "text": "At the end of this Lab, we will:\n\nhave installed R and RStudio on our machines\nunderstood how to add additional R-packages for specific features and graphic capability\nrun code within RStudio and interpret the results\nhave learnt to look for help within R and RStudio"
  },
  {
    "objectID": "content/labs/r-labs/installation/installation.html#Check-In-R",
    "href": "content/labs/r-labs/installation/installation.html#Check-In-R",
    "title": "Lab 01 - Introduce Yourself",
    "section": "\n3.1 Check in",
    "text": "3.1 Check in\nLaunch R by clicking this logo. You should see one console with a command line interpreter. Try typing 2 + 2 and check !\nClose R."
  },
  {
    "objectID": "content/labs/r-labs/installation/installation.html#Check-In-RStudio",
    "href": "content/labs/r-labs/installation/installation.html#Check-In-RStudio",
    "title": "Lab 01 - Introduce Yourself",
    "section": "\n4.1 Check in",
    "text": "4.1 Check in\nLaunch RStudio. You should get a window similar to the screenshot you see here, but yours will be empty. Look at the bottom left pane: this is the same console window you saw when you opened R in step @Check-In-R\n\nPlace your cursor where you see > and type x <- 2 + 2 again hit enter or return, then type x, and hit enter/return again.\nIf [1] 4 prints to the screen, you have successfully installed R and RStudio, and you can move onto installing packages."
  },
  {
    "objectID": "content/labs/r-labs/installation/installation.html#save-and-share",
    "href": "content/labs/r-labs/installation/installation.html#save-and-share",
    "title": "Lab 01 - Introduce Yourself",
    "section": "\n7.1 Save and share",
    "text": "7.1 Save and share\nSave your work so you can share your favorite plot with us. You will not like the looks of your plot if you mouse over to Export and save it. Instead, use ggplot2‚Äôs command for saving a plot with sensible defaults:\n\nhelp(ggsave)\n\n\nggsave(\"file_name_here.pdf\", plot) # please make the filename unique!"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html",
    "href": "content/labs/r-labs/maps/gram-maps.html",
    "title": "The Grammar of Maps",
    "section": "",
    "text": "This RMarkdown document is part of my Workshop Course in R. The intent is to build Skill in coding in R, and also appreciate R as a way to metaphorically visualize information of various kinds, using predominantly geometric figures and structures.\nAll RMarkdown/Quarto files combine code, text, web-images, and figures developed using code. Everything is text; code chunks are enclosed in fences (```)"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#set-up",
    "href": "content/labs/r-labs/maps/gram-maps.html#set-up",
    "title": "The Grammar of Maps",
    "section": "\n3.1 Set Up",
    "text": "3.1 Set Up\nThe setup code chunk below brings into our coding session R packages that provide specific computational abilities and also datasets which we can use.\nTo reiterate: Packages and datasets are not the same thing !! Packages are (small) collections of programs. Datasets are just‚Ä¶.information."
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#using-open-street-map-osm",
    "href": "content/labs/r-labs/maps/gram-maps.html#using-open-street-map-osm",
    "title": "The Grammar of Maps",
    "section": "\n7.1 Using Open Street Map (OSM)",
    "text": "7.1 Using Open Street Map (OSM)\n\nOpenStreetMap (OSM) provides maps of the world mostly created by volunteers. They are completely free to browse and use, with attribution to ¬© OpenStreetMap contributors and adherence to the ODbL license required, and are used by many public and private organisations. OSM data can be downloaded in vector format and used for our own purposes. In this tutorial, we will obtain data from OSM using a query. A query is a request for data from a database. Simple queries can be performed more easily using the osmdata library for R, which automatically constructs the query and imports the data in a convenient format."
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#osm-feature-key-value-pairs",
    "href": "content/labs/r-labs/maps/gram-maps.html#osm-feature-key-value-pairs",
    "title": "The Grammar of Maps",
    "section": "\n7.2 OSM feature key-value pairs",
    "text": "7.2 OSM feature key-value pairs\nOpen Street Map features have attributes in key-value pairs. We can use them to download the specific data we need. These features can easily be explored in the web browser, by using the ‚ÄòQuery features‚Äô button on OpenStreetMap (OSM):\n\nHead off to OSM Street Map to try this out and to get an intuitive understanding of what OSM key-value pairs are, for different types of map features. Look for places of interest to you (features) and see what key-value pairs attach to those features.\nNOTE: key-value pairs are also referred to as tags.\nUseful key-value pairs / tags include:\n\n\n\n\n\n\nKEY\nVALUEs\n\n\n\nbuilding\nyes (all), house residential, apartments\n\n\nhighway\nresidential, service, track, unclassified, footway, path\n\n\namenity\nparking, parking_space, bench; place_of_worship; restaurant, cafe, fast_food; school, waste_basket, fuel, bank, toilets‚Ä¶\n\n\nshop\nconvenience, supermarket, clothes, hairdresser, car-repair‚Ä¶\n\n\nname\nactual name of the place e.g.¬†Main_Street, McDonald‚Äôs, Pizza Hut, Subway\n\n\n\nwaterway\n\n\n\nnatural\n\n\n\nboundary\n\n\n\n\nFor more information see:\nSee OSM Tags for a nice visual description of popular key-value pairs that we can use. See what the highway tag looks like tag : highway"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#downloading-data-from-open-street-map",
    "href": "content/labs/r-labs/maps/gram-maps.html#downloading-data-from-open-street-map",
    "title": "The Grammar of Maps",
    "section": "\n7.3 Downloading Data from Open Street Map",
    "text": "7.3 Downloading Data from Open Street Map\nNow we know the map features we are interested in. We also know what key-value pairs will be used to get this info from OSM. We will get our map data from OSM and then save it avoid repeated downloads. So, please copy/paste and run the following commands in your console.\nDo not run these commands too many times. Re-run this ONLY if you have changed your BOUNDING BOX.\n\n# Eval is set to false here\n# This code is for reference\n# Run these commands ONCE in your Console\n\n# Get all buildings within my bbox\ndat_buildings <-extract_osm_objects (key = \"building\", \n                                     bbox = bbox_2)\n\n# Get all residential roads within my bbox\ndat_roads <- extract_osm_objects (key = 'highway', \n                                     value = c(\"residential\"),\n                                     bbox = bbox_2)\n\n# Get all parks within my bbox\ndat_parks <- extract_osm_objects (key = 'park', \n                                  bbox = bbox_2)\n\n# Get all green areas within my bbox\ndat_greenery <- extract_osm_objects (key = 'landuse', \n                                  value = 'grass', \n                                  bbox = bbox_2)\n\ndat_trees <- extract_osm_objects (key = 'natural', \n                                  value = 'tree', \n                                  bbox = bbox_2)\n\n\n7.3.1 Let us save this data, so we don‚Äôt need to download all this again!\nWe will store the downloaded data as .gpkg files on our local hard drives to use when we run this file again later. We will name our stored files as buildings, parks, roads, and trees, each with the .gpkg file extension, e.g.¬†trees.gpkg.\nCheck your local project folder for these files after executing these commands.\n\n# Eval is set to false here\n# This code is for reference\n# Run these commands ONCE in your Console\n\nst_write(dat_buildings, \n         dsn = \"buildings.gpkg\", \n         append = FALSE, \n         quiet = FALSE)\n\nst_write(dat_parks, dsn = \"parks.gpkg\", \n         append = FALSE, quiet = FALSE)\n\nst_write(dat_greenery, dsn = \"greenery.gpkg\", \n         append = FALSE,quiet = FALSE)\n\nst_write(dat_trees, dsn = \"trees.gpkg\", \n         append = FALSE,quiet = FALSE)\n\nst_write(dat_roads, dsn = \"roads.gpkg\", \n         append = FALSE, quiet = FALSE)\n\nAlways work from here to avoid repeated downloads from OSM. Start from the top ONLY if you intend to map new locations and need to modify your Bounding Box.\n\n7.3.2 Reading Back the saved Data\n\nbuildings <- st_read(\"./buildings.gpkg\")\n\nReading layer `buildings' from data source \n  `C:\\Users\\Arvind\\Documents\\R work\\MyWebsites\\my-quarto-website\\content\\labs\\r-labs\\maps\\buildings.gpkg' \n  using driver `GPKG'\nSimple feature collection with 34766 features and 89 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 77.56221 ymin: 12.90906 xmax: 77.60373 ymax: 12.9497\nGeodetic CRS:  WGS 84\n\nparks <- st_read(\"./parks.gpkg\")\n\nReading layer `parks' from data source \n  `C:\\Users\\Arvind\\Documents\\R work\\MyWebsites\\my-quarto-website\\content\\labs\\r-labs\\maps\\parks.gpkg' \n  using driver `GPKG'\nSimple feature collection with 62 features and 25 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 77.56196 ymin: 12.90692 xmax: 77.60389 ymax: 12.95397\nGeodetic CRS:  WGS 84\n\ngreenery <- st_read(\"./greenery.gpkg\")\n\nReading layer `greenery' from data source \n  `C:\\Users\\Arvind\\Documents\\R work\\MyWebsites\\my-quarto-website\\content\\labs\\r-labs\\maps\\greenery.gpkg' \n  using driver `GPKG'\nSimple feature collection with 2 features and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 77.56776 ymin: 12.91751 xmax: 77.57392 ymax: 12.94811\nGeodetic CRS:  WGS 84\n\ntrees <- st_read(\"./trees.gpkg\")\n\nReading layer `trees' from data source \n  `C:\\Users\\Arvind\\Documents\\R work\\MyWebsites\\my-quarto-website\\content\\labs\\r-labs\\maps\\trees.gpkg' \n  using driver `GPKG'\nSimple feature collection with 153 features and 9 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 77.56566 ymin: 12.90806 xmax: 77.60096 ymax: 12.94914\nGeodetic CRS:  WGS 84\n\nroads <- st_read(\"./roads.gpkg\")\n\nReading layer `roads' from data source \n  `C:\\Users\\Arvind\\Documents\\R work\\MyWebsites\\my-quarto-website\\content\\labs\\r-labs\\maps\\roads.gpkg' \n  using driver `GPKG'\nSimple feature collection with 2242 features and 28 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 77.55895 ymin: 12.90635 xmax: 77.60603 ymax: 12.95636\nGeodetic CRS:  WGS 84\n\n\n\n7.3.3 Let‚Äôs look at the data\nHow many rows? ( Rows -> Features ) What kind of geom column?\n\n# How many buildings?\nnrow(buildings)\n\n[1] 34766\n\nbuildings$geom\n\nGeometry set for 34766 features \nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 77.56221 ymin: 12.90906 xmax: 77.60373 ymax: 12.9497\nGeodetic CRS:  WGS 84\nFirst 5 geometries:\n\n\nPOLYGON ((77.58405 12.93005, 77.5845 12.93005, ...\n\n\nPOLYGON ((77.57568 12.9199, 77.57592 12.9199, 7...\n\n\nPOLYGON ((77.59592 12.94016, 77.59676 12.94022,...\n\n\nPOLYGON ((77.5937 12.94011, 77.59458 12.94015, ...\n\n\nPOLYGON ((77.59321 12.94042, 77.59321 12.94035,...\n\nclass(buildings$geom)\n\n[1] \"sfc_POLYGON\" \"sfc\"        \n\n\nSo the buildings dataset has 34766 buildings and their geometry is naturally a POLYGON type of geometry column.\n\n7.3.4 Your Turn 1\nDo this check for all the other spatial data, in the code chunk below. What kind of geom column does each dataset have?\n\n\n\n\n7.3.5 What Other Kinds of Data could we get from OSM?\nosm_structures returns a data.frame of OSM structure types, associated key-value pairs and unique suffixes which may be appended to data structures/filenames for storage purposes, and suggested colours.\n\nosmplotr::osm_structures()\n\n\n\n  \n\n\n\nWe can use these key-value pairs to download different types of map data."
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#ggplot-and-geom_sf",
    "href": "content/labs/r-labs/maps/gram-maps.html#ggplot-and-geom_sf",
    "title": "The Grammar of Maps",
    "section": "\n8.1 ggplot and geom_sf()",
    "text": "8.1 ggplot and geom_sf()\nFirst we will plot with ggplot and geom_sf() : recall that our data is stored in 5 files: buildings, parks, roads, trees, and greenery.\n\nggplot() +\n  geom_sf(data = buildings, colour = \"orange\") +    # POLYGONS\n  geom_sf(data = roads, col = \"gray20\") +           # LINES\n  geom_sf(data = parks, col = \"darkseagreen1\") +    # POLYGONS\n  geom_sf(data = greenery, col = \"darkseagreen\") +  # POLYGONS\n  geom_sf(data = trees, col = \"green\")              # POINTS\n\n\n\n\nNote how geom_sf is capable of handling any geometry in the sfc column !!\n\ngeom_sf() is an unusual geom because it will draw different geometric objects depending on what simple features are present in the data: you can get points, lines, or polygons.\n\nSo there, we have our first map!"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#map-using-tmap-package",
    "href": "content/labs/r-labs/maps/gram-maps.html#map-using-tmap-package",
    "title": "The Grammar of Maps",
    "section": "\n8.2 Map using tmap package",
    "text": "8.2 Map using tmap package\nWe can also create a map using a package called tmap. Here we also have the option of making the map interactive. tmap plots are made with code in ‚Äúgroups‚Äù: each group starts with a tm_shape() command.\n\n# Group-1\ntm_shape(buildings) +\n  tm_fill(col = \"mediumblue\") +\n\n#Group-2\ntm_shape(roads) +\n  tm_lines(col = \"gold\") +\n\n#Group-3  \ntm_shape(greenery) +\n  tm_polygons(col = \"limegreen\") +\n  \n#Group-4  \ntm_shape(parks) +\n  tm_polygons(col = \"limegreen\") +\n\n#Group-5  \ntm_shape(trees) +\n  tm_dots(col = \"green\")\n\n\n\n\nHow do we make this map interactive? One more line of code !! Add this line in your console and then run the above chunk again\ntmap_mode(\"view\")"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#using-data-from-rnaturalearth",
    "href": "content/labs/r-labs/maps/gram-maps.html#using-data-from-rnaturalearth",
    "title": "The Grammar of Maps",
    "section": "\n9.1 Using data from rnaturalearth\n",
    "text": "9.1 Using data from rnaturalearth\n\nThe rnaturalearth package allows us to download shapes of countries. We can use it to get borders and also internal state/district boundaries.\n\nindia <- \n  ne_states(country =  \"india\", \n            returnclass = \"sf\") # gives a ready sf dataframe !\n\nindia_neighbours <- \n  ne_states(country = (c(\"sri lanka\", \"pakistan\",\n                         \"afghanistan\", \"nepal\",\"bangladesh\", \"bhutan\")\n                       ),\n            returnclass = \"sf\")\n\nLet‚Äôs look at the attribute variable columns to colour our graph and to shape our symbols:\n\nnames(india)\n\n [1] \"featurecla\" \"scalerank\"  \"adm1_code\"  \"diss_me\"    \"iso_3166_2\"\n [6] \"wikipedia\"  \"iso_a2\"     \"adm0_sr\"    \"name\"       \"name_alt\"  \n[11] \"name_local\" \"type\"       \"type_en\"    \"code_local\" \"code_hasc\" \n[16] \"note\"       \"hasc_maybe\" \"region\"     \"region_cod\" \"provnum_ne\"\n[21] \"gadm_level\" \"check_me\"   \"datarank\"   \"abbrev\"     \"postal\"    \n[26] \"area_sqkm\"  \"sameascity\" \"labelrank\"  \"name_len\"   \"mapcolor9\" \n[31] \"mapcolor13\" \"fips\"       \"fips_alt\"   \"woe_id\"     \"woe_label\" \n[36] \"woe_name\"   \"latitude\"   \"longitude\"  \"sov_a3\"     \"adm0_a3\"   \n[41] \"adm0_label\" \"admin\"      \"geonunit\"   \"gu_a3\"      \"gn_id\"     \n[46] \"gn_name\"    \"gns_id\"     \"gns_name\"   \"gn_level\"   \"gn_region\" \n[51] \"gn_a1_code\" \"region_sub\" \"sub_code\"   \"gns_level\"  \"gns_lang\"  \n[56] \"gns_adm1\"   \"gns_region\" \"min_label\"  \"max_label\"  \"min_zoom\"  \n[61] \"wikidataid\" \"name_ar\"    \"name_bn\"    \"name_de\"    \"name_en\"   \n[66] \"name_es\"    \"name_fr\"    \"name_el\"    \"name_hi\"    \"name_hu\"   \n[71] \"name_id\"    \"name_it\"    \"name_ja\"    \"name_ko\"    \"name_nl\"   \n[76] \"name_pl\"    \"name_pt\"    \"name_ru\"    \"name_sv\"    \"name_tr\"   \n[81] \"name_vi\"    \"name_zh\"    \"ne_id\"      \"geometry\"  \n\nnames(india_neighbours)\n\n [1] \"featurecla\" \"scalerank\"  \"adm1_code\"  \"diss_me\"    \"iso_3166_2\"\n [6] \"wikipedia\"  \"iso_a2\"     \"adm0_sr\"    \"name\"       \"name_alt\"  \n[11] \"name_local\" \"type\"       \"type_en\"    \"code_local\" \"code_hasc\" \n[16] \"note\"       \"hasc_maybe\" \"region\"     \"region_cod\" \"provnum_ne\"\n[21] \"gadm_level\" \"check_me\"   \"datarank\"   \"abbrev\"     \"postal\"    \n[26] \"area_sqkm\"  \"sameascity\" \"labelrank\"  \"name_len\"   \"mapcolor9\" \n[31] \"mapcolor13\" \"fips\"       \"fips_alt\"   \"woe_id\"     \"woe_label\" \n[36] \"woe_name\"   \"latitude\"   \"longitude\"  \"sov_a3\"     \"adm0_a3\"   \n[41] \"adm0_label\" \"admin\"      \"geonunit\"   \"gu_a3\"      \"gn_id\"     \n[46] \"gn_name\"    \"gns_id\"     \"gns_name\"   \"gn_level\"   \"gn_region\" \n[51] \"gn_a1_code\" \"region_sub\" \"sub_code\"   \"gns_level\"  \"gns_lang\"  \n[56] \"gns_adm1\"   \"gns_region\" \"min_label\"  \"max_label\"  \"min_zoom\"  \n[61] \"wikidataid\" \"name_ar\"    \"name_bn\"    \"name_de\"    \"name_en\"   \n[66] \"name_es\"    \"name_fr\"    \"name_el\"    \"name_hi\"    \"name_hu\"   \n[71] \"name_id\"    \"name_it\"    \"name_ja\"    \"name_ko\"    \"name_nl\"   \n[76] \"name_pl\"    \"name_pt\"    \"name_ru\"    \"name_sv\"    \"name_tr\"   \n[81] \"name_vi\"    \"name_zh\"    \"ne_id\"      \"geometry\"  \n\n# Look only at attributes\nindia %>% st_drop_geometry() %>% head()\n\n\n\n  \n\n\nindia_neighbours %>% st_drop_geometry() %>% head()\n\n\n\n  \n\n\n\nIn the india data frame:\n- Column iso_a2 contains the country name.\n- Column name contains the name of the state\nIn the india_neighbours data frame:\n- Column gu_a3 contains the country abbreviation\n- Column name contains the name of the state\n- Column iso_3166_2 contains the abbreviation of the state within each neighbouring country.\n\n9.1.1 Map 1\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\n# Plot India\n  tm_shape(india) +\n  tm_polygons(\"name\", # Colour by States in India\n              legend.show = FALSE) +\n  \n# Plot Neighbours\n  tm_shape(india_neighbours) +\n  tm_fill(col = \"gu_a3\") +  # Colour by Country Name\n  \n# Plot the cities in India alone\n  tm_shape(metro %>% dplyr::filter(iso_a3 == \"IND\")) +\n    \n  tm_dots(size = \"pop2020\",legend.size.show = FALSE) +\n    # size by population in 2020\n    \n  tm_layout(legend.show = FALSE) +\n  tm_credits(\"Geographical Boundaries are not accurate\",\n             size = 0.5,\n             position = \"right\") +\n  tm_compass(position = c(\"right\", \"top\")) +\n  tm_scale_bar(position = \"left\") +\n  tmap_style(style = \"classic\") \n\nold-style crs object detected; please recreate object with a recent sf::st_crs()\n\n\ntmap style set to \"classic\"\n\n\nother available styles are: \"white\", \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"watercolor\" \n\n\nCredits not supported in view mode.\n\n\nCompass not supported in view mode.\n\n\nWarning: Number of levels of the variable \"name\" is 35, which is larger than\nmax.categories (which is 30), so levels are combined. Set\ntmap_options(max.categories = 35) in the layer function to show all levels.\n\n\n\n\n\n\n#Try other map styles\n#cobalt #gray #white #watercolor #beaver #classic #watercolor #albatross #bw #col_blind"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#your-turn-2",
    "href": "content/labs/r-labs/maps/gram-maps.html#your-turn-2",
    "title": "The Grammar of Maps",
    "section": "\n9.2 Your Turn 2",
    "text": "9.2 Your Turn 2\nCan you try to download a map area of your home town and plot it as we have above?"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#adding-my-favourite-restaurants-to-the-map",
    "href": "content/labs/r-labs/maps/gram-maps.html#adding-my-favourite-restaurants-to-the-map",
    "title": "The Grammar of Maps",
    "section": "\n9.3 Adding my favourite Restaurants to the map",
    "text": "9.3 Adding my favourite Restaurants to the map\nIs it time to order on Swiggy‚Ä¶\nLet us adding interesting places to our map: say based on your favourite restaurants etc. We need restaurant data: lat/long + name + maybe type of restaurant. This can be manually created ( like all of OSMdata ) or if it is already there we can download using key-value pairs in our OSM data query.\nRestaurants can be downloaded using key= \"amenity\", value = \"restaurant\". Since we want JUST their location, and not the restaurant BUILDINGs, we say return_type = \"points\".\nThere are also other tags to explore!Searching for McDonalds for instance‚Ä¶( key = \"name\", value = \"McDonalds\")\n\n# Again, run these commands in your Console\ndat_R <- extract_osm_objects(bbox = bbox_2, \n                             key = \"amenity\", \n                             value = \"restaurant\", \n                             return_type = \"point\") #<<\n\n# Save the data for future use\nwrite_sf(dat_R, dsn = \"restaurants.gpkg\",append = FALSE, quiet = FALSE)\n\nNote the return_type parameter: we want the location and not the building in which the restaurant is!!"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#reading-the-saved-restaurant-data",
    "href": "content/labs/r-labs/maps/gram-maps.html#reading-the-saved-restaurant-data",
    "title": "The Grammar of Maps",
    "section": "\n9.4 Reading the saved Restaurant Data",
    "text": "9.4 Reading the saved Restaurant Data\n\nrestaurants <- st_read(\"./restaurants.gpkg\")\n\nReading layer `restaurants' from data source \n  `C:\\Users\\Arvind\\Documents\\R work\\MyWebsites\\my-quarto-website\\content\\labs\\r-labs\\maps\\restaurants.gpkg' \n  using driver `GPKG'\nSimple feature collection with 203 features and 33 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 77.56373 ymin: 12.9105 xmax: 77.60104 ymax: 12.94917\nGeodetic CRS:  WGS 84\n\n\nHow many restaurants have we got?\n\nrestaurants %>% nrow()\n\n[1] 203\n\n\nSo the restaurants dataset has 203 restaurants and their geometry is naturally a POINT type of geometry column.\nThese are the names of columns in the Restaurant Data: Note the cuisine column.\n\nnames(restaurants)\n\n [1] \"osm_id\"             \"name\"               \"addr.city\"         \n [4] \"addr.housename\"     \"addr.housenumber\"   \"addr.postcode\"     \n [7] \"addr.street\"        \"alt_name\"           \"amenity\"           \n[10] \"building\"           \"capacity\"           \"cuisine\"           \n[13] \"delivery\"           \"description\"        \"diet.vegetarian\"   \n[16] \"email\"              \"food\"               \"internet_access\"   \n[19] \"level\"              \"name.en\"            \"name.kn\"           \n[22] \"note\"               \"opening_hours\"      \"operator\"          \n[25] \"phone\"              \"smoking\"            \"source\"            \n[28] \"takeaway\"           \"toilets.wheelchair\" \"website\"           \n[31] \"wheelchair\"         \"wikidata\"           \"wikipedia\"         \n[34] \"geom\"              \n\n\nSo let us plot the restaurants as POINTs using the restaurants data we have downloaded. The cuisine attribute looks interesting; let us colour the POINT based on the cuisine offered at that restaurant.\nSo Let‚Äôs look therefore at the cuisine column!\n\n# ( I want pizza...)\nrestaurants$cuisine %>% unique()\n\n [1] NA                                       \n [2] \"indian\"                                 \n [3] \"italian\"                                \n [4] \"regional\"                               \n [5] \"pizza\"                                  \n [6] \"ice_cream\"                              \n [7] \"chinese\"                                \n [8] \"South_Indian\"                           \n [9] \"Multi-cuisne\"                           \n[10] \"South_India\"                            \n[11] \"chicken;regional\"                       \n[12] \"arab\"                                   \n[13] \"indian;seafood;fine_dining\"             \n[14] \"fast_food\"                              \n[15] \"kebab;grill\"                            \n[16] \"chicken\"                                \n[17] \"chinese;sandwich;tea;indian;coffee_shop\"\n[18] \"indian,_japanese\"                       \n[19] \"indian;regional\"                        \n\n\nBig mess‚Ä¶many NAs, some double entries, separated by commas and semicolons‚Ä¶.\n\n\nThe cuisine attribute:\n\n\nNote: The cuisine variable has more than one entry for a given restaurant. We use tidyr::separate() to make multiple columns out of the cuisine column and retain the first one only. Since the entries are badly entered using both ‚Äú;‚Äù and ‚Äú,‚Äù we need to do this twice ;-() Bad Data entry!!\n\n\nLet‚Äôs get one cuisine entry per restaurant, and drop off the ones that do not mention a cuisine at all:\n\nrestaurants <- restaurants %>% \n  drop_na(cuisine) %>% # Knock off nondescript restaurants\n  \n  # Some have more than one classification ;-()\n  # Separated by semicolon or comma, so....\n  separate_wider_delim(cols = cuisine, \n                       names = c(\"cuisine\", NA, NA), \n                       delim = \";\", \n                       too_few = \"align_start\",\n                       too_many = \"drop\") %>% \n  separate_wider_delim(cols = cuisine, \n                       names = c(\"cuisine\", NA, NA), \n                       delim = \",\",\n                       too_few = \"align_start\",\n                       too_many = \"drop\")\n\n# Finally good food?\nrestaurants$cuisine\n\n  [1] \"indian\"       \"italian\"      \"indian\"       \"indian\"       \"regional\"    \n  [6] \"indian\"       \"pizza\"        \"regional\"     \"ice_cream\"    \"ice_cream\"   \n [11] \"indian\"       \"chinese\"      \"chinese\"      \"indian\"       \"italian\"     \n [16] \"regional\"     \"indian\"       \"indian\"       \"italian\"      \"regional\"    \n [21] \"indian\"       \"chinese\"      \"indian\"       \"indian\"       \"indian\"      \n [26] \"indian\"       \"indian\"       \"ice_cream\"    \"pizza\"        \"South_Indian\"\n [31] \"regional\"     \"regional\"     \"Multi-cuisne\" \"South_India\"  \"indian\"      \n [36] \"indian\"       \"chicken\"      \"arab\"         \"indian\"       \"regional\"    \n [41] \"regional\"     \"regional\"     \"regional\"     \"regional\"     \"indian\"      \n [46] \"indian\"       \"indian\"       \"indian\"       \"regional\"     \"regional\"    \n [51] \"italian\"      \"regional\"     \"regional\"     \"regional\"     \"regional\"    \n [56] \"regional\"     \"regional\"     \"regional\"     \"regional\"     \"regional\"    \n [61] \"regional\"     \"regional\"     \"regional\"     \"fast_food\"    \"indian\"      \n [66] \"regional\"     \"italian\"      \"regional\"     \"regional\"     \"regional\"    \n [71] \"regional\"     \"regional\"     \"regional\"     \"italian\"      \"fast_food\"   \n [76] \"regional\"     \"fast_food\"    \"regional\"     \"chinese\"      \"regional\"    \n [81] \"regional\"     \"regional\"     \"regional\"     \"regional\"     \"regional\"    \n [86] \"regional\"     \"regional\"     \"regional\"     \"regional\"     \"regional\"    \n [91] \"regional\"     \"regional\"     \"regional\"     \"regional\"     \"regional\"    \n [96] \"regional\"     \"regional\"     \"regional\"     \"regional\"     \"kebab\"       \n[101] \"chicken\"      \"chinese\"      \"indian\"       \"italian\"      \"indian\"      \n[106] \"indian\"       \"indian\"      \n\n\nLooks clean! Each entry is only ONE and not multiple any more. Now let‚Äôs plot the Restaurants as POINTs:\n\n# http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf\n# \nggplot() +\n  geom_sf(data = buildings, colour = \"burlywood1\") +\n  geom_sf(data = roads, colour = \"gray80\") +\n  geom_sf(\n    data = restaurants %>% drop_na(cuisine),\n    aes(fill = cuisine),\n    colour = \"black\",\n    shape = 21,\n    size = 3\n  ) +\n  theme(legend.position = \"right\") +\n  labs(title = \"Restaurants in South Central Bangalore\",\n       caption = \"Based on osmdata\")\n\n\n\n\nWe could have done a (much!) better job, by combining cuisines into simpler and fewer categories, ( South_India and South_Indian ), but that is for another day!!\nBy now we know that we can use geom_sf() multiple number of times with different datasets to create layered maps in R."
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#globejs-usage",
    "href": "content/labs/r-labs/maps/gram-maps.html#globejs-usage",
    "title": "The Grammar of Maps",
    "section": "\n11.1 globejs usage",
    "text": "11.1 globejs usage\nThe globejs command from the package threejs allows one to plot points, arcs and images on a globe in 3D. The globe can be rotated and and zoomed. Great Circles and historical routes are a good idea for this perhaps.\nRefer to this page for more ideas http://bwlewis.github.io/rthreejs/globejs.html\nWe will generate some random locations and plot them on the 3D globe.\n\n# Random Lats and Longs\nlat <- rpois(10, 60) + rnorm(10, 80)\nlong <- rpois(10, 60) + rnorm(10, 10)\n\n# Random \"Spike\" heights for each location. Population? Tourists? GDP?\nvalue <- rpois(10, lambda = 80)\n \nglobejs(lat = lat, long = long)\n\n\n\n\n\nAs seen, ‚Äúspikes‚Äù are created at the random lat-lon locations. We can control the height/width/colour of the spikes, as well as the initial view of the globe itself: zoom, location and so on\n\nglobejs(\n  lat = lat,\n  long = long,\n  \n  # random heights of the Spikes (!!) at lat-long combo\n  value = value,\n  color = \"red\",\n  # Zoom factor, default is 35\n  fov = 50\n)\n\n\n\n\n\n\nglobejs(\n  lat = lat,\n  long = long,\n  value = value,\n  color = \"red\",\n  pointsize = 4, # width of the columns\n  # Zoom position\n  fov = 35,\n  # initial position of the globe\n  rotationlat = 0.6, #  in RADIANS !!! Good Heavens!!\n  rotationlong = 0.2 #  in RADIANS !!! Good Heavens!!\n)\n\n\n\n\n\n\nglobejs(\n  lat = lat,\n  long = long,\n  value = value,\n  color = \"red\",\n  pointsize = 4,\n  fov = 35,\n  rotationlat = 0.6,\n  rotationlong = 0.2,\n  lightcolor = \"#aaeeff\",\n  emissive = \"#0000ee\",\n  bodycolor = \"#ffffff\",\n  bg = \"grey\"\n)"
  },
  {
    "objectID": "content/labs/r-labs/maps/index.html",
    "href": "content/labs/r-labs/maps/index.html",
    "title": "Lab 01 - Introduce Yourself",
    "section": "",
    "text": "At the end of this Lab, we will:\n\nhave installed R and RStudio on our machines\nunderstood how to add additional R-packages for specific features and graphic capability\nrun code within RStudio and interpret the results\nhave learnt to look for help within R and RStudio"
  },
  {
    "objectID": "content/labs/r-labs/maps/index.html#Check-In-R",
    "href": "content/labs/r-labs/maps/index.html#Check-In-R",
    "title": "Lab 01 - Introduce Yourself",
    "section": "\n3.1 Check in",
    "text": "3.1 Check in\nLaunch R by clicking this logo. You should see one console with a command line interpreter. Try typing 2 + 2 and check !\nClose R."
  },
  {
    "objectID": "content/labs/r-labs/maps/index.html#Check-In-RStudio",
    "href": "content/labs/r-labs/maps/index.html#Check-In-RStudio",
    "title": "Lab 01 - Introduce Yourself",
    "section": "\n4.1 Check in",
    "text": "4.1 Check in\nLaunch RStudio. You should get a window similar to the screenshot you see here, but yours will be empty. Look at the bottom left pane: this is the same console window you saw when you opened R in step @Check-In-R\n\nPlace your cursor where you see > and type x <- 2 + 2 again hit enter or return, then type x, and hit enter/return again.\nIf [1] 4 prints to the screen, you have successfully installed R and RStudio, and you can move onto installing packages."
  },
  {
    "objectID": "content/labs/r-labs/maps/index.html#save-and-share",
    "href": "content/labs/r-labs/maps/index.html#save-and-share",
    "title": "Lab 01 - Introduce Yourself",
    "section": "\n7.1 Save and share",
    "text": "7.1 Save and share\nSave your work so you can share your favorite plot with us. You will not like the looks of your plot if you mouse over to Export and save it. Instead, use ggplot2‚Äôs command for saving a plot with sensible defaults:\n\nShow the Codehelp(ggsave)\n\n\n\nShow the Codeggsave(\"file_name_here.pdf\", plot) # please make the filename unique!\n\n\nUpload this exported plot to Teams -> Assignments.\nBefore you do that, check that you can submit stuff/assignments on the MLS by uploading a cat picture üêà first, especially if you are an ailurophile like me. (Acceptable Breeds: Scottish Folds or Maine Coons)"
  },
  {
    "objectID": "content/labs/r-labs/maps/leaflet.html",
    "href": "content/labs/r-labs/maps/leaflet.html",
    "title": "Playing with Leaflet",
    "section": "",
    "text": "This Tutorial works through the ideas at Leaflet\n\nLeaflet is a JavaScript library for creating dynamic maps that support panning and zooming along with various annotations like markers, polygons, and popups.\n\nIn this tutorial we will work only with vector data. In a second part, we will work with raster data in leaflet.\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.0     ‚úî readr     2.1.4\n‚úî forcats   1.0.0     ‚úî stringr   1.5.0\n‚úî ggplot2   3.4.1     ‚úî tibble    3.2.0\n‚úî lubridate 1.9.2     ‚úî tidyr     1.3.0\n‚úî purrr     1.0.1     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(leaflet)\nlibrary(maps)\n\n\nAttaching package: 'maps'\n\nThe following object is masked from 'package:purrr':\n\n    map\n\nlibrary(sp)\nlibrary(sf)\n\nLinking to GEOS 3.9.3, GDAL 3.5.2, PROJ 8.2.1; sf_use_s2() is TRUE\n\n# Data\nlibrary(osmdata) # Import OSM Vector Data into R\n\nData (c) OpenStreetMap contributors, ODbL 1.0. https://www.openstreetmap.org/copyright\n\nlibrary(osmplotr) # Creating maps with OSM data in R\n\nData (c) OpenStreetMap contributors, ODbL 1.0. http://www.openstreetmap.org/copyright\n\n# library(OpenStreetMap) # Raster Data"
  },
  {
    "objectID": "content/labs/r-labs/maps/leaflet.html#add-shapes-to-a-map",
    "href": "content/labs/r-labs/maps/leaflet.html#add-shapes-to-a-map",
    "title": "Playing with Leaflet",
    "section": "\n2.1 Add Shapes to a Map",
    "text": "2.1 Add Shapes to a Map\nleaflet offers several commands to add points, markers, icons, lines, polylines and polygons to a map. Let us examine a few of these.\n\n2.1.1 Add Markers with popups\n\nm %>% addMarkers(lng = 77.580643, lat = 12.972442, \n                 popup = \"The birthplace of Rvind\")\n\n\n\n\n# Click on the Marker for the popup to appear\n\nThis uses the default pin shape as the Marker.\n\n2.1.2 Adding Popups to a Map\nPopups are small boxes containing arbitrary HTML, that point to a specific point on the map. Use the addPopups() function to add standalone popup to the map.\n\nm %>%\n  addPopups(\n    lng = 77.580643,\n    lat = 12.972442,\n    popup = paste(\n      \"The birthplace of Rvind\",\n      \"<br>\",\n      \"Website: https://the-foundation-series.netlify.app\",\n      \"<br>\"\n    ),\n    \n    # Ensuring we cannot close the popup, else we will not be able to find where it is, since there is no Marker\n    options = popupOptions(closeButton = FALSE)\n  )\n\n\n\n\n\nPopups are usually added to icons, Markers and other shapes can show up when these are clicked.\n\n2.1.3 Adding Labels to a Map\nLabels are messages attached to all shapes, using the argument label wherever it is available.\nLabels are static, and Popups are usually visible on mouse click. Hence a Marker can have both a label and a popup. For example, the function addPopup() offers only a popup argument, whereas the function addMarkers() offers both a popup and a label argument.\nIt is also possible to create labels standalone using addLabelOnlyMarkers() where we can show only text and no Markers.\n\nm %>%\n  addMarkers(\n    lng = 77.580643,\n    lat = 12.972442,\n    \n    # Here is the Label defn.\n    label = \"The birthplace of Rvind\",\n    labelOptions = labelOptions(noHide = TRUE, # Label always visible\n                                textOnly = F, \n                                textsize = 20),\n    \n    # And here is the popup defn.\n    popup = \"This is the Popup Text\"\n  )\n\n\n\n\n\n\n2.1.4 Adding Circles and CircleMarkers on a Map\nWe can add shapes on to a map to depict areas or locations of interest. NOTE: the radius argument works differently in addCircles() and addCircleMarkers().\n\n# Some Cities in the US and their location\nmd_cities <- tibble(\n  name = c(\"Baltimore\",\"Frederick\",\"Rockville\",\"Gaithersburg\",\"Bowie\",\"Hagerstown\",\"Annapolis\",\"College Park\",\"Salisbury\",\"Laurel\"),\n  pop = c(619493,66169,62334,61045,55232,39890,38880,30587,30484,25346),\n  lat = c(39.2920592,39.4143921,39.0840,39.1434,39.0068,39.6418,38.9784,38.9897,38.3607,39.0993), \n  lng = c(-76.6077852,-77.4204875,-77.1528,-77.2014,-76.7791,-77.7200,-76.4922,-76.9378,-75.5994,-76.8483)\n)\n\n\nmd_cities %>%\n  leaflet() %>%\n  addTiles() %>%\n  \n  # CircleMarkers, in blue\n  # radius scales the Marker. Units are in Pixels!!\n  # Here, radius is made proportional to `pop` number\n  addCircleMarkers(radius = ~ pop/1000, # Pixels!!\n                   color = \"blue\",\n                   stroke = FALSE, # no border for the Markers\n                   opacity = 0.8) %>% \n  \n  \n  # Circles, in red\n  addCircles(\n    radius = 5000, # Meters !!!\n    stroke = TRUE,\n    color = \"yellow\", # Stroke Colour\n    weight = 3, # Stroke Weight\n    fill = TRUE,\n    fillColor = \"red\",\n\n  )\n\nAssuming \"lng\" and \"lat\" are longitude and latitude, respectively\nAssuming \"lng\" and \"lat\" are longitude and latitude, respectively\n\n\n\n\n\n\nThe shapes need not be of fixed size or colour; their attributes can be made to correspond to other attribute variables in the geospatial data, as we did with radius in the addCircleMarkers() function above.\n\n2.1.5 Adding Rectangles to a Map\n\n## Adding Rectangles\nleaflet() %>%\n  addTiles() %>%\n  setView(lng = 77.580643, lat = 12.972442, zoom = 6) %>% \n  addRectangles(lat1 = 10.3858, lng1 = 75.0595, \n                lat2 = 12.8890, lng2 = 77.9625)\n\n\n\n\n\n\n2.1.6 Add Polygons to a Map\n\n## Adding Polygons\nleaflet() %>%\n  addTiles() %>%\n  setView(lng = 77.580643, lat = 12.972442, zoom = 6) %>% \n  \n  # arbitrary vector data for lat and lng\n   addPolygons(lng = c(73.5, 75.9, 76.1, 77.23, 79.8),\n               lat =c(10.12, 11.04, 11.87, 12.04, 10.7))\n\n\n\n\n\n\n2.1.7 Add PolyLines to a Map\nThis can be useful say for manually marking a route on a map, with waypoints.\n\nleaflet() %>%\n  addTiles() %>%\n  setView(lng = 77.580643, lat = 12.972442, zoom = 6) %>% \n  \n  # arbitrary vector data for lat and lng\n  # If start and end points are the same, it looks like Polygon\n  # Without the fill\n   addPolylines(lng = c(73.5, 75.9, 76.1, 77.23, 79.8),\n               lat =c(10.12, 11.04, 11.87, 12.04, 10.7)) %>% \n  \n  # Add Waypoint Icons\n  addMarkers(lng = c(73.5, 75.9, 76.1, 77.23, 79.8),\n               lat =c(10.12, 11.04, 11.87, 12.04, 10.7))\n\n\n\n\n\nAs seen, we have created Markers, Labels, Polygons, and PolyLines using fixed.i.e. literal text and numbers. In the following we will also see how external geospatial data columns can be used instead of these literals.\nNOTE: The mapedit package https://r-spatial.org//r/2017/01/30/mapedit_intro.html can also be used to interactively add shapes onto a map and save as an geo-spatial object."
  },
  {
    "objectID": "content/labs/r-labs/maps/leaflet.html#point-data-sources-for-leaflet",
    "href": "content/labs/r-labs/maps/leaflet.html#point-data-sources-for-leaflet",
    "title": "Playing with Leaflet",
    "section": "\n3.1 Point Data Sources for leaflet\n",
    "text": "3.1 Point Data Sources for leaflet\n\nPoint data for markers can come from a variety of sources:\n\n\nSpatialPoints or SpatialPointsDataFrame objects (from the sp package)\n\n\nPOINT, sfc_POINT, and sf objects (from the sf package); only X and Y dimensions will be considered\n\nTwo-column numeric matrices (first column is longitude, second is latitude)\n\n\nData frame/tibble with latitude and longitude columns. You can explicitly tell the marker function which columns contain the coordinate data (e.g. addMarkers(lng = ~Longitude, lat = ~Latitude)), or let the function look for columns named lat/latitude and lon/lng/long/longitude (case insensitive).\n\nSimply provide numeric vectors as lng and lat arguments, which we have covered already in the preceding sections.\n\nNote that MULTIPOINT objects from sf are not supported at this time.\nWe will not consider the use of sp related data structures for plotting POINTs in leaflet since sp is being phased out in favour of the more modern package sf.\n\n3.1.1 Points using simple Data Frames\nLet us read in the data set from data.world that gives us POINT locations of all airports in India in a data frame / tibble. The dataset is available at https://query.data.world/s/ahtyvnm2ybylf65syp4rsb5tulxe6a. You can either download it, save a copy, and read it in as usual, or use the URL itself to read it in from the web. In the latter case, you will need the package data.world and also need to register your credentials for that page with RStudio. The (simple!) instructions are available here at data.world.\n\n#library(devtools)\n#devtools::install_github(\"datadotworld/data.world-r\", build_vignettes = TRUE)\n\nlibrary(data.world)\n\nLoading required package: dwapi\n\n\n\nAttaching package: 'dwapi'\n\n\nThe following object is masked from 'package:dplyr':\n\n    sql\n\nindia_airports <-\n  read_csv(\"https://query.data.world/s/ahtyvnm2ybylf65syp4rsb5tulxe6a\") %>% \n  slice(-1) %>% # Drop the first row which contains labels\n  dplyr::mutate(\n    id = as.integer(id),\n    latitude_deg = as.numeric(latitude_deg),\n    longitude_deg = as.numeric(longitude_deg),\n    elevation_ft = as.integer(elevation_ft)\n  ) %>% \n  rename(\"lon\" = longitude_deg, \"lat\" = latitude_deg) %>% \n  # Remove four locations which seem to be in the African Atlantic\n  filter(!id %in% c(330834, 330867, 325010, 331083))\n\nRows: 345 Columns: 20\n\n\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (20): id, ident, type, name, latitude_deg, longitude_deg, elevation_ft, ...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nindia_airports %>% head()\n\n\n\n  \n\n\n\nLet us plot this in leaflet, using an ESRI National Geographic style map instead of the OSM Base Map. We will also place small circle markers for each airport.\n\nleaflet(data = india_airports) %>% \n  setView(lat = 18, lng = 77, zoom = 4) %>% \n  \n  # Add NatGeo style base map\n  addProviderTiles(providers$Esri.NatGeoWorldMap) %>% # ESRI Basemap\n  \n  # Add Markers for each airport\n  addCircleMarkers(lng = ~lon, lat = ~lat,\n                   # Optional, variables stated for clarity\n                   # leaflet can automatically detect lon-lat columns\n                   # if they are appropriately named in the data\n                   # longitude/lon/lng\n                   # latitude/lat\n                   radius = 2, # Pixels\n                   color = \"red\",\n                   opacity = 1)\n\n\n\n\n\nWe can also change the icon for each airport. Let us try one of theseveral icon families that we can use with leaflet : glyphicons, ionicons, and fontawesome icons.\n\n# Define popup message for each airport\n# Based on data in india_airports\npopup <- paste(\n  \"<strong>\",\n  india_airports$name,\n  \"</strong><br>\",\n  india_airports$iata_code,\n  \"<br>\",\n  india_airports$municipality,\n  \"<br>\",\n  \"Elevation(feet)\",\n  india_airports$elevation_ft,\n  \"<br>\",\n  india_airports$wikipedia_link,\n  \"<br>\"\n)\n\niata_icon <- makeIcon(\n  \"iata-logo-transp.png\", # Downloaded from www.iata.org\n  iconWidth = 24,\n  iconHeight = 24,\n  iconAnchorX = 0,\n  iconAnchorY = 0\n)\n\n# Create the Leaflet map\nleaflet(data = india_airports) %>%\n  setView(lat = 18, lng = 77, zoom = 4) %>%\n  addProviderTiles(providers$Esri.NatGeoWorldMap) %>%\n  addMarkers(\n    icon = iata_icon,\n    popup = popup\n  )\n\nAssuming \"lon\" and \"lat\" are longitude and latitude, respectively\n\n\n\n\n\n\nThere are other icons we can use to mark the POINTs. leaflet allows the use of ionicons, glyphicons, and FontAwesomeIcons\nIt is possible to create a list of icons, so that different Markers can have different icons. Let us try to map the MNCs in the ITPL area of Bangalore: we use the ideas in Using Leaflet Markers @JLA-Data.net\n\n# Make a dataframe of addresses of Companies we wan to plot in ITPL\ncompanies_itpl <-\n  data.frame(\n    ticker = c(\n      \"MBRDI\",\n      \"DTICI\",\n      \"IBM\",\n      \"Exxon\",\n      \"Mindtree\",\n      \"FIS Global\",\n      \"Sasken\",\n      \"LTI\"),\n    lat = c(\n      12.986178620989264,\n      12.984160906190121,\n      12.983659088566357,\n      12.985112265986636,\n      12.983794997606187,\n      12.980658616215155,\n      12.982080447350246,\n      12.981338168875348),\n    lon = c(\n      77.7270652183105,\n      77.72808445774321,\n      77.73103488768001,\n      77.72935046040699,\n      77.7227844126931,\n      77.72685064158782,\n      77.72545589289041,\n      77.72287024338216)\n  ) %>% sf::st_as_sf(coords = c(\"lon\", \"lat\"), crs = 4326)\n \n# Vanilla leaflet map\nleaflet(companies_itpl) %>% \n  addTiles() %>% \n  addMarkers()\n\n\n\n\n\nLet us make a list of logos of the Companies and use them as markers!\n\n# a named list of rescaled icons with links to images\nfavicons <- iconList(\n  \"MBRDI\" = makeIcon(\n    iconUrl = \"https://www.mercedes-benz.com/etc/designs/brandhub/frontend/static-assets/header/logo.svg\", \n    iconWidth = 25,\n    iconHeight = 25\n  ),\n  \"DTICI\" = makeIcon(\n    iconUrl = \"https://media-exp1.licdn.com/dms/image/C4D0BAQGzOep26lC03w/company-logo_200_200/0/1638298367374?e=2147483647&v=beta&t=mPyF4gvNhNFvd-tedbqNzJofq4q9qcw6A9z9jQeLAwc\",\n    iconWidth = 45,\n    iconHeight = 45\n  ),\n  \"IBM\" = makeIcon(\n    iconUrl = \"https://www.ibm.com/favicon.ico\",\n    iconWidth = 25,\n    iconHeight = 25\n  ),\n  \"Exxon\" = makeIcon(\n    iconUrl = \"https://corporate.exxonmobil.com/-/media/Global/Icons/logos/ExxonMobilLogoColor2x.png\",\n    iconWidth = 45,\n    iconHeight = 25\n  ),\n  \"Mindtree\" = makeIcon(\n    iconUrl = \"https://www.mindtree.com/themes/custom/mindtree_theme/mindtree-lnt-logo-png.png\",\n    iconWidth = 75,\n    iconHeight = 25\n  ),\n  \"FIS Global\" = makeIcon(\n    iconUrl = \"https://1000logos.net/wp-content/uploads/2021/09/FIS-Logo-768x432.png\",\n    iconWidth = 25,\n    iconHeight = 25\n  ),\n  \"Sasken\" = makeIcon(\n    iconUrl = \"https://www.sasken.com/sites/all/themes/sasken_website/logo.png\",\n    iconWidth = 35,\n    iconHeight = 35,\n  ),\n  \"LTI\" = makeIcon(\n    iconUrl = \"https://www.lntinfotech.com/wp-content/uploads/2021/09/LTI-logo.svg\",\n    iconWidth = 25,\n    iconHeight = 25\n  )\n)\n\n\n# Create the Leaflet map\n\nleaflet(companies_itpl) %>% \n  addMarkers(icon = ~ favicons[ticker], # lookup based on ticker\n             label = ~ companies_itpl$ticker,\n             labelOptions = labelOptions(noHide = F,offset = c(15,-25))) %>%\n  addProviderTiles(\"CartoDB.Positron\")\n\n\n\n\n\n\n3.1.2 Points using sf objects\nWe will use data from an sf data object. This differs from the earlier situation where we had a simple data frame with lon and lat columns. In sf, the lon and lat info is embedded in the geometry column of the sf data frame.\nThe tmap package has a data set of all World metro cities, titled metro. We will plot these on the map and also scale the markers in proportion to one of the feature attributes, pop2030. The popup will be the name of the metro city. We will also use the CartoDB.Positron base map.\nNote that the metro data set has a POINT geometry, as needed!\n\ndata(metro, package = \"tmap\")\nmetro\n\n\n\n  \n\n\nleaflet(data = metro) %>% \n  setView(lat = 18, lng = 77, zoom = 4) %>% \n  \n  # Add CartoDB.Positron\n  addProviderTiles(providers$CartoDB.Positron) %>% # CartoDB Basemap\n  \n  # Add Markers for each airport\n  addCircleMarkers(radius = ~ sqrt(pop2030)/350,\n                   color = \"red\",\n                   popup = paste(\"Name: \", metro$name, \"<br>\",\n                                  \"Population 2030: \", metro$pop2030))\n\n\n\n\n\nWe can also try downloading an sf data frame with POINT geometry from say OSM data<https://osm. Let us get hold of restaurants data in Malleswaram, Bangalore from OSM data:\n\nbbox<- osmdata::getbb(\"Malleswaram, Bengaluru\")\nbbox\n\n       min      max\nx 77.55033 77.59033\ny 12.98274 13.02274\n\nlocations <- osmplotr::extract_osm_objects(\n  bbox = bbox,\n  key = \"amenity\",\n  value = \"restaurant\",\n  return_type = \"point\") \n\nIssuing query to Overpass API ...\n\n\nAnnounced endpoint: none\n\n\nQuery complete!\n\n\nconverting OSM data to sf format\n\nlocations <- locations %>% \n  dplyr::filter(cuisine == \"indian\")\nlocations %>% head()\n\n\n\n  \n\n\n# Fontawesome icons seem to work in `leaflet` only up to FontAwesome V4.7.0.\n# The Fontawesome V4.7.0 Cheatsheet is here: <https://fontawesome.com/v4/cheatsheet/>\n\n\nleaflet(data = locations, options = leafletOptions(minZoom = 12)) %>% \n  \n  addProviderTiles(providers$CartoDB.Voyager) %>% \n  \n  # Regular `leaflet` code\n  addAwesomeMarkers(icon = awesomeIcons(icon = \"fa-coffee\", \n                                        library = \"fa\",\n                                        markerColor = \"blue\",\n                                        iconColor = \"black\",\n                                        iconRotate = TRUE),\n                     popup = paste(\"Name: \", locations$name,\"<br>\",\n                           \"Food: \", locations$cuisine)) \n\n\n\n\n\nFontawesome Workaround\nFor more later versions of Fontawesome, here below is a workaround from https://github.com/rstudio/leaflet/issues/691. Despite this some fontawesome icons simply do not seem to show up. ;-()\n\nlibrary(fontawesome)\ncoffee <- makeAwesomeIcon(\n  text = fa(\"mug-hot\"), # mug-hot was introduced in fa version 5\n  iconColor = \"black\",\n  markerColor = \"blue\",\n  library = \"fa\"\n)\n\n\nleaflet(data = locations) %>% \n  addProviderTiles(providers$CartoDB.Voyager) %>% \n  \n  # Workaround code\n\n  addAwesomeMarkers(icon = coffee,\n             popup = paste(\"Name: \", locations$name,\"<br>\",\n                           \"Food: \", locations$cuisine, \"<br>\"))\n\n\n\n\n\nNote that leaflet automatically detects the lon/lat columns from within the POINT geometry column of the sf data frame.\n\n3.1.3 Points using Two-Column Matrices\nWe can now quickly try providing lon and lat info in a two column matrix.This can be useful to plot a bunch of points recorded on a mobile phone app.\n\nmysore5 <- matrix(c(runif(5, 76.652985-0.01, 76.652985+0.01),\n                 runif(5, 12.311827-0.01, 12.311827+0.01)),\n                 nrow = 5)\nmysore5\n\n         [,1]     [,2]\n[1,] 76.65387 12.31681\n[2,] 76.66260 12.31176\n[3,] 76.65750 12.31252\n[4,] 76.65581 12.30970\n[5,] 76.64777 12.31510\n\nleaflet(data = mysore5) %>% \n  addProviderTiles(providers$OpenStreetMap) %>% \n  \n# Pick an icon from <https://www.w3schools.com/bootstrap/bootstrap_ref_comp_glyphs.asp>\n  addAwesomeMarkers(icon = awesomeIcons(\n  icon = 'music',\n  iconColor = 'black',\n  library = 'glyphicon'),\n  popup = \"Carnatic Music !!\")"
  },
  {
    "objectID": "content/labs/r-labs/maps/leaflet.html#polygons-lines-and-polylines-data-sources-for-leaflet",
    "href": "content/labs/r-labs/maps/leaflet.html#polygons-lines-and-polylines-data-sources-for-leaflet",
    "title": "Playing with Leaflet",
    "section": "\n3.2 Polygons, Lines, and Polylines Data Sources for leaflet\n",
    "text": "3.2 Polygons, Lines, and Polylines Data Sources for leaflet\n\nWe have seen how to get POINT data into leaflet.\nLine and polygon data can come from a variety of sources:\n\n\nSpatialPolygons, SpatialPolygonsDataFrame, Polygons, and Polygon objects (from the sp package)\n\n\nSpatialLines, SpatialLinesDataFrame, Lines, and Line objects (from the sp package)\n\n\nMULTIPOLYGON, POLYGON, MULTILINESTRING, and LINESTRING objects (from the sf package)\n\n\nmap objects (from the maps package‚Äôs map() function); use map(fill = TRUE) for polygons, FALSE for polylines\n\nTwo-column numeric matrix; the first column is longitude and the second is latitude. Polygons are separated by rows of (NA, NA). It is not possible to represent multi-polygons nor polygons with holes using this method; use SpatialPolygons instead.\n\nWe will concentrate on using sf data into leaflet. We may explore maps() objects at a later date.\n\n3.2.1 Polygons/MultiPolygons and LineString/MultiLineString using sf data frames\nLet us download College buildings, parks, and the cycling lanes in Amsterdam, Netherlands, and plot these in leaflet.\n\nbbox <- osmdata::getbb(\"Amsterdam, Netherlands\")\nbbox\n\n        min       max\nx  4.728756  5.079162\ny 52.278174 52.431064\n\ncolleges <- osmplotr::extract_osm_objects(bbox = bbox,\n                                           key = \"amenity\",\n                                           value = \"college\",\n                                           return_type = \"polygon\" )\n\nIssuing query to Overpass API ...\n\n\nAnnounced endpoint: none\n\n\nQuery complete!\n\n\nconverting OSM data to sf format\n\nparks <- osmplotr::extract_osm_objects(bbox = bbox,\n                                           key = \"park\",\n                                           return_type = \"polygon\" )\n\nIssuing query to Overpass API ...\n\n\nAnnounced endpoint: none\n\n\nQuery complete!\n\n\nconverting OSM data to sf format\n\nroads <- osmplotr::extract_osm_objects(bbox = bbox,\n                                       key = \"highway\",\n                                       value = \"primary\",\n                                       return_type = \"line\")\n\nIssuing query to Overpass API ...\n\n\nAnnounced endpoint: none\n\n\nQuery complete!\n\n\nconverting OSM data to sf format\n\ncyclelanes <-\n  osmplotr::extract_osm_objects(bbox,\n                                key = \"cycleway\",\n                                value =  \"lane\",\n                                return_type = \"line\")\n\nIssuing query to Overpass API ...\n\n\nAnnounced endpoint: none\n\n\nQuery complete!\n\n\nconverting OSM data to sf format\n\n\nWe have 17 colleges in our data and 370 parks in our data.\n\nleaflet() %>% \n  addTiles() %>% \n  addPolygons(data = colleges, popup = ~colleges$name) %>% \n  addPolygons(data = parks, color = \"green\", popup = parks$name) %>% \n  addPolylines(data = roads, color = \"red\") %>% \n  addPolylines(data = cyclelanes, color = \"purple\")"
  },
  {
    "objectID": "content/labs/r-labs/maps/leaflet.html#chapter-3-using-raster-data-in-leaflet",
    "href": "content/labs/r-labs/maps/leaflet.html#chapter-3-using-raster-data-in-leaflet",
    "title": "Playing with Leaflet",
    "section": "\n3.3 Chapter 3: Using Raster Data in leaflet\n",
    "text": "3.3 Chapter 3: Using Raster Data in leaflet\n\nSo far all the geospatial data we have plotted in leaflet has been vector data. We will now explore how to plot raster data using leaflet. Raster data are used to depict continuous variables across space, such as vegitation, salinity, forest cover etc. Satellite imagery is frequently available as raster data.\n\n3.3.1 Importing Raster Data [Work in Progress!]\nRaster data can be imported into R in many ways:\n\nusing the maptiles package\n\nusing the OpenStreetMap package\n\n\nlibrary(terra)\n\nterra 1.7.18\n\n\n\nAttaching package: 'terra'\n\n\nThe following object is masked from 'package:data.world':\n\n    query\n\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\nlibrary(maptiles)\n#library(OpenStreetMap) # causes RStudio to crash..."
  },
  {
    "objectID": "content/labs/r-labs/maps/leaflet.html#adding-legendswork-in-progress",
    "href": "content/labs/r-labs/maps/leaflet.html#adding-legendswork-in-progress",
    "title": "Playing with Leaflet",
    "section": "\n4.1 Adding Legends[Work in Progress!]",
    "text": "4.1 Adding Legends[Work in Progress!]\n\n## Generate some random lat lon data around Bangalore\ndf <- data.frame(lat = runif(20, min = 11.97, max = 13.07),\n                 lng = runif(20, min = 77.48, max = 77.68),\n                 col = sample(c(\"red\", \"blue\", \"green\"), 20, \n                              replace = TRUE),\n                 stringsAsFactors = FALSE)\n\ndf %>%\n  leaflet() %>%\n  addTiles() %>%\n  addCircleMarkers(color = df$col) %>%\n  addLegend(values = df$col, labels = LETTERS[1:3], colors = c(\"blue\", \"red\", \"green\"))\n\nAssuming \"lng\" and \"lat\" are longitude and latitude, respectively"
  },
  {
    "objectID": "content/labs/r-labs/maps/PlayingwithLeaflet.html",
    "href": "content/labs/r-labs/maps/PlayingwithLeaflet.html",
    "title": "Playing with Leaflet",
    "section": "",
    "text": "This Tutorial works through the ideas at Leaflet\n\nLeaflet is a JavaScript library for creating dynamic maps that support panning and zooming along with various annotations like markers, polygons, and popups.\n\nIn this tutorial we will work only with vector data. In a second part, we will work with raster data in leaflet.\n\nlibrary(tidyverse)\nlibrary(leaflet)\nlibrary(maps)\nlibrary(sp)\nlibrary(sf)\n\n# Data\nlibrary(osmdata) # Import OSM Vector Data into R\nlibrary(osmplotr) # Creating maps with OSM data in R\n# library(OpenStreetMap) # Raster Data"
  },
  {
    "objectID": "content/labs/r-labs/maps/PlayingwithLeaflet.html#add-shapes-to-a-map",
    "href": "content/labs/r-labs/maps/PlayingwithLeaflet.html#add-shapes-to-a-map",
    "title": "Playing with Leaflet",
    "section": "\n2.1 Add Shapes to a Map",
    "text": "2.1 Add Shapes to a Map\nleaflet offers several commands to add points, markers, icons, lines, polylines and polygons to a map. Let us examine a few of these.\n\n2.1.1 Add Markers with popups\n\nm %>% addMarkers(lng = 77.580643, lat = 12.972442, \n                 popup = \"The birthplace of Rvind\")\n\n\n\n\n# Click on the Marker for the popup to appear\n\nThis uses the default pin shape as the Marker.\n\n2.1.2 Adding Popups to a Map\nPopups are small boxes containing arbitrary HTML, that point to a specific point on the map. Use the addPopups() function to add standalone popup to the map.\n\nm %>%\n  addPopups(\n    lng = 77.580643,\n    lat = 12.972442,\n    popup = paste(\n      \"The birthplace of Rvind\",\n      \"<br>\",\n      \"Website: https://the-foundation-series.netlify.app\",\n      \"<br>\"\n    ),\n    \n    # Ensuring we cannot close the popup, else we will not be able to find where it is, since there is no Marker\n    options = popupOptions(closeButton = FALSE)\n  )\n\n\n\n\n\nPopups are usually added to icons, Markers and other shapes can show up when these are clicked.\n\n2.1.3 Adding Labels to a Map\nLabels are messages attached to all shapes, using the argument label wherever it is available.\nLabels are static, and Popups are usually visible on mouse click. Hence a Marker can have both a label and a popup. For example, the function addPopup() offers only a popup argument, whereas the function addMarkers() offers both a popup and a label argument.\nIt is also possible to create labels standalone using addLabelOnlyMarkers() where we can show only text and no Markers.\n\nm %>%\n  addMarkers(\n    lng = 77.580643,\n    lat = 12.972442,\n    \n    # Here is the Label defn.\n    label = \"The birthplace of Rvind\",\n    labelOptions = labelOptions(noHide = TRUE, # Label always visible\n                                textOnly = F, \n                                textsize = 20),\n    \n    # And here is the popup defn.\n    popup = \"This is the Popup Text\"\n  )\n\n\n\n\n\n\n2.1.4 Adding Circles and CircleMarkers on a Map\nWe can add shapes on to a map to depict areas or locations of interest. NOTE: the radius argument works differently in addCircles() and addCircleMarkers().\n\n# Some Cities in the US and their location\nmd_cities <- tibble(\n  name = c(\"Baltimore\",\"Frederick\",\"Rockville\",\"Gaithersburg\",\"Bowie\",\"Hagerstown\",\"Annapolis\",\"College Park\",\"Salisbury\",\"Laurel\"),\n  pop = c(619493,66169,62334,61045,55232,39890,38880,30587,30484,25346),\n  lat = c(39.2920592,39.4143921,39.0840,39.1434,39.0068,39.6418,38.9784,38.9897,38.3607,39.0993), \n  lng = c(-76.6077852,-77.4204875,-77.1528,-77.2014,-76.7791,-77.7200,-76.4922,-76.9378,-75.5994,-76.8483)\n)\n\n\nmd_cities %>%\n  leaflet() %>%\n  addTiles() %>%\n  \n  # CircleMarkers, in blue\n  # radius scales the Marker. Units are in Pixels!!\n  # Here, radius is made proportional to `pop` number\n  addCircleMarkers(radius = ~ pop/1000, # Pixels!!\n                   color = \"blue\",\n                   stroke = FALSE, # no border for the Markers\n                   opacity = 0.8) %>% \n  \n  \n  # Circles, in red\n  addCircles(\n    radius = 5000, # Meters !!!\n    stroke = TRUE,\n    color = \"yellow\", # Stroke Colour\n    weight = 3, # Stroke Weight\n    fill = TRUE,\n    fillColor = \"red\",\n\n  )\n\nAssuming \"lng\" and \"lat\" are longitude and latitude, respectively\nAssuming \"lng\" and \"lat\" are longitude and latitude, respectively\n\n\n\n\n\n\nThe shapes need not be of fixed size or colour; their attributes can be made to correspond to other attribute variables in the geospatial data, as we did with radius in the addCircleMarkers() function above.\n\n2.1.5 Adding Rectangles to a Map\n\n## Adding Rectangles\nleaflet() %>%\n  addTiles() %>%\n  setView(lng = 77.580643, lat = 12.972442, zoom = 6) %>% \n  addRectangles(lat1 = 10.3858, lng1 = 75.0595, \n                lat2 = 12.8890, lng2 = 77.9625)\n\n\n\n\n\n\n2.1.6 Add Polygons to a Map\n\n## Adding Polygons\nleaflet() %>%\n  addTiles() %>%\n  setView(lng = 77.580643, lat = 12.972442, zoom = 6) %>% \n  \n  # arbitrary vector data for lat and lng\n   addPolygons(lng = c(73.5, 75.9, 76.1, 77.23, 79.8),\n               lat =c(10.12, 11.04, 11.87, 12.04, 10.7))\n\n\n\n\n\n\n2.1.7 Add PolyLines to a Map\nThis can be useful say for manually marking a route on a map, with waypoints.\n\nleaflet() %>%\n  addTiles() %>%\n  setView(lng = 77.580643, lat = 12.972442, zoom = 6) %>% \n  \n  # arbitrary vector data for lat and lng\n  # If start and end points are the same, it looks like Polygon\n  # Without the fill\n   addPolylines(lng = c(73.5, 75.9, 76.1, 77.23, 79.8),\n               lat =c(10.12, 11.04, 11.87, 12.04, 10.7)) %>% \n  \n  # Add Waypoint Icons\n  addMarkers(lng = c(73.5, 75.9, 76.1, 77.23, 79.8),\n               lat =c(10.12, 11.04, 11.87, 12.04, 10.7))\n\n\n\n\n\nAs seen, we have created Markers, Labels, Polygons, and PolyLines using fixed.i.e. literal text and numbers. In the following we will also see how external geospatial data columns can be used instead of these literals.\nNOTE: The mapedit package https://r-spatial.org//r/2017/01/30/mapedit_intro.html can also be used to interactively add shapes onto a map and save as an geo-spatial object."
  },
  {
    "objectID": "content/labs/r-labs/maps/PlayingwithLeaflet.html#point-data-sources-for-leaflet",
    "href": "content/labs/r-labs/maps/PlayingwithLeaflet.html#point-data-sources-for-leaflet",
    "title": "Playing with Leaflet",
    "section": "\n3.1 Point Data Sources for leaflet\n",
    "text": "3.1 Point Data Sources for leaflet\n\nPoint data for markers can come from a variety of sources:\n\n\nSpatialPoints or SpatialPointsDataFrame objects (from the sp package)\n\n\nPOINT, sfc_POINT, and sf objects (from the sf package); only X and Y dimensions will be considered\n\nTwo-column numeric matrices (first column is longitude, second is latitude)\n\n\nData frame/tibble with latitude and longitude columns. You can explicitly tell the marker function which columns contain the coordinate data (e.g. addMarkers(lng = ~Longitude, lat = ~Latitude)), or let the function look for columns named lat/latitude and lon/lng/long/longitude (case insensitive).\n\nSimply provide numeric vectors as lng and lat arguments, which we have covered already in the preceding sections.\n\nNote that MULTIPOINT objects from sf are not supported at this time.\nWe will not consider the use of sp related data structures for plotting POINTs in leaflet since sp is being phased out in favour of the more modern package sf.\n\n3.1.1 Points using simple Data Frames\nLet us read in the data set from data.world that gives us POINT locations of all airports in India in a data frame / tibble. The dataset is available at https://query.data.world/s/ahtyvnm2ybylf65syp4rsb5tulxe6a. You can either download it, save a copy, and read it in as usual, or use the URL itself to read it in from the web. In the latter case, you will need the package data.world and also need to register your credentials for that page with RStudio. The (simple!) instructions are available here at data.world.\n\n#library(devtools)\n#devtools::install_github(\"datadotworld/data.world-r\", build_vignettes = TRUE)\n\nlibrary(data.world)\n\nLoading required package: dwapi\n\n\n\nAttaching package: 'dwapi'\n\n\nThe following object is masked from 'package:dplyr':\n\n    sql\n\nindia_airports <-\n  read_csv(\"https://query.data.world/s/ahtyvnm2ybylf65syp4rsb5tulxe6a\") %>% \n  slice(-1) %>% # Drop the first row which contains labels\n  dplyr::mutate(\n    id = as.integer(id),\n    latitude_deg = as.numeric(latitude_deg),\n    longitude_deg = as.numeric(longitude_deg),\n    elevation_ft = as.integer(elevation_ft)\n  ) %>% \n  rename(\"lon\" = longitude_deg, \"lat\" = latitude_deg) %>% \n  # Remove four locations which seem to be in the African Atlantic\n  filter(!id %in% c(330834, 330867, 325010, 331083))\n\nRows: 345 Columns: 20\n\n\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (20): id, ident, type, name, latitude_deg, longitude_deg, elevation_ft, ...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nindia_airports %>% head()\n\n\n\n  \n\n\n\nLet us plot this in leaflet, using an ESRI National Geographic style map instead of the OSM Base Map. We will also place small circle markers for each airport.\n\nleaflet(data = india_airports) %>% \n  setView(lat = 18, lng = 77, zoom = 4) %>% \n  \n  # Add NatGeo style base map\n  addProviderTiles(providers$Esri.NatGeoWorldMap) %>% # ESRI Basemap\n  \n  # Add Markers for each airport\n  addCircleMarkers(lng = ~lon, lat = ~lat,\n                   # Optional, variables stated for clarity\n                   # leaflet can automatically detect lon-lat columns\n                   # if they are appropriately named in the data\n                   # longitude/lon/lng\n                   # latitude/lat\n                   radius = 2, # Pixels\n                   color = \"red\",\n                   opacity = 1)\n\n\n\n\n\nWe can also change the icon for each airport. Let us try one of theseveral icon families that we can use with leaflet : glyphicons, ionicons, and fontawesome icons.\n\n# Define popup message for each airport\n# Based on data in india_airports\npopup <- paste(\n  \"<strong>\",\n  india_airports$name,\n  \"</strong><br>\",\n  india_airports$iata_code,\n  \"<br>\",\n  india_airports$municipality,\n  \"<br>\",\n  \"Elevation(feet)\",\n  india_airports$elevation_ft,\n  \"<br>\",\n  india_airports$wikipedia_link,\n  \"<br>\"\n)\n\niata_icon <- makeIcon(\n  \"iata-logo-transp.png\", # Downloaded from www.iata.org\n  iconWidth = 24,\n  iconHeight = 24,\n  iconAnchorX = 0,\n  iconAnchorY = 0\n)\n\n# Create the Leaflet map\nleaflet(data = india_airports) %>%\n  setView(lat = 18, lng = 77, zoom = 4) %>%\n  addProviderTiles(providers$Esri.NatGeoWorldMap) %>%\n  addMarkers(\n    icon = iata_icon,\n    popup = popup\n  )\n\nAssuming \"lon\" and \"lat\" are longitude and latitude, respectively\n\n\n\n\n\n\nThere are other icons we can use to mark the POINTs. leaflet allows the use of ionicons, glyphicons, and FontAwesomeIcons\nIt is possible to create a list of icons, so that different Markers can have different icons. Let us try to map the MNCs in the ITPL area of Bangalore: we use the ideas in Using Leaflet Markers @JLA-Data.net\n\n# Make a dataframe of addresses of Companies we wan to plot in ITPL\ncompanies_itpl <-\n  data.frame(\n    ticker = c(\n      \"MBRDI\",\n      \"DTICI\",\n      \"IBM\",\n      \"Exxon\",\n      \"Mindtree\",\n      \"FIS Global\",\n      \"Sasken\",\n      \"LTI\"),\n    lat = c(\n      12.986178620989264,\n      12.984160906190121,\n      12.983659088566357,\n      12.985112265986636,\n      12.983794997606187,\n      12.980658616215155,\n      12.982080447350246,\n      12.981338168875348),\n    lon = c(\n      77.7270652183105,\n      77.72808445774321,\n      77.73103488768001,\n      77.72935046040699,\n      77.7227844126931,\n      77.72685064158782,\n      77.72545589289041,\n      77.72287024338216)\n  ) %>% sf::st_as_sf(coords = c(\"lon\", \"lat\"), crs = 4326)\n \n# Vanilla leaflet map\nleaflet(companies_itpl) %>% \n  addTiles() %>% \n  addMarkers()\n\n\n\n\n\nLet us make a list of logos of the Companies and use them as markers!\n\n# a named list of rescaled icons with links to images\nfavicons <- iconList(\n  \"MBRDI\" = makeIcon(\n    iconUrl = \"https://www.mercedes-benz.com/etc/designs/brandhub/frontend/static-assets/header/logo.svg\", \n    iconWidth = 25,\n    iconHeight = 25\n  ),\n  \"DTICI\" = makeIcon(\n    iconUrl = \"https://media-exp1.licdn.com/dms/image/C4D0BAQGzOep26lC03w/company-logo_200_200/0/1638298367374?e=2147483647&v=beta&t=mPyF4gvNhNFvd-tedbqNzJofq4q9qcw6A9z9jQeLAwc\",\n    iconWidth = 45,\n    iconHeight = 45\n  ),\n  \"IBM\" = makeIcon(\n    iconUrl = \"https://www.ibm.com/favicon.ico\",\n    iconWidth = 25,\n    iconHeight = 25\n  ),\n  \"Exxon\" = makeIcon(\n    iconUrl = \"https://corporate.exxonmobil.com/-/media/Global/Icons/logos/ExxonMobilLogoColor2x.png\",\n    iconWidth = 45,\n    iconHeight = 25\n  ),\n  \"Mindtree\" = makeIcon(\n    iconUrl = \"https://www.mindtree.com/themes/custom/mindtree_theme/mindtree-lnt-logo-png.png\",\n    iconWidth = 75,\n    iconHeight = 25\n  ),\n  \"FIS Global\" = makeIcon(\n    iconUrl = \"https://1000logos.net/wp-content/uploads/2021/09/FIS-Logo-768x432.png\",\n    iconWidth = 25,\n    iconHeight = 25\n  ),\n  \"Sasken\" = makeIcon(\n    iconUrl = \"https://www.sasken.com/sites/all/themes/sasken_website/logo.png\",\n    iconWidth = 35,\n    iconHeight = 35,\n  ),\n  \"LTI\" = makeIcon(\n    iconUrl = \"https://www.lntinfotech.com/wp-content/uploads/2021/09/LTI-logo.svg\",\n    iconWidth = 25,\n    iconHeight = 25\n  )\n)\n\n\n# Create the Leaflet map\n\nleaflet(companies_itpl) %>% \n  addMarkers(icon = ~ favicons[ticker], # lookup based on ticker\n             label = ~ companies_itpl$ticker,\n             labelOptions = labelOptions(noHide = F,offset = c(15,-25))) %>%\n  addProviderTiles(\"CartoDB.Positron\")\n\n\n\n\n\n\n3.1.2 Points using sf objects\nWe will use data from an sf data object. This differs from the earlier situation where we had a simple data frame with lon and lat columns. In sf, the lon and lat info is embedded in the geometry column of the sf data frame.\nThe tmap package has a data set of all World metro cities, titled metro. We will plot these on the map and also scale the markers in proportion to one of the feature attributes, pop2030. The popup will be the name of the metro city. We will also use the CartoDB.Positron base map.\nNote that the metro data set has a POINT geometry, as needed!\n\ndata(metro, package = \"tmap\")\nmetro\n\n\n\n  \n\n\nleaflet(data = metro) %>% \n  setView(lat = 18, lng = 77, zoom = 4) %>% \n  \n  # Add CartoDB.Positron\n  addProviderTiles(providers$CartoDB.Positron) %>% # CartoDB Basemap\n  \n  # Add Markers for each airport\n  addCircleMarkers(radius = ~ sqrt(pop2030)/350,\n                   color = \"red\",\n                   popup = paste(\"Name: \", metro$name, \"<br>\",\n                                  \"Population 2030: \", metro$pop2030))\n\n\n\n\n\nWe can also try downloading an sf data frame with POINT geometry from say OSM data<https://osm. Let us get hold of restaurants data in Malleswaram, Bangalore from OSM data:\n\nbbox<- osmdata::getbb(\"Malleswaram, Bengaluru\")\nbbox\n\n       min      max\nx 77.55033 77.59033\ny 12.98274 13.02274\n\nlocations <- osmplotr::extract_osm_objects(\n  bbox = bbox,\n  key = \"amenity\",\n  value = \"restaurant\",\n  return_type = \"point\") \n\nIssuing query to Overpass API ...\n\n\nAnnounced endpoint: z.overpass-api.de/api/\n\n\nQuery complete!\n\n\nconverting OSM data to sf format\n\nlocations <- locations %>% \n  dplyr::filter(cuisine == \"indian\")\nlocations %>% head()\n\n\n\n  \n\n\n# Fontawesome icons seem to work in `leaflet` only up to FontAwesome V4.7.0.\n# The Fontawesome V4.7.0 Cheatsheet is here: <https://fontawesome.com/v4/cheatsheet/>\n\n\nleaflet(data = locations, options = leafletOptions(minZoom = 12)) %>% \n  \n  addProviderTiles(providers$CartoDB.Voyager) %>% \n  \n  # Regular `leaflet` code\n  addAwesomeMarkers(icon = awesomeIcons(icon = \"fa-coffee\", \n                                        library = \"fa\",\n                                        markerColor = \"blue\",\n                                        iconColor = \"black\",\n                                        iconRotate = TRUE),\n                     popup = paste(\"Name: \", locations$name,\"<br>\",\n                           \"Food: \", locations$cuisine)) \n\n\n\n\n\nFontawesome Workaround\nFor more later versions of Fontawesome, here below is a workaround from https://github.com/rstudio/leaflet/issues/691. Despite this some fontawesome icons simply do not seem to show up. ;-()\n\nlibrary(fontawesome)\ncoffee <- makeAwesomeIcon(\n  text = fa(\"mug-hot\"), # mug-hot was introduced in fa version 5\n  iconColor = \"black\",\n  markerColor = \"blue\",\n  library = \"fa\"\n)\n\n\nleaflet(data = locations) %>% \n  addProviderTiles(providers$CartoDB.Voyager) %>% \n  \n  # Workaround code\n\n  addAwesomeMarkers(icon = coffee,\n             popup = paste(\"Name: \", locations$name,\"<br>\",\n                           \"Food: \", locations$cuisine, \"<br>\"))\n\n\n\n\n\nNote that leaflet automatically detects the lon/lat columns from within the POINT geometry column of the sf data frame.\n\n3.1.3 Points using Two-Column Matrices\nWe can now quickly try providing lon and lat info in a two column matrix.This can be useful to plot a bunch of points recorded on a mobile phone app.\n\nmysore5 <- matrix(c(runif(5, 76.652985-0.01, 76.652985+0.01),\n                 runif(5, 12.311827-0.01, 12.311827+0.01)),\n                 nrow = 5)\nmysore5\n\n         [,1]     [,2]\n[1,] 76.65462 12.31162\n[2,] 76.64928 12.30791\n[3,] 76.64946 12.30259\n[4,] 76.64796 12.30682\n[5,] 76.65478 12.31749\n\nleaflet(data = mysore5) %>% \n  addProviderTiles(providers$OpenStreetMap) %>% \n  \n# Pick an icon from <https://www.w3schools.com/bootstrap/bootstrap_ref_comp_glyphs.asp>\n  addAwesomeMarkers(icon = awesomeIcons(\n  icon = 'music',\n  iconColor = 'black',\n  library = 'glyphicon'),\n  popup = \"Carnatic Music !!\")"
  },
  {
    "objectID": "content/labs/r-labs/maps/PlayingwithLeaflet.html#polygons-lines-and-polylines-data-sources-for-leaflet",
    "href": "content/labs/r-labs/maps/PlayingwithLeaflet.html#polygons-lines-and-polylines-data-sources-for-leaflet",
    "title": "Playing with Leaflet",
    "section": "\n3.2 Polygons, Lines, and Polylines Data Sources for leaflet\n",
    "text": "3.2 Polygons, Lines, and Polylines Data Sources for leaflet\n\nWe have seen how to get POINT data into leaflet.\nLine and polygon data can come from a variety of sources:\n\n\nSpatialPolygons, SpatialPolygonsDataFrame, Polygons, and Polygon objects (from the sp package)\n\n\nSpatialLines, SpatialLinesDataFrame, Lines, and Line objects (from the sp package)\n\n\nMULTIPOLYGON, POLYGON, MULTILINESTRING, and LINESTRING objects (from the sf package)\n\n\nmap objects (from the maps package‚Äôs map() function); use map(fill = TRUE) for polygons, FALSE for polylines\n\nTwo-column numeric matrix; the first column is longitude and the second is latitude. Polygons are separated by rows of (NA, NA). It is not possible to represent multi-polygons nor polygons with holes using this method; use SpatialPolygons instead.\n\nWe will concentrate on using sf data into leaflet. We may explore maps() objects at a later date.\n\n3.2.1 Polygons/MultiPolygons and LineString/MultiLineString using sf data frames\nLet us download College buildings, parks, and the cycling lanes in Amsterdam, Netherlands, and plot these in leaflet.\n\nbbox <- osmdata::getbb(\"Amsterdam, Netherlands\")\nbbox\n\n        min       max\nx  4.728756  5.079162\ny 52.278174 52.431064\n\ncolleges <- osmplotr::extract_osm_objects(bbox = bbox,\n                                           key = \"amenity\",\n                                           value = \"college\",\n                                           return_type = \"polygon\" )\n\nIssuing query to Overpass API ...\n\n\nAnnounced endpoint: z.overpass-api.de/api/\n\n\nQuery complete!\n\n\nconverting OSM data to sf format\n\nparks <- osmplotr::extract_osm_objects(bbox = bbox,\n                                           key = \"park\",\n                                           return_type = \"polygon\" )\n\nIssuing query to Overpass API ...\n\n\nAnnounced endpoint: z.overpass-api.de/api/\n\n\nQuery complete!\n\n\nconverting OSM data to sf format\n\nroads <- osmplotr::extract_osm_objects(bbox = bbox,\n                                       key = \"highway\",\n                                       value = \"primary\",\n                                       return_type = \"line\")\n\nIssuing query to Overpass API ...\n\n\nAnnounced endpoint: z.overpass-api.de/api/\n\n\nQuery complete!\n\n\nconverting OSM data to sf format\n\ncyclelanes <-\n  osmplotr::extract_osm_objects(bbox,\n                                key = \"cycleway\",\n                                value =  \"lane\",\n                                return_type = \"line\")\n\nIssuing query to Overpass API ...\n\n\nAnnounced endpoint: z.overpass-api.de/api/\n\n\nQuery complete!\n\n\nconverting OSM data to sf format\n\n\nWe have 17 colleges in our data and 370 parks in our data.\n\nleaflet() %>% \n  addTiles() %>% \n  addPolygons(data = colleges, popup = ~colleges$name) %>% \n  addPolygons(data = parks, color = \"green\", popup = parks$name) %>% \n  addPolylines(data = roads, color = \"red\") %>% \n  addPolylines(data = cyclelanes, color = \"purple\")"
  },
  {
    "objectID": "content/labs/r-labs/maps/PlayingwithLeaflet.html#chapter-3-using-raster-data-in-leaflet",
    "href": "content/labs/r-labs/maps/PlayingwithLeaflet.html#chapter-3-using-raster-data-in-leaflet",
    "title": "Playing with Leaflet",
    "section": "\n3.3 Chapter 3: Using Raster Data in leaflet\n",
    "text": "3.3 Chapter 3: Using Raster Data in leaflet\n\nSo far all the geospatial data we have plotted in leaflet has been vector data. We will now explore how to plot raster data using leaflet. Raster data are used to depict continuous variables across space, such as vegitation, salinity, forest cover etc. Satellite imagery is frequently available as raster data.\n\n3.3.1 Importing Raster Data [Work in Progress!]\nRaster data can be imported into R in many ways:\n\nusing the maptiles package\n\nusing the OpenStreetMap package\n\n\nlibrary(terra)\n\nterra 1.7.18\n\n\n\nAttaching package: 'terra'\n\n\nThe following object is masked from 'package:data.world':\n\n    query\n\n\nThe following objects are masked from 'package:igraph':\n\n    blocks, compare\n\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\nlibrary(maptiles)\n#library(OpenStreetMap) # causes RStudio to crash..."
  },
  {
    "objectID": "content/labs/r-labs/maps/PlayingwithLeaflet.html#adding-legendswork-in-progress",
    "href": "content/labs/r-labs/maps/PlayingwithLeaflet.html#adding-legendswork-in-progress",
    "title": "Playing with Leaflet",
    "section": "\n4.1 Adding Legends[Work in Progress!]",
    "text": "4.1 Adding Legends[Work in Progress!]\n\n## Generate some random lat lon data around Bangalore\ndf <- data.frame(lat = runif(20, min = 11.97, max = 13.07),\n                 lng = runif(20, min = 77.48, max = 77.68),\n                 col = sample(c(\"red\", \"blue\", \"green\"), 20, \n                              replace = TRUE),\n                 stringsAsFactors = FALSE)\n\ndf %>%\n  leaflet() %>%\n  addTiles() %>%\n  addCircleMarkers(color = df$col) %>%\n  addLegend(values = df$col, labels = LETTERS[1:3], colors = c(\"blue\", \"red\", \"green\"))\n\nAssuming \"lng\" and \"lat\" are longitude and latitude, respectively"
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html",
    "href": "content/labs/r-labs/networks/index.html",
    "title": "The Grammar of Networks",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE, warning = FALSE)\n\n########################################\n# For General Data Manipulation\nlibrary(tidyverse)\n\n########################################\n# Network Analysis Library (Handle data and Viz)\nlibrary(igraph)\nlibrary(netrankr)\n\n########################################\n# For Network \"Manipulation\"\nlibrary(tidygraph)\n\n# For Network Visualization\nlibrary(ggraph)\nlibrary(graphlayouts)\nlibrary(visNetwork)\n\n# For \"Network\" Datasets\nlibrary(igraphdata)"
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "href": "content/labs/r-labs/networks/index.html#iconify-openmoji-japanese-symbol-for-beginner-introduction",
    "title": "The Grammar of Networks",
    "section": "\n2  Introduction",
    "text": "2  Introduction\nThis Quarto document is part of my workshop course on R . The material is based on A Layered Grammar of Graphics by Hadley Wickham. The intent of this Course is to build Skill in coding in R, and also appreciate R as a way to metaphorically visualize information of various kinds, using predominantly geometric figures and structures.\nAll Quarto document files combine code, text, web-images, and figures developed using code. Everything is text; code chunks are enclosed in fences (```)"
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#goals",
    "href": "content/labs/r-labs/networks/index.html#goals",
    "title": "The Grammar of Networks",
    "section": "\n3 Goals",
    "text": "3 Goals\nAt the end of this Lab session, we should:\n\nknow the types and structures of network data and be able to work with them\nunderstand the basics of modern network packages in R\nbe able to create network visualizations using tidygraph, ggraph( static visualizations ) and visNetwork (interactive visualizations)\nsee directions for how the network metaphor applies in a variety of domains (e.g.¬†biology/ecology, ideas/influence, technology, transportation, to name a few)"
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#pedagogical-note",
    "href": "content/labs/r-labs/networks/index.html#pedagogical-note",
    "title": "The Grammar of Networks",
    "section": "\n4 Pedagogical Note",
    "text": "4 Pedagogical Note\nThe method followed will be based on PRIMM:\n\n\nPREDICT Inspect the code and guess at what the code might do, write predictions\n\n\nRUN the code provided and check what happens\n\nINFER what the parameters of the code do and write comments to explain. What bells and whistles can you see?\n\nMODIFY the parameters code provided to understand the options available. Write comments to show what you have aimed for and achieved.\n\nMAKE : take an idea/concept of your own, and graph it."
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#graph-metaphors",
    "href": "content/labs/r-labs/networks/index.html#graph-metaphors",
    "title": "The Grammar of Networks",
    "section": "\n5 Graph Metaphors",
    "text": "5 Graph Metaphors\nNetwork graphs are characterized by two key terms: nodes and edges\n\n\nNodes : Entities\n\nMetaphors: Individual People? Things? Ideas? Places? to be connected in the network.\nSynonyms: vertices. Nodes have IDs.\n\n\n\nEdges: Connections\n\nMetaphors: Interactions? Relationships? Influence? Letters sent and received? Dependence? between the entities.\nSynonyms: links, ties.\n\n\n\nIn R, we create network representations using node and edge information. One way in which these could be organized are:\n\n\nNode list: a data frame with a single column listing the node IDs found in the edge list. You can also add attribute columns to the data frame such as the names of the nodes or grouping variables. ( Type? Class? Family? Country? Subject? Race? )\n\n\nNode Table\n\n\n\n\n\n\n\nID\nNode Name\nAttribute? Qualities?Categories? Family? Country?Planet?\n\n\n1\nNed\nNursery School Teacher\n\n\n2\nJaguar Paw\nMain Character, Apocalypto\n\n\n3\nJohn Snow\nEpidemiologist\n\n\n\n\n\nEdge list: data frame containing two columns: source node and destination node of an edge. Source and Destination have node IDs.\n\nWeighted network graph: An edge list can also contain additional columns describing attributes of the edges such as a magnitude aspect for an edge. If the edges have a magnitude attribute the graph is considered weighted.\n\n\nEdges Table\n\nFrom\nTo\nRelationship\nWeightage\n\n\n\n1\n3\nFinancial Dealings\n6\n\n\n2\n1\nHistory Lessons\n2\n\n\n2\n3\nVaccination\n15\n\n\n\n\n\nLayout: A geometric arrangement of nodes and edges.\n\nMetaphors: Location? Spacing? Distance? Coordinates? Colour? Shape? Size? Provides visual insight due to the arrangement.\n\n\n\nLayout Algorithms : Method to arranges nodes and edges with the aim of optimizing some metric .\n\nMetaphors: Nodes are masses and edges are springs. The Layout algorithm minimizes the stretching and compressing of all springs.(BTW, are the Spring Constants K the same for all springs?‚Ä¶)\n\n\nDirected and undirected network graph: If the distinction between source and target is meaningful, the network is directed. If the distinction is not meaningful, the network is undirected. Directed edges represent an ordering of nodes, like a relationship extending from one node to another, where switching the direction would change the structure of the network. Undirected edges are simply links between nodes where order does not matter.\n\nExamples:\n - The World Wide Web is an example of a directed network because\n hyperlinks connect one Web page to another, but not necessarily \n the other way around.\n\n - Co-authorship networks represent examples of un-directed networks,\nwhere nodes are authors and they are connected by an edge if they\nhave written a publication together\n\n - When people send e-mail to each other, the distinction between the\nsender (source) and the recipient (target) is clearly meaningful,\ntherefore the network is directed.\n\n\nConnected and Disconnected graphs: If there is some path from any node to any other node, the Networks is said to be Connected. Else, Disconnected."
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#predictruninfer--1",
    "href": "content/labs/r-labs/networks/index.html#predictruninfer--1",
    "title": "The Grammar of Networks",
    "section": "\n6 Predict/Run/Infer -1",
    "text": "6 Predict/Run/Infer -1\n\n6.1 Using tidygraph and ggraph\n\ntidygraph and ggraph are modern R packages for network data. Graph Data setup and manipulation is done in tidygraph and graph visualization with ggraph.\n\n\ntidygraph Data -> ‚ÄúNetwork Object‚Äù in R.\n\nggraph Network Object -> Plots using a chosen layout/algo.\n\nBoth leverage the power of igraph, which is the Big Daddy of all network packages. We will be using the Grey‚Äôs Anatomy dataset in our first foray into networks.\n\n6.2 Step1. Read the data\nDownload these two datasets into your current project-> data folder.\n Grey‚Äôs Anatomy Nodes \n Grey‚Äôs Anatomy Nodes \n\ngrey_nodes <- read_csv(\"../../../materials/data/networks/grey_nodes.csv\")\ngrey_edges <- read_csv(\"../../../materials/data/networks/grey_edges.csv\")\n\ngrey_nodes\n\n\n\n  \n\n\ngrey_edges\n\n\n\n  \n\n\n\n\n\nQuestions and Inferences #1:\n\n\nLook at the console output thumbnail. What does for example name = col_character mean? What attributes (i.e.¬†extra information) are seen for Nodes and Edges? Understand the data in both nodes and edges as shown in the second and third thumbnails. Write some comments and inferences here.\n\n\n\n6.3 Step 2.Create a network object using tidygraph:\nKey function:\n\n\ntbl_graph(): (aka ‚Äútibble graph‚Äù). Key arguments: nodes, edges and directed. Note this is a very versatile command and can take many input forms, such as data structures that result from other packages. Type ?tbl_graph in the Console and see the Usage section.\n\n\nga <- tbl_graph(nodes = grey_nodes, \n                edges = grey_edges, \n                directed = FALSE)\nga\n\n# A tbl_graph: 54 nodes and 57 edges\n#\n# An undirected simple graph with 4 components\n#\n# A tibble: 54 √ó 7\n  name               sex   race  birthyear position  season sign  \n  <chr>              <chr> <chr>     <dbl> <chr>      <dbl> <chr> \n1 Addison Montgomery F     White      1967 Attending      1 Libra \n2 Adele Webber       F     Black      1949 Non-Staff      2 Leo   \n3 Teddy Altman       F     White      1969 Attending      6 Pisces\n4 Amelia Shepherd    F     White      1981 Attending      7 Libra \n5 Arizona Robbins    F     White      1976 Attending      5 Leo   \n6 Rebecca Pope       F     White      1975 Non-Staff      3 Gemini\n# ‚Ñπ 48 more rows\n#\n# A tibble: 57 √ó 4\n   from    to weight type    \n  <int> <int>  <dbl> <chr>   \n1     5    47      2 friends \n2    21    47      4 benefits\n3     5    46      1 friends \n# ‚Ñπ 54 more rows\n\n\n\n\nQuestions and Inferences #2:\n\n\nQuestions and Inferences: What information does the graph object contain? What attributes do the nodes have? What about the edges?\n\n\n\n6.4 Step 3. Plot using ggraph\n\n3a. Quick Plot: autograph() This is to check quickly is the data is imported properly and to decide upon going on to a more elaborate plotting.\n\nautograph(ga)\n\n\n\n\n\n\nQuestions and Inferences #3:\n\n\nQuestions and Inferences: Describe this graph, in simple words here. Try to use some of the new domain words we have just acquired: nodes/edges, connected/disconnected, directed/undirected.\n\n\n3b. More elaborate plot\nKey functions:\n\n\nggraph(layout = \"......\"): Create classic node-edge diagrams; i.e. Sets up the graph. Rather like ggplot for networks!\n\nTwo kinds of geom: one set for nodes, and another for edges\n\ngeom_node_point(aes(.....)): Draws node as ‚Äúpoints‚Äù. Alternatives are circle / arc_bar / tile / voronoi. Remember the geoms that we have seen before in Grammar of Graphics!\ngeom_edge_link(aes(.....)): Draws edges as ‚Äúlinks‚Äù. Alternatives are arc / bend / elbow / hive / loop / parallel / diagonal / point / span /tile.\ngeom_node_text(aes(label = ......), repel = TRUE): Adds text labels (non-overlapping). Alternatives are label /...\nlabs(title = \"....\", subtitle = \"....\", caption = \"....\"): Change main titles, axis labels and legend titles. We know this from our work with ggplot.\n\n\n# Write Comments next to each line \n# About what that line does for the overall graph\n\nggraph(graph = ga,  layout = \"kk\") +\n  #\n  \n  geom_edge_link(width = 2, color = \"pink\") +\n  #\n  \n  geom_node_point(\n    shape = 21,\n    size = 8,\n    fill = \"blue\",\n    color = \"green\",\n    stroke = 2\n  ) +\n  #\n  \n  labs(title = \"Whoo Hoo! My first silly Grey's Anatomy graph in R!\",\n       subtitle = \"Why did I ever get in this course...\",\n       caption = \"Bro, they are doing **cool** things in the other\n       classes...\") \n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` in the `default_aes` field and elsewhere instead.\n\n\n\n\n\n\n\nQuestions and Inferences #3:\n\n\nQuestions and Inferences: What parameters have been changed here, compared to the earlier graph? Where do you see these changes in the code above?\n\n\nLet us Play with this graph and see if we can make some small changes. Colour? Fill? Width? Size? Stroke? Labs? Of course!\n\n# Change the parameters in each of the commands here to new ones\n# Use fixed values for colours or sizes...etc. \n\nggraph(graph = ga,  layout = \"kk\") + \n  geom_edge_link(width = 2) + \n  geom_node_point(shape = 21, size = 8, \n                  fill = \"blue\", \n                  color = \"green\", \n                  stroke = 2) +\n  labs(title = \"Whoo Hoo! My next silly Grey's Anatomy graph in R!\",\n       subtitle = \"Why did I ever get in this course...\",\n       caption = \"Bro, they are doing cool things in the other \n       classes...\") \n\n\n\n\n\n\nQuestions and Inferences #4:\n\n\nQuestions and Inferences: What did the shape parameter achieve? What are the possibilities with shape? How about including alpha?\n\n\n3c. Aesthetic Mapping from Node and Edge attribute columns\nUp to now, we have assigned specific numbers to geometric aesthetics such as shape and size. Now we are ready ( maybe ?) change the meaning and significance of the entire graph and each element within it, and use aesthetics / metaphoric mappings to achieve new meanings or insights. Let us try using aes() inside each geom to map a variable to a geometric aspect.\nDon‚Äôt try to use more than 2 aesthetic mappings simultaneously!!\nThe node elements we can tweak are:\n\nTypes of Nodes: geom_node_****()\n\nNode Parameters: inside geom_node_****(aes(...............))\n-aes(alpha  = node-variable) : opacity; a value between 0 and 1\n-aes(shape  = node-variable) : node shape\n-aes(colour = node-variable) : node colour\n-aes(fill   = node-variable) : fill colour for node\n-aes(size   = node-variable) : size of node\n\nThe edge elements we can tweak are:\n\nType of Edges‚Äù geom_edge_****()\n\nEdge Parameters: inside geom_edge_****(aes(...............))\n-aes(colour = edge-variable) : colour of the edge\n-aes(width  = edge-variable) : width of the edge\n-aes(label  = some_variable) : labels for the edge\n\nType ?geom_node_point and ?geom-edge_link in your Console for more information.\n\nggraph(graph = ga, layout = \"fr\") +\n  geom_edge_link0(aes(width = weight)) + # change variable here\n  \n  geom_node_point(aes(color = race), size = 6) + # change variable here\n\n\n  labs(title = \"Whoo Hoo! Yet another Grey's Anatomy graph in R!\") \n\n\n\n\n\n\nQuestions and Inferences #5:\n\n\nQuestions and Inferences: Describe some of the changes here. What types of edges worked? Which variables were you able to use for nodes and edges and how? What did not work with either of the two?"
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#predictreuseinfer-2",
    "href": "content/labs/r-labs/networks/index.html#predictreuseinfer-2",
    "title": "The Grammar of Networks",
    "section": "\n7 Predict/Reuse/Infer-2",
    "text": "7 Predict/Reuse/Infer-2\n\n# Arc diagram\nggraph(ga, layout = \"linear\") +\n  geom_edge_arc(aes(width = weight), alpha = 0.8) +\n  scale_edge_width(range = c(0.2, 2)) +\n  geom_node_point(size = 2, colour = \"red\") +\n  labs(edge_width = \"Weight\") \n\n\n\n\n\n\nQuestions and Inferences #6:\n\n\nQuestions and Inferences: How does this graph look ‚Äúmetaphorically‚Äù different? Do you see a difference in the relationships between people here? Why?\n\n\n\n# Coord diagram, circular\nggraph(ga, layout = \"linear\", circular = TRUE) + \n  geom_edge_arc(aes(width = weight), alpha = 0.8) + \n  scale_edge_width(range = c(0.2, 2)) +\n  geom_node_point(size = 4,colour = \"red\") + \n  geom_node_text(aes(label = name),repel = TRUE, size = 3,\n                 max.overlaps = 20) +\n  labs(edge_width = \"Weight\") +\n  theme_graph() \n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\n\nQuestions and Inferences #7:\n\n\nQuestions and Inferences: How does this graph look ‚Äúmetaphorically‚Äù different? Do you see a difference in the relationships between people here? Why?"
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#hierarchical-layouts",
    "href": "content/labs/r-labs/networks/index.html#hierarchical-layouts",
    "title": "The Grammar of Networks",
    "section": "\n8 Hierarchical layouts",
    "text": "8 Hierarchical layouts\nThese provide for some alternative metaphorical views of networks. Note that not all layouts are possible for all datasets!!\n\nset_graph_style()\n\n# This dataset contains the graph that describes the class \n# hierarchy for the Flare visualization library.\n# Type ?flare in your Console\nhead(flare$vertices)\n\n\n\n  \n\n\nhead(flare$edges)\n\n\n\n  \n\n\n# flare class hierarchy\ngraph = tbl_graph(edges = flare$edges, nodes = flare$vertices)\n\n# dendrogram\nggraph(graph, layout = \"dendrogram\") + \n  geom_edge_diagonal() + \n  labs(title = \"Dendrogram\")\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n# circular dendrogram\nggraph(graph, layout = \"dendrogram\", circular = TRUE) + \n  geom_edge_diagonal() + \n  geom_node_point(aes(filter = leaf)) + \n  coord_fixed()+ \n  labs(title = \"Circular Dendrogram\")\n\n\n\n# rectangular tree map\nggraph(graph, layout = \"treemap\", weight = size) + \n  geom_node_tile(aes(fill = depth), size = 0.25) + \n  labs(title = \"Rectangular Tree Map\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n# circular tree map\nggraph(graph, layout = \"circlepack\", weight = size) + \n  geom_node_circle(aes(fill = depth), size = 0.25, n = 50) + \n  coord_fixed() + \n  labs(title = \"Circular Tree Map\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n# icicle\nggraph(graph, layout = \"partition\") + \n  geom_node_tile(aes(y = -y, fill = depth))\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n# sunburst (circular icicle)\nggraph(graph, layout = \"partition\", circular = TRUE) +\n  geom_node_arc_bar(aes(fill = depth)) +\n  coord_fixed() + \n  labs(title = \"Circular Icicle\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\n\nQuestions and Inferences #8:\n\n\nQuestions and Inferences: How do graphs look ‚Äúmetaphorically‚Äù different? Do they reveal different aspects of the group? How?"
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#faceting",
    "href": "content/labs/r-labs/networks/index.html#faceting",
    "title": "The Grammar of Networks",
    "section": "\n9 Faceting",
    "text": "9 Faceting\nFaceting allows to create sub-plots according to the values of a qualitative attribute on nodes or edges.\n\n# setting theme_graph \nset_graph_style()\n\n\n# facet edges by type\nggraph(ga,layout = \"linear\", circular = TRUE) + \n  geom_edge_link(aes(color = type)) + \n  geom_node_point() +\n  facet_edges(~ type) +\n  theme(aspect.ratio = 1)\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\nnot found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n# facet nodes by sex\nggraph(ga,layout = \"linear\", circular = TRUE) + \n  geom_edge_link() + \n  geom_node_point() +\n  facet_nodes(~race) +\n  theme(aspect.ratio = 1)\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n# facet both nodes and edges\nggraph(ga,layout = \"linear\", circular = TRUE) + \n  geom_edge_link(aes(color = type)) + \n  geom_node_point() +\n  facet_graph(type ~ race) + \n  th_foreground(border = TRUE) +\n  theme(aspect.ratio = 1, legend.position = \"top\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\n\nQuestions and Inferences #9:\n\n\nQuestions and Inferences: Does splitting up the main graph into subnetworks give you more insight? Describe some of these."
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#network-analysis-with-tidygraph",
    "href": "content/labs/r-labs/networks/index.html#network-analysis-with-tidygraph",
    "title": "The Grammar of Networks",
    "section": "\n10 Network analysis with tidygraph",
    "text": "10 Network analysis with tidygraph\nThe data frame graph representation can be easily augmented with metrics or statistics computed on the graph. Remember how we computed counts with the penguin dataset in Grammar of Graphics.\nBefore computing a metric on nodes or edges use the activate() function to activate either node or edge data frames. Use dplyr verbs (filter, arrange, mutate) to achieve your computation in the proper way.\n\n10.1 Network Centrality\nCentrality is a an ‚Äúill-defined‚Äù metric of node and edge importance in a network. It is therefore calculated in many ways. Type ?centrality in your Console.\n\n\nStandards\n\n\nLet‚Äôs add a few columns to the nodes and edges based on network centrality measures:\n\nga %>% \n  activate(nodes) %>% \n  \n  # Node with  the most connections?\n  mutate(degree = centrality_degree(mode = c(\"in\"))) %>% \n  filter(degree > 0) %>% \n  \n  activate(edges) %>% \n  # \"Busiest\" edge?\n  mutate(betweenness = centrality_edge_betweenness())\n\n# A tbl_graph: 54 nodes and 57 edges\n#\n# An undirected simple graph with 4 components\n#\n# A tibble: 57 √ó 5\n   from    to weight type     betweenness\n  <int> <int>  <dbl> <chr>          <dbl>\n1     5    47      2 friends         20.3\n2    21    47      4 benefits        44.7\n3     5    46      1 friends         39  \n4     5    41      1 friends         66.3\n5    18    41      6 friends         39  \n6    21    41     12 benefits        91.5\n# ‚Ñπ 51 more rows\n#\n# A tibble: 54 √ó 8\n  name               sex   race  birthyear position  season sign   degree\n  <chr>              <chr> <chr>     <dbl> <chr>      <dbl> <chr>   <dbl>\n1 Addison Montgomery F     White      1967 Attending      1 Libra       3\n2 Adele Webber       F     Black      1949 Non-Staff      2 Leo         1\n3 Teddy Altman       F     White      1969 Attending      6 Pisces      4\n# ‚Ñπ 51 more rows\n\n\nPackages tidygraph and ggraph can be pipe-lined to perform analysis and visualization tasks in one go.\n\n# setting theme_graph \nset_graph_style()\n\nga %>% \n  activate(nodes) %>% \n  \n  # Who has the most connections?\n  mutate(degree = centrality_degree()) %>% \n  \n  activate(edges) %>% \n  # Who is the go-through person?\n  mutate(betweenness = centrality_edge_betweenness()) %>%\n  \n  # Now to continue with plotting\n  ggraph(layout = \"nicely\") +\n  geom_edge_link(aes(alpha = betweenness)) +\n  geom_node_point(aes(size = degree, colour = degree)) + \n  \n  # discrete colour legend\n  scale_color_gradient(guide = \"legend\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n# or even less typing\n  ggraph(ga,layout = \"nicely\") +\n  geom_edge_link(aes(alpha = centrality_edge_betweenness())) +\n  geom_node_point(aes(colour = centrality_degree(), \n                      size = centrality_degree())) + \n  scale_color_gradient(guide = \"legend\",\n                       low = \"green\",\n                       high = \"red\") \n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\n\nQuestions and Inferences #10:\n\n\nQuestions and Inferences: How do the Centrality Measures show up in the graph? Would you ‚Äúagree‚Äù with the way we have done it? Try to modify the aesthetics by copy-pasting this chunk below and see how you can make an alternative representation.\n\n\n\n10.2 Analysis and visualizing Network Communities\nWho is close to whom? Which are the groups you can see?\n\n# setting theme_graph \nset_graph_style()\n\n\n# visualize communities of nodes\nga %>% \n  activate(nodes) %>%\n  mutate(community = as.factor(group_louvain())) %>% \n  ggraph(layout = \"graphopt\") + \n  geom_edge_link() + \n  geom_node_point(aes(color = community), size = 5) \n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\n\nQuestions and Inferences #11:\n\n\nQuestions and Inferences: Is the Community depiction clear? How would you do it, with which aesthetic? Copy Paste this chunk below and try.\n\n\n\n10.3 Interactive Graphs with visNetwork\n\nExploring the VisNetwork package. Make graphs wiggle and shake using tidy commands! The package implements interactivity using the physical metaphor of weights and springs we discussed earlier.\nThe visNetwork() function uses a nodes list and edges list to create an interactive graph. The nodes list must include an ‚Äúid‚Äù column, and the edge list must have ‚Äúfrom‚Äù and ‚Äúto‚Äù columns. The function also plots the labels for the nodes, using the names of the cities from the ‚Äúlabel‚Äù column in the node list.\n\nlibrary(visNetwork)\n\n# Prepare the data for plotting by visNetwork\ngrey_nodes\n\n\n\n  \n\n\ngrey_edges\n\n\n\n  \n\n\n# Relabel greys anatomy nodes and edges for VisNetwork\ngrey_nodes_vis <- grey_nodes %>% \n  rowid_to_column(var = \"id\") %>% \n  rename(\"label\" = name) %>% \n  mutate(sex = case_when(sex == \"F\" ~ \"Female\",\n                         sex == \"M\" ~ \"Male\")) %>% \n  replace_na(., list(sex = \"Transgender?\")) %>% \n  rename(\"group\" = sex)\ngrey_nodes_vis\n\n\n\n  \n\n\ngrey_edges_vis <- grey_edges %>% \n  select(from, to) %>% \n  left_join(., grey_nodes_vis, \n            by = c(\"from\" = \"label\")) %>% \n  left_join(., grey_nodes_vis, \n            by = c(\"to\" = \"label\")) %>%\n  select(\"from\"= id.x, \"to\" = id.y)\ngrey_edges_vis\n\n\n\n  \n\n\n\nUsing fontawesome icons\n\ngrey_nodes_vis %>%\n\n  visNetwork(nodes = ., edges = grey_edges_vis) %>% \n  visNodes(font = list(size = 40)) %>% \n  \n  # Colour and icons for each of the gender-groups\n  visGroups(groupname = \"Female\", shape = \"icon\", \n            icon = list(code = \"f182\", size = 75, color = \"tomato\"),\n            shadow = list(enabled = TRUE)) %>% \n  \n  visGroups(groupname = \"Male\", shape = \"icon\", \n            icon = list(code = \"f183\", size = 75, color = \"slateblue\"), \n            shadow = list(enabled = TRUE)) %>% \n  \n  visGroups(groupname = \"Transgender?\", shape = \"icon\", \n            icon = list(code = \"f22c\", size = 75, color = \"fuchsia\"), \n            shadow = list(enabled = TRUE)) %>% \n  \n  #visLegend() %>%\n  #Add the fontawesome icons!!\n  addFontAwesome() %>% \n  \n  # Add Interaction Controls\n  visInteraction(navigationButtons = TRUE,\n                 hover = TRUE,\n                 selectConnectedEdges = TRUE,\n                 hoverConnectedEdges = TRUE,\n                 zoomView = TRUE)\n\n\n\n\n\nThere is another family of icons available in visNetwork, called ionicons. Let‚Äôs see how they look:\n\ngrey_nodes_vis %>%\n\n  visNetwork(nodes = ., edges = grey_edges_vis,) %>%\n  visLayout(randomSeed = 12345) %>%\n  visNodes(font = list(size = 50)) %>%\n  visEdges(color = \"green\") %>%\n  visGroups(\n    groupname = \"Female\",\n    shape = \"icon\",\n    icon = list(\n      face = 'Ionicons',\n      code = \"f25d\",\n      color = \"fuchsia\",\n      size = 125\n    )\n  ) %>%\n  \n  visGroups(\n    groupname = \"Male\",\n    shape = \"icon\",\n    icon = list(\n      face = 'Ionicons',\n      code = \"f202\",\n      color = \"green\",\n      size = 125\n    )\n  ) %>%\n  \n  visGroups(\n    groupname = \"Transgender?\",\n    shape = \"icon\",\n    icon = list(\n      face = 'Ionicons',\n      code = \"f233\",\n      color = \"dodgerblue\",\n      size = 125\n    )\n  ) %>%\n  visLegend() %>%\n  addIonicons() %>%\n  visInteraction(\n    navigationButtons = TRUE,\n    hover = TRUE,\n    selectConnectedEdges = TRUE,\n    hoverConnectedEdges = TRUE,\n    zoomView = TRUE\n  )\n\n\n\n\n\nSome idea of interactivity and controls with visNetwork:\n Star Wars Nodes \n Star Wars Edges \n\n# let's look again at the data\nstarwars_nodes <- read_csv(\"../../../materials/data/networks/star-wars-network-nodes.csv\")\n\nRows: 22 Columns: 2\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (1): name\ndbl (1): id\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nstarwars_edges <- read_csv(\"../../../materials/data/networks/star-wars-network-edges.csv\")\n\nRows: 60 Columns: 3\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (2): source, target\ndbl (1): weight\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n# We need to rename starwars nodes dataframe and edge dataframe columns for visNetwork\nstarwars_nodes_vis <- \n  starwars_nodes %>% \n  rename(\"label\" = name)\n\n# Convert from and to columns to **node ids**\nstarwars_edges_vis <- \n  starwars_edges %>% \n  \n  # Matching Source <- Source Node id (\"id.x\")\n  left_join(., starwars_nodes_vis, by = c(\"source\" = \"label\")) %>% \n  \n  # Matching Target <- Target Node id (\"id.y\")\n  left_join(., starwars_nodes_vis, by = c(\"target\" = \"label\")) %>% \n  \n# Select \"id.x\" and \"id.y\" ONLY\n# Rename them as \"from\" and \"to\"\n# keep \"weight\" column for aesthetics of edges\n  select(\"from\" = id.x, \"to\" = id.y, \"value\" = weight)\n\n# Check everything once\nstarwars_nodes_vis\n\n\n\n  \n\n\nstarwars_edges_vis\n\n\n\n  \n\n\n\nOk, let‚Äôs make things move and shake!!\n\nvisNetwork(nodes = starwars_nodes_vis,\n           edges = starwars_edges_vis) %>% \n  visNodes(font = list(size = 30), shape = \"icon\", \n           icon = list(code = \"f1e3\", size = 75)) %>% \n  addFontAwesome() %>% \n  visEdges(color = \"red\")\n\n\n\n\n\n\nvisNetwork(nodes = starwars_nodes_vis,\n           edges = starwars_edges_vis) %>% \n  visNodes(font = list(size = 30)) %>% \n  visEdges(color = \"red\")"
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#your-assignments",
    "href": "content/labs/r-labs/networks/index.html#your-assignments",
    "title": "The Grammar of Networks",
    "section": "\n11 Your Assignments:",
    "text": "11 Your Assignments:\n\n11.1 Make-1 : With a ready made dataset\nStep 0. Fire up a New Project! Always!\nStep 1. Fire up a new Quarto document. Fill in the YAML header.\nStep 2. Take any one of the ‚ÄúMake1-Datasets‚Äù datasets described below.\nStep 3. Document contents:\n\nIntroduce / Inspect in R your data and describe\nIntroduce your Purpose\nCreate graph objects\nTry different layouts\nWrite comments in the code\nWrite narrative in text with sections, bold ,italic etc.\n\nStep 4. Knit before you submit. Submit only your renderable .qmd file.\n\n11.2 Make1 - Datasets:\n\nAirline Data:\n\n Airlines Nodes \n Airlines Edges \n\nStart with this bit of code in your second chunk, after set up\n\n\n\nairline_nodes <-\n  read_csv(\"./mydatafolder/AIRLINES-NODES.csv\") %>% \n  mutate(Id = Id + 1)\n\nairline_edges <-\n  read_csv(\"./mydatafolder/AIRLINES-EDGES.csv\") %>%\n  mutate(Source = Source + 1, Target = Target + 1)\n\n\nThe Famous Zachary Karate Club dataset\n\n\n\nStart with pulling this data into your Rmarkdown:\ndata(‚Äúkarate‚Äù,package= ‚Äúigraphdata‚Äù) karate\n\nTry ?karate in the console\nNote that this is not a set of nodes, nor edges, but already a graph-object!\nSo no need to create a graph object using tbl_graph.\nYou will need to just go ahead and plot using ggraph.\n\n\nGame of Thrones:\n\n GoT Networks \n\nStart with pulling this data into your Rmarkdown:\n\n\nGoT <- read_rds(\"../../../materials/data/networks/GoT.RDS\")\n\n\nNote that this is a list of 7 graphs from Game of Thrones.\nSelect one using GoT[[index]] where index = 1‚Ä¶7 and then plot directly.\nTry to access the nodes and edges and modify them using any attribute data\n\n\nAny other graph dataset from igraphdata (type ?igraphdata in console)\n\n\nAsk me for help if you need any\n\n11.3 Make-2: Literary Network with TV Show / Book / Story / Play\nThis is in groups. Groups of 4. To be announced\nYou need to create a Network Graph for your favourite Book, play, TV serial or Show. (E.g. Friends, BBT, or LB or HIMYM, B99, TGP, JTV‚Ä¶or Hamlet, Little Women , Pride and Prejudice, or LoTR)\nStep 1. Go to: Literary Networks for instructions.\nStep 2. Make your data using the instructions.\n\nIn the nodes excel, use id and names as your columns. Any other details in other columns to the right.\nIn your edges excel, use from and to are your first columns. Entries in these columns can be names or ids but be consistent and don‚Äôt mix.\n\nStep 3. Decide on 3 answers that you to seek and plan to make graphs for.\nStep 4. Create graph objects. Say 3 visualizations.\nStep 5. Write comments/answers in the code and narrative text. Add pictures from the web using Markdown syntax.\nStep 6. Write Reflection ( ok, a short one!) inside your Quarto document. Make sure it renders !!\nStep 7. Group Submission: Submit the render-able .qmd file AND the data. Quarto Markdown with joint authorship. Each person submits on their Assignments. All get the same grade on this one.\nAsk me for clarifications on what to do after you have read the Instructions in your group."
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#read-more",
    "href": "content/labs/r-labs/networks/index.html#read-more",
    "title": "The Grammar of Networks",
    "section": "\n12 Read more",
    "text": "12 Read more\n\nThomas Lin Pedersen - 1 giraffe, 2 giraffe,GO!\nIgraph: Network Analysis and Visualization. https://CRAN.R-project.org/package=igraph.\nPedersen, Thomas Lin. 2017a. Ggraph: An Implementation of Grammar of Graphics for Graphs and Networks. https://CRAN.R-project.org/package=ggraph.\n‚Äî‚Äî‚Äî. 2017b. Tidygraph: A Tidy Api for Graph Manipulation. https://CRAN.R-project.org/package=tidygraph.\nTyner, Sam, Fran√ßois Briatte, and Heike Hofmann. 2017. ‚ÄúNetwork Visualization with ggplot2.‚Äù The R Journal 9 (1): 27‚Äì59. https://journal.r-project.org/archive/2017/RJ-2017-023/index.html.\nNetwork Datasets https://icon.colorado.edu/#!/networks\nYunran Chen, Introduction to Network Analysis Using R"
  },
  {
    "objectID": "content/labs/r-labs/pronouns/pronouns.html",
    "href": "content/labs/r-labs/pronouns/pronouns.html",
    "title": "Lab-02: Pronouns and Data",
    "section": "",
    "text": "Understand different kinds of data variables\nAppreciate how they can be identified based on the Interrogative Pronouns they answer to\nUnderstand how each kind of variable lends itself to a specific geometric aspect in the data visualization.\nUnderstand how ask Questions of Data to develop Visualizations"
  },
  {
    "objectID": "content/labs/r-labs/pronouns/pronouns.html#set-up",
    "href": "content/labs/r-labs/pronouns/pronouns.html#set-up",
    "title": "Lab-02: Pronouns and Data",
    "section": "\n2.1 Set Up",
    "text": "2.1 Set Up\nThe setup code chunk below brings into our coding session R packages that provide specific computational abilities and also datasets which we can use.\nTo reiterate: Packages and datasets are not the same thing !! Packages are (small) collections of programs. Datasets are just‚Ä¶.information."
  },
  {
    "objectID": "content/labs/r-labs/pronouns/pronouns.html#the-penguins-dataset",
    "href": "content/labs/r-labs/pronouns/pronouns.html#the-penguins-dataset",
    "title": "Lab-02: Pronouns and Data",
    "section": "\n5.1 The penguins dataset",
    "text": "5.1 The penguins dataset\n\nnames(penguins) # Column, i.e. Variable names\n\n[1] \"species\"           \"island\"            \"bill_length_mm\"   \n[4] \"bill_depth_mm\"     \"flipper_length_mm\" \"body_mass_g\"      \n[7] \"sex\"               \"year\"             \n\nhead(penguins) # first six rows\n\n\n\n  \n\n\ntail(penguins) # Last six rows\n\n\n\n  \n\n\ndim(penguins) # Size of dataset\n\n[1] 344   8\n\n# Check for missing data\nany(is.na(penguins) == TRUE)\n\n[1] TRUE\n\n\n\n\n\n\n\n\nInspect the Data\n\n\n\n\nWhat are the variable names()?\nWhat would be the Question you might have asked to obtain each of the variables?\nWhat further questions/meta questions would you ask to ‚Äúprocess‚Äù that variable? ( Hint: Add another word after any of the Interrogative Pronouns, e.g.¬†How‚Ä¶MANY?)\nWhere might the answers take your story?\n\n\n\n\n\n\n\n\n\nYour Turn #1\n\n\n\nState a few questions after discussion with your friend and state possible variables, or what you could DO with the variables, as an answer.\nE.g. Q. How many penguins? A. We need to count‚Ä¶rows?"
  },
  {
    "objectID": "content/labs/r-labs/pronouns/pronouns.html#pronouns-and-variables",
    "href": "content/labs/r-labs/pronouns/pronouns.html#pronouns-and-variables",
    "title": "Lab-02: Pronouns and Data",
    "section": "\n5.2 Pronouns and Variables",
    "text": "5.2 Pronouns and Variables\nIn the Table below, we have a rough mapping of interrogative pronouns to the kinds of variables in the data:\n\n\n\n\n\n\n No \n    Pronoun \n    Answer \n    Variable/Scale \n    Example \n    What Operations? \n  \n\n\n 1 \n    How Many / Much / Heavy? Few? Seldom? Often? When? \n    Quantities, with Scale and a Zero Value.Differences and Ratios /Products are meaningful. \n    Quantitative/Ratio \n    Length,Height,Temperature in Kelvin,Activity,Dose Amount,Reaction Rate,Flow Rate,Concentration,Pulse,Survival Rate \n    Correlation \n  \n\n 2 \n    How Many / Much / Heavy? Few? Seldom? Often? When? \n    Quantities with Scale.\nDifferences are meaningful, but not products or ratios \n    Quantitative/Interval \n    pH,SAT score(200-800),Credit score(300-850),SAT score(200-800),Year of Starting College \n    Mean,Standard Deviation \n  \n\n 3 \n    How, What Kind, What Sort \n    A Manner / Method, Type or Attribute from a list, with list items in some \" order\" ( e.g. good, better, improved, best..) \n    Qualitative/Ordinal \n    Socioeconomic status (Low income, Middle income, High income),Education level (HighSchool, BS, MS, PhD),Satisfaction rating(Very much Dislike, Dislike, Neutral, Like, Very Much Like) \n    Median,Percentile \n  \n\n 4 \n    What, Who, Where, Whom, Which \n    Name, Place, Animal, Thing \n    Qualitative/Nominal \n    Name \n    Count no. of cases,Mode \n  \n\n\n\n\n\nAs you go from Qualitative to Quantitative data types in the table, I hope you can detect a movement from fuzzy groups/categories to more and more crystallized numbers. Each variable/scale can be subjected to the operations of the previous group. In the words of S.S. Stevens (https://psychology.okstate.edu/faculty/jgrice/psyc3214/Stevens_FourScales_1946.pdf)\n\nthe basic operations needed to create each type of scale is cumulative: to an operation listed opposite a particular scale must be added all those operations preceding it.\n\nDo think about this as you work with data.\n\nDo take a look at these references:\n\nhttps://stats.idre.ucla.edu/other/mult-pkg/whatstat/what-is-the-difference-between-categorical-ordinal-and-interval-variables/\nhttps://www.freecodecamp.org/news/types-of-data-in-statistics-nominal-ordinal-interval-and-ratio-data-types-explained-with-examples/"
  },
  {
    "objectID": "content/labs/r-labs/pronouns/pronouns.html#the-mpg-dataset",
    "href": "content/labs/r-labs/pronouns/pronouns.html#the-mpg-dataset",
    "title": "Lab-02: Pronouns and Data",
    "section": "\n5.3 The mpg dataset",
    "text": "5.3 The mpg dataset\n\nnames(mpg) # Column, i.e. Variable names\n\n [1] \"manufacturer\" \"model\"        \"displ\"        \"year\"         \"cyl\"         \n [6] \"trans\"        \"drv\"          \"cty\"          \"hwy\"          \"fl\"          \n[11] \"class\"       \n\nhead(mpg) # first six rows\n\n\n\n  \n\n\ntail(mpg) # Last six rows\n\n\n\n  \n\n\ndim(mpg) # Size of dataset\n\n[1] 234  11\n\n# Check for missing data\nany(is.na(mpg) == TRUE)\n\n[1] FALSE\n\n\n\n5.3.1 YOUR TURN-2\nLook carefully at the variables here. How would you interpret say the cyl variable? Is it a number and therefore Quantitative, or could it be something else?"
  },
  {
    "objectID": "content/labs/r-labs/pronouns/pronouns.html#single-qualitativecategorical-nominal-variable",
    "href": "content/labs/r-labs/pronouns/pronouns.html#single-qualitativecategorical-nominal-variable",
    "title": "Lab-02: Pronouns and Data",
    "section": "\n6.1 Single Qualitative/Categorical/ Nominal Variable",
    "text": "6.1 Single Qualitative/Categorical/ Nominal Variable\n\nQuestions: Which? What Kind? How? How many of each Kind?\n\n\nIsland ( Which island ? )\nSpecies ( Which Species? )\n\n\nCalculations: No of levels / Counts for each level\n\n\n\n\ncount / tally of no. of penguins on each island or in each species\n\nsort and order by island or species\n\n\nCharts: Bar Chart / Pie Chart / Tree Map\n\n\n\ngeom_bar / geom_bar + coord_polar() / Find out!!\n\n\npenguins %>% count(species)\n\n\n\n  \n\n\n\n\nggplot(penguins) + geom_bar(aes(x = island))\n\n\n\nggplot(penguins) + geom_bar(aes(x = sex))\n\n\n\n\n\n6.1.1 YOUR TURN-3"
  },
  {
    "objectID": "content/labs/r-labs/pronouns/pronouns.html#single-quantitative-variable",
    "href": "content/labs/r-labs/pronouns/pronouns.html#single-quantitative-variable",
    "title": "Lab-02: Pronouns and Data",
    "section": "\n6.2 Single Quantitative Variable",
    "text": "6.2 Single Quantitative Variable\n\nQuestions: How many? How few? How often? How much?\nCalculations: max / min / mean / mode / (units)\n\n\n\nmax(), min(), range(), mean(), mode(), summary()\n\n\n\nCharts: Bar Chart / Histogram / Density\n\n\ngeom_histogram() / geom_density()\n\n\n\n\n\nmax(penguins$bill_length_mm)\n\n[1] 59.6\n\nrange(penguins$bill_length_mm, na.rm =TRUE) \n\n[1] 32.1 59.6\n\nsummary(penguins$flipper_length_mm)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    172     190     197     201     213     231 \n\n\n\nggplot(penguins) + geom_density(aes(bill_length_mm))\n\n\n\nggplot(penguins) + geom_histogram(aes(x = bill_length_mm))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n6.2.1 YOUR TURN-4\nAre all the above Quantitative variables ratio variables? Justify."
  },
  {
    "objectID": "content/labs/r-labs/pronouns/pronouns.html#two-variables-quantitative-vs-quantitative",
    "href": "content/labs/r-labs/pronouns/pronouns.html#two-variables-quantitative-vs-quantitative",
    "title": "Lab-02: Pronouns and Data",
    "section": "\n6.3 Two Variables: Quantitative vs Quantitative",
    "text": "6.3 Two Variables: Quantitative vs Quantitative\nWe can easily extend our intuition about one quantitative variable, to a pair of them. What Questions can we ask?\n\nQuestions: How many of this vs How many of that? Does this depend upon that? How are they related? (Remember \\(y = mx + c\\) and friends?)\nCalculations: Correlation / Covariance / T-test / Chi-Square Test for Two Means etc. We won‚Äôt go into this here !\nCharts: Scatter Plot / Line Plot / Regression i.e.¬†best fit lines\n\n\ncor(penguins$bill_length_mm, penguins$bill_depth_mm)\n\n[1] -0.2286256\n\n\n\nggplot(penguins) +\n  geom_point(aes(x = flipper_length_mm,\n                 y = body_mass_g))\n\n\n\nggplot(penguins) +\n  geom_point(aes(x = flipper_length_mm, \n                 y = bill_length_mm))\n\n\n\n\n\n6.3.1 YOUR TURN-5"
  },
  {
    "objectID": "content/labs/r-labs/pronouns/pronouns.html#two-variables-categorical-vs-categorical",
    "href": "content/labs/r-labs/pronouns/pronouns.html#two-variables-categorical-vs-categorical",
    "title": "Lab-02: Pronouns and Data",
    "section": "\n6.4 Two Variables: Categorical vs Categorical",
    "text": "6.4 Two Variables: Categorical vs Categorical\nWhat sort of question could we ask that involves two categorical variables?\n\nQuestions: How Many of this Kind( ~x) are How Many of that Kind( ~y ) ?\n\nCalculations: Counts and Tallies sliced by Category\n\n\ncounts , tally\n\n\n\n\nCharts: Stacked Bar Charts / Grouped Bar Charts / Segmented Bar Chart / Mosaic Chart\n\ngeom_bar()\nUse the second Categorical variables to modify fill, color.\nAlso try to vary the parameter position of the bars.\n\n\n\n\n\n\n\nggplot(penguins) + geom_bar(aes(x = island, \n                                fill = species),\n                            position = \"stack\")\n\n\n\n\nStoryline: ‡§§‡•Ä‡§® ‡§™‡•á‡§®‡§ó‡•Ä‡§®‡•§ ‡§î‡§∞ ‡§§‡•Å‡§Æ ‡§≠‡•Ä ‡§§‡•Ä‡§®(Oh never mind!)\n\n6.4.1 YOUR TURN-6"
  },
  {
    "objectID": "content/labs/r-labs/pronouns/pronouns.html#two-variables-quantitative-vs-qualitative",
    "href": "content/labs/r-labs/pronouns/pronouns.html#two-variables-quantitative-vs-qualitative",
    "title": "Lab-02: Pronouns and Data",
    "section": "\n6.5 Two Variables: Quantitative vs Qualitative",
    "text": "6.5 Two Variables: Quantitative vs Qualitative\nFinally, what if we want to look at Quant variables and Qual variables together? What questions could we ask?\n\nQuestions: How much of this is Which Kind of that? How many vs Which? How many vs How?\nCalculations: Counts, Means, Ranges etc., grouped by Categorical variable.\n\n\nggplot(penguins) + \n    geom_density(aes(x = body_mass_g, \n                 color = island, \n                 fill = island), \n                 alpha = 0.3)\n\n\n\n\n\nCharts: Bar Chart using group / density plots by group / violin plots by group / box plots by group\n\n\n\ngeom_bar / geom_density / geom_violin / geom_boxplot using Categorical Variable for grouping\n\n\nggplot(penguins) + \n    geom_density(aes(x = body_mass_g, \n                 color = island, \n                 fill = island), \n                 alpha = 0.3)\n\n\n\nggplot(penguins) + \n  geom_histogram(aes(x = flipper_length_mm,\n                 fill = sex))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n6.5.1 YOUR TURN-7\n\n6.5.2 Time to Play\n\nCreate a fresh RMarkdown and similarly analyse two datasets of the following data sets\n\n\nAny dataset in your R installation. Type data() in your console to see what is available.\ndiamonds . This dataset is part of the tidyverse package so just type diamonds in your code and there it is.\ngapminder !! Yes!!You will need to install the gapminder package to access this dataset\nmosaicData package datasets. Install mosaicData\ndata.world: Find Datasets of your choice: https://docs.data.world/en/64499-64516-Quickstarts-and-tutorials.html\nkaggle: https://www.kaggle.com/datasets"
  },
  {
    "objectID": "content/labs/r-labs/r-labs-listing.html",
    "href": "content/labs/r-labs/r-labs-listing.html",
    "title": "The Foundation Series",
    "section": "",
    "text": "Critical Result Callback Monitor\n\n\n\n\n\n\n\n\n\n\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to the dplyr package\n\n\n\n\n\n\n\n\n\n\n\n\nJul 6, 2021\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\nLab 01 - Introduce Yourself\n\n\nInstalling and Getting Started with R!\n\n\nPart of my R for Artists and Designers course using the idea of metaphors in written language.\n\n\n\n\n\n\nJul 6, 2021\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\nLab 01 - Introduce Yourself\n\n\nInstalling and Getting Started with R!\n\n\nPart of my R Workshop course using the idea of metaphors in written language.\n\n\n\n\n\n\nJul 6, 2021\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\nLab 01 - Introduce Yourself\n\n\nInstalling and Getting Started with R!\n\n\nPart of my R for Artists and Designers course using the idea of metaphors in written language.\n\n\n\n\n\n\nJul 6, 2021\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\nLab 01 - Introduce Yourself\n\n\nInstalling and Getting Started with R!\n\n\n\n\n\n\n\n\n\nJul 6, 2021\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\nLab 05: Colors with Penguins\n\n\nPalettes from Famous Paintings, GoT, Harry Potter, and Wes Anderson\n\n\n\n\n\n\n\n\n\nJul 10, 2021\n\n\nArvind Venkatadri\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLab 05: What use is a Book without any Pictures?\n\n\n\n\n\n\n\n\n\n\n\n\nJul 6, 2021\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\nLab 06 - The Grammar of Graphics\n\n\nCreating Graphs and Charts using ggplot\n\n\nPart of my R for Artists and Designers course using the idea of metaphors in written language.\n\n\n\n\n\n\nJul 6, 2021\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\nLab 06a: Fonts, Themes, and other Wizardy in ggplot\n\n\nFonts, Themes, and other Wizardy in ggplot\n\n\n\n\n\n\n\n\n\nJul 25, 2021\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\nLab-02: Pronouns and Data\n\n\n\n\n\nPart of my R Workshop using the idea of metaphors in written language.\n\n\n\n\n\n\nJul 6, 2021\n\n\nArvind Venkatadri\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlaying with Leaflet\n\n\n\n\n\n\n\n\n\n\n\n\nMay 13, 2017\n\n\nArvind Venkatadri\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlaying with Leaflet\n\n\n\n\n\n\n\n\n\n\n\n\nMay 13, 2017\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\nThe Grammar of Maps\n\n\nWhere is the Secret Garden?\n\n\nPart of my Workshop course on R\n\n\n\n\n\n\nApr 22, 2021\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\nThe Grammar of Networks\n\n\nVisualizing and Manipulating Network data in R\n\n\nVisualizing and Manipulating Network data in R\n\n\n\n\n\n\nJun 21, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplotly: various examples\n\n\n\n\n\n\n\n\n\n\n\n\nCarson Sievert\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/labs/r-labs/tidy/dplyr.html",
    "href": "content/labs/r-labs/tidy/dplyr.html",
    "title": "Introduction to the dplyr package",
    "section": "",
    "text": "One of the dominant paradigms of working with data in R is to render it into ‚Äútidy‚Äù form. A huge benefit of the tidy way of working is that it influences your thinking with data and helps plan out your operations, in going from purpose to actual code in a swift and intuitive manner. This tidy form allows for a huge variety of data manipulation, summarizing, and plotting tasks, that can be performed using the packages of the tidyverse, and other packages that leverage the power of the tidyverse."
  },
  {
    "objectID": "content/labs/r-labs/tidy/dplyr.html#setting-up-the-packages",
    "href": "content/labs/r-labs/tidy/dplyr.html#setting-up-the-packages",
    "title": "Introduction to the dplyr package",
    "section": "\n2 Setting up the Packages",
    "text": "2 Setting up the Packages\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.0     ‚úî readr     2.1.4\n‚úî forcats   1.0.0     ‚úî stringr   1.5.0\n‚úî ggplot2   3.4.1     ‚úî tibble    3.2.0\n‚úî lubridate 1.9.2     ‚úî tidyr     1.3.0\n‚úî purrr     1.0.1     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors"
  },
  {
    "objectID": "content/labs/r-labs/tidy/dplyr.html#tidy-data",
    "href": "content/labs/r-labs/tidy/dplyr.html#tidy-data",
    "title": "Introduction to the dplyr package",
    "section": "\n3 Tidy Data",
    "text": "3 Tidy Data\n\ndata(starwars)\ndim(starwars)\n\n[1] 87 14\n\nstarwars\n\n\n\n  \n\n\n\n‚ÄúTidy Data‚Äù is an important way of thinking about what data typically look like in R. Let‚Äôs fetch a figure from the web to show the (preferred) structure of data in R.\n\n\nTidy Data\n\n\nThe three features described in the figure above define the nature of tidy data:\n\nVariables in Columns\n\nObservations in Rows and\n\nMeasurements in Cells.\n\nData are imagined to be resulting from an experiment. Each variable represents a parameter/aspect in the experiment. Each row represents an additional datum of measurement. A cell is a single measurement on a single parameter(column) in a single observation(row).\nWhen working with data you must:\n\nFigure out what you want to do. (Purpose)\nDescribe those tasks in the form of a computer program. (Plain English to R Code)\nExecute the program.\n\nThe dplyr package makes these steps fast and easy:\n\nBy constraining your options, it helps you think about your data manipulation challenges.\nIt provides simple ‚Äúverbs‚Äù, functions that correspond to the most common data manipulation tasks, to help you translate your thoughts into code.\nIt uses efficient backends, so you spend less time waiting for the computer.\n\n\nNe‚Äôer you mind about backends ;-) See Shakespeare‚Äôs Hamlet.\n\nThis document introduces you to dplyr‚Äôs basic set of tools, and shows you how to apply them to data frames. dplyr also supports databases via the dbplyr package, once you‚Äôve installed, read vignette(\"dbplyr\") to learn more."
  },
  {
    "objectID": "content/labs/r-labs/tidy/dplyr.html#data-starwars",
    "href": "content/labs/r-labs/tidy/dplyr.html#data-starwars",
    "title": "Introduction to the dplyr package",
    "section": "\n4 Data: starwars",
    "text": "4 Data: starwars\nTo explore the basic data manipulation verbs of dplyr, we‚Äôll use the dataset starwars. This dataset contains 87 characters and comes from the Star Wars API, and is documented in ?starwars\n\nThis means: type ?starwars in the Console. Try.\n\nNote that starwars is a tibble, a modern re-imagining of the data frame. It‚Äôs particularly useful for large datasets because it only prints the first few rows. You can learn more about tibbles at https://tibble.tidyverse.org; in particular you can convert data frames to tibbles with as_tibble().\n\nCheck your Environment Tab to inspect starwars in a separate tab."
  },
  {
    "objectID": "content/labs/r-labs/tidy/dplyr.html#single-table-verbs",
    "href": "content/labs/r-labs/tidy/dplyr.html#single-table-verbs",
    "title": "Introduction to the dplyr package",
    "section": "\n5 Single table verbs",
    "text": "5 Single table verbs\ndplyr aims to provide a function for each basic verb of data manipulation. These verbs can be organised into three categories based on the component of the dataset that they work with:\n\nRows:\n\n\nfilter() chooses rows based on column values.\n\nslice() chooses rows based on location.\n\narrange() changes the order of the rows.\n\n\nColumns:\n\n\nselect() changes whether or not a column is included.\n\nrename() changes the name of columns.\n\nmutate() changes the values of columns and creates new columns.\n\nrelocate() changes the order of the columns.\n\n\nGroups of rows:\n\n\nsummarise() collapses a group into a single row.\n\n\n\n\nThink of the parallels from Microsoft Excel.\n\n\n5.1 The pipe\nAll of the dplyr functions take a data frame (or tibble) as the first argument. Rather than forcing the user to either save intermediate objects or nest functions, dplyr provides the %>% operator from magrittr. x %>% f(y) turns into f(x, y) so the result from one step is then ‚Äúpiped‚Äù into the next step. You can use the pipe to rewrite multiple operations that you can read left-to-right, top-to-bottom (reading the pipe operator as ‚Äúthen‚Äù).\n\n5.2 Filter rows with filter()\n\nfilter() allows you to select a subset of rows in a data frame. Like all single verbs, the first argument is the tibble (or data frame). The second and subsequent arguments refer to variables within that data frame, selecting rows where the expression is TRUE.\nFor example, we can select all character with light skin color and brown eyes with:\n\nNote the double equal to sign (==) below! Equivalent to MS Excel Data -> Filter\n\n\nstarwars %>% filter(skin_color == \"light\", eye_color == \"brown\")\n\n\n\n  \n\n\n\n\n5.3 Arrange rows with arrange()\n\narrange() works similarly to filter() except that instead of filtering or selecting rows, it reorders them. It takes a data frame, and a set of column names (or more complicated expressions) to order by. If you provide more than one column name, each additional column will be used to break ties in the values of preceding columns:\n\nstarwars %>% arrange(height, mass)\n\n\n\n  \n\n\n\nUse desc() to order a column in descending order:\n\nstarwars %>% arrange(desc(height))\n\n\n\n  \n\n\n\n\n5.4 Choose rows using their position with slice()\n\nslice() lets you index rows by their (integer) locations. It allows you to select, remove, and duplicate rows.\n\nThis is an important step in Prediction, Modelling and Machine Learning.\n\nWe can get characters from row numbers 5 through 10.\n\nstarwars %>% slice(5:10)\n\n\n\n  \n\n\n\nIt is accompanied by a number of helpers for common use cases:\n\n\nslice_head() and slice_tail() select the first or last rows.\n\n\nstarwars %>% slice_head(n = 3)\n\n\n\n  \n\n\n\n\n\nslice_sample() randomly selects rows. Use the option prop to choose a certain proportion of the cases.\n\n\nstarwars %>% slice_sample(n = 5)\n\n\n\n  \n\n\nstarwars %>% slice_sample(prop = 0.1)\n\n\n\n  \n\n\n\nUse replace = TRUE to perform a bootstrap sample. If needed, you can weight the sample with the weight argument.\n\nBootstrap samples are a special statistical sampling method. Counterintuitive perhaps, since you sample with replacement. Should remind you of your high school Permutation and Combination class, with all those urn models and so on. If you remember.\n\n\n\nslice_min() and slice_max() select rows with highest or lowest values of a variable. Note that we first must choose only the values which are not NA.\n\n\nstarwars %>%\n  filter(!is.na(height)) %>%\n  slice_min(height, n = 3)\n\n\n\n  \n\n\n\n\n5.5 Select columns with select()\n\nOften you work with large datasets with many columns but only a few are actually of interest to you. select() allows you to rapidly zoom in on a useful subset using operations that usually only work on numeric variable positions:\n\n# Select columns by name\nstarwars %>% select(hair_color, skin_color, eye_color)\n\n\n\n  \n\n\n# Select all columns between hair_color and eye_color (inclusive)\nstarwars %>% select(hair_color:eye_color)\n\n\n\n  \n\n\n# Select all columns except those from hair_color to eye_color (inclusive)\nstarwars %>% select(!(hair_color:eye_color))\n\n\n\n  \n\n\n# Select all columns ending with color\nstarwars %>% select(ends_with(\"color\"))\n\n\n\n  \n\n\n\nThere are a number of helper functions you can use within select(), like starts_with(), ends_with(), matches() and contains(). These let you quickly match larger blocks of variables that meet some criterion. See ?select for more details.\nYou can rename variables with select() by using named arguments:\n\nstarwars %>% select(home_world = homeworld)\n\n\n\n  \n\n\n\nBut because select() drops all the variables not explicitly mentioned, it‚Äôs not that useful. Instead, use rename():\n\nstarwars %>% rename(home_world = homeworld)\n\n\n\n  \n\n\n\n\n5.6 Add new columns with mutate()\n\nBesides selecting sets of existing columns, it‚Äôs often useful to add new columns that are functions of existing columns. This is the job of mutate():\n\nstarwars %>% mutate(height_m = height / 100)\n\n\n\n  \n\n\n\nWe can‚Äôt see the height in meters we just calculated, but we can fix that using a select command.\n\nstarwars %>%\n  mutate(height_m = height / 100) %>%\n  select(height_m, height, everything())\n\n\n\n  \n\n\n\ndplyr::mutate() is similar to the base transform(), but allows you to refer to columns that you‚Äôve just created:\n\nstarwars %>%\n  mutate(\n    height_m = height / 100,\n    BMI = mass / (height_m^2)\n  ) %>%\n  select(BMI, everything())\n\n\n\n  \n\n\n\nIf you only want to keep the new variables, use transmute():\n\nstarwars %>%\n  transmute(\n    height_m = height / 100,\n    BMI = mass / (height_m^2)\n  )\n\n\n\n  \n\n\n\n\n5.7 Change column order with relocate()\n\nUse a similar syntax as select() to move blocks of columns at once\n\nstarwars %>% relocate(sex:homeworld, .before = height)\n\n\n\n  \n\n\n\n\n5.8 Summarise values with summarise()\n\nThe last verb is summarise(). It collapses a data frame to a single row.\n\nstarwars %>% summarise(mean_height = mean(height, na.rm = TRUE))\n\n\n\n  \n\n\n\nIt‚Äôs not that useful until we learn the group_by() verb below.\n\n5.9 Commonalities\nYou may have noticed that the syntax and function of all these verbs are very similar:\n\nThe first argument is a data frame.\nThe subsequent arguments describe what to do with the data frame. You can refer to columns in the data frame directly without using $.\nThe result is a new data frame\n\nTogether these properties make it easy to chain together multiple simple steps to achieve a complex result.\nThese five functions provide the basis of a language of data manipulation. At the most basic level, you can only alter a tidy data frame in five useful ways: you can reorder the rows (arrange()), pick observations and variables of interest (filter() and select()), add new variables that are functions of existing variables (mutate()), or collapse many values to a summary (summarise())."
  },
  {
    "objectID": "content/labs/r-labs/tidy/dplyr.html#combining-functions-with",
    "href": "content/labs/r-labs/tidy/dplyr.html#combining-functions-with",
    "title": "Introduction to the dplyr package",
    "section": "\n6 Combining functions with %>%\n",
    "text": "6 Combining functions with %>%\n\nThe dplyr API is functional in the sense that function calls don‚Äôt have side-effects. You must always save their results. This doesn‚Äôt lead to particularly elegant code, especially if you want to do many operations at once. You either have to do it step-by-step:\n\na1 <- group_by(starwars, species, sex)\na2 <- select(a1, height, mass)\na3 <- summarise(a2,\n  height = mean(height, na.rm = TRUE),\n  mass = mean(mass, na.rm = TRUE)\n)\n\nOr if you don‚Äôt want to name the intermediate results, you need to wrap the function calls inside each other:\n\nsummarise(\n  select(\n    group_by(starwars, species, sex),\n    height, mass\n  ),\n  height = mean(height, na.rm = TRUE),\n  mass = mean(mass, na.rm = TRUE)\n)\n\nAdding missing grouping variables: `species`, `sex`\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\n\n\n  \n\n\n\nThis is difficult to read because the order of the operations is from inside to out. Thus, the arguments are a long way away from the function. To get around this problem, dplyr provides the %>% operator from magrittr. x %>% f(y) turns into f(x, y) so you can use it to rewrite multiple operations that you can read left-to-right, top-to-bottom (reading the pipe operator as ‚Äúthen‚Äù):\n\nstarwars %>%\n  group_by(species, sex) %>%\n  summarise(\n    mean_height = mean(height, na.rm = TRUE),\n    mean_mass = mean(mass, na.rm = TRUE)\n  )\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument."
  },
  {
    "objectID": "content/labs/r-labs/tidy/dplyr.html#patterns-of-operations",
    "href": "content/labs/r-labs/tidy/dplyr.html#patterns-of-operations",
    "title": "Introduction to the dplyr package",
    "section": "\n7 Patterns of operations",
    "text": "7 Patterns of operations\nThe dplyr verbs can be classified by the type of operations they accomplish (we sometimes speak of their semantics, i.e., their meaning). It‚Äôs helpful to have a good grasp of the difference between select and mutate operations.\n\n7.1 Selecting operations\nOne of the appealing features of dplyr is that you can refer to columns from the tibble as if they were regular variables. However, the syntactic uniformity of referring to bare column names hides semantical differences across the verbs. A column symbol supplied to select() does not have the same meaning as the same symbol supplied to mutate().\nSelecting operations expect column names and positions. Hence, when you call select() with bare variable names, they actually represent their own positions in the tibble. The following calls are completely equivalent from dplyr‚Äôs point of view:\n\n# `name` represents the integer 1\nselect(starwars, name)\n\n\n\n  \n\n\nselect(starwars, 1)\n\n\n\n  \n\n\n\nBy the same token, this means that you cannot refer to variables from the surrounding context if they have the same name as one of the columns. In the following example, height still represents 2, not 5:\n\nheight <- 5\nselect(starwars, height)\n\n\n\n  \n\n\n\nOne useful subtlety is that this only applies to bare names and to selecting calls like c(height, mass) or height:mass. In all other cases, the columns of the data frame are not put in scope. This allows you to refer to contextual variables in selection helpers:\n\nname <- \"color\"\nselect(starwars, ends_with(name))\n\n\n\n  \n\n\n\nThese semantics are usually intuitive. But note the subtle difference:\n\nname <- 5\nselect(starwars, name, identity(name))\n\n\n\n  \n\n\n\nIn the first argument, name represents its own position 1. In the second argument, name is evaluated in the surrounding context and represents the fifth column.\n\n7.2 Mutating operations\nMutate semantics are quite different from selection semantics. Whereas select() expects column names or positions, mutate() expects column vectors. We will set up a smaller tibble to use for our examples.\n\ndf <- starwars %>% select(name, height, mass)\n\nWhen we use select(), the bare column names stand for their own positions in the tibble. For mutate() on the other hand, column symbols represent the actual column vectors stored in the tibble. Consider what happens if we give a string or a number to mutate():\n\nmutate(df, \"height\", 2)\n\n\n\n  \n\n\n\nmutate() gets length-1 vectors that it interprets as new columns in the data frame. These vectors are recycled so they match the number of rows. That‚Äôs why it doesn‚Äôt make sense to supply expressions like \"height\" + 10 to mutate(). This amounts to adding 10 to a string! The correct expression is:\n\nmutate(df, height + 10)\n\n\n\n  \n\n\n\nIn the same way, you can unquote values from the context if these values represent a valid column. They must be either length 1 (they then get recycled) or have the same length as the number of rows. In the following example we create a new vector that we add to the data frame:\n\nvar <- seq(1, nrow(df))\nmutate(df, new = var)\n\n\n\n  \n\n\n\nA case in point is group_by(). While you might think it has select semantics, it actually has mutate semantics. This is quite handy as it allows to group by a modified column:\n\ngroup_by(starwars, sex)\n\n\n\n  \n\n\ngroup_by(starwars, sex = as.factor(sex))\n\n\n\n  \n\n\ngroup_by(starwars, height_binned = cut(height, 3))\n\n\n\n  \n\n\n\nThis is why you can‚Äôt supply a column name to group_by(). This amounts to creating a new column containing the string recycled to the number of rows:\n\ngroup_by(df, \"month\")"
  },
  {
    "objectID": "content/labs/r-labs/tidy/dplyr.html#two-table-verbs",
    "href": "content/labs/r-labs/tidy/dplyr.html#two-table-verbs",
    "title": "Introduction to the dplyr package",
    "section": "\n8 Two table verbs",
    "text": "8 Two table verbs\nSometimes our data is spread across more than one table. Often these tables are linked by some common, or common-looking, variable columns. dplyr allows us to work with such data that is spread over more than one table. More information is available here: Two Table Verbs in dplyr\nThe operations/verbs used to manipulate two-table verbs are:\n\nMutating joins, which add new variables to one table from matching rows in another.\n\ninner_join()\n\n\n\n\n\n\n\n\n\nleft_join()\n\n\n\n\n\n\n\nright_join()\n\n\n\n\n\n\n\nfull_join()\n\n\n\n\n\n\n\nFiltering joins, which filter observations from one table based on whether or not they match an observation in the other table.\n\n\nsemi_join(x, y) keeps all observations in x that have a match in y.\n\n\n\n\n\n\n\n\n\n\nanti_join(x, y) drops all observations in x that have a match in\n\n\n\n\n\n\n\n\n\n\n\n\nSet operations, which combine the observations in the data sets as if they were set elements.\n\nunion()\n\n\n\n\n\n\n\n\n\nunion_all(),\n\n\n\n\n\n\n\nintersect(),\n\n\n\n\n\n\n\nsetdiff()\n\n\n\n\n\n\n\nTidyr Operations:\npivot_longer()\npivot_wider()"
  },
  {
    "objectID": "content/labs/r-labs/tidy/moma.html",
    "href": "content/labs/r-labs/tidy/moma.html",
    "title": "Lab 05: What use is a Book without any Pictures?",
    "section": "",
    "text": "This Quarto document is part of my workshop on R. The material is based on A Layered Grammar of Graphics by Hadley Wickham. The course is meant for First Year students pursuing a Degree in Art and Design.\nThe intent is to build Skill in coding in R, and also appreciate R as a way to metaphorically visualize information of various kinds, using predominantly geometric figures and structures.\nAll Quarto document files combine code, text, web-images, and figures developed using code. Everything is text; code chunks are enclosed in fences (```)"
  },
  {
    "objectID": "content/labs/r-labs/tidy/moma.html#setting-up-the-packages",
    "href": "content/labs/r-labs/tidy/moma.html#setting-up-the-packages",
    "title": "Lab 05: What use is a Book without any Pictures?",
    "section": "\n2 Setting up the Packages",
    "text": "2 Setting up the Packages\n\nlibrary(tidyverse)\nlibrary(mosaic)"
  },
  {
    "objectID": "content/labs/r-labs/tidy/moma.html#goals-for-this-lab",
    "href": "content/labs/r-labs/tidy/moma.html#goals-for-this-lab",
    "title": "Lab 05: What use is a Book without any Pictures?",
    "section": "\n3 Goals for this Lab",
    "text": "3 Goals for this Lab\n\nUnderstand the idea of ‚Äútidy‚Äù data\nUsing ‚Äútidy data‚Äù and the ‚Äútidyverse‚Äù way of programming in R allows to translate our thoughts readily into code.\nUnderstand dplyr VERB functions to get to know and manipulate a dataset"
  },
  {
    "objectID": "content/labs/r-labs/tidy/moma.html#pedagogical-note",
    "href": "content/labs/r-labs/tidy/moma.html#pedagogical-note",
    "title": "Lab 05: What use is a Book without any Pictures?",
    "section": "\n4 Pedagogical Note",
    "text": "4 Pedagogical Note\nThe method followed will be based on PRIMM:\n\n\nPREDICT Inspect the code and guess at what the code might do, write predictions\n\n\nRUN the code provided and check what happens\n\nINFER what the parameters of the code do and write comments to explain. What bells and whistles can you see?\n\nMODIFY the parameters code provided to understand the options available. Write comments to show what you have aimed for and achieved.\n\nMAKE : take an idea/concept of your own, and graph it. # Inspiration + data\n\nWe‚Äôll use data from the Museum of Modern Art (MoMA)\n\nPublicly available on GitHub\n\nAs analyzed by fivethirtyeight.com\n\nAnd by others"
  },
  {
    "objectID": "content/labs/r-labs/tidy/moma.html#read-in-the-data",
    "href": "content/labs/r-labs/tidy/moma.html#read-in-the-data",
    "title": "Lab 05: What use is a Book without any Pictures?",
    "section": "\n5 Read in the data",
    "text": "5 Read in the data\nThis dataset has been created by Alison Hill(@apreshill on Twitter). Download the dataset, save it into your data folder and then import it into R:\n  artworks-cleaned.csv\n\n\n\ncategorical variables:  \n               name     class levels    n missing\n1             title character   2015 2253       0\n2            artist character    989 2253       0\n3        artist_bio character    858 2252       1\n4     artist_gender character      2 2243      10\n5  circumference_cm   logical      0    0    2253\n6       diameter_cm   logical      0    0    2253\n7         length_cm   logical      0    0    2253\n8    seat_height_cm   logical      0    0    2253\n9          purchase   logical      2 2253       0\n10             gift   logical      2 2253       0\n11         exchange   logical      2 2253       0\n12   classification character      1 2253       0\n13       department character      5 2253       0\n                                    distribution\n1  Untitled (4.5%), I Got Up... (1.2%) ...      \n2  Pablo Picasso (2.4%) ...                     \n3    (Spanish, 1881‚Äì1973) (2.4%) ...            \n4  Male (88.8%), Female (11.2%)                 \n5   (%)                                         \n6   (%)                                         \n7   (%)                                         \n8   (%)                                         \n9  FALSE (91.2%), TRUE (8.8%)                   \n10 TRUE (51.8%), FALSE (48.2%)                  \n11 FALSE (93.6%), TRUE (6.4%)                   \n12 Painting (100%)                              \n13 Painting & Sculpture (97.4%) ...             \n\nquantitative variables:  \n                name   class    min         Q1 median          Q3      max\n1  artist_birth_year numeric 1839.0 1890.00000 1913.0 1933.000000 1987.000\n2  artist_death_year numeric 1890.0 1956.00000 1976.0 1996.000000 2018.000\n3        num_artists numeric    1.0    1.00000    1.0    1.000000   10.000\n4   n_female_artists numeric    0.0    0.00000    0.0    0.000000    2.000\n5     n_male_artists numeric    0.0    1.00000    1.0    1.000000    9.000\n6      year_acquired numeric 1930.0 1957.00000 1975.0 1996.000000 2017.000\n7       year_created numeric 1872.0 1933.00000 1956.0 1972.000000 2017.000\n8           depth_cm numeric    0.0    0.00000    0.0    6.762503  647.700\n9          height_cm numeric    7.0   61.27760  106.6  182.200000 1011.000\n10          width_cm numeric    4.1   60.96012   99.0  170.000000 4663.449\n           mean          sd    n missing\n1  1911.8624833  27.9805659 2247       6\n2  1974.8152709  25.5436102 1624     629\n3     1.0093250   0.2116175 2252       1\n4     0.1136263   0.3188231 2253       0\n5     0.8952508   0.3717841 2253       0\n6  1975.5494652  23.3948403 2244       9\n7  1953.5284698  27.9329429 2248       5\n8    10.3302084  44.0511526  298    1955\n9   123.8797591  79.6186437 2253       0\n10  131.9462530 152.6158431 2253       0"
  },
  {
    "objectID": "content/labs/r-labs/tidy/moma.html#getting-to-know-your-data-make-up-some-questions",
    "href": "content/labs/r-labs/tidy/moma.html#getting-to-know-your-data-make-up-some-questions",
    "title": "Lab 05: What use is a Book without any Pictures?",
    "section": "\n6 Getting to Know your data: Make up some Questions",
    "text": "6 Getting to Know your data: Make up some Questions\n\n\n\n\n\n\nChallenge #1\n\n\n\nTry to answer all of these questions using dplyr. Answers are below but try them on your own first!\n\nHow many paintings (rows) are in moma?\nHow many variables (columns) are in moma?\nWhat is the first painting acquired by MoMA? Which year? Which artist? What title?\nWhat is the oldest painting in the collection? Which year? Which artist? What title?\nHow many distinct artists are there?\nWhich artist has the most paintings in the collection? How many paintings are by this artist?\nHow many paintings by male vs female artists?\nHow many paintings acquired by year, and by gender of artist, over time?\n\nIf you want more:\n\nHow many artists of each gender are there?\nIn what year were the most paintings acquired? Created?\nIn what year was the first painting by a (solo) female artist acquired? When was that painting created? Which artist? What title?"
  },
  {
    "objectID": "content/labs/r-labs/tidy/moma.html#how-many-paintings-how-many-variables",
    "href": "content/labs/r-labs/tidy/moma.html#how-many-paintings-how-many-variables",
    "title": "Lab 05: What use is a Book without any Pictures?",
    "section": "\n7 How many paintings? How many Variables?",
    "text": "7 How many paintings? How many Variables?\n\nHow many rows/observations are in moma?\n\nHow many variables are in moma?\n\n\n\n\n\n\nTip\n\n\n\nHint: These questions can be answered using the dplyr function glimpse.\n\n\n\n\n\nlibrary(dplyr)\nmoma\n\n\n\n  \n\n\nglimpse(moma)\n\nRows: 2,253\nColumns: 23\n$ title             <chr> \"Rope and People, I\", \"Fire in the Evening\", \"Portra‚Ä¶\n$ artist            <chr> \"Joan Mir√≥\", \"Paul Klee\", \"Paul Klee\", \"Pablo Picass‚Ä¶\n$ artist_bio        <chr> \"(Spanish, 1893‚Äì1983)\", \"(German, born Switzerland. ‚Ä¶\n$ artist_birth_year <dbl> 1893, 1879, 1879, 1881, 1880, 1879, 1943, 1880, 1839‚Ä¶\n$ artist_death_year <dbl> 1983, 1940, 1940, 1973, 1946, 1953, 1977, 1950, 1906‚Ä¶\n$ num_artists       <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1‚Ä¶\n$ n_female_artists  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ n_male_artists    <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1‚Ä¶\n$ artist_gender     <chr> \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"Mal‚Ä¶\n$ year_acquired     <dbl> 1936, 1970, 1966, 1955, 1939, 1968, 1997, 1931, 1934‚Ä¶\n$ year_created      <dbl> 1935, 1929, 1927, 1919, 1925, 1919, 1970, 1929, 1885‚Ä¶\n$ circumference_cm  <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ‚Ä¶\n$ depth_cm          <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ‚Ä¶\n$ diameter_cm       <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ‚Ä¶\n$ height_cm         <dbl> 104.8, 33.8, 60.3, 215.9, 50.8, 129.2, 200.0, 54.6, ‚Ä¶\n$ length_cm         <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ‚Ä¶\n$ width_cm          <dbl> 74.6, 33.3, 36.8, 78.7, 54.0, 89.9, 200.0, 38.1, 96.‚Ä¶\n$ seat_height_cm    <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ‚Ä¶\n$ purchase          <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL‚Ä¶\n$ gift              <lgl> TRUE, FALSE, FALSE, TRUE, TRUE, FALSE, TRUE, TRUE, F‚Ä¶\n$ exchange          <lgl> FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALS‚Ä¶\n$ classification    <chr> \"Painting\", \"Painting\", \"Painting\", \"Painting\", \"Pai‚Ä¶\n$ department        <chr> \"Painting & Sculpture\", \"Painting & Sculpture\", \"Pai‚Ä¶"
  },
  {
    "objectID": "content/labs/r-labs/tidy/moma.html#what-is-the-first-painting-acquired",
    "href": "content/labs/r-labs/tidy/moma.html#what-is-the-first-painting-acquired",
    "title": "Lab 05: What use is a Book without any Pictures?",
    "section": "\n8 What is the first painting acquired?",
    "text": "8 What is the first painting acquired?\n\nWhat is the first painting acquired by MoMA (since they started tracking)?\nWhat year was it acquired?\nWhich artist?\nWhat title?\n\n\n\n\n\n\n\nTip\n\n\n\nHint: These questions can be answered by combining two dplyr functions: select and arrange.\n\n\n\nmoma %>% \n  select(artist, title, year_acquired) %>% \n  arrange(year_acquired)"
  },
  {
    "objectID": "content/labs/r-labs/tidy/moma.html#what-is-the-oldest-painting-in-the-moma-collection",
    "href": "content/labs/r-labs/tidy/moma.html#what-is-the-oldest-painting-in-the-moma-collection",
    "title": "Lab 05: What use is a Book without any Pictures?",
    "section": "\n9 What is the oldest painting in the MoMA collection?",
    "text": "9 What is the oldest painting in the MoMA collection?\n\nWhat is the oldest painting in the MoMA collection historically (since they started tracking)?\nWhat year was it created?\nWhich artist?\nWhat title?\n\n\n\n\n\n\n\nTip\n\n\n\nHint: These questions can be answered by combining two dplyr functions: select and arrange.\n\n\n\nmoma %>% \n  select(artist, title, year_created) %>% \n  arrange(year_created)\n\n\n\n  \n\n\n\n\noldest <- moma %>% \n  select(artist, title, year_created) %>% \n  arrange(year_created) %>% \n  slice(1)\noldest\n\n\n\n  \n\n\n\nTo do inline comments, I could say that the oldest painting is Landscape at Daybreak, painted by Odilon Redon in 1872."
  },
  {
    "objectID": "content/labs/r-labs/tidy/moma.html#how-many-artists",
    "href": "content/labs/r-labs/tidy/moma.html#how-many-artists",
    "title": "Lab 05: What use is a Book without any Pictures?",
    "section": "\n10 How many artists?",
    "text": "10 How many artists?\n\nHow many distinct artists are there?\n\n\n\n\n\n\n\nTip\n\n\n\nHint: Try dplyr::distinct.\n\n\n\nmoma %>% \n  distinct(artist)\n\n\n\n  \n\n\n\nYou could add a tally() too to get just the number of rows. You can also then use pull() to get that single number out of the tibble:\n\nnum_artists <- moma %>% \n  distinct(artist) %>% \n  tally() %>% \n  pull()\nnum_artists\n\n[1] 989\n\n\nThen I can refer to this number in inline comments like: there are 989 total."
  },
  {
    "objectID": "content/labs/r-labs/tidy/moma.html#which-artist-has-the-most-paintings",
    "href": "content/labs/r-labs/tidy/moma.html#which-artist-has-the-most-paintings",
    "title": "Lab 05: What use is a Book without any Pictures?",
    "section": "\n11 Which artist has the most paintings?",
    "text": "11 Which artist has the most paintings?\n\nWhich artist has the most paintings ever owned by moma?\nHow many paintings in the MoMA collection by that artist?\n\n\n\n\n\n\n\nTip\n\n\n\nHint: Try dplyr::count. Use ?count to figure out how to sort the output.\n\n\n\nmoma %>% \n  count(artist, sort = TRUE)\n\n\n\n  \n\n\n\nIn the ?count documentation, it says: ‚Äúcount and tally are designed so that you can call them repeatedly, each time rolling up a level of detail.‚Äù Try running count() again (leave parentheses empty) on your last code chunk. ( before the slice())\n\nmoma %>% \n  count(artist, sort = TRUE) %>% \n  count()"
  },
  {
    "objectID": "content/labs/r-labs/tidy/moma.html#how-many-paintings-by-male-vs-female-artists",
    "href": "content/labs/r-labs/tidy/moma.html#how-many-paintings-by-male-vs-female-artists",
    "title": "Lab 05: What use is a Book without any Pictures?",
    "section": "\n12 How many paintings by male vs female artists?",
    "text": "12 How many paintings by male vs female artists?\n\nmoma %>% \n  count(artist_gender)\n\n\n\n  \n\n\n\nNow together we‚Äôll count the number of artists by gender. You‚Äôll need to give count two variable names in the parentheses: artist_gender and artist.\n\nmoma %>% \n  count(artist_gender, artist, sort = TRUE) \n\n\n\n  \n\n\n\nThis output is not super helpful as we already know that Pablo Picasso has 55 paintings in the MoMA collection. But how can we find out which female artist has the most paintings? We have a few options. Let‚Äôs first add a filter for females.\n\nmoma %>% \n  count(artist_gender, artist, sort = TRUE) %>% \n  filter(artist_gender == \"Female\")\n\n\n\n  \n\n\n\nAnother option is to use another dplyr function called top_n(). Use ?top_n to see how it works. How it won‚Äôt work in this context:\n\nmoma %>% \n  count(artist_gender, artist, sort = TRUE) %>% \n  slice_max(n = 2, order_by = n)\n\n\n\n  \n\n\n\nHow it will work better is following a group_by(artist_gender):\n\nmoma %>% \n  count(artist_gender, artist, sort = TRUE) %>% \n  group_by(artist_gender) %>% \n  slice_max(n = 1, order_by = n)\n\n\n\n  \n\n\n\nNow we can see that Sherrie Levine has 12 paintings. This is a pretty far cry from the 55 paintings by Pablo Picasso."
  },
  {
    "objectID": "content/labs/r-labs/tidy/moma.html#how-many-artists-of-each-gender-are-there",
    "href": "content/labs/r-labs/tidy/moma.html#how-many-artists-of-each-gender-are-there",
    "title": "Lab 05: What use is a Book without any Pictures?",
    "section": "\n13 How many artists of each gender are there?",
    "text": "13 How many artists of each gender are there?\nThis is a harder question to answer than you think! This is because the level of observation in our current moma dataset is unique paintings. We have multiple paintings done by the same artists though, so counting just the number of unique paintings is different than counting the number of unique artists.\nRemember how count can be used back-to-back to roll up a level of detail? Try running count(artist_gender) again on your last code chunk.\n\nmoma %>% \n  count(artist_gender, artist) %>% \n  count(artist_gender)\n\n\n\n  \n\n\n\nThis output takes the previous table (made with count(artist_gender, artist)), and essentially ignores the n column. So we no longer care about how many paintings each individual artist created. Instead, we want to count the rows in this new table where each row is a unique artist. By counting by artist_gender in the last line, we are grouping by levels of that variable (so Female/Male/NA) and nn is the number of unique artists for each gender category recorded."
  },
  {
    "objectID": "content/labs/r-labs/tidy/moma.html#when-were-the-most-paintings-in-the-collection-acquired",
    "href": "content/labs/r-labs/tidy/moma.html#when-were-the-most-paintings-in-the-collection-acquired",
    "title": "Lab 05: What use is a Book without any Pictures?",
    "section": "\n14 When were the most paintings in the collection acquired?",
    "text": "14 When were the most paintings in the collection acquired?\n\nmoma %>% \n  count(year_acquired, sort = TRUE)"
  },
  {
    "objectID": "content/labs/r-labs/tidy/moma.html#when-were-the-most-paintings-in-the-collection-created",
    "href": "content/labs/r-labs/tidy/moma.html#when-were-the-most-paintings-in-the-collection-created",
    "title": "Lab 05: What use is a Book without any Pictures?",
    "section": "\n15 When were the most paintings in the collection created?",
    "text": "15 When were the most paintings in the collection created?\nWhich variable should we count?\n\nmoma %>% \n  count(year_created, sort = TRUE)"
  },
  {
    "objectID": "content/labs/r-labs/tidy/moma.html#what-about-the-first-painting-by-a-solo-female-artist",
    "href": "content/labs/r-labs/tidy/moma.html#what-about-the-first-painting-by-a-solo-female-artist",
    "title": "Lab 05: What use is a Book without any Pictures?",
    "section": "\n16 What about the first painting by a solo female artist?",
    "text": "16 What about the first painting by a solo female artist?\n\n\n\n\n\n\nTip\n\n\n\nHint: Try combining three dplyr functions: filter, select, and arrange.\n\n\nWhen was the first painting by a solo female artist acquired?\n\nmoma %>% \n  filter(num_artists == 1 & n_female_artists == 1) %>% \n  select(title, artist, year_acquired, year_created) %>% \n  arrange(year_acquired)\n\n\n\n  \n\n\n\nWhat is the oldest painting by a solo female artist, and when was it created?\n\nmoma %>% \n  filter(num_artists == 1 & n_female_artists == 1) %>% \n  select(title, artist, year_acquired, year_created) %>% \n  arrange(year_created)\n\n\n\n  \n\n\n\n\n# or, because artist_gender is missing when num_artists > 1\nmoma %>% \n  filter(artist_gender == \"Female\") %>% \n  select(title, artist, year_acquired, year_created) %>% \n  arrange(year_acquired)"
  },
  {
    "objectID": "content/labs/r-labs/tidy/moma.html#how-many-paintings-acquired-by-year-and-by-gender-of-artist-over-time",
    "href": "content/labs/r-labs/tidy/moma.html#how-many-paintings-acquired-by-year-and-by-gender-of-artist-over-time",
    "title": "Lab 05: What use is a Book without any Pictures?",
    "section": "\n17 How many paintings acquired by year, and by gender of artist, over time?",
    "text": "17 How many paintings acquired by year, and by gender of artist, over time?\n\nmoma %>% group_by(year_created, artist_gender) %>% count()"
  },
  {
    "objectID": "content/labs/r-labs/time/index.html",
    "href": "content/labs/r-labs/time/index.html",
    "title": "Lab 01 - Introduce Yourself",
    "section": "",
    "text": "At the end of this Lab, we will:\n\nhave installed R and RStudio on our machines\nunderstood how to add additional R-packages for specific features and graphic capability\nrun code within RStudio and interpret the results\nhave learnt to look for help within R and RStudio"
  },
  {
    "objectID": "content/labs/r-labs/time/index.html#Check-In-R",
    "href": "content/labs/r-labs/time/index.html#Check-In-R",
    "title": "Lab 01 - Introduce Yourself",
    "section": "\n3.1 Check in",
    "text": "3.1 Check in\nLaunch R by clicking this logo. You should see one console with a command line interpreter. Try typing 2 + 2 and check !\nClose R."
  },
  {
    "objectID": "content/labs/r-labs/time/index.html#Check-In-RStudio",
    "href": "content/labs/r-labs/time/index.html#Check-In-RStudio",
    "title": "Lab 01 - Introduce Yourself",
    "section": "\n4.1 Check in",
    "text": "4.1 Check in\nLaunch RStudio. You should get a window similar to the screenshot you see here, but yours will be empty. Look at the bottom left pane: this is the same console window you saw when you opened R in step @Check-In-R\n\nPlace your cursor where you see > and type x <- 2 + 2 again hit enter or return, then type x, and hit enter/return again.\nIf [1] 4 prints to the screen, you have successfully installed R and RStudio, and you can move onto installing packages."
  },
  {
    "objectID": "content/labs/r-labs/time/index.html#save-and-share",
    "href": "content/labs/r-labs/time/index.html#save-and-share",
    "title": "Lab 01 - Introduce Yourself",
    "section": "\n7.1 Save and share",
    "text": "7.1 Save and share\nSave your work so you can share your favorite plot with us. You will not like the looks of your plot if you mouse over to Export and save it. Instead, use ggplot2‚Äôs command for saving a plot with sensible defaults:\n\nhelp(ggsave)\n\n\nggsave(\"file_name_here.pdf\", plot) # please make the filename unique!\n\nUpload this exported plot to Teams -> Assignments.\nBefore you do that, check that you can submit stuff/assignments on the MLS by uploading a cat picture üêà first, especially if you are an ailurophile like me. (Acceptable Breeds: Scottish Folds or Maine Coons)"
  },
  {
    "objectID": "content/posts/01-intro/history.html",
    "href": "content/posts/01-intro/history.html",
    "title": "Boston Terrier",
    "section": "",
    "text": "The Boston Terrier is a breed of dog originating in the United States of America. This ‚ÄúAmerican Gentleman‚Äù was accepted in 1893 by the American Kennel Club as a non-sporting breed.1 Color and markings are important when distinguishing this breed from the AKC standard. They should be either black, brindle or seal with white markings.2 Boston Terriers are small and compact with a short tail and erect ears. The AKC says they are highly intelligent and very easily trained.3 They are friendly and can be stubborn at times. The average life span of a Boston Terrier is around 11 to 13 years.4 The American Kennel Club ranked the Boston Terrier as the 21st most popular breed in 2019.5"
  },
  {
    "objectID": "content/posts/01-intro/history.html#history",
    "href": "content/posts/01-intro/history.html#history",
    "title": "Boston Terrier",
    "section": "History",
    "text": "History\n\n\n\n\n\nTerrier Seated\n\n\n\n\nThe Boston terrier breed originated around 1875, when Robert C. Hooper of Boston purchased from Edward Burnett a dog named Judge (known later as Hooper‚Äôs Judge), which was of a bull and terrier type lineage. Hooper‚Äôs Judge is directly related to the original bull and terrier breeds of the 19th and early 20th centuries. The American Kennel Club cites Hooper‚Äôs Judge as the ancestor of all true modern Boston Terriers. Judge weighed about 32 pounds (15 kg).\n\n\n\n\nJudge was bred to Burnett‚Äôs Gyp (or Kate). Gyp was a white bulldog-type female, owned by Edward Burnett, of Southboro, Massachusetts. She weighed about 20 pounds (9.1 kg), was stocky and strong and had the typical blocky head now shown in Bostons. From this foundation of the breed, subsequent breeders refined the breed into its modern day presentation. Bred down in size from fighting dogs of the bull and terrier types, the Boston Terrier originally weighed up to 44 pounds (20 kg) (Old Boston Bulldogs).\n\n\n\n\n\n\nA young male Boston Terrier with a Brown brindle coat\n\nThe breed was first shown in Boston in 1870. By 1889 the breed had become sufficiently popular in Boston that fanciers formed the American Bull Terrier Club, the breed‚Äôs nickname, ‚Äúroundheads‚Äù. Shortly after, at the suggestion of James Watson (a noted writer and authority), the club changed its name to the Boston Terrier Club and in 1893 it was admitted to membership in the American Kennel Club, thus making it the first US breed to be recognized. It is one of a small number of breeds to have originated in the United States. The Boston Terrier was the first non-sporting dog breed in the US.\nIn the early years, the color and markings were not very important to the breed‚Äôs standard. By the 20th century the breed‚Äôs distinctive markings and color were written into the standard, becoming an essential feature. The Boston Terrier has lost most of its aggressive nature, preferring the company of humans, although some males will still challenge other dogs if they feel their territory is being invaded. Boston University has used Rhett the Boston Terrier as their mascot since 1922. Wofford College in Spartanburg, SC has had a live Boston Terrier mascot named Blitz since 2003 that attends home football games. The Boston Terrier has also been the official state dog of Massachusetts since 1979."
  },
  {
    "objectID": "content/posts/01-intro/history.html#description",
    "href": "content/posts/01-intro/history.html#description",
    "title": "Boston Terrier",
    "section": "Description",
    "text": "Description\nThe Boston Terrier is a compactly built, well-proportioned dog. It has a square-looking head with erect ears and a slightly arched neck. The muzzle is short and generally wrinkle-free, with an even or a slightly undershot bite. The chest is broad and the tail is short. According to international breed standards, the dog should weigh no more than 25 pounds (11 kg). Boston Terriers usually stand up to 15‚Äì17 inches (380‚Äì430 mm) at the withers.\nThe American Kennel Club divides the breed into three classes: under 15 pounds, 15 pounds and under 20 pounds, 20 pounds and not exceeding 25 pounds.\nCoat and color\nThe Boston Terrier is characteristically marked with white in proportion to either black, brindle, seal (color of a wet seal, a very dark brown that looks black except in the bright sun), or a combination of the three. Any other color is not accepted as a Boston Terrier by the American Kennel Club, as they are usually obtained by crossbreeding with other breeds and the dog loses its characteristic ‚Äútuxedo‚Äù appearance.6 Any Boston Terrier from AKC parentage regardless of the color, or if it is a splash or has a blue eye or weak ears, can be and are registered by the AKC and participate in any AKC sporting events.\n\n\n\nA female Boston Terrier with a black coat\n\nAccording to the American Kennel Club, the Boston Terrier‚Äôs markings are broken down into two categories: Required, which consists of a white chest, white muzzle band, and a white band between the eyes; and Desired, which includes the Required markings plus a white collar, white on the forelegs, forelegs, up to the hocks on the rear legs. For conformation showing, symmetrical markings are preferred. Due to the Boston Terrier‚Äôs markings resembling formal wear, in addition to its refined and pleasant personality, the breed is commonly referred to as ‚Äúthe American Gentleman.‚Äù\n\n\n\nAn adult male Boston Terrier with a black coat"
  },
  {
    "objectID": "content/posts/01-intro/history.html#temperament",
    "href": "content/posts/01-intro/history.html#temperament",
    "title": "Boston Terrier",
    "section": "Temperament",
    "text": "Temperament\nBoston Terrier is a gentle breed that typically has a strong, happy-go-lucky, and friendly personality with a merry sense of humor. Boston Terriers are generally eager to please their owner and can be easily trained. They can be very protective of their owners, which may result in aggressive and territorial behavior toward other pets and strangers. The breed requires only a minimal amount of grooming.\nWhile originally bred for fighting as well as hunting rats in garment factories, they were later bred for companionship. They are not considered terriers by the American Kennel Club, however, but are part of the non-sporting group.\nBoth females and males are generally quiet and bark only when necessary, Their usually sensible attitude toward barking makes them excellent choices for apartment dwellers. They enjoy being around people, get along well with children, the elderly, other canines, and non-canine pets, if properly socialized."
  },
  {
    "objectID": "content/posts/01-intro/history.html#grooming",
    "href": "content/posts/01-intro/history.html#grooming",
    "title": "Boston Terrier",
    "section": "Grooming",
    "text": "Grooming\nWith a short, shiny, smooth coat, Boston Terriers require little grooming. Bostons produce light shedding, and weekly brushing of their fine coat is effective at removing loose hair. Brushing promotes the health of the coat because it distributes skin oils, and it also encourages new hair growth. Occasional bathing is suitable for the breed.7\nThe nails of Boston Terriers require regular trimming. Overgrown nails not only have the potential to inflict pain on the breed, but they can also make walking difficult or tear off after getting snagged on something.\nSimilarly to nail trimming, tooth brushing should also be done regularly to promote good oral health. The risk of the breed developing oral pain, gum infection, or bad breath can be decreased with regular tooth brushing that removes plaque buildup and other bacteria. In addition, poor dental hygiene can lead to tooth root abscesses that can lead to damage around the tissue and eventually lead to the loss of teeth."
  },
  {
    "objectID": "content/posts/02-authoring/authoring.html",
    "href": "content/posts/02-authoring/authoring.html",
    "title": "Housing Prices",
    "section": "",
    "text": "In this analysis, we build a model predicting sale prices of houses based on data on houses that were sold in the Duke Forest neighborhood of Durham, NC around November 2020. Let‚Äôs start by loading the packages we‚Äôll use for the analysis.\n\nPackageslibrary(openintro)  # for data\nlibrary(tidyverse)  # for data wrangling and visualization\nlibrary(knitr)      # for tables\nlibrary(broom)      # for model summary\n\n\nWe present the results of exploratory data analysis in Section¬†2 and the regression model in Section¬†3."
  },
  {
    "objectID": "content/posts/02-authoring/authoring.html#sec-eda",
    "href": "content/posts/02-authoring/authoring.html#sec-eda",
    "title": "Housing Prices",
    "section": "\n2 Exploratory data analysis",
    "text": "2 Exploratory data analysis\nThe data contains 98 houses. As part of the exploratory analysis let‚Äôs visualize and summarize the relationship between areas and prices of these houses.\n\n2.1 Data visualization\nFigure¬†1 shows two histograms displaying the distributions of price and area individually.\n\nShow the Codeggplot(duke_forest, aes(x = price)) +\n  geom_histogram(binwidth = 50000) +\n  labs(title = \"Histogram of prices\")\n\nggplot(duke_forest, aes(x = area)) +\n  geom_histogram(binwidth = 250) +\n  labs(title = \"Histogram of areas\")\n\nFigure¬†1: Histograms of individual variables\n\n\n\n\n(a) Histogram of prices\n\n\n\n\n\n\n(b) Histogram of areas\n\n\n\n\n\n\n\nFigure¬†2 displays the relationship between these two variables in a scatterplot.\n\nShow the Codeggplot(duke_forest, aes(x = area, y = price)) +\n  geom_point() +\n  labs(title = \"Price and area of houses in Duke Forest\")\n\n\n\nFigure¬†2: Scatterplot of price vs.¬†area of houses in Duke Forest\n\n\n\n\n\n2.2 Summary statistics\nTable¬†1 displays basic summary statistics for these two variables.\n\nShow the Codeduke_forest %>%\n  summarise(\n    `Median price` = median(price),\n    `IQR price` = IQR(price),\n    `Median area` = median(area),\n    `IQR area` = IQR(area),\n    `Correlation, r` = cor(price, area)\n    ) %>%\n  kable(digits = c(0, 0, 0, 0, 2))\n\n\n\n\n\nMedian price\nIQR price\nMedian area\nIQR area\nCorrelation, r\n\n\n540000\n193125\n2623\n1121\n0.67\n\n\nTable¬†1: Summary statistics for price and area of houses in Duke Forest"
  },
  {
    "objectID": "content/posts/02-authoring/authoring.html#sec-model",
    "href": "content/posts/02-authoring/authoring.html#sec-model",
    "title": "Housing Prices",
    "section": "\n3 Modeling",
    "text": "3 Modeling\nWe can fit a simple linear regression model of the form shown in Equation¬†1.\n\nprice = \\hat{\\beta_0} + \\hat{\\beta_1} \\times area + \\epsilon\n\\tag{1}\nTable¬†2 shows the regression output for this model.\n\nShow the Codeprice_fit <- lm(price ~ area, data = duke_forest)\n  \nprice_fit %>%\n  tidy() %>%\n  kable(digits = c(0, 0, 2, 2, 2))\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n(Intercept)\n116652\n53302.46\n2.19\n0.03\n\n\narea\n159\n18.17\n8.78\n0.00\n\n\n\nTable¬†2: Linear regression model for predicting price from area\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis is a pretty incomplete analysis, but hopefully the document provides a good overview of some of the authoring features of Quarto !"
  },
  {
    "objectID": "content/posts/02-authoring/authoring.html#crossreferences",
    "href": "content/posts/02-authoring/authoring.html#crossreferences",
    "title": "Housing Prices",
    "section": "\n4 Crossreferences",
    "text": "4 Crossreferences\nWe present the results of exploratory data analysis in Section¬†2 and the regression model in Section¬†3 .\nFigure¬†2 displays the relationship between these two variables in a scatterplot.\nTable¬†1 displays basic summary statistics for these two variables.\nWe can fit a simple linear regression model of the form shown in Equation¬†1."
  },
  {
    "objectID": "content/posts/02-authoring/callout-boxes.html",
    "href": "content/posts/02-authoring/callout-boxes.html",
    "title": "Callout Boxes",
    "section": "",
    "text": ":::{.callout-note}\nNote that there are five types of callouts, including:\n`note`, `warning`, `important`, `tip`, and `caution`.\n:::\n\n:::{.callout-tip}\n## Tip With Caption\n\nThis is an example of a callout with a caption.\n:::\n\n:::{.callout-caution collapse=\"true\"}\n## Expand To Learn About Collapse\n\nThis is an example of a 'folded' caution callout that can be expanded by the user. You can use `collapse=\"true\"` to collapse it by default or `collapse=\"false\"` to make a collapsible callout that is expanded by default.\n:::"
  },
  {
    "objectID": "content/posts/02-authoring/figure-layout.html",
    "href": "content/posts/02-authoring/figure-layout.html",
    "title": "Figure Layouts",
    "section": "",
    "text": "# returns a tibble of the files\n# \nfs::dir_info(here::here(\"content/materials/images\"))\n\n\n\n  \n\n\n\n\n# returns paths/files\nfs::dir_ls(here::here(\"content/materials/images\"))\n\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/00_final-screenshot.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/1.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/10.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/11.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/12.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/13.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/14.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/15.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/16.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/17.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/18.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/19.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/1book0.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/1book1.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/1book10.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/1book11.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/1book12.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/1book13.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/1book14.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/1book15.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/1book16.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/1book17.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/1book18.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/1book19.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/1book2.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/1book20.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/1book21.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/1book22.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/1book23.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/1book24.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/1book25.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/1book26.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/1book27.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/1book28.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/1book29.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/1book3.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/1book30.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/1book31.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/1book32.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/1book33.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/1book34.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/1book35.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/1book36.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/1book37.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/1book38.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/1book39.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/1book4.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/1book40.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/1book41.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/1book5.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/1book6.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/1book7.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/1book8.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/1book9.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/20.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book0.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book1.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book10.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book11.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book12.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book13.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book14.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book15.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book16.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book17.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book18.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book19.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book2.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book20.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book21.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book22.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book23.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book24.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book25.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book26.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book27.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book28.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book29.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book3.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book30.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book31.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book32.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book33.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book34.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book35.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book36.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book37.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book38.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book39.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book4.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book40.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book41.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book42.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book43.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book44.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book45.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book46.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book47.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book48.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book49.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book5.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book6.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book7.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book8.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/2book9.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/3.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/330px-BostonTerrier001.jpeg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/330px-Boston_Terrier_Eden_Forever.jpeg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/330px-Boston_Terrier_male.jpeg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/4.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/5.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/6.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/7.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/8.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/9.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/about-ada.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/ada-deployed-landing.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/ada-embed-slides.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/ada-home.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/ada-link-to-slides.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/addin-serve-site.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/Alice-and-Cat.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/Alice-with-flamingo.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/alicedata-lego-colors.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/Anecdote-spotting-a-business-story.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/Anecdote-Spotting-Oral-Stories.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/anti-join.gif\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/area-chart.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/avatar.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/blimp.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/blogdown-lifecycle-01.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/blogdown-lifecycle-02.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/blogdown-lifecycle-03.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/blogdown-lifecycle-04.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/blogdown-lifecycle-05.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/blogdown-lifecycle-06.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/blogdown-lifecycle-07.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/blogdown-lifecycle-08.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/blogdown-lifecycle-09.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/blogdown-lifecycle-1.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/blogdown-lifecycle-10.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/blogdown-lifecycle-2.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/blogdown-lifecycle-3.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/blogdown-lifecycle-4.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/blogdown-lifecycle-5.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/blogdown-lifecycle-6.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/blogdown-lifecycle-7.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/blogdown.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/blue-balloon.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/boston-sleep.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/boston-terrier.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/build-R-directory.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/Catterpillar.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/cocoa-in-viewer.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/common-aesthetics-1.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/data-reveals-crime.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/data-story-panel.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/dear-data.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/Density150.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/DoE.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/Drink-Me.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/drob-themes.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/featured.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/full-join.gif\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/gbbo-spoiler-tweet.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/ggplot-team-gray.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/ggplot-theme-commands.jpeg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/ggplot2-theme-elements-reference.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/Gimble-Gyre.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/Global-Options.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/goat_yoga_in_images.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/goat_yoga_in_slides-images.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/Hatter.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/headers\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/hugo-initial-files.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/hugo-theme-directory.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/InfoSurprise.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/Infovis-model.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/inner-join.gif\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/install-cocoa-eh-theme.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/intersect.gif\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/Jabberwock.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/knit-to-moon-reader.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/left-join-extra.gif\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/left-join.gif\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/Mad-Party.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/Map150.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/markdown-quick-reference.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/netlify-adv-build.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/netlify-basic-build.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/netlify-change-site-name.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/netlify-first-deploy.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/netlify-landing.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/netlify-manual-update.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/netlify-new-git-site.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/netlify-new-site.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/netlify-pick-git-repo.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/netlify-sign-up.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/Network150.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/New-Rmd.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/nightingale.jpeg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/noun-bar-chart-1139189.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/noun-box-plot-3060241.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/noun-line-chart-245859.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/noun-mosaic-chart-2339104.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/noun-probability-distribution-2475107.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/noun-sankey-graph-2797266.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/noun-scatter-plot-4362224.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/osm-online-features.gif\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/P&P.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/peapod.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/penguin.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/perceptual-ranking.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/pygmy.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/R-Console.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/r-data-types.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/rape-capital.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/Ratio-Interval-Ordinal-Nominal.PNG\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/right-join.gif\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/rladylego-pipe.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/rose.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/rstudio-icon.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/Sankey150.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/scatter-graph.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/scatter.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/scatter.svg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/ScatterPlot150.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/semi-join.gif\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/serve-site-public.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/setdiff-rev.gif\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/sf-classes.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/sheep.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/Slide06.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/Slide07.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/Slide08.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/Slide09.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/Slide10.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/standards.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/T-and-T.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/Terrier_Seated_(Boston_Public_Library).jpeg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/theme-team-tweets.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/tidyr-pivoting.gif\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/toilet.jpg\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/union-all.gif\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/union.gif\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/update-logo.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/website-using-blogdown.png\nC:/Users/Arvind/Documents/R work/MyWebsites/my-quarto-website/content/materials/images/Workflow.png"
  },
  {
    "objectID": "content/posts/02-authoring/figure-layout.html#figures",
    "href": "content/posts/02-authoring/figure-layout.html#figures",
    "title": "Figure Layouts",
    "section": "Figures",
    "text": "Figures\nBasic markdown syntax:\n   ![Boston Terrier](images/boston-terrier.png)\n\n\nBoston Terrier"
  },
  {
    "objectID": "content/posts/02-authoring/figure-layout.html#fragments",
    "href": "content/posts/02-authoring/figure-layout.html#fragments",
    "title": "Figure Layouts",
    "section": "Fragments",
    "text": "Fragments\n\n\n   ![Boston terrier](images/boston-terrier.png){fig-align=\"left\"}\n\n\n![](images/boston-terrier.png){fig-align=\"right\" fig-alt=\"A photo a Boston Terrier.\"}"
  },
  {
    "objectID": "content/posts/02-authoring/figure-layout.html#subfigures",
    "href": "content/posts/02-authoring/figure-layout.html#subfigures",
    "title": "Figure Layouts",
    "section": "Subfigures",
    "text": "Subfigures\n::: {#fig-bostons layout-ncol=2}\n\n![Excited](images/boston-terrier.png){#fig-boston width=\"250px\"}\n\n![Sleeping](images/boston-sleep.png){#fig-sleep width=\"250px\"}\n\nTwo states of Howard\n\n:::"
  },
  {
    "objectID": "content/posts/02-authoring/figure-layout.html#subfigures-1",
    "href": "content/posts/02-authoring/figure-layout.html#subfigures-1",
    "title": "Figure Layouts",
    "section": "Subfigures",
    "text": "Subfigures\n\nFigure¬†1: Two states of Howard\n\n\n\n\n(a) Excited\n\n\n\n\n\n\n(b) Sleeping"
  },
  {
    "objectID": "content/posts/03-computation/inline-code.html",
    "href": "content/posts/03-computation/inline-code.html",
    "title": "Inline Code",
    "section": "",
    "text": "library(tidyverse)\n\n‚îÄ‚îÄ Attaching packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 1.3.2 ‚îÄ‚îÄ\n‚úî ggplot2 3.4.0      ‚úî purrr   1.0.0 \n‚úî tibble  3.1.8      ‚úî dplyr   1.0.10\n‚úî tidyr   1.2.1      ‚úî stringr 1.5.0 \n‚úî readr   2.1.3      ‚úî forcats 0.5.2 \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n\nlibrary(palmerpenguins)\nThe dataset contains 344 penguin size measurements from Adelie, Gentoo, Chinstrap species across Torgersen, Biscoe, Dream islands.."
  },
  {
    "objectID": "content/posts/03-computation/inline-code.html#what-about-formatting",
    "href": "content/posts/03-computation/inline-code.html#what-about-formatting",
    "title": "Inline Code",
    "section": "What about formatting?",
    "text": "What about formatting?\n\npen_summary <- penguins |> \n  group_by(species) |> \n  summarize(avg_mass = mean(body_mass_g, na.rm = TRUE))\n\nThe average body mass by species is 3700.6622517, 3733.0882353, 5076.0162602.\nWe can do better!\n\nbody_mass <- scales::label_number(big.mark = \",\", accuracy = 0.1, suffix = \"g\")(pull(pen_summary, avg_mass))\n\nbody_mass\n\n[1] \"3,700.7g\" \"3,733.1g\" \"5,076.0g\"\n\n\nThe average body mass by species is 3,700.7g, 3,733.1g, 5,076.0g.\nWe can still do better!\n\nmass_reporter <- glue::glue_collapse(body_mass, sep = \", \", last = \", and \")\n\nThe average body mass by species is 3,700.7g, 3,733.1g, and 5,076.0g."
  },
  {
    "objectID": "content/posts/03-computation/inline-code.html#reporting-with-lists",
    "href": "content/posts/03-computation/inline-code.html#reporting-with-lists",
    "title": "Inline Code",
    "section": "Reporting with lists",
    "text": "Reporting with lists\nCredit to TJ Mahr\n\nknitted <- list(\n  when = format(Sys.Date()),\n  with = system(\"quarto --version\", intern = TRUE)\n)\n\nReported prepared on 2023-01-05 with quarto version 1.2.280."
  },
  {
    "objectID": "content/posts/03-computation/visual-editor.html",
    "href": "content/posts/03-computation/visual-editor.html",
    "title": "Visual Editor for Quarto",
    "section": "",
    "text": "RStudio 2022.07.1 comes with support for the Visual Mode of editing Quarto and other markdown-based files!\nThis is a WYSIWYM editor, meaning:\n\nWYSIWYM is an acronym that stands for What you see is what you mean. This was positioned to not be confused with WYSIWYG (what you see is what you get). The idea behind WYSIWYG is to display text on screen in much the exact same way as they will appear when printed on paper.\n\nWYSIWYM means that it can be translated differently, (where) the same content can lead to different output formats .\n\nThe Visual Markdown mode in RStudio allows for editing in plain text or visual mode, along with a visual representation of what it will actually look like while maintaining the ability to output to HTML or PDF.\nFull guide guide from the RStudio dev team that covers all the major topics and sub topics of the new features.\n\n\nOS\nDownload\nSize\nSHA-256\n\n\n\nWindows 10/11\n\nRStudio-2022.07.1-554.exe(opens in a new tab)\n\n190.14 MB\n5ab6215b\n\n\nmacOS 10.15+\n\nRStudio-2022.07.1-554.dmg(opens in a new tab)\n\n221.04 MB\n7b1a2285\n\n\nUbuntu 18+/Debian 10+\n\nrstudio-2022.07.1-554-amd64.deb(opens in a new tab)\n\n132.91 MB\n74b9e751\n\n\nUbuntu 22\n\nrstudio-2022.07.1-554-amd64.deb(opens in a new tab)\n\n145.33 MB\n92f2ab75\n\n\nFedora 19/Red Hat 7\n\nrstudio-2022.07.1-554-x86_64.rpm(opens in a new tab)\n\n103.29 MB\n0fc15d16\n\n\nFedora 34/Red Hat 8\n\nrstudio-2022.07.1-554-x86_64.rpm(opens in a new tab)\n\n149.77 MB\n0c4ef334\n\n\nOpenSUSE 15\n\nrstudio-2022.07.1-554-x86_64.rpm(opens in a new tab)\n\n133.76 MB\n45f277d0\n\n\n\n\n\nMarkdown documents can be edited in either source or visual button at the top-right of the document toolbar (or alternatively the ‚åò + ‚áß + F4 keyboard shortcut):\n\n\nPlease see long section of the guide.\n\nIf you have a workflow that involves editing in both visual and source mode, you may want to ensure that the same markdown is written no matter which mode edits originate from. You can accomplish this using the canonical option. For example:\n---\ntitle: \"My Document\"\neditor_options:\n  markdown:\n    wrap: 72\n    references: \n      location: block\n    canonical: true\n---\n\nThe editor toolbar includes buttons for the most commonly used formatting commands:\n\nAdditional commands are available on the Format, Insert, and Table menus:\n\n\n\n\n\n\n\n\n\n\n\n\nCommand\nKeyboard Shortcut\nMarkdown Shortcut\n\n\n\nBold\n‚åò B\n**bold**\n\n\nItalic\n‚åò I\n*italic*\n\n\nCode\n‚åò D\n`code`\n\n\nLink\n‚åò K\n<href>\n\n\nHeading 1\n‚å•‚åò 1\n#\n\n\nHeading 2\n‚å•‚åò 2\n##\n\n\nHeading 3\n‚å•‚åò 3\n###\n\n\nR Code Chunk\n‚å•‚åò I\n```{r}\n\n\n\nYou can also use the catch-all ‚åò/ shortcut to insert just about anything. Just execute the shortcut then type what you want to insert. For example:\nUse the bullet\n\nOutput\n\nOr numbered\n\nNumber list!\n\nHere‚Äôs a link - how to turn into an image?\n\nEXAMPLE LINK: https://rstudio.github.io/visual-markdown-editing/images/visual-editing-omni-list.png"
  },
  {
    "objectID": "content/posts/03-computation/visual-editor.html#equations",
    "href": "content/posts/03-computation/visual-editor.html#equations",
    "title": "Visual Editor for Quarto",
    "section": "Equations",
    "text": "Equations\nLaTeX equations are authored using standard Pandoc markdown syntax (the editor will automatically recognize the syntax and treat the equation as math). When you aren‚Äôt directly editing an equation it will appear as rendered math:\n\\[\nP(E) = {n \\choose k} p^k (2-p)^{n-k}\n\\]\n\nFootnotes\n\nYou can include footnotes using the Insert -> Footnote command (or the ‚áß ‚åò F7 keyboard shortcut). Footnote editing occurs in a pane immediately below the main document:1 <- NOTE THE FOOTMARK1¬†Very fancy footnote to this portion"
  },
  {
    "objectID": "content/posts/03-computation/visual-editor.html#inserting-citations",
    "href": "content/posts/03-computation/visual-editor.html#inserting-citations",
    "title": "Visual Editor for Quarto",
    "section": "Inserting Citations",
    "text": "Inserting Citations\nYou insert citations by either using the Insert -> Citation command or by using markdown syntax directly (e.g.¬†[@cite]).\nCitations go inside square brackets and are separated by semicolons. Each citation must have a key, composed of ‚Äò@‚Äô + the citation identifier from the database, and may optionally have a prefix, a locator, and a suffix. The citation key must begin with a letter, digit, or _, and may contain alphanumerics, _, and internal punctuation characters (:.#$%&-+?<>~/). Here are some examples:\n\n(Rottman-Sagebiel et al. 2018)\nDEMO OF CITATION WITH INSERT"
  },
  {
    "objectID": "content/posts/03-computation/visual-editor.html#embedded-code",
    "href": "content/posts/03-computation/visual-editor.html#embedded-code",
    "title": "Visual Editor for Quarto",
    "section": "Embedded Code",
    "text": "Embedded Code\nSource code which you include in an Quarto document can either by for display only or can be executed by knitr as part of rendering. Code can furthermore be either inline or block (e.g.¬†an Rmd code chunk).\nDisplaying Code\nTo display but not execute code, either use the Insert -> Code Block menu item, or start a new line and type either:\n\n``` (for a plain code block); or\n```<lang> (where <lang> is a language) for a code block with syntax highlighting.\n\nThen press the Enter key. To display code inline, simply surround text with backticks (`code`), or use the Format -> Code menu item.\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\npenguins %>% \n  ggplot(aes(x = body_weight_g, y = flipper_length_mm, color = species)) +\n  geom_point()\nCode Chunks\nTo insert an executable code chunk, use the Insert -> Code Chunk menu item, or start a new line and type:\n```{r}\nThen press the Enter key. Note that r could be another language supported by knitr (e.g.¬†python or sql) and you can also include a chunk label and other chunk options.\nTo include inline R code, you just create normal inline code (e.g.¬†by using backticks or the ‚åò D shortcut) but preface it with r. For example, this inline code will be executed by knitr: 2023-01-05. Note that when the code displays in visual mode it won‚Äôt have the backticks (but they will still appear in source mode).\n\npenguin_plot <- penguins %>% \n  na.omit() %>% \n  ggplot(aes(x = body_mass_g, y = flipper_length_mm)) +\n  geom_point(aes(color = species)) +\n  labs(title = \"Important Penguins\") +\n  geom_smooth(method = \"lm\", color = \"black\")\n\npenguin_plot\n\n\n\n\nR generated Tables\n\nlibrary(gt)\n\npenguins %>% \n  na.omit() %>% \n  select(species, bill_length_mm, body_mass_g) %>% \n  head() %>% \n  gt()\n\n\n\n\n\n\nspecies\n      bill_length_mm\n      body_mass_g\n    \n\n\nAdelie\n39.1\n3750\n\n\nAdelie\n39.5\n3800\n\n\nAdelie\n40.3\n3250\n\n\nAdelie\n36.7\n3450\n\n\nAdelie\n39.3\n3650\n\n\nAdelie\n38.9\n3625\n\n\n\n\n\n\n\nlibrary(reactable)\npenguins %>% \n  filter(species == \"Adelie\") %>% \n  na.omit() %>% \n  select(species, bill_length_mm, body_mass_g) %>% \n  reactable(defaultPageSize = 5)"
  },
  {
    "objectID": "content/posts/04-static/bootswatch-themed.html",
    "href": "content/posts/04-static/bootswatch-themed.html",
    "title": "Bootswatch Themed QMD",
    "section": "",
    "text": "Plots\nThis is the world‚Äôs most amazing plot. Everyone loves penguins.\nPenguins are fancy.\nThere are 344 in the dataset of interest.\n\nShow the codepenguins %>% \n  ggplot(aes(x = bill_length_mm, y = body_mass_g, color = species)) +\n  geom_point()\n\n\n\n\nYou can also include tables. Tables are super cool. I love tables and I love penguins. I am subject to oversight by the penguin overlords. The eternals are friends with the penguins.\n\nShow the codepenguins %>% \n  na.omit() %>% \n  group_by(species, sex) %>% \n  rename(\n    body_mass = body_mass_g, bill_length = bill_length_mm, \n    bill_depth = bill_depth_mm\n    ) %>% \n  summarise(\n    n = n(),\n    across(\n      .cols = c(body_mass, bill_length, bill_depth),\n      .fns = list(mean = mean, sd = sd)\n    ),\n    .groups = \"drop\"\n  ) %>% \n  gt(rowname_col = \"sex\") %>% \n  cols_label(\n    n = \"N\", body_mass_mean = \"Mean\", body_mass_sd = \"SD\", \n    bill_length_mean= \"Mean\", bill_length_sd = \"SD\",\n    bill_depth_mean = \"Mean\", bill_depth_sd = \"SD\"\n    ) %>% \n  gt::tab_spanner(\n    label = \"Body Mass (g)\",\n    columns = 4:5\n  ) %>% \n  gt::tab_spanner(\n    label = \"Bill Length (mm)\",\n    columns = 6:7\n  ) %>% \n  gt::tab_spanner(\n    label = \"Bill Depth (mm)\",\n    columns = 8:9\n  ) %>% \n  fmt_number(\n    columns = c(where(is.numeric), -n)\n  )\n\n\n\n\n\n\n\n\n      species\n      N\n      \n        Body Mass (g)\n      \n      \n        Bill Length (mm)\n      \n      \n        Bill Depth (mm)\n      \n    \n\nMean\n      SD\n      Mean\n      SD\n      Mean\n      SD\n    \n\n\n\nfemale\nAdelie\n73\n3,368.84\n269.38\n37.26\n2.03\n17.62\n0.94\n\n\nmale\nAdelie\n73\n4,043.49\n346.81\n40.39\n2.28\n19.07\n1.02\n\n\nfemale\nChinstrap\n34\n3,527.21\n285.33\n46.57\n3.11\n17.59\n0.78\n\n\nmale\nChinstrap\n34\n3,938.97\n362.14\n51.09\n1.56\n19.25\n0.76\n\n\nfemale\nGentoo\n58\n4,679.74\n281.58\n45.56\n2.05\n14.24\n0.54\n\n\nmale\nGentoo\n61\n5,484.84\n313.16\n49.47\n2.72\n15.72\n0.74"
  },
  {
    "objectID": "content/posts/04-static/old-rmarkdown.html",
    "href": "content/posts/04-static/old-rmarkdown.html",
    "title": "Old Markdown - reactable example",
    "section": "",
    "text": "This is an example from reactable documentation - showing the Twitter followers of some politicians.\nExample adapted from {reactable} documentation\n\nlibrary(reactable)\nlibrary(htmltools)\nlibrary(dplyr)\n\ndata <- read.csv(\"https://glin.github.io/reactable/articles/twitter-followers/twitter_followers.csv\",\n                 stringsAsFactors = FALSE)\n\n# Render a bar chart with a label on the left\nbar_chart <- function(label, width = \"100%\", height = \"14px\", fill = \"#00bfc4\", background = NULL) {\n  bar <- div(style = list(background = fill, width = width, height = height))\n  chart <- div(style = list(flexGrow = 1, marginLeft = \"6px\", background = background), bar)\n  div(style = list(display = \"flex\", alignItems = \"center\"), label, chart)\n}\n\ntbl <- reactable(\n  data,\n  pagination = FALSE,\n  defaultSorted = \"exclusive_followers_pct\",\n  defaultColDef = colDef(headerClass = \"header\", align = \"left\"),\n  columns = list(\n    account = colDef(\n      cell = function(value) {\n        url <- paste0(\"https://twitter.com/\", value)\n        tags$a(href = url, target = \"_blank\", paste0(\"@\", value))\n      },\n      width = 150\n    ),\n    followers = colDef(\n      defaultSortOrder = \"desc\",\n      cell = function(value) {\n        width <- paste0(value * 100 / max(data$followers), \"%\")\n        value <- format(value, big.mark = \",\")\n        value <- format(value, width = 9, justify = \"right\")\n        bar <- div(\n          class = \"bar-chart\",\n          style = list(marginRight = \"6px\"),\n          div(class = \"bar\", style = list(width = width, backgroundColor = \"#3fc1c9\"))\n        )\n        div(class = \"bar-cell\", span(class = \"number\", value), bar)\n      }\n    ),\n    exclusive_followers_pct = colDef(\n      name = \"Exclusive Followers\",\n      defaultSortOrder = \"desc\",\n      cell = JS(\"function(cellInfo) {\n        // Format as percentage\n        const pct = (cellInfo.value * 100).toFixed(1) + '%'\n        // Pad single-digit numbers\n        let value = pct.padStart(5)\n        // Show % on first row only\n        if (cellInfo.viewIndex > 0) {\n          value = value.replace('%', ' ')\n        }\n        // Render bar chart\n        return (\n          '<div class=\\\"bar-cell\\\">' +\n            '<span class=\\\"number\\\">' + value + '</span>' +\n            '<div class=\\\"bar-chart\\\" style=\\\"background-color: #e1e1e1\\\">' +\n              '<div class=\\\"bar\\\" style=\\\"width: ' + pct + '; background-color: #fc5185\\\"></div>' +\n            '</div>' +\n          '</div>'\n        )\n      }\"),\n      html = TRUE\n    )\n  ),\n  compact = TRUE,\n  class = \"followers-tbl\"\n)\n\ntbl\n\n\n\n\n\n\n\n# Add Google Fonts to the page\ntags$link(href = \"https://fonts.googleapis.com/css?family=Karla:400,700|Fira+Mono&display=fallback\",\n          rel = \"stylesheet\")\n\n\n\n\n\n/* CSS for the R Markdown document, inserted through a ```{css} code chunk */\n\n/* Styles for the table container, title, and subtitle */\n.twitter-followers {\n  /* Center the table */\n  margin: 0 auto;\n  /* Reduce the table width */\n  width: 575px;\n  font-family: Karla, \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n}\n\n.followers-header {\n  margin: 8px 0;\n  font-size: 16px;\n}\n\n.followers-title {\n  font-size: 20px;\n  font-weight: 600;\n}\n\n/* Styles for the table */\n.followers-tbl {\n  font-size: 14px;\n  line-height: 18px;\n}\n\n.followers-tbl a {\n  color: inherit;\n}\n\n/* Styles for the column headers */\n.header {\n  border-bottom: 2px solid #555;\n  font-size: 13px;\n  font-weight: 400;\n  text-transform: uppercase;\n}\n\n.header:hover {\n  background-color: #eee;\n}\n\n/* Styles for the bar charts */\n.bar-cell {\n  display: flex;\n  align-items: center;\n}\n\n.number {\n  font-family: \"Fira Mono\", Consolas, Monaco, monospace;\n  font-size: 13.5px;\n  white-space: pre;\n}\n\n.bar-chart {\n  flex-grow: 1;\n  margin-left: 6px;\n  height: 14px;\n}\n\n.bar {\n  height: 100%;\n}"
  },
  {
    "objectID": "content/posts/04-static/penguin-report.html",
    "href": "content/posts/04-static/penguin-report.html",
    "title": "Penguins Distilled",
    "section": "",
    "text": "Diagram of penguin head with indication of bill length and bill depth.\n\n\nLiterate Programming\nPer Donald Knuth\n\nA programming paradigm introduced by Donald Knuth in which a computer program is given an explanation of its logic in a natural language, such as English, interspersed with snippets of macros and traditional source code, from which compilable source code can be generated.\n\nInitial explore\nWe can do a quick exploration of the data with skimr::skim(). This will report the counts of various variables, along with some basic descriptive statistics. The skimr package is fantastic for quickly getting a sense of your datasets.\nAhead of skimr there are 344 penguins in this dataset, and the unique species are Adelie, Gentoo, Chinstrap.\nPer the rOpenSci skimr docs:\n\nskimr provides a frictionless approach to summary statistics which conforms to the principle of least surprise, displaying summary statistics the user can skim quickly to understand their data. It handles different data types and returns a skim_df object which can be included in a pipeline or displayed nicely for the human reader.\n\n\npenguins %>% \n  group_by(species) %>% \n  skimr::skim() %>% \n  select(-contains(\"numeric.p\"))\n\n\n\n\n\nName\nPiped data\n\n\nNumber of rows\n344\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nspecies\n\n\n\nData summaryVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nspecies\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nisland\nAdelie\n0\n1.00\nFALSE\n3\nDre: 56, Tor: 52, Bis: 44\n\n\nisland\nChinstrap\n0\n1.00\nFALSE\n1\nDre: 68, Bis: 0, Tor: 0\n\n\nisland\nGentoo\n0\n1.00\nFALSE\n1\nBis: 124, Dre: 0, Tor: 0\n\n\nsex\nAdelie\n6\n0.96\nFALSE\n2\nfem: 73, mal: 73\n\n\nsex\nChinstrap\n0\n1.00\nFALSE\n2\nfem: 34, mal: 34\n\n\nsex\nGentoo\n5\n0.96\nFALSE\n2\nmal: 61, fem: 58\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nspecies\nn_missing\ncomplete_rate\nmean\nsd\nhist\n\n\n\nbill_length_mm\nAdelie\n1\n0.99\n38.79\n2.66\n‚ñÅ‚ñÜ‚ñá‚ñÜ‚ñÅ\n\n\nbill_length_mm\nChinstrap\n0\n1.00\n48.83\n3.34\n‚ñÇ‚ñá‚ñá‚ñÖ‚ñÅ\n\n\nbill_length_mm\nGentoo\n1\n0.99\n47.50\n3.08\n‚ñÉ‚ñá‚ñÜ‚ñÅ‚ñÅ\n\n\nbill_depth_mm\nAdelie\n1\n0.99\n18.35\n1.22\n‚ñÇ‚ñÜ‚ñá‚ñÉ‚ñÅ\n\n\nbill_depth_mm\nChinstrap\n0\n1.00\n18.42\n1.14\n‚ñÖ‚ñá‚ñá‚ñÜ‚ñÇ\n\n\nbill_depth_mm\nGentoo\n1\n0.99\n14.98\n0.98\n‚ñÖ‚ñá‚ñá‚ñÜ‚ñÇ\n\n\nflipper_length_mm\nAdelie\n1\n0.99\n189.95\n6.54\n‚ñÅ‚ñÜ‚ñá‚ñÖ‚ñÅ\n\n\nflipper_length_mm\nChinstrap\n0\n1.00\n195.82\n7.13\n‚ñÅ‚ñÖ‚ñá‚ñÖ‚ñÇ\n\n\nflipper_length_mm\nGentoo\n1\n0.99\n217.19\n6.48\n‚ñÇ‚ñá‚ñá‚ñÜ‚ñÉ\n\n\nbody_mass_g\nAdelie\n1\n0.99\n3700.66\n458.57\n‚ñÖ‚ñá‚ñá‚ñÉ‚ñÇ\n\n\nbody_mass_g\nChinstrap\n0\n1.00\n3733.09\n384.34\n‚ñÅ‚ñÖ‚ñá‚ñÉ‚ñÅ\n\n\nbody_mass_g\nGentoo\n1\n0.99\n5076.02\n504.12\n‚ñÉ‚ñá‚ñá‚ñá‚ñÇ\n\n\nyear\nAdelie\n0\n1.00\n2008.01\n0.82\n‚ñá‚ñÅ‚ñá‚ñÅ‚ñá\n\n\nyear\nChinstrap\n0\n1.00\n2007.97\n0.86\n‚ñá‚ñÅ‚ñÜ‚ñÅ‚ñá\n\n\nyear\nGentoo\n0\n1.00\n2008.08\n0.79\n‚ñÜ‚ñÅ‚ñá‚ñÅ‚ñá\n\n\n\n\n\nSpecific statistics\nWe can also explore specific statistics\nThe penguins split by species show a specific relationship between weight and flipper length, where the Adelie female penguins are the lighest and have the shortest flippers.\n\npenguins %>% \n  group_by(species, sex) %>% \n  summarize(\n    n = n(), \n    weight = mean(body_mass_g, na.rm = TRUE),\n    flipper_length = mean(flipper_length_mm, na.rm = TRUE)\n    ) %>% \n  arrange(desc(weight))\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 √ó 5\n# Groups:   species [3]\n  species   sex        n weight flipper_length\n  <fct>     <fct>  <int>  <dbl>          <dbl>\n1 Gentoo    male      61  5485.           222.\n2 Gentoo    female    58  4680.           213.\n3 Gentoo    <NA>       5  4588.           216.\n4 Adelie    male      73  4043.           192.\n5 Chinstrap male      34  3939.           200.\n6 Adelie    <NA>       6  3540            186.\n7 Chinstrap female    34  3527.           192.\n8 Adelie    female    73  3369.           188.\n\n\nLooks like the Adelie are the lightest penguin. I want to see their distribution along with the overall distribution.\n\npenguins %>% \n  filter(is.na(sex))\n\n# A tibble: 11 √ó 8\n   species island    bill_length_mm bill_depth_mm flipper_‚Ä¶¬π body_‚Ä¶¬≤ sex    year\n   <fct>   <fct>              <dbl>         <dbl>      <int>   <int> <fct> <int>\n 1 Adelie  Torgersen           NA            NA           NA      NA <NA>   2007\n 2 Adelie  Torgersen           34.1          18.1        193    3475 <NA>   2007\n 3 Adelie  Torgersen           42            20.2        190    4250 <NA>   2007\n 4 Adelie  Torgersen           37.8          17.1        186    3300 <NA>   2007\n 5 Adelie  Torgersen           37.8          17.3        180    3700 <NA>   2007\n 6 Adelie  Dream               37.5          18.9        179    2975 <NA>   2007\n 7 Gentoo  Biscoe              44.5          14.3        216    4100 <NA>   2007\n 8 Gentoo  Biscoe              46.2          14.4        214    4650 <NA>   2008\n 9 Gentoo  Biscoe              47.3          13.8        216    4725 <NA>   2009\n10 Gentoo  Biscoe              44.5          15.7        217    4875 <NA>   2009\n11 Gentoo  Biscoe              NA            NA           NA      NA <NA>   2009\n# ‚Ä¶ with abbreviated variable names ¬π‚Äãflipper_length_mm, ¬≤‚Äãbody_mass_g\n\n\n\nsmaller <- palmerpenguins::penguins %>% \n  filter(species == \"Adelie\", \n         !is.na(body_mass_g))\n\nCleanup the data\nIf you noticed above, there was some NA or missing data. We can remove those rows for now.\n\npenguins_clean <- penguins %>% \n  na.omit() %>% \n  mutate(species_num = as.numeric(species))\n\nPlot Section\nLet‚Äôs move on to some plots, for the overall distributions and for just the Adelie penguins. The overall distribution of the data by species shows some overlap in body weight for Adelie/Chinstrap, but more of a separation for the Gentoo penguins.\n\npenguins %>% \n  ggplot(aes(body_mass_g, fill = species)) + \n  geom_density(color = \"white\", alpha = 0.5) +\n  scale_fill_manual(values = c(\"darkorange\",\"purple\",\"cyan4\")) +\n  labs(x = \"Penguin Bins\")\n\n\n\n\nWhen we compare just within the Adelie penguins, we can see more of a specific separation of male vs female. However, there is still a decent amount of overlapping data.\n\npenguin_plot <- smaller %>% \n  filter(!is.na(sex)) %>% \n  ggplot(aes(body_mass_g, fill = sex)) + \n  geom_density(color = \"white\", alpha = 0.5) +\n    scale_fill_manual(values = c(\"darkorange\",\"purple\")) +\n  labs(x = \"Penguin Bins\")\n\npenguin_plot\n\n\n\n\nLastly we can fit a basic linear model comparing body weight in grams to the flipper length of the penguins by specific species. There is a strong linear relationship, although it‚Äôs a bit difficult to distinguish between Chinstrap and Adelie penguins.\n\npenguin_size_plot <- penguins_clean %>% \n  ggplot(aes(x = body_mass_g, y = flipper_length_mm, color = species)) + \n  scale_color_manual(values = c(\"darkorange\",\"purple\",\"cyan4\")) +\n  geom_point(size = 2, alpha = 0.5) +\n  labs(x = \"Mass (g)\", y = \"Flipper Length (mm)\") +\n  geom_smooth(aes(group = \"none\"), method = \"lm\")\n\npenguin_size_plot\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nWe can save the overall distribution and the linear model plot.\n\nggsave(\"penguin-dist.png\", penguin_plot, \n  dpi = \"retina\", height = 8, width = 8)\n\nggsave(\"penguin-smooth.png\", penguin_size_plot, \n  dpi = \"retina\", height = 8, width = 8)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nModeling section\nMoving on to some basic modeling we can see if what kind of relationships we observe in the data. Note that I‚Äôm really not following any plan, just indicating how you can fit some different models all at once with dplyr + broom.\n\nmodel_inputs <- tibble(\n  model_form = c(\n    list(flipper_length_mm ~ body_mass_g),\n    list(species_num ~ bill_length_mm + body_mass_g + sex),\n    list(flipper_length_mm ~ bill_length_mm + species)\n    ),\n  data = list(penguins_clean)\n) \n\nmodel_metrics <- model_inputs %>% \n  rowwise(model_form, data) %>% \n  summarize(lm = list(lm(model_form, data = data)), .groups = \"drop\") %>% \n  rowwise(model_form, lm, data) %>% \n  summarise(broom::glance(lm), .groups = \"drop\")\n\nWrap up\nWe can then take the model outcomes and throw them into a quick gt table.\n\nmodel_metrics %>% \n  select(model_form, r.squared:p.value) %>% \n  mutate(model_form = as.character(model_form)) %>% \n  gt::gt() %>% \n  gt::fmt_number(r.squared:statistic) %>% \n  gt::fmt_scientific(p.value) %>% \n  gt::cols_width(\n    model_form ~ px(150)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel_form\n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n    \n\n\nflipper_length_mm ~ body_mass_g\n0.76\n0.76\n6.85\n1,060.30\n3.13 √ó 10‚àí105\n\n\n\nspecies_num ~ bill_length_mm + body_mass_g + sex\n0.84\n0.84\n0.36\n583.59\n2.45 √ó 10‚àí131\n\n\n\nflipper_length_mm ~ bill_length_mm + species\n0.83\n0.83\n5.83\n529.22\n1.66 √ó 10‚àí125\n\n\n\n\n\n\n\nOverall, this was a quick overview of the beauty of literate programming. We have R code that is self-documenting, as we capture our thoughts and the outputs in a single document. We know at some level that the code works since it ‚Äúlogs‚Äù the outputs at various stages and could still output to additional log files. To render it has to run successfully in a linear fashion, and it is human readable as code, via the visual editor or even in version control like Git!"
  },
  {
    "objectID": "content/posts/05-presentations/fragments.html#make-these-columns-appear-in-order",
    "href": "content/posts/05-presentations/fragments.html#make-these-columns-appear-in-order",
    "title": "Presentation with Columns?",
    "section": "Make these columns appear in order",
    "text": "Make these columns appear in order\n\n\nThese appear first\n\nMake\nYour\nList\n\n\nThen this\n\n```{r}\nhead(mtcars)\n```\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1"
  },
  {
    "objectID": "content/posts/05-presentations/revealjs-penguins.html#section",
    "href": "content/posts/05-presentations/revealjs-penguins.html#section",
    "title": "Penguin Report Presentation",
    "section": "",
    "text": "Diagram of penguin head with indication of bill length and bill depth."
  },
  {
    "objectID": "content/posts/05-presentations/revealjs-penguins.html#literate-programming",
    "href": "content/posts/05-presentations/revealjs-penguins.html#literate-programming",
    "title": "Penguin Report Presentation",
    "section": "Literate Programming",
    "text": "Literate Programming\nPer Donald Knuth\n\nA programming paradigm introduced by Donald Knuth in which a computer program is given an explanation of its logic in a natural language, such as English, interspersed with snippets of macros and traditional source code, from which compilable source code can be generated."
  },
  {
    "objectID": "content/posts/05-presentations/revealjs-penguins.html#specific-statistics",
    "href": "content/posts/05-presentations/revealjs-penguins.html#specific-statistics",
    "title": "Penguin Report Presentation",
    "section": "Specific statistics",
    "text": "Specific statistics\n\npenguins %>% \n  group_by(species, sex) %>% \n  summarize(\n    n = n(), \n    weight = mean(body_mass_g, na.rm = TRUE),\n    flipper_length = mean(flipper_length_mm, na.rm = TRUE)\n    ) %>% \n  arrange(desc(weight))\n\n# A tibble: 8 √ó 5\n# Groups:   species [3]\n  species   sex        n weight flipper_length\n  <fct>     <fct>  <int>  <dbl>          <dbl>\n1 Gentoo    male      61  5485.           222.\n2 Gentoo    female    58  4680.           213.\n3 Gentoo    <NA>       5  4588.           216.\n4 Adelie    male      73  4043.           192.\n5 Chinstrap male      34  3939.           200.\n6 Adelie    <NA>       6  3540            186.\n7 Chinstrap female    34  3527.           192.\n8 Adelie    female    73  3369.           188."
  },
  {
    "objectID": "content/posts/05-presentations/revealjs-penguins.html#cleanup-the-data",
    "href": "content/posts/05-presentations/revealjs-penguins.html#cleanup-the-data",
    "title": "Penguin Report Presentation",
    "section": "Cleanup the data",
    "text": "Cleanup the data\nIf you noticed above, there was some NA or missing data. We can remove those rows for now.\n\npenguins_clean <- penguins %>% \n  na.omit() %>% \n  mutate(species_num = as.numeric(species))"
  },
  {
    "objectID": "content/posts/05-presentations/revealjs-penguins.html#plot-section",
    "href": "content/posts/05-presentations/revealjs-penguins.html#plot-section",
    "title": "Penguin Report Presentation",
    "section": "Plot Section",
    "text": "Plot Section\nLet‚Äôs move on to some plots, for the overall distributions and for just the Adelie penguins. The overall distribution of the data by species shows some overlap in body weight for Adelie/Chinstrap, but more of a separation for the Gentoo penguins."
  },
  {
    "objectID": "content/posts/05-presentations/revealjs-penguins.html#distribution",
    "href": "content/posts/05-presentations/revealjs-penguins.html#distribution",
    "title": "Penguin Report Presentation",
    "section": "Distribution",
    "text": "Distribution\n\npenguins %>% \n  ggplot(aes(body_mass_g, fill = species)) + \n  geom_density(color = \"white\", alpha = 0.5) +\n  scale_fill_manual(values = c(\"darkorange\",\"purple\",\"cyan4\")) +\n  labs(x = \"Penguin Bins\")"
  },
  {
    "objectID": "content/posts/05-presentations/revealjs-penguins.html#lm-scatter-plot",
    "href": "content/posts/05-presentations/revealjs-penguins.html#lm-scatter-plot",
    "title": "Penguin Report Presentation",
    "section": "LM + Scatter Plot",
    "text": "LM + Scatter Plot\n\npenguin_size_plot <- penguins_clean %>% \n  ggplot(aes(x = body_mass_g, y = flipper_length_mm, color = species)) + \n  scale_color_manual(values = c(\"darkorange\",\"purple\",\"cyan4\")) +\n  geom_point(size = 2, alpha = 0.5) +\n  labs(x = \"Mass (g)\", y = \"Flipper Length (mm)\") +\n  geom_smooth(aes(group = \"none\"), method = \"lm\")\n\npenguin_size_plot"
  },
  {
    "objectID": "content/posts/05-presentations/revealjs-penguins.html#modeling-section",
    "href": "content/posts/05-presentations/revealjs-penguins.html#modeling-section",
    "title": "Penguin Report Presentation",
    "section": "Modeling section",
    "text": "Modeling section\nMoving on to some basic modeling we can see if what kind of relationships we observe in the data. Note that I‚Äôm really not following any plan, just indicating how you can fit some different models all at once with dplyr + broom."
  },
  {
    "objectID": "content/posts/05-presentations/revealjs-penguins.html#section-1",
    "href": "content/posts/05-presentations/revealjs-penguins.html#section-1",
    "title": "Penguin Report Presentation",
    "section": "",
    "text": "model_inputs <- tibble(\n  model_form = c(\n    list(flipper_length_mm ~ body_mass_g),\n    list(species_num ~ bill_length_mm + body_mass_g + sex),\n    list(flipper_length_mm ~ bill_length_mm + species)\n    ),\n  data = list(penguins_clean)\n) \n\nmodel_metrics <- model_inputs %>% \n  rowwise(model_form, data) %>% \n  summarize(lm = list(lm(model_form, data = data)), .groups = \"drop\") %>% \n  rowwise(model_form, lm, data) %>% \n  summarise(broom::glance(lm), .groups = \"drop\")"
  },
  {
    "objectID": "content/posts/05-presentations/revealjs-penguins.html#section-2",
    "href": "content/posts/05-presentations/revealjs-penguins.html#section-2",
    "title": "Penguin Report Presentation",
    "section": "",
    "text": "model_metrics %>% \n  select(model_form, r.squared:p.value) %>% \n  mutate(model_form = as.character(model_form)) %>% \n  gt::gt() %>% \n  gt::fmt_number(r.squared:statistic) %>% \n  gt::fmt_scientific(p.value) %>% \n  gt::cols_width(\n    model_form ~ px(150)\n  )\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n  \n  \n  \n    \n      model_form\n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n    \n  \n  \n    flipper_length_mm ~ body_mass_g\n0.76\n0.76\n6.85\n1,060.30\n3.13 √ó 10‚àí105\n    species_num ~ bill_length_mm + body_mass_g + sex\n0.84\n0.84\n0.36\n583.59\n2.45 √ó 10‚àí131\n    flipper_length_mm ~ bill_length_mm + species\n0.83\n0.83\n5.83\n529.22\n1.66 √ó 10‚àí125"
  },
  {
    "objectID": "content/posts/07-visuals/figure-layout-2.html",
    "href": "content/posts/07-visuals/figure-layout-2.html",
    "title": "Figure Layouts-2",
    "section": "",
    "text": "::: {#fig-bostons layout-nrow=2}\n\n![Excited](boston-terrier.png){#fig-boston width=\"250px\"}\n\n![Sleeping](boston-sleep.png){#fig-sleep width=\"250px\"}\n\n![Still Excited](boston-terrier.png){#fig-boston width=\"250px\"}\n\n![Still sleeping](boston-sleep.png){#fig-sleep width=\"250px\"}\n\nTwo states of Howard, twice\n\n:::"
  },
  {
    "objectID": "content/posts/07-visuals/figure-layout-2.html#subfigures-1",
    "href": "content/posts/07-visuals/figure-layout-2.html#subfigures-1",
    "title": "Figure Layouts-2",
    "section": "Subfigures",
    "text": "Subfigures\n\nFigure¬†1: Two states of Howard, twice\n\n\n\n\n(a) Excited\n\n\n\n\n\n\n(b) Sleeping\n\n\n\n\n\n\n\n\nStill Excited\n\n\n\n\n\n\nStill sleeping"
  },
  {
    "objectID": "content/posts/07-visuals/figure-layout-2.html#subfigures-2",
    "href": "content/posts/07-visuals/figure-layout-2.html#subfigures-2",
    "title": "Figure Layouts-2",
    "section": "Subfigures",
    "text": "Subfigures\n\n\n\n\n\nExcited\n\n\n\n\n\n\nSleeping\n\n\n\n\n\n\nStill Excited\n\n\n\n\n\n\n\n\nStill sleeping\n\n\n\n\nTwo states of Howard, twice"
  },
  {
    "objectID": "content/posts/07-visuals/figure-layout.html",
    "href": "content/posts/07-visuals/figure-layout.html",
    "title": "Figure Layout",
    "section": "",
    "text": "library(ggplot2)"
  },
  {
    "objectID": "content/posts/07-visuals/figure-layout.html#figures",
    "href": "content/posts/07-visuals/figure-layout.html#figures",
    "title": "Figure Layout",
    "section": "Figures",
    "text": "Figures\nBasic markdown syntax:\n         ![Boston Terrier](boston-terrier.png)\n\n\nBoston Terrier"
  },
  {
    "objectID": "content/posts/07-visuals/figure-layout.html#fragments",
    "href": "content/posts/07-visuals/figure-layout.html#fragments",
    "title": "Figure Layout",
    "section": "Fragments",
    "text": "Fragments\n\n\n       ![Boston terrier](boston-terrier.png){fig-align=\"left\"}\n\n\n\n ![](boston-terrier.png){fig-align=\"right\" fig-alt=\"A photo a Boston Terrier.\"}"
  },
  {
    "objectID": "content/posts/07-visuals/figure-layout.html#subfigures",
    "href": "content/posts/07-visuals/figure-layout.html#subfigures",
    "title": "Figure Layout",
    "section": "Subfigures",
    "text": "Subfigures\n\nFigure¬†1: ?(caption)\n\n\n\n![Excited](boston-terrier.png){#fig-boston width=\"250px\"}\n\n![Sleeping](boston-sleep.png){#fig-sleep width=\"250px\"}\n\nTwo states of Howard"
  },
  {
    "objectID": "content/posts/07-visuals/figure-layout.html#subfigures-1",
    "href": "content/posts/07-visuals/figure-layout.html#subfigures-1",
    "title": "Figure Layout",
    "section": "Subfigures",
    "text": "Subfigures\n\nFigure¬†2: Two states of Howard\n\n\n\n\n(a) Excited\n\n\n\n\n\n\n(b) Sleeping"
  },
  {
    "objectID": "content/posts/07-visuals/figure-layout.html#custom-grid",
    "href": "content/posts/07-visuals/figure-layout.html#custom-grid",
    "title": "Figure Layout",
    "section": "Custom grid",
    "text": "Custom grid\n\n\nThis column takes 1/2 of the page\n\nggplot(mtcars, aes(x = cyl, y = disp)) +\n  geom_boxplot()\n\nWarning: Continuous x aesthetic\n‚Ñπ did you forget `aes(group = ...)`?\n\n\n\n\n\n\n\nThis column takes 1/2 of the page\n\nhead(mtcars, 5)[1:4] |> knitr::kable()\n\n\n\n\nmpg\ncyl\ndisp\nhp\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n\n\nDatsun 710\n22.8\n4\n108\n93\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n\n\nHornet Sportabout\n18.7\n8\n360\n175"
  },
  {
    "objectID": "content/posts/07-visuals/gt-summary.html#introduction",
    "href": "content/posts/07-visuals/gt-summary.html#introduction",
    "title": "Overview of using gtsummary\n",
    "section": "Introduction",
    "text": "Introduction\nReproducible reports are an important part of good practices. We often need to report the results from a table in the text of an Quarto report. Inline reporting has been made simple with inline_text(). The inline_text() function reports statistics from {gtsummary} tables inline in an Quarto report."
  },
  {
    "objectID": "content/posts/07-visuals/gt-summary.html#setup",
    "href": "content/posts/07-visuals/gt-summary.html#setup",
    "title": "Overview of using gtsummary\n",
    "section": "Setup",
    "text": "Setup\nBefore going through the tutorial, install and load {gtsummary}.\n\n# install.packages(\"gtsummary\")\nlibrary(gtsummary)"
  },
  {
    "objectID": "content/posts/07-visuals/gt-summary.html#example-data-set",
    "href": "content/posts/07-visuals/gt-summary.html#example-data-set",
    "title": "Overview of using gtsummary\n",
    "section": "Example data set",
    "text": "Example data set\nWe‚Äôll be using the trial data set throughout this example.\n\nThis set contains data from 200 patients who received one of two types of chemotherapy (Drug A or Drug B). The outcomes are tumor response and death.\n\nFor brevity in the tutorial, let‚Äôs keep a subset of the variables from the trial data set.\n\ntrial2 <-\n  trial %>%\n  select(trt, marker, stage)"
  },
  {
    "objectID": "content/posts/07-visuals/gt-summary.html#inline_text_tbl_summary",
    "href": "content/posts/07-visuals/gt-summary.html#inline_text_tbl_summary",
    "title": "Overview of using gtsummary\n",
    "section": "Inline results from tbl_summary()",
    "text": "Inline results from tbl_summary()\nFirst create a basic summary table using tbl_summary() (review tbl_summary() vignette for detailed overview of this function if needed).\n\ntab1 <- tbl_summary(trial2, by = trt)\ntab1\n\n\n\n\n\n\nCharacteristic\n      \nDrug A, N = 981\n\n      \nDrug B, N = 1021\n\n    \n\n\nMarker Level (ng/mL)\n0.84 (0.24, 1.57)\n0.52 (0.19, 1.20)\n\n\n¬†¬†¬†¬†Unknown\n6\n4\n\n\nT Stage\n\n\n\n\n¬†¬†¬†¬†T1\n28 (29%)\n25 (25%)\n\n\n¬†¬†¬†¬†T2\n25 (26%)\n29 (28%)\n\n\n¬†¬†¬†¬†T3\n22 (22%)\n21 (21%)\n\n\n¬†¬†¬†¬†T4\n23 (23%)\n27 (26%)\n\n\n\n\n1 Median (IQR); n (%)\n    \n\n\n\n\nTo report the median (IQR) of the marker levels in each group, use the following commands inline.\n\nThe median (IQR) marker level in the Drug A and Drug B groups are 0.84 (0.24, 1.57) and 0.52 (0.19, 1.20), respectively. Here‚Äôs how the line will appear in your report.\n\n\nThe median (IQR) marker level in the Drug A and Drug B groups are 0.84 (0.24, 1.57) and 0.52 (0.19, 1.20), respectively. If you display a statistic from a categorical variable, include the level argument.\n\n\n25 (25%) resolves to ‚Äú25 (25%)‚Äù"
  },
  {
    "objectID": "content/posts/07-visuals/gt-summary.html#inline_text_tbl_regression",
    "href": "content/posts/07-visuals/gt-summary.html#inline_text_tbl_regression",
    "title": "Overview of using gtsummary\n",
    "section": "Inline results from tbl_regression()",
    "text": "Inline results from tbl_regression()\nSimilar syntax is used to report results from tbl_regression() and tbl_uvregression() tables. Refer to the tbl_regression() vignette if you need detailed guidance on using these functions.\nLet‚Äôs first create a regression model.\n\n# build logistic regression model\nm1 <- glm(response ~ age + stage, trial, family = binomial(link = \"logit\"))\n\nNow summarize the results with tbl_regression(); exponentiate to get the odds ratios.\n\ntbl_m1 <- tbl_regression(m1, exponentiate = TRUE)\ntbl_m1\n\n\n\n\n\n\nCharacteristic\n      \nOR1\n\n      \n95% CI1\n\n      p-value\n    \n\n\nAge\n1.02\n1.00, 1.04\n0.091\n\n\nT Stage\n\n\n\n\n\n¬†¬†¬†¬†T1\n‚Äî\n‚Äî\n\n\n\n¬†¬†¬†¬†T2\n0.58\n0.24, 1.37\n0.2\n\n\n¬†¬†¬†¬†T3\n0.94\n0.39, 2.28\n0.9\n\n\n¬†¬†¬†¬†T4\n0.79\n0.33, 1.90\n0.6\n\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n    \n\n\n\n\nTo report the result for age, use the following commands inline.\n\n1.02 (95% CI 1.00, 1.04; p=0.091) Here‚Äôs how the line will appear in your report.\n\n\n1.02 (95% CI 1.00, 1.04; p=0.091) It is reasonable that you‚Äôll need to modify the text. To do this, use the pattern argument. The pattern argument syntax follows glue::glue() format with referenced R objects being inserted between curly brackets. The default is pattern = \"{estimate} ({conf.level*100}% CI {conf.low}, {conf.high}; {p.value})\". You have access the to following fields within the pattern argument.\n\n\n\n\n\n\n\n\nParameter\n      Description\n    \n\n\n\n{estimate}\n\nprimary estimate (e.g. model coefficient, odds ratio)\n\n\n\n{conf.low}\n\nlower limit of confidence interval\n\n\n\n{conf.high}\n\nupper limit of confidence interval\n\n\n\n{p.value}\n\np-value\n\n\n\n{conf.level}\n\nconfidence level of interval\n\n\n\n{N}\n\nnumber of observations\n\n\n\n\n\n\n\nAge was not significantly associated with tumor response (OR 1.02; 95% CI 1.00, 1.04; p=0.091). Age was not significantly associated with tumor response (OR 1.02; 95% CI 1.00, 1.04; p=0.091). If you‚Äôre printing results from a categorical variable, include the level argument, e.g. inline_text(tbl_m1, variable = stage, level = \"T3\") resolves to ‚Äú0.94 (95% CI 0.39, 2.28; p=0.9)‚Äù.\n\nThe inline_text function has arguments for rounding the p-value (pvalue_fun) and the coefficients and confidence interval (estimate_fun). These default to the same rounding performed in the table, but can be modified when reporting inline.\nFor more details about inline code, review to the RStudio documentation page."
  },
  {
    "objectID": "content/posts/07-visuals/plot-layout.html",
    "href": "content/posts/07-visuals/plot-layout.html",
    "title": "Plot Layout",
    "section": "",
    "text": "library(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union"
  },
  {
    "objectID": "content/posts/07-visuals/plot-layout.html#figure-layout-ncol",
    "href": "content/posts/07-visuals/plot-layout.html#figure-layout-ncol",
    "title": "Plot Layout",
    "section": "Figure layout, ncol",
    "text": "Figure layout, ncol\n\n```{r}\n#| layout-ncol: 2\n#| fig-cap:\n#|   - \"Speed and Stopping Distances of Cars\"\n#|   - \"Engine displacement and fuel efficiency in Cars\"\n\ncars %>% \n  ggplot(aes(x = speed, y = dist)) +\n  geom_point()\n\nmtcars %>% \n  ggplot(aes(x = disp, y = mpg)) +\n  geom_point()\n```\n\n\n\n\n\nSpeed and Stopping Distances of Cars\n\n\n\n\n\n\nEngine displacement and fuel efficiency in Cars"
  },
  {
    "objectID": "content/posts/07-visuals/plot-layout.html#subcaptions",
    "href": "content/posts/07-visuals/plot-layout.html#subcaptions",
    "title": "Plot Layout",
    "section": "Subcaptions:",
    "text": "Subcaptions:\n\n```{r}\n#| label: fig-charts\n#| fig-cap: Charts\n#| fig-subcap:\n#|   - \"Cars\"\n#|   - \"mtcars\"\n#| layout-ncol: 2\n\ncars %>% \n  ggplot(aes(x = speed, y = dist)) +\n  geom_point()\n\nmtcars %>% \n  ggplot(aes(x = disp, y = mpg)) +\n  geom_point()\n```\n\nFigure¬†1: Charts\n\n\n\n\n(a) Cars\n\n\n\n\n\n\n(b) mtcars"
  },
  {
    "objectID": "content/posts/07-visuals/plot-layout.html#figure-layout-custom",
    "href": "content/posts/07-visuals/plot-layout.html#figure-layout-custom",
    "title": "Plot Layout",
    "section": "Figure layout, custom",
    "text": "Figure layout, custom\n\n```{r}\n#| layout: [[45,-10, 45], [50, 50]]\n#| fig-height: 5\n#| fig-align: center\n#| message: false\n#| warning: false\n#| dpi: 300\n\ncars %>% \n  ggplot(aes(x = speed, y = dist)) +\n  geom_smooth() +\n  theme(text = element_text(size = 20))\n\ncars %>% \n  ggplot(aes(x = speed, y = dist)) +\n  geom_point()+\n  theme(text = element_text(size = 20))\n\nmtcars %>% \n  ggplot(aes(x = disp, y = mpg)) +\n  geom_point()+\n  theme(text = element_text(size = 20))\n\nmtcars %>% \n  ggplot(aes(x = cyl, y = mpg, group = cyl, color = factor(cyl))) +\n  geom_boxplot() +\n  geom_jitter() +\n  theme(legend.position = \"none\")+\n  theme(text = element_text(size = 20))\n```"
  },
  {
    "objectID": "content/posts/07-visuals/plots.html",
    "href": "content/posts/07-visuals/plots.html",
    "title": "Plots",
    "section": "",
    "text": "library(palmerpenguins)\nlibrary(ggplot2)\nggplot2::theme_set(ggplot2::theme_minimal())\npenguins <- na.omit(penguins)"
  },
  {
    "objectID": "content/posts/07-visuals/plots.html#ggplot2",
    "href": "content/posts/07-visuals/plots.html#ggplot2",
    "title": "Plots",
    "section": "ggplot2",
    "text": "ggplot2\nCredit to Alison Hill + Allison Horst\n\n```{r}\nmass_flipper <- ggplot(data = penguins, \n                       aes(x = flipper_length_mm,\n                           y = body_mass_g)) +\n  geom_point(aes(color = species, \n                 shape = species),\n             size = 3,\n             alpha = 0.8) +\n  scale_color_manual(values = c(\"darkorange\",\"purple\",\"cyan4\")) +\n  labs(title = \"Penguin size, Palmer Station LTER\",\n       subtitle = \"Flipper length and body mass for Adelie, Chinstrap and Gentoo Penguins\",\n       x = \"Flipper length (mm)\",\n       y = \"Body mass (g)\",\n       color = \"Penguin species\",\n       shape = \"Penguin species\") +\n  theme(legend.position = c(0.2, 0.7),\n        plot.title.position = \"plot\",\n        plot.caption = element_text(hjust = 0, face= \"italic\"),\n        plot.caption.position = \"plot\",\n        plot.background = element_rect(color = \"black\"))\n```"
  },
  {
    "objectID": "content/posts/07-visuals/plots.html#basic-plot",
    "href": "content/posts/07-visuals/plots.html#basic-plot",
    "title": "Plots",
    "section": "Basic plot",
    "text": "Basic plot\n\n```{r}\nmass_flipper\n```"
  },
  {
    "objectID": "content/posts/07-visuals/plots.html#basic-plot-1",
    "href": "content/posts/07-visuals/plots.html#basic-plot-1",
    "title": "Plots",
    "section": "Basic plot",
    "text": "Basic plot\nIncreasing the width/DPI only affects the scaling of the image, it will not overflow.\n\n```{r}\n#| fig-width: 10\n#| fig-height: 4\n#| fig-dpi: 600\nmass_flipper\n```"
  },
  {
    "objectID": "content/posts/07-visuals/plots.html#aside",
    "href": "content/posts/07-visuals/plots.html#aside",
    "title": "Plots",
    "section": "Aside",
    "text": "Aside\n\n\n\n\n\n\nThe palmerpenguins R package contains two datasets that we believe are a viable alternative to Anderson‚Äôs Iris data (see datasets::iris). In this introductory vignette, we‚Äôll highlight some of the properties of these datasets that make them useful for statistics and data science education, as well as software documentation and testing."
  },
  {
    "objectID": "content/posts/07-visuals/plots.html#overflow-content",
    "href": "content/posts/07-visuals/plots.html#overflow-content",
    "title": "Plots",
    "section": "Overflow Content",
    "text": "Overflow Content\nThere are many options for overflow, either left/right\n\n```{r}\n#| column: body-outset-right\n#| fig-width: 10\nmass_flipper\n```\n\n\n\n\n\n```{r}\n#| column: screen-inset-right\n#| fig-width: 10\nmass_flipper\n```\n\n\n\n\n\n```{r}\n#| column: page-inset-left\n#| fig-width: 10\nmass_flipper\n```\n\n\n\n\n\n```{r}\n#| column: screen-left\n#| fig-width: 10\nmass_flipper\n```"
  },
  {
    "objectID": "content/posts/07-visuals/stat-html.html",
    "href": "content/posts/07-visuals/stat-html.html",
    "title": "gtsummary + R Markdown",
    "section": "",
    "text": "library(gtsummary)\nlibrary(tidyverse)\nlibrary(survival)"
  },
  {
    "objectID": "content/posts/07-visuals/stat-html.html#gtsummary-tables",
    "href": "content/posts/07-visuals/stat-html.html#gtsummary-tables",
    "title": "gtsummary + R Markdown",
    "section": "gtsummary tables",
    "text": "gtsummary tables\nTables created with {gtsummary} can be integrated into R markdown documents. The {gtsummary} package was written to be a companion to the {gt} package from RStudio, and {gtsummary} tables are printed using {gt} when possible. Currently, {gt} supports HTML output, with LaTeX and RTF planned for the future.\n\npatient_characteristics <-\n  trial %>%\n  select(trt, age, grade, response) %>%\n  tbl_summary(by = trt)  \npatient_characteristics\n\n\n\n\n\n\nCharacteristic\n      \nDrug A, N = 981\n\n      \nDrug B, N = 1021\n\n    \n\n\nAge\n46 (37, 59)\n48 (39, 56)\n\n\n¬†¬†¬†¬†Unknown\n7\n4\n\n\nGrade\n\n\n\n\n¬†¬†¬†¬†I\n35 (36%)\n33 (32%)\n\n\n¬†¬†¬†¬†II\n32 (33%)\n36 (35%)\n\n\n¬†¬†¬†¬†III\n31 (32%)\n33 (32%)\n\n\nTumor Response\n28 (29%)\n33 (34%)\n\n\n¬†¬†¬†¬†Unknown\n3\n4\n\n\n\n\n1 Median (IQR); n (%)\n    \n\n\n\n\nWith HTML output, you can include complex tables with footnotes, indentation, and spanning table headers.\n\n# Side-by-side Regression Models\n# logistic regression model\nt1 <-\n  glm(response ~ trt + grade + age, trial, family = binomial) %>%\n  tbl_regression(exponentiate = TRUE)\n# time to death Cox model\nt2 <-\n  coxph(Surv(ttdeath, death) ~ trt + grade + age, trial) %>%\n  tbl_regression(exponentiate = TRUE)\n\n# printing merged table\ntbl_merge(\n  tbls = list(t1, t2),\n  tab_spanner = c(\"**Tumor Response**\", \"**Time to Death**\")\n)\n\n\n\n\n\n\n\nCharacteristic\n      \n        Tumor Response\n      \n      \n        Time to Death\n      \n    \n\n\nOR1\n\n      \n95% CI1\n\n      p-value\n      \nHR1\n\n      \n95% CI1\n\n      p-value\n    \n\n\n\nChemotherapy Treatment\n\n\n\n\n\n\n\n\n¬†¬†¬†¬†Drug A\n‚Äî\n‚Äî\n\n‚Äî\n‚Äî\n\n\n\n¬†¬†¬†¬†Drug B\n1.13\n0.60, 2.13\n0.7\n1.30\n0.88, 1.92\n0.2\n\n\nGrade\n\n\n\n\n\n\n\n\n¬†¬†¬†¬†I\n‚Äî\n‚Äî\n\n‚Äî\n‚Äî\n\n\n\n¬†¬†¬†¬†II\n0.85\n0.39, 1.85\n0.7\n1.21\n0.73, 1.99\n0.5\n\n\n¬†¬†¬†¬†III\n1.01\n0.47, 2.15\n>0.9\n1.79\n1.12, 2.86\n0.014\n\n\nAge\n1.02\n1.00, 1.04\n0.10\n1.01\n0.99, 1.02\n0.3\n\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval, HR = Hazard Ratio"
  },
  {
    "objectID": "content/posts/07-visuals/stat-html.html#inline-reporting",
    "href": "content/posts/07-visuals/stat-html.html#inline-reporting",
    "title": "gtsummary + R Markdown",
    "section": "inline reporting",
    "text": "inline reporting\nAny number/statistic from a {gtsummary} table can be reported inline in a R markdown document using the inline_text() function. See example below:\n\nAmong patients who received Drug A, 31 (32%) had grade III tumors.\n\nFor detailed examples using functions from {gtsummary}, visit the {gtsummary} website."
  },
  {
    "objectID": "content/posts/08-knitr/fa-example.html",
    "href": "content/posts/08-knitr/fa-example.html",
    "title": "Font Awesome Quarto Extension",
    "section": "",
    "text": "This extension allows you to use font-awesome icons in your Quarto HTML and PDF documents. It provides an {{< fa >}} shortcode:\n{{< fa icon-name >}}\nFor example:\n\n\nShortcode\nIcon\n\n\n\n{{< fa thumbs-up >}}\n\n\n\n{{< fa folder >}}\n\n\n\n{{< fa chess-pawn >}}\n\n\n\n{{< fa brands bluetooth >}}"
  },
  {
    "objectID": "content/posts/08-knitr/lightbox-extension.html",
    "href": "content/posts/08-knitr/lightbox-extension.html",
    "title": "Example Lightbox Document",
    "section": "",
    "text": "Here is a simple image with a description. This also overrides the description position and places it to the left of the image.\n\n\nBeach in Chilmark"
  },
  {
    "objectID": "content/posts/08-knitr/lightbox-extension.html#elsewhere",
    "href": "content/posts/08-knitr/lightbox-extension.html#elsewhere",
    "title": "Example Lightbox Document",
    "section": "Elsewhere",
    "text": "Elsewhere\nThe below demonstrates placing more than one image in a gallery. Note the usage of the layout-ncol which arranges the images on the page side by date. Adding the group attribute to the markdown images places the images in a gallery grouped together based upon the group name provided.\n\n\n\n\nAquinnah\n\n\n\n\nOak Bluffs\n\n\n\n\n\n\nVineyard lighthouse"
  },
  {
    "objectID": "content/posts/08-knitr/lightbox-extension.html#credits",
    "href": "content/posts/08-knitr/lightbox-extension.html#credits",
    "title": "Example Lightbox Document",
    "section": "Credits",
    "text": "Credits\nThe images in this example were used under the Unsplash license, view originals below:\n\nChilmark Beach\nAquinnah\nGingerbread House\nEdgartown Light\nEdgartown Sailboat"
  },
  {
    "objectID": "content/posts/08-knitr/penguin-params.html",
    "href": "content/posts/08-knitr/penguin-params.html",
    "title": "Penguins Parametric Reports",
    "section": "",
    "text": "We have data about 344 penguins. Only 193 are classified asAdelie. The distribution of the Adelie penguins are shown below:"
  },
  {
    "objectID": "content/posts/09-using-lordicons/index.html",
    "href": "content/posts/09-using-lordicons/index.html",
    "title": "Using Lordicons, Fontawesome Icons,Academicons, and Iconify Icons",
    "section": "",
    "text": "This is just a compilation of the example files from the Quarto website, so that I can have ready-made code to copy and paste."
  },
  {
    "objectID": "content/posts/09-using-lordicons/index.html#installation",
    "href": "content/posts/09-using-lordicons/index.html#installation",
    "title": "Using Lordicons, Fontawesome Icons,Academicons, and Iconify Icons",
    "section": "Installation",
    "text": "Installation\nType these in your Terminal:\n\nIconify: quarto install extension mcanouil/quarto-iconify\n\nFontAwesome: quarto install extension quarto-ext/fontawesome\n\nLordicons: quarto install extension jmgirard/lordicon\n\nAcademicons: quarto install extension schochastics/academicons\n\n\nThese extensions allows you to use a variety of icons in your Quarto HTML documents."
  },
  {
    "objectID": "content/posts/09-using-lordicons/index.html#using-lordicon-shortcodes",
    "href": "content/posts/09-using-lordicons/index.html#using-lordicon-shortcodes",
    "title": "Using Lordicons, Fontawesome Icons,Academicons, and Iconify Icons",
    "section": "Using Lordicon Shortcodes",
    "text": "Using Lordicon Shortcodes\nThe {{< li >}} shortcode renders an icon (specified by its code) after downloading it the lordicon CDN. The {{< lif >}} shortcode renders an icon (specified by its filepath) from a local .json file. Both shortcodes support the same arguments for customization, described below.\n\n\n\n\n\n\n\nPseudocode\nExample Code\nRendered\n\n\n\n{{< li code >}}\n{{< li wlpxtupd >}}\n\n\n\n{{< lif file >}}\n{{< lif church.json >}}\n\n\n\n\nTriggers\ntrigger controls the icon‚Äôs animation type. When using the loop or loop-on-hover triggers, you can also set an optional delay (in ms) between loops.\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n{{< li wxnxiano >}}\n\n\n\n{{< li wxnxiano trigger=click >}}\n\n\n\n{{< li wxnxiano trigger=hover >}}\n\n\n\n{{< li wxnxiano trigger=loop >}}\n\n\n\n{{< li wxnxiano trigger=loop delay=1000 >}}\n\n\n\n{{< li wxnxiano trigger=loop-on-hover >}}\n\n\n\n{{< li wxnxiano trigger=loop-on-hover delay=1000 >}}\n\n\n\n{{< li wxnxiano trigger=morph >}}\n\n\n\n{{< li wxnxiano trigger=boomerang >}}\n\n\n\nSpeed\nspeed controls how quickly the icon‚Äôs animation plays.\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n{{< li lupuorrc trigger=loop speed=0.5 >}}\n\n\n\n{{< li lupuorrc trigger=loop speed=1.0 >}}\n\n\n\n{{< li lupuorrc trigger=loop speed=2.0 >}}\n\n\n\nColors\ncolors controls the icon‚Äôs coloring. Outline icons typically have just a primary and secondary color, but flat and lineal icons can have many more. Each color should be given in rank:color format (where ranks are primary, secondary, tertiary, etc.) and multiple colors should be separated by commas. Colors can be given in HTML color names or hexcodes.\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n{{< li lupuorrc >}}\n\n\n\n{{< li lupuorrc colors=primary:gold >}}\n\n\n\n{{< li lupuorrc colors=primary:gray,secondary:orange >}}\n\n\n\n{{< li lupuorrc colors=primary:#4030e8,secondary:#ee66aa >}}\n\n\n\nStroke\nstroke controls how thick the lines in an icon are.\n\n\nShortcode\nIcon\n\n\n\n{{< li lupuorrc stroke=50 >}}\n\n\n\n{{< li lupuorrc stroke=100 >}}\n\n\n\n{{< li lupuorrc stroke=150 >}}\n\n\n\nScale\nscale controls how large or zoomed in the icon is.\n\n\nShortcode\nIcon\n\n\n\n{{< li lupuorrc scale=25 >}}\n\n\n\n{{< li lupuorrc scale=50 >}}\n\n\n\n{{< li lupuorrc scale=100 >}}\n\n\n\nAxis X\nx controls the horizontal position of the center of the icon.\n\n\nShortcode\nIcon\n\n\n\n{{< li lupuorrc x=25 >}}\n\n\n\n{{< li lupuorrc x=50 >}}\n\n\n\n{{< li lupuorrc x=100 >}}\n\n\n\nAxis Y\ny controls the vertical position of the center of the icon.\n\n\nShortcode\nIcon\n\n\n\n{{< li lupuorrc y=25 >}}\n\n\n\n{{< li lupuorrc y=50 >}}\n\n\n\n{{< li lupuorrc y=100 >}}"
  },
  {
    "objectID": "content/posts/09-using-lordicons/index.html#using-academicons-shortcodes",
    "href": "content/posts/09-using-lordicons/index.html#using-academicons-shortcodes",
    "title": "Using Lordicons, Fontawesome Icons,Academicons, and Iconify Icons",
    "section": "Using Academicons Shortcodes",
    "text": "Using Academicons Shortcodes\nThis extension allows you to use academicons in your Quarto HTML documents. It provides an {{< ai >}} shortcode:\n\n\nMandatory <icon-name>:\n{{< ai <icon-name> >}}\n\n\nOptional <size=...>:\n{{< ai <icon-name> <size=...> >}}\n\n\nFor example:\n\n\nShortcode\nIcon\n\n\n\n{{< ai arxiv >}}\n\n\n\n{{< ai google-scholar >}}\n\n\n\n{{< ai open-access >}}\n\n\n\n{{< ai open-access size=5x >}}"
  },
  {
    "objectID": "content/posts/09-using-lordicons/index.html#using-fontawesome-icons",
    "href": "content/posts/09-using-lordicons/index.html#using-fontawesome-icons",
    "title": "Using Lordicons, Fontawesome Icons,Academicons, and Iconify Icons",
    "section": "Using Fontawesome Icons",
    "text": "Using Fontawesome Icons\nThis extension allows you to use font-awesome icons in your Quarto HTML and PDF documents. It provides an {{< fa >}} shortcode:\n\n\nMandatory <icon-name>:\n{{< fa <icon-name> >}}\n\n\nOptional <group>, <size=...>, and <title=...>:\n{{< fa <group> <icon-name> <size=...> <title=...> >}}\n\n\nFor example:\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n{{< fa thumbs-up >}}\n\n\n\n{{< fa folder >}}\n\n\n\n{{< fa chess-pawn >}}\n\n\n\n{{< fa brands bluetooth >}}\n\n\n\n\n{{< fa brands twitter size=2xl >}} (HTML only)\n\n\n\n\n{{< fa brands github size=5x >}} (HTML only)\n\n\n\n{{< fa battery-half size=Huge >}}\n\n\n\n{{< fa envelope title=\"An envelope\" >}}"
  },
  {
    "objectID": "content/posts/09-using-lordicons/index.html#using-iconify-shortcodes",
    "href": "content/posts/09-using-lordicons/index.html#using-iconify-shortcodes",
    "title": "Using Lordicons, Fontawesome Icons,Academicons, and Iconify Icons",
    "section": "Using Iconify Shortcodes",
    "text": "Using Iconify Shortcodes\nThis extension allows you to use Iconify icons in your Quarto HTML documents. It provides an {{< iconify >}} shortcode:\n\n\nMandatory <icon-name>:\n{{< iconify <icon-name> >}}\n\n\nOptional <set> (default is fluent-emoji) <size=...>, <width=...>, <height=...>, <flip=...>, and <rotate=...>:\n{{< iconify <set> <icon-name> <size=...> <width=...> <height=...> <flip=...> <rotate=...> >}}\nIf <size=...> is defined, <width=...> and <height=...> are not used.\nSee https://docs.iconify.design/iconify-icon/ for more details.\n\n\nFor example:\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n{{< iconify exploding-head >}}\n\n\n\n{{< iconify exploding-head size=2xl >}}\n\n\n\n{{< iconify exploding-head size=5x rotate=180deg >}}\n\n\n\n{{< iconify exploding-head size=Huge >}}\n\n\n\n{{< iconify fluent-emoji-high-contrast 1st-place-medal >}}\n\n\n\n{{< iconify twemoji 1st-place-medal >}}\n\n\n\n{{< iconify line-md loading-alt-loop >}}\n\n\n\n{{< iconify fa6-brands apple width=50px height=10px rotate=90deg flip=vertical >}}"
  },
  {
    "objectID": "content/posts/10-using-nutshell/nutshell.html",
    "href": "content/posts/10-using-nutshell/nutshell.html",
    "title": "Nutshell: Expandable Explanations",
    "section": "",
    "text": "Nutshell is a tool to make ‚Äúexpandable explanations‚Äù, like this! This lets your readers learn only the details they need, just-in-time, always-in-context.\nUnlike links, Nutshell lets you include only the snippet you need, not the whole page. Plus, instead of being lost in a jungle of new tabs, your reader stays on one page, keeping their flow of reading. Even if you interrupt a sentence, Nutshell recaps the sentence afterwards, so your reader never loses context.\nYou can find more information on the nutshell webpage and here is a live demo !"
  },
  {
    "objectID": "content/posts/10-using-nutshell/nutshell.html#test",
    "href": "content/posts/10-using-nutshell/nutshell.html#test",
    "title": "Nutshell: Expandable Explanations",
    "section": "Test",
    "text": "Test\nThis is a senseless paragraph"
  },
  {
    "objectID": "content/posts/10-using-nutshell/nutshell.html#testing-links",
    "href": "content/posts/10-using-nutshell/nutshell.html#testing-links",
    "title": "Nutshell: Expandable Explanations",
    "section": "Testing Links",
    "text": "Testing Links\n\n:link to senseless paragraph\n:link to wikipedia article\n:link to invisible sections"
  },
  {
    "objectID": "content/posts/10-using-nutshell/nutshell.html#x-invisible",
    "href": "content/posts/10-using-nutshell/nutshell.html#x-invisible",
    "title": "Nutshell: Expandable Explanations",
    "section": ":x invisible",
    "text": ":x invisible\nUse ## :x header to include an invisible section that can be linked to via nutshell"
  },
  {
    "objectID": "content/posts/12-tufte-style-article/index.html",
    "href": "content/posts/12-tufte-style-article/index.html",
    "title": "A Tufte Handout Example",
    "section": "",
    "text": "The Tufte handout style is a style that Edward Tufte uses in his books and handouts. Tufte‚Äôs style is known for its extensive use of sidenotes, tight integration of graphics with text, and well-set typography. This style has been implemented in LaTeX and HTML/CSS1, respectively. We have ported both implementations into the tufte package. If you want LaTeX/PDF output, you may use the tufte_handout format for handouts, and tufte_book for books. For HTML output, use tufte_html. These formats can be either specified in the YAML metadata at the beginning of an R Markdown document (see an example below), or passed to the rmarkdown::render() function. See Allaire et al. (2022) for more information about rmarkdown.1¬†See Github repositories tufte-latex and tufte-css\n---\ntitle: \"An Example Using the Tufte Style\"\nauthor: \"John Smith\"\noutput:\n  tufte::tufte_handout: default\n  tufte::tufte_html: default\n---\nThere are two goals of this package:\n\nTo produce both PDF and HTML output with similar styles from the same R Markdown document;\nTo provide simple syntax to write elements of the Tufte style such as side notes and margin figures, e.g.¬†when you want a margin figure, all you need to do is the chunk option fig.margin = TRUE, and we will take care of the details for you, so you never need to think about \\begin{marginfigure} \\end{marginfigure} or <span class=\"marginfigure\"> </span>; the LaTeX and HTML code under the hood may be complicated, but you never need to learn or write such code.\n\nIf you have any feature requests or find bugs in tufte, please do not hesitate to file them to https://github.com/rstudio/tufte/issues. For general questions, you may ask them on StackOverflow: https://stackoverflow.com/tags/rmarkdown."
  },
  {
    "objectID": "content/posts/12-tufte-style-article/index.html#margin-figures",
    "href": "content/posts/12-tufte-style-article/index.html#margin-figures",
    "title": "A Tufte Handout Example",
    "section": "Margin Figures",
    "text": "Margin Figures\nImages and graphics play an integral role in Tufte‚Äôs work. To place figures in the margin you can use the knitr chunk option fig.margin = TRUE. For example:\n\nlibrary(ggplot2)\nmtcars2 <- mtcars\nmtcars2$am <- factor(\n  mtcars$am, labels = c('automatic', 'manual')\n)\nggplot(mtcars2, aes(hp, mpg, color = am)) +\n  geom_point() + geom_smooth() +\n  theme(legend.position = 'bottom')\n\n\n\n\nFigure¬†1: MPG vs horsepower, colored by transmission.\n\n\n\nNote the use of the fig.cap chunk option to provide a figure caption. You can adjust the proportions of figures using the fig.width and fig.height chunk options. These are specified in inches, and will be automatically scaled down to fit within the handout margin."
  },
  {
    "objectID": "content/posts/12-tufte-style-article/index.html#arbitrary-margin-content",
    "href": "content/posts/12-tufte-style-article/index.html#arbitrary-margin-content",
    "title": "A Tufte Handout Example",
    "section": "Arbitrary Margin Content",
    "text": "Arbitrary Margin Content\nIn fact, you can include anything in the margin using the knitr engine named marginfigure. Unlike R code chunks ```{r}, you write a chunk starting with ```{marginfigure} instead, then put the content in the chunk. See an example on the right about the first fundamental theorem of calculus.\n\nWe know from _the first fundamental theorem of calculus_ that for $x$ in $[a, b]$:\n$$\\frac{d}{dx}\\left( \\int_{a}^{x} f(u)\\,du\\right)=f(x).$$\n\nFor the sake of portability between LaTeX and HTML, you should keep the margin content as simple as possible (syntax-wise) in the marginefigure blocks. You may use simple Markdown syntax like **bold** and _italic_ text, but please refrain from using footnotes, citations, or block-level elements (e.g.¬†blockquotes and lists) there.\nNote: if you set echo = FALSE in your global chunk options, you will have to add echo = TRUE to the chunk to display a margin figure, for example ```{marginfigure, echo = TRUE}."
  },
  {
    "objectID": "content/posts/12-tufte-style-article/index.html#full-width-figures",
    "href": "content/posts/12-tufte-style-article/index.html#full-width-figures",
    "title": "A Tufte Handout Example",
    "section": "Full Width Figures",
    "text": "Full Width Figures\nYou can arrange for figures to span across the entire page by using the chunk option fig.fullwidth = TRUE.\n\nggplot(diamonds, aes(carat, price)) + geom_smooth() +\n  facet_grid(~ cut)\n\n\n\nFigure¬†2: A full width figure.\n\n\n\n\nOther chunk options related to figures can still be used, such as fig.width, fig.cap, out.width, and so on. For full width figures, usually fig.width is large and fig.height is small. In the above example, the plot size is \\(10 \\times 2\\)."
  },
  {
    "objectID": "content/posts/12-tufte-style-article/index.html#arbitrary-full-width-content",
    "href": "content/posts/12-tufte-style-article/index.html#arbitrary-full-width-content",
    "title": "A Tufte Handout Example",
    "section": "Arbitrary Full Width Content",
    "text": "Arbitrary Full Width Content\nAny content can span to the full width of the page. This feature requires Pandoc 2.0 or above. All you need is to put your content in a fenced Div with the class fullwidth, e.g.,\n::: {.fullwidth}\nAny _full width_ content here.\n:::\nBelow is an example:\n\nR is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under the terms of the GNU General Public License versions 2 or 3. For more information about these matters see https://www.gnu.org/licenses/."
  },
  {
    "objectID": "content/posts/12-tufte-style-article/index.html#main-column-figures",
    "href": "content/posts/12-tufte-style-article/index.html#main-column-figures",
    "title": "A Tufte Handout Example",
    "section": "Main Column Figures",
    "text": "Main Column Figures\nBesides margin and full width figures, you can of course also include figures constrained to the main column. This is the default type of figures in the LaTeX/HTML output.\n\nggplot(diamonds, aes(cut, price)) + geom_boxplot()\n\n\n\nFigure¬†3: A figure in the main column."
  },
  {
    "objectID": "content/posts/13-rapp-sample-document/index.html",
    "href": "content/posts/13-rapp-sample-document/index.html",
    "title": "Sample Blog Post Template",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\nThis is inline code plus a small code chunk.\n\nlibrary(tidyverse)\n\nggplot(mpg) +\n  geom_jitter(aes(cty, hwy), size = 4, alpha = 0.5) \n\n\n\n\n\n\n\n\n\n\nTransforming OLS estimates\nMaximizing likelihood\n\n\n\n\nShow the Codepreds_lm %>% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  theme_minimal(base_size = 12) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\nShow the Codeglm.mod <- glm(sex ~ body_mass_g + bill_length_mm + species, family = binomial, data = dat)\n\npreds <- dat %>% \n  mutate(\n    prob.fit = glm.mod$fitted.values,\n    prediction = if_else(prob.fit > 0.5, 'male', 'female'),\n    correct = if_else(sex == prediction, 'correct', 'incorrect')\n  )\n\n\npreds %>% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  theme_minimal(base_size = 10) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\\[\n\\int_0^1 f(x) \\ dx\n\\]\n\n\n\n\ngeom_density(\n  mapping = NULL,\n  data = NULL,\n  stat = \"density\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  outline.type = \"upper\"\n)\n\n\nstat_density(\n  mapping = NULL,\n  data = NULL,\n  geom = \"area\",\n  position = \"stack\",\n  ...,\n  bw = \"nrd0\",\n  adjust = 1,\n  kernel = \"gaussian\",\n  n = 512,\n  trim = FALSE,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\n\n\n\n\nggplot(data = gapminder::gapminder, mapping = aes(x = lifeExp, fill = continent)) +\n  stat_density(position = \"identity\", alpha = 0.5)\n\n\n\n\nBla bla bla. This is a caption in the margin. Super cool isn‚Äôt it?"
  },
  {
    "objectID": "content/posts/listing.html",
    "href": "content/posts/listing.html",
    "title": "The Foundation Series",
    "section": "",
    "text": "A Tufte Handout Example\n\n\n\n\n\n\nJJ Allaire and Yihui Xie\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\nBootswatch Themed QMD\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoston Terrier\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\nCallout Boxes\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\nCallouts in PDF\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample Lightbox Document\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure Layout\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure Layouts\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure Layouts-2\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\nFont Awesome Quarto Extension\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\nHousing Prices\n\n\n\n\n\n\nArvind Venkatadri\n\n\nJan 8, 2023\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\nInline Code\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\nNutshell: Expandable Explanations\n\n\n\n\n\n\nDavid Schoch\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\nOld Markdown - reactable example\n\n\n\n\n\n\nTom Mock\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\nOverview of using gtsummary\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPenguin Report Presentation\n\n\n\n\n\n\nTom Mock\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPenguins Distilled\n\n\nA great new article on Penguins\n\n\n\nTom Mock\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\nPenguins Parametric Reports\n\n\n\n\n\n\n\n\n\nJul 13, 2022\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\nPlot Layout\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\nPlots\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\nPresentation with Columns?\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\nSample Blog Post Template\n\n\nDescription: This is a test post. In this post, I try out different functionalities\n\n\n\nArvind Venkatadri\n\n\nJun 1, 2022\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\nUsing Lordicons, Fontawesome Icons,Academicons, and Iconify Icons\n\n\n\n\n\n\nArvind Venkatadri\n\n\nJan 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisual Editor for Quarto\n\n\n\n\n\n\nThomas Mock\n\n\nInvalid Date\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\ngtsummary + R Markdown\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "",
    "text": "This is an attempt to rework , using tidygraph and ggraph, much of Network Visualization with R Polnet 2018 Workshop Tutorial, Washington, DC by Prof.¬†Katherine (Katya) Ognyanova.\nThe aim is to get a working acquaintance with both these packages and also to appreciate some of the concepts in Networks. My code is by no means intended to be elegant; it merely works and there are surely many improvements that people may think of!\nI have attempted to write code for the Sections 2:5.\nI have retained Prof.¬†Ognyanova‚Äôs text in all places."
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#contents",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#contents",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "CONTENTS",
    "text": "CONTENTS\n\nWorking with colors in R plots\nReading in the network data\nNetwork plots in ‚Äòigraph‚Äô\nPlotting two-mode networks\nPlotting multiplex networks\nQuick example using ‚Äònetwork‚Äô\nSimple plot animations in R\nInteractive JavaScript networks\nInteractive and dynamic networks with ndtv-d3\nPlotting networks on a geographic map"
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#dataset-1-edgelist--",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#dataset-1-edgelist--",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "‚Äî‚Äî-~~ DATASET 1: edgelist ~~‚Äî‚Äî-",
    "text": "‚Äî‚Äî-~~ DATASET 1: edgelist ~~‚Äî‚Äî-\n\n# Read in the data:\nnodes <- read.csv(\"./Dataset1-Media-Example-NODES.csv\", header = T, as.is = T)\nlinks <- read.csv(\"./Dataset1-Media-Example-EDGES.csv\", header = T, as.is = T)\n\n\n# Examine the data:\nhead(nodes)\n\n   id               media media.type type.label audience.size\n1 s01            NY Times          1  Newspaper            20\n2 s02     Washington Post          1  Newspaper            25\n3 s03 Wall Street Journal          1  Newspaper            30\n4 s04           USA Today          1  Newspaper            32\n5 s05            LA Times          1  Newspaper            20\n6 s06       New York Post          1  Newspaper            50\n\nhead(links)\n\n  from  to      type weight\n1  s01 s02 hyperlink     22\n2  s01 s03 hyperlink     22\n3  s01 s04 hyperlink     21\n4  s01 s15   mention     20\n5  s02 s01 hyperlink     23\n6  s02 s03 hyperlink     21\n\n\nConverting the data to an igraph object:\nThe graph_from_data_frame() function takes two data frames: ‚Äòd‚Äô and ‚Äòvertices‚Äô. - ‚Äòd‚Äô describes the edges of the network - it should start with two columns containing the source and target node IDs for each network tie. - ‚Äòvertices‚Äô should start with a column of node IDs. It can be omitted. - Any additional columns in either data frame are interpreted as attributes.\nNOTE: ID columns need not be numbers or integers!!\n\nnet <- graph_from_data_frame(d = links, vertices = nodes, directed = T)\n\n# Examine the resulting object:\nclass(net)\n\n[1] \"igraph\"\n\nnet\n\nIGRAPH 8d2aa7a DNW- 17 49 -- \n+ attr: name (v/c), media (v/c), media.type (v/n), type.label (v/c),\n| audience.size (v/n), type (e/c), weight (e/n)\n+ edges from 8d2aa7a (vertex names):\n [1] s01->s02 s01->s03 s01->s04 s01->s15 s02->s01 s02->s03 s02->s09 s02->s10\n [9] s03->s01 s03->s04 s03->s05 s03->s08 s03->s10 s03->s11 s03->s12 s04->s03\n[17] s04->s06 s04->s11 s04->s12 s04->s17 s05->s01 s05->s02 s05->s09 s05->s15\n[25] s06->s06 s06->s16 s06->s17 s07->s03 s07->s08 s07->s10 s07->s14 s08->s03\n[33] s08->s07 s08->s09 s09->s10 s10->s03 s12->s06 s12->s13 s12->s14 s13->s12\n[41] s13->s17 s14->s11 s14->s13 s15->s01 s15->s04 s15->s06 s16->s06 s16->s17\n[49] s17->s04\n\n\nThe description of an igraph object starts with four letters: -D or U, for a directed or undirected graph -N for a named graph (where nodes have a name attribute) -W for a weighted graph (where edges have a weight attribute) -B for a bipartite (two-mode) graph (where nodes have a type attribute) The two numbers that follow (17 49) refer to the number of nodes and edges in the graph. The description also lists node & edge attributes.\nWe can access the nodes, edges, and their attributes:\n\nE(net)\n\n+ 49/49 edges from 8d2aa7a (vertex names):\n [1] s01->s02 s01->s03 s01->s04 s01->s15 s02->s01 s02->s03 s02->s09 s02->s10\n [9] s03->s01 s03->s04 s03->s05 s03->s08 s03->s10 s03->s11 s03->s12 s04->s03\n[17] s04->s06 s04->s11 s04->s12 s04->s17 s05->s01 s05->s02 s05->s09 s05->s15\n[25] s06->s06 s06->s16 s06->s17 s07->s03 s07->s08 s07->s10 s07->s14 s08->s03\n[33] s08->s07 s08->s09 s09->s10 s10->s03 s12->s06 s12->s13 s12->s14 s13->s12\n[41] s13->s17 s14->s11 s14->s13 s15->s01 s15->s04 s15->s06 s16->s06 s16->s17\n[49] s17->s04\n\nV(net)\n\n+ 17/17 vertices, named, from 8d2aa7a:\n [1] s01 s02 s03 s04 s05 s06 s07 s08 s09 s10 s11 s12 s13 s14 s15 s16 s17\n\nE(net)$type\n\n [1] \"hyperlink\" \"hyperlink\" \"hyperlink\" \"mention\"   \"hyperlink\" \"hyperlink\"\n [7] \"hyperlink\" \"hyperlink\" \"hyperlink\" \"hyperlink\" \"hyperlink\" \"hyperlink\"\n[13] \"mention\"   \"hyperlink\" \"hyperlink\" \"hyperlink\" \"mention\"   \"mention\"  \n[19] \"hyperlink\" \"mention\"   \"mention\"   \"hyperlink\" \"hyperlink\" \"mention\"  \n[25] \"hyperlink\" \"hyperlink\" \"mention\"   \"mention\"   \"mention\"   \"hyperlink\"\n[31] \"mention\"   \"hyperlink\" \"mention\"   \"mention\"   \"mention\"   \"hyperlink\"\n[37] \"mention\"   \"hyperlink\" \"mention\"   \"hyperlink\" \"mention\"   \"mention\"  \n[43] \"mention\"   \"hyperlink\" \"hyperlink\" \"hyperlink\" \"hyperlink\" \"mention\"  \n[49] \"hyperlink\"\n\nV(net)$media\n\n [1] \"NY Times\"            \"Washington Post\"     \"Wall Street Journal\"\n [4] \"USA Today\"           \"LA Times\"            \"New York Post\"      \n [7] \"CNN\"                 \"MSNBC\"               \"FOX News\"           \n[10] \"ABC\"                 \"BBC\"                 \"Yahoo News\"         \n[13] \"Google News\"         \"Reuters.com\"         \"NYTimes.com\"        \n[16] \"WashingtonPost.com\"  \"AOL.com\"            \n\n\n\n# Using tidygraph\ntbl_graph(nodes, links, directed = TRUE) %>% \n  activate(edges) %>% \n  select(type)\n\n# A tbl_graph: 17 nodes and 49 edges\n#\n# A directed multigraph with 1 component\n#\n# Edge Data: 49 √ó 3 (active)\n   from    to type     \n  <int> <int> <chr>    \n1     1     2 hyperlink\n2     1     3 hyperlink\n3     1     4 hyperlink\n4     1    15 mention  \n5     2     1 hyperlink\n6     2     3 hyperlink\n# ‚Ä¶ with 43 more rows\n#\n# Node Data: 17 √ó 5\n  id    media               media.type type.label audience.size\n  <chr> <chr>                    <int> <chr>              <int>\n1 s01   NY Times                     1 Newspaper             20\n2 s02   Washington Post              1 Newspaper             25\n3 s03   Wall Street Journal          1 Newspaper             30\n# ‚Ä¶ with 14 more rows\n\ntbl_graph(nodes, links, directed = TRUE) %>% \n  activate(nodes) %>% \n  select(media)\n\n# A tbl_graph: 17 nodes and 49 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 17 √ó 1 (active)\n  media              \n  <chr>              \n1 NY Times           \n2 Washington Post    \n3 Wall Street Journal\n4 USA Today          \n5 LA Times           \n6 New York Post      \n# ‚Ä¶ with 11 more rows\n#\n# Edge Data: 49 √ó 4\n   from    to type      weight\n  <int> <int> <chr>      <int>\n1     1     2 hyperlink     22\n2     1     3 hyperlink     22\n3     1     4 hyperlink     21\n# ‚Ä¶ with 46 more rows\n\n\nOr find specific nodes and edges by attribute:(that returns objects of type vertex sequence / edge sequence)\n\nV(net)[media == \"BBC\"]\n\n+ 1/17 vertex, named, from 8d2aa7a:\n[1] s11\n\nE(net)[type == \"mention\"]\n\n+ 20/49 edges from 8d2aa7a (vertex names):\n [1] s01->s15 s03->s10 s04->s06 s04->s11 s04->s17 s05->s01 s05->s15 s06->s17\n [9] s07->s03 s07->s08 s07->s14 s08->s07 s08->s09 s09->s10 s12->s06 s12->s14\n[17] s13->s17 s14->s11 s14->s13 s16->s17\n\n\n\n#Using tidygraph\ntbl_graph(nodes, links, directed = TRUE) %>% \n  activate(nodes) %>% \n  filter(media == \"BBC\")\n\n# A tbl_graph: 1 nodes and 0 edges\n#\n# A rooted tree\n#\n# Node Data: 1 √ó 5 (active)\n  id    media media.type type.label audience.size\n  <chr> <chr>      <int> <chr>              <int>\n1 s11   BBC            2 TV                    34\n#\n# Edge Data: 0 √ó 4\n# ‚Ä¶ with 4 variables: from <int>, to <int>, type <chr>, weight <int>\n\ntbl_graph(nodes, links, directed = TRUE) %>% \n  activate(edges) %>% \n  filter(type == \"mention\")\n\n# A tbl_graph: 17 nodes and 20 edges\n#\n# A directed simple graph with 3 components\n#\n# Edge Data: 20 √ó 4 (active)\n   from    to type    weight\n  <int> <int> <chr>    <int>\n1     1    15 mention     20\n2     3    10 mention      2\n3     4     6 mention      1\n4     4    11 mention     22\n5     4    17 mention      2\n6     5     1 mention      1\n# ‚Ä¶ with 14 more rows\n#\n# Node Data: 17 √ó 5\n  id    media               media.type type.label audience.size\n  <chr> <chr>                    <int> <chr>              <int>\n1 s01   NY Times                     1 Newspaper             20\n2 s02   Washington Post              1 Newspaper             25\n3 s03   Wall Street Journal          1 Newspaper             30\n# ‚Ä¶ with 14 more rows\n\n\nIf you need them, you can extract an edge list or a matrix back from the igraph networks.\n\nas_edgelist(net, names = T)\n\n      [,1]  [,2] \n [1,] \"s01\" \"s02\"\n [2,] \"s01\" \"s03\"\n [3,] \"s01\" \"s04\"\n [4,] \"s01\" \"s15\"\n [5,] \"s02\" \"s01\"\n [6,] \"s02\" \"s03\"\n [7,] \"s02\" \"s09\"\n [8,] \"s02\" \"s10\"\n [9,] \"s03\" \"s01\"\n[10,] \"s03\" \"s04\"\n[11,] \"s03\" \"s05\"\n[12,] \"s03\" \"s08\"\n[13,] \"s03\" \"s10\"\n[14,] \"s03\" \"s11\"\n[15,] \"s03\" \"s12\"\n[16,] \"s04\" \"s03\"\n[17,] \"s04\" \"s06\"\n[18,] \"s04\" \"s11\"\n[19,] \"s04\" \"s12\"\n[20,] \"s04\" \"s17\"\n[21,] \"s05\" \"s01\"\n[22,] \"s05\" \"s02\"\n[23,] \"s05\" \"s09\"\n[24,] \"s05\" \"s15\"\n[25,] \"s06\" \"s06\"\n[26,] \"s06\" \"s16\"\n[27,] \"s06\" \"s17\"\n[28,] \"s07\" \"s03\"\n[29,] \"s07\" \"s08\"\n[30,] \"s07\" \"s10\"\n[31,] \"s07\" \"s14\"\n[32,] \"s08\" \"s03\"\n[33,] \"s08\" \"s07\"\n[34,] \"s08\" \"s09\"\n[35,] \"s09\" \"s10\"\n[36,] \"s10\" \"s03\"\n[37,] \"s12\" \"s06\"\n[38,] \"s12\" \"s13\"\n[39,] \"s12\" \"s14\"\n[40,] \"s13\" \"s12\"\n[41,] \"s13\" \"s17\"\n[42,] \"s14\" \"s11\"\n[43,] \"s14\" \"s13\"\n[44,] \"s15\" \"s01\"\n[45,] \"s15\" \"s04\"\n[46,] \"s15\" \"s06\"\n[47,] \"s16\" \"s06\"\n[48,] \"s16\" \"s17\"\n[49,] \"s17\" \"s04\"\n\nas_adjacency_matrix(net, attr = \"weight\")\n\n17 x 17 sparse Matrix of class \"dgCMatrix\"\n\n\n  [[ suppressing 17 column names 's01', 's02', 's03' ... ]]\n\n\n                                                     \ns01  . 22 22 21 .  .  .  .  .  .  .  .  .  . 20  .  .\ns02 23  . 21  . .  .  .  .  1  5  .  .  .  .  .  .  .\ns03 21  .  . 22 1  .  .  4  .  2  1  1  .  .  .  .  .\ns04  .  . 23  . .  1  .  .  .  . 22  3  .  .  .  .  2\ns05  1 21  .  . .  .  .  .  2  .  .  .  .  . 21  .  .\ns06  .  .  .  . .  1  .  .  .  .  .  .  .  .  . 21 21\ns07  .  .  1  . .  .  . 22  . 21  .  .  .  4  .  .  .\ns08  .  .  2  . .  . 21  . 23  .  .  .  .  .  .  .  .\ns09  .  .  .  . .  .  .  .  . 21  .  .  .  .  .  .  .\ns10  .  .  2  . .  .  .  .  .  .  .  .  .  .  .  .  .\ns11  .  .  .  . .  .  .  .  .  .  .  .  .  .  .  .  .\ns12  .  .  .  . .  2  .  .  .  .  .  . 22 22  .  .  .\ns13  .  .  .  . .  .  .  .  .  .  . 21  .  .  .  .  1\ns14  .  .  .  . .  .  .  .  .  .  1  . 21  .  .  .  .\ns15 22  .  .  1 .  4  .  .  .  .  .  .  .  .  .  .  .\ns16  .  .  .  . . 23  .  .  .  .  .  .  .  .  .  . 21\ns17  .  .  .  4 .  .  .  .  .  .  .  .  .  .  .  .  .\n\n# Using tidygraph\n# No direct command seems available ...\n\n\n# Or data frames describing nodes and edges:\nigraph::as_data_frame(x = net, what = \"edges\")\n\n   from  to      type weight\n1   s01 s02 hyperlink     22\n2   s01 s03 hyperlink     22\n3   s01 s04 hyperlink     21\n4   s01 s15   mention     20\n5   s02 s01 hyperlink     23\n6   s02 s03 hyperlink     21\n7   s02 s09 hyperlink      1\n8   s02 s10 hyperlink      5\n9   s03 s01 hyperlink     21\n10  s03 s04 hyperlink     22\n11  s03 s05 hyperlink      1\n12  s03 s08 hyperlink      4\n13  s03 s10   mention      2\n14  s03 s11 hyperlink      1\n15  s03 s12 hyperlink      1\n16  s04 s03 hyperlink     23\n17  s04 s06   mention      1\n18  s04 s11   mention     22\n19  s04 s12 hyperlink      3\n20  s04 s17   mention      2\n21  s05 s01   mention      1\n22  s05 s02 hyperlink     21\n23  s05 s09 hyperlink      2\n24  s05 s15   mention     21\n25  s06 s06 hyperlink      1\n26  s06 s16 hyperlink     21\n27  s06 s17   mention     21\n28  s07 s03   mention      1\n29  s07 s08   mention     22\n30  s07 s10 hyperlink     21\n31  s07 s14   mention      4\n32  s08 s03 hyperlink      2\n33  s08 s07   mention     21\n34  s08 s09   mention     23\n35  s09 s10   mention     21\n36  s10 s03 hyperlink      2\n37  s12 s06   mention      2\n38  s12 s13 hyperlink     22\n39  s12 s14   mention     22\n40  s13 s12 hyperlink     21\n41  s13 s17   mention      1\n42  s14 s11   mention      1\n43  s14 s13   mention     21\n44  s15 s01 hyperlink     22\n45  s15 s04 hyperlink      1\n46  s15 s06 hyperlink      4\n47  s16 s06 hyperlink     23\n48  s16 s17   mention     21\n49  s17 s04 hyperlink      4\n\nigraph::as_data_frame(x = net, what = \"vertices\")\n\n    name               media media.type type.label audience.size\ns01  s01            NY Times          1  Newspaper            20\ns02  s02     Washington Post          1  Newspaper            25\ns03  s03 Wall Street Journal          1  Newspaper            30\ns04  s04           USA Today          1  Newspaper            32\ns05  s05            LA Times          1  Newspaper            20\ns06  s06       New York Post          1  Newspaper            50\ns07  s07                 CNN          2         TV            56\ns08  s08               MSNBC          2         TV            34\ns09  s09            FOX News          2         TV            60\ns10  s10                 ABC          2         TV            23\ns11  s11                 BBC          2         TV            34\ns12  s12          Yahoo News          3     Online            33\ns13  s13         Google News          3     Online            23\ns14  s14         Reuters.com          3     Online            12\ns15  s15         NYTimes.com          3     Online            24\ns16  s16  WashingtonPost.com          3     Online            28\ns17  s17             AOL.com          3     Online            33\n\n#Using tidygraph\ntbl_graph(nodes, links, directed = TRUE) %>% \n  activate(nodes) %>% \n  as_tibble()\n\n# A tibble: 17 √ó 5\n   id    media               media.type type.label audience.size\n   <chr> <chr>                    <int> <chr>              <int>\n 1 s01   NY Times                     1 Newspaper             20\n 2 s02   Washington Post              1 Newspaper             25\n 3 s03   Wall Street Journal          1 Newspaper             30\n 4 s04   USA Today                    1 Newspaper             32\n 5 s05   LA Times                     1 Newspaper             20\n 6 s06   New York Post                1 Newspaper             50\n 7 s07   CNN                          2 TV                    56\n 8 s08   MSNBC                        2 TV                    34\n 9 s09   FOX News                     2 TV                    60\n10 s10   ABC                          2 TV                    23\n11 s11   BBC                          2 TV                    34\n12 s12   Yahoo News                   3 Online                33\n13 s13   Google News                  3 Online                23\n14 s14   Reuters.com                  3 Online                12\n15 s15   NYTimes.com                  3 Online                24\n16 s16   WashingtonPost.com           3 Online                28\n17 s17   AOL.com                      3 Online                33\n\ntbl_graph(nodes, links, directed = TRUE)%>% \n  activate(edges) %>% \n  as_tibble()\n\n# A tibble: 49 √ó 4\n    from    to type      weight\n   <int> <int> <chr>      <int>\n 1     1     2 hyperlink     22\n 2     1     3 hyperlink     22\n 3     1     4 hyperlink     21\n 4     1    15 mention       20\n 5     2     1 hyperlink     23\n 6     2     3 hyperlink     21\n 7     2     9 hyperlink      1\n 8     2    10 hyperlink      5\n 9     3     1 hyperlink     21\n10     3     4 hyperlink     22\n# ‚Ä¶ with 39 more rows\n\n\n\n# You can also access the network matrix directly:\nnet[1,]\n\ns01 s02 s03 s04 s05 s06 s07 s08 s09 s10 s11 s12 s13 s14 s15 s16 s17 \n  0  22  22  21   0   0   0   0   0   0   0   0   0   0  20   0   0 \n\nnet[5,7]\n\n[1] 0\n\n# Using tidygraph\n# Does not seem possible, even with `as.matrix()`.\n# Returns tibbles only as in the code chunk above\n\n\n# First attempt to plot the graph:\nplot(net) # not pretty!\n\n\n\n# Removing loops from the graph:\nnet <-\n  igraph::simplify(net, remove.multiple = F, remove.loops = T)\n\n# Let's and reduce the arrow size and remove the labels:\nplot(net, edge.arrow.size = .4, vertex.label = NA)\n\n\n\n\n\n# Using tidygraph\ntbl_graph(nodes, links, directed = TRUE) %>%\n  ggraph(., layout = \"graphopt\") +\n  geom_edge_link(\n    color = \"grey\",\n    end_cap = circle(0.2, \"cm\"),\n    start_cap = circle(0.2, \"cm\"),\n    # clears an area near the node\n    \n    arrow = arrow(\n      type = \"closed\",\n      ends = \"last\",\n      length = unit(3, \"mm\")\n    )\n  ) +\n  geom_node_point(size = 6, shape = 21, fill = \"orange\") +\n  geom_node_text(aes(label = id))\n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` in the `default_aes` field and elsewhere instead.\n\n\n\n\n# Removing loops from the graph:\n# From the docs:\n# convert() is a shorthand for performing both `morph` and `crystallise` along with extracting a single tbl_graph (defaults to the first). For morphs w(h)ere you know they only create a single graph, and you want to keep it, this is an easy way.\n#\ntbl_graph(nodes, links, directed = TRUE) %>%\n\n  convert(to_simple) %>%\n  ggraph(., layout = \"graphopt\") +\n  geom_edge_link(\n    color = \"grey\",\n    end_cap = circle(0.2, \"cm\"),\n    start_cap = circle(0.2, \"cm\"),\n \n    arrow = arrow(\n      type = \"closed\",\n      ends = \"last\",\n      length = unit(3, \"mm\")\n    )\n  ) +\n  geom_node_point(size = 6, shape = 21, fill = \"orange\")"
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#dataset-2-matrix",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#dataset-2-matrix",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "‚Äî‚Äî-~~ DATASET 2: matrix ‚Äî‚Äî‚Äì",
    "text": "‚Äî‚Äî-~~ DATASET 2: matrix ‚Äî‚Äî‚Äì\n\n# Read in the data:\nnodes2 <- read.csv(\"./Dataset2-Media-User-Example-NODES.csv\", header = T, as.is = T)\nlinks2 <- read.csv(\"./Dataset2-Media-User-Example-EDGES.csv\", header = T, row.names = 1)\n\n# Examine the data:\nhead(nodes2)\n\n   id   media media.type media.name audience.size\n1 s01     NYT          1  Newspaper            20\n2 s02    WaPo          1  Newspaper            25\n3 s03     WSJ          1  Newspaper            30\n4 s04    USAT          1  Newspaper            32\n5 s05 LATimes          1  Newspaper            20\n6 s06     CNN          2         TV            56\n\nhead(links2)\n\n    U01 U02 U03 U04 U05 U06 U07 U08 U09 U10 U11 U12 U13 U14 U15 U16 U17 U18 U19\ns01   1   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\ns02   0   0   0   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\ns03   0   0   0   0   0   1   1   1   1   0   0   0   0   0   0   0   0   0   0\ns04   0   0   0   0   0   0   0   0   1   1   1   0   0   0   0   0   0   0   0\ns05   0   0   0   0   0   0   0   0   0   0   1   1   1   0   0   0   0   0   0\ns06   0   0   0   0   0   0   0   0   0   0   0   0   1   1   0   0   1   0   0\n    U20\ns01   0\ns02   1\ns03   0\ns04   0\ns05   0\ns06   0\n\n# links2 is a matrix for a two-mode network:\nlinks2 <- as.matrix(links2)\ndim(links2)\n\n[1] 10 20\n\ndim(nodes2)\n\n[1] 30  5\n\n\nNote: What is a two-mode network? A network that as a node$type variable and can be a bipartite or a k-partite network as a result.\n\n# Create an igraph network object from the two-mode matrix:\nnet2 <- igraph::graph_from_incidence_matrix(links2)\n\n# To transform a one-mode network matrix into an igraph object,\n# we would use graph_from_adjacency_matrix()\n\n# A built-in vertex attribute 'type' shows which mode vertices belong to.\ntable(V(net2)$type)\n\n\nFALSE  TRUE \n   10    20 \n\n# Basic igraph plot\nplot(net2,vertex.label = NA)\n\n\n\n\n\n# using tidygraph\n# For all objects that are not node and edge data_frames\n# tidygraph uses `as_tbl_graph()`\n# \ngraph <- as_tbl_graph(links2)\ngraph %>% activate(nodes) %>% as_tibble()\n\n# A tibble: 30 √ó 2\n   type  name \n   <lgl> <chr>\n 1 FALSE s01  \n 2 FALSE s02  \n 3 FALSE s03  \n 4 FALSE s04  \n 5 FALSE s05  \n 6 FALSE s06  \n 7 FALSE s07  \n 8 FALSE s08  \n 9 FALSE s09  \n10 FALSE s10  \n# ‚Ä¶ with 20 more rows\n\ngraph %>% activate(edges) %>% as_tibble()\n\n# A tibble: 31 √ó 3\n    from    to weight\n   <int> <int>  <dbl>\n 1     1    11      1\n 2     1    12      1\n 3     1    13      1\n 4     2    14      1\n 5     2    15      1\n 6     2    30      1\n 7     3    16      1\n 8     3    17      1\n 9     3    18      1\n10     3    19      1\n# ‚Ä¶ with 21 more rows\n\ngraph %>% \n  ggraph(., layout = \"graphopt\") + \n  geom_edge_link(color = \"grey\") + \n  geom_node_point(fill = \"orange\", \n                  shape = 21, size = 6, \n                  color = \"black\")\n\n\n\n\n\n# Examine the resulting object:\nclass(net2)\n\n[1] \"igraph\"\n\nnet2\n\nIGRAPH 9008ce1 UN-B 30 31 -- \n+ attr: type (v/l), name (v/c)\n+ edges from 9008ce1 (vertex names):\n [1] s01--U01 s01--U02 s01--U03 s02--U04 s02--U05 s02--U20 s03--U06 s03--U07\n [9] s03--U08 s03--U09 s04--U09 s04--U10 s04--U11 s05--U11 s05--U12 s05--U13\n[17] s06--U13 s06--U14 s06--U17 s07--U14 s07--U15 s07--U16 s08--U16 s08--U17\n[25] s08--U18 s08--U19 s09--U06 s09--U19 s09--U20 s10--U01 s10--U11\n\n\nNote: The remaining attributes for the nodes ( in data frame nodes2) are not (yet) a part of the graph, either with igraph or with tidygraph."
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#plot-parameters-in-igraph",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#plot-parameters-in-igraph",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "‚Äî‚Äî~~ Plot parameters in igraph ‚Äî‚Äî‚Äì",
    "text": "‚Äî‚Äî~~ Plot parameters in igraph ‚Äî‚Äî‚Äì\nCheck out the node options (starting with ‚Äòvertex.‚Äô) and the edge options (starting with ‚Äòedge.‚Äô).\n\n?igraph.plotting\n\nstarting httpd help server ... done\n\n\nWe can set the node & edge options in two ways - one is to specify them in the plot() function, as we are doing below.\n\nPlot with curved edges (edge.curved = .1) and reduce arrow size:\n\n\nplot(net, edge.arrow.size = .4, edge.curved = .1)\n\n\n\n# Using tidygraph\ngraph <- tbl_graph(nodes, links, directed = TRUE)\ngraph\n\n# A tbl_graph: 17 nodes and 49 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 17 √ó 5 (active)\n  id    media               media.type type.label audience.size\n  <chr> <chr>                    <int> <chr>              <int>\n1 s01   NY Times                     1 Newspaper             20\n2 s02   Washington Post              1 Newspaper             25\n3 s03   Wall Street Journal          1 Newspaper             30\n4 s04   USA Today                    1 Newspaper             32\n5 s05   LA Times                     1 Newspaper             20\n6 s06   New York Post                1 Newspaper             50\n# ‚Ä¶ with 11 more rows\n#\n# Edge Data: 49 √ó 4\n   from    to type      weight\n  <int> <int> <chr>      <int>\n1     1     2 hyperlink     22\n2     1     3 hyperlink     22\n3     1     4 hyperlink     21\n# ‚Ä¶ with 46 more rows\n\ngraph %>% ggraph(., layout = \"graphopt\") +\n  geom_edge_arc(\n    color = \"grey\",\n    strength = 0.1,\n    end_cap = circle(.2, \"cm\"),\n\n    arrow = arrow(\n      type = \"closed\",\n      ends = \"both\",\n      length = unit(3, \"mm\")\n    )\n  ) +\n  geom_node_point(\n    fill = \"orange\",\n    shape = 21,\n    size = 8,\n    color = \"black\"\n  ) +\n  geom_node_text(aes(label = id))\n\n\n\n\n\nSet node color to orange and the border color to hex 555555\nReplace the vertex label with the node names stored in ‚Äúmedia‚Äù\n\n\nplot(\n  net,\n  edge.arrow.size = .2,\n  edge.curved = 0,\n  vertex.color = \"orange\",\n  vertex.frame.color = \"#555555\",\n  vertex.label = V(net)$media,\n  vertex.label.color = \"black\",\n  vertex.label.cex = .7\n)\n\n\n\n# Using tidygraph\n#graph <- tbl_graph(nodes, links, directed = TRUE)\n#graph\ngraph %>%\n  ggraph(., layout = \"gem\") +\n  geom_edge_link(\n    color = \"grey\",\n    end_cap = circle(.3, \"cm\"),\n    \n    arrow = arrow(\n      type = \"closed\",\n      ends = \"both\",\n      length = unit(1, \"mm\")\n    )\n  ) +\n  geom_node_point(\n    fill = \"orange\",\n    shape = 21,\n    size = 6,\n    color = \"#555555\"\n  ) +\n  geom_node_text(aes(label = media))\n\n\n\n\nThe second way to set attributes is to add them to the igraph object.\n\nGenerate colors based on media type:\n\n\ncolrs <- c(\"gray50\", \"tomato\", \"gold\")\nV(net)$color <- colrs[V(net)$media.type]\nplot(net)\n\n\n\n\n\nCompute node degrees (#links) and use that to set node size:\n\n\ndeg <- igraph::degree(net, mode = \"all\")\nV(net)$size <- deg*3\n# Alternatively, we can set node size based on audience size:\nV(net)$size <- V(net)$audience.size*0.7\nV(net)$size\n\n [1] 14.0 17.5 21.0 22.4 14.0 35.0 39.2 23.8 42.0 16.1 23.8 23.1 16.1  8.4 16.8\n[16] 19.6 23.1\n\n# The labels are currently node IDs.\n# Setting them to NA will render no labels:\nV(net)$label.color <- \"black\"\nV(net)$label <- NA\n\n# Set edge width based on weight:\nE(net)$width <- E(net)$weight/6\n\n#change arrow size and edge color:\nE(net)$arrow.size <- .2\nE(net)$edge.color <- \"gray80\"\n\n# We can even set the network layout:\ngraph_attr(net, \"layout\") <- layout_with_lgl\nplot(net)\n\n\n\n\n\n# Using tidygraph\n# graph <- tbl_graph(nodes, links, directed = TRUE)\n# graph\ngraph %>%\n  activate(nodes) %>%\n  mutate(size = centrality_degree()) %>%\n  ggraph(., layout = \"lgl\") +\n  geom_edge_link(\n    aes(width = weight),\n    color = \"grey80\",\n    end_cap = circle(.2, \"cm\"),\n    arrow = arrow(\n      type = \"closed\",\n      ends = \"last\",\n      length = unit(1, \"mm\")\n    )\n  ) +\n  geom_node_point(aes(fill = type.label, size = size),\n                  shape = 21,\n                  color = \"black\") +\n  scale_fill_manual(name = \"Media Type\",\n                    values = c(\"grey50\", \"gold\", \"tomato\")) +\n  scale_edge_width(range = c(0.2, 1.5), guide = \"none\") +\n  scale_size_continuous(\"Degree\", range =  c(2, 16)) +\n  \n  guides(fill = guide_legend(title = \"Media Type\",\n                             override.aes = list(pch = 21, size = 4)))\n\n\n\n\nWe can also override the attributes explicitly in the plot:\n\nplot(net, edge.color = \"orange\", vertex.color = \"gray50\")\n\n\n\n\nWe can also add a legend explaining the meaning of the colors we used:\n\nplot(net)\nlegend(x = -2.1, y = -1.1, \n       c(\"Newspaper\",\"Television\", \"Online News\"), \n       pch = 21,col = \"#777777\", \n       pt.bg = colrs, pt.cex = 2.5, bty = \"n\", ncol = 1)\n\n\n\n# legends are automatic with the tidygraph + ggraph flow\n\nSometimes, especially with semantic networks, we may be interested in plotting only the labels of the nodes:\n\nplot(net, vertex.shape = \"none\", vertex.label = V(net)$media,\n     vertex.label.font = 2, vertex.label.color = \"gray40\",\n     vertex.label.cex = .7, edge.color = \"gray85\")\n\n\n\n#using tidygraph\n\nggraph(net, layout = \"gem\") +\n  geom_edge_link(color = \"grey80\", width = 2,\n                 end_cap = circle(0.5,\"cm\"), \n                 start_cap = circle(0.5, \"cm\")) +\n    geom_node_text(aes(label = media))\n\n\n\n\nLet‚Äôs color the edges of the graph based on their source node color. We‚Äôll get the starting node for each edge with ends().\nNote: Edge attribute is being set by start node.\n\nedge.start <- ends(net, es = E(net), names = F)[,1]\nedge.col <- V(net)$color[edge.start] # How simple this is !!!\n# The three colors are recycled \n# \nplot(net, edge.color = edge.col, edge.curved = .4)\n\n\n\n\nNOTE: The source node colour has been set using the media.type, which is a node attribute. Node attributes are not typically accessible to edges. So we need to build a combo data frame using dplyr, so that edges can use this node attribute. ( There may be other ways‚Ä¶)\n\n# Using tidygraph\n# Make a \"combo\" data frame of nodes *and* edges with left_join()\n# Join by `from` so that type.label is based on from = edge.start\n\nlinks %>%\n  left_join(., nodes, by = c(\"from\" = \"id\")) %>%\n  tbl_graph(edges = ., nodes = nodes) %>%\n  \n  mutate(size = centrality_degree()) %>%\n  \n  ggraph(., layout = \"lgl\") +\n  geom_edge_arc(aes(color = type.label,\n                    width = weight),\n                strength = 0.3)  +\n  geom_node_point(aes(fill = type.label,\n                      # type.label is now available as edge attribute\n                      size = size),\n                  shape = 21,\n                  color = \"black\") +\n  scale_fill_manual(\n    name = \"Media Type\",\n    values = c(\"grey50\", \"gold\", \"tomato\"),\n    guide = \"legend\"\n  ) +\n  scale_edge_color_manual(name = \"Source Type\",\n                          values = c(\"grey80\", \"gold\", \"tomato\")) +\n  scale_edge_width(range = c(0.2, 1.5), guide = \"none\") +\n  scale_size_continuous(\"Degree\", range =  c(2, 16)) +\n  # not \"limits\"!\n  guides(fill = guide_legend(override.aes = list(pch = 21,\n                                                 size = 4)))"
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#network-layouts-in-igraph",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#network-layouts-in-igraph",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "‚Äî‚Äî-~~ Network Layouts in ‚Äòigraph‚Äô ‚Äî‚Äî‚Äì",
    "text": "‚Äî‚Äî-~~ Network Layouts in ‚Äòigraph‚Äô ‚Äî‚Äî‚Äì\nNetwork layouts are algorithms that return coordinates for each node in a network.\nLet‚Äôs generate a slightly larger 100-node graph using a preferential attachment model (Barabasi-Albert).\n\nnet.bg <- sample_pa(n =  100, power =  1.2)\nV(net.bg)$size <- 8\nV(net.bg)$frame.color <- \"white\"\nV(net.bg)$color <- \"orange\"\nV(net.bg)$label <- \"\"\nE(net.bg)$arrow.mode <- 0\nplot(net.bg)\n\n\n\n# Using tidygraph\ngraph <- play_barabasi_albert(n = 100, power = 1.2)\ngraph %>%\n  ggraph(., layout = \"graphopt\") +\n  geom_edge_link(color = \"grey\") +\n  geom_node_point(color = \"orange\", size = 4) +\n  theme_graph()\n\n\n\n\nNow let‚Äôs plot this network using the layouts available in igraph. You can set the layout in the plot function:\n\nplot(net.bg, layout = layout_randomly)\n\n\n\n\nOr calculate the vertex coordinates in advance:\n\nl <- layout_in_circle(net.bg)\nplot(net.bg, layout = l)\n\n\n\n# Using tidygraph\n# graph <- play_barabasi_albert(n = 100, power = 1.2)\ngraph %>% ggraph(., layout = \"circle\") +\n  geom_edge_link(color = \"grey\") +\n  geom_node_point(color = \"orange\", size = 2) +\n  theme_graph() +\n  theme(aspect.ratio = 1)\n\n\n\n\nl is simply a matrix of x,y coordinates (N x 2) for the N nodes in the graph. You can generate your own:\n\nl\n\n                [,1]          [,2]\n  [1,]  1.000000e+00  0.000000e+00\n  [2,]  9.980267e-01  6.279052e-02\n  [3,]  9.921147e-01  1.253332e-01\n  [4,]  9.822873e-01  1.873813e-01\n  [5,]  9.685832e-01  2.486899e-01\n  [6,]  9.510565e-01  3.090170e-01\n  [7,]  9.297765e-01  3.681246e-01\n  [8,]  9.048271e-01  4.257793e-01\n  [9,]  8.763067e-01  4.817537e-01\n [10,]  8.443279e-01  5.358268e-01\n [11,]  8.090170e-01  5.877853e-01\n [12,]  7.705132e-01  6.374240e-01\n [13,]  7.289686e-01  6.845471e-01\n [14,]  6.845471e-01  7.289686e-01\n [15,]  6.374240e-01  7.705132e-01\n [16,]  5.877853e-01  8.090170e-01\n [17,]  5.358268e-01  8.443279e-01\n [18,]  4.817537e-01  8.763067e-01\n [19,]  4.257793e-01  9.048271e-01\n [20,]  3.681246e-01  9.297765e-01\n [21,]  3.090170e-01  9.510565e-01\n [22,]  2.486899e-01  9.685832e-01\n [23,]  1.873813e-01  9.822873e-01\n [24,]  1.253332e-01  9.921147e-01\n [25,]  6.279052e-02  9.980267e-01\n [26,] -1.608143e-16  1.000000e+00\n [27,] -6.279052e-02  9.980267e-01\n [28,] -1.253332e-01  9.921147e-01\n [29,] -1.873813e-01  9.822873e-01\n [30,] -2.486899e-01  9.685832e-01\n [31,] -3.090170e-01  9.510565e-01\n [32,] -3.681246e-01  9.297765e-01\n [33,] -4.257793e-01  9.048271e-01\n [34,] -4.817537e-01  8.763067e-01\n [35,] -5.358268e-01  8.443279e-01\n [36,] -5.877853e-01  8.090170e-01\n [37,] -6.374240e-01  7.705132e-01\n [38,] -6.845471e-01  7.289686e-01\n [39,] -7.289686e-01  6.845471e-01\n [40,] -7.705132e-01  6.374240e-01\n [41,] -8.090170e-01  5.877853e-01\n [42,] -8.443279e-01  5.358268e-01\n [43,] -8.763067e-01  4.817537e-01\n [44,] -9.048271e-01  4.257793e-01\n [45,] -9.297765e-01  3.681246e-01\n [46,] -9.510565e-01  3.090170e-01\n [47,] -9.685832e-01  2.486899e-01\n [48,] -9.822873e-01  1.873813e-01\n [49,] -9.921147e-01  1.253332e-01\n [50,] -9.980267e-01  6.279052e-02\n [51,] -1.000000e+00 -3.216286e-16\n [52,] -9.980267e-01 -6.279052e-02\n [53,] -9.921147e-01 -1.253332e-01\n [54,] -9.822873e-01 -1.873813e-01\n [55,] -9.685832e-01 -2.486899e-01\n [56,] -9.510565e-01 -3.090170e-01\n [57,] -9.297765e-01 -3.681246e-01\n [58,] -9.048271e-01 -4.257793e-01\n [59,] -8.763067e-01 -4.817537e-01\n [60,] -8.443279e-01 -5.358268e-01\n [61,] -8.090170e-01 -5.877853e-01\n [62,] -7.705132e-01 -6.374240e-01\n [63,] -7.289686e-01 -6.845471e-01\n [64,] -6.845471e-01 -7.289686e-01\n [65,] -6.374240e-01 -7.705132e-01\n [66,] -5.877853e-01 -8.090170e-01\n [67,] -5.358268e-01 -8.443279e-01\n [68,] -4.817537e-01 -8.763067e-01\n [69,] -4.257793e-01 -9.048271e-01\n [70,] -3.681246e-01 -9.297765e-01\n [71,] -3.090170e-01 -9.510565e-01\n [72,] -2.486899e-01 -9.685832e-01\n [73,] -1.873813e-01 -9.822873e-01\n [74,] -1.253332e-01 -9.921147e-01\n [75,] -6.279052e-02 -9.980267e-01\n [76,] -1.836910e-16 -1.000000e+00\n [77,]  6.279052e-02 -9.980267e-01\n [78,]  1.253332e-01 -9.921147e-01\n [79,]  1.873813e-01 -9.822873e-01\n [80,]  2.486899e-01 -9.685832e-01\n [81,]  3.090170e-01 -9.510565e-01\n [82,]  3.681246e-01 -9.297765e-01\n [83,]  4.257793e-01 -9.048271e-01\n [84,]  4.817537e-01 -8.763067e-01\n [85,]  5.358268e-01 -8.443279e-01\n [86,]  5.877853e-01 -8.090170e-01\n [87,]  6.374240e-01 -7.705132e-01\n [88,]  6.845471e-01 -7.289686e-01\n [89,]  7.289686e-01 -6.845471e-01\n [90,]  7.705132e-01 -6.374240e-01\n [91,]  8.090170e-01 -5.877853e-01\n [92,]  8.443279e-01 -5.358268e-01\n [93,]  8.763067e-01 -4.817537e-01\n [94,]  9.048271e-01 -4.257793e-01\n [95,]  9.297765e-01 -3.681246e-01\n [96,]  9.510565e-01 -3.090170e-01\n [97,]  9.685832e-01 -2.486899e-01\n [98,]  9.822873e-01 -1.873813e-01\n [99,]  9.921147e-01 -1.253332e-01\n[100,]  9.980267e-01 -6.279052e-02\n\nl <- cbind(1:vcount(net.bg), c(1, vcount(net.bg):2))\nplot(net.bg, layout = l)\n\n\n\n# Using tidygraph\n# graph <- play_barabasi_albert(n = 100, power = 1.2)\ngraph %>% ggraph(., layout = l) +\n  geom_edge_link(color = \"grey\") +\n  geom_node_point(color = \"orange\", size = 2)+\n  theme_graph()\n\n\n\n\nThis layout is just an example and not very helpful - thankfully igraph has a number of built-in layouts, including:\n\nRandomly placed vertices\n\n\nl <- layout_randomly(net.bg)\nplot(net.bg, layout = l)\n\n\n\n# Using tidygraph\n# graph <- play_barabasi_albert(n = 100, power = 1.2)\ngraph %>% ggraph(., layout = layout_randomly(.)) + \n  geom_edge_link0(colour = \"grey\") + \n  geom_node_point(colour = \"orange\", size = 4)\n\n\n\n\n\nCircle layout\n\n\nl <- layout_in_circle(net.bg)\nplot(net.bg, layout = l)\n\n\n\n# Using tidygraph\n# graph <- play_barabasi_albert(n = 100, power = 1.2)\ngraph %>% ggraph(., layout = layout_in_circle(.)) + \n  geom_edge_link0(colour = \"grey\") + \n  geom_node_point(colour = \"orange\") +\n  theme(aspect.ratio = 1)\n\n\n\n\n\n3D sphere layout\n\n\nl <- layout_on_sphere(net.bg)\nplot(net.bg, layout = l)\n\n\n\n# Using tidygraph\n# graph <- play_barabasi_albert(n = 100, power = 1.2)\ngraph %>% ggraph(., layout = layout_on_sphere(.)) + \n  geom_edge_link0(colour = \"grey\") + \n  geom_node_point(colour = \"orange\")\n\n\n\n\n\nThe Fruchterman-Reingold force-directed algorithm: Nice but slow, most often used in graphs smaller than ~1000 vertices.\n\n\nl <- layout_with_fr(net.bg)\nplot(net.bg, layout = l)\n\n\n\n# Using tidygraph\n# graph <- play_barabasi_albert(n = 100, power = 1.2)\ngraph %>% ggraph(., layout = layout_with_fr(.)) + \n  geom_edge_link0(colour = \"grey\") + \n  geom_node_point(colour = \"orange\")\n\n\n\n\nYou will also notice that the F-R layout is not deterministic - different runs will result in slightly different configurations. Saving the layout in l allows us to get the exact same result multiple times.\n\npar(mfrow = c(2,2), mar = c(1,1,1,1))\nplot(net.bg, layout = layout_with_fr)\nplot(net.bg, layout = layout_with_fr)\nplot(net.bg, layout = l)\nplot(net.bg, layout = l)\n\n\n\n\nBy default, the coordinates of the plots are rescaled to the [-1,1] interval for both x and y. You can change that with the parameter rescale = FALSE and rescale your plot manually by multiplying the coordinates by a scalar. You can use norm_coords to normalize the plot with the boundaries you want. This way you can create more compact or spread out layout versions.\n\n#Get the layout coordinates:\nl <- layout_with_fr(net.bg)\n# Normalize them so that they are in the -1, 1 interval:\nl <- norm_coords(l, ymin = -1, ymax = 1, xmin = -1, xmax = 1)\n\npar(mfrow = c(2,2), mar = c(0,0,0,0))\nplot(net.bg, rescale = F, layout = l*0.4)\nplot(net.bg, rescale = F, layout = l*0.8)\nplot(net.bg, rescale = F, layout = l*1.2)\nplot(net.bg, rescale = F, layout = l*1.6)\n\n\n\n# Using tidygraph\n# Can't do this with tidygraph ( multiplying layout * scalar ), it seems\n\nAnother popular force-directed algorithm that produces nice results for connected graphs is Kamada Kawai. Like Fruchterman Reingold, it attempts to minimize the energy in a spring system.\n\nl <- layout_with_kk(net.bg)\nplot(net.bg, layout = l)\n\n\n\n# Using tidygraph\n# graph <- play_barabasi_albert(n = 100, power = 1.2)\ngraph %>% ggraph(., layout = layout_with_kk(.)) + \n  geom_edge_link0(colour = \"grey\") + \n  geom_node_point(colour = \"orange\", size = 4)\n\n\n\n\nThe MDS (multidimensional scaling) algorithm tries to place nodes based on some measure of similarity or distance between them. More similar/less distant nodes are placed closer to each other. By default, the measure used is based on the shortest paths between nodes in the network. That can be changed with the dist parameter.\n\nplot(net.bg, layout = layout_with_mds)\n\n\n\n# Using tidygraph\n# graph <- play_barabasi_albert(n = 100, power = 1.2)\ngraph %>% ggraph(., layout = layout_with_mds(.)) + \n  geom_edge_link0(colour = \"grey\") + \n  geom_node_point(colour = \"orange\", size = 4)\n\n\n\n\nThe LGL algorithm is for large connected graphs. Here you can specify a root- the node that will be placed in the middle of the layout.\n\nplot(net.bg, layout = layout_with_lgl)\n\n\n\n# Using tidygraph\n# graph <- play_barabasi_albert(n = 100, power = 1.2)\ngraph %>% ggraph(., layout = layout_with_lgl(.)) + \n  geom_edge_link0(colour = \"grey\") + \n  geom_node_point(colour = \"orange\", size = 4)\n\n\n\n\nBy default, igraph uses a layout called layout_nicely which selects an appropriate layout algorithm based on the properties of the graph. Check out all available layouts in igraph:\n\n?igraph::layout_\n\n\nlayouts <- grep(\"^layout_\", ls(\"package:igraph\"), value = TRUE)[-1]\n\n# Remove layouts that do not apply to our graph.\nlayouts <- layouts[!grepl(\"bipartite|merge|norm|sugiyama|tree\", layouts)]\n\npar(mfrow = c(3,3), mar = c(1,1,1,1))\n\nfor (layout in layouts) {\n  print(layout)\n  l <- do.call(layout, list(net))\n  plot(net, edge.arrow.mode = 0, layout = l, main = layout) }\n\n[1] \"layout_as_star\"\n\n\n[1] \"layout_components\"\n\n\n[1] \"layout_in_circle\"\n\n\n[1] \"layout_nicely\"\n\n\n[1] \"layout_on_grid\"\n\n\n[1] \"layout_on_sphere\"\n\n\n[1] \"layout_randomly\"\n\n\n[1] \"layout_with_dh\"\n\n\n[1] \"layout_with_drl\"\n\n\n\n\n\n[1] \"layout_with_fr\"\n\n\n[1] \"layout_with_gem\"\n\n\n[1] \"layout_with_graphopt\"\n\n\n[1] \"layout_with_kk\"\n\n\n[1] \"layout_with_lgl\"\n\n\n[1] \"layout_with_mds\""
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#highlighting-specific-nodes-or-links",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#highlighting-specific-nodes-or-links",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "‚Äî‚Äî-~~ Highlighting specific nodes or links ‚Äî‚Äî‚Äì",
    "text": "‚Äî‚Äî-~~ Highlighting specific nodes or links ‚Äî‚Äî‚Äì\nSometimes we want to focus the visualization on a particular node or a group of nodes. Let‚Äôs represent distance from the NYT:\n\n\ndistances() calculates shortest path from vertices in ‚Äòv‚Äô to ones in ‚Äòto‚Äô.\n\n\ndist.from.NYT <- distances(net, \n                           v = V(net)[media == \"NY Times\"], \n                           to = V(net), \n                           weights = NA)\n\n#Set colors to plot the distances:\noranges <- colorRampPalette(c(\"dark red\", \"gold\"))\ncol <- oranges(max(dist.from.NYT)+1)\ncol <- col[dist.from.NYT+1]\n\n# Let's have same coordinates for Nodes in both graph renderings\n# Then we can verify that the distance calculations are the same for both renderings\ncoords <- igraph::layout_nicely(net)\nplot(net, vertex.label = dist.from.NYT,\n     vertex.color = col, vertex.label.color = \"black\",\n     layout = coords)\n\n\n\n\n\n# Using tidygraph\ngraph <- tbl_graph(nodes, links, directed = TRUE)\ngraph\n\n# A tbl_graph: 17 nodes and 49 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 17 √ó 5 (active)\n  id    media               media.type type.label audience.size\n  <chr> <chr>                    <int> <chr>              <int>\n1 s01   NY Times                     1 Newspaper             20\n2 s02   Washington Post              1 Newspaper             25\n3 s03   Wall Street Journal          1 Newspaper             30\n4 s04   USA Today                    1 Newspaper             32\n5 s05   LA Times                     1 Newspaper             20\n6 s06   New York Post                1 Newspaper             50\n# ‚Ä¶ with 11 more rows\n#\n# Edge Data: 49 √ó 4\n   from    to type      weight\n  <int> <int> <chr>      <int>\n1     1     2 hyperlink     22\n2     1     3 hyperlink     22\n3     1     4 hyperlink     21\n# ‚Ä¶ with 46 more rows\n\n# Set up NY Times as root node first\n# V(net)[media == \"NY Times\"] cannot be used since it returns an `igraph.vs` ( i.e. a list ) object. \n# We need an integer node id.\nroot_nyt <- graph %>%\n  activate(nodes) %>%\n  as_tibble() %>%\n  rowid_to_column(var = \"node_id\") %>%\n  filter(media == \"NY Times\") %>%\n  select(node_id) %>% as_vector()\nroot_nyt\n\nnode_id \n      1 \n\ngraph %>%\n  activate(nodes) %>%\n  mutate(size = centrality_degree()) %>%\n  \n  # new stuff:\n  # breadth first search for all distances from the root node\n  mutate(order = bfs_dist(root = root_nyt)) %>%\n  \n  ggraph(., layout = coords) + # same layout\n  geom_edge_link(\n    aes(width = weight),\n    color = \"grey80\",\n    end_cap = circle(.2, \"cm\"),\n    arrow = arrow(\n      type = \"closed\",\n      ends = \"last\",\n      length = unit(1, \"mm\")\n    )\n  ) +\n  geom_node_point(aes(fill = order,\n                      size = size),\n                  shape = 21,\n                  color = \"black\") +\n  \n  geom_node_text(aes(label = order)) +\n  \n  scale_fill_gradient(\n    name = \"Distance from NY Times\",\n    low = \"dark red\",\n    high = \"gold\",\n    guide = \"legend\"\n  ) +\n  scale_edge_width(range = c(0.2, 1.5), guide = \"none\") +\n  scale_size_continuous(\"Degree\", range =  c(2, 16)) +\n  guides(fill = guide_legend(override.aes = list(pch = 21,\n                                                 size = 4)))\n\n\n\n\nOr, a bit more readable:\n\nplot(net, vertex.color = col, \n     vertex.label = dist.from.NYT, edge.arrow.size = .6,\n     vertex.label.color = \"white\", \n     vertex.size = V(net)$size*1.6, \n     edge.width = 2,\n     layout = norm_coords(layout_with_lgl(net))*1.4, rescale = F)"
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#path-highlighting",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#path-highlighting",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "Path Highlighting",
    "text": "Path Highlighting\nWe can also highlight paths between the nodes in the network.\n\nSay here between MSNBC and the New York Post\n\n\nnews.path <- shortest_paths(net,\n                            from  =  V(net)[media == \"MSNBC\"],\n                            to   =  V(net)[media == \"New York Post\"],\n                            output  =  \"both\")  #both path nodes and edges\nnews.path.distance <- distances(net,\n                                V(net)[media == \"MSNBC\"],\n                                V(net)[media == \"New York Post\"] )\nnews.path\n\n$vpath\n$vpath[[1]]\n+ 4/17 vertices, named, from 8e65729:\n[1] s08 s03 s12 s06\n\n\n$epath\n$epath[[1]]\n+ 3/48 edges from 8e65729 (vertex names):\n[1] s08->s03 s03->s12 s12->s06\n\n\n$predecessors\nNULL\n\n$inbound_edges\nNULL\n\nnews.path.distance\n\n    s06\ns08   5\n\n#Generate edge color variable to plot the path:\necol <- rep(\"gray80\", ecount(net))\necol[unlist(news.path$epath)] <- \"orange\"\n\n#Generate edge width variable to plot the path:\new <- rep(2, ecount(net))\new[unlist(news.path$epath)] <- 4\n\n#Generate node color variable to plot the path:\nvcol <- rep(\"gray40\", vcount(net))\nvcol[unlist(news.path$vpath)] <- \"gold\"\n\nplot(net, vertex.color = vcol, \n     edge.color = ecol,\n     edge.width = ew, \n     edge.arrow.mode = 0,\n     ## added lines\n     vertex.label = V(net)$media,\n     vertex.label.font = 2, \n     vertex.label.color = \"gray40\",\n     vertex.label.cex = .7,\n     layout = coords * 1.5)\n\n\n\n\n\n# Using tidygraph\n# We need to use:\n# to_shortest_path(graph, from, to, mode = \"out\", weights = NULL)\n# Let's set up `to` and `from` nodes\n#\n# V(net)[media == \"NY Times\"] cannot be used since it returns an `igraph.vs` ( i.e. a list ) object.\n# We need integer node ids for `from` and `to` in `to_shortest_path`\n\nmsnbc <- graph %>%\n  activate(nodes) %>%\n  as_tibble() %>%\n  rowid_to_column(var = \"node_id\") %>%\n  filter(media == \"MSNBC\") %>%\n  select(node_id) %>% as_vector()\nmsnbc\n\nnode_id \n      8 \n\nnypost <- graph %>%\n  activate(nodes) %>%\n  as_tibble() %>%\n  rowid_to_column(var = \"node_id\") %>%\n  filter(media == \"New York Post\") %>%\n  select(node_id) %>% as_vector()\nnypost\n\nnode_id \n      6 \n\n# Let's create a fresh graph object using morph\n# However we want to merge it back with the original `graph`\n# to get an overlay plot\n#\n# # Can do this to obtain a separate graph\n# convert(to_shortest_path,from = msnbc,to = nypost)\n# However we want to merge it back with the original `graph`\n# to get an overlay plot\nmsnbc_nyp <-\n  graph %>%\n  # first mark all nodes and edges as *not* on the shortest path\n  activate(nodes) %>%\n  mutate(shortest_path_node = FALSE) %>%\n  activate(edges) %>%\n  mutate(shortest_path_edge = FALSE) %>%\n  \n  # Find shortest path between MSNBC and NY Post\n  morph(to_shortest_path, from = msnbc, to = nypost) %>%\n  \n  # Now to mark the shortest_path nodes as TRUE\n  activate(nodes) %>%\n  mutate(shortest_path_node = TRUE) %>%\n  \n  # Now to mark the shortest_path edges as TRUE\n  activate(edges) %>%\n  mutate(shortest_path_edge = TRUE) %>%\n  #\n  # Merge back into main graph; Still saving it as a `msnbc_nyp`\n  unmorph()\nmsnbc_nyp\n\n# A tbl_graph: 17 nodes and 49 edges\n#\n# A directed multigraph with 1 component\n#\n# Edge Data: 49 √ó 5 (active)\n   from    to type      weight shortest_path_edge\n  <int> <int> <chr>      <int> <lgl>             \n1     1     2 hyperlink     22 FALSE             \n2     1     3 hyperlink     22 FALSE             \n3     1     4 hyperlink     21 FALSE             \n4     1    15 mention       20 FALSE             \n5     2     1 hyperlink     23 FALSE             \n6     2     3 hyperlink     21 FALSE             \n# ‚Ä¶ with 43 more rows\n#\n# Node Data: 17 √ó 6\n  id    media               media.type type.label audience.size shortest_path_n‚Ä¶\n  <chr> <chr>                    <int> <chr>              <int> <lgl>           \n1 s01   NY Times                     1 Newspaper             20 FALSE           \n2 s02   Washington Post              1 Newspaper             25 FALSE           \n3 s03   Wall Street Journal          1 Newspaper             30 TRUE            \n# ‚Ä¶ with 14 more rows\n\nmsnbc_nyp %>%\n  activate(nodes) %>%\n  mutate(size = centrality_degree()) %>%\n  ggraph(layout = coords) +\n  #geom_edge_link0(colour = \"grey\") +\n  geom_edge_link0(aes(colour = shortest_path_edge,\n                      width = shortest_path_edge)) +\n  \n  geom_node_point(aes(size = size,\n                      fill = shortest_path_node), shape = 21) +\n  geom_node_text(aes(label = media)) +\n  \n  scale_size_continuous(\"Degree\", range =  c(2, 16)) +\n  scale_fill_manual(\"Shortest Path\",\n                    values = c(\"grey\", \"gold\")) +\n  \n  scale_edge_width_manual(values = c(1, 4)) +\n  \n  scale_edge_colour_manual(values = c(\"grey\", \"orange\")) +\n  guides(\n    fill = guide_legend(override.aes = list(pch = 21,\n                                            size = 6)),\n    edge_colour = \"none\",\n    edge_width = \"none\"\n  )\n\n\n\n\n\nHighlight the edges going into or out of a vertex, for instance the WSJ. For a single node, use incident(), for multiple nodes use incident_edges()\n\n\n\ninc.edges <-\n  incident(net, V(net)[media == \"Wall Street Journal\"], mode = \"all\")\n\n#Set colors to plot the selected edges.\necol <- rep(\"gray80\", ecount(net))\necol[inc.edges] <- \"orange\"\nvcol <- rep(\"grey40\", vcount(net))\nvcol[V(net)$media == \"Wall Street Journal\"] <- \"gold\"\nplot(\n  net,\n  vertex.color = vcol,\n  edge.color = ecol,\n  edge.width = 2,\n  layout = coords\n)\n\n\n\n\n\n# Using tidygraph\nwsj <- graph %>% \n  activate(nodes) %>% \n  as_tibble() %>% \n  rowid_to_column(var = \"node_id\") %>% \n  filter(media == \"Wall Street Journal\") %>% \n  select(node_id) %>% as_vector()\n\ngraph %>% \n  activate(nodes) %>% \n  mutate(wsj_adjacent = node_is_adjacent(to = wsj, mode = \"all\", \n                                         include_to = TRUE),\n         size = centrality_degree()) %>% \n  mutate(WSJ = if_else(media == \"Wall Street Journal\", TRUE, FALSE)) %>% \n  activate(edges) %>% \n  mutate(wsj_links = edge_is_incident(wsj)) %>% \n  \n  ggraph(., layout = coords) +\n  geom_edge_link0(aes(colour = wsj_links), width = 2) + \n  \n  geom_node_point(aes(fill = WSJ, \n                      size = size),shape = 21) +\n  \n  geom_node_text(aes(label = media), repel = TRUE) + \n  \n  scale_fill_manual(\"WSJ Neighbours\", \n                      values = c(\"grey\", \"gold\"), \n                      guide = guide_legend(override.aes = \n                                             list(pch = 21, \n                                                  size = 5))) + \n  scale_edge_colour_manual(\"WSJ Links\", \n                      values = c(\"grey\", \"orange\")) + \n  scale_size(\"Degree\", range = c( 2, 16)) +\n  ggtitle(label = \"Highlighting WSJ Neighbours and Links\") +\n  guides(shape = \"none\", fill = \"none\" #, colour = \"none\"\n  )"
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#highlight-neighbours",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#highlight-neighbours",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "Highlight Neighbours",
    "text": "Highlight Neighbours\nOr we can highlight the immediate neighbors of a vertex, say WSJ. The neighbors function finds all nodes one step out from the focal actor. To find the neighbors for multiple nodes, use adjacent_vertices(). To find node neighborhoods going more than one step out, use function ego() with parameter order set to the number of steps out to go from the focal node(s).\n\nneigh.nodes <- neighbors(net, V(net)[media == \"Wall Street Journal\"], mode = \"out\")\n\n# Set colors to plot the neighbors:\nvcol[neigh.nodes] <- \"#ff9d00\"\nplot(net, vertex.color = vcol)\n\n\n\n\n\n# Using tidygraph\nwsj <- graph %>% \n  activate(nodes) %>% \n  as_tibble() %>% \n  rowid_to_column(var = \"node_id\") %>% \n  filter(media == \"Wall Street Journal\") %>% \n  select(node_id) %>% as_vector()\n\ngraph %>% \n  activate(nodes) %>% \n  mutate(wsj_adjacent = node_is_adjacent(to = wsj, mode = \"all\", \n  # remove WSJ from the list!\n  # highlight only the neighbours\n  \n                                         include_to = FALSE),\n         size = centrality_degree()) %>% \n  mutate(WSJ = if_else(media == \"Wall Street Journal\", TRUE, FALSE)) %>% \n  activate(edges) %>% \n  mutate(wsj_links = edge_is_incident(wsj)) %>% \n  \n  ggraph(., layout = coords) +\n  geom_edge_link0(aes(colour = wsj_links), width = 2) + \n  \n  geom_node_point(aes(fill = wsj_adjacent, \n                      size = size),shape = 21) +\n  \n  geom_node_text(aes(label = media), repel = TRUE) + \n  \n  scale_fill_manual(\"WSJ Neighbours\", \n                      values = c(\"grey\", \"gold\"), \n                      guide = guide_legend(override.aes = \n                                             list(pch = 21, \n                                                  size = 5))) + \n  scale_edge_colour_manual(\"WSJ Links\", \n                      values = c(\"grey\", \"orange\")) + \n  scale_size(\"Degree\", range = c( 2, 16)) +\n  ggtitle(label = \"Highlighting WSJ Neighbours and Links\") +\n  guides(shape = \"none\", fill = \"none\" #, colour = \"none\"\n  )\n\n\n\n\nAnother way to draw attention to a group of nodes: (This is generally not recommended since, depending on layout, nodes that are not ‚Äòmarked‚Äô can accidentally get placed on top of the mark)\n\nplot(net, mark.groups = c(1,4,5,8), mark.col = \"#C5E5E7\", mark.border = NA)\n\n\n\n# Mark multiple groups:\nplot(net, mark.groups = list(c(1,4,5,8), c(15:17)),\n          mark.col = c(\"#C5E5E7\",\"#ECD89A\"), mark.border = NA)"
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#interactive-plotting-with-tkplot",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#interactive-plotting-with-tkplot",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "‚Äî‚Äî-~~ Interactive plotting with ‚Äòtkplot‚Äô ‚Äî‚Äî‚Äì",
    "text": "‚Äî‚Äî-~~ Interactive plotting with ‚Äòtkplot‚Äô ‚Äî‚Äî‚Äì\nR and igraph offer interactive plotting capabilities (mostly helpful for small networks)\n\ntkid <- tkplot(net) #tkid is the id of the tkplot\n\nl <- tkplot.getcoords(tkid) # grab the coordinates from tkplot\nplot(net, layout = l)"
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#other-ways-to-represent-a-network",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#other-ways-to-represent-a-network",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "‚Äî‚Äî-~~ Other ways to represent a network ‚Äî‚Äî‚Äì",
    "text": "‚Äî‚Äî-~~ Other ways to represent a network ‚Äî‚Äî‚Äì\nOne reminder that there are other ways to represent a network:\n\nHeatmap of the network matrix:\n\n\nnetm <- as_adjacency_matrix(net, attr = \"weight\", sparse = F)\ncolnames(netm) <- V(net)$media\nrownames(netm) <- V(net)$media\n\npalf <- colorRampPalette(c(\"gold\", \"dark orange\"))\n\n# The Rowv & Colv parameters turn dendrograms on and off\nheatmap(netm[,17:1], Rowv  =  NA, Colv  =  NA, col  =  palf(20),\n        scale = \"none\", margins = c(10,10) )\n\n\n\n\n\nDegree distribution\n\n\ndeg.dist <- degree_distribution(net, cumulative = T, mode = \"all\")\n# degree is available in `sna` too\nplot(x = 0:max(igraph::degree(net)), y = 1-deg.dist, pch = 19, cex = 1.4, col = \"orange\", xlab = \"Degree\", ylab = \"Cumulative Frequency\")\n\n\n\n# Using Tidygraph\n# https://stackoverflow.com/questions/18356860/cumulative-histogram-with-ggplot2\ngraph %>% \n  activate(nodes) %>% \n  mutate(degree = centrality_degree(mode = \"all\")) %>% \n  as_tibble() %>% \n  ggplot(aes(x = degree, y = stat(count))) +\n  # geom_histogram(aes(y = cumsum(..count..)), binwidth = 1) + \n  stat_bin(aes(y = cumsum(..count..)),\n                binwidth = 1,# Ta-Da !!\n                geom =\"point\",color =\"orange\", size = 5)\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `after_stat(count)` instead."
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#plotting-two-mode-networks",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#plotting-two-mode-networks",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "4. Plotting two-mode networks",
    "text": "4. Plotting two-mode networks\n\nhead(nodes2)\n\n   id   media media.type media.name audience.size\n1 s01     NYT          1  Newspaper            20\n2 s02    WaPo          1  Newspaper            25\n3 s03     WSJ          1  Newspaper            30\n4 s04    USAT          1  Newspaper            32\n5 s05 LATimes          1  Newspaper            20\n6 s06     CNN          2         TV            56\n\nhead(links2)\n\n    U01 U02 U03 U04 U05 U06 U07 U08 U09 U10 U11 U12 U13 U14 U15 U16 U17 U18 U19\ns01   1   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\ns02   0   0   0   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\ns03   0   0   0   0   0   1   1   1   1   0   0   0   0   0   0   0   0   0   0\ns04   0   0   0   0   0   0   0   0   1   1   1   0   0   0   0   0   0   0   0\ns05   0   0   0   0   0   0   0   0   0   0   1   1   1   0   0   0   0   0   0\ns06   0   0   0   0   0   0   0   0   0   0   0   0   1   1   0   0   1   0   0\n    U20\ns01   0\ns02   1\ns03   0\ns04   0\ns05   0\ns06   0\n\nnet2\n\nIGRAPH 9008ce1 UN-B 30 31 -- \n+ attr: type (v/l), name (v/c)\n+ edges from 9008ce1 (vertex names):\n [1] s01--U01 s01--U02 s01--U03 s02--U04 s02--U05 s02--U20 s03--U06 s03--U07\n [9] s03--U08 s03--U09 s04--U09 s04--U10 s04--U11 s05--U11 s05--U12 s05--U13\n[17] s06--U13 s06--U14 s06--U17 s07--U14 s07--U15 s07--U16 s08--U16 s08--U17\n[25] s08--U18 s08--U19 s09--U06 s09--U19 s09--U20 s10--U01 s10--U11\n\nplot(net2)\n\n\n\n\nThis time we will make nodes look different based on their type. Media outlets are blue squares, audience nodes are orange circles:\n\nV(net2)$color <- c(\"steel blue\", \"orange\")[V(net2)$type+1]\nV(net2)$shape <- c(\"square\", \"circle\")[V(net2)$type+1]\n\n# Media outlets will have name labels, audience members will not:\nV(net2)$label <- \"\"\nV(net2)$label[V(net2)$type == F] <- nodes2$media[V(net2)$type == F]\nV(net2)$label.cex = .6\nV(net2)$label.font = 2\n\nplot(net2, vertex.label.color = \"white\", vertex.size = (2-V(net2)$type)*8)\n\n\n\n\n\n# Using tidygraph\nas_tbl_graph(x = links2, directed = TRUE) %>%\n  activate(nodes) %>%\n  left_join(nodes2, by = c(\"name\" = \"id\")) %>%\n  ggraph(layout = \"nicely\") +\n  geom_edge_link0() +\n  geom_node_point(aes(shape = type, fill = type, size = type)) +\n  geom_node_text(aes(label = if_else(type, \"\", media)), colour = \"white\", size = 3) +\n  \n  scale_shape_manual(\n    \"Type\",\n    values = c(22, 21),\n    labels = c(\"Media\", \"Persons\"),\n    guide = guide_legend(override.aes = list(size = 6))\n  ) +\n  \n  scale_fill_manual(\n    \"Type\",\n    values = c(\"dodgerblue\", \"orange\"),\n    labels = c(\"Media\", \"Persons\")\n  ) +\n  \n  scale_size_manual(values = c(10, 4), guide = \"none\") \n\n\n\n\nigraph has a built-in bipartite layout, though it‚Äôs not the most helpful:\n\nplot(net2, vertex.label = NA, vertex.size = 7, layout = layout_as_bipartite)\n\n\n\n# using tidygraph\nas_tbl_graph(x = links2, directed = TRUE) %>%\n  activate(nodes) %>%\n  left_join(nodes2, by = c(\"name\" = \"id\")) %>%\n  ggraph(., layout = \"igraph\", algorithm = \"bipartite\") +\n  geom_edge_link0() +\n  geom_node_point(aes(shape = type, fill = type, size = type)) +\n  geom_node_text(aes(label = if_else(type, \"\", media)), colour = \"white\", size = 3) +\n  \n  scale_shape_manual(\n    \"Type\",\n    values = c(22, 21),\n    labels = c(\"Media\", \"Persons\"),\n    guide = guide_legend(override.aes = list(size = 6))\n  ) +\n  \n  scale_fill_manual(\n    \"Type\",\n    values = c(\"dodgerblue\", \"orange\"),\n    labels = c(\"Media\", \"Persons\")\n  ) +\n  \n  scale_size_manual(values = c(10, 4), guide = \"none\") \n\n\n\n\n\nUsing text as nodes:\n\n\npar(mar = c(0,0,0,0))\nplot(net2, vertex.shape = \"none\", vertex.label = nodes2$media,\n     vertex.label.color = V(net2)$color, vertex.label.font = 2,\n     vertex.label.cex = .95, edge.color = \"gray70\",  edge.width = 2)\n\n\n\n# Using tidygraph\nas_tbl_graph(x = links2, directed = TRUE) %>%\n  activate(nodes) %>%\n  left_join(nodes2, by = c(\"name\" = \"id\")) %>%\n  ggraph(layout = \"nicely\") +\n  geom_edge_link(end_cap = circle(.4,\"cm\"), \n                 start_cap = circle(0.4, \"cm\")) +\n  # geom_node>point(aes(shape = type, fill = type, size = type)) +\n  geom_node_text(aes(label= media, colour = type), size = 4) +\n  \n  scale_shape_manual(\n    \"Type\",\n    values = c(22, 21),\n    labels = c(\"Media\", \"Persons\"),\n    guide = guide_legend(override.aes = list(size = 4))\n  ) +\n  \n  scale_fill_manual(\n    \"Type\",\n    values = c(\"dodgerblue\", \"orange\"),\n    labels = c(\"Media\", \"Persons\")\n  ) +\n  \n  scale_size_manual(values = c(10, 4), guide = \"none\") \n\n\n\n\n\nUsing images as nodes You will need the ‚Äòpng‚Äô package to do this:\n\n\n# install.packages(\"png\")\nlibrary(\"png\")\n\nimg.1 <- readPNG(\"./images/news.png\")\nimg.2 <- readPNG(\"./images/user.png\")\n\nV(net2)$raster <- list(img.1, img.2)[V(net2)$type+1]\n\npar(mar = c(3,3,3,3))\n\nplot(net2, vertex.shape = \"raster\", vertex.label = NA,\n     vertex.size = 16, vertex.size2 = 16, edge.width = 2)\n\n\n# By the way, you can also add any image you want to any plot. For example, many #network graphs could be improved by a photo of a puppy carrying a basket full of kittens.\nimg.3 <- readPNG(\"./images/puppy.png\")\nrasterImage(img.3,  xleft = -1.7, xright = 0, ybottom = -1.2, ytop = 0)\n\n\n\n# The numbers after the image are coordinates for the plot.\n# The limits of your plotting area are given in par()$usr\n\n\n# Using ~~tidygraph~~ visNetwork\n# See this cheatsheet:\n# system.file(\"fontAwesome/Font_Awesome_Cheatsheet.pdf\", package = \"visNetwork\")\nlibrary(visNetwork)\n\nas_tbl_graph(x = links2, directed = TRUE) %>%\n  activate(nodes) %>%\n  left_join(nodes2, by = c(\"name\" = \"id\")) %>% \n  \n  # visNetwork needs a \"group\" variable for grouping...\n  mutate(group = as.character(type)) %>% \n  visIgraph(.) %>% \n  visGroups(groupname = \"FALSE\",shape = \"icon\", \n            icon = list(code = \"f26c\", size = 75, color = \"orange\")) %>% \n  visGroups(groupname = \"TRUE\",shape = \"icon\", \n            icon = list(code = \"f007\", size = 75)) %>% \n  addFontAwesome()\n\n\n\n\n\nWe can also generate and plot bipartite projections for the two-mode network : (co-memberships are easy to calculate by multiplying the network matrix by its transposed matrix, or using igraph‚Äôs bipartite.projection function)\n\nnet2.bp <- bipartite.projection(net2)\n\n#We can calculate the projections manually as well:\nas_incidence_matrix(net2)  %*% t(as_incidence_matrix(net2))\n\n    s01 s02 s03 s04 s05 s06 s07 s08 s09 s10\ns01   3   0   0   0   0   0   0   0   0   1\ns02   0   3   0   0   0   0   0   0   1   0\ns03   0   0   4   1   0   0   0   0   1   0\ns04   0   0   1   3   1   0   0   0   0   1\ns05   0   0   0   1   3   1   0   0   0   1\ns06   0   0   0   0   1   3   1   1   0   0\ns07   0   0   0   0   0   1   3   1   0   0\ns08   0   0   0   0   0   1   1   4   1   0\ns09   0   1   1   0   0   0   0   1   3   0\ns10   1   0   0   1   1   0   0   0   0   2\n\nt(as_incidence_matrix(net2)) %*%   as_incidence_matrix(net2)\n\nWarning in rm(list = cmd, envir = .tkplot.env): object 'tkp.1' not found\n\n\n    U01 U02 U03 U04 U05 U06 U07 U08 U09 U10 U11 U12 U13 U14 U15 U16 U17 U18 U19\nU01   2   1   1   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0\nU02   1   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\nU03   1   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\nU04   0   0   0   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\nU05   0   0   0   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\nU06   0   0   0   0   0   2   1   1   1   0   0   0   0   0   0   0   0   0   1\nU07   0   0   0   0   0   1   1   1   1   0   0   0   0   0   0   0   0   0   0\nU08   0   0   0   0   0   1   1   1   1   0   0   0   0   0   0   0   0   0   0\nU09   0   0   0   0   0   1   1   1   2   1   1   0   0   0   0   0   0   0   0\nU10   0   0   0   0   0   0   0   0   1   1   1   0   0   0   0   0   0   0   0\nU11   1   0   0   0   0   0   0   0   1   1   3   1   1   0   0   0   0   0   0\nU12   0   0   0   0   0   0   0   0   0   0   1   1   1   0   0   0   0   0   0\nU13   0   0   0   0   0   0   0   0   0   0   1   1   2   1   0   0   1   0   0\nU14   0   0   0   0   0   0   0   0   0   0   0   0   1   2   1   1   1   0   0\nU15   0   0   0   0   0   0   0   0   0   0   0   0   0   1   1   1   0   0   0\nU16   0   0   0   0   0   0   0   0   0   0   0   0   0   1   1   2   1   1   1\nU17   0   0   0   0   0   0   0   0   0   0   0   0   1   1   0   1   2   1   1\nU18   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   1   1   1\nU19   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   1   1   1   2\nU20   0   0   0   1   1   1   0   0   0   0   0   0   0   0   0   0   0   0   1\n    U20\nU01   0\nU02   0\nU03   0\nU04   1\nU05   1\nU06   1\nU07   0\nU08   0\nU09   0\nU10   0\nU11   0\nU12   0\nU13   0\nU14   0\nU15   0\nU16   0\nU17   0\nU18   0\nU19   1\nU20   2\n\npar(mfrow = c(1, 2))\n\nplot(\n  net2.bp$proj1,\n  vertex.label.color = \"black\",\n  vertex.label.dist = 2,\n  vertex.label = nodes2$media[!is.na(nodes2$media.type)]\n)\n\nplot(\n  net2.bp$proj2,\n  vertex.label.color = \"black\",\n  vertex.label.dist = 2,\n  vertex.label = nodes2$media[is.na(nodes2$media.type)]\n)\n\n\n\n\n\n# Using tidygraph\n# Calculate projections and add attributes/labels\nproj1 <-\n  as_incidence_matrix(net2)  %*% t(as_incidence_matrix(net2)) %>%\n  as_tbl_graph() %>%\n  activate(nodes) %>%\n  left_join(., nodes2, by = c(\"name\" = \"id\"))\n\nWarning in (is.null(rownames(x)) && is.null(colnames(x))) || colnames(x) == :\n'length(x) = 10 > 1' in coercion to 'logical(1)'\n\nproj2 <-\n  t(as_incidence_matrix(net2)) %*%   as_incidence_matrix(net2) %>% as_tbl_graph() %>%\n  activate(nodes) %>%\n  left_join(., nodes2, by = c(\"name\" = \"id\"))\n\nWarning in (is.null(rownames(x)) && is.null(colnames(x))) || colnames(x) == :\n'length(x) = 20 > 1' in coercion to 'logical(1)'\n\np1 <- proj1 %>%\n  ggraph(layout = \"graphopt\") +\n  geom_edge_link0() +\n  geom_node_point(size = 6, colour = \"orange\") +\n  geom_node_text(aes(label = media), repel = TRUE)\n\np2 <- proj2 %>%\n  ggraph(layout = \"graphopt\") +\n  geom_edge_link0() +\n  geom_node_point(\n    aes(colour = media.type),\n    size = 6,\n    shape  = 15,\n    colour = \"dodgerblue\"\n  ) +\n  geom_node_text(aes(label = media), repel = TRUE)\n\np1 + p2"
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#plotting-multiplex-networks",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#plotting-multiplex-networks",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "5. Plotting multiplex networks",
    "text": "5. Plotting multiplex networks\nIn some cases, the networks we want to plot are multigraphs: they can have multiple edges connecting the same two nodes. A related concept, multiplex networks, contain multiple types of ties ‚Äì e.g. friendship, romantic, and work relationships between individuals.\nIn our example network, we also have two tie types: hyperlinks and mentions. One thing we can do is plot each type of tie separately:\n\nE(net)$width <- 2\nplot(\n  net,\n  edge.color = c(\"dark red\", \"slategrey\")[(E(net)$type == \"hyperlink\") +\n                                            1],\n  vertex.color = \"gray40\",\n  layout = layout_in_circle,\n  edge.curved = .3\n)\n\n\n\n# Another way to delete edges using the minus operator:\nnet.m <- net - E(net)[E(net)$type == \"hyperlink\"]\nnet.h <- net - E(net)[E(net)$type == \"mention\"]\n\n#Plot the two links separately:\npar(mfrow = c(1, 2))\n\nplot(net.h,\n     vertex.color = \"orange\",\n     layout = layout_with_fr,\n     main = \"Tie: Hyperlink\")\nplot(net.m,\n     vertex.color = \"lightsteelblue2\",\n     layout = layout_with_fr,\n     main = \"Tie: Mention\")\n\n\n\n\n\nMake sure the nodes stay in the same place in both plots:\n\n\npar(mfrow = c(1, 2), mar = c(1, 1, 4, 1))\n\nl <- layout_with_fr(net)\nplot(net.h,\n     vertex.color = \"orange\",\n     layout = l,\n     main = \"Tie: Hyperlink\")\nplot(net.m,\n     vertex.color = \"lightsteelblue2\",\n     layout = l,\n     main = \"Tie: Mention\")\n\n\n\n\n\n#Using tidygraph\n\nlayout <- layout_in_circle(net)\np1 <- tbl_graph(nodes, links, directed = TRUE) %>% \n  activate(nodes) %>% \n  mutate(size = centrality_degree()) %>% \n  activate(edges) %>% \n  filter(type == \"hyperlink\") %>% \n  \n  # reusing the earlier computed layout\n  ggraph(layout = layout) +\n  geom_edge_arc(strength = 0.05) +\n  geom_node_point(aes(size = size), shape = 21, \n                  fill = \"orange\") +\n  scale_size(range = c(2, 12)) + \n  labs(title = \"Tie: Hyperlink\") + \n  theme(aspect.ratio = 1,,\n        legend.position = \"bottom\")\n\np2 <- tbl_graph(nodes, links, directed = TRUE) %>% \n  activate(nodes) %>% \n  mutate(size = centrality_degree()) %>% \n  activate(edges) %>% \n  filter(type == \"mention\") %>% \n   # reusing the earlier computed layout\n  ggraph(layout = layout) +\n  geom_edge_arc(strength = 0.05) +\n  geom_node_point(aes(size = size), shape = 21, \n                  fill = \"lightsteelblue2\") +\n  scale_size(range = c(2, 12)) + \n  labs(title = \"Tie: Mention\") + \n  theme(aspect.ratio = 1, legend.position = \"bottom\")\n\nwrap_plots(p1, p2,guides = \"collect\") & \n  # note this \"pipe\" for patchwork!\n  theme(legend.position = \"none\")\n\n\n\n\nIn our example network, we don‚Äôt have node dyads connected by multiple types of connections (we never have both a ‚Äòhyperlink‚Äô and a ‚Äòmention‚Äô tie between the same two news outlets) ‚Äì however that could happen.\nNote: See the edges between s03 and s10‚Ä¶these are in opposite directions. So no dyads.\n\nlayout <- layout_in_circle(net)\ntbl_graph(nodes, links, directed = TRUE) %>%  \n  activate(nodes) %>% \n  mutate(size = centrality_degree()) %>% \n\n  # reusing the earlier computed layout\n  ggraph(layout = layout) +\n  geom_edge_arc(strength = 0.05, aes(colour = type)) +\n  geom_node_point(aes(size = size), shape = 21, \n                  fill = \"orange\") +\n  geom_node_text(aes(label = id), repel = TRUE) +\n  scale_size(range = c(2, 12)) + \n  labs(title = \"Tie: Hyperlink\") + \n  theme(aspect.ratio = 1,,\n        legend.position = \"bottom\")\n\n\n\n\nOne challenge in visualizing multiplex networks is that multiple edges between the same two nodes may get plotted on top of each other in a way that makes them impossible to distinguish. For example, let us generate a simple multiplex network with two nodes and three ties between them:\n\nmultigtr <- graph(edges = c(1, 2, 1, 2, 1, 2), n = 2)\n\nl <- layout_with_kk(multigtr)\n\n# Let's just plot the graph:\nplot(\n  multigtr,\n  vertex.color = \"lightsteelblue\",\n  vertex.frame.color = \"white\",\n  vertex.size = 40,\n  vertex.shape = \"circle\",\n  vertex.label = NA,\n  edge.color = c(\"gold\", \"tomato\", \"yellowgreen\"),\n  edge.width = 10,\n  edge.arrow.size = 5,\n  edge.curved = 0.1,\n  layout = l\n)\n\n\n\n# Using tidygraph\nmultigtr %>% \n  as_tbl_graph() %>% \n  activate(edges) %>% \n  mutate(edge_col = c(\"gold\", \"tomato\", \"yellowgreen\")) %>% \nggraph(., layout = l) +\n  geom_edge_arc(strength = 0.1, aes(colour = edge_col)) + \n  geom_node_point(size = 4, colour = \"lightsteelblue\") +\n  theme(legend.position = \"none\")\n\n\n\n\nBecause all edges in the graph have the same curvature, they are drawn over each other so that we only see the last one. What we can do is assign each edge a different curvature. One useful function in ‚Äòigraph‚Äô called curve_multiple() can help us here. For a graph G, curve.multiple(G) will generate a curvature for each edge that maximizes visibility.\n\nplot(\n  multigtr,\n  vertex.color = \"lightsteelblue\",\n  vertex.frame.color = \"white\",\n  vertex.size = 40,\n  vertex.shape = \"circle\",\n  vertex.label = NA,\n  edge.color = c(\"gold\", \"tomato\", \"yellowgreen\"),\n  edge.width = 10,\n  edge.arrow.size = 5,\n  edge.curved = curve_multiple(multigtr),\n  layout = l\n)\n\n\n\n\n\nmultigtr %>% \n  as_tbl_graph() %>% \n  activate(edges) %>% \n  mutate(edge_col = c(\"gold\", \"tomato\", \"yellowgreen\")) %>% \nggraph(., layout = l) +\n  geom_edge_fan(strength = 0.1, aes(colour = edge_col),width = 2) + \n  geom_node_point(size = 4, colour = \"lightsteelblue\") +\n  theme(legend.position = \"none\")\n\n\n\n\nAnd that is the end of this reoworked tutorial! Hope you enjoyed it and found it useful!!"
  },
  {
    "objectID": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html",
    "href": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html",
    "title": "A TRIZ Analysis of Lawrence of Arabia",
    "section": "",
    "text": "The movie Lawrence of Arabia is justly famous for its terrific story, great cast of characters and actors, and some truly legendary scenes. Here I take one of the iconic scenes from the movie, the attack on the port town of Aqaba and interpret the entire event and the build up to it from a TRIZ viewpoint.\nI will give a short description of the movie plot first.\nI then discuss the source of the problem, how it occurred to Lawrence, and how he went about solving it in this dramatic fashion. I give an analysis of the Problem from a (classical) TRIZ perspective, including the formulation of the Contradiction, Identification of Causes, the statement of the Ideal Final Result, and finally using the TRIZ Contradiction Matrix to find Inventive Principles that inform Lawrence‚Äôs solution. I also dwell in passing upon aspects of how this story can be interpreted as a manifestation of Mihaly Csikszentmihalyi‚Äôs Creativity Systems Model.\nThis piece stems from my teaching a course on Creative Thinking and Problem Solving based on TRIZ, titled Play and Invent, over the past 8 years or more at the Srishti Manipal Institute of Art, Design, and Technology, Bangalore, India. (https://srishtimanipalinstitute.in)."
  },
  {
    "objectID": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html#lawrence-of-arabia-a-summary",
    "href": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html#lawrence-of-arabia-a-summary",
    "title": "A TRIZ Analysis of Lawrence of Arabia",
    "section": "Lawrence of Arabia: a Summary",
    "text": "Lawrence of Arabia: a Summary\nThe movie is the story of T.E. Lawrence, the English officer who successfully united and led the diverse, often warring, Arab tribes during World War I in order to fight the Turks. The stellar cast includes Peter O‚ÄôToole as Lawrence, Omar Sharif as Ali, Alec Guinness as Prince Feisal, Anthony Quinn as Auda Abu Tayi, Claude Raines as Dryden, and Anthony Quayle as Col. Brighton. The director was David Lean. The editing of the film by Anne Coates is also much admired. (https://womenfilmeditors.princeton.edu/tag/lawrence-of-arabia/)\nLawrence is a complex, talented, and yet simple man, who is extremely well read (Greek philosophy and the Koran, for example) and is also an expert in Arab affairs and has considerable skill at map-making. Due to his being interpreted as insolent and insubordinate , he is given a lowly job at the HQ in Cairo. Dryden manages to convince the General that Lawrence should be allowed to go into Arabia and to find out what kind of long-term plans Prince Feisal is making for Arabia.\nHere is the map of the events that are unfolding in the movie at this time.11¬†I was not able to ascertain who is the author of this map. I would be happy to write to obtain permission and use it with acknowledgement.\n\n\n\n\n\nLawrence encounters Ali in dramatic fashion at the Masturah Well, on the way to meet Feisal, and his Arab guide is shot by Ali, a direct experience for Lawrence of inter-tribe rivalry in Arabia. (Ali is a Harith, and Tafas the guide was a Hashemi). Lawrence peremptorily rejects an offer of help from Ali, and finds his way alone to Wadi Safra, where Feisal is camped. He is met by Col. Brighton as he nears the camp. Both enter camp just in time to witness another bombing raid by Turkish airplanes.\nLater in the meeting with Feisal, Brighton tries to convince Feisal to retreat to Yenbo (Yanbu) and be out of range for the Turks, and where the British Army would supply them, train them to fight against the Turks. Feisal reluctantly accepts this plan, though he would rather the British navy take the port city of Aqaba and supply his army from there. Brighton simply scoffs at that idea, because the Turkish have 12 inch guns at Aqaba and the British have other things to do.\nLawrence has already intrigued Feisal by completing a verse from the Koran as it was being read by Selim, the cleric. At the end of the meeting, Feisal confronts Lawrence alone, as to his intentions in Arabia and finds out, to his astonishment, that Lawrence has his own interpretation of what his tasks and loyalties were, and these did not necessarily coincide with those of Brighton. In fact, Lawrence is not in favour of the Arab Army‚Äôs retreat to Yenbo, as it would become one small part of the British Army. As a parting remark, Feisal says to Lawrence that the Arabs need what no man can provide, a miracle.\nHere is the video of that terrific tent meeting scene."
  },
  {
    "objectID": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html#lawrences-problem",
    "href": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html#lawrences-problem",
    "title": "A TRIZ Analysis of Lawrence of Arabia",
    "section": "Lawrence‚Äôs Problem",
    "text": "Lawrence‚Äôs Problem\nLawrence does not sleep that night. Provoked by Feisal‚Äôs parting remark, he sits up all night on a sand dune close to the camp, thinking about how Aqaba could be taken, since he wants the Arabs to continue fighting from where they were, and even advance if possible with British help. His detailed understanding of the Arabian geography, his knowledge of the Aqaba port and its fortifications, all come to the fore here. Aqaba is a port at the head end of a narrow gulf to the east of the Sinai Peninsula.\nIn the early morning, seemingly in a eureka moment, he decides that attacking Aqaba from the landward side would be a good solution, since the guns there could not be turned around.\nHere is Lawrence trying to convince Ali about this plan:\n\n\n\n\nLawrence does not inform Brighton of his plans, nor even Feisal. It is Ali who informs Feisal of this enterprise. Clearly, Lawrence does not consider Brighton as a member of his Field (as defined by Csikszentmihalyi in his Creativity Systems Model), but Feisal is a Field Member to Ali.\nApropos, the act of sitting up all night can be seen as the Incubation and Elaboration stages of the 5 Stages of Creativity from Csikszentmihalyi (Preparation, Incubation, Insight, Elaboration, Execution)."
  },
  {
    "objectID": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html#a-triz-analysis-of-the-plan-to-take-aqaba",
    "href": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html#a-triz-analysis-of-the-plan-to-take-aqaba",
    "title": "A TRIZ Analysis of Lawrence of Arabia",
    "section": "A TRIZ Analysis of the Plan to Take Aqaba",
    "text": "A TRIZ Analysis of the Plan to Take Aqaba\nFor a TRIZ workflow, we proceed as follows:\nFirst, using the method described in Open Source TRIZ, (https://www.youtube.com/watch?v=cah0OhCH55k), we identify knobs or parameters within the situation and see how turning these could lead to identifying a Cause for a Problem in the form of a Contradiction.\nHere below is a quick Ishikawa Diagram for this purpose:\n\n\n\n\n\n\n\n\nTurning the knobs/parameters in the Ishikawa Diagram, it seems that if the British allies attack Aqaba, they may win, BUT they may lose a few warships. If the Arabs want to attack, they are too small in number and have no warships, and hence their chances of success are very slim. Herein lies the Contradiction, which we can now specify as an Administrative Contradiction(AC) in plain English:\n\n\nAC: The Arabs need the British to supply them via Aqaba port. Aqaba has huge guns and they will sink the British ships in that narrow gulf if they try a naval attack. So the Arabs need to take Aqaba without losing British ships.\n\n\nNext, based on this Contradiction and the inspection of the Ishikawa Diagram above, we are now ready to define the Ideal Final Result:\n\n\nIFR: The Arabs need to attack and take Aqaba port, and the big guns there should have no effect.\n\n\nNote how the tone of this IFR is like a ‚Äúeat my cake and have it too‚Äù. Very typical for IFRs, this impossible-sounding tone!\nLet us take the AC and convert it into a Technical Contradiction(TC). We will look at the 48 TRIZ Parameters in the TRIZ Contradiction Matrix (PDF) and see which Parameter we want to improve, while not worsening another. Here is what we can attempt, stating the Contradiction both ways2:2¬†The Contradiction Matrix is not quite symmetric, so stating the Contradiction both ways allows us to access a slightly larger set of Inventive Principles from two cells of the Matrix.\n\n- TC 1: Increase Duration of Action of a Moving Object (12) and not worsen Stability of Objects Composition (21)- TC 2: Increase Stability of Objects Composition (21) and not worsen Duration of Action of a Moving Object (12)\n\nHere we choose these Parameters based on our IFR that the guns at Aqaba should not affect the Arab attack at all. The Parameters chosen from the TRIZ Matrix can be thought of as metaphors for the knobs that lie within our AC. Going from the AC to the TC is an act of making metaphors. We could easily have chosen the Parameter Power(18) or Illumination Intensity(23) to ‚Äúmetaphorize‚Äù the effectiveness of the attack, if our imaginations run in that direction. There is here a considerable flexibility and possibility for imaginative interpretations of the AC, but using the language of TRIZ.\nWe could even stretch to making a Physical Contradiction(PC)3 happen:3¬†Arriving at Physical Contradictions is not always easy! If we can, then there are a very crisp set of TRIZ Separation Principles that we can apply to solve the Problem.\n\n\nPC: The Ships must be near the guns but not be near enough to be shot at. (They must be near and not near at the same time)"
  },
  {
    "objectID": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html#solving-the-technical-contradiction",
    "href": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html#solving-the-technical-contradiction",
    "title": "A TRIZ Analysis of Lawrence of Arabia",
    "section": "Solving the Technical Contradiction",
    "text": "Solving the Technical Contradiction\nLet us take the both the TC-s into the Contradiction Matrix and arrive at the list of TRIZ Inventive Principles. Here is the Matrix solution for TC-1 in the figure below:\n\n\n\n\n\n\n\n\nThe Inventive Principles are:(TC1)\n\nIP 13 (The Other Way Around)\n\nIP 35 (Parameter Change)\n\nIP 24 (Intermediary)\n\nIP 40 (Composite Materials)\n\nand (TC2)\n\nIP 10 (Preliminary Action)\n\nIP 5 (Merging)\n\nIP 35 (Parameter Change)\n\nIP 13 (The Other Way Around)\n\nHow are we to apply these Inventive Principles? Here again is an imaginative exercise as we map these Generalized Solutions back into the Problem at hand:\n\nIP 13: The Other Way Around. How? Not attack by sea? Wait‚Ä¶ATTACK BY LAND!! Change the DIRECTION of Attack! So attack from the other side, the land side!! (We could retrospectively add this parameter to the Ishikawa Diagram too). Will this work? Yes, the guns can‚Äôt turn around !!\nIP 35: Parameter Change. But ‚Äúships‚Äù on land?? Note, the desert is an ocean into which no oar is dipped. Sand and Water are both Resources in the problem, as we have duly noted in the Ishikawa Diagram. So a different kind of ocean and therefore a different kind of ship? At a stretch, we can say the warships of the British Navy are being substituted with the use of ‚Ä¶.Camels!! And, metaphorically speaking, it is still an attack using ships‚Ä¶.The Ships of the Desert!! Parameter Change.\nIP 10: Prior Action. How? Lawrence and Ali are far from Aqaba and cannot do anything ‚Äúin advance‚Äù. What could this be?\nIP 5: Merging. However, on the way to Aqaba, Lawrence and Ali must recruit the Howeitat tribe ‚Äúin advance‚Äù of their attack !! As Lawrence tells Ali, If 50 men came out of the Nefud Desert, they might be 50 men other men would join. This is in accordance with what IP 10 is suggesting, to get other tribes to join in, in advance of the attack.\nIP 40: Composite Materials. What object within the situation can we reconstitute with smaller pieces of different types? The British Army‚Ä¶so an army made up of pieces? Yes! The Tribes need to unite into one composite army.\nAnd, instead of large warships, the Arabs switch to a composite force with camels‚Ä¶\n\nSo IP 13 works nicely now, along with IP 35 and IP 40, to give us a camel-borne attack from the landward side. IP 10 also teams up with IP 40 and IP 5 to give the idea of tribe unification.\nAnd so Lawrence and Ali, with the help of Auda Abu Tayi, attack Aqaba port from the landward side by crossing the Nefud desert on camels, and take it! And we have justified their decision using TRIZ !!\nHere is the final solution in action !!\n\n\n\n\n I hope that was as much fun to read as it was for me to write it up !!"
  },
  {
    "objectID": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html#points-to-ponder",
    "href": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html#points-to-ponder",
    "title": "A TRIZ Analysis of Lawrence of Arabia",
    "section": "Points to Ponder",
    "text": "Points to Ponder\n\nDo we each of us need a Dryden to vouch for us and help us get access to the Field?\nDoes TRIZ work in both mundane and industrial contexts? (Yes of course!)\nCan we just take the 40 Inventive Principles directly and throw them at every Problem, without necessarily going through the process of creating Contradictions and IFR? Hipple‚Äôs book has a remark in this direction."
  },
  {
    "objectID": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html#references",
    "href": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.en.html#references",
    "title": "A TRIZ Analysis of Lawrence of Arabia",
    "section": "References",
    "text": "References\n\nLawrence of Arabia at the Internet Movie Data Base https://www.imdb.com/title/tt0056172/\n\nMihaly Csikszentmihalyi, Creativity, Flow, and the Psychology of Discovery and Invention. Harper Perennial; Reprint edition (August 6, 2013)\nJack Hipple, The Ideal Result and How to Achieve It. Springer; 2012th edition (June 26, 2012)\nValery Souchkov, Defining Contradictions. http://www.xtriz.com/Training/TRIZ_DefineContradiction_Tutorial.pdf"
  },
  {
    "objectID": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html",
    "href": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html",
    "title": "The TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad",
    "section": "",
    "text": "Here we go with another of my TRIZ Chronicles. The first edition analysing the famous movie Lawrence of Arabia is here.\nThe recent series of ads by Spotify have been very well received, by the ad industry as well. There have been several of these ads, each with a solid little story and a great punch line. There is one popular one that I have selected here, featuring a Mother shopping for veggies, while her son watches (and ‚Äúlistens‚Äù) to her bargaining with the shopkeeper. Here I take just that highly effective ad and interpret it from a TRIZ viewpoint.\nThis piece stems from my teaching a course on Creative Thinking and Problem Solving based on TRIZ, titled Play and Invent, over the past 8 years or more at the Srishti Manipal Institute of Art, Design, and Technology, Bangalore, India. (https://srishtimanipalinstitute.in)."
  },
  {
    "objectID": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#the-famous-spotify-ad",
    "href": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#the-famous-spotify-ad",
    "title": "The TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad",
    "section": "The Famous Spotify Ad",
    "text": "The Famous Spotify Ad\nLet us watch the Spotify ad first, before analyzing it!"
  },
  {
    "objectID": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#the-young-mans-problem",
    "href": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#the-young-mans-problem",
    "title": "The TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad",
    "section": "The Young Man‚Äôs Problem",
    "text": "The Young Man‚Äôs Problem\nIn order to make a story out of this, I want make a Protagonist out the young man in the ad. It is he who has the problem and he who is going to apply TRIZ to solve it. I discuss the source of his Problem and give an analysis of the Problem from a (classical) TRIZ perspective, including the formulation of the Contradiction, Identification of Causes, the statement of the Ideal Final Result, and finally using the TRIZ Contradiction Matrix to find Inventive Principles, that lead to the solution, which of course, is meant to unerringly include Spotify !\nFirst a philosophical digression:‚Äî\nSeveral authors have taken a Game View of life. James P Carse‚Äôs famous book titled Finite and Infinite Games speaks of Play, Types of Games, Rules, Winning and our own aims in the Game itself. A similar articulation is, in my opinion, that of Mihaly Csikszentlmihalyi in his concept of Flow, shown here below:\n\n\nFrom Sketchplanations\n\n\nWhen the Game presents very little Challenge, we are bored. When the Game demands extreme skills the challenge is too much for us and we experience anxiety. When the Challenge presented is just barely matched by our Skill, we are in the zone of Flow, or what I call Play.\nA good metaphoric image for this experience is as follows:‚Äî that we live in a space where the Floor of Boredom is always rising and would crush us against our Ceiling of Anxiety. One Way to deal with this is to develop more Skills and push the Ceiling away, effectively moving into the zone of Flow. Another Way of looking at this is what Carse suggests: When Play is no longer possible, change the Game.\nSo what does all this have to do with getting veggies?\nThe ad is, in my opinion, all about Boredom, and how to avoid it. And not offend anybody. The Young Man (hereinafter, ‚ÄúYM‚Äù) simply has to accompany his Mom, and be there while she gets the veggies. I will exaggerate his irritation and his boredom at the risk of offending young people likely to read this, and say that he would rather not be there but he does not want to hurt Mom.\nWe are now ready for the TRIZ based Analysis of this Problem!"
  },
  {
    "objectID": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#a-triz-analysis-of-a-visit-to-the-subzi-mandi",
    "href": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#a-triz-analysis-of-a-visit-to-the-subzi-mandi",
    "title": "The TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad",
    "section": "A TRIZ Analysis of a Visit to the Subzi Mandi",
    "text": "A TRIZ Analysis of a Visit to the Subzi Mandi\nFor a TRIZ workflow, we proceed as before:\nFirst, using the method described in Open Source TRIZ, we identify knobs or parameters within the situation and see how turning these could lead to identifying a Cause for a Problem in the form of a Contradiction.\nHere below is a quick Ishikawa Diagram to help us identify the Parameters of this Problem:\n\n\n\n\n\n\n\n\nTurning the knobs/parameters in the Ishikawa Diagram, it seems that if the YM goes to the market with Mom, he would most likely get bored, but would please Mom. If he doesn‚Äôt go, then he chills at home, but Mom is going to justifiably furious. Herein lies the Contradiction, which we can now specify as an Administrative Contradiction(AC) in plain English:\n\n\nAC: The YM wants to chill at home but Mom wants him to take her veggie shopping. He has to put up with the Waste of Time, and being bored, and Stress at being away from friends.\n\n\nNext, based on this Contradiction and the inspection of the Ishikawa Diagram above, we are now ready to define a TRIZ Ideal Final Result:\n\n\nIFR: The YM must go to the Market and not be bored.\n\n\nNote again the impossible sounding way of expressing the IFR! One needs practice, like the Queen in Alice in Wonderland, who could think of Six Impossible Things before Breakfast ! Also note there could be other ways of specifying the IFR. See below, section Alternative Ideas for IFR.\nLet us take the AC and convert it into a Technical Contradiction(TC), keeping this IFR in mind. We will look at the 48 TRIZ Parameters in the TRIZ Contradiction Matrix (PDF) and see which Parameter we want to improve, while not worsening another. Here is what we can obtain. We will analyze each Contradiction both ways1:1¬†The Contradiction Matrix is not quite symmetric, so stating the Contradiction both ways allows us to access a slightly larger set of Inventive Principles from two cells of the Matrix.\n\n- TC 1: Improve Loss of Time (26) and not worsen Effect of External Harmful Factors (30)\n- TC 2: Improve Increase Productivity (44) and not worsen Stress (19)\n\nHere we choose these Parameters based on our IFR that while going to the Market may be unavoidable, Boredom need not ensue. Parameters chosen from the TRIZ Matrix can be thought of as metaphors for the knobs that lie within our AC. Going from the AC to the TC is an act of making metaphors. We could easily have chosen the Parameter Noise(29) as the ‚Äúmetaphoric thing‚Äù to avoid, but the current IFR doesn‚Äôt quite support that. There is here a considerable flexibility and possibility for imaginative interpretations of the AC, but using the language of TRIZ.\nWe could also formulate a Physical Contradiction(PC)2:2¬†Arriving at Physical Contradictions is not always easy! If we can, then there are a very crisp set of TRIZ Separation Principles that we can apply to solve the Problem.\n\n\nPC: The YM must be in the market and not be in the market at the same time.\n\n\nwhich is aimed squarely at one of the Assumptions in the Problem, that the YM simply has to go. Again, if the IFR is formulated differently we could obtain a very different set of AC and PC. See below, section Alternative Ideas for IFR.\nIn a future post, we will deal with using the PC and the TRIZ Separation Principles to solve Problems."
  },
  {
    "objectID": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#solving-the-technical-contradiction",
    "href": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#solving-the-technical-contradiction",
    "title": "The TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad",
    "section": "Solving the Technical Contradiction",
    "text": "Solving the Technical Contradiction\nLet us take the both the TC-s into the Contradiction Matrix and arrive at the list of TRIZ Inventive Principles. Here is the Matrix solution for TC-1 in the figure below:\n\n\n\n\n\n\n\n\n\nThe two squares for the TC1 have been circled in red, solving TC-1.\nThe Inventive Principles are:(TC1, TC2, both ways)\n\n1(Segmentation)\n35(Parameter Change)\n21(Skipping)\n18(Mechanical Vibration) (!!)\n2(Taking Out/Separation)\n10(Prior Action)\n\n36(Phase Transitions)\nand with TC2:\n\n3(Local Quality)\n14(Spheroidality/Curvature)\n9(Preliminary Anti-Action)\n37(Thermal Expansion)\n40(Composite Materials)\n25(Self Service)\n24(Intermediary)\n\nThat is a considerable list for us to try to use!! Let us apply some these Inventive Principles! Viewing these Inventive Principles as we Generalized Solutions we try to map these back into the Problem at hand:\n\n\n35(Parameter Change): Which Parameter to change? Location? No. Sound? Change the ‚ÄúBargaining Talk‚Äù into what? Sweet Musical Lyrics!!üéµü§£\n\n18(Mechanical Vibration) : What, make noise of your own? Yes! Play Music !!üîâ ü§£\n\n14(Spheroidality): Wear ‚Äúspherical‚Äù headphones!!üéß! Create a ‚Äúsound sphere‚Äù! This is a long shot!!\n\n3(Local Quality): also indicates the creation of a ‚Äúlocal‚Äù cocoon around the YM, but needs to be combined with 18(Mechanical Vibration) to truly arrive at the musical solution!\n\nOne could make decent interpretations of 2(Taking Out/Separation), and 24(Intermediary), but we are already there! The rest are perhaps (at least to me!) not very evocative, unless 37(Thermal Expansion) means ‚Äúthrow a temper tantrum at Mom‚Äù? Never! So there you have it! The Cinderella song played on Spotify becomes not just a noise canceller but actually seems to substitute the very conversation between Mom and the vendor. And the YM has successfully attained Flow ! And the IFR too, since with the music in his head, he is effectively ‚Äúin the marketplace and not in the marketplace at the same time!\nAnd I attained Flow in writing this!!"
  },
  {
    "objectID": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#alternative-ideas-for-ifr",
    "href": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#alternative-ideas-for-ifr",
    "title": "The TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad",
    "section": "Alternative Ideas for IFR",
    "text": "Alternative Ideas for IFR\nWe note in passing that there is more than one way of formulating the Ideal Final Result. Here are two more examples:\n\n\nIFR2: The veggies should arrive without (the YM) going to the Market\nIFR3: Food should be prepared without having to go buy veggies.\n\n\nClearly these are at least as good as the one we have chosen, sounding nicely ‚Äúimpossible‚Äù in their own right! The point is that in the analysis of the Problem, we do need to ask Who has the Problem, as we did, and the IFR needs to stem from there. These alternative IFRs could well be the Voice of (another) Customer.\nIf there is any interesting situation that could be analyzed with TRIZ, please send me a DM! Thanks !"
  },
  {
    "objectID": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#references",
    "href": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#references",
    "title": "The TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad",
    "section": "References",
    "text": "References\n\nJames P Carse, Finite and Infinite Games, Free Press, 1986. ISBN: 0-02-905980-1\nMihaly Csikszentmihalyi, Creativity, Flow, and the Psychology of Discovery and Invention. Harper Perennial; Reprint edition (August 6, 2013)\nJack Hipple, The Ideal Result and How to Achieve It. Springer; 2012th edition (June 26, 2012)\nValery Souchkov, Defining Contradictions. http://www.xtriz.com/Training/TRIZ_DefineContradiction_Tutorial.pdf"
  },
  {
    "objectID": "content/projects/2023-11-03-Owind/index.html",
    "href": "content/projects/2023-11-03-Owind/index.html",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the O-Wind Turbine",
    "section": "",
    "text": "Here we go with another of my TRIZ Chronicles ! The earlier editions are here: Lawrence of Arabia and Spotify.\nThis is another piece stems from my teaching a course on Creative Thinking and Problem Solving based on TRIZ, titled Play and Invent, over the past 8 years or more at the Srishti Manipal Institute of Art, Design, and Technology, Bangalore INDIA."
  },
  {
    "objectID": "content/projects/2023-11-03-Owind/index.html#a-triz-analysis-of-the-dyson-o-wind-generator",
    "href": "content/projects/2023-11-03-Owind/index.html#a-triz-analysis-of-the-dyson-o-wind-generator",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the O-Wind Turbine",
    "section": "A TRIZ Analysis of the Dyson O-Wind Generator",
    "text": "A TRIZ Analysis of the Dyson O-Wind Generator\nFor a TRIZ workflow, we proceed as before:\n\nFirst, using the method described in Open Source TRIZ, we identify knobs or parameters within the situation\nWe see how turning these could lead to identifying a Statement / Cause for a Problem in the form of a Contradiction.\nRe-word the plain English Contradiction into TRIZ Parameters and look it up in the Contradiction Matrix. Obtain the Inventive Principles.\nApply these Inventive Principles into your Problem and solve it.\n\nIn the video itself, we heard about how electrical power consumption centers are the urban areas and these are far away from the generation sites. Local generation is a good idea to reduce these costs.\nWhat would be the problems with using Wind based power generation around the home? Here below is a quick Ishikawa Diagram to help us identify the Parameters of this Problem:\n\n\n\n\n\nLooking at this Diagram, with the aspects identified, we could pair them off and see how they affect one another. In doing so, we could make up several problem. Let us state at some of our Problems: I have marked some of these with question marks since I am using imagination here and not direct primary research or information to formulate these. Note that some these may sound very naive, but that is way to start!\n\nI would like to have access the the OWind but it needs to be not too close to the walls for it to harness the wind?\nHow to tap the power from the generator? What if the connection wires get twisted? ( (Do I need a [commutator](https://en.wikipedia.org/wiki/Commutator_(electric)) like arrangement?)\n\nas an Administrative Contradiction(AC) in plain English:\n\n\nAC: We wish to pull out plastics from the river without interrupting other human activities (economic / recreational / transportation )\n\n\nNext, based on this Contradiction and the inspection of the Ishikawa Diagram above, we are now ready to define a TRIZ Ideal Final Result:\n\n\nIFR:The River must flow. The Plastics must not.\n\n\nShort and pithy.\nWe could do alternative Problem Definitions too, depending upon what our focus was. (Our focus here is the river). The way we would look at the Parameters in the Ishikawa Diagram would be different in each case. More on this in just a bit!\nLet us take our AC and convert it into a Technical Contradiction(TC), keeping this IFR in mind. We will look at the 48 TRIZ Parameters in the TRIZContradictionMatrix (PDF) and see which Parameter we want to improve, while not worsening another. Here is what we can obtain. We will analyze the Contradiction both ways1:1¬†The Contradiction Matrix is not quite symmetric, so stating the Contradiction both ways allows us to access a slightly larger set of Inventive Principles from two cells of the Matrix.\n\n- TC 1: Increase Volume of Stationary Object(8) and not worsen Harmful effects acting on the system(40)\n- TC 2: Improve Harmful effects acting on the system(40) and not worsen Volume of Stationary Object(8)\n\nHere we choose these Parameters based on our IFR, which also reflects that we choose the make the river our focus. We want to increase the volume of the stationary plastic in the river, while not worsening the harmful effects acting on the river. Note how the IFR is included here, in using the word stationary for the object: the plastic must not flow, while the river must.\nAs stated in my previous articles ( Lawrence of Arabia and Spotify), there is considerable flexibility and possibility for imaginative interpretations of the AC, using the language of TRIZ, the 48 Parameters in the Contradiction Matrix. Parameters chosen from the TRIZ Matrix can be thought of as metaphors for the knobs that lie within our AC; Going from the AC to the TC is an act of making metaphors. We could easily have chosen the Parameter Amount of Substance (10) i.e.¬†the plastic on the river, as the ‚Äúmetaphoric thing‚Äù to improve but the current IFR speaks of the river. Or we could choose Aesthetics and Appearance (39) as our target, leading to different solutions perhaps, but in a focussed manner. TRIZ tends to focus our attention like no other method that I know of.\nWe could also formulate a Physical Contradiction(PC)2:2¬†Arriving at Physical Contradictions is not always easy! If we can, then there are a very crisp set of TRIZ Separation Principles that we can apply to solve the Problem.\n\n\nPC: The Plastic must be in the River and not in the River at the same time.\n\n\nwhich is aimed at the river, as with the IFR. Again, if the IFR is formulated differently we could obtain a very different set of AC and PC."
  },
  {
    "objectID": "content/projects/2023-11-03-Owind/index.html#solving-the-technical-contradiction",
    "href": "content/projects/2023-11-03-Owind/index.html#solving-the-technical-contradiction",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the O-Wind Turbine",
    "section": "Solving the Technical Contradiction",
    "text": "Solving the Technical Contradiction\nLet us take the both the TC-s into the Contradiction Matrix and arrive at the list of TRIZ Inventive Principles. Here is the Matrix solution for TC-1 in the figure below; The square for solutions to TC-1 has been circled in red.\n\nHere is what the Matrix suggests:\nFor TC-1:\n\n39(Inert Atmosphere)\n24(Intermediary)\n19(Periodic Action)\n\n27(Cheap Short-Living Objects) (!!)\nand with TC-2:\n\n5(Merging)\n17(Another Dimension)\n39(Inert Atmosphere)\n19(Periodic Action)\n\nThis is neat list for us to try to use!! Let us apply some these Inventive Principles! Viewing these Inventive Principles as we Generalized Solutions we try to map these back into the Problem at hand:\n\n\n(39)Inert Atmosphere: Hmm‚Ä¶what can be inert in the River? What could ‚Äúinert‚Äù mean here? The River should be inert‚Ä¶so must not flow! OK, so a place where it must be still, perhaps!!\n\n24(Intermediary): OK, something between the River and the Plastic. But‚Ä¶we don‚Äôt want nets or dams or weirs‚Ä¶then what? Need to think!!\n\n19(Periodic Action): What can be ‚Äúperiodic‚Äù in the River? Waves? But how do we make waves? Need to think some more!!\n\n27(Cheap Short Living Objects): What can be cheap short and short living in the River water? Not fish, surely‚Ä¶wait, BUBBLES!!!\n\nSo (breathlessly): we create bubbles in the water and use them as an Intermediary to create waves/barriers that nudge the Plastic where we want it it to be, for collection. And of course bubbles are both Inert and Atmosphere, literally! And they are Cheap and Short-Lived so we need to regenerate them Periodically! We can periodically sweep the river with a broom made of bubbles and not seriously disturb any other activity. Waah TRIZ, waah!\n5(Merging) is quite easy to interpret now, in retrospect. 17(Another Dimension) is also evocative and powerful as a Solution: the River flows horizontally but the barrier we need to create is vertical. Yet another interpretation could be that the Bubbles dissolve in the River water (assuming another dimension, as it were) and give us the benefit of purifying the water too, as fountains do!!\nSo there you have it! It seems that Going Dutch even with bubbles, is a good idea !"
  },
  {
    "objectID": "content/projects/2023-11-03-Owind/index.html#using-triz-separation-principles",
    "href": "content/projects/2023-11-03-Owind/index.html#using-triz-separation-principles",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the O-Wind Turbine",
    "section": "Using TRIZ Separation Principles",
    "text": "Using TRIZ Separation Principles\nAs Hipple explains, there is frequently an underlying physical parameter, such as length, breadth, weight, or energy, or speed for example that lies at the root of our Technical Contradiction.\nOur IFR states that we want the Plastics to not flow, and the River to flow at the same time. But the Plastics are in the River! This is a sort of a Physical Contradiction!\nCan we make the River ‚Äúless of a River‚Äù in places? Can we make separate zones in the River that flow and not flow, and allow us to harvest the plastic?Indeed,our Inventive Principles such as (39)Inert Atmosphere and 24(Intermediary) suggest such a ‚Äúspatial‚Äù separation. So we are applying SEPARATION in SPACE here, to solve a Physical Contradiction related to ‚Äútime‚Äù. TRIZ offers us a smaller and more easily ‚Äúmemorizable‚Äù set of metaphoric solutions in the form of Separation Principles. Here they are:\n\nSeparation in Time\nSeparation in Space\nSeparation on Condition\nSeparation between Parts and the Whole\n\nNote that while these Separation Principles are just a handful to memorize, they are in my opinion, a little harder to apply straightway. But then practice would help us. Note that all the Inventive Principles in the Matrix can be, and have been, classified as to whether they hew to a particular kind of Separation Principle.\nIf there is any interesting situation that could be analyzed with TRIZ, please send me a DM! Thanks !"
  },
  {
    "objectID": "content/projects/2023-11-03-Owind/index.html#references",
    "href": "content/projects/2023-11-03-Owind/index.html#references",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the O-Wind Turbine",
    "section": "References",
    "text": "References\n\nMeijer, J.J.L, Emmerik, T., Ent, R., Schmidt, C., Lebreton, L. (2021). More than 1000 rivers account for 80% of global riverine plastic emissions into the ocean. Science Advances. https://www.science.org/doi/10.1126/sciadv.aaz5803\nJack Hipple, The Ideal Result and How to Achieve It. Springer; 2012th edition (June 26, 2012)\nValery Souchkov, Defining Contradictions. http://www.xtriz.com/Training/TRIZ_DefineContradiction_Tutorial.pdf\nOpen Source TRIZ: Making Contradictions. https://www.youtube.com/watch?v=cah0OhCH55k"
  },
  {
    "objectID": "content/projects/DSU/A1.html#instructions",
    "href": "content/projects/DSU/A1.html#instructions",
    "title": "A1",
    "section": "Instructions",
    "text": "Instructions\n\nEach Question in this Assignment is a chart.\nEach Chart is accompanied by a set of short questions.\nYour responses to these can be R-code, or text.\nPlease number your answers as 1.a, 1.b, 1.c‚Ä¶..2.a, 2.b‚Ä¶on your Answer Sheet.\nAll aboard? Let‚Äôs go!"
  },
  {
    "objectID": "content/projects/DSU/A1.html#question-1",
    "href": "content/projects/DSU/A1.html#question-1",
    "title": "A1",
    "section": "Question 1",
    "text": "Question 1\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/projects/DSU/A1.html#question-1-1",
    "href": "content/projects/DSU/A1.html#question-1-1",
    "title": "A1",
    "section": "Question 1",
    "text": "Question 1\n\n\n\nWhat is the ggplot geometry used in this graph?\nWhat does the legend show?"
  },
  {
    "objectID": "content/projects/DSU/A1.html#question-2",
    "href": "content/projects/DSU/A1.html#question-2",
    "title": "A1",
    "section": "Question 2",
    "text": "Question 2\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/projects/DSU/A1.html#question-3",
    "href": "content/projects/DSU/A1.html#question-3",
    "title": "A1",
    "section": "Question 3",
    "text": "Question 3\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/projects/DSU/A1.html#question-4",
    "href": "content/projects/DSU/A1.html#question-4",
    "title": "A1",
    "section": "Question 4",
    "text": "Question 4\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/projects/DSU/A1.html#question-5",
    "href": "content/projects/DSU/A1.html#question-5",
    "title": "A1",
    "section": "Question 5",
    "text": "Question 5\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/projects/DSU/A1.html#question-6",
    "href": "content/projects/DSU/A1.html#question-6",
    "title": "A1",
    "section": "Question 6",
    "text": "Question 6\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/projects/DSU/A1.html#question-7",
    "href": "content/projects/DSU/A1.html#question-7",
    "title": "A1",
    "section": "Question 7",
    "text": "Question 7\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/projects/DSU/A1.html#question-8",
    "href": "content/projects/DSU/A1.html#question-8",
    "title": "A1",
    "section": "Question 8",
    "text": "Question 8\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/projects/DSU/A1.html#question-9",
    "href": "content/projects/DSU/A1.html#question-9",
    "title": "A1",
    "section": "Question 9",
    "text": "Question 9\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/projects/DSU/A1.html#question-10",
    "href": "content/projects/DSU/A1.html#question-10",
    "title": "A1",
    "section": "Question 10",
    "text": "Question 10\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/projects/DSU/A1.html#question-11",
    "href": "content/projects/DSU/A1.html#question-11",
    "title": "A1",
    "section": "Question 11",
    "text": "Question 11\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/projects/DSU/A1.html#question-12",
    "href": "content/projects/DSU/A1.html#question-12",
    "title": "A1",
    "section": "Question 12",
    "text": "Question 12\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/projects/DSU/A1.html#question-13",
    "href": "content/projects/DSU/A1.html#question-13",
    "title": "A1",
    "section": "Question 13",
    "text": "Question 13\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/projects/DSU/A1.html#question-14",
    "href": "content/projects/DSU/A1.html#question-14",
    "title": "A1",
    "section": "Question 14",
    "text": "Question 14\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/projects/DSU/A1.html#question-15",
    "href": "content/projects/DSU/A1.html#question-15",
    "title": "A1",
    "section": "Question 15",
    "text": "Question 15\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment#1-IA-EMBA-T4-2023, Arvind Venkatadri"
  },
  {
    "objectID": "content/projects/DSU/QP.html",
    "href": "content/projects/DSU/QP.html",
    "title": "QP",
    "section": "",
    "text": "‚ÄúOnline Retail‚Äù dataset: This dataset contains transactional data from an online retailer, including information on customers, products, and sales. You can download the dataset from the UCI Machine Learning Repository using the following link: https://archive.ics.uci.edu/ml/datasets/Online+Retail.\n‚ÄúBank Marketing‚Äù dataset: This dataset contains information on various attributes of customers in a banking context, as well as their corresponding responses to marketing campaigns (yes or no). You can download the dataset from the UCI Machine Learning Repository using the following link: https://archive.ics.uci.edu/ml/datasets/Bank+Marketing.\n‚ÄúAirbnb‚Äù dataset: This dataset contains information on various attributes of Airbnb listings in several cities, as well as their corresponding rental prices. You can download the dataset from the Inside Airbnb website using the following link: http://insideairbnb.com/get-the-data.html.\n‚ÄúCredit Card Fraud Detection‚Äù dataset: This dataset contains credit card transactions labeled as either fraudulent or non-fraudulent. You can download the dataset from the Kaggle website using the following link: https://www.kaggle.com/mlg-ulb/creditcardfraud.\n‚ÄúHuman Resources Analytics‚Äù dataset: This dataset contains information on various attributes of employees in a company, as well as their corresponding employee retention status. You can download the dataset from the Kaggle website using the following link: https://www.kaggle.com/ludobenistant/hr-analytics.\n‚ÄúChurn Analysis‚Äù dataset: This dataset contains information on various attributes of customers in a telecom company, as well as their corresponding churn status. You can download the dataset from the Kaggle website using the following link: https://www.kaggle.com/becksddf/churn-in-telecoms-dataset.\n‚ÄúMarketing Analytics‚Äù dataset: This dataset contains information on various attributes of customers in a marketing context, as well as their corresponding purchase behavior. You can download the dataset from the Kaggle website using the following link: https://www.kaggle.com/pankajjsh06/ibm-watson-marketing-customer-value-data.\n‚ÄúCustomer Segmentation‚Äù dataset: This dataset contains customer transactional data from an online grocery store, and can be used for segmentation analysis. You can download the dataset from the UCI Machine Learning Repository using the following link: https://archive.ics.uci.edu/ml/datasets/Online+Retail+II.\n‚ÄúSales Analytics‚Äù dataset: This dataset contains information on various attributes of sales transactions in a retail context, including customer demographics, product attributes, and sales figures. You can download the dataset from the Kaggle website using the following link: https://www.kaggle.com/kyanyoga/sample-sales-data.\n‚ÄúStock Price Prediction‚Äù dataset: This dataset contains historical stock prices for a variety of companies, and can be used to predict future prices. You can download the dataset from the Kaggle website using the following link: https://www.kaggle.com/borismarjanovic/price-volume-data-for-all-us-stocks-etfs.\n‚ÄúTitanic‚Äù dataset: This dataset contains information on passengers aboard the Titanic, including demographic information and survival status. You can download the dataset from the Kaggle website using the following link: https://www.kaggle.com/c/titanic.\n‚ÄúBoston Housing‚Äù dataset: This dataset contains information on various attributes of houses in Boston, as well as their corresponding median values. You can load the dataset directly in R using the following code: data(boston).\n\n‚ÄúBreast Cancer Wisconsin‚Äù dataset: This dataset contains information on various characteristics of breast cancer cells, as well as their corresponding diagnoses (benign or malignant). You can load the dataset directly in R using the following code: data(wdbc, package = \"mclust\").\n\n‚ÄúAbalone‚Äù dataset: This dataset contains information on various physical measurements of abalone, as well as their corresponding ages. You can download the dataset from the UCI Machine Learning Repository using the following link: https://archive.ics.uci.edu/ml/datasets/Abalone.\n‚ÄúHeart Disease‚Äù dataset: This dataset contains information on various risk factors for heart disease, as well as their corresponding diagnoses (presence or absence of heart disease). You can download the dataset from the UCI Machine Learning Repository using the following link: https://archive.ics.uci.edu/ml/datasets/Heart+Disease.\n‚ÄúMushroom‚Äù dataset: This dataset contains information on various characteristics of mushrooms, as well as their corresponding edibility. You can download the dataset from the UCI Machine Learning Repository using the following link: https://archive.ics.uci.edu/ml/datasets/Mushroom.\n‚ÄúWine‚Äù dataset: This dataset contains information on various physicochemical properties of wines from three different cultivars, as well as their corresponding cultivars. You can load the dataset directly in R using the following code: data(wine).\n‚ÄúFashion MNIST‚Äù dataset: This dataset contains images of various types of clothing, as well as their corresponding labels. You can download the dataset from the Kaggle website using the following link: https://www.kaggle.com/zalando-research/fashionmnist.\n‚ÄúCar Evaluation‚Äù dataset: This dataset contains information on various attributes of cars, as well as their corresponding evaluations (unacceptable, acceptable, good, or very good). You can download the dataset from the UCI Machine Learning Repository using the following link: https://archive.ics.uci.edu/ml/datasets/Car+Evaluation.\n‚ÄúCervical Cancer‚Äù dataset: This dataset contains information on various risk factors for cervical cancer, as well as their corresponding diagnoses (presence or absence of cervical cancer). You can download the dataset from the UCI Machine Learning Repository using the following link: https://archive.ics.uci.edu/ml/datasets/Cervical+cancer+%28Risk+Factors%29.\n‚ÄúForest Fires‚Äù dataset: This dataset contains information on various meteorological and other factors that contribute to forest fires, as well as their corresponding burned area. You can download the dataset from the UCI Machine Learning Repository using the following link: https://archive.ics.uci.edu/ml/datasets/Forest+Fires.\n‚ÄúConcrete Compressive Strength‚Äù dataset: This dataset contains information on various factors that contribute to the compressive strength of concrete, as well as their corresponding strength values. You can download the dataset from the UCI Machine Learning Repository using the following link: https://archive.ics\nThe ‚ÄúOnline News Popularity‚Äù dataset, which is available on the UCI Machine Learning Repository. This dataset contains information about articles published by Mashable, a popular online news website, and includes features such as the article‚Äôs title, text, and number of images and videos, as well as the article‚Äôs popularity (measured by the number of shares on social media). To download the dataset in R, you can use the following url: https://archive.ics.uci.edu/ml/machine-learning-databases/00332/OnlineNewsPopularity.zip\n\n\nExploratory data analysis: Explore the dataset using various visualization techniques in R / Radiant / Orange (e.g., histograms, boxplots, scatterplots, etc.). Identify any potential outliers, missing data, or other data quality issues. Summarize the main characteristics of the dataset.\nStatistical model development: Choose a response variable of interest (e.g., wine quality, marketing campaign outcome, iris species) and build a linear regression model to predict this variable based on the other variables in the dataset. Evaluate the performance of the model using appropriate metrics (e.g., mean squared error, R-squared, etc.). Discuss the strengths and limitations of the model.\nBasic ML algorithms: Choose a response variable of interest (e.g., wine quality, marketing campaign outcome, iris species) and build a classification model using a basic ML algorithm in R (e.g., decision tree and random forests.). Evaluate the performance of the model using appropriate metrics (e.g., accuracy, precision, recall, F1-score, etc.). Discuss the strengths and limitations of the model."
  },
  {
    "objectID": "content/projects/fsp-discussions/index.html",
    "href": "content/projects/fsp-discussions/index.html",
    "title": "FSP Discussions 2021",
    "section": "",
    "text": "Slides and Tutorials\n.nbsp;.nbsp;"
  },
  {
    "objectID": "content/projects/fsp-discussions/index.html#introduction",
    "href": "content/projects/fsp-discussions/index.html#introduction",
    "title": "FSP Discussions 2021",
    "section": "Introduction",
    "text": "Introduction"
  },
  {
    "objectID": "content/projects/fsp-discussions/index.html#this-is-a-summary-of-the-group-discussion-among",
    "href": "content/projects/fsp-discussions/index.html#this-is-a-summary-of-the-group-discussion-among",
    "title": "FSP Discussions 2021",
    "section": "This is a summary of the group discussion among:",
    "text": "This is a summary of the group discussion among:\n1. Sadhvi Jawa\n2. Minashshi Singh\n3. Vidhu Gandhi\n4. Yash Bhandari\n5. Arvind Venkatadri"
  },
  {
    "objectID": "content/projects/fsp-doe/index.html",
    "href": "content/projects/fsp-doe/index.html",
    "title": "A Design of Experiments Class",
    "section": "",
    "text": "Code Lab"
  },
  {
    "objectID": "content/projects/fsp-doe/index.html#fa-envelope-introduction",
    "href": "content/projects/fsp-doe/index.html#fa-envelope-introduction",
    "title": "A Design of Experiments Class",
    "section": " Introduction",
    "text": "Introduction\nThis is a brief description and analysis of a Design of Experiments module conducted as a part of the Order and Chaos course, in the Foundation Studies Program (FSP 2021-2022) at SMI, MAHE, Bangalore."
  },
  {
    "objectID": "content/projects/fsp-doe/index.html#context",
    "href": "content/projects/fsp-doe/index.html#context",
    "title": "A Design of Experiments Class",
    "section": "Context",
    "text": "Context\nA Short Term Memory(STM) Test was the investigative tool used to verify several Hypotheses that were documented on the subject of STM.\nThis article describes the statistical analysis that was done with the readings. In particular, Permutations Tests were used to verify the effect size for each of three parameters that were hypothesized.\nFor more information, please click on the icon above to look at the Lab document."
  },
  {
    "objectID": "content/projects/fsp-doe/index.html#references",
    "href": "content/projects/fsp-doe/index.html#references",
    "title": "A Design of Experiments Class",
    "section": "References",
    "text": "References\n\nLawrance, A. J. 1996. ‚ÄúA Design of Experiments Workshop as an Introduction to Statistics.‚Äù American Statistician 50 (2): 156‚Äì58. doi:10.1080/00031305.1996.10474364.\nErnst, Michael D. 2004. ‚ÄúPermutation Methods: A Basis for Exact Inference.‚Äù Statistical Science 19 (4): 676‚Äì85. doi:10.1214/088342304000000396.\nPruim R, Kaplan DT, Horton NJ (2017). ‚ÄúThe mosaic Package: Helping Students to ‚ÄòThink with Data‚Äô Using R.‚Äù The R Journal, 9(1), 77‚Äì102. https://journal.r-project.org/archive/2017/RJ-2017-024/index.html."
  },
  {
    "objectID": "content/projects/fsp-manifesto/index.html",
    "href": "content/projects/fsp-manifesto/index.html",
    "title": "My Teaching Manifesto",
    "section": "",
    "text": "This is a short Statement of Values, Beliefs, and Content in my Teaching.\n\nArvind Venkatadri."
  },
  {
    "objectID": "content/projects/fsp-portfolio/index.html",
    "href": "content/projects/fsp-portfolio/index.html",
    "title": "Teaching in this Pandemic Year 2020-2021",
    "section": "",
    "text": "This is a short Portfolio of Teaching Initiatives and Student Outcomes during this pandemic year, 2020-2021, from Arvind Venkatadri."
  },
  {
    "objectID": "content/projects/fsp-portfolio-2022/index.html",
    "href": "content/projects/fsp-portfolio-2022/index.html",
    "title": "The Foundation Series",
    "section": "",
    "text": "This is a short Portfolio of Teaching Initiatives and Student Outcomes during this post(?)-pandemic year, 2021-2022, from Arvind Venkatadri."
  },
  {
    "objectID": "content/projects/listing.html",
    "href": "content/projects/listing.html",
    "title": "The Foundation Series",
    "section": "",
    "text": "This is a dummy blog posts\n\n\nThis is a test post. In this post, I try out different functionalities\n\n\n\nArvind Venkatadri\n\n\nJun 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject 2\n\n\nThis is a test post. In this post, I try out different functionalities\n\n\n\nArvind Venkatadri\n\n\nJun 1, 2022\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\nA Design of Experiments Class\n\n\nA Design of Experiments Class\n\n\n\nArvind Venkatadri\n\n\nJan 31, 2022\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\nA TRIZ Analysis of Lawrence of Arabia\n\n\nThe Attack on Aqaba: A TRIZ Analysis\n\n\n\nArvind Venkatadri\n\n\nDec 20, 2022\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\nA Tidygraph version of a Popular Network Science Tutorial\n\n\n\n\n\n\nArvind Venkatadri\n\n\nJun 16, 2021\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\nA1\n\n\n\n\n\n\nArvind Venkatadri\n\n\nMar 3, 2023\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\nFSP Discussions 2021\n\n\n\n\n\n\nArvind Venkatadri\n\n\nJul 27, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy Teaching Manifesto\n\n\n\n\n\n\nArvind Venkatadri\n\n\nAug 20, 2021\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\nQP\n\n\nCreated using ChatGPT on 3 March 2023 by Arvind Venkatadri\n\n\n\nArvind Venkatadri\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResources for Order and Chaos\n\n\n\n\n\n\nArvind Venkatadri\n\n\nDec 12, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTeaching in this Pandemic Year 2020-2021\n\n\n\n\n\n\nArvind Venkatadri\n\n\nAug 20, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad\n\n\nPunjabi Pop and Getting the Veggies\n\n\n\nArvind Venkatadri\n\n\nJan 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe TRIZ Chronicles: TRIZ Analysis of the O-Wind Turbine\n\n\nIs there a cheap and effective way to generate power using the Wind, right in your home?\n\n\n\nArvind Venkatadri\n\n\nMar 1, 2023\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\n\n\n\n\nArvind Venkatadri\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/projects/project-1/index.html",
    "href": "content/projects/project-1/index.html",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\nThis is inline code plus a small code chunk.\n\nShow the Codelibrary(tidyverse)\n\nggplot(mpg) +\n  geom_jitter(aes(cty, hwy), size = 4, alpha = 0.5) \n\n\n\n\n\n\n\n\n\n\nTransforming OLS estimates\nMaximizing likelihood\n\n\n\n\nShow the Codepreds_lm %>% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  theme_minimal(base_size = 12) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\nShow the Codeglm.mod <- glm(sex ~ body_mass_g + bill_length_mm + species, family = binomial, data = dat)\n\npreds <- dat %>% \n  mutate(\n    prob.fit = glm.mod$fitted.values,\n    prediction = if_else(prob.fit > 0.5, 'male', 'female'),\n    correct = if_else(sex == prediction, 'correct', 'incorrect')\n  )\n\n\npreds %>% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  theme_minimal(base_size = 10) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\\[\n\\int_0^1 f(x) \\ dx\n\\]\n\n\n\n\nShow the Codegeom_density(\n  mapping = NULL,\n  data = NULL,\n  stat = \"density\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  outline.type = \"upper\"\n)\n\n\n\nShow the Codestat_density(\n  mapping = NULL,\n  data = NULL,\n  geom = \"area\",\n  position = \"stack\",\n  ...,\n  bw = \"nrd0\",\n  adjust = 1,\n  kernel = \"gaussian\",\n  n = 512,\n  trim = FALSE,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\n\n\n\n\n\nShow the Codeggplot(data = gapminder::gapminder, mapping = aes(x = lifeExp, fill = continent)) +\n  stat_density(position = \"identity\", alpha = 0.5)\n\n\n\n\nBla bla bla. This is a caption in the margin. Super cool isn‚Äôt it?"
  },
  {
    "objectID": "content/projects/project-2/index.html",
    "href": "content/projects/project-2/index.html",
    "title": "Project 2",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\nThis is inline code plus a small code chunk.\n\nShow the Codelibrary(tidyverse)\n\nggplot(mpg) +\n  geom_jitter(aes(cty, hwy), size = 4, alpha = 0.5) \n\n\n\n\n\n\n\n\n\n\nTransforming OLS estimates\nMaximizing likelihood\n\n\n\n\nShow the Codepreds_lm %>% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  theme_minimal(base_size = 12) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\nShow the Codeglm.mod <- glm(sex ~ body_mass_g + bill_length_mm + species, family = binomial, data = dat)\n\npreds <- dat %>% \n  mutate(\n    prob.fit = glm.mod$fitted.values,\n    prediction = if_else(prob.fit > 0.5, 'male', 'female'),\n    correct = if_else(sex == prediction, 'correct', 'incorrect')\n  )\n\n\npreds %>% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  theme_minimal(base_size = 10) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\\[\n\\int_0^1 f(x) \\ dx\n\\]\n\n\n\n\nShow the Codegeom_density(\n  mapping = NULL,\n  data = NULL,\n  stat = \"density\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  outline.type = \"upper\"\n)\n\n\n\nShow the Codestat_density(\n  mapping = NULL,\n  data = NULL,\n  geom = \"area\",\n  position = \"stack\",\n  ...,\n  bw = \"nrd0\",\n  adjust = 1,\n  kernel = \"gaussian\",\n  n = 512,\n  trim = FALSE,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\n\n\n\n\n\nShow the Codeggplot(data = gapminder::gapminder, mapping = aes(x = lifeExp, fill = continent)) +\n  stat_density(position = \"identity\", alpha = 0.5)\n\n\n\n\nBla bla bla. This is a caption in the margin. Super cool isn‚Äôt it?"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#image-right",
    "href": "content/slides/projects-slides/portfolio/index.html#image-right",
    "title": "Introduction to Networks in R",
    "section": ".image-right",
    "text": ".image-right\n\n\nWe can use the .image-right and .image-left classes to insert images in the background\nThese images will be placed behind most other content"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#image-left",
    "href": "content/slides/projects-slides/portfolio/index.html#image-left",
    "title": "Introduction to Networks in R",
    "section": ".image-left",
    "text": ".image-left\n\n\n\n\nIt is therefore recommened that you use multiple columns to only have text on the background area"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#section",
    "href": "content/slides/projects-slides/portfolio/index.html#section",
    "title": "Introduction to Networks in R",
    "section": "",
    "text": "background images"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#setting-background-colors",
    "href": "content/slides/projects-slides/portfolio/index.html#setting-background-colors",
    "title": "Introduction to Networks in R",
    "section": "Setting background colors",
    "text": "Setting background colors\nyou can set your background as you normally would"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#video-slide-title",
    "href": "content/slides/projects-slides/portfolio/index.html#video-slide-title",
    "title": "Introduction to Networks in R",
    "section": "Video Slide Title",
    "text": "Video Slide Title\nThis slides‚Äôs background video will play in a loop with audio muted."
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#slide-title",
    "href": "content/slides/projects-slides/portfolio/index.html#slide-title",
    "title": "Introduction to Networks in R",
    "section": "Slide Title",
    "text": "Slide Title"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#further-modifying-theme",
    "href": "content/slides/projects-slides/portfolio/index.html#further-modifying-theme",
    "title": "Introduction to Networks in R",
    "section": "Further Modifying theme",
    "text": "Further Modifying theme\nIf you want to modify theme, you can specify the .scss my modifying the yaml to look like this\nformat: \n  letterbox-revealjs:\n    theme: [default, style.scss]"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#modifying-letterbox-background",
    "href": "content/slides/projects-slides/portfolio/index.html#modifying-letterbox-background",
    "title": "Introduction to Networks in R",
    "section": "Modifying letterbox background",
    "text": "Modifying letterbox background\nThe background colors can be with with, where #444444 represents the new background color and #222222 represents the color of the shadow\n.quarto-light {\n  background-color: #444444;\n}\n\n.quarto-dark {\n  background-color: #444444;\n}\n\n.slides {\n  box-shadow: #222222 0px 0px 30px 0px;\n}"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#quarto",
    "href": "content/slides/projects-slides/portfolio/index.html#quarto",
    "title": "Introduction to Networks in R",
    "section": "Quarto",
    "text": "Quarto\nQuarto enables you to weave together content and executable code into a finished presentation. To learn more about Quarto presentations see https://quarto.org/docs/presentations/."
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#bullets",
    "href": "content/slides/projects-slides/portfolio/index.html#bullets",
    "title": "Introduction to Networks in R",
    "section": "Bullets",
    "text": "Bullets\nWhen you click the Render button a document will be generated that includes:\n\nContent authored with markdown\nOutput from executable code"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#code",
    "href": "content/slides/projects-slides/portfolio/index.html#code",
    "title": "Introduction to Networks in R",
    "section": "Code",
    "text": "Code\nWhen you click the Render button a presentation will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\n[1] 2"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/LICENSE.html",
    "href": "content/slides/projects-slides/portfolio/LICENSE.html",
    "title": "The Foundation Series",
    "section": "",
    "text": "MIT License\nCopyright (c) 2022 quarto-letterbox authors\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ‚ÄúSoftware‚Äù), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED ‚ÄúAS IS‚Äù, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "content/slides/r-slides/graphics/index.html#image-right",
    "href": "content/slides/r-slides/graphics/index.html#image-right",
    "title": "The Grammar of Graphics in R",
    "section": ".image-right",
    "text": ".image-right\n\n\nWe can use the .image-right and .image-left classes to insert images in the background\nThese images will be placed behind most other content"
  },
  {
    "objectID": "content/slides/r-slides/graphics/index.html#image-left",
    "href": "content/slides/r-slides/graphics/index.html#image-left",
    "title": "The Grammar of Graphics in R",
    "section": ".image-left",
    "text": ".image-left\n\n\n\n\nIt is therefore recommened that you use multiple columns to only have text on the background area"
  },
  {
    "objectID": "content/slides/r-slides/graphics/index.html#section",
    "href": "content/slides/r-slides/graphics/index.html#section",
    "title": "The Grammar of Graphics in R",
    "section": "",
    "text": "background images"
  },
  {
    "objectID": "content/slides/r-slides/graphics/index.html#setting-background-colors",
    "href": "content/slides/r-slides/graphics/index.html#setting-background-colors",
    "title": "The Grammar of Graphics in R",
    "section": "Setting background colors",
    "text": "Setting background colors\nyou can set your background as you normally would"
  },
  {
    "objectID": "content/slides/r-slides/graphics/index.html#video-slide-title",
    "href": "content/slides/r-slides/graphics/index.html#video-slide-title",
    "title": "The Grammar of Graphics in R",
    "section": "Video Slide Title",
    "text": "Video Slide Title\nThis slides‚Äôs background video will play in a loop with audio muted."
  },
  {
    "objectID": "content/slides/r-slides/graphics/index.html#slide-title",
    "href": "content/slides/r-slides/graphics/index.html#slide-title",
    "title": "The Grammar of Graphics in R",
    "section": "Slide Title",
    "text": "Slide Title"
  },
  {
    "objectID": "content/slides/r-slides/graphics/index.html#further-modifying-theme",
    "href": "content/slides/r-slides/graphics/index.html#further-modifying-theme",
    "title": "The Grammar of Graphics in R",
    "section": "Further Modifying theme",
    "text": "Further Modifying theme\nIf you want to modify theme, you can specify the .scss my modifying the yaml to look like this\nformat: \n  letterbox-revealjs:\n    theme: [default, style.scss]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/index.html#modifying-letterbox-background",
    "href": "content/slides/r-slides/graphics/index.html#modifying-letterbox-background",
    "title": "The Grammar of Graphics in R",
    "section": "Modifying letterbox background",
    "text": "Modifying letterbox background\nThe background colors can be with with, where #444444 represents the new background color and #222222 represents the color of the shadow\n.quarto-light {\n  background-color: #444444;\n}\n\n.quarto-dark {\n  background-color: #444444;\n}\n\n.slides {\n  box-shadow: #222222 0px 0px 30px 0px;\n}"
  },
  {
    "objectID": "content/slides/r-slides/graphics/index.html#quarto",
    "href": "content/slides/r-slides/graphics/index.html#quarto",
    "title": "The Grammar of Graphics in R",
    "section": "Quarto",
    "text": "Quarto\nQuarto enables you to weave together content and executable code into a finished presentation. To learn more about Quarto presentations see https://quarto.org/docs/presentations/."
  },
  {
    "objectID": "content/slides/r-slides/graphics/index.html#bullets",
    "href": "content/slides/r-slides/graphics/index.html#bullets",
    "title": "The Grammar of Graphics in R",
    "section": "Bullets",
    "text": "Bullets\nWhen you click the Render button a document will be generated that includes:\n\nContent authored with markdown\nOutput from executable code"
  },
  {
    "objectID": "content/slides/r-slides/graphics/index.html#code",
    "href": "content/slides/r-slides/graphics/index.html#code",
    "title": "The Grammar of Graphics in R",
    "section": "Code",
    "text": "Code\nWhen you click the Render button a presentation will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\n[1] 2"
  },
  {
    "objectID": "content/slides/r-slides/graphics/LICENSE.html",
    "href": "content/slides/r-slides/graphics/LICENSE.html",
    "title": "The Foundation Series",
    "section": "",
    "text": "MIT License\nCopyright (c) 2022 quarto-letterbox authors\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ‚ÄúSoftware‚Äù), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED ‚ÄúAS IS‚Äù, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#how-does-one-read-shakespeare",
    "href": "content/slides/r-slides/graphics/metaphors.html#how-does-one-read-shakespeare",
    "title": "Metaphors with Graphics",
    "section": "How does one read Shakespeare?",
    "text": "How does one read Shakespeare?\n\nshakespeareTo code or not to code, that is the question‚Ä¶"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#what-is-a-grammar-of-graphics",
    "href": "content/slides/r-slides/graphics/metaphors.html#what-is-a-grammar-of-graphics",
    "title": "Metaphors with Graphics",
    "section": "What is a Grammar of Graphics?",
    "text": "What is a Grammar of Graphics?\n\nCode looks and reads like English.\n\nHas verbs, nouns, some adjectives‚Ä¶.\n\n‚Äì\n\nDescribes Information/ideas/concepts from any source domain.\n\n‚Äì\n\nGEOMETRY as the target domain : What comes out of R is predominantly ‚Äúgeometry‚Äù"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#how-do-we-express-visuals-in-words",
    "href": "content/slides/r-slides/graphics/metaphors.html#how-do-we-express-visuals-in-words",
    "title": "Metaphors with Graphics",
    "section": "How do we express visuals in words?",
    "text": "How do we express visuals in words?\n.font120[ - Data to be visualized]\n\n\n\n\n\n\n.font120[ - .hlb[Geom]etric objects that appear on the plot]\n\n\n\n\n.font120[ - .hlb[Aes]thetic mappings from data to visual component]\n\n\n\n.font120[ - .hlb[Stat]istics transform data on the way to visualization]\n\n\n\n\n\n\n.font120[ - .hlb[Coord]inates organize location of geometric objects]\n\n\n\n\n.font120[ - .hlb[Scale]s define the range of values for aesthetics]\n\n\n\n.font120[ - .hlb[Facet]s group into subplots]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#the-essence-of-ggplot",
    "href": "content/slides/r-slides/graphics/metaphors.html#the-essence-of-ggplot",
    "title": "Metaphors with Graphics",
    "section": "The Essence of ggplot",
    "text": "The Essence of ggplot\nall ggplot2\n\naes(x = , y = ) (aesthetics)\naes(x = , y = , color = ) (add color)\naes(x = , y = , size = ) (add size)\n+ facet_wrap(~ ) (facetting)\n+ scale_ ( add a scale)"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#gg-is-for-grammar-of-graphics",
    "href": "content/slides/r-slides/graphics/metaphors.html#gg-is-for-grammar-of-graphics",
    "title": "Metaphors with Graphics",
    "section": "gg is for Grammar of Graphics",
    "text": "gg is for Grammar of Graphics\n.left-column[ ### Data ### Aesthetics ### Geoms\n+ geom_*()\n]\n.right-column[\n\n\n\n\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#the-five-named-graphs",
    "href": "content/slides/r-slides/graphics/metaphors.html#the-five-named-graphs",
    "title": "Metaphors with Graphics",
    "section": "The Five-Named Graphs",
    "text": "The Five-Named Graphs\n\nScatterplot: geom_point()\nLine graph: geom_line()\nHistogram: geom_histogram()\nBoxplot: geom_boxplot()\nBar graph: geom_bar() or geom_col (see Lab 02)"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#chunk-penguins",
    "href": "content/slides/r-slides/graphics/metaphors.html#chunk-penguins",
    "title": "Metaphors with Graphics",
    "section": "Chunk : penguins",
    "text": "Chunk : penguins\n\n\n# A tibble: 6 √ó 8\n  species island    bill_length_mm bill_depth_mm flipper_l‚Ä¶¬π body_‚Ä¶¬≤ sex    year\n  <fct>   <fct>              <dbl>         <dbl>       <int>   <int> <fct> <int>\n1 Adelie  Torgersen           39.1          18.7         181    3750 male   2007\n2 Adelie  Torgersen           39.5          17.4         186    3800 fema‚Ä¶  2007\n3 Adelie  Torgersen           40.3          18           195    3250 fema‚Ä¶  2007\n4 Adelie  Torgersen           NA            NA            NA      NA <NA>   2007\n5 Adelie  Torgersen           36.7          19.3         193    3450 fema‚Ä¶  2007\n6 Adelie  Torgersen           39.3          20.6         190    3650 male   2007\n# ‚Ä¶ with abbreviated variable names ¬π‚Äãflipper_length_mm, ¬≤‚Äãbody_mass_g\n\n\nWe see the first few rows of the dataset penguins. We see that there are a few NA data observations too. Let us remove them for now."
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#chunk-mapping",
    "href": "content/slides/r-slides/graphics/metaphors.html#chunk-mapping",
    "title": "Metaphors with Graphics",
    "section": "Chunk: Mapping",
    "text": "Chunk: Mapping\n.pull-left[\n\n\n\n]\n.pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#chunk-mapping-1",
    "href": "content/slides/r-slides/graphics/metaphors.html#chunk-mapping-1",
    "title": "Metaphors with Graphics",
    "section": "Chunk: Mapping",
    "text": "Chunk: Mapping\n.pull-left[\n\n\n\n]\n.pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#chunk-mapping-2",
    "href": "content/slides/r-slides/graphics/metaphors.html#chunk-mapping-2",
    "title": "Metaphors with Graphics",
    "section": "Chunk: Mapping",
    "text": "Chunk: Mapping\n.pull-left[\n\n\n\n]\n.pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#chunk-mapping-3",
    "href": "content/slides/r-slides/graphics/metaphors.html#chunk-mapping-3",
    "title": "Metaphors with Graphics",
    "section": "Chunk: Mapping",
    "text": "Chunk: Mapping\n.pull-left[\n\n\n\n]\n.pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#chunk-geom_point_position_colour",
    "href": "content/slides/r-slides/graphics/metaphors.html#chunk-geom_point_position_colour",
    "title": "Metaphors with Graphics",
    "section": "Chunk: Geom_Point_Position_Colour",
    "text": "Chunk: Geom_Point_Position_Colour\n.pull-left[\n\n\n\n] .pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#chunk-geom_point_position_colour-1",
    "href": "content/slides/r-slides/graphics/metaphors.html#chunk-geom_point_position_colour-1",
    "title": "Metaphors with Graphics",
    "section": "Chunk: Geom_Point_Position_Colour",
    "text": "Chunk: Geom_Point_Position_Colour\n.pull-left[\n\n\n\nWe can leave out the ‚Äúmapping‚Äù word and just use aes .\nWhy is there no plot?\nü§î üí≠\nRight !! We have not used a geom command yet!! ] .pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#chunk-geom_point_position_colour-2",
    "href": "content/slides/r-slides/graphics/metaphors.html#chunk-geom_point_position_colour-2",
    "title": "Metaphors with Graphics",
    "section": "Chunk: Geom_Point_Position_Colour",
    "text": "Chunk: Geom_Point_Position_Colour\n.pull-left[\n\n\n\nNote that the points are located by position coordinates on both x and y axis, and coloured by the island variable.\n] .pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#chunk-geom_point_position_colour-3",
    "href": "content/slides/r-slides/graphics/metaphors.html#chunk-geom_point_position_colour-3",
    "title": "Metaphors with Graphics",
    "section": "Chunk: Geom_Point_Position_Colour",
    "text": "Chunk: Geom_Point_Position_Colour\n.pull-left[\n\n\n\nNote that the points are located by position coordinates on both x and y axis, and coloured by the island variable.\nAnd we‚Äôve fixed size = 4!\n] .pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#alpha",
    "href": "content/slides/r-slides/graphics/metaphors.html#alpha",
    "title": "Metaphors with Graphics",
    "section": "Alpha",
    "text": "Alpha\n.pull-left[\n\n\n\nAre the points all overlapping? Can we see them better? ]\n.pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#alpha-1",
    "href": "content/slides/r-slides/graphics/metaphors.html#alpha-1",
    "title": "Metaphors with Graphics",
    "section": "Alpha",
    "text": "Alpha\n.pull-left[\n\n\n\nAre the points all overlapping? Can we see them better?\n]\n.pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#chunk-box-plot",
    "href": "content/slides/r-slides/graphics/metaphors.html#chunk-box-plot",
    "title": "Metaphors with Graphics",
    "section": "Chunk: Box Plot",
    "text": "Chunk: Box Plot\n.pull-left[\n\n\n\n]\n.pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#chunk-box-plot-1",
    "href": "content/slides/r-slides/graphics/metaphors.html#chunk-box-plot-1",
    "title": "Metaphors with Graphics",
    "section": "Chunk: Box Plot",
    "text": "Chunk: Box Plot\n.pull-left[\n\n\n\n]\n.pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#chunk-geom_bar_1",
    "href": "content/slides/r-slides/graphics/metaphors.html#chunk-geom_bar_1",
    "title": "Metaphors with Graphics",
    "section": "Chunk: Geom_Bar_1",
    "text": "Chunk: Geom_Bar_1\n.pull-left[ ::: {.cell hash=‚Äòmetaphors_cache/revealjs/3a_4be4390b2bb8a0bb273e1c4174c47445‚Äô}\n::: ] .pull-right[ ::: {.cell hash=‚Äòmetaphors_cache/revealjs/unnamed-chunk-11_c54ec299e4510a718f20ea0f35e457fa‚Äô} ::: {.cell-output-display}  ::: :::]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#chunk-geom_bar_1-1",
    "href": "content/slides/r-slides/graphics/metaphors.html#chunk-geom_bar_1-1",
    "title": "Metaphors with Graphics",
    "section": "Chunk: Geom_Bar_1",
    "text": "Chunk: Geom_Bar_1\n.pull-left[\n\n\n\n] .pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#chunk-geom_bar_1-2",
    "href": "content/slides/r-slides/graphics/metaphors.html#chunk-geom_bar_1-2",
    "title": "Metaphors with Graphics",
    "section": "Chunk: Geom_Bar_1",
    "text": "Chunk: Geom_Bar_1\n.pull-left[\n\n\n\nThe bars are plotted with positions on the x-axis, defined by the species variable, and heights mapped to the y-axis.\nHow did the graph ‚Äúknow‚Äù the heights of the bars?\ngeom_bar has an internal count statistic computation. Many geom_s have internal computation that are accessible to programmers.\n] .pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#geom_bar_position_stack_and_dodge",
    "href": "content/slides/r-slides/graphics/metaphors.html#geom_bar_position_stack_and_dodge",
    "title": "Metaphors with Graphics",
    "section": "Geom_Bar_Position_Stack_and_Dodge",
    "text": "Geom_Bar_Position_Stack_and_Dodge\n.pull-left[ When using more than a pair of variables with a bar chart, we have a few more position options:\n\n\n\n] .pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#geom_bar_position_stack_and_dodge-1",
    "href": "content/slides/r-slides/graphics/metaphors.html#geom_bar_position_stack_and_dodge-1",
    "title": "Metaphors with Graphics",
    "section": "Geom_Bar_Position_Stack_and_Dodge",
    "text": "Geom_Bar_Position_Stack_and_Dodge\n.pull-left[ When using more than a pair of variables with a bar chart, we have a few more position options:\n\n\n\nThe bars are coloured by the island variable and are stacked in position. ] .pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#geom_bar_position_stack_and_dodge-2",
    "href": "content/slides/r-slides/graphics/metaphors.html#geom_bar_position_stack_and_dodge-2",
    "title": "Metaphors with Graphics",
    "section": "Geom_Bar_Position_Stack_and_Dodge",
    "text": "Geom_Bar_Position_Stack_and_Dodge\n.pull-left[ And here we use the dodge option: ::: {.cell hash=‚Äòmetaphors_cache/revealjs/5c_305f0626b040002d5728ac2d1abf747a‚Äô}\n::: ]\n.pull-right[ ::: {.cell hash=‚Äòmetaphors_cache/revealjs/unnamed-chunk-16_b1b9949a06c2c12e5ed535f2455b2d6c‚Äô} ::: {.cell-output-display}  ::: :::]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#facetting",
    "href": "content/slides/r-slides/graphics/metaphors.html#facetting",
    "title": "Metaphors with Graphics",
    "section": "Facetting",
    "text": "Facetting\n.pull-left[\n\n\n\n] .pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#facetting-1",
    "href": "content/slides/r-slides/graphics/metaphors.html#facetting-1",
    "title": "Metaphors with Graphics",
    "section": "Facetting",
    "text": "Facetting\n.pull-left[\n\n\n\n]\n.pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#facetting-2",
    "href": "content/slides/r-slides/graphics/metaphors.html#facetting-2",
    "title": "Metaphors with Graphics",
    "section": "Facetting",
    "text": "Facetting\n.pull-left[\n\n\n\n]\n.pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#facetting-3",
    "href": "content/slides/r-slides/graphics/metaphors.html#facetting-3",
    "title": "Metaphors with Graphics",
    "section": "Facetting",
    "text": "Facetting\n.pull-left[\n\n\n\nThe graph has split into small multiples, based on the number of islands. ]\n.pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#still-more-facetting",
    "href": "content/slides/r-slides/graphics/metaphors.html#still-more-facetting",
    "title": "Metaphors with Graphics",
    "section": "Still more Facetting",
    "text": "Still more Facetting\n.pull-left[ ::: {.cell hash=‚Äòmetaphors_cache/revealjs/6e_11fffde8991ec6023b002517c71fb782‚Äô}\n:::\nWhat if we have even more ‚Äúfactor‚Äù variables? We have island and species‚Ä¶can we split further?\n]\n.pull-right[ ::: {.cell hash=‚Äòmetaphors_cache/revealjs/unnamed-chunk-21_8baf2a1f3d326823f4808f0ec3a1c274‚Äô} ::: {.cell-output-display}  ::: :::]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#still-more-facetting-1",
    "href": "content/slides/r-slides/graphics/metaphors.html#still-more-facetting-1",
    "title": "Metaphors with Graphics",
    "section": "Still more Facetting",
    "text": "Still more Facetting\n.pull-left[\n\n\n\nThe graph has split into multiples, based on the number of islands and the number of species.\n] .pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#and-shall-we-look-briefly-at-colour",
    "href": "content/slides/r-slides/graphics/metaphors.html#and-shall-we-look-briefly-at-colour",
    "title": "Metaphors with Graphics",
    "section": "And shall we look briefly at colour?",
    "text": "And shall we look briefly at colour?"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#finallycolour-just-a-bit",
    "href": "content/slides/r-slides/graphics/metaphors.html#finallycolour-just-a-bit",
    "title": "Metaphors with Graphics",
    "section": "Finally‚Ä¶Colour !! ( Just a bit )",
    "text": "Finally‚Ä¶Colour !! ( Just a bit )\n.pull-left[\n\n\n\n] .pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#finallycolour-just-a-bit-1",
    "href": "content/slides/r-slides/graphics/metaphors.html#finallycolour-just-a-bit-1",
    "title": "Metaphors with Graphics",
    "section": "Finally‚Ä¶Colour !! ( Just a bit )",
    "text": "Finally‚Ä¶Colour !! ( Just a bit )\n.pull-left[ ::: {.cell hash=‚Äòmetaphors_cache/revealjs/8b_af117602443139046988dd623fbb1cb2‚Äô}\n::: We are using the RColorBrewer package here. Type RColorBrewer::display.brewer.all() in your Console and see what palettes are available.\n] .pull-right[ ::: {.cell hash=‚Äòmetaphors_cache/revealjs/unnamed-chunk-24_43588c1909bff1ebea8c039025075182‚Äô} ::: {.cell-output-display}  ::: :::]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#chunk-colour-just-a-bit",
    "href": "content/slides/r-slides/graphics/metaphors.html#chunk-colour-just-a-bit",
    "title": "Metaphors with Graphics",
    "section": "Chunk: Colour !! ( Just a bit )",
    "text": "Chunk: Colour !! ( Just a bit )\n.pull-left[\n\n\n\n] .pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#chunk-colour-just-a-bit-1",
    "href": "content/slides/r-slides/graphics/metaphors.html#chunk-colour-just-a-bit-1",
    "title": "Metaphors with Graphics",
    "section": "Chunk: Colour !! ( Just a bit )",
    "text": "Chunk: Colour !! ( Just a bit )\n.pull-left[\n\n\n\n] .pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#chunk-colour-just-a-bit-2",
    "href": "content/slides/r-slides/graphics/metaphors.html#chunk-colour-just-a-bit-2",
    "title": "Metaphors with Graphics",
    "section": "Chunk: Colour !! ( Just a bit )",
    "text": "Chunk: Colour !! ( Just a bit )\n.pull-left[\n\n\n\n] .pull-right[\n\n]"
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#conclusion",
    "href": "content/slides/r-slides/graphics/metaphors.html#conclusion",
    "title": "Metaphors with Graphics",
    "section": "Conclusion",
    "text": "Conclusion\n\nggplot takes a dataframe/tibble as the data argument\nThe aes-thetic arguments can be x, y, colour, shape, alpha for example‚Ä¶\nThe geom_*() commands specify the kind of plot, from a geometric perspective\nTogether, the ggplot package offers a Grammar of near-English commands which allow us to plot data in various ways."
  },
  {
    "objectID": "content/slides/r-slides/graphics/metaphors.html#references",
    "href": "content/slides/r-slides/graphics/metaphors.html#references",
    "title": "Metaphors with Graphics",
    "section": "References",
    "text": "References\n\nWickham, Hadley. (2010) ‚ÄúA Layered Grammar of Graphics‚Äù. Journal of Computational and Graphical Statistics, 19(1).\nWilkinson, Leland. (2005). The Grammar of Graphics. (UChicago authentication required)"
  },
  {
    "objectID": "content/slides/r-slides/nature/index.html#what-makes-human-experience",
    "href": "content/slides/r-slides/nature/index.html#what-makes-human-experience",
    "title": "The Nature of Data",
    "section": "What makes Human Experience?",
    "text": "What makes Human Experience?\n\nHow would we begin to describe this experience?\n\n\n\nWhere / When?\nWho?\nHow?\nHow Big? How small? How frequent? How sudden?\n\n\n\n\n\nAnd‚Ä¶.How Surprising ! How Shocking! How sad‚Ä¶How Wonderful !!!\nSo: Our Questions, and our Surprise lead us to creating Human Experiences."
  },
  {
    "objectID": "content/slides/r-slides/nature/index.html#does-this-surprise-you",
    "href": "content/slides/r-slides/nature/index.html#does-this-surprise-you",
    "title": "The Nature of Data",
    "section": "Does this Surprise you?",
    "text": "Does this Surprise you?"
  },
  {
    "objectID": "content/slides/r-slides/nature/index.html#the-element-of-surprise",
    "href": "content/slides/r-slides/nature/index.html#the-element-of-surprise",
    "title": "The Nature of Data",
    "section": "The Element of Surprise?",
    "text": "The Element of Surprise?\n\n\n\n\n\nJane Austen knew a lot about human information processing as these snippets from Pride and Prejudice (published in 1813 ‚Äì over 200 years ago) show :\n\nShe was a woman of mean understanding, little information , and uncertain temper.\nCatherine and Lydia had information for them of a different sort.\nWhen this information was given, and they had all taken their seats, Mr.¬†Collins was at leisure to look around him and admire,‚Ä¶\nYou could not have met with a person more capable of giving you certain information on that head than myself, for I have been connected with his family in a particular manner from my infancy.\nThis information made Elizabeth smile, as she thought of poor Miss Bingley.\nThis information, however, startled Mrs.¬†Bennet ‚Ä¶\nhttps://www.cs.bham.ac.uk/research/projects/cogaff/misc/austen-info.html"
  },
  {
    "objectID": "content/slides/r-slides/nature/index.html#claude-shannon-and-information",
    "href": "content/slides/r-slides/nature/index.html#claude-shannon-and-information",
    "title": "The Nature of Data",
    "section": "Claude Shannon and Information",
    "text": "Claude Shannon and Information\n\nhttps://plus.maths.org/content/information-surprise"
  },
  {
    "objectID": "content/slides/r-slides/nature/index.html#human-experience-is.data",
    "href": "content/slides/r-slides/nature/index.html#human-experience-is.data",
    "title": "The Nature of Data",
    "section": "Human Experience is‚Ä¶.Data??",
    "text": "Human Experience is‚Ä¶.Data??"
  },
  {
    "objectID": "content/slides/r-slides/nature/index.html#experiments-and-hypotheses-a-kitchen-experiment",
    "href": "content/slides/r-slides/nature/index.html#experiments-and-hypotheses-a-kitchen-experiment",
    "title": "The Nature of Data",
    "section": "Experiments and Hypotheses: A Kitchen Experiment",
    "text": "Experiments and Hypotheses: A Kitchen Experiment\n\n\nInputs are: Ingredients, Recipes, Processes\nOutputs are: Taste, Texture, Colour, Quantity!!\n\nUsed without permission from https://safetyculture.com/topics/design-of-experiments/"
  },
  {
    "objectID": "content/slides/r-slides/nature/index.html#what-is-the-result-of-an-experiment",
    "href": "content/slides/r-slides/nature/index.html#what-is-the-result-of-an-experiment",
    "title": "The Nature of Data",
    "section": "What is the Result of an Experiment?",
    "text": "What is the Result of an Experiment?\n\n\n\nAll experiments give us data about phenomena\nWe obtain data about the things that happen: Outputs\nWhat makes things happen?: Inputs\nHow?: Process\nWhen? Factors\nHow much ‚Äúoutput‚Äù is caused by how much ‚Äúinput‚Äù? Effect Size\n\n\nAll Experiments stem from Human Curiosity, a Hypothesis, and a Desire to Find out and Talk about Something"
  },
  {
    "objectID": "content/slides/r-slides/nature/index.html#a-famous-lady-and-her-famous-experiment",
    "href": "content/slides/r-slides/nature/index.html#a-famous-lady-and-her-famous-experiment",
    "title": "The Nature of Data",
    "section": "A Famous Lady and her Famous Experiment",
    "text": "A Famous Lady and her Famous Experiment\n\n\n\n\n\n\n\n\n\n\nIn 1853, Turkey declared war on Russia. After the Russian Navy destroyed a Turkish squadron in the Black Sea, Great Britain and France joined with Turkey. In September of the following year, the British landed on the Crimean Peninsula and set out, with the French and Turks, to take the Russian naval base at Sevastopol.\nWhat followed was a tragicomedy of errors ‚Äì failure of supply, failed communications, international rivalries. Conditions in the armies were terrible, and disease ate through their ranks. They finally did take Sevastopol a year later, after a ghastly assault. It was ugly business all around. Well over half a million soldiers lost their lives during the Crimean War."
  },
  {
    "objectID": "content/slides/r-slides/nature/index.html#florence-nightingales-data",
    "href": "content/slides/r-slides/nature/index.html#florence-nightingales-data",
    "title": "The Nature of Data",
    "section": "Florence Nightingale‚Äôs Data",
    "text": "Florence Nightingale‚Äôs Data\n\n\n   Month Year Disease.rate Wounds.rate Other.rate\n1    Apr 1854          1.4         0.0        7.0\n2    May 1854          6.2         0.0        4.6\n3    Jun 1854          4.7         0.0        2.5\n4    Jul 1854        150.0         0.0        9.6\n5    Aug 1854        328.5         0.4       11.9\n6    Sep 1854        312.2        32.1       27.7\n7    Oct 1854        197.0        51.7       50.1\n8    Nov 1854        340.6       115.8       42.8\n9    Dec 1854        631.5        41.7       48.0\n10   Jan 1855       1022.8        30.7      120.0"
  },
  {
    "objectID": "content/slides/r-slides/nature/index.html#how-does-data-look-like-then",
    "href": "content/slides/r-slides/nature/index.html#how-does-data-look-like-then",
    "title": "The Nature of Data",
    "section": "How Does Data look Like, then?",
    "text": "How Does Data look Like, then?\n\n\n\n\nUsing Interrogative Pronouns\n\nNominal: What? Who? Where? (Factors, Dimensions)\nOrdinal: Which Types? What Sizes? How Big? (Factors, Dimensions)\nInterval: How Often? (Numbers, Facts)\nRatio: How many? How much? How heavy? (Numbers, Facts)"
  },
  {
    "objectID": "content/slides/r-slides/nature/index.html#types-of-variables-in-nightingale-data",
    "href": "content/slides/r-slides/nature/index.html#types-of-variables-in-nightingale-data",
    "title": "The Nature of Data",
    "section": "Types of Variables in Nightingale Data",
    "text": "Types of Variables in Nightingale Data\n\n\n\n\nUsing Interrogative Pronouns\n\nNominal: None\nOrdinal: (Factors, Dimensions)\n\nHOW? War, Disease, Other\n\nInterval: (Numbers, Facts)\n\nWHEN? Year, Month\n\nRatio: (Numbers, Facts)\n\nHOW MANY? Rate of Deaths (War, Disease, Other)"
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#what-makes-human-experience",
    "href": "content/slides/r-slides/nature/new.html#what-makes-human-experience",
    "title": "The Nature of Data",
    "section": "What makes Human Experience?",
    "text": "What makes Human Experience?\n()\nHow would we begin to describe this experience?\n\n\n\nWhere / When?\nWho?\nHow?\nHow Big? How small? How frequent? How sudden?\n\n\n\n\n\nAnd‚Ä¶.How Surprising ! How Shocking! How sad‚Ä¶How Wonderful !!!\nSo: Our Questions, and our Surprise lead us to creating Human Experiences."
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#does-this-surprise-you",
    "href": "content/slides/r-slides/nature/new.html#does-this-surprise-you",
    "title": "The Nature of Data",
    "section": "Does this Surprise you?",
    "text": "Does this Surprise you?"
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#the-element-of-surprise",
    "href": "content/slides/r-slides/nature/new.html#the-element-of-surprise",
    "title": "The Nature of Data",
    "section": "The Element of Surprise?",
    "text": "The Element of Surprise?\n\n\n\n\n\nJane Austen knew a lot about human information processing as these snippets from Pride and Prejudice (published in 1813 ‚Äì over 200 years ago) show :\n\nShe was a woman of mean understanding, little information , and uncertain temper.\nCatherine and Lydia had information for them of a different sort.\nWhen this information was given, and they had all taken their seats, Mr.¬†Collins was at leisure to look around him and admire,‚Ä¶\nYou could not have met with a person more capable of giving you certain information on that head than myself, for I have been connected with his family in a particular manner from my infancy.\nThis information made Elizabeth smile, as she thought of poor Miss Bingley.\nThis information, however, startled Mrs.¬†Bennet ‚Ä¶\nhttps://www.cs.bham.ac.uk/research/projects/cogaff/misc/austen-info.html"
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#claude-shannon-and-information",
    "href": "content/slides/r-slides/nature/new.html#claude-shannon-and-information",
    "title": "The Nature of Data",
    "section": "Claude Shannon and Information",
    "text": "Claude Shannon and Information\n\nhttps://plus.maths.org/content/information-surprise"
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#human-experience-is.data",
    "href": "content/slides/r-slides/nature/new.html#human-experience-is.data",
    "title": "The Nature of Data",
    "section": "Human Experience is‚Ä¶.Data??",
    "text": "Human Experience is‚Ä¶.Data??"
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#experiments-and-hypotheses-a-kitchen-experiment",
    "href": "content/slides/r-slides/nature/new.html#experiments-and-hypotheses-a-kitchen-experiment",
    "title": "The Nature of Data",
    "section": "Experiments and Hypotheses: A Kitchen Experiment",
    "text": "Experiments and Hypotheses: A Kitchen Experiment\n\n\nInputs are: Ingredients, Recipes, Processes\nOutputs are: Taste, Texture, Colour, Quantity!!\n\nUsed without permission from https://safetyculture.com/topics/design-of-experiments/"
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#what-is-the-result-of-an-experiment",
    "href": "content/slides/r-slides/nature/new.html#what-is-the-result-of-an-experiment",
    "title": "The Nature of Data",
    "section": "What is the Result of an Experiment?",
    "text": "What is the Result of an Experiment?\n\n\n\nAll experiments give us data about phenomena\nWe obtain data about the things that happen: Outputs\nWhat makes things happen?: Inputs\nHow?: Process\nWhen? Factors\nHow much ‚Äúoutput‚Äù is caused by how much ‚Äúinput‚Äù? Effect Size\n\n\nAll Experiments stem from Human Curiosity, a Hypothesis, and a Desire to Find out and Talk about Something"
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#a-famous-lady-and-her-famous-experiment",
    "href": "content/slides/r-slides/nature/new.html#a-famous-lady-and-her-famous-experiment",
    "title": "The Nature of Data",
    "section": "A Famous Lady and her Famous Experiment",
    "text": "A Famous Lady and her Famous Experiment\n\n\n\n\n\n\n\n\n\n\nIn 1853, Turkey declared war on Russia. After the Russian Navy destroyed a Turkish squadron in the Black Sea, Great Britain and France joined with Turkey. In September of the following year, the British landed on the Crimean Peninsula and set out, with the French and Turks, to take the Russian naval base at Sevastopol.\nWhat followed was a tragicomedy of errors ‚Äì failure of supply, failed communications, international rivalries. Conditions in the armies were terrible, and disease ate through their ranks. They finally did take Sevastopol a year later, after a ghastly assault. It was ugly business all around. Well over half a million soldiers lost their lives during the Crimean War."
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#florence-nightingales-data",
    "href": "content/slides/r-slides/nature/new.html#florence-nightingales-data",
    "title": "The Nature of Data",
    "section": "Florence Nightingale‚Äôs Data",
    "text": "Florence Nightingale‚Äôs Data\n\n\n   Month Year Disease.rate Wounds.rate Other.rate\n1    Apr 1854          1.4         0.0        7.0\n2    May 1854          6.2         0.0        4.6\n3    Jun 1854          4.7         0.0        2.5\n4    Jul 1854        150.0         0.0        9.6\n5    Aug 1854        328.5         0.4       11.9\n6    Sep 1854        312.2        32.1       27.7\n7    Oct 1854        197.0        51.7       50.1\n8    Nov 1854        340.6       115.8       42.8\n9    Dec 1854        631.5        41.7       48.0\n10   Jan 1855       1022.8        30.7      120.0"
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#blank-slide",
    "href": "content/slides/r-slides/nature/new.html#blank-slide",
    "title": "The Nature of Data",
    "section": "Blank Slide",
    "text": "Blank Slide"
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#how-does-data-look-like-then",
    "href": "content/slides/r-slides/nature/new.html#how-does-data-look-like-then",
    "title": "The Nature of Data",
    "section": "How Does Data look Like, then?",
    "text": "How Does Data look Like, then?\n\n\n\n\nTypes of Variables - Using Interrogative Pronouns\n\nNominal: What? Who? Where? (Factors, Dimensions)\nOrdinal: Which Types? What Sizes? How Big? (Factors, Dimensions)\nInterval: How Often? (Numbers, Facts)\nRatio: How many? How much? How heavy? (Numbers, Facts)"
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#types-of-variables-in-nightingale-data",
    "href": "content/slides/r-slides/nature/new.html#types-of-variables-in-nightingale-data",
    "title": "The Nature of Data",
    "section": "Types of Variables in Nightingale Data",
    "text": "Types of Variables in Nightingale Data\n\n\nUsing Interrogative Pronouns\n\nNominal: None\nOrdinal: (Factors, Dimensions)\n\nHOW? War, Disease, Other\n\nInterval: (Numbers, Facts)\n\nWHEN? Year, Month\n\nRatio: (Numbers, Facts)\n\nHOW MANY? Rate of Deaths (War, Disease, Other)\n\n\n\n\n\n\n\n\nhttps://arvindvenkatadri.com"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#image-right",
    "href": "content/slides/r-slides/networks/index.html#image-right",
    "title": "Introduction to Networks in R",
    "section": ".image-right",
    "text": ".image-right\n\n\nWe can use the .image-right and .image-left classes to insert images in the background\nThese images will be placed behind most other content"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#image-left",
    "href": "content/slides/r-slides/networks/index.html#image-left",
    "title": "Introduction to Networks in R",
    "section": ".image-left",
    "text": ".image-left\n\n\n\n\nIt is therefore recommened that you use multiple columns to only have text on the background area"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#section",
    "href": "content/slides/r-slides/networks/index.html#section",
    "title": "Introduction to Networks in R",
    "section": "",
    "text": "background images"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#setting-background-colors",
    "href": "content/slides/r-slides/networks/index.html#setting-background-colors",
    "title": "Introduction to Networks in R",
    "section": "Setting background colors",
    "text": "Setting background colors\nyou can set your background as you normally would"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#video-slide-title",
    "href": "content/slides/r-slides/networks/index.html#video-slide-title",
    "title": "Introduction to Networks in R",
    "section": "Video Slide Title",
    "text": "Video Slide Title\nThis slides‚Äôs background video will play in a loop with audio muted."
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#slide-title",
    "href": "content/slides/r-slides/networks/index.html#slide-title",
    "title": "Introduction to Networks in R",
    "section": "Slide Title",
    "text": "Slide Title"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#further-modifying-theme",
    "href": "content/slides/r-slides/networks/index.html#further-modifying-theme",
    "title": "Introduction to Networks in R",
    "section": "Further Modifying theme",
    "text": "Further Modifying theme\nIf you want to modify theme, you can specify the .scss my modifying the yaml to look like this\nformat: \n  letterbox-revealjs:\n    theme: [default, style.scss]"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#modifying-letterbox-background",
    "href": "content/slides/r-slides/networks/index.html#modifying-letterbox-background",
    "title": "Introduction to Networks in R",
    "section": "Modifying letterbox background",
    "text": "Modifying letterbox background\nThe background colors can be with with, where #444444 represents the new background color and #222222 represents the color of the shadow\n.quarto-light {\n  background-color: #444444;\n}\n\n.quarto-dark {\n  background-color: #444444;\n}\n\n.slides {\n  box-shadow: #222222 0px 0px 30px 0px;\n}"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#quarto",
    "href": "content/slides/r-slides/networks/index.html#quarto",
    "title": "Introduction to Networks in R",
    "section": "Quarto",
    "text": "Quarto\nQuarto enables you to weave together content and executable code into a finished presentation. To learn more about Quarto presentations see https://quarto.org/docs/presentations/."
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#bullets",
    "href": "content/slides/r-slides/networks/index.html#bullets",
    "title": "Introduction to Networks in R",
    "section": "Bullets",
    "text": "Bullets\nWhen you click the Render button a document will be generated that includes:\n\nContent authored with markdown\nOutput from executable code"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#code",
    "href": "content/slides/r-slides/networks/index.html#code",
    "title": "Introduction to Networks in R",
    "section": "Code",
    "text": "Code\nWhen you click the Render button a presentation will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\n[1] 2"
  },
  {
    "objectID": "content/slides/r-slides/networks/LICENSE.html",
    "href": "content/slides/r-slides/networks/LICENSE.html",
    "title": "The Foundation Series",
    "section": "",
    "text": "MIT License\nCopyright (c) 2022 quarto-letterbox authors\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ‚ÄúSoftware‚Äù), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED ‚ÄúAS IS‚Äù, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "content/slides/r-slides/r-slides-listing.html",
    "href": "content/slides/r-slides/r-slides-listing.html",
    "title": "The Foundation Series",
    "section": "",
    "text": "Introduction to Networks in R\n\n\nUsing tidygraph and visNetwork\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nMIT License\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nMIT License\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nMetaphors with Graphics\n\n\nFrom Code to Geometry\n\n\n\n\n\n\n\n\n\nArvind Venkatadri\n\n\n\n\n\n\n  \n\n\n\n\nThe Grammar of Graphics in R\n\n\nUsing the tidyverse\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nThe Nature of Data\n\n\nHow does Human Experience link with Data?\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nThe Nature of Data\n\n\nHow does Human Experience link with Data?\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nWorking in R\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/slides/r-slides/working-in-R/index.html#introduction",
    "href": "content/slides/r-slides/working-in-R/index.html#introduction",
    "title": "Working in R",
    "section": "Introduction",
    "text": "Introduction"
  },
  {
    "objectID": "content/slides/r-slides/working-in-R/index.html#data-structures",
    "href": "content/slides/r-slides/working-in-R/index.html#data-structures",
    "title": "Working in R",
    "section": "Data Structures",
    "text": "Data Structures"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Metaphorics",
    "section": "",
    "text": "Hi, I‚Äôm Arvind Venkatadri.\nI‚Äôm an Adjunct Professor at DSU, Bangalore, INDIA, with a passion for R, Data Visualization, Complexity Science, and Creative Thinking and Problem Solving with TRIZ. On this website, I share my course materials and methods. I also blog about TRIZ and Data Science on occasion.\nTo get started, you can check out my courses. You can find me on Twitter, or GitHub, and on LinkedIn! Feel free to reach out to me via mail too!"
  },
  {
    "objectID": "readme.html",
    "href": "readme.html",
    "title": "The Foundation Series",
    "section": "",
    "text": "Preparatory Work to moving my full website to Quarto!"
  }
]