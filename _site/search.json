[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Hi, I’m Arvind Venkatadri.",
    "section": "",
    "text": "Hi, I’m Arvind Venkatadri.\nI’m an Adjunct Professor at DSU, Bangalore, INDIA, with a passion for R, Data Visualization, Complexity Science, Literature, and Creative Thinking and Problem Solving with TRIZ. On this blog, I share and teach what I learn.\nTo get started, you can check out my courses. You can find me on Twitter or GitHub and YouTube. Feel free to reach out to me via mail !\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi, I’m Arvind Venkatadri.",
    "section": "",
    "text": "Hi, I’m Arvind Venkatadri.\nI’m an Adjunct Professor at the School of Commerce and Management Studies, DSU, and an external Faculty Member at the Srishti Manipal Institute of Art, Design, and Technology (SMI), in Bangalore, INDIA. I am passionate about working on R, Data Visualization, Complexity Science, and Creative Thinking and Problem Solving with TRIZ. On this website, I share my course materials and methods. I also blog about TRIZ and Data Science on occasion.\nTo get started, you can check out my courses on this website.\nMy other teaching websites are:\n- Teaching R to Artists and Designers\n- Foundation Courses at Srishti\n\nMy student portfolios are here:\nhttps://we-r-us.netlify.app/portfolio/\nhttps://form-and-structure.netlify.app/portfolio/\n\nYou can find me on Twitter, or GitHub, and on LinkedIn! Feel free to reach out to me via mail too!\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/work-related/fsp-discussions/index.html",
    "href": "content/work-related/fsp-discussions/index.html",
    "title": "FSP Discussions 2021",
    "section": "",
    "text": "Slides and Tutorials\n.nbsp;.nbsp;"
  },
  {
    "objectID": "content/work-related/fsp-discussions/index.html#introduction",
    "href": "content/work-related/fsp-discussions/index.html#introduction",
    "title": "FSP Discussions 2021",
    "section": "Introduction",
    "text": "Introduction"
  },
  {
    "objectID": "content/work-related/fsp-discussions/index.html#this-is-a-summary-of-the-group-discussion-among",
    "href": "content/work-related/fsp-discussions/index.html#this-is-a-summary-of-the-group-discussion-among",
    "title": "FSP Discussions 2021",
    "section": "This is a summary of the group discussion among:",
    "text": "This is a summary of the group discussion among:\n1. Sadhvi Jawa\n2. Minashshi Singh\n3. Vidhu Gandhi\n4. Yash Bhandari\n5. Arvind Venkatadri"
  },
  {
    "objectID": "content/work-related/fsp-portfolio/index.html",
    "href": "content/work-related/fsp-portfolio/index.html",
    "title": "Teaching in this Pandemic Year 2020-2021",
    "section": "",
    "text": "This is a short Portfolio of Teaching Initiatives and Student Outcomes during this pandemic year, 2020-2021, from Arvind Venkatadri.\n\n\n\n Back to top"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html",
    "title": "🐉 Visualizing Categorical Data",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic) # Our trusted friend\nlibrary(skimr)\nlibrary(vcd) # Michael Friendly's package, Visualizing Categorical Data\nlibrary(vcdExtra) # Categorical Data Sets\nlibrary(ggmosaic) # Mosaic Plots\nlibrary(resampledata) # More datasets\n\nlibrary(GGally) # Correlation Plots\nlibrary(sjPlot) # Likert Scale Plots\nlibrary(sjlabelled) # Creating Labelled Data for Likert Plots\n\nlibrary(ggpubr) # Colours, Themes and new geometries in ggplot\nlibrary(ca) # Correspondence Analysis, for use some day\n\n## Making Tables\nlibrary(kableExtra) # html styled tables\nlibrary(gt)",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🐉 Visualizing Categorical Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#sec-setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#sec-setting-up-r-packages",
    "title": "🐉 Visualizing Categorical Data",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic) # Our trusted friend\nlibrary(skimr)\nlibrary(vcd) # Michael Friendly's package, Visualizing Categorical Data\nlibrary(vcdExtra) # Categorical Data Sets\nlibrary(ggmosaic) # Mosaic Plots\nlibrary(resampledata) # More datasets\n\nlibrary(GGally) # Correlation Plots\nlibrary(sjPlot) # Likert Scale Plots\nlibrary(sjlabelled) # Creating Labelled Data for Likert Plots\n\nlibrary(ggpubr) # Colours, Themes and new geometries in ggplot\nlibrary(ca) # Correspondence Analysis, for use some day\n\n## Making Tables\nlibrary(kableExtra) # html styled tables\nlibrary(gt)",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🐉 Visualizing Categorical Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#introduction",
    "title": "🐉 Visualizing Categorical Data",
    "section": "\n Introduction",
    "text": "Introduction\nTo recall, a categorical variable is one for which the possible measured or assigned values consist of a discrete set of categories, which may be ordered or unordered. Some typical examples are:\n\nGender, with categories “Male,” “Female.”\nMarital status, with categories “Never married,” “Married,” “Separated,” “Divorced,” “Widowed.”\nFielding position (in baseball cricket), with categories “Slips,”Cover “,”Mid-off “Deep Fine Leg”, “Close-in”, “Deep”…\nSide effects (in a pharmacological study), with categories “None,” “Skin rash,” “Sleep disorder,” “Anxiety,” . . ..\nPolitical attitude, with categories “Left,” “Center,” “Right.”\nParty preference (in India), with categories “BJP” “Congress,” “AAP,” “TMC”…\nTreatment outcome, with categories “no improvement,” “some improvement,” or “marked improvement.”\nAge, with categories “0–9,” “10–19,” “20–29,” “30–39,” . . . .\nNumber of children, with categories 0, 1, 2, . . . .\n\nAs these examples suggest, categorical variables differ in the number of categories: we often distinguish binary variables (or dichotomous variables) such as Gender from those with more than two categories (called polytomous variables).",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🐉 Visualizing Categorical Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#categorical-data",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#categorical-data",
    "title": "🐉 Visualizing Categorical Data",
    "section": "\n Categorical Data",
    "text": "Categorical Data\nFrom the {vcd package} vignette:\n\nThe first thing you need to know is that categorical data can be represented in three different forms in R, and it is sometimes necessary to convert from one form to another, for carrying out statistical tests, fitting models or visualizing the results.\n\n\nCase Data\nFrequency Data\nCross-Tabular Count Data\n\nLet us first see examples of each.\n\n\nCase Form\nFrequency Data Form\nTable form\n\n\n\nFrom Michael Friendly Discrete Data Analysis and Visualization :\n\nIn many circumstances, data is recorded on each individual or experimental unit. Data in this form is called case data, or data in case form. Containing individual observations with one or more categorical factors, used as classifying variables. The total number of observations is nrow(X), and the number of variables is ncol(X).\n\n\n\n R\n web-r\n\n\n\nclass(Arthritis)\n# Tibble as HTML for presentation\nArthritis %&gt;%  \n  head(10) %&gt;% \n  kbl(caption = \"Case Form: Arthritis Treatments and Effects&lt;br&gt; First 10 Observations\", centering = TRUE) %&gt;%\n  kable_classic_2(html_font = \"Cambria\", full_width = F) %&gt;% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"responsive\"))\n\n\n\n[1] \"data.frame\"\n\n\n\n\nCase Form: Arthritis Treatments and Effects\nFirst 10 Observations\n\nID\nTreatment\nSex\nAge\nImproved\n\n\n\n57\nTreated\nMale\n27\nSome\n\n\n46\nTreated\nMale\n29\nNone\n\n\n77\nTreated\nMale\n30\nNone\n\n\n17\nTreated\nMale\n32\nMarked\n\n\n36\nTreated\nMale\n46\nMarked\n\n\n23\nTreated\nMale\n58\nMarked\n\n\n75\nTreated\nMale\n59\nNone\n\n\n39\nTreated\nMale\n59\nMarked\n\n\n33\nTreated\nMale\n63\nNone\n\n\n55\nTreated\nMale\n63\nNone\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\nThe Arthritis data set has three factors and two integer variables. One of the three factors Improved is an ordered factor.\n\nID\nTreatment: a factor; Placebo or Treated\nSex: a factor, M / F\nAge: integer\nImproved: Ordinal factor; None &lt; Some &lt; Marked\n\nEach row in the Arthritis dataset is a separate case or observation.\n\n\nData in frequency form has already been tabulated and aggregated by counting over the (combinations of) categories of the table variables. When the data are in case form, we can always trace any observation back to its individual identifier or data record, since each row is a unique observation or case; the reverse, with the Frequency Form is rarely possible.\nFrequency Data is usually a data frame, with columns of categorical variables and at least one column containing frequency or count information.\n\n\n R\n web-r\n\n\n\n\nstr(GSS)\n\n'data.frame':   6 obs. of  3 variables:\n $ sex  : Factor w/ 2 levels \"female\",\"male\": 1 2 1 2 1 2\n $ party: Factor w/ 3 levels \"dem\",\"indep\",..: 1 1 2 2 3 3\n $ count: num  279 165 73 47 225 191\n\n# Tibble as HTML for presentation\nGSS %&gt;%\n  kbl(caption = \"General Social Survey\",centering = TRUE) %&gt;%\n  kable_classic_2(html_font = \"Cambria\", full_width = F) %&gt;% \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"responsive\"))\n\n\n\n\nGeneral Social Survey\n\nsex\nparty\ncount\n\n\n\nfemale\ndem\n279\n\n\nmale\ndem\n165\n\n\nfemale\nindep\n73\n\n\nmale\nindep\n47\n\n\nfemale\nrep\n225\n\n\nmale\nrep\n191\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nRespondents in the GSS survey were classified by sex and party identification. As can be seen, there is a count for every combination of the two categorical variables, sex and party.\n\n\nTable Form Data can be a matrix, array or table object, whose elements are the frequencies in an n-way table. The variable names (factors) and their levels are given by dimnames(X).\n\nHairEyeColor\n\n, , Sex = Male\n\n       Eye\nHair    Brown Blue Hazel Green\n  Black    32   11    10     3\n  Brown    53   50    25    15\n  Red      10   10     7     7\n  Blond     3   30     5     8\n\n, , Sex = Female\n\n       Eye\nHair    Brown Blue Hazel Green\n  Black    36    9     5     2\n  Brown    66   34    29    14\n  Red      16    7     7     7\n  Blond     4   64     5     8\n\nclass(HairEyeColor)\n\n[1] \"table\"\n\n\nHairEyeColor is a “two-way” table, consisting of two tables, one for Sex = Female and the other for Sex = Male. The total number of observations is sum(X). The number of dimensions of the table is length(dimnames(X)), and the table sizes are given by sapply(dimnames(X), length). The data looks like a n-dimensional cube and needs n-way tables to represent.\n\nsum(HairEyeColor)\n\n[1] 592\n\ndimnames(HairEyeColor)\n\n$Hair\n[1] \"Black\" \"Brown\" \"Red\"   \"Blond\"\n\n$Eye\n[1] \"Brown\" \"Blue\"  \"Hazel\" \"Green\"\n\n$Sex\n[1] \"Male\"   \"Female\"\n\nsapply(dimnames(HairEyeColor), length)\n\nHair  Eye  Sex \n   4    4    2 \n\n\nA good way to think of tabular data is to think of a Rubik’s Cube.\n\n\nRubik’s Cube model for Multi-Table Data\n\n\n\n\n\n\n\nRubik’s Cube and Categorical Data Tables\n\n\n\nEach of the edges is an Ordinal Variable, each segment represents a level in the variable. So each face of the Cube represents two ordinal variables. Any segment is at the intersection of two (independent) levels of two variables, and the colour may be visualized as a count. This array of counts on a face is a 2D or 2-Way Table. ( More on this later )\n\n\nSince we can only print 2D tables, we hold one face in front and the image we see is a 2-Way Table. Turning the Cube by 90 degrees gives us another face with 2 variables, with one variable in common with the previous face. If we consider two faces together, we get two 2-way tables, effectively allowing us to contemplate 3 categorical variables.\nMultiple 2-Way tables can be flattened into a single long table that contains all counts for all combinations of categorical variables. This can be visualized as “opening up” and laying flat the Rubik’s cube, as with a cardboard model of it.\n\nftable(HairEyeColor)\n\n            Sex Male Female\nHair  Eye                  \nBlack Brown       32     36\n      Blue        11      9\n      Hazel       10      5\n      Green        3      2\nBrown Brown       53     66\n      Blue        50     34\n      Hazel       25     29\n      Green       15     14\nRed   Brown       10     16\n      Blue        10      7\n      Hazel        7      7\n      Green        7      7\nBlond Brown        3      4\n      Blue        30     64\n      Hazel        5      5\n      Green        8      8\n\n\nFinally, we may need to convert the (multiple) tables into a data frame or tibble:\n## Convert the two tables into a data frame\nHairEyeColor %&gt;% \n  as_tibble() \n# Tibble as HTML for presentation\nHairEyeColor %&gt;% \n  as_tibble() %&gt;%  # Convert\n  kbl(caption = \"Hair Eye and Color\") %&gt;% \n  kable_classic_2(html_font = \"Cambria\", full_width = F) %&gt;% \n  kable_styling(bootstrap_options = c(\"hover\", \"striped\", \"responsive\"))\n\n\n\n\n  \n\n\n\n\n\nHair Eye and Color\n\nHair\nEye\nSex\nn\n\n\n\nBlack\nBrown\nMale\n32\n\n\nBrown\nBrown\nMale\n53\n\n\nRed\nBrown\nMale\n10\n\n\nBlond\nBrown\nMale\n3\n\n\nBlack\nBlue\nMale\n11\n\n\nBrown\nBlue\nMale\n50\n\n\nRed\nBlue\nMale\n10\n\n\nBlond\nBlue\nMale\n30\n\n\nBlack\nHazel\nMale\n10\n\n\nBrown\nHazel\nMale\n25\n\n\nRed\nHazel\nMale\n7\n\n\nBlond\nHazel\nMale\n5\n\n\nBlack\nGreen\nMale\n3\n\n\nBrown\nGreen\nMale\n15\n\n\nRed\nGreen\nMale\n7\n\n\nBlond\nGreen\nMale\n8\n\n\nBlack\nBrown\nFemale\n36\n\n\nBrown\nBrown\nFemale\n66\n\n\nRed\nBrown\nFemale\n16\n\n\nBlond\nBrown\nFemale\n4\n\n\nBlack\nBlue\nFemale\n9\n\n\nBrown\nBlue\nFemale\n34\n\n\nRed\nBlue\nFemale\n7\n\n\nBlond\nBlue\nFemale\n64\n\n\nBlack\nHazel\nFemale\n5\n\n\nBrown\nHazel\nFemale\n29\n\n\nRed\nHazel\nFemale\n7\n\n\nBlond\nHazel\nFemale\n5\n\n\nBlack\nGreen\nFemale\n2\n\n\nBrown\nGreen\nFemale\n14\n\n\nRed\nGreen\nFemale\n7\n\n\nBlond\nGreen\nFemale\n8",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🐉 Visualizing Categorical Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#sec-creating-contingency-tables",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#sec-creating-contingency-tables",
    "title": "🐉 Visualizing Categorical Data",
    "section": "\n Creating Contingency Tables",
    "text": "Creating Contingency Tables\nMany plots for Categorical Data ( as we shall see ) require that the data be converted into a Contingency Table ; the Statistical tests for Proportions ( the \\(\\chi^2\\) test ) also needs Contingency Tables. The Frequency Table we encountered earlier is very close to being a full-fledged Contingency Table; one only needs to add the margin counts! So what is a Contingency Table?\nFrom Wolfram Alpha:\n\nA contingency table, sometimes called a two-way frequency table, is a tabular mechanism with at least two rows and two columns used in statistics to present categorical data in terms of frequency counts. More precisely, an \\(r \\times c\\) contingency table shows the observed frequency of two variables the observed frequencies of which are arranged into \\(r\\) rows and \\(c\\) columns. The intersection of a row and a column of a contingency table is called a cell.\n\nIn this section we understand how to make Contingency Tables from each of the three forms. We will use vcd, mosaic and the tidyverse packages for our purposes. Then we will see how they can be visualized.\n\n\nUsing base R\nUsing vcd\nUsing mosaic\nUsing tidyverse\n\n\n\n\n# One Way Table ( one variable )\ntable(Arthritis$Treatment) # Contingency Table\n\n\nPlacebo Treated \n     43      41 \n\n# 1-way Contingency Table\ntable(Arthritis$Treatment) %&gt;% addmargins() # Contingency Table with margins\n\n\nPlacebo Treated     Sum \n     43      41      84 \n\n# 2-Way Contingency Tables\n# Choosing Treatment and Improved\ntable(Arthritis$Treatment, Arthritis$Improved) %&gt;% addmargins() \n\n         \n          None Some Marked Sum\n  Placebo   29    7      7  43\n  Treated   13    7     21  41\n  Sum       42   14     28  84\n\n# Choosing Treatment and Sex\ntable(Arthritis$Sex, Arthritis$Improved) %&gt;% addmargins()\n\n        \n         None Some Marked Sum\n  Female   25   12     22  59\n  Male     17    2      6  25\n  Sum      42   14     28  84\n\n\nWe can use table() ( and also xtabs() ) to generate multi-dimensional tables too (More than 2-way) These will be printed out as a series of 2D tables, one for each value/level of the “third” parameter. We can then flatten this set of tables using ftable() and add margins to convert into a Contingency Table:\n\nmy_arth_table &lt;- table(Arthritis$Treatment, Arthritis$Sex, Arthritis$Improved)\nmy_arth_table\n\n, ,  = None\n\n         \n          Female Male\n  Placebo     19   10\n  Treated      6    7\n\n, ,  = Some\n\n         \n          Female Male\n  Placebo      7    0\n  Treated      5    2\n\n, ,  = Marked\n\n         \n          Female Male\n  Placebo      6    1\n  Treated     16    5\n\n# Now flatten \nftable(my_arth_table) \n\n                None Some Marked\n                                \nPlacebo Female    19    7      6\n        Male      10    0      1\nTreated Female     6    5     16\n        Male       7    2      5\n\nftable(my_arth_table) %&gt;% addmargins()\n\n             Sum\n    19  7  6  32\n    10  0  1  11\n     6  5 16  27\n     7  2  5  14\nSum 42 14 28  84\n\n\nA bit strange that the column labels disappear in the ftable when margins are added…maybe need to investigate the FUN argument to add_margins().\n\n\nThe vcd ( Visualize Categorical Data ) package by Michael Friendly has a convenient function to create Contingency Tables: structable(); this function produces a ‘flat’ representation of a high-dimensional contingency table constructed by recursive splits (similar to the construction of mosaic charts/graphs). structable tends to render flat tables, of the kind that can be thought of as a “text representation” of the vcd::mosaic plot:\nThe arguments of structable are:\n\na formula \\(y + p \\sim x + z\\) which shows which variables are to be included as columns and rows respectively on a table;\na data argument, which can indicate a data frame from where the variables are drawn.\n\n# Three Way!!\narth_vcd &lt;- vcd::structable(data = Arthritis, Treatment ~ Improved + Sex)\narth_vcd\nclass(arth_vcd)\n\n\n\n                Treatment Placebo Treated\nImproved Sex                             \nNone     Female                19       6\n         Male                  10       7\nSome     Female                 7       5\n         Male                   0       2\nMarked   Female                 6      16\n         Male                   1       5\n[1] \"structable\" \"ftable\"    \n\n\n\n# With Margins\narth_vcd %&gt;% as.matrix() %&gt;% addmargins()\n\n\n\n               Treatment\nImproved_Sex    Placebo Treated Sum\n  None_Female        19       6  25\n  None_Male          10       7  17\n  Some_Female         7       5  12\n  Some_Male           0       2   2\n  Marked_Female       6      16  22\n  Marked_Male         1       5   6\n  Sum                43      41  84\n\n\n\n# HairEyeColor is in multiple table form\nHairEyeColor\n# structable flattens these into one, as for a mosaic chart\nvcd::structable(HairEyeColor)\n# As tibble\nvcd::structable(HairEyeColor) %&gt;% as_tibble()\n\n\n\n, , Sex = Male\n\n       Eye\nHair    Brown Blue Hazel Green\n  Black    32   11    10     3\n  Brown    53   50    25    15\n  Red      10   10     7     7\n  Blond     3   30     5     8\n\n, , Sex = Female\n\n       Eye\nHair    Brown Blue Hazel Green\n  Black    36    9     5     2\n  Brown    66   34    29    14\n  Red      16    7     7     7\n  Blond     4   64     5     8\n\n\n             Eye Brown Blue Hazel Green\nHair  Sex                              \nBlack Male          32   11    10     3\n      Female        36    9     5     2\nBrown Male          53   50    25    15\n      Female        66   34    29    14\nRed   Male          10   10     7     7\n      Female        16    7     7     7\nBlond Male           3   30     5     8\n      Female         4   64     5     8\n\n\n\n\n\n  \n\n\n\n\nUCBAdmissions is already in Frequency Form i.e. a Contingency Table. But it is a set of (two-way) Contingency Tables:\nUCBAdmissions\n###\nvcd::structable(UCBAdmissions)\n###\nstructable(UCBAdmissions) %&gt;% as.matrix() %&gt;% addmargins()\n\n\n\n, , Dept = A\n\n          Gender\nAdmit      Male Female\n  Admitted  512     89\n  Rejected  313     19\n\n, , Dept = B\n\n          Gender\nAdmit      Male Female\n  Admitted  353     17\n  Rejected  207      8\n\n, , Dept = C\n\n          Gender\nAdmit      Male Female\n  Admitted  120    202\n  Rejected  205    391\n\n, , Dept = D\n\n          Gender\nAdmit      Male Female\n  Admitted  138    131\n  Rejected  279    244\n\n, , Dept = E\n\n          Gender\nAdmit      Male Female\n  Admitted   53     94\n  Rejected  138    299\n\n, , Dept = F\n\n          Gender\nAdmit      Male Female\n  Admitted   22     24\n  Rejected  351    317\n\n\n              Gender Male Female\nAdmit    Dept                   \nAdmitted A            512     89\n         B            353     17\n         C            120    202\n         D            138    131\n         E             53     94\n         F             22     24\nRejected A            313     19\n         B            207      8\n         C            205    391\n         D            279    244\n         E            138    299\n         F            351    317\n\n\n\n\n            Gender\nAdmit_Dept   Male Female  Sum\n  Admitted_A  512     89  601\n  Admitted_B  353     17  370\n  Admitted_C  120    202  322\n  Admitted_D  138    131  269\n  Admitted_E   53     94  147\n  Admitted_F   22     24   46\n  Rejected_A  313     19  332\n  Rejected_B  207      8  215\n  Rejected_C  205    391  596\n  Rejected_D  279    244  523\n  Rejected_E  138    299  437\n  Rejected_F  351    317  668\n  Sum        2691   1835 4526\n\n\n\nNote that structable does not permit the adding of margins directly; it needs to be converted to a matrix for addmargins() to do its work.\n\n\nI think this is the simplest and most elegant way of obtaining Contingency Tables:\n# One Way Table\nmosaicCore::tally( ~ substance, data = HELPrct, margins = TRUE)\n# Two-Way Tables\n# Two ways of producing the same result\ntally( sex ~ substance, data = HELPrct, margins = TRUE)\ntally(~ sex | substance, data = HELPrct, margins = TRUE)\n\n\n\nsubstance\nalcohol cocaine  heroin   Total \n    177     152     124     453 \n\n\n        substance\nsex      alcohol cocaine heroin\n  female      36      41     30\n  male       141     111     94\n  Total      177     152    124\n\n\n\n\n        substance\nsex      alcohol cocaine heroin\n  female      36      41     30\n  male       141     111     94\n  Total      177     152    124\n\n\n\n\n\nSo far these packages give Contingency Tables that are easy to see for humans; some of these structures are also capable being passed directly to commands such as stats::chisq.test() or janitor::chisq.test().\nOften we need Contingency Tables that are in tibble form, and we need to perform some data processing using dplyr to get there. Doing this with the tidyverse set of packages may seem counter-intuitive and long-winded, but the workflow is easily understandable.\nFirst we develop the counts:\ndiamonds %&gt;% count(cut)\ndiamonds %&gt;% count(clarity)\ndiamonds %&gt;% \n  group_by(cut, clarity) %&gt;% \n  dplyr::summarise( count = n())\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n\n\n  \n\n\n\n\nWe need to have the individual levels of cut as rows and the individual levels of clarity as columns. This means that we need to pivot this from “long to wide”1 to obtain a Contingency Table:\n\ndiamonds %&gt;% \n  group_by(cut, clarity) %&gt;% \n  dplyr::summarise( count = n()) %&gt;% \n  \n  pivot_wider(id_cols = cut, \n              names_from = clarity, \n              values_from = count) %&gt;% \n  \n  # Now add the row and column totals using the `janitor` package\n  janitor::adorn_totals(where = c(\"row\", \"col\")) %&gt;%\n  \n  # Recover to tibble since janitor gives a \"tabyl\" format \n  # ( which can be useful too !)\n  as_tibble()\n\n\n  \n\n\n### Another Way \ndiamonds %&gt;% \n  group_by(cut, clarity) %&gt;% \n  dplyr::summarise( count = n()) %&gt;% \n  \n  pivot_wider(id_cols = cut, \n              names_from = clarity, \n              values_from = count) %&gt;% \n  \n  # Now add the row and column totals using the `dplyr` package\n  # From: https://stackoverflow.com/a/67885521\n  mutate(\"row_totals\"  = sum(across(where(is.integer)))) %&gt;% \n  ungroup() %&gt;% \n  add_row(cut = \"col_total\", summarize(., across(where(is.integer), sum)))",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🐉 Visualizing Categorical Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#plots-for-categorical-data",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#plots-for-categorical-data",
    "title": "🐉 Visualizing Categorical Data",
    "section": "\n Plots for Categorical Data",
    "text": "Plots for Categorical Data\nLet us now examine the various kinds of plots we can make with Categorical Data. We will start with simple Bar plots, then move to plotting entire Contingency Tables, and then look Balloon Plots as an alternative. Finally we will look at a special case of survey data and look at Likert Plots.\n\n Simple Bar Plots\nWe have already seen bar plots, which allow us to plot counts of categorical data. These can be used for say 2 or 3 Categorical variables, with not too many levels. However, for more complex data, if there are a large number of categorical variables, or if the categorical variables have many levels, the bar plot is not adequate.\nRecollect that the bar plot computes counts to plot with.\n\n Mosaic Plots\nFrom Michael Friendly:\n\nThe familiar techniques for displaying raw data are often disappointing when applied to categorical data. The simple scatterplot, for example, widely used to show the relation between quantitative response and predictors, when applied to discrete variables, gives a display of the category combinations, with all identical values overplotted, and no representation of their frequency. (AV: Scatter plots do not do counting internally!)\n\n\nInstead, frequencies of categorical variables are often best represented graphically using areas rather than as position along a scale. Using the visual attribute:\n\n\\[\\pmb{area \\sim frequency}\\]\n\nallows creating novel graphical displays of frequency data for special circumstances.\n\nLet us not look at some sample plots that embody this area-frequency principle. A mosaic plot is basically an area-proportional visualization of (typically observed) frequencies (i.e counts), consisting of tiles (corresponding to the cells) created by recursively splitting a rectangle vertically and horizontally. Thus, the area of each tile is proportional to the corresponding cell entry given the dimensions of previous splits.\n\n\nUsing vcd\nUsing ggmosaic\nUsing ggformula\nActual and Expected Contingency Tables\n\n\n\nThe vcd::mosaic() function needs the data in contingency table form. We will use our vcd::structable() function to construct one:\narthritis_table &lt;- vcd::structable(~ Treatment + Improved, \n                                   data = Arthritis)\narthritis_table\nvcd::mosaic(arthritis_table, \n            gp = shading_max, \n            main = \"Arthritis Treatment Dataset\")\n\n\n\n          Improved None Some Marked\nTreatment                          \nPlacebo              29    7      7\nTreated              13    7     21\n\n\n\n\n\n\n\ndata(\"GSS2002\", package = \"resampledata\")\n\ngss2002 &lt;- GSS2002 %&gt;% \n  # select two categorical variables from the dataset\n  dplyr::select(Education, DeathPenalty) %&gt;% \n  drop_na(Education, DeathPenalty)\ngss2002\n\n\n  \n\n\n# make a tally table\ngss_table &lt;- mosaic::tally(DeathPenalty ~ Education, data = gss2002)\ngss_table %&gt;% addmargins()\n\n            Education\nDeathPenalty Left HS   HS Jr Col Bachelors Graduate  Sum\n      Favor      117  511     71       135       64  898\n      Oppose      72  200     16        71       50  409\n      Sum        189  711     87       206      114 1307\n\n# gss_table is *not* a tibble, but a *table* object. \nvcd::mosaic(gss_table, gp = shading_hsv)\n\n\n\n\n\n\n\n\n\n\n\n\n\nPearson Residuals in Mosaic Plots\n\n\n\nThe mosaic chart provides more additional information than does the corresponding bar chart. The individual tiles are coloured based on the value of the Pearson Residual (explained below). The Pearson residual defines the (scaled) difference between the actual count in the cell, with what might be expected if there was no correlation between the two Qual variables.\nAn uncoloured Mosaic Chart indicates that the residual differences are small and that there is no correlation.\n\n\n\n\n\n\n\n\nWhat are Pearson Residuals?\n\n\n\nThe mosaic chart is a visualization of the obtained count on which the tile is constructed.\nIt is also possible to compute a per-cell expected count, if the categorical variables are independent, that is, not correlated. The null hypothesis is that the variables are independent. The test for independence, as any inferential test, is based on comparing the observed counts with the expected counts, under the null hypothesis. So, what might the expected frequency of a cell be in cross-tabulation table for cell \\(i,j\\) given no relationship between the variables of interest?\nRepresent the sum of row \\(i\\) with \\(n_{+i}\\), the sum of column \\(j\\) with \\(n_{j+}\\), and the grand total of all the observations with \\(n\\). And independence of variables means that their joint probability is the product of their probabilities. Therefore, the Expected Cell Frequency/Count is given by:\n\\[\n\\begin{array}{lcl} ~Expected~Count~ e_{i,j} & = & P(Row~and~Column) * n\\\\\n& = & P(row) \\times P(column) * n\\\\\n& = & \\frac{rowSum}{n} *\\frac{colSum}{n} \\times n\\\\\n& = &\\frac{rowSum *colSum}{n}\\\\\n\\end{array}\n\\]\nand therefore:\n\\[\ne_{i,j} = \\frac{(n_{+i})(n_{j+})}{n}\n\\]\nNow, the Pearson Residual in each cell is equivalent to the “z-score” of that cell.\nRecall the z-score idea: we subtract the mean and divide by the std. deviation to get the z-score. In the Contingency Table, we have counts which are usually modeled as an (integer) Poisson distribution, for which mean (i.e Expected value) and variance are identical. Thus we get the Pearson Residual as:\n\\[\nr_{i,j} = \\frac{(Actual - Expected)}{\\sqrt{\\displaystyle Expected}}\n\\] and therefore:\n\\[\nr_{i,j} = \\frac{(o_{i,j}- e_{i,j})}{\\sqrt{\\displaystyle e_{i,j}}}\n\\]\nThe comparison of what occurred to what is expected is based on their difference, scaled by the square root of the expected, the Pearson Residual.\nThe sum of all the squared Pearson residuals is the chi-square statistic, χ2, upon which the inferential analysis follows. \\[\nχ2 = \\sum_{i=1}^R\\sum_{j=1}^C{r_{i,j}^2}\n\\] where R and C are number of rows and columns in the Contingency Table, the levels in the two Qual variables.\nWe will treat the \\(X^2\\) test in the Module on Inference for Two Proportions.. See also the sub-section here below on Actual and Expected Contingency Tables.\n\n\n\n\nThis is perhaps the simplest way, but does use a different package and also does not use the formula notation: there is no gf_mosaic command yet!\nggmosaic takes a tibble with Qualitative variables, internally computes the counts/table, and plots the mosaic plot:\n\n#library(ggmosaic)\n#\n# Set graph theme\ntheme_set(new = theme_custom())\n\ngss2002\n\n\n  \n\n\nggplot(data = gss2002) +\n  ggmosaic::geom_mosaic(aes(x = product(DeathPenalty, Education), \n                  fill = DeathPenalty))\n\n\n\n\n\n\n\n\n\nThis needs quite some work, to convert the Contingency Table into a mosaic plot; perhaps not the most intuitive of methods either. This code has been developed using this Stackoverflow post.\n\n# Reference\n# https://stackoverflow.com/questions/19233365/how-to-create-a-marimekko-mosaic-plot-in-ggplot2\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\ngss_summary &lt;- gss2002 %&gt;% \n  dplyr::group_by(Education, DeathPenalty) %&gt;%\n  dplyr::summarise(count = n()) %&gt;% # This is good for a chisq test\n  \n  # Data is still grouped by `Education`\n  # Add two more columns to facilitate mosaic Plot\n  # These two columns are quite unusual...\n  mutate(edu_count = sum(count), \n         edu_prop = count / sum(count)) %&gt;%\n  ungroup()\ngss_summary\n\n\n  \n\n\n# This works but is not very intuitive...\ngf_col(edu_prop ~ Education, data = gss_summary,\n       width = ~ edu_count, # Not typically used in a column chart\n       fill = ~ DeathPenalty,\n       stat = \"identity\", \n       position = \"fill\", \n       color = \"black\") %&gt;% \n  \n  gf_text(edu_prop ~ Education, \n          label = ~ scales::percent(edu_prop),\n          position = position_stack(vjust = 0.5)) %&gt;% \n  \n  gf_facet_grid(~ Education, \n                scales = \"free_x\", \n                space = \"free_x\") %&gt;% \n  \n  gf_theme(scale_fill_manual(values = c(\"orangered\", \"palegreen3\"))) \n\n\n\n\n\n\n\n\n\nWe briefly. discussed this idea in the vcd sub-section above.\nContingency Tables reveal relationships between Qualitative variables, by examining counts of observations for each combination of the levels of the variables. Two Qual variables with \\(m\\) and \\(n\\) levels will give us a table of size \\(m \\times n\\).\nIn the module on statistical inference for Qualitative data we will see how we can create not only actual Contingency Tables ( as above) but that which would have existed if the two Qualitative variables had no relationship whatsoever. This is called the “Expected Contingency Table”.\nWe can calculate and plot both of these with the vcd package, as shown below:\nvcd::mosaic(~ Sex + Eye + Hair, data = HairEyeColor)\nvcd::mosaic(~ Sex + Eye + Hair, data = HairEyeColor, type = \"expected\")\n\n\n\n\n\n\n\n\nFigure 1: Actual Contingency Table\n\n\n\n\n\n\n\n\n\nFigure 2: Expected Contingency Table\n\n\n\n\n\nFrom an inspection of these paired plots, we see the difference between situatlons when Qualitative variables are not related to that when they are related.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🐉 Visualizing Categorical Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#balloon-plots",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#balloon-plots",
    "title": "🐉 Visualizing Categorical Data",
    "section": "\n Balloon Plots",
    "text": "Balloon Plots\nThere is another visualization of Categorical Data, called a Balloon Plot. We will use the housetasks dataset from the package ggpubr. This data is already in Contingency Table form (without the margin totals)!\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nhousetasks &lt;- read.delim(\n  system.file(\"demo-data/housetasks.txt\", \n              package = \"ggpubr\"), row.names = 1)\nhead(housetasks, 4)\n\n\n  \n\n\nggpubr::ggballoonplot(housetasks, fill = \"value\", \n                      #ggtheme = theme_pubr()\n                      ) +\n  scale_fill_viridis_c(option = \"C\") +\n  labs(title = \"A Balloon Plot for Categorical Data\")\n\n\n\n\n\n\n\nAnd repeat with the familiar HairEyeColor dataset:\n# Set graph theme\ntheme_set(new = theme_custom())\n\ndf &lt;- as_tibble(HairEyeColor)\ndf\nggballoonplot(df, x = \"Hair\", y = \"Eye\", size = \"n\",\n              fill = \"n\",\n              #ggtheme = theme_pubr()\n              ) +\n  scale_fill_viridis_c(option = \"C\") + \n  labs(title = \"Balloon Plot\")\n# Balloon Plot with facetting\nggballoonplot(df, x = \"Hair\", y = \"Eye\", size = \"n\",\n              fill = \"n\", facet.by = \"Sex\",\n              #ggtheme = theme_pubr()\n              ) +\n  scale_fill_viridis_c(option = \"C\") + \n  labs(title = \"Balloon Plot with Facetting\", \n       subtitle = \"Hair and Eye Color\")\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nNote the somewhat different syntax with ggballoonplot: the variable names are enclosed in quotes.\nBalloon Plots work because they use color and size aesthetics to represent categories and counts respectively.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🐉 Visualizing Categorical Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#conclusion",
    "title": "🐉 Visualizing Categorical Data",
    "section": "\n Conclusion",
    "text": "Conclusion\nHow are the bar plots for categorical data different from histograms? Why don’t “regular” scatter plots simply work for Categorical data? Discuss!\nThere are quite a few things we can do with Qualitative/Categorical data:\n\nMake simple bar charts with colours and facetting\nMake Contingency Tables for a \\(X^2\\)-test\nMake Mosaic Plots to show how the categories stack up\nMake Balloon Charts as an alternative\nMake Likert Charts for Survey Questionnaire Data\n\nThen, draw your inferences and tell the story!",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🐉 Visualizing Categorical Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#your-turn",
    "title": "🐉 Visualizing Categorical Data",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nTake some of the categorical datasets from the vcd and vcdExtra packages and recreate the plots from this module. Go to https://vincentarelbundock.github.io/Rdatasets/articles/data.html and type “vcd” in the search box. You can directly load CSV files from there, using read_csv(\"url-to-csv\").",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🐉 Visualizing Categorical Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#references",
    "title": "🐉 Visualizing Categorical Data",
    "section": "\n References",
    "text": "References\n\nNice Chi-square interactive story at https://statisticalstories.xyz/chi-square\nMine Cetinkaya-Rundel and Johanna Hardin. An Introduction to Modern Statistics, Chapter 4. https://openintro-ims.netlify.app/explore-categorical.html\nUsing the strcplot command from vcd, https://cran.r-project.org/web/packages/vcd/vignettes/strucplot.pdf\nCreating Frequency Tables with vcd, https://cran.r-project.org/web/packages/vcdExtra/vignettes/A_creating.html\nCreating mosaic plots with vcd, https://cran.r-project.org/web/packages/vcdExtra/vignettes/D_mosaics.html\nMichael Friendly, Corrgrams: Exploratory displays for correlation matrices. The American Statistician August 19, 2002 (v1.5). https://www.datavis.ca/papers/corrgram.pdf\nVisualizing Categorical Data in R\nH. Riedwyl & M. Schüpbach (1994), Parquet diagram to plot contingency tables. In F. Faulbaum (ed.), Softstat ’93: Advances in Statistical Software, 293–299. Gustav Fischer, New York.\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggmosaic\n0.3.3\nJeppson, Hofmann, and Cook (2021)\n\n\nggpubr\n0.6.0\nKassambara (2023)\n\n\njanitor\n2.2.0\nFirke (2023)\n\n\nkableExtra\n1.4.0\nZhu (2024)\n\n\nresampledata\n0.3.1\nChihara and Hesterberg (2018)\n\n\nsjlabelled\n1.2.0\nLüdecke (2022)\n\n\nsjPlot\n2.8.16\nLüdecke (2024)\n\n\nvcd\n1.4.12\n\nMeyer, Zeileis, and Hornik (2006); Zeileis, Meyer, and Hornik (2007); Meyer et al. (2023)\n\n\n\nvcdExtra\n0.8.5\nFriendly (2023)\n\n\n\n\n\n\nChihara, Laura M., and Tim C. Hesterberg. 2018. Mathematical Statistics with Resampling and r. 2nd ed. Hoboken, NJ: John Wiley & Sons. https://sites.google.com/site/chiharahesterberg/home.\n\n\nFirke, Sam. 2023. janitor: Simple Tools for Examining and Cleaning Dirty Data. https://CRAN.R-project.org/package=janitor.\n\n\nFriendly, Michael. 2023. vcdExtra: “vcd” Extensions and Additions. https://CRAN.R-project.org/package=vcdExtra.\n\n\nJeppson, Haley, Heike Hofmann, and Di Cook. 2021. ggmosaic: Mosaic Plots in the “ggplot2” Framework. https://CRAN.R-project.org/package=ggmosaic.\n\n\nKassambara, Alboukadel. 2023. ggpubr: “ggplot2” Based Publication Ready Plots. https://CRAN.R-project.org/package=ggpubr.\n\n\nLüdecke, Daniel. 2022. sjlabelled: Labelled Data Utility Functions (Version 1.2.0). https://doi.org/10.5281/zenodo.1249215.\n\n\n———. 2024. sjPlot: Data Visualization for Statistics in Social Science. https://CRAN.R-project.org/package=sjPlot.\n\n\nMeyer, David, Achim Zeileis, and Kurt Hornik. 2006. “The Strucplot Framework: Visualizing Multi-Way Contingency Tables with Vcd.” Journal of Statistical Software 17 (3): 1–48. https://doi.org/10.18637/jss.v017.i03.\n\n\nMeyer, David, Achim Zeileis, Kurt Hornik, and Michael Friendly. 2023. vcd: Visualizing Categorical Data. https://CRAN.R-project.org/package=vcd.\n\n\nZeileis, Achim, David Meyer, and Kurt Hornik. 2007. “Residual-Based Shadings for Visualizing (Conditional) Independence.” Journal of Computational and Graphical Statistics 16 (3): 507–25. https://doi.org/10.1198/106186007X237856.\n\n\nZhu, Hao. 2024. kableExtra: Construct Complex Table with “kable” and Pipe Syntax. https://CRAN.R-project.org/package=kableExtra.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🐉 Visualizing Categorical Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#footnotes",
    "title": "🐉 Visualizing Categorical Data",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://tidyr.tidyverse.org/articles/pivot.html↩︎",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🐉 Visualizing Categorical Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/files/parts.html",
    "href": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/files/parts.html",
    "title": "Tutorial: Part of a Whole in R",
    "section": "",
    "text": "We will create Data Visualizations in R to show Parts of a Whole. As always, we will consistently use the Project Mosaic ecosystem of packages in R (mosaic, mosaicData and ggformula). Some specialized plots ( e.g. Fan Plots) may require us to load other R Packages. These will be introduced appropriately.\n\nRecall the standard method for all commands from the mosaic package:\ngoal( y ~ x | z, data = mydata, …)\n\n\n\n\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggridges\n0.5.6\nWilke (2024)\n\n\nNHANES\n2.1.0\nPruim (2015)\n\n\nTeachHist\n0.2.1\nLange (2023)\n\n\nTeachingDemos\n2.13\nSnow (2024)\n\n\n\n\n\n\nLange, Carsten. 2023. TeachHist: A Collection of Amended Histograms Designed for Teaching Statistics. https://CRAN.R-project.org/package=TeachHist.\n\n\nPruim, Randall. 2015. NHANES: Data from the US National Health and Nutrition Examination Study. https://CRAN.R-project.org/package=NHANES.\n\n\nSnow, Greg. 2024. TeachingDemos: Demonstrations for Teaching and Learning. https://CRAN.R-project.org/package=TeachingDemos.\n\n\nWilke, Claus O. 2024. ggridges: Ridgeline Plots in “ggplot2”. https://CRAN.R-project.org/package=ggridges."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/files/parts.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/files/parts.html#references",
    "title": "Tutorial: Part of a Whole in R",
    "section": "",
    "text": "Package\nVersion\nCitation\n\n\n\nggridges\n0.5.6\nWilke (2024)\n\n\nNHANES\n2.1.0\nPruim (2015)\n\n\nTeachHist\n0.2.1\nLange (2023)\n\n\nTeachingDemos\n2.13\nSnow (2024)\n\n\n\n\n\n\nLange, Carsten. 2023. TeachHist: A Collection of Amended Histograms Designed for Teaching Statistics. https://CRAN.R-project.org/package=TeachHist.\n\n\nPruim, Randall. 2015. NHANES: Data from the US National Health and Nutrition Examination Study. https://CRAN.R-project.org/package=NHANES.\n\n\nSnow, Greg. 2024. TeachingDemos: Demonstrations for Teaching and Learning. https://CRAN.R-project.org/package=TeachingDemos.\n\n\nWilke, Claus O. 2024. ggridges: Ridgeline Plots in “ggplot2”. https://CRAN.R-project.org/package=ggridges."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html",
    "title": "EDA: Interactive Correlation Graphs in R",
    "section": "",
    "text": "We will create Tables for Correlations, and graphs for Correlations in R. As always, we will consistently use the Project Mosaic ecosystem of packages in R (mosaic, mosaicData and ggformula).\n\n\nlibrary(tidyverse)\nlibrary(mosaic) # package for stats, simulations, and basic plots\nlibrary(mosaicData) # package containing datasets\nlibrary(ggformula) # package for professional looking plots, that use the formula interface from mosaic\nlibrary(NHANES) # survey data collected by the US National Center for Health Statistics (NCHS)\nlibrary(corrplot) # For Correlogram plots\nlibrary(plotly)\nlibrary(echarts4r)\n\n\n\n\n\n\n\nInteractive Graphs with echarts4r\n\n\n\nWe will also start using echarts4r side by side for interactive graphs.\n\nEvery function in the package starts with e_.\nYou start coding a visualization by creating an echarts object with the e_charts() function. That takes your data frame and x-axis column as arguments.\nNext, you add a function for the type of chart (e_line(), e_bar(), etc.) with the y-axis series column name as an argument.\nThe rest is mostly customization! echarts4r takes some effort in getting used to, but it totally worth it!\n\n\n\n\nLet us inspect what datasets are available in the package mosaicData. Run this command in your Console: data(package = “mosaicData”)\nThe popup tab shows a lot of datasets we could use. Let us continue to use the famous Galton dataset and inspect it: (We will save the inspect output as an R object for use later)\n\ndata(\"Galton\")\ngalton_describe &lt;- inspect(Galton)\ngalton_describe$categorical\n\n\n  \n\n\ngalton_describe$quantitative\n\n\n  \n\n\n\nThe inspect command already gives us a series of statistical measures of different variables of interest. As discussed previously, we can retain the output of inspect and use it in our reports: (there are ways of dressing up these tables too)\nThe dataset is described as:Try help(\"Galton\") in your Console.\n\nA data frame with 898 observations on the following variables.\n- family a factor with levels for each family\n- father the father’s height (in inches)\n- mother the mother’s height (in inches)\n- sex the child’s sex: F or M\n- height the child’s height as an adult (in inches)\n- nkids the number of adult children in the family, or, at least, the number whose heights Galton recorded.\n\nThere is a lot of Description generated by the mosaic::inspect() command ! What can we say about the dataset and its variables? How big is the dataset? How many variables? What types are they, Quant or Qual? If they are Qual, what are the levels? Are they ordered levels? Discuss!\n\nWhat Questions might we have, that we could answer with a Statistical Measure, or Correlation chart?\n\n\n\n\n\n\nPair-wise Correlation Plot\n\n\n\nQ.1 Which are the variables that have significant pair-wise correlations? What polarity are these correlations?\n\n# Pulling out the list of Quant variables from NHANES\ngalton_quant &lt;- galton_describe$quantitative\ngalton_quant$name\n\n[1] \"father\" \"mother\" \"height\" \"nkids\" \n\nGGally::ggpairs(\n  Galton,\n  columns = c(\"father\", \"mother\", \"height\", \"nkids\"),\n  diag = list(\"densityDiag\"),\n  title = \"Galton Data Correlations Plot\"\n) %&gt;% \n  plotly::ggplotly()\n\n\n\n\n\nInsight: There are significant, but low value correlations in the Galton dataset. height is best correlated with father (\\(0.275\\)). The Scatter Plots shown in the plot also visually demonstrate the (lack of) large value correlations.\nWe cannot have too many variables in this kind of plot. We will shortly see how to plot correlations when there are a large number of variables.\n\n\n\n\n\n\n\n\nHeatmap\n\n\n\necharts4r does not have a comprehensive combination plot like what GGally offers. However, we can plot a Correlation Heatmap using echarts4r:\n\nGalton %&gt;% select(where(is.numeric)) %&gt;% \n  mosaic::cor() %&gt;% \n  e_charts(height = 300) %&gt;% \n  e_correlations(order = \"hclust\", visual_map = TRUE) %&gt;% \n  e_title(\"Galton Correlations Heatmap\")\n\n\n\n\n\nInsight: Moving the cursor over the heatmap gives us the an indication of the correlation scores between variables. The visual map slider moves automatically to indicate the scores. We can also move the slider ourselves to “filter” the heatmap!\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.2: Can we plot a Correlogram for this dataset?\n\n#library(corrplot)\n\ngalton_num_var &lt;- Galton %&gt;% select(father, mother, height, nkids)\ngalton_cor &lt;- cor(galton_num_var)\ngalton_cor %&gt;%\n  corrplot(method = \"ellipse\",\n           type = \"lower\",\n           main = \"Correlogram for Galton dataset\")\n\n\n\n\n\n\n\nInsight: Again, height is positively correlated to father and mother as depicted by the rightward-sloping blue ellipses. And height is negatively correlated (very slightly) with nkids, with leftward-sloping reddish ellipses. (See the color palette + legend below the figure).\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.3: What do the correlation tests tell us?\nmosaic::cor_test(height ~ father, data = Galton)\nmosaic::cor_test(height ~ mother, data = Galton)\n\n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and father\nt = 8.5737, df = 896, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2137851 0.3347455\nsample estimates:\n      cor \n0.2753548 \n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and mother\nt = 6.1628, df = 896, p-value = 1.079e-09\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.1380554 0.2635982\nsample estimates:\n      cor \n0.2016549 \n\n\n\nInsight: The tests give us the same values seen before, along with the confidence intervals for the correlation estimate. These represent the uncertainty that exists in our estimates.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.4: What does this correlation look when split by sex of Child?\nWe will use the mosaic function cor_test to get these results:\n# For the sons\nmosaic::cor_test(height ~ father, \n                 data = Galton %&gt;% filter(sex == \"M\"))\ncor_test(height ~ mother, data = Galton %&gt;% \n           filter(sex == \"M\"))\n\n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and father\nt = 9.1498, df = 463, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3114667 0.4656805\nsample estimates:\n      cor \n0.3913174 \n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and mother\nt = 7.628, df = 463, p-value = 1.367e-13\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2508178 0.4125305\nsample estimates:\n      cor \n0.3341309 \n\n\n\n# For the daughters\ncor_test(height ~ father, \n         data = Galton %&gt;% filter(sex == \"F\"))\ncor_test(height ~ mother, \n         data = Galton %&gt;% filter(sex == \"F\"))\n\n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and father\nt = 10.719, df = 431, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3809944 0.5300812\nsample estimates:\n      cor \n0.4587605 \n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and mother\nt = 6.8588, df = 431, p-value = 2.421e-11\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2261463 0.3962226\nsample estimates:\n      cor \n0.3136984 \n\n\n\nInsight: Son’s heights are correlated more with father than with mother. This trend is even more so for daughters! Hmmm…mother’s influence on children is clearly not with height.\n\n\n\n\n\n\n\n\n\nCorrelation Tests and Uncertainty\n\n\n\nNote how the cor.test reports a correlation score and the p-value for the same. There is also a confidence interval reported for the correlation score, an interval within which we are 95% sure that the true correlation value is to be found. Note that GGally too reports the significance of the correlation scores using *** or **. This indicates the p-value in the scores obtained by GGally; Presumably, there is an internal cor.test that is run for each pair of variables and the p-value and confidence levels are also computed internally.\nWe can also visualise this uncertainty and the confidence levels in a plot too, using gf_errorbar and a handy set of functions within purrr which is part of the tidyverse: Assuming heights is the target variable we want to correlate every other (quantitative) variable against, we can proceed very quickly as follows:\n\nall_corrs &lt;- Galton %&gt;% \n  select(where(is.numeric)) %&gt;% \n  \n  # leave off height to get all the remaining ones\n  select(- height) %&gt;%  \n  \n  # perform a cor.test for all variables against height\n  purrr::map(.x = .,\n             .f = \\(x) cor.test(x, Galton$height)) %&gt;%\n  \n  # tidy up the cor.test outputs into a tidy data frame\n  map_dfr(broom::tidy, .id = \"predictor\") \n\nall_corrs\n\n\n  \n\n\n\n\nall_corrs %&gt;% \n  e_charts(predictor) %&gt;% \n  e_bar(estimate, colorBy = \"data\", legend = FALSE) %&gt;% \n  e_error_bar(lower = conf.low, upper = conf.high) %&gt;% \n  \n  e_y_axis(name = \"Correlation with `height`\", \n           nameLocation = \"middle\", nameGap = 35) %&gt;% \n  e_x_axis(name = \"Parameter\", nameLocation = \"center\",\n           nameGap = 35, type = \"category\") %&gt;% \n  e_tooltip()\n\n\n\n\nall_corrs %&gt;% \n  mutate(sd = (conf.high-conf.low)/2) %&gt;% \n  plot_ly() %&gt;% add_bars(y = ~estimate, x = ~predictor, \n                         error_y = ~ list(array = sd, color = \"black\"))\n\n\n\n\n\nInsight: We can clearly see the size of the correlations and the confidence intervals marked in this plot. father has somewhat greater correlation with children’s height, as compared to mother. nkids seems to matter very slightly, in a negative way.\nThis kind of plot will be very useful when we pursue linear regression models.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.5. How can we show this correlation in a set of Scatter Plots + Regression Lines? Can we recreate Galton’s famous diagram?\n# For the father\nGalton %&gt;% \n  group_by(sex) %&gt;% \n  e_charts(father, height = 300) %&gt;% \n  e_scatter(height,symbol_size = 8) %&gt;% \n  e_lm(height ~ father,legend = FALSE) %&gt;% \n  e_x_axis(name = \"father\", nameLocation = \"middle\", nameGap = 35,\n           min = 60, max = 80) %&gt;% \n  e_y_axis(name = \"height\", nameLocation = \"middle\", nameGap = 35,\n           min = 50, max = 80) %&gt;% \n  e_tooltip()\n# for the mother\nGalton %&gt;% \n  group_by(sex) %&gt;% \n  e_charts(mother, height = 300) %&gt;% \n  e_scatter(height,symbol_size = 8) %&gt;% \n  e_lm(height ~ mother,legend = FALSE) %&gt;% \n  e_x_axis(name = \"mother\", nameLocation = \"middle\", nameGap = 35,\n           min = 55, max = 75) %&gt;% \n  e_y_axis(name = \"height\", nameLocation = \"middle\", nameGap = 35,\n           min = 50, max = 80) %&gt;% \n  e_tooltip()\n\n\n\n\n\n\n\n\n\n\n\n\nInsight: Visibly the scatter plots are slightly tilted upward to the right, showing a positive correlation for both sons’ and daughters’ heights with that of the father and mother.\n\n\n\n\n\n\n\n\nGalton’s Plot\n\n\n\nAn approximation to Galton’s famous plot (see Wikipedia):\n\ngf_point(height ~ (father + mother)/2, data = Galton) %&gt;% \n  gf_smooth(method = \"lm\") %&gt;% \n  gf_density_2d(n = 8) %&gt;% \n  gf_abline(slope = 1) %&gt;% \n  gf_theme(theme_minimal())\n\n\n\n\n\n\n\nInsight: How would you interpret this plot1? As yet we are not able to reproduce this with charts4r.\n\n\n\nWe will “live code” this in class!\n\nWe have a decent Correlations related workflow in R:\n- load the dataset\n- inspect the dataset, identify Quant and Qual variables\n- Develop Pair-Wise plots + Correlations using GGally::ggpairs()\n- Develop Correlogram corrplot::corrplot\n- Check everything with a cor_test\n- Use purrr + cor.test to plot correlations and confidence intervals for multiple Quant variables\n- Plot scatter plots using gf_point.\n- Add extra lines using gf_abline() to compare hypotheses that you may have."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#setting-up-r-packages",
    "title": "EDA: Interactive Correlation Graphs in R",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic) # package for stats, simulations, and basic plots\nlibrary(mosaicData) # package containing datasets\nlibrary(ggformula) # package for professional looking plots, that use the formula interface from mosaic\nlibrary(NHANES) # survey data collected by the US National Center for Health Statistics (NCHS)\nlibrary(corrplot) # For Correlogram plots\nlibrary(plotly)\nlibrary(echarts4r)\n\n\n\n\n\n\n\nInteractive Graphs with echarts4r\n\n\n\nWe will also start using echarts4r side by side for interactive graphs.\n\nEvery function in the package starts with e_.\nYou start coding a visualization by creating an echarts object with the e_charts() function. That takes your data frame and x-axis column as arguments.\nNext, you add a function for the type of chart (e_line(), e_bar(), etc.) with the y-axis series column name as an argument.\nThe rest is mostly customization! echarts4r takes some effort in getting used to, but it totally worth it!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#case-study-1-dataset-from-mosaicdata",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#case-study-1-dataset-from-mosaicdata",
    "title": "EDA: Interactive Correlation Graphs in R",
    "section": "",
    "text": "Let us inspect what datasets are available in the package mosaicData. Run this command in your Console: data(package = “mosaicData”)\nThe popup tab shows a lot of datasets we could use. Let us continue to use the famous Galton dataset and inspect it: (We will save the inspect output as an R object for use later)\n\ndata(\"Galton\")\ngalton_describe &lt;- inspect(Galton)\ngalton_describe$categorical\n\n\n  \n\n\ngalton_describe$quantitative\n\n\n  \n\n\n\nThe inspect command already gives us a series of statistical measures of different variables of interest. As discussed previously, we can retain the output of inspect and use it in our reports: (there are ways of dressing up these tables too)\nThe dataset is described as:Try help(\"Galton\") in your Console.\n\nA data frame with 898 observations on the following variables.\n- family a factor with levels for each family\n- father the father’s height (in inches)\n- mother the mother’s height (in inches)\n- sex the child’s sex: F or M\n- height the child’s height as an adult (in inches)\n- nkids the number of adult children in the family, or, at least, the number whose heights Galton recorded.\n\nThere is a lot of Description generated by the mosaic::inspect() command ! What can we say about the dataset and its variables? How big is the dataset? How many variables? What types are they, Quant or Qual? If they are Qual, what are the levels? Are they ordered levels? Discuss!\n\nWhat Questions might we have, that we could answer with a Statistical Measure, or Correlation chart?\n\n\n\n\n\n\nPair-wise Correlation Plot\n\n\n\nQ.1 Which are the variables that have significant pair-wise correlations? What polarity are these correlations?\n\n# Pulling out the list of Quant variables from NHANES\ngalton_quant &lt;- galton_describe$quantitative\ngalton_quant$name\n\n[1] \"father\" \"mother\" \"height\" \"nkids\" \n\nGGally::ggpairs(\n  Galton,\n  columns = c(\"father\", \"mother\", \"height\", \"nkids\"),\n  diag = list(\"densityDiag\"),\n  title = \"Galton Data Correlations Plot\"\n) %&gt;% \n  plotly::ggplotly()\n\n\n\n\n\nInsight: There are significant, but low value correlations in the Galton dataset. height is best correlated with father (\\(0.275\\)). The Scatter Plots shown in the plot also visually demonstrate the (lack of) large value correlations.\nWe cannot have too many variables in this kind of plot. We will shortly see how to plot correlations when there are a large number of variables.\n\n\n\n\n\n\n\n\nHeatmap\n\n\n\necharts4r does not have a comprehensive combination plot like what GGally offers. However, we can plot a Correlation Heatmap using echarts4r:\n\nGalton %&gt;% select(where(is.numeric)) %&gt;% \n  mosaic::cor() %&gt;% \n  e_charts(height = 300) %&gt;% \n  e_correlations(order = \"hclust\", visual_map = TRUE) %&gt;% \n  e_title(\"Galton Correlations Heatmap\")\n\n\n\n\n\nInsight: Moving the cursor over the heatmap gives us the an indication of the correlation scores between variables. The visual map slider moves automatically to indicate the scores. We can also move the slider ourselves to “filter” the heatmap!\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.2: Can we plot a Correlogram for this dataset?\n\n#library(corrplot)\n\ngalton_num_var &lt;- Galton %&gt;% select(father, mother, height, nkids)\ngalton_cor &lt;- cor(galton_num_var)\ngalton_cor %&gt;%\n  corrplot(method = \"ellipse\",\n           type = \"lower\",\n           main = \"Correlogram for Galton dataset\")\n\n\n\n\n\n\n\nInsight: Again, height is positively correlated to father and mother as depicted by the rightward-sloping blue ellipses. And height is negatively correlated (very slightly) with nkids, with leftward-sloping reddish ellipses. (See the color palette + legend below the figure).\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.3: What do the correlation tests tell us?\nmosaic::cor_test(height ~ father, data = Galton)\nmosaic::cor_test(height ~ mother, data = Galton)\n\n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and father\nt = 8.5737, df = 896, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2137851 0.3347455\nsample estimates:\n      cor \n0.2753548 \n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and mother\nt = 6.1628, df = 896, p-value = 1.079e-09\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.1380554 0.2635982\nsample estimates:\n      cor \n0.2016549 \n\n\n\nInsight: The tests give us the same values seen before, along with the confidence intervals for the correlation estimate. These represent the uncertainty that exists in our estimates.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.4: What does this correlation look when split by sex of Child?\nWe will use the mosaic function cor_test to get these results:\n# For the sons\nmosaic::cor_test(height ~ father, \n                 data = Galton %&gt;% filter(sex == \"M\"))\ncor_test(height ~ mother, data = Galton %&gt;% \n           filter(sex == \"M\"))\n\n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and father\nt = 9.1498, df = 463, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3114667 0.4656805\nsample estimates:\n      cor \n0.3913174 \n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and mother\nt = 7.628, df = 463, p-value = 1.367e-13\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2508178 0.4125305\nsample estimates:\n      cor \n0.3341309 \n\n\n\n# For the daughters\ncor_test(height ~ father, \n         data = Galton %&gt;% filter(sex == \"F\"))\ncor_test(height ~ mother, \n         data = Galton %&gt;% filter(sex == \"F\"))\n\n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and father\nt = 10.719, df = 431, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3809944 0.5300812\nsample estimates:\n      cor \n0.4587605 \n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and mother\nt = 6.8588, df = 431, p-value = 2.421e-11\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2261463 0.3962226\nsample estimates:\n      cor \n0.3136984 \n\n\n\nInsight: Son’s heights are correlated more with father than with mother. This trend is even more so for daughters! Hmmm…mother’s influence on children is clearly not with height."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#correlation-tests-and-uncertainty",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#correlation-tests-and-uncertainty",
    "title": "EDA: Interactive Correlation Graphs in R",
    "section": "",
    "text": "Correlation Tests and Uncertainty\n\n\n\nNote how the cor.test reports a correlation score and the p-value for the same. There is also a confidence interval reported for the correlation score, an interval within which we are 95% sure that the true correlation value is to be found. Note that GGally too reports the significance of the correlation scores using *** or **. This indicates the p-value in the scores obtained by GGally; Presumably, there is an internal cor.test that is run for each pair of variables and the p-value and confidence levels are also computed internally.\nWe can also visualise this uncertainty and the confidence levels in a plot too, using gf_errorbar and a handy set of functions within purrr which is part of the tidyverse: Assuming heights is the target variable we want to correlate every other (quantitative) variable against, we can proceed very quickly as follows:\n\nall_corrs &lt;- Galton %&gt;% \n  select(where(is.numeric)) %&gt;% \n  \n  # leave off height to get all the remaining ones\n  select(- height) %&gt;%  \n  \n  # perform a cor.test for all variables against height\n  purrr::map(.x = .,\n             .f = \\(x) cor.test(x, Galton$height)) %&gt;%\n  \n  # tidy up the cor.test outputs into a tidy data frame\n  map_dfr(broom::tidy, .id = \"predictor\") \n\nall_corrs\n\n\n  \n\n\n\n\nall_corrs %&gt;% \n  e_charts(predictor) %&gt;% \n  e_bar(estimate, colorBy = \"data\", legend = FALSE) %&gt;% \n  e_error_bar(lower = conf.low, upper = conf.high) %&gt;% \n  \n  e_y_axis(name = \"Correlation with `height`\", \n           nameLocation = \"middle\", nameGap = 35) %&gt;% \n  e_x_axis(name = \"Parameter\", nameLocation = \"center\",\n           nameGap = 35, type = \"category\") %&gt;% \n  e_tooltip()\n\n\n\n\nall_corrs %&gt;% \n  mutate(sd = (conf.high-conf.low)/2) %&gt;% \n  plot_ly() %&gt;% add_bars(y = ~estimate, x = ~predictor, \n                         error_y = ~ list(array = sd, color = \"black\"))\n\n\n\n\n\nInsight: We can clearly see the size of the correlations and the confidence intervals marked in this plot. father has somewhat greater correlation with children’s height, as compared to mother. nkids seems to matter very slightly, in a negative way.\nThis kind of plot will be very useful when we pursue linear regression models.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.5. How can we show this correlation in a set of Scatter Plots + Regression Lines? Can we recreate Galton’s famous diagram?\n# For the father\nGalton %&gt;% \n  group_by(sex) %&gt;% \n  e_charts(father, height = 300) %&gt;% \n  e_scatter(height,symbol_size = 8) %&gt;% \n  e_lm(height ~ father,legend = FALSE) %&gt;% \n  e_x_axis(name = \"father\", nameLocation = \"middle\", nameGap = 35,\n           min = 60, max = 80) %&gt;% \n  e_y_axis(name = \"height\", nameLocation = \"middle\", nameGap = 35,\n           min = 50, max = 80) %&gt;% \n  e_tooltip()\n# for the mother\nGalton %&gt;% \n  group_by(sex) %&gt;% \n  e_charts(mother, height = 300) %&gt;% \n  e_scatter(height,symbol_size = 8) %&gt;% \n  e_lm(height ~ mother,legend = FALSE) %&gt;% \n  e_x_axis(name = \"mother\", nameLocation = \"middle\", nameGap = 35,\n           min = 55, max = 75) %&gt;% \n  e_y_axis(name = \"height\", nameLocation = \"middle\", nameGap = 35,\n           min = 50, max = 80) %&gt;% \n  e_tooltip()\n\n\n\n\n\n\n\n\n\n\n\n\nInsight: Visibly the scatter plots are slightly tilted upward to the right, showing a positive correlation for both sons’ and daughters’ heights with that of the father and mother.\n\n\n\n\n\n\n\n\nGalton’s Plot\n\n\n\nAn approximation to Galton’s famous plot (see Wikipedia):\n\ngf_point(height ~ (father + mother)/2, data = Galton) %&gt;% \n  gf_smooth(method = \"lm\") %&gt;% \n  gf_density_2d(n = 8) %&gt;% \n  gf_abline(slope = 1) %&gt;% \n  gf_theme(theme_minimal())\n\n\n\n\n\n\n\nInsight: How would you interpret this plot1? As yet we are not able to reproduce this with charts4r."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#case-study-2-dataset-from-nhanes",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#case-study-2-dataset-from-nhanes",
    "title": "EDA: Interactive Correlation Graphs in R",
    "section": "",
    "text": "We will “live code” this in class!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#conclusion",
    "title": "EDA: Interactive Correlation Graphs in R",
    "section": "",
    "text": "We have a decent Correlations related workflow in R:\n- load the dataset\n- inspect the dataset, identify Quant and Qual variables\n- Develop Pair-Wise plots + Correlations using GGally::ggpairs()\n- Develop Correlogram corrplot::corrplot\n- Check everything with a cor_test\n- Use purrr + cor.test to plot correlations and confidence intervals for multiple Quant variables\n- Plot scatter plots using gf_point.\n- Add extra lines using gf_abline() to compare hypotheses that you may have."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#footnotes",
    "title": "EDA: Interactive Correlation Graphs in R",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://www.researchgate.net/figure/Galtons-smoothed-correlation-diagram-for-the-data-on-heights-of-parents-and-children_fig15_226400313↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Ranking/ranking.html",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Ranking/ranking.html",
    "title": "👌 Ratings and Rankings",
    "section": "",
    "text": "TBD.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "👌 Ratings and Rankings"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Ranking/ranking.html#slides-and-tutorials",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Ranking/ranking.html#slides-and-tutorials",
    "title": "👌 Ratings and Rankings",
    "section": "",
    "text": "TBD.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "👌 Ratings and Rankings"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Ranking/ranking.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Ranking/ranking.html#setting-up-r-packages",
    "title": "👌 Ratings and Rankings",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nknitr::opts_chunk$set(echo = TRUE)\nlibrary(tidyverse) # includes ggplot for plotting\nlibrary(mosaic)\nlibrary(ggformula)\nlibrary(RColorBrewer) # colour palettes\n\nlibrary(ggbump) # Bump Charts\nlibrary(ggiraphExtra) # Radar, Spine, Donut and Donut-Pie combo charts !!\nlibrary(ggalt) # New geometries, coordinate systems, statistical transformations, scales and fonts\n\n# install.packages(\"devtools\")\n# devtools::install_github(\"ricardo-bion/ggradar\")\nlibrary(ggradar) # Radar Plots",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "👌 Ratings and Rankings"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Ranking/ranking.html#what-graphs-are-we-going-to-see-today",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Ranking/ranking.html#what-graphs-are-we-going-to-see-today",
    "title": "👌 Ratings and Rankings",
    "section": "\n What graphs are we going to see today?",
    "text": "What graphs are we going to see today?\nWhen we wish to compare the size of things and rank them, there are quite a few ways to do it.\nBar Charts and Lollipop Charts are immediately obvious when we wish to rank things on one aspect or parameter, e.g. mean income vs gender. We can also put two lollipop charts back-to-back to make a Dumbbell Chart to show comparisons/ranks across two datasets based on one aspect, e.g change in mean income over two years, across gender.\nWhen we wish to rank the multiple objects against multiple aspects or parameters, then we can use Bump Charts and Radar Charts, e.g performance of one or more products against multiple criteria (cost, size, performance…)s.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "👌 Ratings and Rankings"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Ranking/ranking.html#lollipop-charts",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Ranking/ranking.html#lollipop-charts",
    "title": "👌 Ratings and Rankings",
    "section": "\n Lollipop Charts",
    "text": "Lollipop Charts\nLet’s make a toy dataset of Products and Ratings:\n\n# Sample data set\nset.seed(1)\ndf1 &lt;- tibble(product = LETTERS[1:10],\n              rank = sample(20:35, 10, replace = TRUE))\ndf1\n\n# A tibble: 10 × 2\n   product  rank\n   &lt;chr&gt;   &lt;int&gt;\n 1 A          28\n 2 B          23\n 3 C          26\n 4 D          20\n 5 E          21\n 6 F          32\n 7 G          26\n 8 H          30\n 9 I          33\n10 J          21\n\n\n\n\nUsing ggformula\nUsing ggplot\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\n###\ngf_segment(0 + rank ~ product + product, data = df1) %&gt;% \n# A formula with shape y + yend ~ x + xend.\n  \n  gf_point(rank ~ product, colour = ~ product, \n           size = 5,\n           ylab = \"Rank\",\n           xlab = \"Product\") \n###\ngf_segment(0 + rank ~ fct_reorder(product, - rank) + \n             fct_reorder(product, - rank), \n           data = df1) %&gt;%\n  # A formula with shape y + yend ~ x + xend.\n  \n  gf_point(rank ~ product, colour = ~ product, size = 5) %&gt;%\n  \n  gf_refine(coord_flip()) %&gt;%\n  gf_labs(x = \"Product\", y = \"Rank\") \n\n\n\n\n\n\n\n\n\n\nWe have flipped the chart horizontally and reordered the \\(x\\) categories in order of decreasing ( or increasing ) \\(y\\), using forcats::fct_reorder.\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\n###\nggplot(df1) +\n  geom_segment(aes(y = 0, yend = rank, \n                   x = product, \n                   xend = product)) +\n  geom_point(aes(y = rank, x = product, colour = product), size = 5) +\n  labs(title = \"Product Ratings\", x = \"Product\", y = \"Rank\")\n###\nggplot(df1) +\n  geom_segment(aes(y = 0, yend = rank, \n                   x = fct_reorder(product, -rank),\n                   xend = fct_reorder(product, -rank)))  +\n  geom_point(aes(x = product, y = rank, colour = product), size = 5) +\n  labs(title = \"Product Ratings\", x = \"Product\", y = \"Rank\")  +\n  coord_flip()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBusiness Insights from Lollipop Plots\n\n\n\n\nVery simple chart, almost like a bar chart\nDifferences between the same set of data across one aspect (i.e. rank) is very quickly apparent\n\nOrdering the dataset by the attribute (i.e ordering product by rank) makes the message very clear.\nEven a large number of data can safely be visualized and understood",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "👌 Ratings and Rankings"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Ranking/ranking.html#dumbbell-charts",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Ranking/ranking.html#dumbbell-charts",
    "title": "👌 Ratings and Rankings",
    "section": "\n Dumbbell Charts",
    "text": "Dumbbell Charts\nA lollipop chart compares a set of data against one aspect. What if we have more than one? Say sales in many product lines across two years?\nLet us once again construct a very similar looking toy dataset, but with two columns for ratings, one for each of two years:\n\n# Sample data set\n# Wide Format data!\nset.seed(2)\ndf2 &lt;- tibble(product = LETTERS[1:10],\n              rank_year1 = sample(20:35, 10, replace = TRUE),\n              rank_year2 = sample(15:45, 10, replace = TRUE))\ndf2\n\n# A tibble: 10 × 3\n   product rank_year1 rank_year2\n   &lt;chr&gt;        &lt;int&gt;      &lt;int&gt;\n 1 A               24         43\n 2 B               34         23\n 3 C               25         32\n 4 D               25         25\n 5 E               35         15\n 6 F               27         17\n 7 G               20         36\n 8 H               32         30\n 9 I               20         32\n10 J               31         33\n\n\nA short diversion: we can also make this data into long form: this will become useful very shortly!\n\n\n\n\n\n\n Wide Form and Long Form Data\n\n\n\nLook at the data: this is wide form data. The columns pertaining to each of the Product-Features would normally be stacked into two columns, one with the Feature and the other with the score. Note the trio: Qual(product) + Qual(year) + Quant(scores):\n\n# With Long Format Data\ndf2_long &lt;- df2 %&gt;% \n  pivot_longer(cols = c(dplyr::starts_with(\"rank\")), \n               names_to = \"year\", values_to = \"scores\")\ndf2_long\n\n# A tibble: 20 × 3\n   product year       scores\n   &lt;chr&gt;   &lt;chr&gt;       &lt;int&gt;\n 1 A       rank_year1     24\n 2 A       rank_year2     43\n 3 B       rank_year1     34\n 4 B       rank_year2     23\n 5 C       rank_year1     25\n 6 C       rank_year2     32\n 7 D       rank_year1     25\n 8 D       rank_year2     25\n 9 E       rank_year1     35\n10 E       rank_year2     15\n11 F       rank_year1     27\n12 F       rank_year2     17\n13 G       rank_year1     20\n14 G       rank_year2     36\n15 H       rank_year1     32\n16 H       rank_year2     30\n17 I       rank_year1     20\n18 I       rank_year2     32\n19 J       rank_year1     31\n20 J       rank_year2     33\n\n\nA cool visualization of this operation was created by Garrick Aden-Buie:\n\n\n\n\n\nUsing ggformula\nUsing ggplot\nUsing ggalt\nComparison barchart\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\n## With Wide Form Data\n## \ndf2 %&gt;% gf_segment(product + product ~ rank_year1 + rank_year2 ) %&gt;% \n  gf_point(product ~ rank_year1, size = 3, colour = \"#123456\") %&gt;% \n  gf_point(product ~ rank_year2, size = 3, colour = \"#bad744\") %&gt;% gf_labs(x = \"Rank\", y = \"Product\")\n\n\n\n\n\n\n## With Long Form Data\ndf2_long %&gt;% \n  gf_segment(product + product ~ scores + scores, \n             group = ~ year, colour = \"grey\", \n             linewidth = 2, data = ., caption = \"Segments not Showing? Why?\") %&gt;% \n  gf_point(size = 3, colour = ~ year)\n\n\n\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\n\n## With Wide Format Data\nggplot(df2, aes(y = product, yend = product, x = rank_year1, xend = rank_year2)) + \n  geom_segment(size = 3, color=\"#e3e2e1\") + \n  geom_point(aes(rank_year1, product), colour = \"#5b8124\", size = 3)   +\n  geom_point(aes(rank_year2, product), colour = \"#bad744\", size = 3) + \n  labs(x = \"Rank\", y = \"Product\")\n## With Long Form Data\ndf2_long %&gt;% \n  ggplot(aes(y = product, x = scores, group = year, colour = year)) + \n  geom_segment(aes(y = product, yend = product, x = scores, xend = scores, group = year)) + \n  geom_point(size = 3) +\n  labs(caption = \"Segments not showing? Why?\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\ndf2 %&gt;% ggplot() + \n  geom_dumbbell(aes(y = product, x = rank_year1, xend = rank_year2),\n                size = 3, color = \"#e3e2e1\", \n                colour_x = \"#5b8124\", colour_xend = \"#bad744\",\n                dot_guide=TRUE,  # Try FALSE\n                dot_guide_size=0.25) +\n  \n  labs(x = NULL, y = NULL, title = \"ggplot2 geom_dumbbell with dot guide\", caption = \"Made with ggalt\") +\n  #theme_minimal() +\n  theme(panel.grid.major.x=element_line(size=0.05)) +\n  theme(panel.grid.major.y=element_blank())\n\n\n\n\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\ndf2_long %&gt;% \n  gf_col(product ~ scores, group = ~ year, fill = ~ year, position = \"dodge\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBusiness Insights from Dumbbell Plots\n\n\n\n\nDumbbell Plots are clearly they are more intuitive and clear than the bar chart\nDifferences between the same set of data at two different aspects is very quickly apparent\n\nDifferences in differences(DID) are also quite easily apparent. Experiments do use these metrics and these plots would be very useful there.\n\nggalt works nicely with additional visible guides rendered in the chart\nHowever: long form data does not seem to work out very well. There is no chart rendered and the code does not throw an error either…need to investigate!",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "👌 Ratings and Rankings"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Ranking/ranking.html#bump-charts",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Ranking/ranking.html#bump-charts",
    "title": "👌 Ratings and Rankings",
    "section": "\n Bump Charts",
    "text": "Bump Charts\nBump Charts track the ranking of several objects based on other parameters, such as time/month or even category. For instance, what is the opinion score of a set of products across various categories of users?\n\nyear &lt;- rep(2019:2021, 4)\nposition &lt;- c(4, 2, 2, 3, 1, 4, 2, 3, 1, 1, 4, 3)\nproduct &lt;- c(\"A\", \"A\", \"A\",\n            \"B\", \"B\", \"B\", \n            \"C\", \"C\", \"C\",\n            \"D\", \"D\", \"D\")\n\ndf3 &lt;- tibble(year,position,product)\n\ndf3\n\n# A tibble: 12 × 3\n    year position product\n   &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;  \n 1  2019        4 A      \n 2  2020        2 A      \n 3  2021        2 A      \n 4  2019        3 B      \n 5  2020        1 B      \n 6  2021        4 B      \n 7  2019        2 C      \n 8  2020        3 C      \n 9  2021        1 C      \n10  2019        1 D      \n11  2020        4 D      \n12  2021        3 D      \n\n\n\n\n\n\n\n\nggbump uses ggplot syntax\n\n\n\nWe need to use a new package called, what else, ggbump to create our Bump Charts: Here again we do not yet have a ggformula equivalent. ( Though it may be possible with a combination of gf_point and gf_polygon, and pre-computing the coordinates. Seems long-winded.)\nNote the + syntax with ggplot code!!\n\n\n\n#library(ggbump)\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\n###\ndf3 %&gt;%  \n  ggplot() +\n  geom_bump(aes(x = year, y = position, color = product)) +\n  geom_point(aes(x = year, y = position, color = product),\n             size = 6) +\n  xlab(\"Year\") +\n  ylab(\"Rank\") +\n  scale_color_brewer(palette = \"RdBu\") + # Change Colour Scale\n  scale_x_discrete(limits = c(2019, 2020, 2021)) # Check warning here...\n\n\n\n\n\n\n\nWe can add labels along the “bump lines” and remove the legend altogether:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\n###\nggplot(df3) +\n  geom_bump(aes(x = year, y = position, color = product)) +\n  geom_point(aes(x = year, y = position, color = product),\n             size = 6) +\n  scale_color_brewer(palette = \"RdBu\") + # Change Colour Scale\n# Same as before up to here\n# Add the labels at start and finish\n\n  geom_text(data = df3 %&gt;% filter(year == min(year)),\n            aes(x = year - 0.1, label = product, y = position),\n            size = 5, hjust = 1) +\n  geom_text(data = df3 %&gt;% filter(year == max(year)),\n            aes(x = year + 0.1, label = product, y = position),\n            size = 5, hjust = 0) +\n  xlab(\"Year\") +\n  ylab(\"Rank\") +\n  scale_x_discrete(limits = c(2019:2021)) + \n  theme(legend.position = \"\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nBusiness Insights from Bump Charts\n\n\n\n\nBump charts are good for depicting Ranks/Scores pertaining to a set of data, as they vary over another aspect, for a set of products\nCannot have too many levels in the aspect parameter, else the graph gets too hard to make sense with.\nFor instance if we had 10 years in the data above, we would have lost the plot, literally! Better off with a dumbbell plot in such a case.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "👌 Ratings and Rankings"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Ranking/ranking.html#radar-charts",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Ranking/ranking.html#radar-charts",
    "title": "👌 Ratings and Rankings",
    "section": "\n Radar Charts",
    "text": "Radar Charts\nWhat if your marketing folks had rated some products along several different desirable criteria? Such data, where a certain set of items (Qualitative!!) are rated (Quantitative!) against another set (Qualitative again!!) can be plotted on a roughly circular set of axes, with the radial distance defining the rank against each axes. Such a plot is called a radar plot.\nOf course, we will use the aptly named ggradar, which is at this time (Feb 2023) a development version and not yet part of CRAN. We will still try it, and another package ggiraphExtra which IS a part of CRAN (and has some other capabilities too, which are worth exploring!)\n\n#library(ggradar)\n\nset.seed(4)\ndf4 &lt;- tibble(Product = c(\"G1\", \"G2\", \"G3\"),\n              Power = runif(3), \n              Cost = runif(3),\n              Harmony = runif(3),\n              Style = runif(3),\n              Size = runif(3),\n              Manufacturability = runif(3),\n              Durability = runif(3),\n              Universality = runif(3))\ndf4\n\n# A tibble: 3 × 9\n  Product   Power  Cost Harmony  Style  Size Manufacturability Durability\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;             &lt;dbl&gt;      &lt;dbl&gt;\n1 G1      0.586   0.277   0.724 0.0731 0.100             0.455      0.962\n2 G2      0.00895 0.814   0.906 0.755  0.954             0.971      0.762\n3 G3      0.294   0.260   0.949 0.286  0.416             0.584      0.715\n# ℹ 1 more variable: Universality &lt;dbl&gt;\n\n\n\n\n\n\n\n\n Using ggradar\n\n\n\n\nggradar::ggradar(plot.data = df4,\n                 axis.label.size = 3, # Titles of Params\n                 grid.label.size = 4, # Score Values/Circles\n                 group.point.size = 3,# Product Points Sizes\n                 group.line.width = 1, # Product Line Widths\n                 fill = TRUE, # fill the radar polygons\n                 fill.alpha = 0.3, # Not too dark, Arvind\n                 legend.title = \"Product\") \n\n\n\n\n\n\n\n\n Using ggiraphExtra\n\nFrom the ggiraphExtra website:\n\nPackage ggiraphExtra contains many useful functions for exploratory plots. These functions are made by both ‘ggplot2’ and ‘ggiraph’ packages. You can make a static ggplot or an interactive ggplot by setting the parameter interactive=TRUE.\n\n\n# library(ggiraphExtra)\n\nggiraphExtra::ggRadar(data = df4,\n        aes(colour = Product),\n        interactive = FALSE, # try TRUE\n        rescale = FALSE,\n        title = \"Using ggiraphExtra\"\n          )  + # recale = TRUE makes it look different...try!!\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBusiness Insights from Radar Plots\n\n\n\n\nDifferences in scores for a given item across several aspect or parameters are readily apparent.\nThese can also be compared, parameter for parameter, with more than one item\nthe same set of data at two different aspects is very quickly apparent\nData is clearly in wide form\nBoth ggradar and ggiraphExtra render very similar-looking radar charts and the syntax is not too intimidating!!",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "👌 Ratings and Rankings"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Ranking/ranking.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Ranking/ranking.html#your-turn",
    "title": "👌 Ratings and Rankings",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nTake the HELPrct dataset from our well used mosaicData package. Plot ranking charts using each of the public health issues that you can see in that dataset. What choice will you make for the the axes?\nTry the SaratogaHouses dataset also from mosaicData.\n\n\n\n\n\nmpg %&gt;% gf_point(hwy ~ cty)\n\n\n\n\n\n\n\n\n\nmpg %&gt;% gf_point(hwy ~ cty, \n                 color = ~ cyl)",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "👌 Ratings and Rankings"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Ranking/ranking.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Ranking/ranking.html#references",
    "title": "👌 Ratings and Rankings",
    "section": "\n References",
    "text": "References\n\nHighcharts Blog. Why you need to start using dumbbell chartshttps://github.com/hrbrmstr/ggalt#lollipop-charts\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggalt\n0.4.0\nRudis, Bolker, and Schulz (2017)\n\n\nggbump\n0.1.0\nSjoberg (2020)\n\n\nggiraphExtra\n0.3.0\nMoon (2020)\n\n\nggradar\n0.2\nBion (2024)\n\n\n\n\n\n\nBion, Ricardo. 2024. ggradar: Create Radar Charts Using Ggplot2. https://github.com/ricardo-bion/ggradar.\n\n\nMoon, Keon-Woong. 2020. ggiraphExtra: Make Interactive “ggplot2.” Extension to “ggplot2” and “ggiraph”. https://CRAN.R-project.org/package=ggiraphExtra.\n\n\nRudis, Bob, Ben Bolker, and Jan Schulz. 2017. ggalt: Extra Coordinate Systems, “Geoms,” Statistical Transformations, Scales and Fonts for “ggplot2”. https://CRAN.R-project.org/package=ggalt.\n\n\nSjoberg, David. 2020. ggbump: Bump Chart and Sigmoid Curves. https://CRAN.R-project.org/package=ggbump.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "👌 Ratings and Rankings"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html",
    "title": "🕔 Time Series",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(ggformula) # Our Formula based graphing package\n\n# Wrangling\nlibrary(lubridate)  # Deal with dates. Part of the tidyverse anyway!\n\nlibrary(fpp3) # Robert Hyndman's textbook package, Loads all the core time series packages, see messages\n\n# Plots\nlibrary(timetk) # Tidy Time series analysis and plots\nlibrary(tsbox) # Plotting and Time Series File Transformations\n# library(TSstudio) # Plots, Decomposition, and Modelling with Time Series.\n# Seems hard to get to work in Quarto ;-()\nlibrary(timetk) # Visualizing, Wrangling and Modelling Time Series by Matt Dancho\n\n# Modelling\nlibrary(sweep) # New (07/2023) package to bring broom-like features to time series models\n\n# devtools::install_github(\"FinYang/tsdl\")\nlibrary(tsdl) # Time Series Data Library from Rob Hyndman\n\n\n\n\n\n\n\nmosaic and ggformula command template\n\n\n\nNote the standard method for all commands from the mosaic and ggformula packages: goal( y ~ x | z, data = _____)\nWith ggformula, one can create any graph/chart using: gf_***(y ~ x | z, data = _____)\nIn practice, we often use: dataframe %&gt;%  gf_***(y ~ x | z) which has cool benefits such as “autocompletion” of variable names, as we shall see. The “***” indicates what kind of graph you desire: histogram, bar, scatter, density; the “___” is the name of your dataset that you want to plot with.\n\n\n\n\n\n\n\n\nggplot command template\n\n\n\nThe ggplot2 template is used to identify the dataframe, identify the x and y axis, and define visualized layers:\nggplot(data = ---, mapping = aes(x = ---, y = ---)) + geom_----()\nNote: —- is meant to imply text you supply. e.g. function names, data frame names, variable names.\nIt is helpful to see the argument mapping, above. In practice, rather than typing the formal arguments, code is typically shorthanded to this:\ndataframe %&gt;%  ggplot(aes(xvar, yvar)) + geom_----()"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#setting-up-r-packages",
    "title": "🕔 Time Series",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(ggformula) # Our Formula based graphing package\n\n# Wrangling\nlibrary(lubridate)  # Deal with dates. Part of the tidyverse anyway!\n\nlibrary(fpp3) # Robert Hyndman's textbook package, Loads all the core time series packages, see messages\n\n# Plots\nlibrary(timetk) # Tidy Time series analysis and plots\nlibrary(tsbox) # Plotting and Time Series File Transformations\n# library(TSstudio) # Plots, Decomposition, and Modelling with Time Series.\n# Seems hard to get to work in Quarto ;-()\nlibrary(timetk) # Visualizing, Wrangling and Modelling Time Series by Matt Dancho\n\n# Modelling\nlibrary(sweep) # New (07/2023) package to bring broom-like features to time series models\n\n# devtools::install_github(\"FinYang/tsdl\")\nlibrary(tsdl) # Time Series Data Library from Rob Hyndman\n\n\n\n\n\n\n\nmosaic and ggformula command template\n\n\n\nNote the standard method for all commands from the mosaic and ggformula packages: goal( y ~ x | z, data = _____)\nWith ggformula, one can create any graph/chart using: gf_***(y ~ x | z, data = _____)\nIn practice, we often use: dataframe %&gt;%  gf_***(y ~ x | z) which has cool benefits such as “autocompletion” of variable names, as we shall see. The “***” indicates what kind of graph you desire: histogram, bar, scatter, density; the “___” is the name of your dataset that you want to plot with.\n\n\n\n\n\n\n\n\nggplot command template\n\n\n\nThe ggplot2 template is used to identify the dataframe, identify the x and y axis, and define visualized layers:\nggplot(data = ---, mapping = aes(x = ---, y = ---)) + geom_----()\nNote: —- is meant to imply text you supply. e.g. function names, data frame names, variable names.\nIt is helpful to see the argument mapping, above. In practice, rather than typing the formal arguments, code is typically shorthanded to this:\ndataframe %&gt;%  ggplot(aes(xvar, yvar)) + geom_----()"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#introduction",
    "title": "🕔 Time Series",
    "section": "\n Introduction",
    "text": "Introduction\nAny metric that is measured over regular time intervals forms a time series. Analysis of Time Series is commercially important because of industrial need and relevance, especially with respect to Forecasting (Weather data, sports scores, population growth figures, stock prices, demand, sales, supply…). For example, in the graph shown below are the temperatures over time in two US cities:\n\n\nWhat can we do with Time Series? As with other datasets, we have to begin by answering fundamental questions, such as:\n\nWhat are the types of time series?\nHow do we visualize time series?\nHow might we summarize time series to get aggregate numbers, say by week, month, quarter or year?\nHow do we decompose the time series into level, trend, and seasonal components?\nHoe might we make a model of the underlying process that creates these time series?\nHow do we make useful forecasts with the data we have?\n\nWe will first look at the multiple data formats for time series in R. Alongside we will look at the R packages that work with these formats and create graphs and measures using those objects. Then we examine data wrangling of time series, where we look at packages that offer dplyr-like ability to group and summarize time series using the time variable. We will finally look at obtaining the components of the time series and try our hand at modelling and forecasting."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#time-series-formats-conversion-and-plotting",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#time-series-formats-conversion-and-plotting",
    "title": "🕔 Time Series",
    "section": "\n Time Series Formats, Conversion, and Plotting",
    "text": "Time Series Formats, Conversion, and Plotting\nThere are multiple formats for time series data. The ones that we are likely to encounter most are:\n\nThe ts format: We may simply have a single series of measurements that are made over time, stored as a numerical vector. The stats::ts() function will convert a numeric vector into an R time series ts object, which is the most basic time series object in R. The base-R ts object is used by established packages forecast and is also supported by newer packages such as tsbox.\nThe tibble format: the simplest and most familiar data format is of course the standard tibble/data frame, with or without an explicit time column/variable to indicate that the other variables vary with time. The standard tibble object is used by many packages, e.g. timetk & modeltime.\nThe modern tsibble format: this is a new modern format for time series analysis. The special tsibble object (“time series tibble”) is used by fable, feasts and others from the tidyverts set of packages.\n\nThere are many other time-oriented data formats too…probably too many, such a tibbletime and TimeSeries objects. For now the best way to deal with these, should you encounter them, is to convert them (Using tsbox) to a tibble or a tsibble and work with these.\n\n\nStandards\n\nTo start, we will use simple ts data first, and then do another with tibble format that we can plot as is. We will then do more after conversion to tsibble format, and then a third example with a ground-up tsibble dataset.\n\n Base-R ts format data\nThere are a few datasets in base R that are in ts format already.\n\nAirPassengers\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1949 112 118 132 129 121 135 148 148 136 119 104 118\n1950 115 126 141 135 125 149 170 170 158 133 114 140\n1951 145 150 178 163 172 178 199 199 184 162 146 166\n1952 171 180 193 181 183 218 230 242 209 191 172 194\n1953 196 196 236 235 229 243 264 272 237 211 180 201\n1954 204 188 235 227 234 264 302 293 259 229 203 229\n1955 242 233 267 269 270 315 364 347 312 274 237 278\n1956 284 277 317 313 318 374 413 405 355 306 271 306\n1957 315 301 356 348 355 422 465 467 404 347 305 336\n1958 340 318 362 348 363 435 491 505 404 359 310 337\n1959 360 342 406 396 420 472 548 559 463 407 362 405\n1960 417 391 419 461 472 535 622 606 508 461 390 432\n\nstr(AirPassengers)\n\n Time-Series [1:144] from 1949 to 1961: 112 118 132 129 121 135 148 148 136 119 ...\n\n\nThis can be easily plotted using base R and other more recent packages:\n# Base R\nplot(AirPassengers)\n# tsbox static plot\ntsbox::ts_plot(AirPassengers,ylab = \"Passengers\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne can see that there is an upward trend and also seasonal variations that also increase over time. This is an example of a multiplicative time series, which we will discuss later.\nLet us take data that is “time oriented” but not in ts format. We use the command ts to convert a numeric vector to ts format: the syntax of ts() is:\nSyntax: objectName &lt;- ts(data, start, end, frequency), where,\n\n\ndata : represents the data vector\n\nstart : represents the first observation in time series\n\nend : represents the last observation in time series\n\nfrequency : represents number of observations per unit time. For example 1=annual, 4=quarterly, 12=monthly, 7=weekly, etc.\n\nWe will pick simple numerical vector data ( i.e. not a time series ) ChickWeight:\n\nChickWeight %&gt;% head()\n\n\n  \n\n\n# Filter for Chick #1 and for Diet #1\nChickWeight_ts &lt;- ChickWeight %&gt;% \n  filter(Chick == 1, Diet ==1) %&gt;% \n  select(weight, Time)\n\nChickWeight_ts &lt;- stats::ts(ChickWeight_ts$weight, frequency = 2) \nstr(ChickWeight_ts)\n\n Time-Series [1:12] from 1 to 6.5: 42 51 59 64 76 93 106 125 149 171 ...\n\n\nNow we can plot this in many ways:\nplot(ChickWeight_ts) # Using base-R\n# ts_boxable(ChickWeight_ts)\n# Using tsbox\ntsbox::ts_plot(ChickWeight_ts,\n               ylab = \"Weight of Chick #1\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Using TSstudio\nTSstudio::ts_plot(ChickWeight_ts,\n                  Xtitle = \"Time\", \n                  Ytitle = \"Weight of Chick #1\")\n\n\n\n\n\nWe see that the weights of a young chick specimen increases over time.\n\ntibble data\nThe ts data format can handle only one time series. If we want multiple time series, based on say Qualitative variables, we need other data formats. Using the familiar tibble structure opens up new possibilities.\n\nWe can have multiple time series within a tibble (think of numerical time-series data like GDP, Population, Imports, Exports for multiple countries as with the gapminder1data we saw earlier).\n\nIt also allows for data processing with dplyr such as filtering and summarizing.\n\n\n\ngapminder data\n\n\n\n  \n\n\n\nLet us read and inspect in the US births data from 2000 to 2014. Download this data by clicking on the icon below, and saving the downloaded file in a sub-folder called data inside your project.\n Download the US Births data \nRead this data in:\n\nbirths_2000_2014 &lt;- read_csv(\"../data/US_births_2000-2014_SSA.csv\")\nglimpse(births_2000_2014)\n\nRows: 5,479\nColumns: 5\n$ year          &lt;dbl&gt; 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 20…\n$ month         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ date_of_month &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ day_of_week   &lt;dbl&gt; 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3,…\n$ births        &lt;dbl&gt; 9083, 8006, 11363, 13032, 12558, 12466, 12516, 8934, 794…\n\ninspect(births_2000_2014)\n\n\nquantitative variables:  \n           name   class  min   Q1 median    Q3   max         mean          sd\n1          year numeric 2000 2003   2007  2011  2014  2006.999270    4.321085\n2         month numeric    1    4      7    10    12     6.522723    3.449075\n3 date_of_month numeric    1    8     16    23    31    15.730243    8.801151\n4   day_of_week numeric    1    2      4     6     7     3.999817    2.000502\n5        births numeric 5728 8740  12343 13082 16081 11350.068261 2325.821049\n     n missing\n1 5479       0\n2 5479       0\n3 5479       0\n4 5479       0\n5 5479       0\n\nbirths_2000_2014\n\n\n  \n\n\n\nThis is just a tibble containing a single data variable births that varies over time. All other variables, although depicting time, are numerical columns. There are no Qualitative variables (yet!).\nPlotting tibble time series\n\n\nUsing ggformula\nUsing tsbox and TSstudio\nUsing ggplot\n\n\n\nWe will now plot this using ggformula. Using the separate year/month/week and day_of_week / day_of_month columns, we can plot births over time, colouring by day_of_week, for example:\n# grouping by day_of_week\nbirths_2000_2014 %&gt;% \n  gf_line(births ~ year, \n          group = ~ day_of_week, \n          color = ~ day_of_week) %&gt;% \n  gf_point(title = \"By Day of Week\") %&gt;% \n  gf_theme(scale_colour_distiller(palette = \"Paired\"))\n# Grouping by date_of_month\nbirths_2000_2014 %&gt;% \n  gf_line(births ~ year, \n          group = ~ date_of_month, \n          color = ~ date_of_month) %&gt;% \n  gf_point(title = \"By Date of Month\") %&gt;% \n  gf_theme(scale_colour_distiller(palette = \"Paired\")) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNot particularly illuminating. This is because the data is daily and we have considerable variation over time, and here we have too much data to visualize. Summaries will help, so we could calculate the the mean births on a month basis in each year and plot that:\nbirths_2000_2014_monthly &lt;- births_2000_2014 %&gt;% \n# Convert month to factor/Qual variable!\n# So that we can have discrete colours for each month\n# Using base::factor()\n# Could use forcats::as_factor() also\n  mutate(month = base::factor(month, labels = month.abb)) %&gt;%\n# `month.abb` is a built-in dataset containing names of months.\n  group_by(year, month) %&gt;% \n  summarise(mean_monthly_births = mean(births, na.rm = TRUE))\nbirths_2000_2014_monthly\nbirths_2000_2014_monthly %&gt;% \n  gf_line(mean_monthly_births ~ year, \n          group = ~ month, \n          colour = ~ month, linewidth = 1) %&gt;% \n  gf_point(size = 1.5, title = \"Summaries of Monthly Births over the years\") %&gt;% \n    # palette for 12 colours\n  gf_theme(scale_colour_brewer(palette = \"Paired\")) \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThese are graphs for the same month each year: we have a January graph and a February graph and so on. So…average births per month were higher in all months during 2005 to 2007 and have dropped since.\n\n\nWe can do similar graphs using day_of_week as our basis for grouping, instead of month:\nbirths_2000_2014_weekly &lt;- births_2000_2014 %&gt;% \n  mutate(day_of_week = base::factor(day_of_week,\n          levels = c(1,2,3,4,5,6,7), \n          labels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"))) %&gt;% \n  group_by(year, day_of_week) %&gt;% \n  summarise(mean_daily_births = mean(births, na.rm = TRUE))\nbirths_2000_2014_weekly\nbirths_2000_2014_weekly %&gt;%   \n  gf_line(mean_daily_births ~ year, \n          group = ~ day_of_week, \n          colour = ~ day_of_week, \n          linewidth = 1,\n          data = .) %&gt;% \n  gf_point(size = 2) %&gt;% \n  # palette for 12 colours\n  gf_theme(scale_colour_brewer(palette = \"Paired\"))\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy are fewer babies born on weekends?\n\n\n\nLooks like an interesting story here…there are significantly fewer births on average on Sat and Sun, over the years! Why? Should we watch Grey’s Anatomy ?\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that this is still using just tibble data, without converting it or using it as a time series. So far we are simply treating the year/month/day variables are simple variables and using dplyr to group and summarize. We have not created an explicit time or date variable.\n\n\n\n\nLet us create a time variable in our dataset now:\n\n\ntsbox::ts_plot needs just the date and the births columns to plot with and not be confused by the other numerical columns, so let us create a single date column from these three, but retain them for now.\n\nTSstudio::ts_plot also needs a date column.\n\nSo there are several numerical variables for year, month, and day_of_month, day_of_week, and of course the births on a daily basis.\nWe use the lubridate package from the tidyverse:\n\nbirths_timeseries &lt;- \n  births_2000_2014 %&gt;% \n  mutate(date = lubridate::make_date(year = year,\n                                     month = month,\n                                     day = date_of_month)) %&gt;% \n  select(date, births, year, month,date_of_month, day_of_week)\n\nbirths_timeseries\n\n\n  \n\n\n\n\n\n\n\n\n\nExtract from help(tsbox)\n\n\n\nIn data frames, i.e., in a data.frame, a data.table, or a tibble, tsbox stores one or multiple time series in the ‘long’ format. tsbox detects a value, a time column, and zero, one or several id columns. Column detection is done in the following order:\n\nStarting on the right, the first first numeric or integer column is used as value column.\n\nUsing the remaining columns and starting on the right again, the first Date, POSIXct, numeric or character column is used as time column. character strings are parsed by anytime::anytime(). The timestamp, time, indicates the beginning of a period.\n\n\nAll remaining columns are id columns. Each unique combination of id columns points to a (unique) time series.\n\nAlternatively, the time column and the value column to be explicitly named as time and value. If explicit names are used, the column order will be ignored. If columns are detected automatically, a message is returned.\n\n\nPlotting this directly, after selecting the relevant variables, so that they will be auto-detected:\n\nbirths_timeseries %&gt;% \n  select(date, births) %&gt;% \n  tsbox::ts_plot()\n\n[time]: 'date' [value]: 'births' \n\n\n\n\n\n\n\n\n\nbirths_timeseries %&gt;% \n  select(date, births) %&gt;% \n  TSstudio::ts_plot(Xtitle = \"Year\",\n                    Ytitle = \"Births\",\n                    title = \"Births Time Series\",\n                    Xgrid = TRUE,Ygrid = TRUE,\n                    slider = TRUE,\n                    width = 1) # linewidth\n\n\n\n\n\nQuite messy, as before. We need use the summarised data, as before. We will do this in the next section.\n\n\nWe will now plot this using ggplot for completeness. Using the separate year/month/week and day_of_week / day_of_month columns, we can plot births over time, colouring by day_of_week, for example:\n# grouping by day_of_week\nbirths_2000_2014 %&gt;% \n  ggplot(aes(year, births,\n             group = day_of_week, \n             color = day_of_week)) + \n  geom_line() +  \n  geom_point() +\n  labs(title = \"By Day of Week\") + \n  scale_colour_distiller(palette = \"Paired\")\n# Grouping by date_of_month\nbirths_2000_2014 %&gt;%  ggplot(aes(year, births,\n                                 group = date_of_month, \n                                 color = date_of_month)) + \n  geom_line() + \n  geom_point() +  \n  labs(title = \"By Date of Month\") + \n  scale_colour_distiller(palette = \"Paired\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbirths_2000_2014_monthly &lt;- births_2000_2014 %&gt;% \n# Convert month to factor/Qual variable!\n# So that we can have discrete colours for each month\n# Using base::factor()\n# Could use forcats::as_factor() also\n\n  mutate(month = base::factor(month, labels = month.abb)) %&gt;%\n# `month.abb` is a built-in dataset containing names of months.\n\n  group_by(year, month) %&gt;% \n  summarise(mean_monthly_births = mean(births, na.rm = TRUE))\nbirths_2000_2014_monthly\n###\nbirths_2000_2014_monthly %&gt;% \n  ggplot(aes(year, mean_monthly_births,\n               group = month, colour = month)) + \n  geom_line(linewidth = 1) + \n  geom_point(size = 1.5) + \n  labs(title = \"Summaries of Monthly Births over the years\") + \n    \n  # palette for 12 colours\n  scale_colour_brewer(palette = \"Paired\")\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nbirths_2000_2014_weekly &lt;- births_2000_2014 %&gt;% \n  mutate(day_of_week = base::factor(day_of_week,\n          levels = c(1,2,3,4,5,6,7), \n          labels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"))) %&gt;% \n  group_by(year, day_of_week) %&gt;% \n  summarise(mean_daily_births = mean(births, na.rm = TRUE))\nbirths_2000_2014_weekly\nbirths_2000_2014_weekly %&gt;%\n  ggplot(aes(year, mean_daily_births, \n             group = day_of_week,\n             colour = day_of_week)) + \n  geom_line() + \n  geom_point() + \n    \n  # palette for 12 colours\n  scale_colour_brewer(palette = \"Paired\")\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntsibble data\nFinally, we have tsibble (“time series tibble”) format data, which contains three main components:\n\nan index variable that defines time;\na set of key variables, usually categorical, that define sets of observations, over time. This allows for each combination of the categorical variables to define a separate time series.\na set of quantitative variables, that represent the quantities that vary over time (i.e index)\n\nHere is Robert Hyndman’s video introducing tsibbles:\n\nThe package tsibbledata contains several ready made tsibble format data.  Let us try PBS, which is a dataset containing Monthly Medicare prescription data in Australia.Run data(package = \"tsibbledata\") in your Console to find out about these.\n\ndata(\"PBS\")\n# inspect(PBS) # does not work since mosaic cannot handle tsibbles\nPBS\n\n\n  \n\n\n\nData Description: This is a large-ish dataset:Run PBS in your console\n\n67K observations\n336 combinations of key variables (Concession, Type, ATC1, ATC2) which are categorical, as foreseen.\nData appears to be monthly, as indicated by the 1M.\nthe time index variable is called Month, formatted as yearmonth, a new type of variable introduced in the tsibble package\n\nNote that there are multiple Quantitative variables (Scripts,Cost), each sliced into 336 time-series, a feature which is not supported in the ts format, but is supported in a tsibble. The Qualitative Variables are described below. Type help(\"PBS\") in your Console.\nThe data is dis-aggregated/grouped using four keys:\n- Concession: Concessional scripts are given to pensioners, unemployed, dependents, and other card holders\n- Type: Co-payments are made until an individual’s script expenditure hits a threshold ($290.00 for concession, $1141.80 otherwise). Safety net subsidies are provided to individuals exceeding this amount.\n- ATC1: Anatomical Therapeutic Chemical index (level 1). 15 types\n- ATC2: Anatomical Therapeutic Chemical index (level 2). 84 types, nested inside ATC1.\nLet us simply plot Cost over time:\n\n\nUsing ggformula\nUsing ggplot\n\n\n\n\nPBS %&gt;% \n  gf_point(Cost ~ Month, data = .) %&gt;% \n  gf_line(title = \"PBS Costs vs time\") \n\n\n\n\n\n\n\n\n\n\nPBS %&gt;% ggplot(aes(Month, Cost)) + \n  geom_point() + \n  geom_line() + \n  labs(title = \"PBS Costs vs time\") \n\n\n\n\n\n\n\n\n\n\nThis basic plot is quite messy, and it is now time (sic!) for us to look at summaries of the data using dplyr-like verbs."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#time-series-wrangling",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#time-series-wrangling",
    "title": "🕔 Time Series",
    "section": "\n Time-Series Wrangling",
    "text": "Time-Series Wrangling\nWe have now arrived at the need to filter, group, and summarize time-series data. We can do this in two ways, with two packages:\n\n\n\n\n\n\ntsibble has dplyr-like functions\n\n\n\nUsing tsibble data, the tsibble package has specialized filter and group_by functions to do with the index (i.e time) variable and the key variables, such as index_by() and group_by_key().\nFiltering based on Qual variables can be done with dplyr. We can use dplyr functions such as group_by, mutate(), filter(), select() and summarise() to work with tsibble objects.\n\n\n\n\n\n\n\n\ntimetk also has dplyr-like functions!\n\n\n\nUsing tibbles, timetk provides functions such as summarize_by_time, filter_by_time and slidify that are quite powerful. Again, as with tsibble, dplyr can always be used for other variables (i.e non-time).\n\n\nLet us first see how many observations there are for each combo of keys:\nPBS %&gt;% \n  count()\n# Grouped Counts\nPBS %&gt;% \n  tsibble::group_by_key(ATC1, ATC2, Concession, Type) %&gt;% \n  dplyr::count()\n# dplyr grouping\nPBS %&gt;% \n  dplyr::group_by(ATC1, ATC2) %&gt;% \n  dplyr::count()\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nWe have 336 combinations of Qualitative variables, each combo containing 204 observations (except some! Take a look!): so let us filter for a few such combinations and plot:\n# Costs\nPBS %&gt;% \n  tsibble::group_by_key(ATC1, ATC2, Concession, Type) %&gt;%\n  gf_line(Cost ~ Month, \n          colour = ~ Type, \n          data = .) %&gt;% \n  gf_point(title = \"Costs, per Month\") \n# Scripts\nPBS %&gt;% \n  tsibble::group_by_key(ATC1, ATC2, Concession, Type) %&gt;%\n  gf_line(Scripts ~ Month, \n          colour = ~ Type, \n          data = .) %&gt;% \n  gf_point(title = \"Scripts, per Month\") \n# Costs variable for a specific combo of Qual variables(keys)\nPBS %&gt;% \n  dplyr::filter(Concession == \"General\", \n                      ATC1 == \"A\",\n                      ATC2 == \"A10\") %&gt;% \n  gf_line(Cost ~ Month, \n          colour = ~ Type, \n          data = .) %&gt;% \n  gf_point(title = \"Costs, per Month for General/A/A10 category patients\")\n# Scripts variable for a specific combo of Qual variables(keys)\nPBS %&gt;% \n  dplyr::filter(Concession == \"General\", \n                      ATC1 == \"A\",\n                      ATC2 == \"A10\") %&gt;% \n  gf_line(Scripts ~ Month, \n          colour = ~ Type, \n          data = .) %&gt;% \n  gf_point(title = \"Scripts, per Month for General/A/A10 category patients\") \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs can be seen, very different time patterns based on the two Types of payment methods, and also with Costs and Scripts. Strongly seasonal for both, with seasonal variation increasing over the years, a clear sign of a multiplicative time series. There is a strong upward trend with both types of subsidies, Safety net and Co-payments. But these trends are somewhat different in magnitude for specific combinations of ATC1 and ATC2 categories.\nWe can use tsibble’s dplyr-like commands to develop summaries by year, quarter, month(original data): Look carefully at the new time variable created each time:\n\n# Original Data\nPBS\n\n\n  \n\n\n# Cost Summary by Month, which is the original data\n# Only grouping happens here\n# New Variable Name to make grouping visible\nPBS %&gt;% \n  tsibble::group_by_key(ATC1, ATC2, Concession, Type) %&gt;% \n  tsibble::index_by(Month_Group = Month) %&gt;% \n  dplyr::summarise(across(.cols = c(Cost, Scripts),\n                          .fn = mean,\n                          .names = \"mean_{.col}\"))\n\n\n  \n\n\n# Cost Summary by Quarter\nPBS %&gt;% \n  tsibble::group_by_key(ATC1, ATC2, Concession, Type) %&gt;% \n  tsibble::index_by(Year_Quarter = yearquarter(Month)) %&gt;% # And the change here!\n  dplyr::summarise(across(.cols = c(Cost, Scripts),\n                          .fn = mean,\n                          .names = \"mean_{.col}\"))\n\n\n  \n\n\n# Cost Summary by Year\nPBS %&gt;% \n  tsibble::group_by_key(ATC1, ATC2, Concession, Type) %&gt;% \n  index_by(Year_Group = year(Month)) %&gt;% # Note this change!!!\n  dplyr::summarise(across(.cols = c(Cost, Scripts),\n                          .fn = mean,\n                          .names = \"mean_{.col}\"))\n\n\n  \n\n\n\nFinally, it may be a good idea to convert some tibble into a tsibble to leverage some of functions that tsibble offers:\n\nbirths_tsibble &lt;- births_2000_2014 %&gt;% \n  mutate(date = lubridate::make_date(year = year,\n                                     month = month,\n                                     day = date_of_month)) %&gt;%\n  # Convert to tsibble\n  tsibble::as_tsibble(index = date) # Time Variable\n\nbirths_tsibble\n\n\n  \n\n\n\nThis is DAILY data of course. Let us say we want to group by month and plot mean monthly births as before, but now using tsibble and the index variable:\n\n\ntsibble vs timetk: Basic Plot\ntsibble vs timetk: Grouped Plot 1\ntsibble vs timetk: Grouped Plot 2\n\n\n\nbirths_tsibble %&gt;%\n  gf_line(births ~ date, \n          data = ., \n          title = \"Basic tsibble plotted with ggformula\")\n# timetk **can** plot tsibbles. \nbirths_tsibble %&gt;% \n  timetk::plot_time_series(.date_var = date, \n                           .value = births,\n                           .title = \"Tsibble Plotted with timetk\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbirths_tsibble %&gt;% \n  tsibble::index_by(month_index = ~ tsibble::yearmonth(.)) %&gt;% \n  dplyr::summarise(mean_births = mean(births, na.rm = TRUE)) %&gt;% \n  gf_point(mean_births ~ month_index, \n           data = ., \n           title = \"Monthly Aggregate with tsibble\") %&gt;% \n  gf_line() %&gt;% \n  gf_smooth(se = FALSE, method = \"loess\") \nbirths_timeseries %&gt;% \n  # timetk cannot wrangle tsibbles\n  # timetk needs tibble or data frame\n  timetk::summarise_by_time(.date_var = date, \n                            .by = \"month\", \n                            mean = mean(births)) %&gt;% \n  timetk::plot_time_series(date, mean,\n                           .title = \"Monthly aggregate births with timetk\",\n                           .x_lab = \"year\", \n                           .y_lab = \"Mean Monthly Births\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApart from the bump during in 2006-2007, there are also seasonal trends that repeat each year, which we glimpsed earlier.\n\n\nbirths_tsibble %&gt;% \n  tsibble::index_by(year_index = ~ lubridate::year(.)) %&gt;% \n  dplyr::summarise(mean_births = mean(births, na.rm = TRUE)) %&gt;%\n  gf_point(mean_births ~ year_index, data = .) %&gt;% \n  gf_line() %&gt;% \n  gf_smooth(se = FALSE, method = \"loess\") \nbirths_timeseries %&gt;% \n  timetk::summarise_by_time(.date_var = date, \n                            .by = \"year\", \n                            mean = mean(births)) %&gt;% \n  timetk::plot_time_series(date, mean,\n                           .title = \"Yearly aggregate births with timetk\",\n                           .x_lab = \"year\", \n                           .y_lab = \"Mean Yearly Births\")"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#candle-stick-plots",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#candle-stick-plots",
    "title": "🕔 Time Series",
    "section": "\n Candle-Stick Plots",
    "text": "Candle-Stick Plots\nHmm…can we try to plot boxplots over time (Candle-Stick Plots)? Over month / quarter or year?\n\n Monthly Box Plots\nbirths_tsibble %&gt;%\n  index_by(month_index = ~ yearmonth(.)) %&gt;% \n  # 15 years\n  # No need to summarise, since we want boxplots per year / month\n  gf_boxplot(births ~ date, \n             group =  ~ month_index, \n             fill = ~ month_index, data = .) \n  # plot the groups\n  # 180 plots!!\n\nbirths_timeseries %&gt;% \n  # timetk::summarise_by_time(.date_var = date, \n  #                           .by = \"month\", \n  #                           mean = mean(births)) %&gt;% \n  timetk::plot_time_series_boxplot(date, births,\n                           .title = \"Monthly births with timetk\",\n                           .x_lab = \"year\", .period = \"month\",\n                           .y_lab = \"Mean Monthly Births\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Quarterly boxplots\n\nbirths_tsibble %&gt;%\n  index_by(qrtr_index = ~ yearquarter(.)) %&gt;% # 60 quarters over 15 years\n  # No need to summarise, since we want boxplots per year / month\n  gf_boxplot(births ~ date, \n             group = ~ qrtr_index,\n             fill = ~ qrtr_index,\n             data = .)  # 60 plots!!\n\n\n\n\n\n\nbirths_timeseries %&gt;% \n  timetk::plot_time_series_boxplot(date, births,\n                           .title = \"Quarterly births with timetk\",\n                           .x_lab = \"year\", .period = \"quarter\",\n                           .y_lab = \"Mean Monthly Births\")\n\n\n\n\n\n\n Yearwise boxplots\n\nbirths_tsibble %&gt;% \n  index_by(year_index = ~ lubridate::year(.)) %&gt;% # 15 years, 15 groups\n    # No need to summarise, since we want boxplots per year / month\n\n  gf_boxplot(births ~ date, \n             group = ~ year_index, \n             fill = ~ year_index, \n             data = .) %&gt;%  # plot the groups 15 plots\n  gf_labs(title = \"Yearly aggregate births with ggformula\") %&gt;% \n  gf_theme(scale_fill_distiller(palette = \"Spectral\")) \n\n\n\n\n\n\nbirths_timeseries %&gt;% \n  timetk::plot_time_series_boxplot(date, births,\n                           .title = \"Yearly aggregate births with timetk\",\n                           .x_lab = \"year\", .period = \"year\",\n                           .y_lab = \"Births\")\n\n\n\n\n\nAlthough the graphs are very busy, they do reveal seasonality trends at different periods.\n\nHow about a heatmap? We can cook up a categorical variable based on the number of births (low, fine, high) and use that to create a heatmap:\n\nbirths_2000_2014 %&gt;%\n  mutate(birthrate = case_when(births &gt;= 10000 ~ \"high\",\n                               births &lt;= 8000 ~ \"low\",\n                               TRUE ~ \"fine\")) %&gt;%\n  \n  gf_tile(\n    data = .,\n    year ~ month,\n    fill = ~ birthrate,\n    color = \"black\"\n  ) %&gt;%\n  \n  gf_theme(scale_x_time(\n    breaks = 1:12,\n    labels = c(\"Jan\",\"Feb\",\"Mar\",\"Apr\",\n               \"May\",\"Jun\",\"Jul\",\"Aug\",\n               \"Sep\",\"Oct\",\"Nov\",\"Dec\")\n  )) %&gt;%\n  \n  gf_theme(theme_classic())"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#conclusion",
    "title": "🕔 Time Series",
    "section": "\n Conclusion",
    "text": "Conclusion\nWe have seen a good few data formats for time series, and how to work with them and plot them. We have also seen how to decompose time series into periodic and aperiodic components, which can be used to make business decisions."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#your-turn",
    "title": "🕔 Time Series",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nChoose some of the datasets in the tsdl and in the tsibbledata packages. Plot basic, filtered and model-based graphs for these and interpret."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#references",
    "title": "🕔 Time Series",
    "section": "\n References",
    "text": "References\n\nRobert Hyndman, Forecasting: Principles and Practice (Third Edition). available online\nTime Series Analysis at Our Coding Club"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#readings",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#readings",
    "title": "🕔 Time Series",
    "section": "\n Readings",
    "text": "Readings\n\nThe Nuclear Threat—The Shadow Peace, part 1\n11 Ways to Visualize Changes Over Time – A Guide\nWhat is seasonal adjustment and why is it used?\nThe start-at-zero rule"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#extra-stuff",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#extra-stuff",
    "title": "🕔 Time Series",
    "section": "Extra Stuff",
    "text": "Extra Stuff\nUsing tsbox and TSstudio\nLet us create a time variable in our dataset now:\n\n\ntsbox::ts_plot needs just the date and the births columns to plot with and not be confused by the other numerical columns, so let us create a single date column from these three, but retain them for now.\n\nTSstudio::ts_plot also needs a date column.\n\nSo there are several numerical variables for year, month, and day_of_month, day_of_week, and of course the births on a daily basis.\nWe use the lubridate package from the tidyverse:\n\nbirths_timeseries &lt;- \n  births_2000_2014 %&gt;% \n  mutate(date = lubridate::make_date(year = year,\n                                     month = month,\n                                     day = date_of_month)) %&gt;% \n  select(date, births, year, month,date_of_month, day_of_week)\n\nbirths_timeseries\n\n\n  \n\n\n\n\n\n\n\n\n\nExtract from help(tsbox)\n\n\n\nIn data frames, i.e., in a data.frame, a data.table, or a tibble, tsbox stores one or multiple time series in the ‘long’ format. tsbox detects a value, a time column, and zero, one or several id columns. Column detection is done in the following order:\n\nStarting on the right, the first first numeric or integer column is used as value column.\n\nUsing the remaining columns and starting on the right again, the first Date, POSIXct, numeric or character column is used as time column. character strings are parsed by anytime::anytime(). The timestamp, time, indicates the beginning of a period.\n\n\nAll remaining columns are id columns. Each unique combination of id columns points to a (unique) time series.\n\nAlternatively, the time column and the value column to be explicitly named as time and value. If explicit names are used, the column order will be ignored. If columns are detected automatically, a message is returned.\n\n\nPlotting this directly, after selecting the relevant variables, so that they will be auto-detected:\n\nbirths_timeseries %&gt;% \n  select(date, births) %&gt;% \n  tsbox::ts_plot()\n\n[time]: 'date' [value]: 'births' \n\n\n\n\n\n\n\n\n\nbirths_timeseries %&gt;% \n  select(date, births) %&gt;% \n  TSstudio::ts_plot(Xtitle = \"Year\",\n                    Ytitle = \"Births\",\n                    title = \"Births Time Series\",\n                    Xgrid = TRUE,Ygrid = TRUE,\n                    slider = TRUE,\n                    width = 1) # linewidth\n\n\n\n\n\nQuite messy, as before. We need use the summarised data, as before. We will do this in the next section."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#references-1",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#references-1",
    "title": "🕔 Time Series",
    "section": "\n References",
    "text": "References\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggridges\n0.5.6\nWilke (2024)\n\n\nNHANES\n2.1.0\nPruim (2015)\n\n\nTeachHist\n0.2.1\nLange (2023)\n\n\nTeachingDemos\n2.13\nSnow (2024)\n\n\n\n\n\n\nLange, Carsten. 2023. TeachHist: A Collection of Amended Histograms Designed for Teaching Statistics. https://CRAN.R-project.org/package=TeachHist.\n\n\nPruim, Randall. 2015. NHANES: Data from the US National Health and Nutrition Examination Study. https://CRAN.R-project.org/package=NHANES.\n\n\nSnow, Greg. 2024. TeachingDemos: Demonstrations for Teaching and Learning. https://CRAN.R-project.org/package=TeachingDemos.\n\n\nWilke, Claus O. 2024. ggridges: Ridgeline Plots in “ggplot2”. https://CRAN.R-project.org/package=ggridges."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#footnotes",
    "title": "🕔 Time Series",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://www.gapminder.org/data/↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html",
    "title": "Analysis of Time Series in R",
    "section": "",
    "text": "library(tidyverse) # For tidy data processing and plotting\nlibrary(lubridate)  # Deal with dates\n\nlibrary(mosaic) # Out go to package for everything\n\nlibrary(fpp3) # Robert Hyndman's time series analysis package\nlibrary(timetk)  # Convert data frames to time series-specific objects\nlibrary(forecast)  # Make forecasts and decompose time series\n\n# devtools::install_github(\"FinYang/tsdl\")\nlibrary(tsdl) # Time Series Data Library from Rob Hyndman"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html#setting-up-r-packages",
    "title": "Analysis of Time Series in R",
    "section": "",
    "text": "library(tidyverse) # For tidy data processing and plotting\nlibrary(lubridate)  # Deal with dates\n\nlibrary(mosaic) # Out go to package for everything\n\nlibrary(fpp3) # Robert Hyndman's time series analysis package\nlibrary(timetk)  # Convert data frames to time series-specific objects\nlibrary(forecast)  # Make forecasts and decompose time series\n\n# devtools::install_github(\"FinYang/tsdl\")\nlibrary(tsdl) # Time Series Data Library from Rob Hyndman"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html#introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html#introduction",
    "title": "Analysis of Time Series in R",
    "section": "\n Introduction",
    "text": "Introduction\nWe have seen how to plot different formats of time series, and how to create summary plots using packages like tsibble and timetk.\nWe will now see how a time series can be broken down to its components so as to systematically understand and analyze it. Thereafter, we examine how to model the timeseries, and make forecasts, a task more like synthesis.\nWe have to begin by answering fundamental questions such as:\n\nWhat are the types of time series?\nHow does one process and analyze time series data?\nHow does one plot time series?\nHow to decompose it? How to extract a level, a trend, and seasonal components from a time series?\nWhat is auto correlation etc.\nWhat is a stationary time series?"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html#case-study--1-walmart-sales-dataset-from-timetk",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html#case-study--1-walmart-sales-dataset-from-timetk",
    "title": "Analysis of Time Series in R",
    "section": "\n Case Study -1: Walmart Sales Dataset from timetk\n",
    "text": "Case Study -1: Walmart Sales Dataset from timetk\n\nLet us inspect what datasets are available in the package timetk. Type data(package = \"timetk\") in your Console to see what datasets are available.\nLet us choose the Walmart Sales dataset. See here for more details: Walmart Recruiting - Store Sales Forecasting |Kaggle\nwalmart_sales_weekly\nglimpse(walmart_sales_weekly)\ninspect(walmart_sales_weekly)\n# Try this in your Console\n# help(\"walmart_sales_weekly\")\n\n\n\n\n  \n\n\n\nRows: 1,001\nColumns: 17\n$ id           &lt;fct&gt; 1_1, 1_1, 1_1, 1_1, 1_1, 1_1, 1_1, 1_1, 1_1, 1_1, 1_1, 1_…\n$ Store        &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ Dept         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ Date         &lt;date&gt; 2010-02-05, 2010-02-12, 2010-02-19, 2010-02-26, 2010-03-…\n$ Weekly_Sales &lt;dbl&gt; 24924.50, 46039.49, 41595.55, 19403.54, 21827.90, 21043.3…\n$ IsHoliday    &lt;lgl&gt; FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA…\n$ Type         &lt;chr&gt; \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A…\n$ Size         &lt;dbl&gt; 151315, 151315, 151315, 151315, 151315, 151315, 151315, 1…\n$ Temperature  &lt;dbl&gt; 42.31, 38.51, 39.93, 46.63, 46.50, 57.79, 54.58, 51.45, 6…\n$ Fuel_Price   &lt;dbl&gt; 2.572, 2.548, 2.514, 2.561, 2.625, 2.667, 2.720, 2.732, 2…\n$ MarkDown1    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ MarkDown2    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ MarkDown3    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ MarkDown4    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ MarkDown5    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CPI          &lt;dbl&gt; 211.0964, 211.2422, 211.2891, 211.3196, 211.3501, 211.380…\n$ Unemployment &lt;dbl&gt; 8.106, 8.106, 8.106, 8.106, 8.106, 8.106, 8.106, 8.106, 7…\n\n\n\n\n\ncategorical variables:  \n       name     class levels    n missing\n1        id    factor   3331 1001       0\n2 IsHoliday   logical      2 1001       0\n3      Type character      1 1001       0\n                                   distribution\n1 1_1 (14.3%), 1_3 (14.3%), 1_8 (14.3%) ...    \n2 FALSE (93%), TRUE (7%)                       \n3 A (100%)                                     \n\nDate variables:  \n  name class      first       last min_diff max_diff    n missing\n1 Date  Date 2010-02-05 2012-10-26   0 days   7 days 1001       0\n\nquantitative variables:  \n           name   class         min          Q1      median          Q3\n1         Store numeric      1.0000      1.0000      1.0000      1.0000\n2          Dept numeric      1.0000      3.0000     13.0000     93.0000\n3  Weekly_Sales numeric   6165.7300  28257.3000  39886.0600  77943.5700\n4          Size numeric 151315.0000 151315.0000 151315.0000 151315.0000\n5   Temperature numeric     35.4000     57.7900     69.6400     80.4900\n6    Fuel_Price numeric      2.5140      2.7590      3.2900      3.5940\n7     MarkDown1 numeric    410.3100   4039.3900   6154.1400  10121.9700\n8     MarkDown2 numeric      0.5000     40.4800    144.8700   1569.0000\n9     MarkDown3 numeric      0.2500      6.0000     25.9650    101.6400\n10    MarkDown4 numeric      8.0000    577.1400   1822.5500   3750.5900\n11    MarkDown5 numeric    554.9200   3127.8800   4325.1900   6222.2500\n12          CPI numeric    210.3374    211.5312    215.4599    220.6369\n13 Unemployment numeric      6.5730      7.3480      7.7870      7.8380\n           max         mean           sd    n missing\n1       1.0000 1.000000e+00 0.000000e+00 1001       0\n2      95.0000 3.585714e+01 3.849159e+01 1001       0\n3  148798.0500 5.464634e+04 3.627627e+04 1001       0\n4  151315.0000 1.513150e+05 0.000000e+00 1001       0\n5      91.6500 6.830678e+01 1.420767e+01 1001       0\n6       3.9070 3.219699e+00 4.260286e-01 1001       0\n7   34577.0600 8.090766e+03 6.550983e+03  357     644\n8   46011.3800 2.941315e+03 7.873661e+03  294     707\n9   55805.5100 1.225400e+03 7.811934e+03  350     651\n10  32403.8700 3.746085e+03 5.948867e+03  357     644\n11  20475.3200 5.018655e+03 3.254071e+03  357     644\n12    223.4443 2.159969e+02 4.337818e+00 1001       0\n13      8.1060 7.610420e+00 3.825958e-01 1001       0\n\n\n\nThe data is described as:\n\nA tibble: 9,743 x 3\n\n\nid Factor. Unique series identifier (4 total)\n\nStore Numeric. Store ID.\n\nDept Numeric. Department ID.\n\nDate Date. Weekly timestamp.\n\nWeekly_Sales Numeric. Sales for the given department in the given store.\n\nIsHoliday Logical. Whether the week is a “special” holiday for the store.\n\nType Character. Type identifier of the store.\n\nSize Numeric. Store square-footage\n\nTemperature Numeric. Average temperature in the region.\n\nFuel_Price Numeric. Cost of fuel in the region.\n\nMarkDown1, MarkDown2, MarkDown3, MarkDown4, MarkDown5 Numeric. Anonymized data related to promotional markdowns that Walmart is running. MarkDown data is only available after Nov 2011, and is not available for all stores all the time. Any missing value is marked with an NA.\n\nCPI Numeric. The consumer price index.\n\nUnemployment Numeric. The unemployment rate in the region.\n\n\nVery cool to know that mosaic::inspect() identifies date variables separately!\n\n\n\n\n\n\nNot yet a time series!\n\n\n\nThis is still a tibble, with a time-oriented variable of course, but not yet a time-series object. The data frame has the YMD columns repeated for each Dept, giving us what is called “long” form data. To deal with this repetition, we will always need to split the Weekly_Sales by the Dept column before we plot or analyze.\n\n\n\n#|label: walmart sales tsibble\nwalmart_tsibble &lt;- \n  walmart_sales_weekly %&gt;% \n  as_tsibble(index = Date, # Time Variable\n             key = c(id, Store, Dept, Type))\n             \n  #  Identifies unique \"subject\" who are measures\n  #  All other variables such as Weekly_sales become \"measured variable\"\n  #  Each observation should be uniquely identified by index and key\nwalmart_tsibble\n\n\n  \n\n\n\n\n Basic Time Series Plots\nThe easiest way is to use autoplot from the feasts package. You may need to specify the actual measured variable, if there is more than one numerical column:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nfeasts::autoplot(walmart_tsibble, .vars = Weekly_Sales)\n\n\n\n\n\n\n\ntimetk gives us interactive plots that may be more evocative than the static plot above. The basic plot function with timetk is plot_time_series. There are arguments for the date variable, the value you want to plot, colours, groupings etc.\nLet us explore this dataset using timetk, using our trusted method of asking Questions:\n\n\n\n\n\n\nNote\n\n\n\nQ.1 How are the weekly sales different for each Department?\nThere are 7 number of Departments. So we should be fine plotting them and also facetting with them, as we will see in a bit:\n\nwalmart_tsibble %&gt;% \n  timetk::plot_time_series(.date_var = Date, \n                           .value = Weekly_Sales,\n                           .color_var = Dept, \n                           .legend_show = TRUE,\n                           .title = \"Walmart Sales Data by Department\",\n                           .smooth = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQ.2. What do the sales per Dept look like during the month of December (Christmas time) in 2012? Show the individual Depts as facets.\nWe can of course zoom into the interactive plot above, but if we were to plot it anyway:\n\n# Only include rows from  1 to December 31, 2011\n# Data goes only up to Oct 2012\n\nwalmart_tsibble %&gt;% \n  # Each side of the time_formula is specified as the character 'YYYY-MM-DD HH:MM:SS',\n  timetk::filter_by_time(.date_var = Date,\n                         .start_date = \"2011-12-01\",\n                         .end_date = \"2011-12-31\") %&gt;%\n\n  plot_time_series(.date_var = Date, \n                   .value = Weekly_Sales, \n                   .color_var = Dept, \n                   .facet_vars = Dept, \n                   .facet_ncol = 2,\n                   .smooth = FALSE) # Only 4 points per graph\n\n\n\n\n\nClearly the “unfortunate” Dept#13 has seen something of a Christmas drop in sales, as has Dept#38 ! The rest, all is well, it seems…\n\n\n\n Too much noise? How about some averaging?\n\n\n\n\n\n\nNote\n\n\n\nQ.3 How do we smooth out some of the variations in the time series to be able to understand it better?\nSometimes there is too much noise in the time series observations and we want to take what is called a rolling average. For this we will use the function timetk::slidify to create an averaging function of our choice, and then apply it to the time series using regular dplyr::mutate\n\n# Let's take the average of Sales for each month in each Department.\n# Our **function** will be named \"rolling_avg_month\": \n\nrolling_avg_month = timetk::slidify(.period = 4, # every 4 weeks\n                            .f = mean, # The function to use\n                            .align = \"center\", # Aligned with middle of month\n                            .partial = TRUE) # To catch any leftover half weeks\nrolling_avg_month\n\nfunction (...) \n{\n    slider_2(..., .slider_fun = slider::pslide, .f = .f, .period = .period, \n        .align = .align, .partial = .partial, .unlist = .unlist)\n}\n&lt;bytecode: 0x1392c4610&gt;\n&lt;environment: 0x1392c3ae8&gt;\n\n\nOK, slidify creates a function! Let’s apply it to the Walmart Sales time series…\n\nwalmart_tsibble %&gt;% \n  # group_by(Dept) %&gt;% # Is this needed?\n  mutate(avg_monthly_sales = rolling_avg_month(Weekly_Sales)) %&gt;% \n  # ungroup() %&gt;% # Is this needed?\n  timetk::plot_time_series(Date, avg_monthly_sales,\n                           .color_var = Dept, # Does the grouping!\n                           .smooth = FALSE)\n\n\n\n\n\nCurves are smoother now. Need to check whether the averaging was done on a per-Dept basis…should we have had a group_by(Dept) before the averaging, and ungroup() before plotting? Try it !!\n\n\n\n Decomposing Time Series: Trends, Seasonal Patterns, and Cycles\nEach data point (\\(Y_t\\)) at time \\(t\\) in a Time Series can be expressed as either a sum or a product of 4 components, namely, Seasonality(\\(S_t\\)), Trend(\\(T_t\\)), Cyclic, and Error(\\(e_t\\)) (a.k.a White Noise).\n\n\nTrend: pattern exists when there is a long-term increase or decrease in the data.\n\nSeasonal: pattern exists when a series is influenced by seasonal factors (e.g., the quarter of the year, the month, or day of the week).\n\nCyclic: pattern exists when data exhibit rises and falls that are not of fixed period (duration usually of at least 2 years). Often combined with Trend into “Trend-Cycle”.\n\nError or Noise: Random component\n\nWhen data is non-seasonal this means breaking it up into only trend and irregular components. To estimate the trend component of a non-seasonal time series that can be described using an additive model, it is common to use a smoothing method, such as calculating the simple moving average of the time series.\ntimetk has the ability to achieve this: Let us plot the trend, seasonal, cyclic and irregular aspects of Weekly_Sales for Dept 38:\n\nwalmart_tsibble %&gt;% \n  filter(Dept == \"38\") %&gt;% \n  timetk::plot_stl_diagnostics(.data = .,\n                               .date_var = Date, \n                               .value = Weekly_Sales)\n\n\n\n\n\nWe can do this for all Depts using fable and fabletools:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n##\nwalmart_decomposed &lt;- \n  walmart_tsibble %&gt;% \n  \n  # If we want to filter, we do it here\n  #filter(Dept == \"38\") %&gt;% \n  # \n\nfabletools::model(stl = STL(Weekly_Sales))\n\nfabletools::components(walmart_decomposed)\n\n\n  \n\n\nfeasts::autoplot(components((walmart_decomposed)))"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html#case-study-2-dataset-from-nycflights13",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html#case-study-2-dataset-from-nycflights13",
    "title": "Analysis of Time Series in R",
    "section": "\n Case Study #2: Dataset from nycflights13\n",
    "text": "Case Study #2: Dataset from nycflights13\n\nLet us try the flights dataset from the package nycflights13. Try data(package = \"nycflights13\") in your Console.\nWe have the following datasets in the nycflights13 package:\n\n\nairlines Airline names.\n\nairports Airport metadata\n\nflights Flights data\n\nplanes Plane metadata.\n\nweather Hourly weather data\n\nLet us analyze the flights data:\n\ndata(\"flights\", package = \"nycflights13\")\nglimpse(flights)\n\nRows: 336,776\nColumns: 19\n$ year           &lt;int&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2…\n$ month          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ day            &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ dep_time       &lt;int&gt; 517, 533, 542, 544, 554, 554, 555, 557, 557, 558, 558, …\n$ sched_dep_time &lt;int&gt; 515, 529, 540, 545, 600, 558, 600, 600, 600, 600, 600, …\n$ dep_delay      &lt;dbl&gt; 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, -2, -2, -1…\n$ arr_time       &lt;int&gt; 830, 850, 923, 1004, 812, 740, 913, 709, 838, 753, 849,…\n$ sched_arr_time &lt;int&gt; 819, 830, 850, 1022, 837, 728, 854, 723, 846, 745, 851,…\n$ arr_delay      &lt;dbl&gt; 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2, -3, 7, -1…\n$ carrier        &lt;chr&gt; \"UA\", \"UA\", \"AA\", \"B6\", \"DL\", \"UA\", \"B6\", \"EV\", \"B6\", \"…\n$ flight         &lt;int&gt; 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79, 301, 4…\n$ tailnum        &lt;chr&gt; \"N14228\", \"N24211\", \"N619AA\", \"N804JB\", \"N668DN\", \"N394…\n$ origin         &lt;chr&gt; \"EWR\", \"LGA\", \"JFK\", \"JFK\", \"LGA\", \"EWR\", \"EWR\", \"LGA\",…\n$ dest           &lt;chr&gt; \"IAH\", \"IAH\", \"MIA\", \"BQN\", \"ATL\", \"ORD\", \"FLL\", \"IAD\",…\n$ air_time       &lt;dbl&gt; 227, 227, 160, 183, 116, 150, 158, 53, 140, 138, 149, 1…\n$ distance       &lt;dbl&gt; 1400, 1416, 1089, 1576, 762, 719, 1065, 229, 944, 733, …\n$ hour           &lt;dbl&gt; 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6…\n$ minute         &lt;dbl&gt; 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 59, 0…\n$ time_hour      &lt;dttm&gt; 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013-01-01 0…\n\nmosaic::inspect(flights)\n\n\ncategorical variables:  \n     name     class levels      n missing\n1 carrier character     16 336776       0\n2 tailnum character   4043 334264    2512\n3  origin character      3 336776       0\n4    dest character    105 336776       0\n                                   distribution\n1 UA (17.4%), B6 (16.2%), EV (16.1%) ...       \n2 N725MQ (0.2%), N722MQ (0.2%) ...             \n3 EWR (35.9%), JFK (33%), LGA (31.1%)          \n4 ORD (5.1%), ATL (5.1%), LAX (4.8%) ...       \n\nquantitative variables:  \n             name   class  min   Q1 median   Q3  max        mean          sd\n1            year integer 2013 2013   2013 2013 2013 2013.000000    0.000000\n2           month integer    1    4      7   10   12    6.548510    3.414457\n3             day integer    1    8     16   23   31   15.710787    8.768607\n4        dep_time integer    1  907   1401 1744 2400 1349.109947  488.281791\n5  sched_dep_time integer  106  906   1359 1729 2359 1344.254840  467.335756\n6       dep_delay numeric  -43   -5     -2   11 1301   12.639070   40.210061\n7        arr_time integer    1 1104   1535 1940 2400 1502.054999  533.264132\n8  sched_arr_time integer    1 1124   1556 1945 2359 1536.380220  497.457142\n9       arr_delay numeric  -86  -17     -5   14 1272    6.895377   44.633292\n10         flight integer    1  553   1496 3465 8500 1971.923620 1632.471938\n11       air_time numeric   20   82    129  192  695  150.686460   93.688305\n12       distance numeric   17  502    872 1389 4983 1039.912604  733.233033\n13           hour numeric    1    9     13   17   23   13.180247    4.661316\n14         minute numeric    0    8     29   44   59   26.230100   19.300846\n        n missing\n1  336776       0\n2  336776       0\n3  336776       0\n4  328521    8255\n5  336776       0\n6  328521    8255\n7  328063    8713\n8  336776       0\n9  327346    9430\n10 336776       0\n11 327346    9430\n12 336776       0\n13 336776       0\n14 336776       0\n\ntime variables:  \n       name   class               first                last min_diff   max_diff\n1 time_hour POSIXct 2013-01-01 05:00:00 2013-12-31 23:00:00   0 secs 25200 secs\n       n missing\n1 336776       0\n\n\nWe have time-related columns; Apart from year, month, day we have time_hour; and time-event numerical data such as arr_delay (arrival delay) and dep_delay (departure delay). We also have categorical data such as carrier, origin, dest, flight and tailnum of the aircraft. It is also a large dataset containing 330K entries. Enough to play with!!\nLet us replace the NAs in arr_delay and dep_delay with zeroes for now, and convert it into a time-series object with tsibble:\n\nflights_delay_ts &lt;- flights %&gt;% \n  \n  mutate(arr_delay = replace_na(arr_delay, 0), \n         dep_delay = replace_na(dep_delay, 0)) %&gt;% \n  \n  select(time_hour, arr_delay, dep_delay, \n         carrier, origin, dest, \n         flight, tailnum) %&gt;% \n  \n  tsibble::as_tsibble(index = time_hour, \n                      # All the remaining identify unique entries\n                      # Along with index\n                      # Many of these variables are common\n                      # Need *all* to make unique entries!\n                      key = c(carrier, origin, dest,flight, tailnum), \n                      validate = TRUE) # Making sure each entry is unique\n\n\nflights_delay_ts\n\n\n  \n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQ.1. Plot the monthly average arrival delay by carrier\n\n\nUsing tsibble\nUsing timetk\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nmean_arr_delays_by_carrier &lt;- \n  flights_delay_ts %&gt;%\n  group_by(carrier) %&gt;% \n  \n  index_by(month = ~ yearmonth(.)) %&gt;% \n  # index_by uses (year, yearquarter, yearmonth, yearweek, as.Date)\n  # to create a new column to show the time-grouping\n  # year / quarter / month/ week, or day...\n  # which IS different from traditional dplyr\n  \n  summarise(mean_arr_delay = \n              mean(arr_delay, na.rm = TRUE)\n  )\n\nmean_arr_delays_by_carrier\n# colours from the \"I want Hue\" website\ncolour_values &lt;- c(\"#634bd1\",\"#35d25a\",\"#757bff\",\"#fa9011\",\"#72369a\",\n               \"#617d00\",\"#d094ff\",\"#81d8a4\",\"#e63d2d\",\"#0080c0\",\n               \"#9e4500\",\"#98a9ff\",\"#efbd7f\",\"#474e8c\",\"#ffa1e4\",\n               \"#8a3261\",\"#a6c1f8\",\"#a16e96\")\n\n# Plotting with ggformula\nmean_arr_delays_by_carrier %&gt;%\n  gf_hline(yintercept = 0, color = \"grey\") %&gt;%\n  gf_line(\n    mean_arr_delay ~ month,\n    group = ~ carrier,\n    color = ~ carrier,\n    linewidth = 1.5,\n    title = \"Average Monthly Arrival Delays by Carrier\",\n    caption = \"Using tsibble + ggformula\"\n  ) %&gt;%\n  \n  gf_facet_wrap(vars(carrier), nrow = 4) %&gt;%\n\n  gf_refine(scale_color_manual(name = \"Airlines\",\n                               values = colour_values)) %&gt;%\n\n  gf_theme(theme(axis.text.x = element_text(angle = 45, \n                                            hjust = 1, \n                                            size = 6)))\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\nmean_arr_delays_by_carrier2 &lt;- \n  flights_delay_ts %&gt;% \n  as_tibble() %&gt;%\n  group_by(carrier) %&gt;% \n  \n  # summarize_by_time REQUIRES a tibble\n  # Cannot do this with a tsibble\n  timetk::summarise_by_time(.date_var = time_hour,\n                            .by = \"month\",\n                            mean_arr_delay = mean(arr_delay))\n\n\nmean_arr_delays_by_carrier2\ncolour_values &lt;- c(\"#634bd1\",\"#35d25a\",\"#757bff\",\"#fa9011\",\"#72369a\",\n               \"#617d00\",\"#d094ff\",\"#81d8a4\",\"#e63d2d\",\"#0080c0\",\n               \"#9e4500\",\"#98a9ff\",\"#efbd7f\",\"#474e8c\",\"#ffa1e4\",\n               \"#8a3261\",\"#a6c1f8\",\"#a16e96\")\np &lt;- mean_arr_delays_by_carrier2 %&gt;%\n  \n  timetk::plot_time_series(\n    #.data = .,\n    .date_var = time_hour, # no change to time variable name!\n    .value = mean_arr_delay,\n    .color_var = carrier,\n    .facet_vars = carrier,\n    .smooth = FALSE,\n    # .smooth_degree = 1,\n    # keep .smooth off since it throws warnings if there are too few points\n    # Like if we do quarterly or even yearly summaries\n    # Use only for smaller values of .smooth_degree (0,1)\n    .interactive = FALSE, .line_size = 2,\n    .facet_ncol = 4,.legend_show = FALSE, .facet_scales = \"fixed\",\n    .title = \"Average Monthly Arrival Delays by Carrier\",\n    .y_lab = \"Arrival Delays over Time\", .x_lab = \"Time\"\n  ) + \n  geom_hline(yintercept = 0, color = \"grey\") + \n  scale_colour_manual(name = \"Airline\", \n                      values = colour_values) +\n  theme_classic() + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(caption = \"Using timetk\")\n\n# Reverse the layers in the plot\n# Zero line BELOW, time series above\n# https://stackoverflow.com/questions/20249653/insert-layer-underneath-existing-layers-in-ggplot2-object\n\np$layers = rev(p$layers)\np\n\n\n\n\n  \n\n\n\n\n\n\n\nInsights: - Clearly airline OO has had some serious arrival delay problems in the first half of 2013… - most other delays are around the zero-line, with some variations in both directions\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQ.2. Plot a candlestick chart for total flight delays for a particular month for each origin across airlines!\n\n\nUsing ggformula\nUsing timetk\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n##\nflights_delay_ts %&gt;%\n  mutate(total_delay = arr_delay + dep_delay,\n         month = lubridate::month(time_hour, \n                                  # Makes ordinal factor with month labels\n                                  label = TRUE)) %&gt;% \n  filter(month == \"Dec\") %&gt;%\n  \n  gf_boxplot(total_delay ~ ., \n             color = ~ origin, \n             fill = ~ origin, alpha = 0.3) %&gt;%\n  gf_facet_wrap(vars(carrier), nrow = 3, scales = \"free_y\") %&gt;%\n  gf_theme(theme(axis.text.x = element_blank()))\n\n\n\n\n\n\n\n\n\n\nflights_delay_ts %&gt;% \n  mutate(total_delay = arr_delay + dep_delay) %&gt;%\n  timetk::filter_by_time(.start_date = \"2013-12\", \n                         .end_date = \"2013-12\") %&gt;%\n  \n  timetk::plot_time_series_boxplot(\n    .date_var = time_hour,\n    .value = total_delay,\n    .color_var = origin,\n    .facet_vars = carrier,\n    .period = \"month\",\n    .interactive = FALSE,\n    # .smooth_degree = 1,\n    # keep .smooth off since it throws warnings if there are too few points\n    # Like if we do quarterly or even yearly summaries\n    # Use only for smaller values of .smooth_degree (0,1)\n    .smooth = FALSE\n  )\n\n\n\n\n\n\n\nInsights: - JFK has more outliers in total_delay than EWR and LGA - Just a hint of more delays in April… and July?\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQ.3. Plot a heatmap chart for total flight delays by origin, aggregated by month\n\n\nUsing tsibble + ggformula\nUsing timetk\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n##\navg_delays_month &lt;- flights_delay_ts %&gt;% \n  group_by(origin) %&gt;% \n  mutate(total_delay = arr_delay + dep_delay) %&gt;% \n  index_by(month = ~ yearmonth(.)) %&gt;% \n  # index_by uses (year, yearquarter, yearmonth, yearweek, as.Date)\n  # to create a new column to show the time-grouping\n  # year / quarter / month/ week, or day...\n  # which IS different from traditional dplyr\n    summarise(mean_monthly_delay = mean(total_delay, na.rm = TRUE)\n  )\n\navg_delays_month \n\n\n  \n\n\n# three origins 12 months therefore 36 rows\n# Tsibble index_by + summarise also gives us a  month` column \n\n\n\nggformula::gf_tile(origin ~ month, fill = ~ mean_monthly_delay, \n                   color = \"black\", data = avg_delays_month,\n                   title = \"Mean Flight Delays from NY Airports in 2013\") %&gt;% \n  gf_theme(scale_fill_viridis_c(option = \"A\")) \n\n\n\n\n\n\n# \"magma\" (or \"A\") inferno\" (or \"B\") \"plasma\" (or \"C\") \n# \"viridis\" (or \"D\") \"cividis\" (or \"E\") \n# \"rocket\" (or \"F\") \"mako\" (or \"G\") \"turbo\" (or \"H\")\n\n\n\n\n\n\n\nInsights: - TBD - TBD"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html#seasons-trends-cycles-and-random-changes",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html#seasons-trends-cycles-and-random-changes",
    "title": "Analysis of Time Series in R",
    "section": "\n Seasons, Trends, Cycles, and Random Changes",
    "text": "Seasons, Trends, Cycles, and Random Changes\nHere are how the different types of patterns in time series are as follows:\n\nTrend: A trend exists when there is a long-term increase or decrease in the data. It does not have to be linear. Sometimes we will refer to a trend as “changing direction”, when it might go from an increasing trend to a decreasing trend.\n\n\nSeasonal: A seasonal pattern occurs when a time series is affected by seasonal factors such as the time of the year or the day of the week. Seasonality is always of a fixed and known period. The monthly sales of drugs (with the PBS data) shows seasonality which is induced partly by the change in the cost of the drugs at the end of the calendar year.\n\n\nCyclic: A cycle occurs when the data exhibit rises and falls that are not of a fixed frequency. These fluctuations are usually due to economic conditions, and are often related to the “business cycle”. The duration of these fluctuations is usually at least 2 years.\n\nThe function feasts::STL allows us to create these decompositions.\nLet us try to find and plot these patterns in Time Series.\n\nbirths_2000_2014 &lt;- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/births/US_births_2000-2014_SSA.csv\")\n##\nbirths_tsibble &lt;- \n  births_2000_2014 %&gt;%\n  mutate(index = lubridate::make_date(year = year, \n                                      month = month,\n                                      day = date_of_month)) %&gt;%\n  tsibble::as_tsibble(index = index) %&gt;%\n  select(index, births)\n##\nbirths_STL_yearly &lt;- \n  births_tsibble %&gt;% \n  fabletools::model(STL(births ~ season(period = \"year\")))\n\nfabletools::components(births_STL_yearly)\n\n\n  \n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n##\nfeasts::autoplot(components(births_STL_yearly))"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html#conclusion",
    "title": "Analysis of Time Series in R",
    "section": "\n Conclusion",
    "text": "Conclusion\nWe can plot most of the common Time Series Plots with the help of the tidyverts packages: ( tsibble, feast, fable and fabletools) , along with timetk and ggformula.\nThere are other plot packages to investigate, such as dygraphs\nRecall that we have used the tsibble format for the data. There are other formats such as ts, xts and others which are meant for time series analysis. But for our present purposes, we are able to do things with the capabilities of timetk."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html#references",
    "title": "Analysis of Time Series in R",
    "section": "\n References",
    "text": "References\n\nRob J Hyndman and George Athanasopoulos, Forecasting: Principles and Practice (3rd ed), Available Online https://otexts.com/fpp3/\nWhat is seasonal adjustment and why is it used?\nThe start-at-zero rule\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggridges\n0.5.6\nWilke (2024)\n\n\nNHANES\n2.1.0\nPruim (2015)\n\n\nTeachHist\n0.2.1\nLange (2023)\n\n\nTeachingDemos\n2.13\nSnow (2024)\n\n\n\n\n\n\nLange, Carsten. 2023. TeachHist: A Collection of Amended Histograms Designed for Teaching Statistics. https://CRAN.R-project.org/package=TeachHist.\n\n\nPruim, Randall. 2015. NHANES: Data from the US National Health and Nutrition Examination Study. https://CRAN.R-project.org/package=NHANES.\n\n\nSnow, Greg. 2024. TeachingDemos: Demonstrations for Teaching and Learning. https://CRAN.R-project.org/package=TeachingDemos.\n\n\nWilke, Claus O. 2024. ggridges: Ridgeline Plots in “ggplot2”. https://CRAN.R-project.org/package=ggridges."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/intro-favstats.html",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/intro-favstats.html",
    "title": "📊 Descriptive Statistics",
    "section": "",
    "text": "This tutorial uses web-r that allows you to run all code within your browser, on all devices. Most code chunks herein are formatted in a tabbed structure ( like in an old-fashioned library) with duplicated code. The tabs in front have regular R code that will work when copy-pasted in your RStudio session. The tab “behind” has the web-R code that can work directly in your browser, and can be modified as well. The R code is also there to make sure you have original code to go back to, when you have made several modifications to the code on the web-r tabs and need to compare your code with the original!\n\n\nRun selected code using either:\n\nmacOS: ⌘ + ↩︎/Return\n\nWindows/Linux: Ctrl + ↩︎/Enter\n\n\n\nRun the entire code by clicking the “Run code” button or pressing Shift+↩︎.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📊 Descriptive Statistics"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/intro-favstats.html#using-web-r",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/intro-favstats.html#using-web-r",
    "title": "📊 Descriptive Statistics",
    "section": "",
    "text": "This tutorial uses web-r that allows you to run all code within your browser, on all devices. Most code chunks herein are formatted in a tabbed structure ( like in an old-fashioned library) with duplicated code. The tabs in front have regular R code that will work when copy-pasted in your RStudio session. The tab “behind” has the web-R code that can work directly in your browser, and can be modified as well. The R code is also there to make sure you have original code to go back to, when you have made several modifications to the code on the web-r tabs and need to compare your code with the original!\n\n\nRun selected code using either:\n\nmacOS: ⌘ + ↩︎/Return\n\nWindows/Linux: Ctrl + ↩︎/Enter\n\n\n\nRun the entire code by clicking the “Run code” button or pressing Shift+↩︎.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📊 Descriptive Statistics"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/intro-favstats.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/intro-favstats.html#setting-up-r-packages",
    "title": "📊 Descriptive Statistics",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(palmerpenguins) # dataset \"penguins\"\nlibrary(skimr)\nlibrary(kableExtra)",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📊 Descriptive Statistics"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/intro-favstats.html#how-do-we-grasp-data",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/intro-favstats.html#how-do-we-grasp-data",
    "title": "📊 Descriptive Statistics",
    "section": "\n How do we Grasp Data?",
    "text": "How do we Grasp Data?\nWe spoke of Experiments and Data Gathering in the first module Nature of Data. This helped us to obtain data.\nAs we discussed in that same Module, for us to grasp the significance of the data, we need to describe it; the actual data is usually too vast for us to comprehend in its entirety. Anything more than a handful of observations in a dataset is enough for us to require other ways of grasping it.\nThe first thing we need to do, therefore, is to reduce it to a few salient numbers that allow us to summarize the data.\n\n\n\n\n\n\nReduction is Addition\n\n\n\nSuch a reduction may seem paradoxical but is one of the important tenets of statistics: reduction, while taking away information, ends up adding to insight.\nSteven Stigler (2016) is the author of the book “The Seven Pillars of Statistical Wisdom”. One of the Big Ideas in Statistics from that book is: Aggregation\n\nThe first pillar I will call Aggregation, although it could just as well be given the nineteenth-century name, “The Combination of Observations,” or even reduced to the simplest example, taking a mean. Those simple names are misleading, in that I refer to an idea that is now old but was truly revolutionary in an earlier day—and it still is so today, whenever it reaches into a new area of application. How is it revolutionary? By stipulating that, given a number of observations, you can actually gain information by throwing information away! In taking a simple arithmetic mean, we discard the individuality of the measures, subsuming them to one summary.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📊 Descriptive Statistics"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/intro-favstats.html#case-study-1",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/intro-favstats.html#case-study-1",
    "title": "📊 Descriptive Statistics",
    "section": "\n Case Study-1",
    "text": "Case Study-1\nWe will first use a dataset mpg that is available in R as part of one of the R packages that we have loaded with the library() command.\n\n Examine the Data\nIt is usually a good idea to make crisp business-like tables, for the data itself, and the schema as revealed by one of the outputs of the three methods to be presented below. There are many methods to do this; one of the simplest and effective ones is to use the kable set of commands from the knitr and kableExtra packages:\n\nmpg %&gt;% \n  head(10) %&gt;%\n  kbl(\n    # add Human Readable column names\n    col.names = c(\"Manufacturer\", \"Model\", \"Engine\\nDisplacement\", \n                    \"Model\\n Year\", \"Cylinders\", \"Transmission\",\n                    \"Drivetrain\", \"City\\n Mileage\", \"Highway\\n Mileage\",\n                    \"Fuel\", \"Class\\nOf\\nVehicle\"), \n    caption = \"MPG Dataset\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \n                                      \"condensed\", \"responsive\"),\n                full_width = F, position = \"center\")\n\n\n\n\nMPG Dataset\n\nManufacturer\nModel\nEngine Displacement\nModel Year\nCylinders\nTransmission\nDrivetrain\nCity Mileage\nHighway Mileage\nFuel\nClass Of Vehicle\n\n\n\naudi\na4\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\ncompact\n\n\naudi\na4\n1.8\n1999\n4\nmanual(m5)\nf\n21\n29\np\ncompact\n\n\naudi\na4\n2.0\n2008\n4\nmanual(m6)\nf\n20\n31\np\ncompact\n\n\naudi\na4\n2.0\n2008\n4\nauto(av)\nf\n21\n30\np\ncompact\n\n\naudi\na4\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\ncompact\n\n\naudi\na4\n2.8\n1999\n6\nmanual(m5)\nf\n18\n26\np\ncompact\n\n\naudi\na4\n3.1\n2008\n6\nauto(av)\nf\n18\n27\np\ncompact\n\n\naudi\na4 quattro\n1.8\n1999\n4\nmanual(m5)\n4\n18\n26\np\ncompact\n\n\naudi\na4 quattro\n1.8\n1999\n4\nauto(l5)\n4\n16\n25\np\ncompact\n\n\naudi\na4 quattro\n2.0\n2008\n4\nmanual(m6)\n4\n20\n28\np\ncompact\n\n\n\n\n\n\n\n\n\nNext we will look at a few favourite statistics or “favstats” that we can derive from data. R is full of packages that can provide very evocative and effective summaries of data. We will first start with the dplyr package from the tidyverse, the skimr package, then the mosaic package. We will look at the summary outputs from these and learn how to interpret them.\n\n\nUsing dpylr::glimpse()\nUsing skimr::skim()\nUsing mosaic::inspect()\n\n\n\nThe dplyr package offers a convenient command called glimpse:\n\nglimpse(mpg)\n\nRows: 234\nColumns: 11\n$ manufacturer &lt;chr&gt; \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"…\n$ model        &lt;chr&gt; \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4 quattro\", \"…\n$ displ        &lt;dbl&gt; 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.1, 1.8, 1.8, 2.0, 2.0, 2.…\n$ year         &lt;int&gt; 1999, 1999, 2008, 2008, 1999, 1999, 2008, 1999, 1999, 200…\n$ cyl          &lt;int&gt; 4, 4, 4, 4, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 8, 8, …\n$ trans        &lt;chr&gt; \"auto(l5)\", \"manual(m5)\", \"manual(m6)\", \"auto(av)\", \"auto…\n$ drv          &lt;chr&gt; \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"4\", \"4\", \"4\", \"4\", \"4…\n$ cty          &lt;int&gt; 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, 15, 17, 17, 1…\n$ hwy          &lt;int&gt; 29, 29, 31, 30, 26, 26, 27, 26, 25, 28, 27, 25, 25, 25, 2…\n$ fl           &lt;chr&gt; \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p…\n$ class        &lt;chr&gt; \"compact\", \"compact\", \"compact\", \"compact\", \"compact\", \"c…\n\n\n\n\n\n\n\n\nDescriptive Stat Summary from dplyr::glimpse()\n\n\n\nVery crisp output, giving us the size of the dataset (234 X 11) and the nature of the variable columns, along with their first few entries. The chr variables are usually Categorical/Qualitative, the int or dbl (double precision) are usually Numerical/Quantitative. But be careful! Verify that this is as per your intent, interpret the variables and modify their encoding as needed.\n\n\n\n\nLet us look at mpgusing skimr::skim().\nFrom the output of ?skimr:\n\nThe format of the results are a single wide data frame combining the results, with some additional attributes and two metadata columns:\n\n\n\nskim_variable: name of the original variable\n\nskim_type: class of the variable\n\nWe can use skim(dataset) directly as shown below:\n\nskimr::skim(mpg) # explicitly stating package name\n\n\nData summary\n\n\nName\nmpg\n\n\nNumber of rows\n234\n\n\nNumber of columns\n11\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nmanufacturer\n0\n1\n4\n10\n0\n15\n0\n\n\nmodel\n0\n1\n2\n22\n0\n38\n0\n\n\ntrans\n0\n1\n8\n10\n0\n10\n0\n\n\ndrv\n0\n1\n1\n1\n0\n3\n0\n\n\nfl\n0\n1\n1\n1\n0\n5\n0\n\n\nclass\n0\n1\n3\n10\n0\n7\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\ndispl\n0\n1\n3.47\n1.29\n1.6\n2.4\n3.3\n4.6\n7\n▇▆▆▃▁\n\n\nyear\n0\n1\n2003.50\n4.51\n1999.0\n1999.0\n2003.5\n2008.0\n2008\n▇▁▁▁▇\n\n\ncyl\n0\n1\n5.89\n1.61\n4.0\n4.0\n6.0\n8.0\n8\n▇▁▇▁▇\n\n\ncty\n0\n1\n16.86\n4.26\n9.0\n14.0\n17.0\n19.0\n35\n▆▇▃▁▁\n\n\nhwy\n0\n1\n23.44\n5.95\n12.0\n18.0\n24.0\n27.0\n44\n▅▅▇▁▁\n\n\n\n\n\nTaken together, we have the following:\n\n\n\n\n\n\nDescriptive Stat Summary from skimr::skim()\n\n\n\n\nA Data Summary: it lists the dimensions of the mpg dataset: 234 rows and 11 columns. 6 columns are character formatted, the remaining 5 are numeric. The dataset is not “grouped” (more on this later).\nThe second part of the output shows a table with the character variables which are therefore factor variables with levels.\nThe third part shows a table listing the names and summary stats for the numerical variables. We have mean, sd, all the quantiles (p0, p25, p50(median), p75 and p100 percentiles) and a neat little histogram for each. From the histogram we can see that year is two-valued, cyl is three-valued, and cty and hwy are continuous… Again check that this is as you intend them to be. We may need to modify the encoding if needed.\n\n\n\n\n\nWe get very similar output from mosaic::inspect():\n\ninspect(mpg)\n\n\ncategorical variables:  \n          name     class levels   n missing\n1 manufacturer character     15 234       0\n2        model character     38 234       0\n3        trans character     10 234       0\n4          drv character      3 234       0\n5           fl character      5 234       0\n6        class character      7 234       0\n                                   distribution\n1 dodge (15.8%), toyota (14.5%) ...            \n2 caravan 2wd (4.7%) ...                       \n3 auto(l4) (35.5%), manual(m5) (24.8%) ...     \n4 f (45.3%), 4 (44%), r (10.7%)                \n5 r (71.8%), p (22.2%), e (3.4%) ...           \n6 suv (26.5%), compact (20.1%) ...             \n\nquantitative variables:  \n   name   class    min     Q1 median     Q3  max        mean       sd   n\n1 displ numeric    1.6    2.4    3.3    4.6    7    3.471795 1.291959 234\n2  year integer 1999.0 1999.0 2003.5 2008.0 2008 2003.500000 4.509646 234\n3   cyl integer    4.0    4.0    6.0    8.0    8    5.888889 1.611534 234\n4   cty integer    9.0   14.0   17.0   19.0   35   16.858974 4.255946 234\n5   hwy integer   12.0   18.0   24.0   27.0   44   23.440171 5.954643 234\n  missing\n1       0\n2       0\n3       0\n4       0\n5       0\n\n\n\n\n\n\n\n\nDescriptive Stat Summary from mosaic::inspect()\n\n\n\nWe see that the output of mosaic::inspect() is organized as follows:\n\nThere are two dataframes/tables in the output, one describing the Qualitative Variables and the other describing the Quantitative Variables.\nIn the table describing the Qual variables, we have:\n\n\nname: Name of the variable in the (parent) dataset. i.e Column Names\n\nclass: format of that column\n\nlevels: All these variables are factors, with levels shown here. Some for example, manufacturer has 15 levels, and there are 234 rows\n\n\n\ninspect also conveniently shows how much data is missing and in which variables. This is a very important consideration in the use of the data for analytics purposes.\n\n\nWe can save and see the outputs separately:\n\nmpg_describe &lt;- inspect(mpg)\nmpg_describe$categorical\nmpg_describe$quantitative",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📊 Descriptive Statistics"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/intro-favstats.html#case-study-2",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/intro-favstats.html#case-study-2",
    "title": "📊 Descriptive Statistics",
    "section": "\n Case Study-2",
    "text": "Case Study-2\nInstead of taking a “built-in” dataset , i.e. one that is part of an R package that we can load with library(), let us try the above process with a data set that we obtain from the internet. We will use this superb repository of datasets created by Vincent Arel-Bundock: https://vincentarelbundock.github.io/Rdatasets/articles/data.html\nLet us choose a modest-sized dataset, say this dataset on Doctor Visits, which is available online https://vincentarelbundock.github.io/Rdatasets/csv/AER/DoctorVisits.csv and read it into R.\n\n\n\n\n\n\nReading external data into R\n\n\n\nThe read_csv() command from R package readr allows us to read both locally saved data on our hard disk, or data available in a shared folder online. Avoid using the read.csv() from base R, though it will show up in your code auto-complete set of options!\n\n\n\n# From Vincent Arel-Bundock's dataset website\n# https://vincentarelbundock.github.io/Rdatasets\n# \n# read_csv can read data directly from the net\n# \ndocVisits &lt;- read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/AER/DoctorVisits.csv\")\n\nRows: 5190 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): gender, private, freepoor, freerepat, nchronic, lchronic\ndbl (7): rownames, visits, age, income, illness, reduced, health\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nSo, a data frame containing 5,190 observations on 12 variables.\n\n\n\n\n\n\nHow about a locally stored CSV file?\n\n\n\nWe can also use a locally downloaded and stored CSV file. Assuming the file is stored in a subfolder called data inside your R project folder, we can proceed as follows:\n\n```{r}\n#| eval: false\ndocVisits &lt;- read_csv(\"data/DoctorVisits.csv\")\n```\n\n\n\nLet us quickly report the data itself, as in a real report. Note that we can use the features of the kableExtra package to dress up this table too!!\n\ndocVisits %&gt;%\n  head(10) %&gt;%\n  kbl(caption = \"Doctor Visits Dataset\",\n      # Add Human Readable Names if desired\n      # col.names(..names that you may want..)\n      ) %&gt;%\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\",\n                          \"condensed\", \"responsive\"),\n    full_width = F, position = \"center\")\n\n\n\n\nDoctor Visits Dataset\n\nrownames\nvisits\ngender\nage\nincome\nillness\nreduced\nhealth\nprivate\nfreepoor\nfreerepat\nnchronic\nlchronic\n\n\n\n1\n1\nfemale\n0.19\n0.55\n1\n4\n1\nyes\nno\nno\nno\nno\n\n\n2\n1\nfemale\n0.19\n0.45\n1\n2\n1\nyes\nno\nno\nno\nno\n\n\n3\n1\nmale\n0.19\n0.90\n3\n0\n0\nno\nno\nno\nno\nno\n\n\n4\n1\nmale\n0.19\n0.15\n1\n0\n0\nno\nno\nno\nno\nno\n\n\n5\n1\nmale\n0.19\n0.45\n2\n5\n1\nno\nno\nno\nyes\nno\n\n\n6\n1\nfemale\n0.19\n0.35\n5\n1\n9\nno\nno\nno\nyes\nno\n\n\n7\n1\nfemale\n0.19\n0.55\n4\n0\n2\nno\nno\nno\nno\nno\n\n\n8\n1\nfemale\n0.19\n0.15\n3\n0\n6\nno\nno\nno\nno\nno\n\n\n9\n1\nfemale\n0.19\n0.65\n2\n0\n5\nyes\nno\nno\nno\nno\n\n\n10\n1\nmale\n0.19\n0.15\n1\n0\n0\nyes\nno\nno\nno\nno\n\n\n\n\n\n\n\n\n\n\n Examine the Data\n\n\nUsing dplyr::glimpse()\nUsing skimr::skim()\nUsing mosaic::inspect()\n\n\n\n\nglimpse(docVisits)\n\nRows: 5,190\nColumns: 13\n$ rownames  &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ visits    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, …\n$ gender    &lt;chr&gt; \"female\", \"female\", \"male\", \"male\", \"male\", \"female\", \"femal…\n$ age       &lt;dbl&gt; 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, …\n$ income    &lt;dbl&gt; 0.55, 0.45, 0.90, 0.15, 0.45, 0.35, 0.55, 0.15, 0.65, 0.15, …\n$ illness   &lt;dbl&gt; 1, 1, 3, 1, 2, 5, 4, 3, 2, 1, 1, 2, 3, 4, 3, 2, 1, 1, 1, 1, …\n$ reduced   &lt;dbl&gt; 4, 2, 0, 0, 5, 1, 0, 0, 0, 0, 0, 0, 13, 7, 1, 0, 0, 1, 0, 0,…\n$ health    &lt;dbl&gt; 1, 1, 0, 0, 1, 9, 2, 6, 5, 0, 0, 2, 1, 6, 0, 7, 5, 0, 0, 0, …\n$ private   &lt;chr&gt; \"yes\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"yes\", \"ye…\n$ freepoor  &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", …\n$ freerepat &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", …\n$ nchronic  &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"yes\", \"yes\", \"no\", \"no\", \"no\", \"no\"…\n$ lchronic  &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", …\n\n\n\n\n\n\n\n\nDescriptive Stat Summary from dplyr::glimpse()\n\n\n\nVery crisp output, giving us the size of the dataset (5190 X 13) and the nature of the variable columns, along with their first few entries. There are several Quantitative variables: visits, age, income, illness, reduced and health; the rest seem to be Qualitative variables.\nAlways document your data with variable descriptions when you share it, a data dictionary!\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\nvisits\nNumber of doctor visits in past 2 weeks.\n\n\ngender\nFactor indicating gender.\n\n\nage\nAge in years divided by 100.\n\n\nincome\nAnnual income in tens of thousands of dollars.\n\n\nillness\nNumber of illnesses in past 2 weeks.\n\n\nreduced\nNumber of days of reduced activity in past 2 weeks due to illness or injury.\n\n\nhealth\nGeneral health questionnaire score using Goldberg’s method.\n\n\nprivate\nFactor. Does the individual have private health docVisits?\n\n\nfreepoor\nFactor. Does the individual have free government health docVisits due to low income?\n\n\nfreerepat\nFactor. Does the individual have free government health docVisits due to old age, disability or veteran status?\n\n\nnchronic\nFactor. Is there a chronic condition not limiting activity?\n\n\nlchronic\nFactor. Is there a chronic condition limiting activity?\n\n\n\n\n\n\nskim(docVisits) %&gt;% kbl()\n\n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\ncharacter.min\ncharacter.max\ncharacter.empty\ncharacter.n_unique\ncharacter.whitespace\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\ncharacter\ngender\n0\n1\n4\n6\n0\n2\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\nprivate\n0\n1\n2\n3\n0\n2\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\nfreepoor\n0\n1\n2\n3\n0\n2\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\nfreerepat\n0\n1\n2\n3\n0\n2\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\nnchronic\n0\n1\n2\n3\n0\n2\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\nlchronic\n0\n1\n2\n3\n0\n2\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\nrownames\n0\n1\nNA\nNA\nNA\nNA\nNA\n2595.5000000\n1498.3682792\n1.00\n1298.25\n2595.50\n3892.75\n5190.00\n▇▇▇▇▇\n\n\nnumeric\nvisits\n0\n1\nNA\nNA\nNA\nNA\nNA\n0.3017341\n0.7981338\n0.00\n0.00\n0.00\n0.00\n9.00\n▇▁▁▁▁\n\n\nnumeric\nage\n0\n1\nNA\nNA\nNA\nNA\nNA\n0.4063854\n0.2047818\n0.19\n0.22\n0.32\n0.62\n0.72\n▇▂▁▂▅\n\n\nnumeric\nincome\n0\n1\nNA\nNA\nNA\nNA\nNA\n0.5831599\n0.3689067\n0.00\n0.25\n0.55\n0.90\n1.50\n▇▆▅▅▂\n\n\nnumeric\nillness\n0\n1\nNA\nNA\nNA\nNA\nNA\n1.4319846\n1.3841524\n0.00\n0.00\n1.00\n2.00\n5.00\n▇▂▂▁▁\n\n\nnumeric\nreduced\n0\n1\nNA\nNA\nNA\nNA\nNA\n0.8618497\n2.8876284\n0.00\n0.00\n0.00\n0.00\n14.00\n▇▁▁▁▁\n\n\nnumeric\nhealth\n0\n1\nNA\nNA\nNA\nNA\nNA\n1.2175337\n2.1242665\n0.00\n0.00\n0.00\n2.00\n12.00\n▇▁▁▁▁\n\n\n\n\n\n\n\n\n\n\n\n\nDescriptive Stat Summary from skimr::skim()\n\n\n\n\nA Data Summary: it lists the dimensions of the docVisits dataset: 5190 rows and 13 columns. 6 columns are character formatted, the remaining 7 are numeric. The dataset is not “grouped” (more on this later).\nThe second part of the output shows a table with the character variables which are therefore factor variables with levels.\nThe third part shows a table listing the names and summary stats for the numerical variables. We have mean, sd, all the quantiles (p0, p25, p50(median), p75 and p100 percentiles) and a neat little histogram for each.\nCan we consider the health Goldberg score a Qualitative variable, to be understood as “ranks” between a minimum and maximum? It is just possible…\n\n\n\n\n\n\ninspect(docVisits)\n\n\ncategorical variables:  \n       name     class levels    n missing\n1    gender character      2 5190       0\n2   private character      2 5190       0\n3  freepoor character      2 5190       0\n4 freerepat character      2 5190       0\n5  nchronic character      2 5190       0\n6  lchronic character      2 5190       0\n                                   distribution\n1 female (52.1%), male (47.9%)                 \n2 no (55.7%), yes (44.3%)                      \n3 no (95.7%), yes (4.3%)                       \n4 no (79%), yes (21%)                          \n5 no (59.7%), yes (40.3%)                      \n6 no (88.3%), yes (11.7%)                      \n\nquantitative variables:  \n      name   class  min      Q1  median      Q3     max         mean\n1 rownames numeric 1.00 1298.25 2595.50 3892.75 5190.00 2595.5000000\n2   visits numeric 0.00    0.00    0.00    0.00    9.00    0.3017341\n3      age numeric 0.19    0.22    0.32    0.62    0.72    0.4063854\n4   income numeric 0.00    0.25    0.55    0.90    1.50    0.5831599\n5  illness numeric 0.00    0.00    1.00    2.00    5.00    1.4319846\n6  reduced numeric 0.00    0.00    0.00    0.00   14.00    0.8618497\n7   health numeric 0.00    0.00    0.00    2.00   12.00    1.2175337\n            sd    n missing\n1 1498.3682792 5190       0\n2    0.7981338 5190       0\n3    0.2047818 5190       0\n4    0.3689067 5190       0\n5    1.3841524 5190       0\n6    2.8876284 5190       0\n7    2.1242665 5190       0\n\n\n\n\n\n\n\n\nDescriptive Stat Summary from mosaic::inspect()\n\n\n\nWe see that the output of mosaic::inspect() is organized very similarly to the output from skim. Is there any missing data? Both skim and mosaic report on the data completion for each variable in the dataset.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📊 Descriptive Statistics"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/intro-favstats.html#groups-and-counts-of-qualitative-variables",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/intro-favstats.html#groups-and-counts-of-qualitative-variables",
    "title": "📊 Descriptive Statistics",
    "section": "\n Groups and Counts of Qualitative Variables",
    "text": "Groups and Counts of Qualitative Variables\nWhat is the most important dialogue uttered in the movie “Sholay”?\nRecall our discussion in Types of Data Variables. We have looked at means, limits, and percentiles of Quantitative variables. Another good idea to examine datasets is to look at counts, proportions,and frequencies with respect to Qualitative variables.\nWe typically do this with the dplyr package from the tidyverse.\n\n\ndiamonds dataset\ndocVisits dataset\n\n\n\n\ndiamonds %&gt;% count(cut)\n\n\n  \n\n\ndiamonds %&gt;% count(color)\n\n\n  \n\n\ndiamonds %&gt;% count(clarity)\n\n\n  \n\n\n### All combinations of cut, color, clarity\ndiamonds %&gt;% \n  count(across(where(is.ordered)))\n\n\n  \n\n\n\n\n\n\n\n\n\nBusiness Insights from Groups and Counts (diamonds)\n\n\n\nWe see that the groups for each level of cut, color, and clarity are not the same size: for instance the group with the “ideal” cut is largest at 21K observations, and “fair” has only 1.6K observations.\nGroup counts based on color are also not balanced, and nor are those for clarity. Counting all combinations of these three factors also shows imbalanced counts.\nThese aspects may need to be factored into downstream modelling or machine learning tasks. (Usually by stratification wrt levels of the Qualitative variables)\nThe levels are not too many, so tables work, and so would bar charts, which we will examine next. If there are too many levels in any factor, tables are a better option. Bar charts can still be plotted, but it may be preferable to lump smaller categories/levels together. (Using the forcats package)\n\n\n\n\n\n## Counting by the obvious factor variables\ndocVisits %&gt;% count(gender)\n\n\n  \n\n\ndocVisits %&gt;% count(private)\n\n\n  \n\n\ndocVisits %&gt;% count(freepoor)\n\n\n  \n\n\ndocVisits %&gt;% count(freerepat)\n\n\n  \n\n\ndocVisits %&gt;% count(lchronic)\n\n\n  \n\n\ndocVisits %&gt;% count(nchronic)\n\n\n  \n\n\n\n\n# Now for all Combinations...\n# Maybe too much to digest...\ndocVisits %&gt;% count(across(where(is.character)))\n\n\n  \n\n\n# Shall we try counting by some variables that might be factors?\n# Even if they are labeled as &lt;dbl&gt;?\n# \ndocVisits %&gt;% count(illness)\n\n\n  \n\n\ndocVisits %&gt;% count(health)\n\n\n  \n\n\n\n\n\n\n\n\n\nBusiness Insights from Groups and Counts (docVisits)\n\n\n\n\nMost of the counts are roughly balanced across the levels of the factors; however, freepoor and lchronic show unbalanced counts…\nThe factors are too numerous for a combination count table to very useful..\nCounting by illness and health does show that these two columns have a limited set of integer entries across over 5000 rows!! So these can be thought of as factors if needed in the analysis.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📊 Descriptive Statistics"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/intro-favstats.html#groups-and-summaries-of-quantitative-variables",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/intro-favstats.html#groups-and-summaries-of-quantitative-variables",
    "title": "📊 Descriptive Statistics",
    "section": "\n Groups and Summaries of Quantitative Variables",
    "text": "Groups and Summaries of Quantitative Variables\nWe saw that we could obtain numerical summary stats such as means, medians, quartiles, maximum/minimum of entire Quantitative variables, i.e the complete column. However, we often need identical numerical summary stats of parts of a Quantitative variable. Why?\nNote that we have Qualitative variables as well in a typical dataset. These Qual variables help us to group the entire dataset based on their combinations of levels. We can now think of summarizing Quant variables within each such group.\nLet us work through these ideas for both our familiar datasets.\n\n\ndiamonds dataset\ndocvisits dataset\n\n\n\n\ndiamonds %&gt;% \n  group_by(clarity) %&gt;% \n  summarize(average_price = mean(price), count = n())\n\n\n  \n\n\ndiamonds %&gt;% \n  group_by(clarity, color) %&gt;% \n  summarize(average_price = mean(price), count = n())\n\n\n  \n\n\n\n\n\n\n\n\n\nBusiness Insights from Grouped Quant Summaries (diamonds)\n\n\n\nWe have a good range of mean_prices over clarity and cut. The number of groups are large enough (&gt;&gt; 7!) to warrant a chart, which we will make in our next module on Distributions.\n\n\n\n\n\ndocVisits %&gt;%\n  group_by(gender) %&gt;% \n  summarize(average_visits = mean(visits), count = n())\n\n\n  \n\n\ndocVisits %&gt;%\n  group_by(gender) %&gt;% \n  summarize(average_visits = mean(visits), count = n())\n\n\n  \n\n\ndocVisits %&gt;% group_by(freepoor,nchronic) %&gt;% \n  summarise(mean_income = mean(income),\n            average_visits = mean(visits),\n            count = n())\n\n\n  \n\n\n\n\n\n\n\n\n\nBusiness Insights from Grouped Quant Summaries (docVisits)\n\n\n\nClearly the people who are freepoor ( On Govt Insurance) AND with a chronic condition are those who have lower average income and a higher average number of visits to the doctor…but there are relatively few of them (n = 55) in this dataset.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📊 Descriptive Statistics"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/intro-favstats.html#more-on-dplyr",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/intro-favstats.html#more-on-dplyr",
    "title": "📊 Descriptive Statistics",
    "section": "\n More on dplyr",
    "text": "More on dplyr\nThe dplyr package is capable of doing much more than just count, group_by and summarize. We will encounter this package many times more as we build our intuition about data visualization. A full tutorial on dplyr is linked to the icon below:\n\n\n\ndplyr Tutorial",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📊 Descriptive Statistics"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/intro-favstats.html#reporting-tables-for-data-and-the-data-schema",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/intro-favstats.html#reporting-tables-for-data-and-the-data-schema",
    "title": "📊 Descriptive Statistics",
    "section": "\n Reporting Tables for Data and the Data Schema",
    "text": "Reporting Tables for Data and the Data Schema\n\n\n\n\n\n\nData and the Data Schema are Different!!\n\n\n\nNote that all the three methods (dplyr::glimpse(), skimr::skim(), and mosaic::inspect()) report the schema of the original dataframe. The schema are also formatted as data frames! However they do not “contain” the original data! Do not confuse between the data and it’s reported schema!\n\n\nAs stated earlier, it is usually a good idea to make crisp business-like tables, for the data itself, and of the schema as revealed by one of the outputs of the three methods presented above. There are many methods to do this; one of the simplest and effective ones is to use the kable set of commands from the knitr package that we have installed already:\n\nmpg %&gt;% \n  head(10) %&gt;%\n  kbl(col.names = c(\"Manufacturer\", \"Model\", \"Engine\\nDisplacement\", \n                    \"Model\\n Year\", \"Cylinders\", \"Transmission\",\n                    \"Drivetrain\", \"City\\n Mileage\", \"Highway\\n Mileage\",\n                    \"Fuel\", \"Class\\nOf\\nVehicle\"), \n      longtable = FALSE, centering = TRUE,\n      caption = \"MPG Dataset\") %&gt;%\n    kable_styling(bootstrap_options = c(\"striped\", \"hover\", \n                                        \"condensed\", \"responsive\"),\n                  full_width = F, position = \"center\")\n\n\n\n\nMPG Dataset\n\nManufacturer\nModel\nEngine Displacement\nModel Year\nCylinders\nTransmission\nDrivetrain\nCity Mileage\nHighway Mileage\nFuel\nClass Of Vehicle\n\n\n\naudi\na4\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\ncompact\n\n\naudi\na4\n1.8\n1999\n4\nmanual(m5)\nf\n21\n29\np\ncompact\n\n\naudi\na4\n2.0\n2008\n4\nmanual(m6)\nf\n20\n31\np\ncompact\n\n\naudi\na4\n2.0\n2008\n4\nauto(av)\nf\n21\n30\np\ncompact\n\n\naudi\na4\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\ncompact\n\n\naudi\na4\n2.8\n1999\n6\nmanual(m5)\nf\n18\n26\np\ncompact\n\n\naudi\na4\n3.1\n2008\n6\nauto(av)\nf\n18\n27\np\ncompact\n\n\naudi\na4 quattro\n1.8\n1999\n4\nmanual(m5)\n4\n18\n26\np\ncompact\n\n\naudi\na4 quattro\n1.8\n1999\n4\nauto(l5)\n4\n16\n25\np\ncompact\n\n\naudi\na4 quattro\n2.0\n2008\n4\nmanual(m6)\n4\n20\n28\np\ncompact\n\n\n\n\n\n\n\n\n\nAnd for the schema from skim(), with some extra bells and whistles on the table:\n\nskim(mpg) %&gt;%\n  kbl(align = \"c\", caption = \"Skim Output for mpg Dataset\") %&gt;%\nkable_paper(full_width = F)\n\n\n\nSkim Output for mpg Dataset\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\ncharacter.min\ncharacter.max\ncharacter.empty\ncharacter.n_unique\ncharacter.whitespace\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\ncharacter\nmanufacturer\n0\n1\n4\n10\n0\n15\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\nmodel\n0\n1\n2\n22\n0\n38\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\ntrans\n0\n1\n8\n10\n0\n10\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\ndrv\n0\n1\n1\n1\n0\n3\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\nfl\n0\n1\n1\n1\n0\n5\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\nclass\n0\n1\n3\n10\n0\n7\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\ndispl\n0\n1\nNA\nNA\nNA\nNA\nNA\n3.471795\n1.291959\n1.6\n2.4\n3.3\n4.6\n7\n▇▆▆▃▁\n\n\nnumeric\nyear\n0\n1\nNA\nNA\nNA\nNA\nNA\n2003.500000\n4.509646\n1999.0\n1999.0\n2003.5\n2008.0\n2008\n▇▁▁▁▇\n\n\nnumeric\ncyl\n0\n1\nNA\nNA\nNA\nNA\nNA\n5.888889\n1.611535\n4.0\n4.0\n6.0\n8.0\n8\n▇▁▇▁▇\n\n\nnumeric\ncty\n0\n1\nNA\nNA\nNA\nNA\nNA\n16.858974\n4.255946\n9.0\n14.0\n17.0\n19.0\n35\n▆▇▃▁▁\n\n\nnumeric\nhwy\n0\n1\nNA\nNA\nNA\nNA\nNA\n23.440171\n5.954643\n12.0\n18.0\n24.0\n27.0\n44\n▅▅▇▁▁\n\n\n\n\n\n\nSee https://haozhu233.github.io/kableExtra/ for more options on formatting the table with kableExtra.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📊 Descriptive Statistics"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/intro-favstats.html#a-quick-quiz",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/intro-favstats.html#a-quick-quiz",
    "title": "📊 Descriptive Statistics",
    "section": "\n A Quick Quiz",
    "text": "A Quick Quiz\n\n\n\n\n\n\nWarning\n\n\n\nIt is always a good idea to look for variables in data that may be incorrectly formatted. For instance, a variable marked as numerical may have the values 1-2-3-4 which represent options, sizes, or say months. in which case it would have to be interpreted as a factor.\n\n\nLet us take a small test with the mpg dataset:\n\nWhat is the number of qualitative/categorical variables in the mpg data?    \n\nHow many manufacturers are named in this dataset?    \n\nHow many levels does the variable drv have?    \n\nHow many quantitative/numerical variables shown in the mpg data?    \n\nBut the variable hwy\ncty\ncyl\ndispl   is actually a qualitative variable.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📊 Descriptive Statistics"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/intro-favstats.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/intro-favstats.html#conclusion",
    "title": "📊 Descriptive Statistics",
    "section": "\n Conclusion",
    "text": "Conclusion\nThe three methods given here give us a very comprehensive look into the structure of the dataset.\nUse the kable set of commands to make a smart-looking of the data and the outputs of any of the three methods.\nMake these part of your Workflow.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📊 Descriptive Statistics"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/intro-favstats.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/intro-favstats.html#references",
    "title": "📊 Descriptive Statistics",
    "section": "\n References",
    "text": "References\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nmosaic\n1.9.1\nPruim, Kaplan, and Horton (2017)\n\n\npalmerpenguins\n0.1.1\nHorst, Hill, and Gorman (2020)\n\n\nskimr\n2.1.5\nWaring et al. (2022)\n\n\n\n\n\n\nHorst, Allison Marie, Alison Presmanes Hill, and Kristen B Gorman. 2020. palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data. https://doi.org/10.5281/zenodo.3960218.\n\n\nPruim, Randall, Daniel T Kaplan, and Nicholas J Horton. 2017. “The Mosaic Package: Helping Students to ‘Think with Data’ Using r.” The R Journal 9 (1): 77–102. https://journal.r-project.org/archive/2017/RJ-2017-024/index.html.\n\n\nStigler, Stephen M. 2016. “The Seven Pillars of Statistical Wisdom,” March. https://doi.org/10.4159/9780674970199.\n\n\nWaring, Elin, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia, Hao Zhu, and Shannon Ellis. 2022. skimr: Compact and Flexible Summaries of Data. https://CRAN.R-project.org/package=skimr.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📊 Descriptive Statistics"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/180-RefMat/other-tools.html",
    "href": "content/courses/Analytics/Descriptive/Modules/180-RefMat/other-tools.html",
    "title": "📚 Miscellaneous Graphing Tools, and References",
    "section": "",
    "text": "https://rawgraphs.io\n\n\n\nhttps://datawrapper.de\n\n\n\nhttps://hdlab.stanford.edu/palladio/\n\n\n\nhttps://infogram.com/\n\n\n\nhttps://www.visme.co/chart-maker/\n\n\n\nhttps://flourish.studio/ https://www.figma.com/",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📚 Miscellaneous Graphing Tools, and References"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/180-RefMat/other-tools.html#what-other-free-tools-are-there-on-the-web",
    "href": "content/courses/Analytics/Descriptive/Modules/180-RefMat/other-tools.html#what-other-free-tools-are-there-on-the-web",
    "title": "📚 Miscellaneous Graphing Tools, and References",
    "section": "",
    "text": "https://rawgraphs.io\n\n\n\nhttps://datawrapper.de\n\n\n\nhttps://hdlab.stanford.edu/palladio/\n\n\n\nhttps://infogram.com/\n\n\n\nhttps://www.visme.co/chart-maker/\n\n\n\nhttps://flourish.studio/ https://www.figma.com/",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📚 Miscellaneous Graphing Tools, and References"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/180-RefMat/other-tools.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/180-RefMat/other-tools.html#references",
    "title": "📚 Miscellaneous Graphing Tools, and References",
    "section": "References",
    "text": "References\n\nGetting started with Flourish & Figma to create beautiful custom charts, https://inside.mediahack.co.za/getting-started-with-flourish-figma-to-create-beautiful-custom-charts-34e4efb8fd3d\nFlowing Data Chart Types https://flowingdata.com/chart-types/\nGeeks for Geeks: Chart Types https://www.geeksforgeeks.org/r-charts-and-graphs/\nFinancial Times Visual Vocabulary (Interactive) https://ft-interactive.github.io/visual-vocabulary/\nFinancial Times Visual Vocabulary (PDF) https://github.com/Financial-Times/chart-doctor/blob/main/visual-vocabulary/FT4schools_RGS.pdf\nFinancial Times Data Journalism Visuals https://www.ft.com/visual-and-data-journalism\nSeverino Ribecca and John Schwabish , The Graphic Continuum https://www.severinoribecca.one/portfolio-item/the-graphic-continuum/\nWeb based tools for Dataviz https://policyviz.com/resources/data-viz-tools/\nNightingale Data Visualization Society Blog: How to visualize categorical data: https://nightingaledvs.com/endless-river-an-overview-of-dataviz-for-categorical-data/\nJohn Schwabish’s policyviz Data Viz catalogue: https://datastudio.google.com/s/quUUlgosF4U",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📚 Miscellaneous Graphing Tools, and References"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/180-RefMat/other-tools.html#papers",
    "href": "content/courses/Analytics/Descriptive/Modules/180-RefMat/other-tools.html#papers",
    "title": "📚 Miscellaneous Graphing Tools, and References",
    "section": "Papers",
    "text": "Papers\n1.Christopher G. Healey Department of Computer Science, North Carolina State University. Perception in Visualization",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📚 Miscellaneous Graphing Tools, and References"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html",
    "title": "The Grammar of Networks",
    "section": "",
    "text": "# Fonts\n# Run these next few commands IN YOUR CONSOLE once. \n#install.packages(\"extrafontdb\")\n#library(extrafont)\n#extrafont::font_import(paths = NULL, recursive = TRUE, prompt = TRUE,pattern = NULL)\n\n########################################\n# For General Data Manipulation\nlibrary(tidyverse)\n\n########################################\n# Network Analysis Library (Handle data and Viz)\nlibrary(igraph)\n\n########################################\n# For Network \"Manipulation\"\nlibrary(tidygraph)\n\n# For Network Visualization\nlibrary(ggraph)\nlibrary(graphlayouts)\nlibrary(visNetwork)\n\n# For \"Network\" Datasets\nlibrary(igraphdata)\n\n# Fonts\nlibrary(ggtext) # Claus Wilke's package\nlibrary(showtext)\nlibrary(fontawesome)\n# For repeatable layouts, some can be random!!\nset.seed(12345)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#setting-up-r-packages",
    "title": "The Grammar of Networks",
    "section": "",
    "text": "# Fonts\n# Run these next few commands IN YOUR CONSOLE once. \n#install.packages(\"extrafontdb\")\n#library(extrafont)\n#extrafont::font_import(paths = NULL, recursive = TRUE, prompt = TRUE,pattern = NULL)\n\n########################################\n# For General Data Manipulation\nlibrary(tidyverse)\n\n########################################\n# Network Analysis Library (Handle data and Viz)\nlibrary(igraph)\n\n########################################\n# For Network \"Manipulation\"\nlibrary(tidygraph)\n\n# For Network Visualization\nlibrary(ggraph)\nlibrary(graphlayouts)\nlibrary(visNetwork)\n\n# For \"Network\" Datasets\nlibrary(igraphdata)\n\n# Fonts\nlibrary(ggtext) # Claus Wilke's package\nlibrary(showtext)\nlibrary(fontawesome)\n# For repeatable layouts, some can be random!!\nset.seed(12345)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#introduction",
    "title": "The Grammar of Networks",
    "section": "\n Introduction",
    "text": "Introduction\nNetwork graphs show relationships between entities: what sort they are, how strong they are, and even of they change over time.\nWe will examine data structures pertaining both to the entities and the relationships between them and look at the data object that can combine these aspects together. Then we will see how these are plotted, what the structure of the plot looks like. There are also metrics that we can calculate for the network, based on its structure. We will of course examine geometric metaphors that can represent various classes of entities and their relationships.\nNetwork graphs can be rendered both as static and interactive and we will examine R packages that render both kinds of plots.\nThere is a another kind of structure: one that combines spatial and network data in one. We will defer that for a future module !"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#what-kind-network-graphs-will-we-make",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#what-kind-network-graphs-will-we-make",
    "title": "The Grammar of Networks",
    "section": "What kind Network graphs will we make?",
    "text": "What kind Network graphs will we make?\nHere is a network map of the characters in Victor Hugo’s Les Miserables:\n\n\nAnd this: the well known Zachary’s Karate Club dataset visualized as a network"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#goals",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#goals",
    "title": "The Grammar of Networks",
    "section": "\n Goals",
    "text": "Goals\nAt the end of this Lab session, we should:\n\nknow the types and structures of network data and be able to work with them\nunderstand the basics of modern network packages in R\nbe able to create network visualizations using tidygraph, ggraph( static visualizations ) and visNetwork (interactive visualizations)\nsee directions for how the network metaphor applies in a variety of domains (e.g. biology/ecology, ideas/influence, technology, transportation, to name a few)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#pedagogical-note",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#pedagogical-note",
    "title": "The Grammar of Networks",
    "section": "\n Pedagogical Note",
    "text": "Pedagogical Note\nThe method followed will be based on PRIMM:\n\n\nPREDICT Inspect the code and guess at what the code might do, write predictions\n\n\nRUN the code provided and check what happens\n\nINFER what the parameters of the code do and write comments to explain. What bells and whistles can you see?\n\nMODIFY the parameters code provided to understand the options available. Write comments to show what you have aimed for and achieved.\n\nMAKE : take an idea/concept of your own, and graph it."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#graph-metaphors",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#graph-metaphors",
    "title": "The Grammar of Networks",
    "section": "\n Graph Metaphors",
    "text": "Graph Metaphors\nNetwork graphs are characterized by two key terms: nodes and edges\n\n\nNodes : Entities\n\nMetaphors: Individual People? Things? Ideas? Places? to be connected in the network.\nSynonyms: vertices. Nodes have IDs.\n\n\n\nEdges: Connections\n\nMetaphors: Interactions? Relationships? Influence? Letters sent and received? Dependence? between the entities.\nSynonyms: links, ties.\n\n\n\nIn R, we create network representations using node and edge information. One way in which these could be organized are:\n\n\nNode list: a data frame with a single column listing the node IDs found in the edge list. You can also add attribute columns to the data frame such as the names of the nodes or grouping variables. ( Type? Class? Family? Country? Subject? Race? )\n\n\nNode Table\n\n\n\n\n\n\n\nID\nNode Name\nAttribute? Qualities?Categories? Family? Country?Planet?\n\n\n1\nNed\nNursery School Teacher\n\n\n2\nJaguar Paw\nMain Character, Apocalypto\n\n\n3\nJohn Snow\nEpidemiologist\n\n\n\n\n\nEdge list: data frame containing two columns: source node and destination node of an edge. Source and Destination have node IDs.\n\nWeighted network graph: An edge list can also contain additional columns describing attributes of the edges such as a magnitude aspect for an edge. If the edges have a magnitude attribute the graph is considered weighted.\n\n\nEdges Table\n\nFrom\nTo\nRelationship\nWeightage\n\n\n\n1\n3\nFinancial Dealings\n6\n\n\n2\n1\nHistory Lessons\n2\n\n\n2\n3\nVaccination\n15\n\n\n\n\n\nLayout: A geometric arrangement of nodes and edges.\n\nMetaphors: Location? Spacing? Distance? Coordinates? Colour? Shape? Size? Provides visual insight due to the arrangement.\n\n\n\nLayout Algorithms : Method to arranges nodes and edges with the aim of optimizing some metric .\n\nMetaphors: Nodes are masses and edges are springs. The Layout algorithm minimizes the stretching and compressing of all springs.(BTW, are the Spring Constants K the same for all springs?…)\n\n\nDirected and undirected network graph: If the distinction between source and target is meaningful, the network is directed. If the distinction is not meaningful, the network is undirected. Directed edges represent an ordering of nodes, like a relationship extending from one node to another, where switching the direction would change the structure of the network. Undirected edges are simply links between nodes where order does not matter.\n\n\n\n\n\n\n\nExamples\n\n\n\n\nThe World Wide Web is an example of a directed network because hyperlinks connect one Web page to another, but not necessarily the other way around.\nCo-authorship networks represent examples of un-directed networks, where nodes are authors and they are connected by an edge if they have written a publication together\nWhen people send e-mail to each other, the distinction between the sender (source) and the recipient (target) is clearly meaningful, therefore the network is directed.\n\n\n\n\n\nConnected and Disconnected graphs: If there is some path from any node to any other node, the Networks is said to be Connected. Else, Disconnected."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#predictruninfer-1",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#predictruninfer-1",
    "title": "The Grammar of Networks",
    "section": "Predict/Run/Infer-1",
    "text": "Predict/Run/Infer-1\nUsing tidygraph and ggraph\n\ntidygraph and ggraph are modern R packages for network data. Graph Data setup and manipulation is done in tidygraph and graph visualization with ggraph.\n\n\ntidygraph Data -&gt; “Network Object” in R.\n\nggraph Network Object -&gt; Plots using a chosen layout/algo.\n\nBoth leverage the power of igraph, which is the Big Daddy of all network packages. We will be using the Grey’s Anatomy dataset in our first foray into networks.\nStep1. Read the data\nDownload these two datasets into your current project-&gt; data folder.\n Grey’s Anatomy Nodes \n Grey’s Anatomy Edges \ngrey_nodes &lt;- read_csv(\"data/grey_nodes.csv\")\ngrey_edges &lt;- read_csv(\"data/grey_edges.csv\")\n\ngrey_nodes\ngrey_edges\n\n\n\n\n  \n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #1\n\n\n\nLook at the output thumbnails. What attributes (i.e. extra information) are seen for Nodes and Edges?\n\n\nStep 2.Create a network object using tidygraph:\nKey function:\n\n\ntbl_graph(): (aka “tibble graph”). Key arguments: nodes, edges and directed. Note this is a very versatile command and can take many input forms, such as data structures that result from other packages. Type ?tbl_graph in the Console and see the Usage section.\n\n\nga &lt;- tbl_graph(nodes = grey_nodes, \n                edges = grey_edges, \n                directed = FALSE)\nga\n\n# A tbl_graph: 54 nodes and 57 edges\n#\n# An undirected simple graph with 4 components\n#\n# Node Data: 54 × 7 (active)\n   name               sex   race  birthyear position  season sign    \n   &lt;chr&gt;              &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   \n 1 Addison Montgomery F     White      1967 Attending      1 Libra   \n 2 Adele Webber       F     Black      1949 Non-Staff      2 Leo     \n 3 Teddy Altman       F     White      1969 Attending      6 Pisces  \n 4 Amelia Shepherd    F     White      1981 Attending      7 Libra   \n 5 Arizona Robbins    F     White      1976 Attending      5 Leo     \n 6 Rebecca Pope       F     White      1975 Non-Staff      3 Gemini  \n 7 Jackson Avery      M     Black      1981 Resident       6 Leo     \n 8 Miranda Bailey     F     Black      1969 Attending      1 Virgo   \n 9 Ben Warren         M     Black      1972 Other          6 Aquarius\n10 Henry Burton       M     White      1972 Non-Staff      7 Cancer  \n# ℹ 44 more rows\n#\n# Edge Data: 57 × 4\n   from    to weight type    \n  &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt;   \n1     5    47      2 friends \n2    21    47      4 benefits\n3     5    46      1 friends \n# ℹ 54 more rows\n\n\n\n\n\n\n\n\nQuestions and Inferences #2\n\n\n\nWhat information does the graph object contain? What attributes do the nodes have? What about the edges?\n\n\nStep 3. Plot using ggraph\n\n3a. Quick Plot: autograph() This is to check quickly is the data is imported properly and to decide upon going on to a more elaborate plotting.\n\nautograph(ga)\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #3\n\n\n\nDescribe this graph, in simple words here. Try to use some of the new domain words we have just acquired: nodes/edges, connected/disconnected, directed/undirected.\n\n\n3b. More elaborate plot\nKey functions:\n\n\nggraph(layout = \"......\"): Create classic node-edge diagrams; i.e. Sets up the graph. Rather like ggplot for networks!\n\nTwo kinds of geom: one set for nodes, and another for edges\n\ngeom_node_point(aes(.....)): Draws node as “points”. Alternatives are circle / arc_bar / tile / voronoi. Remember the geoms that we have seen before in Grammar of Graphics!\ngeom_edge_link0(aes(.....)): Draws edges as “links”. Alternatives are arc / bend / elbow / hive / loop / parallel / diagonal / point / span /tile.\ngeom_node_text(aes(label = ......), repel = TRUE): Adds text labels (non-overlapping). Alternatives are label /...\nlabs(title = \"....\", subtitle = \"....\", caption = \"....\"): Change main titles, axis labels and legend titles. We know this from our work with ggplot.\n\n\n# Write Comments next to each line \n# About what that line does for the overall graph\n\nggraph(graph = ga,  layout = \"kk\") +\n  #\n  geom_edge_link0(width = 2, color = \"pink\") +\n  #\n  geom_node_point(shape = 21, size = 8,\n                  fill = \"blue\",\n                  color = \"green\",\n                  stroke = 2) +\n  \n  labs(title = \"Whoo Hoo! My First Silly Grey's Anatomy graph in R!\",\n       subtitle = \"Why did I ever get in this course...\",\n       caption = \"Bro, they are doing cool things in the other classes...\\n And the show is even more cool!\") +\n  \n  set_graph_style(family = \"Roboto\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #4:\n\n\n\nWhat parameters have been changed here, compared to the earlier graph? Where do you see these changes in the code above?\n\n\nLet us Play with this graph and see if we can make some small changes. Colour? Fill? Width? Size? Stroke? Labs? Of course!\n\n# Change the parameters in each of the commands here to new ones\n# Use fixed values for colours or sizes...etc. \n\nggraph(graph = ga,  layout = \"kk\") + \n  geom_edge_link0(width = 2) + \n  geom_node_point(shape = 21, size = 4, \n                  fill = \"moccasin\", \n                  color = \"firebrick\", \n                  stroke = 2) +\n  labs(title = \"Whoo Hoo! My next silly Grey's Anatomy graph in R!\",\n       subtitle = \"Why did I ever get in this course...\",\n       caption = \"Bro, they are doing cool things in the other classes...\") + \nset_graph_style(family = \"Roboto\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #5\n\n\n\nWhat did the shape parameter achieve? What are the possibilities with shape? How about including alpha?\n\n\n3c. Aesthetic Mapping from Node and Edge attribute columns\nUp to now, we have assigned specific numbers to geometric aesthetics such as shape and size. Now we are ready ( maybe ?) change the meaning and significance of the entire graph and each element within it, and use aesthetics / metaphoric mappings to achieve new meanings or insights. Let us try using aes() inside each geom to map a variable to a geometric aspect.\nDon’t try to use more than 2 aesthetic mappings simultaneously!!\nThe node elements we can tweak are:\n\nTypes of Nodes: geom_node_****()\n\nNode Parameters: inside geom_node_****(aes(...............))\n-aes(alpha  = node-variable) : opacity; a value between 0 and 1\n-aes(shape  = node-variable) : node shape\n-aes(colour = node-variable) : node colour\n-aes(fill   = node-variable) : fill colour for node\n-aes(size   = node-variable) : size of node\n\nThe edge elements we can tweak are:\n\nType of Edges” geom_edge_****()\n\nEdge Parameters: inside geom_edge_****(aes(...............))\n-aes(colour = edge-variable) : colour of the edge\n-aes(width  = edge-variable) : width of the edge\n-aes(label  = some_variable) : labels for the edge\n\nType ?geom_node_point and ?geom-edge_link in your Console for more information.\n\nggraph(graph = ga, layout = \"fr\") +\n  geom_edge_link0(aes(width = weight)) + # change variable here\n  \n  geom_node_point(aes(color = race), size = 6) + # change variable here\n\n  labs(title = \"Whoo Hoo! Yet another Grey's Anatomy graph in R!\",\n       subtitle = \"Colouring Nodes by Attribute\",\n       caption = \"Grey's Anatomy\") +\n  \n  scale_edge_width(range = c(0.2,2)) +\n  set_graph_style(family = \"roboto\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #6\n\n\n\nDescribe some of the changes here. What types of edges worked? Which variables were you able to use for nodes and edges and how? What did not work with either of the two?"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#predictreuseinfer-2",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#predictreuseinfer-2",
    "title": "The Grammar of Networks",
    "section": "Predict/Reuse/Infer-2",
    "text": "Predict/Reuse/Infer-2\n\n# Arc diagram\n\nggraph(ga, layout = \"linear\") +\n  geom_edge_arc0(aes(width = weight), alpha = 0.8) +\n  scale_edge_width(range = c(0.2, 2)) +\n  geom_node_point(size = 2, colour = \"red\") +\n  labs(edge_width = \"Weight\",title = \"Grey's Anatomy\", subtitle = \"Arc Layout\") +\n  set_graph_style(family = \"Roboto\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #7\n\n\n\nHow does this graph look “metaphorically” different? Do you see a difference in the relationships between people here? Why?\n\n\n\n# Coord diagram, circular\nggraph(ga, layout = \"linear\", circular = TRUE) + # Note the layout!\n  geom_edge_arc0(aes(width = weight), alpha = 0.8) + \n  scale_edge_width(range = c(0.2, 2)) +\n  \n  geom_node_point(size = 3,colour = \"red\") + \n  geom_node_text(aes(label = name),\n                 repel = TRUE, size = 2,check_overlap = TRUE, \n                 max.overlaps = 25) +\n  labs(edge_width = \"Weight\")  +\n  theme(aspect.ratio = 1) +\n  set_graph_style(family = \"Roboto\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #8\n\n\n\nHow does this graph look “metaphorically” different? Do you see a difference in the relationships between people here? Why?"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#hierarchical-layouts",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#hierarchical-layouts",
    "title": "The Grammar of Networks",
    "section": "Hierarchical layouts",
    "text": "Hierarchical layouts\nThese provide for some alternative metaphorical views of networks. Note that not all layouts are possible for all datasets!!\n\n# set_graph_style()\n\n# This dataset contains the graph that describes the class \n# hierarchy for the Flare visualization library.\n# Type ?flare in your Console\nhead(flare$vertices)\n\n\n  \n\n\nhead(flare$edges)\n\n\n  \n\n\n# flare class hierarchy\ngraph &lt;-  tbl_graph(edges = flare$edges, nodes = flare$vertices)\n\n##\nset_graph_style(family = \"Roboto\")\n##\n\n# dendrogram\nggraph(graph, layout = \"dendrogram\") + \n  geom_edge_diagonal() + \n  labs(title = \"Dendrogram\") \n\n# circular dendrogram\nggraph(graph, layout = \"dendrogram\", circular = TRUE) + \n  geom_edge_diagonal0() + \n  geom_node_point(aes(filter = leaf)) + \n  coord_fixed()+ \n  labs(title = \"Circular Dendrogram\")\n\n# rectangular tree map\nggraph(graph, layout = \"treemap\", weight = size) + \n  geom_node_tile(aes(fill = depth), size = 0.25) + \n  scale_fill_distiller(palette = \"Pastel1\") + \n  labs(title = \"Rectangular Tree Map\")\n\n\n# circular tree map\nggraph(graph, layout = \"circlepack\", weight = size) + \n  geom_node_circle(aes(fill = depth), size = 0.25, n = 50) + \n  scale_fill_distiller(palette = \"Accent\") + \n  coord_fixed() + \n  labs(title = \"Circular Tree Map\")\n\n\n# icicle\nggraph(graph, layout = \"partition\") + \n  geom_node_tile(aes(y = -y, fill = depth)) + \n  scale_fill_distiller(palette = \"Set3\") + \n  labs(title = \"Icicle Chart\")\n\n# sunburst (circular icicle)\nggraph(graph, layout = \"partition\", circular = TRUE) +\n  geom_node_arc_bar(aes(fill = depth)) +\n  scale_fill_distiller(palette = \"Spectral\") + \n  coord_fixed() + \n  labs(title = \"Circular Icicle\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #9\n\n\n\nHow do graphs look “metaphorically” different? Do they reveal different aspects of the group? How?"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#faceting",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#faceting",
    "title": "The Grammar of Networks",
    "section": "Faceting",
    "text": "Faceting\nFaceting allows to create sub-plots according to the values of a qualitative attribute on nodes or edges.\n##\nset_graph_style(family = \"Roboto\",size = 8)\n##\n# facet edges by type\nggraph(ga,layout = \"linear\", circular = TRUE) + \n  geom_edge_link0(aes(color = type)) + \n  geom_node_point() +\n  facet_edges(~ type) +\n  th_foreground(border = TRUE) +\n  theme(aspect.ratio = 1)\n# facet nodes by sex\nggraph(ga,layout = \"linear\", circular = TRUE) + \n  geom_edge_link0() + \n  geom_node_point() +\n  facet_nodes(~race) +\n  th_foreground(border = TRUE) +\n  theme(aspect.ratio = 1)\n# facet both nodes and edges\nggraph(ga,layout = \"linear\", circular = TRUE) + \n  geom_edge_link0(aes(color = type)) + \n  geom_node_point() +\n  facet_graph(type ~ race) + \n  th_foreground(border = TRUE) +\n  theme(aspect.ratio = 1, legend.position = \"right\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #10\n\n\n\nDoes splitting up the main graph into sub-networks give you more insight? Describe some of these."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#network-analysis-with-tidygraph",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#network-analysis-with-tidygraph",
    "title": "The Grammar of Networks",
    "section": "Network analysis with tidygraph",
    "text": "Network analysis with tidygraph\nThe data frame graph representation can be easily augmented with metrics or statistics computed on the graph. Remember how we computed counts with the penguin dataset in Grammar of Graphics.\nBefore computing a metric on nodes or edges use the activate() function to activate either node or edge data frames. Use dplyr verbs (filter, arrange, mutate) to achieve your computation in the proper way.\nNetwork Centrality: Go-To and Go-Through People!\nCentrality is a an “ill-defined” metric of node and edge importance in a network. It is therefore calculated in many ways. Type ?centrality in your Console.\n\n\nStandards\n\nLet’s add a few columns to the nodes and edges based on network centrality measures:\n\nga %&gt;% \n  activate(nodes) %&gt;% \n  \n  # Node with  the most connections?\n  mutate(degree = centrality_degree(mode = c(\"in\"))) %&gt;% \n  filter(degree &gt; 0) %&gt;% \n  \n  activate(edges) %&gt;% \n  # \"Busiest\" edge?\n  mutate(betweenness = centrality_edge_betweenness())\n\n# A tbl_graph: 54 nodes and 57 edges\n#\n# An undirected simple graph with 4 components\n#\n# Edge Data: 57 × 5 (active)\n    from    to weight type         betweenness\n   &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt;\n 1     5    47      2 friends             20.3\n 2    21    47      4 benefits            44.7\n 3     5    46      1 friends             39  \n 4     5    41      1 friends             66.3\n 5    18    41      6 friends             39  \n 6    21    41     12 benefits            91.5\n 7    37    41      5 professional       164. \n 8    31    41      2 professional        98.8\n 9    20    31      3 professional        47.2\n10    17    31      4 friends            102. \n# ℹ 47 more rows\n#\n# Node Data: 54 × 8\n  name               sex   race  birthyear position  season sign   degree\n  &lt;chr&gt;              &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1 Addison Montgomery F     White      1967 Attending      1 Libra       3\n2 Adele Webber       F     Black      1949 Non-Staff      2 Leo         1\n3 Teddy Altman       F     White      1969 Attending      6 Pisces      4\n# ℹ 51 more rows\n\n\nPackages tidygraph and ggraph can be pipe-lined to perform analysis and visualization tasks in one go.\n\n##\nset_graph_style(family = \"Roboto\")\n##\nggraph(ga,layout = \"nicely\") +\n    geom_edge_link0(aes(alpha = centrality_edge_betweenness())) + \n    \n    geom_node_point(aes(colour = centrality_degree(), \n                        size = centrality_degree())) +\n    \n    geom_node_text(aes(label = name), repel = TRUE, size = 1.5) +\n    \n    scale_size(name = \"Degree\", range = c(0.5, 5)) + \n    \n    scale_color_gradient(name = \"Degree\", # SAME NAME!!\n                         low = \"blue\", high = \"red\", \n                         aesthetics = c(\"colour\", \"fill\"), \n                         guide = guide_legend(reverse = FALSE)) + \n    \n    scale_edge_alpha(name = \"Betweenness\", range = c(0.05, 1)) +\n    labs(title = \"Grey's Anatomy\", \n         subtitle = \"Nodes Scaled by Degree, Edges shaded by Betweenness\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #11\n\n\n\nHow do the Centrality Measures show up in the graph? Would you “agree” with the way we have done it? Try to modify the aesthetics by copy-pasting this chunk below and see how you can make an alternative representation.\n\n\nAnalysis and Visualizing Network Communities\nWho is close to whom? Which are the groups you can see?\n\n##\nset_graph_style(family = \"Roboto\")\n##\n# visualize communities of nodes\nga %&gt;% \n  activate(nodes) %&gt;%\n  mutate(community = as.factor(group_louvain())) %&gt;% \n  ggraph(layout = \"graphopt\") + \n  geom_edge_link0() + \n  geom_node_point(aes(color = community), size = 3) +\n  labs(title = \"Grey's Anatomy\", subtitle = \"Nodes Coloured by Community Detection Algorithm (Louvain)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #12\n\n\n\nIs the Community depiction clear? How would you do it, with which aesthetic? Copy Paste this chunk below and try.\n\n\nInteractive Graphs with visNetwork\n\nExploring the VisNetwork package. Make graphs wiggle and shake using tidy commands! The package implements interactivity using the physical metaphor of weights and springs we discussed earlier.\nThe visNetwork() function uses a nodes list and edges list to create an interactive graph. The nodes list must include an “id” column, and the edge list must have “from” and “to” columns. The function also plots the labels for the nodes, using the names of the cities from the “label” column in the node list.\nlibrary(visNetwork)\n\n# Prepare the data for plotting by visNetwork\ngrey_nodes\ngrey_edges\n# Relabel greys anatomy nodes and edges for VisNetwork\ngrey_nodes_vis &lt;- grey_nodes %&gt;% \n  rowid_to_column(var = \"id\") %&gt;% \n  rename(\"label\" = name) %&gt;% \n  mutate(sex = case_when(sex == \"F\" ~ \"Female\",\n                         sex == \"M\" ~ \"Male\")) %&gt;% \n  replace_na(., list(sex = \"Transgender?\")) %&gt;% \n  rename(\"group\" = sex)\ngrey_nodes_vis\ngrey_edges_vis &lt;- grey_edges %&gt;% \n  select(from, to) %&gt;% \n  left_join(., grey_nodes_vis, \n            by = c(\"from\" = \"label\")) %&gt;% \n  left_join(., grey_nodes_vis, \n            by = c(\"to\" = \"label\")) %&gt;%\n  select(\"from\"= id.x, \"to\" = id.y)\ngrey_edges_vis\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nUsing fontawesome icons\n\ngrey_nodes_vis %&gt;%\n\n  visNetwork(nodes = ., edges = grey_edges_vis) %&gt;% \n  visNodes(font = list(size = 40)) %&gt;% \n  \n  # Colour and icons for each of the gender-groups\n  visGroups(groupname = \"Female\", shape = \"icon\", \n            icon = list(code = \"f182\", size = 75, color = \"tomato\"),\n            shadow = list(enabled = TRUE)) %&gt;% \n  \n  visGroups(groupname = \"Male\", shape = \"icon\", \n            icon = list(code = \"f183\", size = 75, color = \"slateblue\"), \n            shadow = list(enabled = TRUE)) %&gt;% \n  \n  visGroups(groupname = \"Transgender?\", shape = \"icon\", \n            icon = list(code = \"f22c\", size = 75, color = \"fuchsia\"), \n            shadow = list(enabled = TRUE)) %&gt;% \n  \n  #visLegend() %&gt;%\n  #Add the fontawesome icons!!\n  addFontAwesome(version = \"4.7.0\") %&gt;% \n  \n  # Add Interaction Controls\n  visInteraction(navigationButtons = TRUE,\n                 hover = TRUE,\n                 selectConnectedEdges = TRUE,\n                 hoverConnectedEdges = TRUE,\n                 zoomView = TRUE)\n\n\n\n\n\nThere is another family of icons available in visNetwork, called ionicons. Let’s see how they look:\n\ngrey_nodes_vis %&gt;%\n\n  visNetwork(nodes = ., edges = grey_edges_vis,) %&gt;%\n  visLayout(randomSeed = 12345) %&gt;%\n  visNodes(font = list(size = 50)) %&gt;%\n  visEdges(color = \"green\") %&gt;%\n  visGroups(\n    groupname = \"Female\",\n    shape = \"icon\",\n    icon = list(\n      face = 'Ionicons',\n      code = \"f25d\",\n      color = \"fuchsia\",\n      size = 125\n    )\n  ) %&gt;%\n  \n  visGroups(\n    groupname = \"Male\",\n    shape = \"icon\",\n    icon = list(\n      face = 'Ionicons',\n      code = \"f202\",\n      color = \"green\",\n      size = 125\n    )\n  ) %&gt;%\n  \n  visGroups(\n    groupname = \"Transgender?\",\n    shape = \"icon\",\n    icon = list(\n      face = 'Ionicons',\n      code = \"f233\",\n      color = \"dodgerblue\",\n      size = 125\n    )\n  ) %&gt;%\n  visLegend() %&gt;%\n  addIonicons() %&gt;%\n  visInteraction(\n    navigationButtons = TRUE,\n    hover = TRUE,\n    selectConnectedEdges = TRUE,\n    hoverConnectedEdges = TRUE,\n    zoomView = TRUE\n  )\n\n\n\n\n\nSome idea of interactivity and controls with visNetwork:\n Star Wars Nodes \n Star Wars Edges \n\n# let's look again at the data\nstarwars_nodes &lt;- read_csv(\"data/star-wars-network-nodes.csv\")\nstarwars_edges &lt;- read_csv(\"data/star-wars-network-edges.csv\")\n\n# We need to rename starwars nodes dataframe and edge dataframe columns for visNetwork\nstarwars_nodes_vis &lt;- \n  starwars_nodes %&gt;% \n  rename(\"label\" = name)\n\n# Convert from and to columns to **node ids**\nstarwars_edges_vis &lt;- \n  starwars_edges %&gt;% \n  \n  # Matching Source &lt;- Source Node id (\"id.x\")\n  left_join(., starwars_nodes_vis, by = c(\"source\" = \"label\")) %&gt;% \n  \n  # Matching Target &lt;- Target Node id (\"id.y\")\n  left_join(., starwars_nodes_vis, by = c(\"target\" = \"label\")) %&gt;% \n  \n# Select \"id.x\" and \"id.y\" ONLY\n# Rename them as \"from\" and \"to\"\n# keep \"weight\" column for aesthetics of edges\n  select(\"from\" = id.x, \"to\" = id.y, \"value\" = weight)\n\n# Check everything once\nstarwars_nodes_vis\nstarwars_edges_vis\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nOk, let’s make things move and shake!!\n\nvisNetwork(nodes = starwars_nodes_vis,\n           edges = starwars_edges_vis) %&gt;% \n  visNodes(font = list(size = 30)) %&gt;% \n  visEdges(color = \"red\")\n\n\n\n\n\n\nvisNetwork(nodes = starwars_nodes_vis,\n           edges = starwars_edges_vis) %&gt;% \n  visNodes(font = list(size = 30), shape = \"icon\", \n           icon = list(code = \"f1e3\", size = 75)) %&gt;% \n  visEdges(color = list(color = \"red\", hover = \"green\", highlight = \"black\")) %&gt;% \n  visInteraction(hover = TRUE) %&gt;% \n  addFontAwesome(version = \"4.7.0\")"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#your-assignments",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#your-assignments",
    "title": "The Grammar of Networks",
    "section": "Your Assignments:",
    "text": "Your Assignments:\nMake-1 : With a readymade dataset\nStep 0. Sine qua non! Fire up a New Project! Always!\nStep 1. Fire up a new Quarto document. Fill in the YAML header.\nStep 2. Take any one of the “Make1-Datasets” datasets described below.\nStep 3. Document contents:\n\nIntroduce / Inspect in R your data and describe\nIntroduce your Purpose\nCreate graph objects\nTry different layouts\nWrite comments in the code\nWrite narrative in text with sections, bold ,italic etc.\n\nStep 4. Knit before you submit. Submit your entire project in a .zip file.\nMake1 - Datasets:\n\n\n\n\n\n\nAirline Data:\n\n\n\n Airlines Nodes \n Airlines Edges \nStart with this bit of code in your second chunk, after set up\n\n```{r}\n#| label: start up code for Airlines\n#| eval: false ## remove this!!\nairline_nodes &lt;-\n  read_csv(\"./mydatafolder/AIRLINES-NODES.csv\") %&gt;% \n  mutate(Id = Id + 1)\n\nairline_edges &lt;-\n  read_csv(\"./mydatafolder/AIRLINES-EDGES.csv\") %&gt;%\n  mutate(Source = Source + 1, Target = Target + 1)\n```\n\n\n\n\n\n\n\n\n\nThe Famous Zachary Karate Club dataset\n\n\n\n\nStart with pulling this data into your Quarto:\n\n\n```{r}\n#| eval: false ## remove this!\ndata(\"karate\",package= \"igraphdata\")\nkarate\n```\n\n\nTry ?karate in the console\n\nNote that this is not a set of nodes, nor edges, but already a graph-object!\n\nSo no need to create a graph object using tbl_graph.\n\nYou will need to just go ahead and plot using ggraph.\n\n\n\n\n\n\n\n\n\nGame of Thrones:\n\n\n\n GoT Networks \n\nStart with pulling this data into your Rmarkdown:\n\n\n```{r}\n#| label: start-up code for GoT\n#| eval: false ## remove this!!\n\nGoT &lt;- read_rds(\"data/GoT.RDS\")\n```\n\n\nNote that this is a list of 7 graphs from Game of Thrones.\nSelect one using GoT[[index]] where index = 1…7 and then plot directly.\nTry to access the nodes and edges and modify them using any attribute data\n\n\n\n\n\n\n\n\n\nOther Datasets\n\n\n\n\nChoose any other graph dataset from igraphdata\n\n(type ?igraphdata in console)\n\nAsk me for help if you need any\n\n\n\n\nMake-2: Literary Network with TV Show / Book / Story / Play\nYou need to create a Network Graph for your favourite Book, play, TV serial or Show. (E.g. Friends, BBT, or LB or HIMYM, B99, TGP, JTV…or Hamlet, Little Women , Pride and Prejudice, or LoTR)\n\nStep 1. Go to: Literary Networks for instructions.\n\nStep 2. Make your data using the instructions.\n\nIn the nodes excel, use id and names as your columns. Any other details in other columns to the right.\n\nIn your edges excel, use from and to as your first columns.\n\nEntries in these columns can be names or ids but be consistent and don’t mix.\n\n\n\nStep 3. Decide on 3 answers that you to seek and plan to make graphs for.\nStep 4. Create graph objects. Say 3 visualizations.\nStep 5. Write comments/answers in the code and narrative text. Add pictures from the web using Markdown syntax.\nStep 6. Write Reflection ( ok, a short one!) inside your Quarto document. Make sure it renders !!\nStep 7. Group Submission: Submit the render-able .qmd file AND the data. Quarto Markdown with joint authorship. Each person submits on their Assignments. All get the same grade on this one.\n\nAsk me for clarifications on what to do after you have read the Instructions in your group."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#references",
    "title": "The Grammar of Networks",
    "section": "\n References",
    "text": "References\n\n\nHadley Wickham, Danielle Navarro, and Thomas Lin Pedersen, ggplot2: Elegant Graphics for Data Analysis. https://ggplot2-book.org/networks\n\nOmar Lizardo and Isaac Jilbert, Social Networks: An Introduction. https://bookdown.org/omarlizardo/_main/\n\nMark Hoffman, Methods for Network Analysis. https://bookdown.org/markhoff/social_network_analysis/\n\n\nStatistical Analysis of Network Data with R, 2nd Edition.https://github.com/kolaczyk/sand\n\n\nThomas Lin Pedersen - 1 giraffe, 2 giraffe,GO!\n\nTyner, Sam, François Briatte, and Heike Hofmann. 2017. “Network Visualization with ggplot2.” The R Journal 9 (1): 27–59. https://journal.r-project.org/archive/2017/RJ-2017-023/index.html\n\nNetwork Datasets https://icon.colorado.edu/#!/networks\n\nYunran Chen, Introduction to Network Analysis Using R\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggraph\n2.2.1\nPedersen (2024a)\n\n\nggtext\n0.1.2\nWilke and Wiernik (2022)\n\n\ngraphlayouts\n1.1.1\nDavid Schoch (2023)\n\n\nigraph\n2.0.3\n\nCsardi and Nepusz (2006); Csárdi et al. (2024)\n\n\n\nigraphdata\n1.0.1\nCsardi (2015)\n\n\nsand\n2.0.0\nKolaczyk and Csárdi (2020)\n\n\nshowtext\n0.9.7\nQiu and See file AUTHORS for details. (2024)\n\n\ntidygraph\n1.3.1\nPedersen (2024b)\n\n\nvisNetwork\n2.1.2\nAlmende B.V. and Contributors and Thieurmel (2022)\n\n\n\n\n\n\nAlmende B.V. and Contributors, and Benoit Thieurmel. 2022. visNetwork: Network Visualization Using “vis.js” Library. https://CRAN.R-project.org/package=visNetwork.\n\n\nCsardi, Gabor. 2015. igraphdata: A Collection of Network Data Sets for the “igraph” Package. https://CRAN.R-project.org/package=igraphdata.\n\n\nCsardi, Gabor, and Tamas Nepusz. 2006. “The Igraph Software Package for Complex Network Research.” InterJournal Complex Systems: 1695. https://igraph.org.\n\n\nCsárdi, Gábor, Tamás Nepusz, Vincent Traag, Szabolcs Horvát, Fabio Zanini, Daniel Noom, and Kirill Müller. 2024. igraph: Network Analysis and Visualization in r. https://doi.org/10.5281/zenodo.7682609.\n\n\nDavid Schoch. 2023. “graphlayouts: Layout Algorithms for Network Visualizations in r.” Journal of Open Source Software 8 (84): 5238. https://doi.org/10.21105/joss.05238.\n\n\nKolaczyk, Eric, and Gábor Csárdi. 2020. sand: Statistical Analysis of Network Data with r, 2nd Edition. https://CRAN.R-project.org/package=sand.\n\n\nPedersen, Thomas Lin. 2024a. ggraph: An Implementation of Grammar of Graphics for Graphs and Networks. https://CRAN.R-project.org/package=ggraph.\n\n\n———. 2024b. tidygraph: A Tidy API for Graph Manipulation. https://CRAN.R-project.org/package=tidygraph.\n\n\nQiu, Yixuan, and authors/contributors of the included software. See file AUTHORS for details. 2024. showtext: Using Fonts More Easily in r Graphs. https://CRAN.R-project.org/package=showtext.\n\n\nWilke, Claus O., and Brenton M. Wiernik. 2022. ggtext: Improved Text Rendering Support for “ggplot2”. https://CRAN.R-project.org/package=ggtext."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/files/evolutions.html",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/files/evolutions.html",
    "title": "Tutorial on Evolutions and Flow",
    "section": "",
    "text": "Tutorial Content to be written up when Arvind has time !!!\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{venkatadri,\n  author = {Venkatadri, Arvind},\n  title = {Tutorial on {Evolutions} and {Flow}},\n  url = {https://av-quarto.netlify.app/content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/files/evolutions.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nVenkatadri, Arvind. n.d. “Tutorial on Evolutions and Flow.”\nhttps://av-quarto.netlify.app/content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/files/evolutions.html."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/90-Space/spatial.html",
    "href": "content/courses/Analytics/Descriptive/Modules/90-Space/spatial.html",
    "title": "🗺 Visualising Spatial Data",
    "section": "",
    "text": "Spatial Data \n\n Static Maps \n\n Interactive Maps",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🗺 Visualising Spatial Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/90-Space/spatial.html#slides-and-tutorials",
    "href": "content/courses/Analytics/Descriptive/Modules/90-Space/spatial.html#slides-and-tutorials",
    "title": "🗺 Visualising Spatial Data",
    "section": "",
    "text": "Spatial Data \n\n Static Maps \n\n Interactive Maps",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🗺 Visualising Spatial Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/90-Space/spatial.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/90-Space/spatial.html#setting-up-r-packages",
    "title": "🗺 Visualising Spatial Data",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nknitr::opts_chunk$set(echo = TRUE)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(osmdata)\nlibrary(rnaturalearth)",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🗺 Visualising Spatial Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/90-Space/spatial.html#introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/90-Space/spatial.html#introduction",
    "title": "🗺 Visualising Spatial Data",
    "section": "\n Introduction",
    "text": "Introduction\nFirst; let us watch a short, noisy video on maps:",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🗺 Visualising Spatial Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/90-Space/spatial.html#what-kind-of-visualizations-will-we-make",
    "href": "content/courses/Analytics/Descriptive/Modules/90-Space/spatial.html#what-kind-of-visualizations-will-we-make",
    "title": "🗺 Visualising Spatial Data",
    "section": "\n What kind of visualizations will we make?",
    "text": "What kind of visualizations will we make?\nWe will first understand the structure of spatial data and where to find it. For now, we will deal with vector spatial data; the discussion on raster data will be dealt with in another future module.\nWe will get hands-on with making maps, both static and interactive.\n\n Choropleth Map\nWhat does choropleth1 mean? And what kind of information could this map represent?\n\n\n\n Bubble Map\nWhat information could this map below represent?\n\n\nLet us now look at the slides. Then we will understand how the R packages sf, tmap work to create maps, using data downloadable into R using osmdata and osmplotr. We will also make interactive maps with leaflet and mapview; tmap is also capable of creating interactive maps.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🗺 Visualising Spatial Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/90-Space/spatial.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/90-Space/spatial.html#your-turn",
    "title": "🗺 Visualising Spatial Data",
    "section": "\n Your Turn",
    "text": "Your Turn\n\n Animal and Bird Migration\n\nHead off to movebank.org. Look at a few species of interest and choose one.\nDownload the data ( ESRI Shapefile). Note: You will get a .zip file with a good many files in it. Save all of them, but read only the .shp file into R.\nImport that into R using sf_read()\n\nSee how you can plot locations, tracks and colour by species….based on the data you download.\nFor tutorial info: https://movebankworkshopraleighnc.netlify.app/\n\n\n UFO Sightings\nHere is a UFO Sighting dataset, containing location and text descriptions. https://github.com/planetsig/ufo-reports/blob/master/csv-data/ufo-scrubbed-geocoded-time-standardized.csv\n\n Sales Data from kaggle\nHead off to Kaggle and search for Geographical Sales related data. Make both static and interactive maps with this data. Justify your decisions for type of map.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🗺 Visualising Spatial Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/90-Space/spatial.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/90-Space/spatial.html#references",
    "title": "🗺 Visualising Spatial Data",
    "section": "\n References",
    "text": "References\n\nHadley Wickham, Danielle Navarro and Thomas Lin Pedersen. ggplot2: Elegant Graphics for Data Analysis, https://ggplot2-book.org/maps.html\nRobin Lovelace, Jakub Nowosad, Jannes Muenchow. Geocomputation with R, https://r.geocompx.org/\nEmine Fidan. Guide to Creating Interactive Maps in R, https://bookdown.org/eneminef/DRR_Bookdown/\nNikita Voevodin. R, Not the Best Practices, https://bookdown.org/voevodin_nv/R_Not_the_Best_Practices/maps.html\nWant to make a cute logo-like map? Try https://prettymapp.streamlit.app\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nosmdata\n0.2.5\nMark Padgham et al. (2017)\n\n\nrnaturalearth\n1.0.1\nMassicotte and South (2023)\n\n\nsf\n1.0.16\n\nPebesma (2018); Pebesma and Bivand (2023)\n\n\n\ntmap\n3.3.4\nTennekes (2018)\n\n\n\n\n\n\nMark Padgham, Bob Rudis, Robin Lovelace, and Maëlle Salmon. 2017. “Osmdata.” Journal of Open Source Software 2 (14): 305. https://doi.org/10.21105/joss.00305.\n\n\nMassicotte, Philippe, and Andy South. 2023. rnaturalearth: World Map Data from Natural Earth. https://CRAN.R-project.org/package=rnaturalearth.\n\n\nPebesma, Edzer. 2018. “Simple Features for R: Standardized Support for Spatial Vector Data.” The R Journal 10 (1): 439–46. https://doi.org/10.32614/RJ-2018-009.\n\n\nPebesma, Edzer, and Roger Bivand. 2023. Spatial Data Science: With applications in R. Chapman and Hall/CRC. https://doi.org/10.1201/9780429459016.\n\n\nTennekes, Martijn. 2018. “tmap: Thematic Maps in R.” Journal of Statistical Software 84 (6): 1–39. https://doi.org/10.18637/jss.v084.i06.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🗺 Visualising Spatial Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/90-Space/spatial.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/90-Space/spatial.html#footnotes",
    "title": "🗺 Visualising Spatial Data",
    "section": "Footnotes",
    "text": "Footnotes\n\nEtymology. From Ancient Greek χώρα (khṓra, “location”) + πλῆθος (plêthos, “a great number”) + English map. First proposed in 1938 by American geographer John Kirtland Wright to mean “quantity in area,” although maps of the type have been used since the early 19th century.↩︎",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🗺 Visualising Spatial Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions-interactive.html",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions-interactive.html",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "",
    "text": "# options(tibble.print_min = 4L, tibble.print_max = 4L,digits = 3)\nlibrary(tidyverse)\nlibrary(mosaic) # package for stats, simulations, and basic plots\nlibrary(mosaicData) # package containing datasets\nlibrary(ggformula) # package for professional looking plots, that use the formula interface from mosaic\nlibrary(skimr) # Summary statistics about variables in data frames\nlibrary(NHANES) # survey data collected by the US National Center for Health Statistics (NCHS)\n\nlibrary(echarts4r) # Interactive graphs using Javascript in R\nlibrary(plotly) # An older more established package for interactive graphs using Javascript in R\n\n\nggplot2::theme_set(new = theme_classic())"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions-interactive.html#setup-the-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions-interactive.html#setup-the-packages",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "",
    "text": "# options(tibble.print_min = 4L, tibble.print_max = 4L,digits = 3)\nlibrary(tidyverse)\nlibrary(mosaic) # package for stats, simulations, and basic plots\nlibrary(mosaicData) # package containing datasets\nlibrary(ggformula) # package for professional looking plots, that use the formula interface from mosaic\nlibrary(skimr) # Summary statistics about variables in data frames\nlibrary(NHANES) # survey data collected by the US National Center for Health Statistics (NCHS)\n\nlibrary(echarts4r) # Interactive graphs using Javascript in R\nlibrary(plotly) # An older more established package for interactive graphs using Javascript in R\n\n\nggplot2::theme_set(new = theme_classic())"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions-interactive.html#introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions-interactive.html#introduction",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Introduction",
    "text": "Introduction\nWe will query our dataset, developing insights and new questions as each Table or Bar/Histogram chart yields new information. This process of exploration is iterative, structured, and intuitive. Intermediate results may on occasion be messy or not very insightful!\nWe will consistently use the Project Mosaic ecosystem of packages in R (mosaic, mosaicData and ggformula).\n\n\n\n\n\n\nFormula Interface\n\n\n\nNote the standard method for all commands from the mosaic package:goal( y ~ x | z, data = mydata, …) With ggformula, one can create any graph/chart using:gf_geometry(y ~ x | z, data = mydata)\nORmydata %&gt;% gf_geometry( y ~ x | z)\nThe second method may be preferable, especially if you have done some data manipulation first! More later! ggformula supports many types of plots (using geometry), such as scatter, bar, histogram, density, boxplots, maps and many other statistical plots.\n\n\n\n\n\n\n\n\nInteractive Graphs with echarts4r\n\n\n\nWe will also start using echarts4r side by side for interactive graphs.\n\nEvery function in the package starts with e_.\nYou start coding a visualization by creating an echarts object with the e_charts() function. That takes your data frame and x-axis column as arguments.\nNext, you add a function for the type of chart (e_line(), e_bar(), etc.) with the y-axis series column name as an argument.\nThe rest is mostly customization! echarts4r takes some effort in getting used to, but it totally worth it!\n\n\n\nThe website for echarts4r is https://echarts4r.john-coene.com/articles/get_started.html. You should also quickly view this short introductory video on echarts4r:"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions-interactive.html#case-study-1-galton-dataset-from-mosaicdata",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions-interactive.html#case-study-1-galton-dataset-from-mosaicdata",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Case Study-1: Galton Dataset from mosaicData\n",
    "text": "Case Study-1: Galton Dataset from mosaicData\n\nLet us choose the famous Galton dataset:\n\ndata(\"Galton\")\nGalton &lt;- as_tibble(Galton)\n\n\n Look at the Data:\n\nskim(Galton)\n\n\nData summary\n\n\nName\nGalton\n\n\nNumber of rows\n898\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nfamily\n0\n1\nFALSE\n197\n185: 15, 166: 11, 66: 11, 130: 10\n\n\nsex\n0\n1\nFALSE\n2\nM: 465, F: 433\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nfather\n0\n1\n69.23\n2.47\n62\n68\n69.0\n71.0\n78.5\n▁▅▇▂▁\n\n\nmother\n0\n1\n64.08\n2.31\n58\n63\n64.0\n65.5\n70.5\n▂▅▇▃▁\n\n\nheight\n0\n1\n66.76\n3.58\n56\n64\n66.5\n69.7\n79.0\n▁▇▇▅▁\n\n\nnkids\n0\n1\n6.14\n2.69\n1\n4\n6.0\n8.0\n15.0\n▃▇▆▂▁\n\n\n\n\n\nWhat can we say about the dataset and its variables? How big is the dataset? How many variables? What types are they, Quant or Qual? What are the means, medians and inter-quartile ranges for the Quant variables? If they are Qual, what are the levels? Are they ordered levels?\nThere is a lot of Description generated by the skimr::skim command (and equivalently by the mosaic::inspect() command)! Try both and see which output suits you. The first table above describes the Qual variables: family and sex. The second table describes the Quant variables, and gives us their statistical summaries as well and a neat little histogram to boot. The data are described as: Type help(Galton) in your Console\n\nA data frame with 898 observations on the following variables.\n\n\nfamily an ID for each family, a factor with levels for each family\n\nfather the father’s height (in inches)\n\nmother the mother’s height (in inches)\n\nsex the child’s sex: F or M\n\nheight the child’s height as an adult (in inches)\n\nnkids the number of adult children in the family, or, at least, the number whose heights Galton recorded.\n\n\n\n Counts, and Charts with Counts\nNow that we know the variables, let us look at counts of data observations(rows). We know from our examination of variable types that counting of observations must be done on the basis of Qualitative variables. So let us count and plot the counts in bar charts.\n\n\n\n\n\n\nQuestion\n\n\n\nQ.1 How many families in the data for each value of nkids(i.e. Count of families by size)?\n\n\n\n\nComputations\nUsing ggformula\nUsing echarts4r\nUsing plotly\n\n\n\n\nGalton_counts &lt;- Galton %&gt;%\n  group_by(nkids) %&gt;% \n  summarise(children = n()) %&gt;% \n  # just to check\n  mutate(\n    No_of_families = as.integer(children/nkids),\n    # Why do we divide\n    \n    running_count_of_children = cumsum(children),\n    running_count_of_families = cumsum(No_of_families)) \nGalton_counts\n\n\n  \n\n\n\n\n\n\nGalton_counts %&gt;% \n  gf_col(No_of_families ~ nkids) %&gt;% \n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\n\n\nGalton_counts %&gt;% \n  e_charts(nkids) %&gt;% \n  e_bar(No_of_families,\n        colorBy = \"data\",\n        legend = FALSE) %&gt;% # Or \"series\"\n  \n# https://echarts4r.john-coene.com/articles/grid.html\n# echarts4r does not \"automatically\" name the axes!\n# And look at the \"categorical\" x-axis below!\n\n  e_x_axis(name = \"Family Size\", nameLocation = \"center\", \n           nameGap = 25, type = \"category\") %&gt;% \n  e_y_axis(name = \"Count\", nameLocation = \"center\", nameGap = 25,) %&gt;% \n  \n  e_tooltip(trigger = \"item\") %&gt;% \n  e_title(\"No of Families of each size\")\n\n\n\n\n\n\n\n\nGalton_counts %&gt;% \n  plot_ly(x = ~ nkids, y = ~ No_of_families) %&gt;% \n  add_bars()\n\n\n\n\n\n\n\n\nInsight: There are 32 1-kid families; and \\(128/8 = 16\\) 8-kid families! There is one great great 15-kid family. (Did you get the idea behind why we divide here?)\n\n\n\n\n\n\nQuestion\n\n\n\nQ.2. What is the count of Children by sex of the child and by family size nkids?\n\n\n\n\nUsing ggformula\nUsing echarts4r\n\n\n\n\nGalton_counts_by_sex &lt;- Galton %&gt;% \n  mutate(family = as.integer(family)) %&gt;% \n  group_by(nkids, sex) %&gt;% \n  summarise(count_by_sex = n()) %&gt;% \n  ungroup() %&gt;% \n  group_by(sex)\nGalton_counts_by_sex %&gt;%\n  gf_col(count_by_sex ~ nkids | sex, fill = ~ sex, data = .)\n\n\n\n\n\n\n\n\n\n\nGalton_counts_by_sex &lt;- Galton %&gt;% \n  mutate(family = as.integer(family)) %&gt;% \n  group_by(nkids, sex) %&gt;% \n  summarise(count_by_sex = n()) %&gt;% \n  ungroup() %&gt;% \n  group_by(sex) \nGalton_counts_by_sex\n\n\n  \n\n\nGalton_counts_by_sex%&gt;% \n  e_charts(nkids) %&gt;% \n  e_bar(count_by_sex) %&gt;% \n\n  e_x_axis(name = \"Family Size (nkids)\", nameLocation = \"center\",\n           nameGap = 20, type = \"category\") %&gt;%\n  e_y_axis(name = \"How Many Children?\",\n           nameGap = 20,\n           nameTextStyle = list(align = \"center\"),\n           nameLocation = \"center\") %&gt;%\n  e_legend(right = 25, orient = \"vertical\") %&gt;% \n  e_facet(cols = 2,rows = 1) %&gt;% \n  e_tooltip(trigger = \"item\") %&gt;% \n  e_title(\"Child Counts by Sex over Family Size\")\n\n\n\n\n\n\n\n\nInsight: Hmm…decent gender balance overall, across family sizes nkids.\n\n\n\n\n\n\nFollow-up Question\n\n\n\nFollow up Question: How would we look for “gender balance” in individual families? Should we look at the family column ?\n\n\n\nGalton %&gt;% \n  mutate(family = as.integer(family)) %&gt;% \n  group_by(family, sex) %&gt;% \n  summarise(count_by_sex = n()) %&gt;% \n  ungroup() %&gt;% \n  group_by(sex) %&gt;% \n  \n  e_charts(family) %&gt;% \n  e_bar(count_by_sex) %&gt;% \n  \n  e_x_axis(name = \"nkids\", nameLocation = \"center\",\n           nameGap = 25, type = \"category\") %&gt;% \n  e_y_axis(name = \"How Many Children?\", \n           nameGap = 25, nameLocation = \"center\") %&gt;% \n  e_legend(right = 5) %&gt;% \n  e_facet(cols = 2,rows = 1) %&gt;% \n  e_tooltip(trigger = \"item\") %&gt;% \n  e_title(\"Child Counts by Sex over Family ID\")\n\n\n\n\n\nInsight: The No of Children were distributed similarly across family sizenkids… However, this plot is too crowded and does not lead to any great insight. Using family ID was silly to plot against, wasn’t it? Not all exploratory plots will be “necessary” in the end. But they are part of the journey of getting better acquainted with the data!\n\n {{}} Stat Summaries and Distributions\nOK, on to the Quantitative variables now! What Questions might we have, that could relate not to counts by Qual variables, but to the numbers in Quant variables. Stat measures, like their ranges, max and min? Means, medians, distributions? And how these vary on the basis of Qual variables? All this using histograms and densities.\n\n\n\n\n\n\nSummary Stats\n\n\n\nAs Stigler(Stigler 2016) said, summaries are the first thing to look at in data. skimr::skim has already given us a lot summary data for Quant variables. We can now use mosaic::favstats to develop these further, by slicing / facetting these wrt other Qual variables. Let us tabulate some quick stat summaries of the important variables in Galton.\n\n\n\n# summaries facetted by sex of child\nmeasures &lt;- favstats(~ height | sex, data = Galton)\nmeasures\n\n\n  \n\n\n\nInsight: We saw earlier that the mean height of the Children was 66 inches. However, are Sons taller than Daughters? Difference in mean height is 5 inches! AND…that was the same difference between fathers and mothers mean heights! Is it so simple then?\n\n\n\n\n\n\nQuestion\n\n\n\nQ.4 How are the heights of the children distributed? Here is where we need a e_histogram…\n\n\n\nGalton %&gt;% \n  e_charts() %&gt;% \n  e_histogram(serie = height) %&gt;% \n  e_tooltip(trigger = \"item\") %&gt;% \n  \n  e_mark_line(data = list(xAxis = mean(Galton$height)),\n              label = list(label = \"Mean Height\",\n                           label.position = \"end\"),\n              lineStyle = list(color = \"red\", width = 1.5, \n                               type = \"solid\")) %&gt;% \n# See https://echarts.apache.org/en/option.html#series-line.markLine\n\n  e_x_axis(name = \"Height\", nameLocation = \"center\") %&gt;% \n  e_y_axis(name = \"Counts\", nameLocation = \"center\", nameGap = 30) %&gt;% \n  e_title(\"Distribution of Heights in Galton\")\n\n\n\n\n\nInsight: Fairly symmetric distribution…but there are a few very short and some very tall children! Try to change the no. of bins to check of we are missing some pattern. This is not completely easy with echarts4r which uses the “Sturges” algorithm to set the number of bins. Need to figure this out from the echarts Apache API docs.\n\n\n\n\n\n\nQuestion\n\n\n\nQ.5 Is there a difference in height distributions between Male and Female children?(Quant variable sliced by Qual variable)\n\n\nWe will use the raw Galton data and previously-computed measures:\n\nGalton %&gt;%\n  group_by(sex) %&gt;% \n  e_charts(height = 300) %&gt;% \n  e_density(height) %&gt;% \n  e_mark_line(data = list(xAxis = measures %&gt;% filter(sex == \"M\") %&gt;% \n                            select(mean) %&gt;% as.numeric()),\n  # This code colours both v-lines red...how?\n              lineStyle = list(color = \"red\", width = 1.5, \n                               type = \"solid\")) %&gt;% \n  # Upto here gives one line in red colour, correctly\n\n  e_mark_line(data = list(xAxis = measures %&gt;% \n                            filter(sex == \"F\") %&gt;%\n                            select(mean) %&gt;% as.numeric()),\n              \n  # This piece of code has no effect...wonder why not?\n  # BOTH lines are in red ...why??\n              lineStyle = list(color = \"black\", width = 1.5, \n                               type = \"solid\")) %&gt;% \n  e_title(\"Distributions of Height by Sex in Galton\") %&gt;% \n  e_x_axis(name = \"Height\", nameLocation = \"center\") %&gt;% \n  e_legend(right = 5)\n\n\n\n\n\nInsight: There is a visible difference in average heights between girls and boys. Is that significant, however? We will need a statistical inference test to figure that out!! Claus Wilke1 says comparisons of Quant variables across groups are best made between densities and not histograms…\n\n\n\n\n\n\nQuestion\n\n\n\nQ.6 Are Mothers generally shorter than fathers?\n\n\n\nGalton %&gt;% \n  e_charts(height = 300) %&gt;% \n  e_density(father) %&gt;% \n  e_density(mother) %&gt;% \n  e_mark_line(data = list(xAxis = mean(Galton$mother)),\n              lineStyle = list(color = \"red\", width = 1.5, \n                               type = \"solid\")) %&gt;% \n  e_mark_line(data = list(xAxis = mean(Galton$father),\n              lineStyle = list(color = \"black\", width = 1.5, \n                               type = \"solid\"))) %&gt;% \n  e_legend(right = 10)\n\n\n\n\n\nInsight: Yes, moms are on average shorter than dads in this dataset. Again, is this difference statistically significant? We will find out in when we do Inference.\n\n\n\n\n\n\nQuestion\n\n\n\nQ.7a. Are heights of children different based on the number of kids in the family? And For Male and Female children?\n\n\n\nGalton %&gt;%\n  group_by(nkids) %&gt;% \n  e_charts(height = 400) %&gt;% \n  e_boxplot(height,colorBy = \"data\",\n            itemStyle = list(borderWidth = 3)) %&gt;% \n  e_y_axis(max = 80, min = 50, name = \"height\", nameLocation = \"center\",\n           nameGap = 25, margin = 5) %&gt;% # adds +/- 5 to y-axis limits\n  \n e_x_axis(name = \"Family Size\", \n           nameLocation = \"center\", \n           nameGap = 25, type = \"category\") %&gt;% # makes a category axis showing factors\n\n  e_tooltip() %&gt;% \n  e_title(\"Heights over Family Size\")\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.7b. Are heights of children different for Male and Female children?\n\n\n\n# Can do better at colouring/filling and facetting...\nGalton %&gt;%\n  group_by(nkids,sex) %&gt;% \n  e_charts(height = 400) %&gt;% # no x-variable needed for boxplots\n  e_boxplot(height,colorBy = \"data\",\n            itemStyle = list(borderWidth = 3)) %&gt;% \n  e_y_axis(max = 80, min = 50, name = \"height\", nameLocation = \"center\",\n           nameGap = 25, margin = 5) %&gt;% # adds +/- 5 to y-axis limits\n  \n  e_x_axis(name = \"Family Size\", \n           nameLocation = \"center\", \n           nameGap = 25, type = \"category\") %&gt;% # makes a category axis showing factors\n\n  e_tooltip() %&gt;% \n  e_title(\"Heights by Sex over Family Size\")\n\n\n\n\n\nInsight: So, at all family “strengths”, the male children are taller than the female children. Box plots are used to show distributions of numeric data values and compare them between multiple groups (i.e Categorical Data, here sex and nkids).\n\n\n\n\n\n\nQuestion\n\n\n\nQ.8 Does the mean height of children in a family vary with the number of children in the family? (family size)?\n\n\n\nGalton %&gt;% \n  group_by(nkids) %&gt;% \n  summarise(mean_height = mean(height)) %&gt;% \n  e_charts(nkids,height = 300) %&gt;% \n  e_bar(mean_height,colorBy = \"data\", legend = FALSE) %&gt;% \n  e_x_axis(name = \"nkids\",nameLocation = \"center\", nameGap = 25,\n           type = \"category\") %&gt;% \n  e_y_axis(name = \"mean height\", nameLocation = \"center\", nameGap = 25) %&gt;% \n  e_tooltip(trigger = \"item\")\n\n\n\n\n\nInsight: Hmm…The graph shows that mean heights do not vary much with family size nkids. We saw this with the box plots earlier. This would be useful information in a Modelling and Prediction exercise.\n\n\n\n\n\n\nFollow-up Question\n\n\n\nQ. 8a. Is height difference between sons and daughters related to height difference between father and mother?\nDifferences between father and mother heights influencing height…this would be like height ~ (father-mother). This would be a relationship between two Quant variables. A histogram would not serve here and we plot this as a Scatter Plot:\n\n\n\nGalton %&gt;% \n  group_by(family,sex) %&gt;% \n  \n  # Parental Height Difference\n  mutate(diff_height = father - mother) %&gt;% \n  select(family, sex, height, diff_height) %&gt;% \n  ungroup() %&gt;% \n  group_by(sex) %&gt;% \n\n  e_charts(diff_height, height = 300) %&gt;% \n  e_scatter(height, symbol_size = 8) %&gt;%\n  \n  # Fit a trend line\n  e_lm(height ~ diff_height,\n       name = c(\"Female\", \"Male\")) %&gt;% \n  e_x_axis(max = 18, min = -5,\n           name = \"Father - Mother Height\", \n           nameLocation = \"center\", nameGap = 25) %&gt;% \n  e_y_axis(max = 80, min = 50,  \n           name = \"Children's Heights\", \n           nameLocation = \"center\", nameGap = 25) %&gt;% \n  e_tooltip(axisPointer = list(type = \"cross\"))\n\n\n\n\n\nInsight: There seems no relationship, or a very small one, between children’s heights on the y-axis and the difference in parental height differences on the x-axis…\nAnd so on…..we can proceed from simple visualizations based on Questions to larger questions that demand inference and modelling. We hinted briefly on these in the above Case Study."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions-interactive.html#case-study-2-dataset-from-nhanes",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions-interactive.html#case-study-2-dataset-from-nhanes",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Case Study-2: Dataset from NHANES\n",
    "text": "Case Study-2: Dataset from NHANES\n\nLet us try the NHANES dataset. Try help(NHANES) in your Console.\n\ndata(\"NHANES\")\n\n\n Look at the Data\n\nskim(NHANES)\n\n\nData summary\n\n\nName\nNHANES\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n76\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n31\n\n\nnumeric\n45\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nSurveyYr\n0\n1.00\nFALSE\n2\n200: 5000, 201: 5000\n\n\nGender\n0\n1.00\nFALSE\n2\nfem: 5020, mal: 4980\n\n\nAgeDecade\n333\n0.97\nFALSE\n8\n40: 1398, 0-: 1391, 10: 1374, 20: 1356\n\n\nRace1\n0\n1.00\nFALSE\n5\nWhi: 6372, Bla: 1197, Mex: 1015, Oth: 806\n\n\nRace3\n5000\n0.50\nFALSE\n6\nWhi: 3135, Bla: 589, Mex: 480, His: 350\n\n\nEducation\n2779\n0.72\nFALSE\n5\nSom: 2267, Col: 2098, Hig: 1517, 9 -: 888\n\n\nMaritalStatus\n2769\n0.72\nFALSE\n6\nMar: 3945, Nev: 1380, Div: 707, Liv: 560\n\n\nHHIncome\n811\n0.92\nFALSE\n12\nmor: 2220, 750: 1084, 250: 958, 350: 863\n\n\nHomeOwn\n63\n0.99\nFALSE\n3\nOwn: 6425, Ren: 3287, Oth: 225\n\n\nWork\n2229\n0.78\nFALSE\n3\nWor: 4613, Not: 2847, Loo: 311\n\n\nBMICatUnder20yrs\n8726\n0.13\nFALSE\n4\nNor: 805, Obe: 221, Ove: 193, Und: 55\n\n\nBMI_WHO\n397\n0.96\nFALSE\n4\n18.: 2911, 30.: 2751, 25.: 2664, 12.: 1277\n\n\nDiabetes\n142\n0.99\nFALSE\n2\nNo: 9098, Yes: 760\n\n\nHealthGen\n2461\n0.75\nFALSE\n5\nGoo: 2956, Vgo: 2508, Fai: 1010, Exc: 878\n\n\nLittleInterest\n3333\n0.67\nFALSE\n3\nNon: 5103, Sev: 1130, Mos: 434\n\n\nDepressed\n3327\n0.67\nFALSE\n3\nNon: 5246, Sev: 1009, Mos: 418\n\n\nSleepTrouble\n2228\n0.78\nFALSE\n2\nNo: 5799, Yes: 1973\n\n\nPhysActive\n1674\n0.83\nFALSE\n2\nYes: 4649, No: 3677\n\n\nTVHrsDay\n5141\n0.49\nFALSE\n7\n2_h: 1275, 1_h: 884, 3_h: 836, 0_t: 638\n\n\nCompHrsDay\n5137\n0.49\nFALSE\n7\n0_t: 1409, 0_h: 1073, 1_h: 1030, 2_h: 589\n\n\nAlcohol12PlusYr\n3420\n0.66\nFALSE\n2\nYes: 5212, No: 1368\n\n\nSmokeNow\n6789\n0.32\nFALSE\n2\nNo: 1745, Yes: 1466\n\n\nSmoke100\n2765\n0.72\nFALSE\n2\nNo: 4024, Yes: 3211\n\n\nSmoke100n\n2765\n0.72\nFALSE\n2\nNon: 4024, Smo: 3211\n\n\nMarijuana\n5059\n0.49\nFALSE\n2\nYes: 2892, No: 2049\n\n\nRegularMarij\n5059\n0.49\nFALSE\n2\nNo: 3575, Yes: 1366\n\n\nHardDrugs\n4235\n0.58\nFALSE\n2\nNo: 4700, Yes: 1065\n\n\nSexEver\n4233\n0.58\nFALSE\n2\nYes: 5544, No: 223\n\n\nSameSex\n4232\n0.58\nFALSE\n2\nNo: 5353, Yes: 415\n\n\nSexOrientation\n5158\n0.48\nFALSE\n3\nHet: 4638, Bis: 119, Hom: 85\n\n\nPregnantNow\n8304\n0.17\nFALSE\n3\nNo: 1573, Yes: 72, Unk: 51\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nID\n0\n1.00\n61944.64\n5871.17\n51624.00\n56904.50\n62159.50\n67039.00\n71915.00\n▇▇▇▇▇\n\n\nAge\n0\n1.00\n36.74\n22.40\n0.00\n17.00\n36.00\n54.00\n80.00\n▇▇▇▆▅\n\n\nAgeMonths\n5038\n0.50\n420.12\n259.04\n0.00\n199.00\n418.00\n624.00\n959.00\n▇▇▇▆▃\n\n\nHHIncomeMid\n811\n0.92\n57206.17\n33020.28\n2500.00\n30000.00\n50000.00\n87500.00\n100000.00\n▃▆▃▁▇\n\n\nPoverty\n726\n0.93\n2.80\n1.68\n0.00\n1.24\n2.70\n4.71\n5.00\n▅▅▃▃▇\n\n\nHomeRooms\n69\n0.99\n6.25\n2.28\n1.00\n5.00\n6.00\n8.00\n13.00\n▂▆▇▂▁\n\n\nWeight\n78\n0.99\n70.98\n29.13\n2.80\n56.10\n72.70\n88.90\n230.70\n▂▇▂▁▁\n\n\nLength\n9457\n0.05\n85.02\n13.71\n47.10\n75.70\n87.00\n96.10\n112.20\n▁▃▆▇▃\n\n\nHeadCirc\n9912\n0.01\n41.18\n2.31\n34.20\n39.58\n41.45\n42.92\n45.40\n▁▂▇▇▅\n\n\nHeight\n353\n0.96\n161.88\n20.19\n83.60\n156.80\n166.00\n174.50\n200.40\n▁▁▁▇▂\n\n\nBMI\n366\n0.96\n26.66\n7.38\n12.88\n21.58\n25.98\n30.89\n81.25\n▇▆▁▁▁\n\n\nPulse\n1437\n0.86\n73.56\n12.16\n40.00\n64.00\n72.00\n82.00\n136.00\n▂▇▃▁▁\n\n\nBPSysAve\n1449\n0.86\n118.15\n17.25\n76.00\n106.00\n116.00\n127.00\n226.00\n▃▇▂▁▁\n\n\nBPDiaAve\n1449\n0.86\n67.48\n14.35\n0.00\n61.00\n69.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nBPSys1\n1763\n0.82\n119.09\n17.50\n72.00\n106.00\n116.00\n128.00\n232.00\n▂▇▂▁▁\n\n\nBPDia1\n1763\n0.82\n68.28\n13.78\n0.00\n62.00\n70.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys2\n1647\n0.84\n118.48\n17.49\n76.00\n106.00\n116.00\n128.00\n226.00\n▃▇▂▁▁\n\n\nBPDia2\n1647\n0.84\n67.66\n14.42\n0.00\n60.00\n68.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys3\n1635\n0.84\n117.93\n17.18\n76.00\n106.00\n116.00\n126.00\n226.00\n▃▇▂▁▁\n\n\nBPDia3\n1635\n0.84\n67.30\n14.96\n0.00\n60.00\n68.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nTestosterone\n5874\n0.41\n197.90\n226.50\n0.25\n17.70\n43.82\n362.41\n1795.60\n▇▂▁▁▁\n\n\nDirectChol\n1526\n0.85\n1.36\n0.40\n0.39\n1.09\n1.29\n1.58\n4.03\n▅▇▂▁▁\n\n\nTotChol\n1526\n0.85\n4.88\n1.08\n1.53\n4.11\n4.78\n5.53\n13.65\n▂▇▁▁▁\n\n\nUrineVol1\n987\n0.90\n118.52\n90.34\n0.00\n50.00\n94.00\n164.00\n510.00\n▇▅▂▁▁\n\n\nUrineFlow1\n1603\n0.84\n0.98\n0.95\n0.00\n0.40\n0.70\n1.22\n17.17\n▇▁▁▁▁\n\n\nUrineVol2\n8522\n0.15\n119.68\n90.16\n0.00\n52.00\n95.00\n171.75\n409.00\n▇▆▃▂▁\n\n\nUrineFlow2\n8524\n0.15\n1.15\n1.07\n0.00\n0.48\n0.76\n1.51\n13.69\n▇▁▁▁▁\n\n\nDiabetesAge\n9371\n0.06\n48.42\n15.68\n1.00\n40.00\n50.00\n58.00\n80.00\n▁▂▆▇▂\n\n\nDaysPhysHlthBad\n2468\n0.75\n3.33\n7.40\n0.00\n0.00\n0.00\n3.00\n30.00\n▇▁▁▁▁\n\n\nDaysMentHlthBad\n2466\n0.75\n4.13\n7.83\n0.00\n0.00\n0.00\n4.00\n30.00\n▇▁▁▁▁\n\n\nnPregnancies\n7396\n0.26\n3.03\n1.80\n1.00\n2.00\n3.00\n4.00\n32.00\n▇▁▁▁▁\n\n\nnBabies\n7584\n0.24\n2.46\n1.32\n0.00\n2.00\n2.00\n3.00\n12.00\n▇▅▁▁▁\n\n\nAge1stBaby\n8116\n0.19\n22.65\n4.77\n14.00\n19.00\n22.00\n26.00\n39.00\n▆▇▅▂▁\n\n\nSleepHrsNight\n2245\n0.78\n6.93\n1.35\n2.00\n6.00\n7.00\n8.00\n12.00\n▁▅▇▁▁\n\n\nPhysActiveDays\n5337\n0.47\n3.74\n1.84\n1.00\n2.00\n3.00\n5.00\n7.00\n▇▇▃▅▅\n\n\nTVHrsDayChild\n9347\n0.07\n1.94\n1.43\n0.00\n1.00\n2.00\n3.00\n6.00\n▇▆▂▂▂\n\n\nCompHrsDayChild\n9347\n0.07\n2.20\n2.52\n0.00\n0.00\n1.00\n6.00\n6.00\n▇▁▁▁▃\n\n\nAlcoholDay\n5086\n0.49\n2.91\n3.18\n1.00\n1.00\n2.00\n3.00\n82.00\n▇▁▁▁▁\n\n\nAlcoholYear\n4078\n0.59\n75.10\n103.03\n0.00\n3.00\n24.00\n104.00\n364.00\n▇▁▁▁▁\n\n\nSmokeAge\n6920\n0.31\n17.83\n5.33\n6.00\n15.00\n17.00\n19.00\n72.00\n▇▂▁▁▁\n\n\nAgeFirstMarij\n7109\n0.29\n17.02\n3.90\n1.00\n15.00\n16.00\n19.00\n48.00\n▁▇▂▁▁\n\n\nAgeRegMarij\n8634\n0.14\n17.69\n4.81\n5.00\n15.00\n17.00\n19.00\n52.00\n▂▇▁▁▁\n\n\nSexAge\n4460\n0.55\n17.43\n3.72\n9.00\n15.00\n17.00\n19.00\n50.00\n▇▅▁▁▁\n\n\nSexNumPartnLife\n4275\n0.57\n15.09\n57.85\n0.00\n2.00\n5.00\n12.00\n2000.00\n▇▁▁▁▁\n\n\nSexNumPartYear\n5072\n0.49\n1.34\n2.78\n0.00\n1.00\n1.00\n1.00\n69.00\n▇▁▁▁▁\n\n\n\n\n\nAgain, lots of data from skim, about the Quant and Qual variables. Spend a little time looking through this output.\n\nWhich variables could have been data that was given/stated by each respondent?\nAnd which ones could have been measured dependent data variables? Why do you think so?\nWhy is there so much missing data? Which variable are the most affected by this?\n\n\n Counts, and Charts with Counts\n\n\n\n\n\n\nQuestion\n\n\n\nQ.1 What are the Education levels and the counts of people with those levels?\n\n\n\nNHANES %&gt;% \n  group_by(Education) %&gt;% \n  summarise(total = n())\n\n\n  \n\n\n# This also works\n# tally(~Education, data = NHANES) %&gt;% as_tibble()\n\nInsight: The count goes up as we go from lower Education levels to higher. Need to keep that in mind. How do we understand the large number of NA entries?\n\n\n\n\n\n\nQuestion\n\n\n\nQ.2 How do counts of Education vs Work-status look like?\n\n\nNHANES %&gt;% \n  mutate(Education = as.factor(Education)) %&gt;% \n  group_by(Work,Education) %&gt;% \n  summarise(count = n())\nNHANES %&gt;% \n  group_by(Work, Education) %&gt;% \n  summarise(count = n()) %&gt;% \n  e_charts(Education, height = 300) %&gt;% \n  e_bar(count) %&gt;% \n  e_y_axis(max = 1750) %&gt;% \n  e_x_axis(type = \"category\")  %&gt;% e_tooltip()\n\n\n\n\n  \n\n\n\n\n\n\n\n\nInsight: Clear increase in the number of Working people as Education goes from 8th Grade to College. No surprise. Are the NotWorking counts a surprise?\n\n {{}} Stat Summaries, Histograms, and Densities\n\n\n\n\n\n\nQuestion\n\n\n\nQ.3. What is the distribution of Physical Activity Days, across Gender? Across Education?\n\n\n# NHANES %&gt;% gf_histogram( ~ PhysActiveDays | Education, fill = ~ Education)\nNHANES %&gt;% \n  group_by(Gender) %&gt;% \n  e_charts(PhysActiveDays, height = 350) %&gt;%  \n  e_histogram(PhysActiveDays) %&gt;% \n  e_x_axis(max = 8) %&gt;% \n  e_facet(cols = 2, rows = 1) %&gt;% e_tooltip()\nNHANES %&gt;% \n  group_by(Education) %&gt;% \n  e_charts(PhysActiveDays, height = 350) %&gt;% \n  e_histogram(PhysActiveDays) %&gt;% \n  e_x_axis(max = 8) %&gt;% \n  e_facet(rows = 1, cols = 3) %&gt;% e_tooltip()\n\n\n\n\n\n\n\n\n\n\n\n\nInsight: Can we conclude anything here? The populations in each category are different, as indicated by the different y-axis scales, so what do we need to do? Take percentages or ratios of course, per-capita! How would one do that?\n\n\n\n\n\n\nQuestion\n\n\n\nQ.3a. What is the distribution of Physical Activity Days, across Education and Sex, per capita?\n\n\nNHANES %&gt;% \n  group_by(Gender) %&gt;% \n  summarize(mean_active = mean(PhysActiveDays,na.rm = TRUE))\nNHANES %&gt;% \n  group_by(Education) %&gt;% \n  summarize(mean_active = mean(PhysActiveDays,na.rm = TRUE))\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nInsight: Hmm..no great differences in per-capita physical activity. Females are marginally more active than males. No need to even plot this.\n::: {.callout-note title=“Question”} Q.4. How are people Ages distributed across levels of Education?\n# Recall there are missing data\n# gf_boxplot(Age ~ Education, \n#            fill = ~ Education, # Always a good idea to fill boxes\n#            data = NHANES) %&gt;% \n#   gf_theme(theme_classic()) %&gt;% plotly::ggplotly()\n\nNHANES %&gt;% \n  mutate(Education = as.factor(Education)) %&gt;% \n  group_by(Education) %&gt;% \n  e_charts(height = 300) %&gt;% # Should not mention x-variable!!!\n  e_boxplot(Age, colorBy = \"data\",\n             itemStyle = list(borderWidth = 3)) %&gt;% \n  e_y_axis(name = \"Age\", nameLocation = \"middle\", max = 100, min = 0, nameGap = 25) %&gt;% \n  e_x_axis(type = \"category\", axisTick = list(alignWithLabel = TRUE), \n           axisLabel = list(interval = 0)) %&gt;% # ensures all tick labels on x-axis\n  e_tooltip()\n\n\n\n\n\n\n\n\nInsight: Older age groups are somewhat more heavily represented in groups with lower educational status. But College Graduates also have slightly older age distributions…So do College Educated people live longer? That is a nice Question for some Inferential Modelling. And how to interpret the NA group?\n\n\n\n\n\n\nQuestion\n\n\n\nQ.5. How is Education distributed over Race?\n\n\nNHANES_by_Race1 &lt;- NHANES %&gt;% \n  group_by(Race1) %&gt;% \n  summarize(population = n())\nNHANES_by_Race1\nNHANES %&gt;% group_by(Education, Race1) %&gt;% \n  summarize( n = n()) %&gt;% \n  left_join(NHANES_by_Race1, by = c(\"Race1\" = \"Race1\")) %&gt;% \n  mutate(percapita_educated = (n/population)*100) %&gt;% \n  ungroup() %&gt;%  \n  group_by(Race1) %&gt;% # Aesthetic 1\n  e_charts(Education, height = 350) %&gt;%  # Aesthetic #2\n  e_bar(percapita_educated) %&gt;% # Aesthetic #3\n  \n  e_x_axis(type = \"category\", axisTick = list(alignWithLabel = TRUE), \n           axisLabel = list(interval = 0)) %&gt;% \n  e_y_axis(max = 35) %&gt;% \n  e_facet(rows = 2,cols = 3) %&gt;% \n  e_flip_coords()\n\n\n\n\n  \n\n\n\n\n\n\n\n\nInsight: Blacks, Hispanics, and Mexicans tend to have fewer people with college degrees, as a percentage of their population. Asians and other immigrants have a significant tendency towards higher education!\n\n\n\n\n\n\nQuestion\n\n\n\nQ.6. What is the distribution of people’s BMI, split by Gender? By Race1?\n\n\n# One can also plot both histograms and densities in an overlay fashion,\n\nNHANES %&gt;% group_by(Gender) %&gt;% \n  e_charts(height = 300) %&gt;% \n  e_density(BMI)\nNHANES %&gt;% group_by(Race1) %&gt;% \n  e_charts(height = 350) %&gt;% \n  e_density(BMI) %&gt;% \n  e_facet(rows = 2, cols = 3)\n\n\n\n\n\n\n\n\n\n\n\n\nInsight: Non-white races tend to have larger portions of their populations with larger BMI. So these races perhaps tend to obesity. By and large BMI distributions are normal.\n\n\n\n\n\n\nQuestion\n\n\n\nQ.7. What is the distribution of people’s Testosterone level vs BMI? Split By Race1?\n\n\n\nNHANES %&gt;%  \n  gf_density2d(Testosterone ~ BMI | Race1) %&gt;% \n  gf_theme(theme_classic()) %&gt;% \nplotly::ggplotly()\n\n\n\n\n\nInsight: Low testosterone levels exist across all BMI values, but healthy levels of T exists only over a smaller range of BMI.\nNote: echarts4r does not seem to provide a 2D-density plot…yet!!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions-interactive.html#case-study-3-a-complete-example-with-banned-books",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions-interactive.html#case-study-3-a-complete-example-with-banned-books",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Case Study #3: A complete example with Banned Books",
    "text": "Case Study #3: A complete example with Banned Books\nHere is a dataset from Jeremy Singer-Vine’s blog, Data Is Plural. This is a list of all books banned in schools across the US.\n Download the data \n\n Look at the Data\n\nbanned &lt;- readxl::read_xlsx(path = \"../data/banned.xlsx\",\n                            sheet = \"Sorted by Author & Title\")\nskim(banned)\n\n\nData summary\n\n\nName\nbanned\n\n\nNumber of rows\n1586\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n10\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nAuthor\n0\n1.00\n7\n29\n0\n797\n0\n\n\nTitle\n0\n1.00\n2\n155\n0\n1145\n0\n\n\nType of Ban\n0\n1.00\n21\n36\n0\n4\n0\n\n\nSecondary Author(s)\n1488\n0.06\n9\n187\n0\n61\n0\n\n\nIllustrator(s)\n1222\n0.23\n8\n35\n0\n192\n0\n\n\nTranslator(s)\n1576\n0.01\n14\n25\n0\n9\n0\n\n\nState\n0\n1.00\n4\n14\n0\n26\n0\n\n\nDistrict\n0\n1.00\n4\n40\n0\n86\n0\n\n\nDate of Challenge/Removal\n0\n1.00\n5\n15\n0\n15\n0\n\n\nOrigin of Challenge\n0\n1.00\n13\n16\n0\n2\n0\n\n\n\n\n\nInsight: Clearly the variables are all Qualitative, except perhaps for Date of Challenge/Removal, (which in this case has been badly mangled by Excel) So we need to make counts based on the* levels* of the Qual variables and plot Bar/Column charts. We will not find a use for histograms or densities.\nLet us try to answer this question, about counts:\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is the count of banned books by type and by US state?\n\n\n\nbanned_by_state &lt;- \n  banned %&gt;% \n  group_by(State) %&gt;% \n  summarise(total = n()) %&gt;% \n  ungroup()\nbanned_by_state\n\n\n  \n\n\nbanned %&gt;% \n  group_by(State, `Type of Ban`) %&gt;% \n  summarise(count = n()) %&gt;% \n  ungroup() %&gt;% \n  left_join(., banned_by_state, by = c(\"State\" = \"State\")) %&gt;% \n  \n #  pivot_wider(.,id_cols = State,\n #              names_from = `Type of Ban`,\n #              values_from = count) %&gt;% janitor::clean_names() %&gt;% \n #  replace_na(list(banned_from_libraries_and_classrooms = 0,\n #                  banned_from_libraries = 0,\n #                  banned_pending_investigation = 0,\n #                  banned_from_classrooms = 0)) %&gt;% \n # mutate(total = sum(across(where(is.integer)))) %&gt;%\ngf_col(count ~ reorder(State, total), \n          fill = ~ `Type of Ban`) %&gt;% \n  gf_labs(x = \"Count of Banned Books\",\n          y = \"State\") %&gt;% \n  gf_refine(coord_flip()) %&gt;% \n  gf_theme(theme = theme_minimal())\n\n\n\n\n\n\n\nInsight: Do you want to live in Texas? If you are both illiterate and interested in horses, perhaps."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions-interactive.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions-interactive.html#conclusion",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Conclusion",
    "text": "Conclusion\nAnd that is a wrap!! Try to work with this procedure:\n\nInspect the data using skim or inspect\n\nIdentify Qualitative and Quantitative variables\n\nNotice variables that have missing data\n\nDevelop Counts of Observations for combinations of Qualitative variables (factors)\n\nDevelop Histograms and Densities, and slice them by Qualitative variables to develop facetted plots as needed\nAt each step record the insight and additional questions!!\n\nContinue with other Descriptive Graphs as needed\n\nAnd then on the inference and modelling!!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions-interactive.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions-interactive.html#references",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n References",
    "text": "References\n\nSharon Machlis, Plot in R with echarts4r, InfoWorld https://www.infoworld.com/article/3607068/plot-in-r-with-echarts4r.html\n\nA detailed analysis of the NHANES dataset, https://awagaman.people.amherst.edu/stat230/Stat230CodeCompilationExampleCodeUsingNHANES.pdf"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions-interactive.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions-interactive.html#footnotes",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "Footnotes",
    "text": "Footnotes\n\nFundamentals of Data Visualization (clauswilke.com)↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/listing.html",
    "href": "content/courses/Analytics/Descriptive/listing.html",
    "title": "Descriptive Analytics",
    "section": "",
    "text": "Title\n\n\n\n\n\n\n\n\n\n🕶 Science, Human Experience, Experiments, and Data\n\n\n\n\n\n\n\n📊 Descriptive Statistics\n\n\n\n\n\n\n\n📊 Distributions, Densities, Bar Plots, and Boxplots\n\n\n\n\n\n\n\n📎 Correlations\n\n\n\n\n\n\n\n🐉 Visualizing Categorical Data\n\n\n\n\n\n\n\n🐉 Visualizing Survey Data\n\n\n\n\n\n\n\n🕔 Time Series\n\n\n\n\n\n\n\n🍕 Parts of a Whole\n\n\n\n\n\n\n\n🕸 Change, Evolution, and Flow\n\n\n\n\n\n\n\n👌 Ratings and Rankings\n\n\n\n\n\n\n\n🗺 Visualising Spatial Data\n\n\n\n\n\n\n\n📚 Miscellaneous Graphing Tools, and References\n\n\n\n\n\n\n\nEDA Workflow\n\n\n\n\n\n\n\nThe Grammar of Networks\n\n\n\n\n\nNo matching items\n\n Back to top",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "",
    "text": "knitr::opts_chunk$set(tidy = TRUE)\nlibrary(tidyverse) # Tidy data processing \nlibrary(ggformula) # Formula based plots\nlibrary(mosaic) # Data inspection and Statistical Inference \nlibrary(broom) # Tidy outputs from Statistical Analyses\nlibrary(infer) # Statistical Inference\nlibrary(patchwork) # Arranging Plots",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#setting-up-r-packages",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "",
    "text": "knitr::opts_chunk$set(tidy = TRUE)\nlibrary(tidyverse) # Tidy data processing \nlibrary(ggformula) # Formula based plots\nlibrary(mosaic) # Data inspection and Statistical Inference \nlibrary(broom) # Tidy outputs from Statistical Analyses\nlibrary(infer) # Statistical Inference\nlibrary(patchwork) # Arranging Plots",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#introduction",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#introduction",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n Introduction",
    "text": "Introduction\nSuppose we have three sales strategies on our website, to sell a certain product, say men’s shirts. We have observations of customer website interactions over several months. How do we know which strategy makes people buy the fastest ?\nIf there is a University course that is offered in parallel in three different classrooms, is there a difference between the average marks obtained by students in each of the classrooms?\nIn each case we have a set of observations in each category: Interaction Time vs Sales Strategy in the first example, and Student Marks vs Classroom in the second. We can take mean scores in each category and decide to compare them. How do we make the comparisons? One way would be to compare them pair-wise. But with this rapidly becomes intractable and also dangerous: with increasing number of groups, the number of mean-comparisons becomes very large \\(N\\choose 2\\) and with each comparison the possibility of some difference showing up, just by chance, increases! And we end up making the wrong inference and perhaps the wrong decision.\nThe trick is of course to make comparisons all at once and ANOVA is the technique that allows us to do just that. In this tutorial, we will compare the Hatching Time of frog spawn1, at three different lab temperatures.\nIn this tutorial, our research question is:\n\n\n\n\n\n\nResearch Question\n\n\n\nHow does frogspawn hatching time vary with different temperature settings?",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#workflow-read-the-data",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#workflow-read-the-data",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n Workflow: Read the Data",
    "text": "Workflow: Read the Data\nDownload the data by clicking the button below.\n Download the frogs data \n\n\n\n\n\n\nData Folder\n\n\n\nSave the CSV in a subfolder titled “data” inside your R work folder.\n\n\n\nfrogs_orig &lt;- read_csv(\"data/frogs.csv\")\nfrogs_orig\n\n\n  \n\n\n\nOur response variable is the hatching Time. Our explanatory variable is a factor, Temperature, with 3 levels: 13°C, 18°C and 25°C. Different samples of spawn were subject to each of these temperatures respectively.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#workflow-clean-the-data",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#workflow-clean-the-data",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n Workflow: Clean the Data",
    "text": "Workflow: Clean the Data\nThe data is badly organized, with a separate column for each Temperature, and a common column for Sample ID. There are NA entries since not all samples of spawn can be subject to all temperatures. (E.g. Sample ID #1 was maintained at 13°C).\nWe will first stack up the Temperature** columns into a single column, separate that into pieces and then retain just the number part (13, 18, 25), getting rid of the word Temperature from the column titles. Then the remaining numerical column with temperatures (13, 18, 25) will be converted into a factor.\nWe will use pivot_longer()and separate_wider_regex() to achieve this. [See this animation for pivot_longer(): https://haswal.github.io/pivot/ ]\nfrogs_orig %&gt;%\n  pivot_longer(\n    .,\n    cols = starts_with(\"Temperature\"),\n    cols_vary = \"fastest\",\n    # new in pivot_longer\n    names_to = \"Temp\",\n    values_to = \"Time\"\n  ) %&gt;%\n  drop_na() %&gt;%\n  \n  separate_wider_regex(\n    cols = Temp,\n  # knock off the unnecessary \"Temperature\" word everywhere\n  # Just keep the digits thereafter\n    patterns = c(\"Temperature\",\n                 TempFac = \"\\\\d+\"),\n    cols_remove = TRUE\n  ) %&gt;%\n  \n  # Convert Temp into TempFac, a 3-level factor\n  mutate(TempFac = factor(\n    x = TempFac,\n    levels = c(13, 18, 25),\n    labels = c(\"13\", \"18\", \"25\")\n  )) %&gt;%\n  rename(\"Id\" = `Frogspawn sample id`) -&gt; frogs_long\n\nfrogs_long\nfrogs_long %&gt;% count(TempFac)\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nSo we have cleaned up our data and have 20 samples for Hatching Time per TempFac setting.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#workflow-eda",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#workflow-eda",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n Workflow: EDA",
    "text": "Workflow: EDA\nLet us plot some histograms and boxplots of Hatching Time:\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n\ngf_histogram(\n  data = frogs_long,\n  ~ Time,\n  fill = ~ TempFac,\n  stat = \"count\",\n  alpha = 0.5\n) %&gt;%\n  gf_vline(xintercept = ~ mean(Time)) %&gt;%\n  gf_labs(x = \"Hatching Time\", y = \"Count\") %&gt;%\n  gf_text(7 ~ (mean(Time) + 2),\n          label = \"Overall Mean\") %&gt;%\n  gf_refine(guides(fill = guide_legend(title = \"Temperature level (°C)\")))\n###\ngf_boxplot(data = frogs_long,\n           Time ~ TempFac,\n           fill = ~ TempFac,\n           alpha = 0.5) %&gt;%\n  gf_vline(xintercept = ~ mean(Time)) %&gt;%\n  gf_labs(x = \"Temperature\", y = \"Hatching Time\",\n          caption = \"X-axis is reversed\") %&gt;%\n  gf_refine(scale_x_discrete(limits = rev),\n            guides(fill = guide_legend(title = \"Temperature level (°C)\"))) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe histograms look well separated and the box plots also show very little overlap. So we can reasonably hypothesize that Temperature has a significant effect on Hatching Time.\nOne more slightly esoteric plot: Jitter/Scatter with a new categorical x-axis offered by the ggprism package:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n\nlibrary(ggprism)\ngf_jitter(\n  frogs_long,\n  Time ~ TempFac,\n  color = ~ TempFac,\n  xlab = \"Temperature as Factor\",\n  ylab = \"Hatching Time\",\n  caption = \"Using `ggprism` package\"\n) %&gt;%\n  gf_theme(theme_prism(base_family = theme_get()$font$family)) %&gt;%\n  gf_refine(theme(legend.position = \"none\"),\n            scale_x_discrete(guide = \"prism_bracket\"))\n\n\n\n\n\n\n\nLet’s go ahead with our ANOVA test.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#workflow-anova",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#workflow-anova",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n Workflow: ANOVA",
    "text": "Workflow: ANOVA\nWe will first execute the ANOVA test with code and evaluate the results. Then we will do an intuitive walk through of the process and finally, hand-calculate entire analysis for clear understanding.\n\n\nANOVA Test with Code\nANOVA Intuitive\n\nANOVA Manually Demonstrated2(Apologies to Spinoza)\n\n\n\nR offers a very simple command to execute an ANOVA test: Note the familiar formula of stating the variables:\n\nfrogs_anova &lt;- aov(Time ~ TempFac, data = frogs_long)\nsummary(frogs_anova)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)    \nTempFac      2 1020.9   510.5   385.9 &lt;2e-16 ***\nResiduals   57   75.4     1.3                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nfrogs_anova %&gt;% broom::tidy()\n\n\n  \n\n\nsummary.lm(frogs_anova) %&gt;% broom::tidy()\n\n\n  \n\n\nfrogs_anova %&gt;% broom::glance()\n\n\n  \n\n\n\nThe effect of Temperature on Hatching time is significant, with a p-value of \\(&lt;2e-16\\). The F-statistic for the ANOVA test is given by \\(385.9\\), which is very high, and the r.squared value ( to be discussed later) is also large, \\(0.931\\). Clearly Temperature has a very significant effect on the hatching Time.\nTo find which specific value of TempFac has the most effect will require pairwise comparison of the group means, using a standard t-test. The confidence level for such repeated comparisons will need what is called Bonferroni correction3 to prevent us from detecting a significant (pair-wise) difference simply by chance. To do this we take \\(\\alpha = 0.05\\), the confidence level used and divide it by \\(K\\), the number of pair-wise comparisons we intend to make. This new value is used to decide on the significance of the estimated parameter. So the pairwise comparisons in our current data will have to use \\(\\alpha/3 = 0.0166\\) as the confidence level.\n\n\nAll that is very well, but what is happening under the hood of the aov() command?\nConsider a data set with a single Quant and a single Qual variable. The Qual variable has two levels, the Quant data has 20 observations per Qual level.\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nIn Fig A, the horizontal black line is the overall mean of quant, denoted as \\(\\mu_{tot}\\). The vertical black lines to the points show the departures of each point from this overall mean. The sum of squares of these vertical black lines in Fig A is called the Total Sum of Squares (SST).\n\\[\nSST = \\Sigma (y - \\mu_{tot})^2\n\\tag{1}\\]\n\n\n\n\n\n\nNote\n\n\n\nIf there are \\(k\\) levels in qual and \\(n\\) observations \\(y_ n\\) for each level, we can also write:\n\\[\nSST =\n\\sum_{i=1}^{kn}y_i^2 - \\frac{ \\left( \\sum_{i=1}^{kn}\ny_i \\right)^2}{kn}\n\\]\n\n\nIn Fig B, the horizontal green and red lines are the means of the individual groups, respectively \\(\\mu_A\\) and \\(\\mu_B\\). The green and red vertical lines are the departures, or errors, of each point from its own group-mean. The sum of the squares of the green and red lines is called the Total Error Sum of Squares (SSE).\n\\[\nSSE = \\Sigma [(y - \\mu_i)^2 +... (y - \\mu_k)^2]\n\\tag{2}\\]\nIf the \\(\\mu_A\\) and \\(\\mu_B\\) are different from \\(\\mu_{tot}\\), then what would be the relationship between \\(SST\\) and \\(SSE\\) ? Clearly if the all means are identical then the \\(SST\\) and \\(SSE\\) are equal, since the two coloured lines would be in the same place as the black line. It should be clear that if \\(\\mu_A\\) and \\(\\mu_B\\) are different from the overall mean \\(\\mu_{tot}\\), then \\(SSE &lt; SST\\).\nSo, when we desire to detect if the two groups are different in their means, we take the difference:\n\\[\nSSA = SST - SSE\n\\tag{3}\\]\n\\(SSA\\) is called the Treatment Sum of Squares and is a measure the differences in means of observations at different levels of the factor.\n\n\n\n\n\n\nNote\n\n\n\n\\(SSA\\) can also directly be re-written in a very symmetric fashion as:\n\\[\n\\frac{\\sum_{i=1}^{k} \\left( \\sum_{j=1}^{n}y_{ij}\\right)^2 }{n} - \\frac{\\left( \\sum_{i=1}^{kn}\ny_i \\right)^2}{kn}\n\\]\nNote that in the first term, we are calculating sums of observations within each group in the inner summation, which is like a per-group mean(without the division). The outer summation takes the sum of squares of these undivided summations and divides by \\(n\\).\n\n\nComparing \\(SSA\\) and \\(SSE\\) now provides us with a method that helps us decide whether these means are different. \\(SSA\\) is the leftover unexplained error using \\(\\mu_{tot}\\) as the estimate (NULL Hypothesis). \\(SSE\\) is the unexplained error using individual means per group (Alternative Hypothesis). The logic in comparing these two global differences and local differences is there must be a significant reduction in unexplained error going from NULL to Alternative Hypothesis.\nBefore we compare, we need to scale: since each of these measures uses a different set of observations, the comparison is done after scaling each of \\(SSA\\) and \\(SSE\\) by the number of observations influencing them. (a sort of “per capita” error). This means that we need to divide each of \\(SSA\\) and \\(SSE\\) by their degrees of freedom, which gives us a ratio of variances, the F-statistic:\n\\[\nF_{stat} = \\frac{SSA / df_{SSA}}{SSE / df_{SSE}}\n\\]\nwhere \\(df_{SSA}\\) and \\(df_{SSE}\\) are respectively the degrees of freedom in \\(SSA\\) and \\(SSE\\). And so we are in effect deciding if means are significantly different by analyzing (a ratio of) variances! Hence AN-alysis O-f VA-riance, ANOVA.\nIn order to find which of the means is significantly different from others, we need to make a pair-wise comparison of the means, applying the Bonferroni correction as stated before. This means we divide the critical p.value we expect by the number of comparisons we make between levels of the Qual variable. More on this shortly.\n\n\nNow that we understand what aov() is doing, let us hand-calculate the numbers for our frogs dataset and check. Let us visualize our calculations first.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere is the SST:\n\n# Calculate overall sum squares SST\n\n\nfrogs_overall &lt;- frogs_long %&gt;% \n  summarise(mean_time = mean(Time), \n            # Overall mean across all readings\n            # The Black Line\n            \n            SST = sum((Time - mean_time)^2),\n            n = n())\nfrogs_overall\n\nSST &lt;- frogs_overall$SST\nSST\n\n\n  \n\n\n\n[1] 1096.333\n\n\nAnd here is the SSE:\n\n# Calculate sums of square errors *within* each group\n# with respect to individual group means\n\nfrogs_within_groups &lt;- frogs_long %&gt;% \n  group_by(TempFac) %&gt;% \n   summarise(mean_time = mean(Time),\n            variance_time = var(Time),\n            group_error_squares = sum((Time - mean_time)^2),\n            n = n())\nfrogs_within_groups\n\nfrogs_SSE &lt;- frogs_within_groups %&gt;% \n  summarise(SSE = sum(group_error_squares))\n\nSSE &lt;- frogs_SSE$SSE\nSSE\n\n\n  \n\n\n\n[1] 75.4\n\n\nOK, we have \\(SST\\) and \\(SSE\\), so let’s get \\(SSA\\):\n\nSST\nSSE\nSSA &lt;- SST - SSE\nSSA\n\n[1] 1096.333\n[1] 75.4\n[1] 1020.933\n\n\nWe have \\(SST = 1096\\), \\(SSE = 75.4\\) and therefore \\(SSA = 1020.9\\).\nIn order to calculate the F-Statistic, we need to compute the variances, using these sum of squares. We obtain variances by dividing by their Degrees of Freedom:\n\\[\nF_{stat} = \\frac{SSA / df_{SSA}}{SSE / df_{SSE}}\n\\]\nwhere \\(df_{SSA}\\) and \\(df_{SSE}\\) are respectively the degrees of freedom in SSA and SSE.\nLet us calculate these Degrees of Freedom.\nWith \\(k = 3\\) levels in the factor TempFac, and \\(n = 20\\) points per level, \\(SST\\) clearly has degree of freedom \\(kn-1\\), since it uses all observations but loses one degree to calculate the global mean. (If each level did not have the same number of points \\(n\\), we simply take all observations less one as the degrees of freedom for \\(SST\\)).\n\\(SSE\\) has \\(k*(n-1)\\) as degrees of freedom, since each of the \\(k\\) groups there are \\(n\\) observations and each group loses one degree to calculate its own group mean.\nAnd therefore \\(SSA\\), being their difference, has \\(k-1\\) degrees of freedom.\nWe can still calculate these in R, for the sake of method and clarity:\n\n# Error Sum of Squares SSE\ndf_SSE &lt;- frogs_long %&gt;%\n  \n  # Takes into account \"unbalanced\" situations\n  group_by(TempFac) %&gt;%\n  summarise(per_group_df_SSE = n() - 1) %&gt;%\n  summarise(df_SSE = sum(per_group_df_SSE)) %&gt;% as.numeric()\n\n\n## Overall Sum of Squares SST\ndf_SST &lt;- frogs_long %&gt;%\n  summarise(df_SST = n() - 1) %&gt;% as.integer()\n\n\n# Treatment Sum of Squares SSA\nk &lt;- length(unique(frogs_long$TempFac))\ndf_SSA &lt;- k - 1\n\nThe degrees of freedom for the quantities are:\ndf_SST\ndf_SSE\ndf_SSA\n\n\n\n[1] 59\n[1] 57\n[1] 2\n\n\n\nNow we are ready to compute the F-statistic: dividing each sum-of-squares byt its degrees of freedom gives us variances which we will compare, using the F-statistic as a ratio:\n\n# Finally F_Stat!\n# Combine the sum-square_error for each level of the factor\n# Weighted by degrees of freedom **per level**\n# Which are of course equal here ;-D\n\nMSE &lt;- frogs_within_groups %&gt;% \n  summarise(mean_square_error = sum(group_error_squares/df_SSE)) %&gt;% \n  as.numeric()\nMSE\n\nMSA &lt;- SSA/df_SSA # This is OK\nMSA\n\nF_stat &lt;- MSA/MSE\nF_stat\n\n[1] 1.322807\n[1] 510.4667\n[1] 385.8966\n\n\nThe F-stat is compared with a critical value of the F-statistic,F_crit which is computed using the formula for the f-distribution in R. As with our hypothesis tests, we set the significance level to \\(\\alpha = 0.95\\), but here with the Bonferroni correction, and quote the two relevant degrees of freedom as parameters to qf() which computes the critical F value as a quartile:\n\nF_crit &lt;-  \n  qf(p = (1 - 0.05/3),  # Significance level is 5% + Bonferroni Correction\n          df1 = df_SSA, # Numerator degrees of freedom \n          df2 = df_SSE  # Denominator degrees of freedom\n     ) \nF_crit\nF_stat\n\n[1] 4.403048\n[1] 385.8966\n\n\nThe F_crit value can also be seen in a plot4,5:\n# Set graph theme\ntheme_set(new = theme_custom())\n#\ngf_dist(dist = \"f\",\n        params = list(df1 = df_SSA, df2 = df_SSE), \n        # linewidth  = 1,\n        #xlim = c(0, 400),\n        #title = \"F distribution for Frog ANOVA\"\n        ) %&gt;% \n  gf_vline(xintercept = F_crit, linetype = \"dotted\", \n           colour = \"red\",title = \"F distribution for Frog ANOVA\") %&gt;% \n  gf_vline(xintercept = F_stat, linetype = \"dashed\", \n           color = \"dodgerblue\") %&gt;% \n  gf_text( 0.25 ~ 360, label = \"F_stat\", colour = \"dodgerblue\") %&gt;% \n  gf_text( 0.25 ~ 20, label = \"F_crit\", colour = \"red\")  %&gt;%\n  gf_theme(theme=theme_classic())\nmosaic::xpf(q = F_crit, \n            df1 = df_SSA, df2 = df_SSE,\n            log.p = FALSE,lower.tail = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[1] 0.9833333\n\n\n\nAny value of F more than the F_crit occurs with smaller probability than \\(0.05/3\\). Our F_stat is much higher than F_crit, by orders of magnitude! And so we can say with confidence that Temperature has a significant effect on spawn Time.\nAnd that is how ANOVA computes!",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#workflow-checking-anova-assumptions",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#workflow-checking-anova-assumptions",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n Workflow: Checking ANOVA Assumptions",
    "text": "Workflow: Checking ANOVA Assumptions\nANOVA makes 3 fundamental assumptions:\n\nData (and errors) are normally distributed.\nVariances are equal.\nObservations are independent.\n\nWe can check these using checks and graphs.\n\n Checks for Normality\nThe shapiro.wilk test tests if a vector of numeric data is normally distributed and rejects the hypothesis of normality when the p-value is less than or equal to 0.05. \n\nshapiro.test(x = frogs_long$Time)\n\n\n    Shapiro-Wilk normality test\n\ndata:  frogs_long$Time\nW = 0.92752, p-value = 0.001561\n\n\nThe p-value is very low and we cannot reject the (alternative) hypothesis that the overall data is not normal. How about normality at each level of the factor?\n\nfrogs_grouped &lt;- frogs_long %&gt;% \n  group_by(TempFac) %&gt;% \n  nest(.key = \"list\") # naming the nested column \"list\"\n\n# Checking if we can purrr\nfrogs_grouped %&gt;% \n  purrr::pluck(\"list\", 1) %&gt;% \n  dplyr::select(Time) %&gt;% \n  as_vector() %&gt;% \n  shapiro.test(.)\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.88954, p-value = 0.02638\n\n# OK now we are set for group-wise Shapiro-Wilk testing with purrr:\n\nfrogs_grouped %&gt;% \n  mutate(shaptest = \n           purrr::map(.x = list, # Column name is \"list\"\n                      .f = \\(.x) select(.data = .x, \n                                        Time) %&gt;% \n                                 as_vector() %&gt;% \n                                 shapiro.test(.)),\n         \n         params = \n           purrr::map(.x = shaptest,\n                      .f = \\(.x) broom::tidy(.x))) %&gt;% \n  \n  select(TempFac, params) %&gt;% \n  unnest(cols = params)\n\n\n  \n\n\n\n\n\n#### Using `dplyr::group_modify()\nfrogs_long %&gt;%\n  group_by(TempFac) %&gt;%\n  group_modify( ~ .x %&gt;%\n                  select(Time) %&gt;%\n                  \n                  as_vector() %&gt;% shapiro.test() %&gt;%\n                  broom::tidy())\nThe shapiro.wilk test makes a NULL Hypothesis that the data are normally distributed and estimates the probability that this could have happened by chance. Except for TempFac = 18 the p-values are less than 0.05 and we can reject the NULL hypothesis that each of these is normally distributed. Perhaps this is a sign that we need more than 20 samples per factor level. Let there be more frogs !!!\nWe can also check the residuals post-model:\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n\nfrogs_anova$residuals %&gt;% \n  as_tibble() %&gt;% \n  gf_dhistogram(~ value,data = .) %&gt;% \n  gf_fitdistr() \nfrogs_anova$residuals %&gt;%\n  as_tibble() %&gt;% \n  gf_qq(~ value, data = .) %&gt;% \n  gf_qqstep() %&gt;% \n  gf_qqline() \nshapiro.test(frogs_anova$residuals)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  frogs_anova$residuals\nW = 0.94814, p-value = 0.01275\n\n\n\nUnsurprisingly, the residuals are also not normally distributed either.\n\n Check for Similar Variance\nResponse data with different variances at different levels of an explanatory variable are said to exhibit heteroscedasticity. This violates one of the assumptions of ANOVA.\nTo check if the Time readings are similar in variance across levels of TempFac, we can use the Levene Test, or since our per-group observations are not normally distributed, a non-parametric rank-based Fligner-Killeen Test. The NULL hypothesis is that the data are with similar variances. The tests assess how probable this is with the given data assuming this NULL hypothesis:\nfrogs_long %&gt;% \n  group_by(TempFac) %&gt;% \n  summarise(variance = var(Time))\n\n# Not too different...OK on with the test\nfligner.test(Time ~ TempFac, data = frogs_long)\n\n\nDescTools::LeveneTest(Time ~ TempFac, data = frogs_long)\n\n\n\n\n  \n\n\n\n\n    Fligner-Killeen test of homogeneity of variances\n\ndata:  Time by TempFac\nFligner-Killeen:med chi-squared = 0.53898, df = 2, p-value = 0.7638\n\n\n\n\n\n  \n\n\n\n\nIt seems that there is no cause for concern here; the data do not have significantly different variances.\n\n Independent Observations\nThis is an experiment design concern; the way the data is gathered must be specified such that data for each level of the factors ( factor combinations if there are more than one) should be independent.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#workflow-effect-size",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#workflow-effect-size",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n Workflow: Effect Size",
    "text": "Workflow: Effect Size\nThe simplest way to find the actual effect sizes detected by an ANOVA test is to use (paradoxically) the summary.lm() command:\n\ntidy_anova &lt;- \n  frogs_anova %&gt;% \n  summary.lm() %&gt;% \n  broom::tidy()\ntidy_anova\n\n\n  \n\n\n\nIt may take a bit of effort to understand this. First the TempFac is arranged in order of levels, and the mean at the \\(TempFac = 13\\) is titled Intercept. That is \\(26.3\\). The other two means for levels \\(18\\) and \\(25\\) are stated as differences from this intercept, \\(-5.3\\) and \\(-10.1\\) respectively. The p.value for all these effect sizes is well below the desired confidence level of \\(0.05\\).\n\n\n\n\n\n\nStandard Errors\n\n\n\nObserve that the std.error for the intercept is \\(0.257\\) while that for TempFac18 and TempFac25 is \\(0.257 \\times \\sqrt2 = 0.363\\) since the latter are differences in means, while the former is a single mean. The Variance of a difference is the sum of the individual variances, which are equal here.\n\n\nWe can easily plot bar-chart with error bars for the effect size:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\ntidy_anova %&gt;% \n  mutate(hi = estimate + std.error,\n         lo = estimate - std.error) %&gt;% \n  gf_hline(data = ., yintercept = 0, \n           colour =\"grey\", \n           linewidth = 2) %&gt;% \n  gf_col(estimate ~ term, \n         fill = \"grey\", \n         color = \"black\",\n         width = 0.15) %&gt;% \n  gf_errorbar(hi + lo ~ term,\n              color = \"blue\",\n              width = 0.2) %&gt;% \n  gf_point(estimate ~ term,\n           color = \"red\", \n           size = 3.5) %&gt;% \n  gf_refine(scale_x_discrete(\"Temp Values\", \n                             labels = c(\"13°C\", \"18°C\", \"25°C\")))\n\n\n\n\n\n\n\nIf we want an “absolute value” plot for effect size, it needs just a little bit of work:\n\n# Merging group averages with `std.error`\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n\nfrogs_long %&gt;% \n  group_by(TempFac) %&gt;% \n  summarise(mean = mean(Time)) %&gt;% \n  cbind(std.error = tidy_anova$std.error) %&gt;% \n  mutate(hi = mean + std.error,\n         lo = mean - std.error) %&gt;% \n  gf_hline(data = ., yintercept = 0, \n           colour =\"grey\", \n           linewidth = 2) %&gt;% \n  gf_col(mean ~ TempFac, \n         fill = \"grey\", \n         color = \"black\", width = 0.15) %&gt;% \n  gf_errorbar(hi + lo ~ TempFac,\n                color = \"blue\",\n                width =0.2) %&gt;% \n  gf_point(mean ~ TempFac, \n           color = \"red\", \n           size = 3.5) %&gt;% \n  gf_refine(scale_x_discrete(\"Temp Values\", \n                             labels = c(\"13°C\", \"18°C\", \"25°C\")))\n\n\n\n\n\n\n\nIn both graphs, note the difference in the error-bar heights.\nThe ANOVA test does not tell us that the “treatments” (i.e. levels of TempFac) are equally effective. We need to use a multiple comparison procedure to arrive at an answer to that question. We compute the pair-wise differences in effect-size:\n\nfrogs_anova %&gt;% stats::TukeyHSD()\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Time ~ TempFac, data = frogs_long)\n\n$TempFac\n       diff        lwr       upr p adj\n18-13  -5.3  -6.175224 -4.424776     0\n25-13 -10.1 -10.975224 -9.224776     0\n25-18  -4.8  -5.675224 -3.924776     0\n\n\nWe see that each of the pairwise differences in effect-size is significant, with p = 0 !\nUsing other packages\n\n\nUsing ggstatsplot\nUsing supernova\n\n\n\nThere is a very neat package called ggstatsplot6 that allows us to plot very comprehensive statistical graphs. Let us quickly do this:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nlibrary(ggstatsplot)\nfrogs_long %&gt;%\n  ggstatsplot::ggbetweenstats(x = TempFac, y = Time,\n                              title = \"ANOVA : Frogs Spawn Time vs Temperature Setting\")\n\n\n\n\n\n\n\n\n\nWe can also obtain crisp-looking anova tables from the new supernova package 7, which is based on the methods discussed in Judd et al. Section 11\nlibrary(supernova)\nsupernova::supernova(frogs_anova)\nsupernova::pairwise(frogs_anova)\n\n\n\n Analysis of Variance Table (Type III SS)\n Model: Time ~ TempFac\n\n                               SS df      MS       F   PRE     p\n ----- --------------- | -------- -- ------- ------- ----- -----\n Model (error reduced) | 1020.933  2 510.467 385.897 .9312 .0000\n Error (from model)    |   75.400 57   1.323                    \n ----- --------------- | -------- -- ------- ------- ----- -----\n Total (empty model)   | 1096.333 59  18.582                    \n\n\n\n\n\n\n  group_1 group_2    diff pooled_se       q    df   lower  upper p_adj\n  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 18      13       -5.300     0.257 -20.608    57  -6.175 -4.425 .0000\n2 25      13      -10.100     0.257 -39.272    57 -10.975 -9.225 .0000\n3 25      18       -4.800     0.257 -18.664    57  -5.675 -3.925 .0000\n\n\n\nThe supernova table clearly shows the reduction the Sum of Squares as we go from a NULL (empty) model to a full ANOVA model.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#workflow-anova-using-permutation-tests",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#workflow-anova-using-permutation-tests",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n Workflow: ANOVA using Permutation Tests",
    "text": "Workflow: ANOVA using Permutation Tests\nWe wish to establish the significance of the effect size due to each of the levels in TempFac. From the normality tests conducted earlier we see that except at one level of TempFac, the times are are not normally distributed. Hence we opt for a Permutation Test to check for significance of effect.\nAs remarked in Ernst8, the non-parametric permutation test can be both exact and also intuitively easier for students to grasp. Permutations are easily executed in R, using packages such as mosaic9.\nWe proceed with a Permutation Test for TempFac. We shuffle the levels (13, 18, 25) randomly between the Times and repeat the ANOVA test each time and calculate the F-statistic. The Null distribution is the distribution of the F-statistic over the many permutations and the p-value is given by the proportion of times the F-statistic equals or exceeds that observed.\nWe will use mosaic first, and also try with infer.\n\n\nUsing mosaic\nUsing infer\n\n\n\nmosaic offers an easy and intuitive way of doing a repeated permutation test, using the do() command. We will shuffle the TempFac factor to jumble up the Time observations, 4999 times. Each time we shuffle, we compute the F_statistic and record it. We then plot the 4999 F-statistics and compare that with the real-world observation of F-stat.\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nobs_F_stat &lt;- \n  frogs_anova %&gt;% \n  broom::tidy() %&gt;% \n  select(statistic)\nobserved_mosaic &lt;- obs_F_stat$statistic[1]\nobserved_mosaic\n\n[1] 385.8966\n\nnull_dist_mosaic &lt;- do(4999) * aov(Time ~ shuffle(TempFac), \n                                    data = frogs_long)\nnull_dist_mosaic %&gt;% head()\n\n\n  \n\n\nnull_dist_mosaic %&gt;% drop_na() %&gt;% \n  select(F) %&gt;% \n  gf_histogram(data = ., ~ F, \n               fill = ~ F &gt;= observed_mosaic,\n               title = \"Null Distribution of ANOVA F-statistic\",\n               xlab = \"Simulated F values (using Permutation)\",\n               ylab = \"Count\") %&gt;% \n  gf_vline(xintercept = observed_mosaic) %&gt;%\n  gf_text(750 ~ observed_mosaic - 325, label = \"Observed F\") %&gt;%\n  gf_refine(scale_x_continuous(trans = \"log10\"),\n            annotation_logticks(),\n            scale_fill_discrete(name = \"Simulated F &gt; Observed F ?\"))\n\n\n\n\n\n\n\nThe Null distribution of the F_statistic under permutation shows it never crosses the real-world observed value, testifying as to the strength of the effect of TempFac on hatching Time. And the p-value is:\n\np_value &lt;- mean(null_dist_mosaic$F &gt;= observed_mosaic, na.rm = TRUE)\np_value\n\n[1] 0\n\n\n\n\nWe calculate the observed F-stat with infer, which also has a very direct, if verbose, syntax for doing permutation tests:\n\nobserved_infer &lt;- \n  frogs_long %&gt;% \n  specify(Time ~ TempFac) %&gt;% \n  hypothesise(null = \"independence\") %&gt;% \n  calculate(stat = \"F\")\nobserved_infer\n\n\n  \n\n\n\nWe see that the observed F-Statistic is of course \\(385.8966\\) as before. Now we use infer to generate a NULL distribution using permutation of the factor TempFac:\n\nnull_dist_infer &lt;- frogs_long %&gt;% \n  specify(Time ~ TempFac) %&gt;% \n  hypothesise(null = \"independence\") %&gt;% \n  generate(reps = 4999 , type = \"permute\") %&gt;% \n  calculate(stat = \"F\")\n\nhead(null_dist_infer)\n\n\n  \n\n\nnull_dist_infer %&gt;% \n  visualise(method = \"simulation\") +\n  shade_p_value(obs_stat = observed_infer$stat, direction = \"right\") + \n  scale_x_continuous(trans = \"log10\", expand = c(0,0)) +\n  coord_cartesian(xlim = c(0.2,500), clip = \"off\") + \n  annotation_logticks(outside = FALSE) + \n  theme_custom()\n\n\n\n\n\n\n\nAs seen, the infer based permutation test also shows that the permutationally generated F-statistics are nowhere near that which was observed. The effect of TempFac is very strong.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#conclusions",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#conclusions",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n Conclusions",
    "text": "Conclusions\nWe have discussed ANOVA as a means of modelling the effects of a Categorical variable on a Continuous (Quant) variable. ANOVA can be carried out using the standard formula aov when assumptions on distributions, variances, and independence are met. Permutation ANOVA tests can be carried out when these assumptions do not quite hold.\n\n\n\n\n\n\nTwo-Way ANOVA\n\n\n\nWhat if we have two Categorical variables as predictors? We then need to perform a Two-Way ANOVA analysis, where we look at the predictors individually (main effects) and together (interaction effects). Here too, we need to verify if the number of observations are balanced across all combinations of factors of the two Qualitative predictors. There are three different classical approaches (Type1, Type2 and Type3 ANOVA) for testing hypotheses in ANOVA for unbalanced designs, as they are called. (Langsrud 2003).\n\n\n\n\n\n\n\n\nInformative Hypothesis Testing: Models which incorporate a priori Beliefs\n\n\n\nNote that when we specified our research question, we had no specific hypothesis about the means, other than that they might be different. In many situations, we may have reason to believe in the relative “ordering” of the means for different levels of the Categorical variable. The one-sided t-test is the simplest example (e.g., \\(\\mu_1 &gt;= 0\\) and \\(\\mu_1 &gt;= \\mu_2\\)); this readily extends to the multi-parameter setting, where more than one inequality constraint can be imposed on the parameters (e.g., \\(\\mu_1 &lt;= \\mu_2 &lt;= \\mu_3\\).\nIt is possible to incorporate these beliefs into the ANOVA model, using what is called as informative hypothesis testing, which have certain advantages compared to unconstrained models. The R package called restriktor has the capability to develop such models with beliefs.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#sec-references",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#sec-references",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n References",
    "text": "References\n\nThe ANOVA tutorial at Our Coding Club\n\nAntoine Soetewey. How to: one-way ANOVA by hand. https://statsandr.com/blog/how-to-one-way-anova-by-hand/\n\nANOVA in R - Stats and R https://statsandr.com/blog/anova-in-r/\n\nMichael Crawley.(2013) The R Book,second edition. Chapter 11.\n\nDavid C Howell, Permutation Tests for Factorial ANOVA Designs\n\nMarti Anderson, Permutation tests for univariate or multivariate analysis of variance and regression\n\nJudd, Charles M., Gary H. McClelland, and Carey S. Ryan.(2017). “Introduction to Data Analysis.” In, 1–9. Routledge. https://doi.org/10.4324/9781315744131-1.\n\nPatil, I. (2021). Visualizations with statistical details: The ‘ggstatsplot’ approach. Journal of Open Source Software, 6(61), 3167, doi:10.21105/joss.03167\n\nLangsrud, Øyvind. (2003). ANOVA for unbalanced data: Use type II instead of type III sums of squares. Statistics and Computing. 13. 163-167. https://doi.org/10.1023/A:1023260610025. https://www.researchgate.net/publication/220286726_ANOVA_for_unbalanced_data_Use_type_II_instead_of_type_III_sums_of_squares\n\nKim TK. (2017). Understanding one-way ANOVA using conceptual figures. Korean J Anesthesiol. 2017 Feb;70(1):22-26. doi: 10.4097/kjae.2017.70.1.22. Epub 2017 Jan 26. PMID: 28184262; PMCID: PMC5296382.\n\n\nAnova – Type I/II/III SS explained.https://mcfromnz.wordpress.com/2011/03/02/anova-type-iiiiii-ss-explained/\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nDescTools\n0.99.54\nSignorell (2024)\n\n\nggprism\n1.0.5\nDawson (2024)\n\n\nggstatsplot\n0.12.3\nPatil (2021)\n\n\nggtext\n0.1.2\nWilke and Wiernik (2022)\n\n\nrestriktor\n0.5.30\nVanbrabant and Kuiper (2023)\n\n\nsupernova\n3.0.0\nBlake et al. (2024)\n\n\n\n\n\n\nBlake, Adam, Jeff Chrabaszcz, Ji Son, and Jim Stigler. 2024. supernova: Judd, McClelland, & Ryan Formatting for ANOVA Output. https://CRAN.R-project.org/package=supernova.\n\n\nDawson, Charlotte. 2024. ggprism: A “ggplot2” Extension Inspired by “GraphPad Prism”. https://CRAN.R-project.org/package=ggprism.\n\n\nLangsrud, Øyvind. 2003. Statistics and Computing 13 (2): 163–67. https://doi.org/10.1023/a:1023260610025.\n\n\nPatil, Indrajeet. 2021. “Visualizations with statistical details: The ‘ggstatsplot’ approach.” Journal of Open Source Software 6 (61): 3167. https://doi.org/10.21105/joss.03167.\n\n\nSignorell, Andri. 2024. DescTools: Tools for Descriptive Statistics. https://CRAN.R-project.org/package=DescTools.\n\n\nVanbrabant, Leonard, and Rebecca Kuiper. 2023. restriktor: Restricted Statistical Estimation and Inference for Linear Models. https://CRAN.R-project.org/package=restriktor.\n\n\nWilke, Claus O., and Brenton M. Wiernik. 2022. ggtext: Improved Text Rendering Support for “ggplot2”. https://CRAN.R-project.org/package=ggtext.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#footnotes",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/ANOVA.html#footnotes",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "Footnotes",
    "text": "Footnotes\n\nThe ANOVA tutorial at Our Coding Club.↩︎\nSpinoza: Ethics Geometrically Demonstrated: spinoza1665.pdf (earlymoderntexts.com)↩︎\nhttps://www.openintro.org/go/?id=anova-supplement&referrer=/book/ahss/index.php↩︎\nPruim R, Kaplan DT, Horton NJ (2017). “The mosaic Package: Helping Students to ‘Think with Data’ Using R.” The R Journal, 9(1), 77–102. https://journal.r-project.org/archive/2017/RJ-2017-024/index.html.↩︎\nmosaic::xpf() gives both a graph and the probabilities.↩︎\nggplot2 Based Plots with Statistical Details • ggstatsplot https://indrajeetpatil.github.io/ggstatsplot/↩︎\nhttps://github.com/UCLATALL/supernova↩︎\nErnst, Michael D. 2004. “Permutation Methods: A Basis for Exact Inference.” Statistical Science 19 (4): 676–85. doi:10.1214/088342304000000396.↩︎\nPruim R, Kaplan DT, Horton NJ (2017). “The mosaic Package: Helping Students to ‘Think with Data’ Using R.” The R Journal, 9(1), 77–102. https://journal.r-project.org/archive/2017/RJ-2017-024/index.html.↩︎",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/single-prop.html",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/single-prop.html",
    "title": "🃏 Testing a Single Proportion",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(ggformula)\n\n## Datasets from Chihara and Hesterberg's book (Second Edition)\nlibrary(resampledata)\n\n## Datasets from Cetinkaya-Rundel and Hardin's book (First Edition)\nlibrary(openintro)",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🃏 Testing a Single Proportion"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/single-prop.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/single-prop.html#setting-up-r-packages",
    "title": "🃏 Testing a Single Proportion",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(ggformula)\n\n## Datasets from Chihara and Hesterberg's book (Second Edition)\nlibrary(resampledata)\n\n## Datasets from Cetinkaya-Rundel and Hardin's book (First Edition)\nlibrary(openintro)",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🃏 Testing a Single Proportion"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/single-prop.html#introduction",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/single-prop.html#introduction",
    "title": "🃏 Testing a Single Proportion",
    "section": "\n Introduction",
    "text": "Introduction\nWe saw from the diagram created by Allen Downey that there is only one test! We will now use this philosophy to develop a technique that allows us to mechanize several Statistical Models in that way, with nearly identical code.\nWe will use two packages in R, mosaic to develop our intuition for what are called permutation based statistical tests. (There is also a more recent package called infer in R which can do pretty much all of this, including visualization. In my opinion, the code is a little too high-level and does not offer quite the detailed insight that the mosaic package does).",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🃏 Testing a Single Proportion"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/single-prop.html#estimating-a-single-proportion",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/single-prop.html#estimating-a-single-proportion",
    "title": "🃏 Testing a Single Proportion",
    "section": "Estimating a Single Proportion",
    "text": "Estimating a Single Proportion\nVisualizing a Single Proportion\nHypothesis Testing for a Single Proportion\nIntent\nUncertainty in Estimation\nVariance",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🃏 Testing a Single Proportion"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/single-prop.html#permutation-visually-demonstrated",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/single-prop.html#permutation-visually-demonstrated",
    "title": "🃏 Testing a Single Proportion",
    "section": "\n Permutation Visually Demonstrated",
    "text": "Permutation Visually Demonstrated\nWe will look visually at a permutation exercise. We will create dummy data that contains the following case study:\n\nA set of identical resumes was sent to male and female evaluators. The candidates in the resumes were of both genders. We wish to see if there was difference in the way resumes were evaluated, by male and female evaluators. (We use just one male and one female evaluator here, to keep things simple!)\n\n\n\n\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n         M \n-0.3333333 \n\n\n\n\n\n\n\n\n\nSo, we have a solid disparity in percentage of selection between the two evaluators!\n\n Permutation\nNow we pretend that there is no difference between the selections made by either set of evaluators. So we can just:\n\nPool up all the evaluations\n\nArbitrarily re-assign a given candidate(selected or rejected) to either of the two sets of evaluators, by permutation.\n\n\nHow would that pooled shuffled set of evaluations look like?\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\nAs can be seen, the ratio is different!\nWe can now check out our Hypothesis that there is no bias. We can shuffle the data many many times, calculating the ratio each time, and plot the distribution of the differences in selection ratio and see how that artificially created distribution compares with the originally observed figure from Mother Nature.\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nnull_dist &lt;- do(4999) * diff(mean(\n  candidate_selected ~ shuffle(evaluator), \n  data = data))\n# null_dist %&gt;% names()\nnull_dist %&gt;% gf_histogram( ~ M, \n                  fill = ~ (M &lt;= obs_difference), \n                  bins = 25,show.legend = FALSE,\n                  xlab = \"Bias Proportion\", \n                  ylab = \"How Often?\",\n                  title = \"Permutation Test on Difference between Groups\",\n                  subtitle = \"\") %&gt;% \n  gf_vline(xintercept = ~ obs_difference, color = \"red\" ) %&gt;% \n  gf_label(500 ~ obs_difference, label = \"Observed\\n Bias\", \n           show.legend = FALSE) \nmean(~ M&lt;= obs_difference, data = null_dist)\n\n\n\n\n\n\n\n\n\n \n\n\n[1] 0.00220044\n\n\n\nWe see that the artificial data can hardly ever (\\(p = 0.012\\)) mimic what the real world experiment is showing. Hence we had good reason to reject our NULL Hypothesis that there is no bias.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🃏 Testing a Single Proportion"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/single-prop.html#case-study-1-tbd",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/single-prop.html#case-study-1-tbd",
    "title": "🃏 Testing a Single Proportion",
    "section": "\n Case Study #1: TBD",
    "text": "Case Study #1: TBD",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🃏 Testing a Single Proportion"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/single-prop.html#case-study-2-weight-vs-exercise-in-the-yrbss-survey",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/single-prop.html#case-study-2-weight-vs-exercise-in-the-yrbss-survey",
    "title": "🃏 Testing a Single Proportion",
    "section": "\n Case Study #2: Weight vs Exercise in the YRBSS Survey",
    "text": "Case Study #2: Weight vs Exercise in the YRBSS Survey",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🃏 Testing a Single Proportion"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/single-prop.html#an-interactive-app",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/single-prop.html#an-interactive-app",
    "title": "🃏 Testing a Single Proportion",
    "section": "\n An interactive app",
    "text": "An interactive app\nhttps://openintro.shinyapps.io/CLT_prop/",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🃏 Testing a Single Proportion"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/single-prop.html#conclusion",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/single-prop.html#conclusion",
    "title": "🃏 Testing a Single Proportion",
    "section": "\n Conclusion",
    "text": "Conclusion",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🃏 Testing a Single Proportion"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/single-prop.html#references",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/single-prop.html#references",
    "title": "🃏 Testing a Single Proportion",
    "section": "\n References",
    "text": "References\n\nMine Çetinkaya-Rundel and Johanna Hardin, OpenIntro Modern Statistics: Chapter 17\n\nLaura M. Chihara, Tim C. Hesterberg, Mathematical Statistics with Resampling and R. 3 August 2018.© 2019 John Wiley & Sons, Inc.\n\nhttps://iconarray.com/download\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggbrace\n0.1.1\nHuber (2024)\n\n\nopenintro\n2.4.0\nÇetinkaya-Rundel et al. (2022)\n\n\nresampledata\n0.3.1\nChihara and Hesterberg (2018)\n\n\n\n\n\n\nÇetinkaya-Rundel, Mine, David Diez, Andrew Bray, Albert Y. Kim, Ben Baumer, Chester Ismay, Nick Paterno, and Christopher Barr. 2022. openintro: Data Sets and Supplemental Functions from “OpenIntro” Textbooks and Labs. https://CRAN.R-project.org/package=openintro.\n\n\nChihara, Laura M., and Tim C. Hesterberg. 2018. Mathematical Statistics with Resampling and r. 2nd ed. Hoboken, NJ: John Wiley & Sons. https://sites.google.com/site/chiharahesterberg/home.\n\n\nHuber, Nicolas. 2024. ggbrace: Curly Braces for “ggplot2”. https://CRAN.R-project.org/package=ggbrace.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🃏 Testing a Single Proportion"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html",
    "title": "Inference for Two Independent Means",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE, message = TRUE, warning = TRUE, fig.align = \"center\")\nlibrary(tidyverse)\nlibrary(mosaic) # Our go-to package\nlibrary(infer) # An alternative package for inference using tidy data\nlibrary(broom) # Clean test results in tibble form\nlibrary(skimr) # data inspection\n\nlibrary(resampledata) # Datasets from Chihara and Hesterberg's book\nlibrary(openintro) # datasets\nlibrary(gt) # for tables\n\n\n\nflowchart TD\n    A[Inference for Independent Means] --&gt;|Check Assumptions| B[Normality: Shapiro-Wilk Test shapiro.test\\n Variances: Fisher F-test var.test]\n    B --&gt; C{OK?}\n    C --&gt;|Yes, both\\n Parametric| D[t.test]\n    D &lt;--&gt;F[Linear Model\\n Method] \n    C --&gt;|Yes, but not variance\\n Parametric| W[t.test with\\n Welch Correction]\n    W&lt;--&gt;F\n    C --&gt;|No\\n Non-Parametric| E[wilcox.test]\n    E &lt;--&gt; G[Linear Model\\n with\\n Signed-Ranks]\n    C --&gt;|No\\n Non-Parametric| P[Bootstrap\\n or\\n Permutation]\n    P &lt;--&gt; Q[Linear Model\\n with Signed-Rank\\n with Permutation]\n\n\n\n\nflowchart TD\n    A[Inference for Independent Means] --&gt;|Check Assumptions| B[Normality: Shapiro-Wilk Test shapiro.test\\n Variances: Fisher F-test var.test]\n    B --&gt; C{OK?}\n    C --&gt;|Yes, both\\n Parametric| D[t.test]\n    D &lt;--&gt;F[Linear Model\\n Method] \n    C --&gt;|Yes, but not variance\\n Parametric| W[t.test with\\n Welch Correction]\n    W&lt;--&gt;F\n    C --&gt;|No\\n Non-Parametric| E[wilcox.test]\n    E &lt;--&gt; G[Linear Model\\n with\\n Signed-Ranks]\n    C --&gt;|No\\n Non-Parametric| P[Bootstrap\\n or\\n Permutation]\n    P &lt;--&gt; Q[Linear Model\\n with Signed-Rank\\n with Permutation]"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#introduction",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#introduction",
    "title": "Inference for Two Independent Means",
    "section": "",
    "text": "flowchart TD\n    A[Inference for Independent Means] --&gt;|Check Assumptions| B[Normality: Shapiro-Wilk Test shapiro.test\\n Variances: Fisher F-test var.test]\n    B --&gt; C{OK?}\n    C --&gt;|Yes, both\\n Parametric| D[t.test]\n    D &lt;--&gt;F[Linear Model\\n Method] \n    C --&gt;|Yes, but not variance\\n Parametric| W[t.test with\\n Welch Correction]\n    W&lt;--&gt;F\n    C --&gt;|No\\n Non-Parametric| E[wilcox.test]\n    E &lt;--&gt; G[Linear Model\\n with\\n Signed-Ranks]\n    C --&gt;|No\\n Non-Parametric| P[Bootstrap\\n or\\n Permutation]\n    P &lt;--&gt; Q[Linear Model\\n with Signed-Rank\\n with Permutation]\n\n\n\n\nflowchart TD\n    A[Inference for Independent Means] --&gt;|Check Assumptions| B[Normality: Shapiro-Wilk Test shapiro.test\\n Variances: Fisher F-test var.test]\n    B --&gt; C{OK?}\n    C --&gt;|Yes, both\\n Parametric| D[t.test]\n    D &lt;--&gt;F[Linear Model\\n Method] \n    C --&gt;|Yes, but not variance\\n Parametric| W[t.test with\\n Welch Correction]\n    W&lt;--&gt;F\n    C --&gt;|No\\n Non-Parametric| E[wilcox.test]\n    E &lt;--&gt; G[Linear Model\\n with\\n Signed-Ranks]\n    C --&gt;|No\\n Non-Parametric| P[Bootstrap\\n or\\n Permutation]\n    P &lt;--&gt; Q[Linear Model\\n with Signed-Rank\\n with Permutation]"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#inspecting-and-charting-data",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#inspecting-and-charting-data",
    "title": "Inference for Two Independent Means",
    "section": "\n Inspecting and Charting Data",
    "text": "Inspecting and Charting Data\nA.  Check for Normality\nStatistical tests for means usually require a couple of checks1 2:\n\nAre the data normally distributed?\n\nAre the data variances similar?:\n\nLet us also complete a check for normality: the shapiro.wilk test checks whether a Quant variable is from a normal distribution; the NULL hypothesis is that the data are from a normal distribution.\nB.  Check for Variances\n\n\n\n\n\n\nConditions:\n\n\n\n\nThe two variables are not normally distributed.\nThe two variances are also significantly different."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#hypothesis",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#hypothesis",
    "title": "Inference for Two Independent Means",
    "section": "\n Hypothesis",
    "text": "Hypothesis"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#observed-and-test-statistic",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#observed-and-test-statistic",
    "title": "Inference for Two Independent Means",
    "section": "\n Observed and Test Statistic",
    "text": "Observed and Test Statistic"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#inference",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#inference",
    "title": "Inference for Two Independent Means",
    "section": "\n Inference",
    "text": "Inference\n::: panel-tabset\nUsing the Parametric t.test\n\nUsing the non-parametric wilcox.test\n\nUsing the Linear Model Interpretation\nUsing the Permutation Test\nAll Tests Together"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#inspecting-and-charting-data-1",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#inspecting-and-charting-data-1",
    "title": "Inference for Two Independent Means",
    "section": "\n Inspecting and Charting Data",
    "text": "Inspecting and Charting Data\n\ndata(yrbss)\nyrbss\nyrbss_inspect &lt;- inspect(yrbss)\nyrbss_inspect$categorical\nyrbss_inspect$quantitative\n\n\n  \n\n\n  \n\n\n  \n\n\n\nWe have 13K data entries, and with 13 different variables, some Qual and some Quant. Many entries are missing too, typical of real-world data and something we will have to account for in our computations. The meaning of each variable can be found by bringing up the help file. Type this in your console: help(yrbss)\nIn this tutorial, our research question is:\n\n\n\n\n\n\nResearch Question\n\n\n\nDoes weight of highschoolers in this dataset vary with gender?"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#inspecting-and-charting-data-2",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#inspecting-and-charting-data-2",
    "title": "Inference for Two Independent Means",
    "section": "\n Inspecting and Charting Data",
    "text": "Inspecting and Charting Data\nFirst, histograms and densities of the variable we are interested in:\nyrbss_select_gender &lt;- yrbss %&gt;% \n  select(weight, gender, physically_active_7d) %&gt;% \n  drop_na(weight) # Sadly dropping off NA data\n\nyrbss_select_gender %&gt;%\n  gf_density( ~ weight,\n              fill = ~ gender,\n              alpha = 0.5,\n              title = \"Highschoolers' Weights by Gender\") %&gt;%\n  gf_theme(theme_classic())\nyrbss_select_gender %&gt;%\n  gf_boxplot(weight ~ gender,\n              fill = ~ gender,\n              alpha = 0.5,\n              title = \"Highschoolers' Weights by Gender\") %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverlapped Distribution plot shows some difference in the means; and the Boxplots show visible difference in the medians.\nA.  Check for Normality\nAs stated before, statistical tests for means usually require a couple of checks:\n\nAre the data normally distributed?\n\nAre the data variances similar?\n\nLet us also complete a visual check for normality,with plots since we cannot do a shapiro.test:\n\n\n\n\n\n\nShapiro-Wilks Test\n\n\n\nThe longest data it can take (in R) is 5000. Since our data is longer, we will cannot use this procedure and have to resort to visual means.\n\n\nmale_student_weights &lt;- yrbss_select_gender %&gt;% filter(gender == \"male\") %&gt;% select(weight)\nfemale_student_weights &lt;- yrbss_select_gender %&gt;% filter(gender == \"female\") %&gt;% select(weight)\n#shapiro.test(male_student_weights$weight)\n#shapiro.test(female_student_weights$weight)\n\nyrbss_select_gender %&gt;%\n  gf_density( ~ weight,\n              fill = ~ gender,\n              alpha = 0.5,\n              title = \"Highschoolers' Weights by Gender\") %&gt;%\n  gf_facet_grid(~ gender) %&gt;% \n  gf_fitdistr(dist = \"dnorm\") %&gt;% \n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\n\n\nDistributions are not too close to normal…perhaps a hint of a rightward skew, suggesting that there are some obese students.\nWe can plot Q-Q plots3 for both variables, and also compare both data with normally-distributed data generated with the same means and standard deviations:\nyrbss_select_gender %&gt;% \n  gf_qq(~ weight | gender) %&gt;% \n  gf_qqline(ylab = \"scores\") %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\n\n\nNo real evidence (visually) of the variables being normally distributed.\nB.  Check for Variances\nLet us check if the two variables have similar variances: the var.test does this for us, with a NULL hypothesis that the variances are not significantly different:\n\nvar.test( weight ~  gender, data = yrbss_select_gender, \n          conf.int = TRUE,\n          conf.level = 0.95) %&gt;% \n  broom::tidy()\n\n#qf(0.975,6164, 6413)\n\n\n  \n\n\n\nThe p.value being so small, we are able to reject the NULL Hypothesis that the variances of weight are nearly equal across the two exercise regimes.\n\n\n\n\n\n\nConditions\n\n\n\n\nThe two variables are not normally distributed.\nThe two variances are also significantly different.\n\n\n\nThis means that the parametric t.test must be eschewed in favour of the non-parametric wilcox.test. We will use that, and also attempt linear models with rank data, and a final permutation test."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#hypothesis-1",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#hypothesis-1",
    "title": "Inference for Two Independent Means",
    "section": "\n Hypothesis",
    "text": "Hypothesis\nBased on the graphs, how would we formulate our Hypothesis? We wish to infer whether there is difference in mean weight across gender. So accordingly:\n\\[\nH_0: \\mu_{male} = \\mu_{female}\\\\\n\\\\\\\nH_a: \\mu_{male} \\ne \\mu_{female}\\\n\\]"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#observed-and-test-statistic-1",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#observed-and-test-statistic-1",
    "title": "Inference for Two Independent Means",
    "section": "\n Observed and Test Statistic",
    "text": "Observed and Test Statistic\nWhat would be the test statistic we would use? The difference in means. Is the observed difference in the means between the two groups of scores non-zero? We use the diffmean function, from mosaic:\nobs_diff_gender &lt;- diffmean(weight ~ gender, data = yrbss_select_gender) \n\nobs_diff_gender\n\n\n\ndiffmean \n11.70089"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#inference-1",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#inference-1",
    "title": "Inference for Two Independent Means",
    "section": "\n Inference",
    "text": "Inference\nType help(wilcox.test) in your Console.\n\nUsing the wilcox.test\nUsing the Linear Model\nUsing the Permutation Test\n\n\n\nSince the data variables do not satisfy the assumption of being normally distributed, and the variances are significantly different, we use the classical wilcox.test, which implements what we need here: the Mann-Whitney U test:4\n\nThe Mann-Whitney test as a test of mean ranks. It first ranks all your values from high to low, computes the mean rank in each group, and then computes the probability that random shuffling of those values between two groups would end up with the mean ranks as far apart as, or further apart, than you observed. No assumptions about distributions are needed so far. (emphasis mine)\n\nWe will use the mosaic variant).  Our model would be:\n\\[\nmean(rank(Weight_{male})) - mean(rank(Weight_{female})) =\n\\beta_0\n\\\\\\\nH_0: \\beta_0 = 0;\\\\\n\\\\\\\nH_a: \\beta_0 \\ne 0\n\\]\n\nwilcox.test(weight ~ gender, data = yrbss_select_gender, \n            conf.int = TRUE, \n            conf.level = 0.95) %&gt;% \n  broom::tidy()\n\n\n  \n\n\n\nThe p.value is negligible and we are able to reject the NULL hypothesis that the means are equal.\n\n\nWe can apply the linear-model-as-inference interpretation to the ranked data data to implement the non-parametric test as a Linear Model:\n\\[\nlm(rank(weight) \\sim  gender) = \\beta_0 + \\beta_1 * gender\n\\\\\nH_0: \\beta_1 = 0\\\\\n\\\\\\\nH_a: \\beta_1 \\ne 0\\\\\n\\]\n\n# Create a sign-rank function\n#signed_rank &lt;- function(x) {sign(x) * rank(abs(x))}\n\nlm(rank(weight) ~ gender, \n   data = yrbss_select_gender) %&gt;% \n  broom::tidy(conf.int = TRUE,\n              conf.level = 0.95)\n\n\n  \n\n\n\n\n\n\n\n\n\nDummy Variables in lm\n\n\n\nNote how the Qual variable was used here in Linear Regression! The gender variable was treated as a binary “dummy” variable5.\n\n\n\n\nWe saw from the diagram created by Allen Downey that there is only one test6! We will now use this philosophy to develop a technique that allows us to mechanize several Statistical Models in that way, with nearly identical code. For the specific data at hand, we need to shuffle the records between Semifinal and Final on a per Swimmer basis and take the test statistic (difference between the two swim records for each swimmer). Another way to look at this is to take the differences between Semifinal and Final scores and shuffle the differences to either polarity. We will follow this method in the code below:\nnull_dist_weight &lt;- \n  do(9999) * diffmean(data = yrbss_select_gender, weight ~ shuffle(gender))\nnull_dist_weight\ngf_histogram(data = null_dist_weight, ~ diffmean, bins = 25) %&gt;%\n  gf_vline(xintercept = obs_diff_gender, colour = \"red\") %&gt;% \n  gf_theme(theme_classic())\ngf_ecdf(data = null_dist_weight, ~ diffmean) %&gt;%\n  gf_vline(xintercept = obs_diff_gender, colour = \"red\")  %&gt;% \n  gf_theme(theme_classic())\nprop1(~ diffmean &lt;= obs_diff_gender, data = null_dist_weight)\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprop_TRUE \n        1 \n\n\n\nClearly the observed_diff_weight is much beyond anything we can generate with permutations with gender! And hence there is a significant difference in weights across gender!\n\n\n\nAll Tests Together\nWe can put all the test results together to get a few more insights about the tests:\n\nwilcox.test(weight ~ gender, data = yrbss_select_gender, \n            conf.int = TRUE, \n            conf.level = 0.95) %&gt;% \n  broom::tidy() %&gt;% \n  gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"cyan\"), cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)) %&gt;% \n  tab_header(title = \"wilcox.test\")\n\nlm(rank(weight) ~ gender, \n   data = yrbss_select_gender) %&gt;% \n  broom::tidy(conf.int = TRUE,\n              conf.level = 0.95) %&gt;% \n  gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"cyan\"),cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)) %&gt;% \n  tab_header(title = \"Linear Model with Ranked Data\")\n\n\n\n\n\n\n\nwilcox.test\n\n\nestimate\nstatistic\np.value\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n-11.33999\n10808212\n0\n-11.34003\n-10.87994\nWilcoxon rank sum test with continuity correction\ntwo.sided\n\n\n\n\n\n\n\n\nLinear Model with Ranked Data\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n4836.157\n42.52745\n113.71848\n0\n4752.797\n4919.517\n\n\ngendermale\n2851.246\n59.55633\n47.87478\n0\n2734.507\n2967.986"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#footnotes",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#footnotes",
    "title": "Inference for Two Independent Means",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://stats.stackexchange.com/questions/2492/is-normality-testing-essentially-useless↩︎\nhttps://www.allendowney.com/blog/2023/01/28/never-test-for-normality/↩︎\nhttps://stats.stackexchange.com/questions/92374/testing-large-dataset-for-normality-how-and-is-it-reliable↩︎\nhttps://stats.stackexchange.com/q/113337↩︎\nhttps://en.wikipedia.org/wiki/Dummy_variable_(statistics)↩︎\nhttps://allendowney.blogspot.com/2016/06/there-is-still-only-one-test.html↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#inspecting-and-charting-data-3",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#inspecting-and-charting-data-3",
    "title": "Inference for Two Independent Means",
    "section": "\n Inspecting and Charting Data",
    "text": "Inspecting and Charting Data\nWe can make distribution plots for weight by physical_3plus:\n\ngf_boxplot(weight ~ physical_3plus, \n          fill = ~ physical_3plus,\n          data = yrbss_select_phy, xlab = \"Days of Exercise &gt;=3 \") %&gt;% \n  gf_theme(theme_classic())\n\n\n\n\n\n\ngf_density(~ weight,\n          fill = ~ physical_3plus,\n          data = yrbss_select_phy) %&gt;% \n  gf_theme(theme_classic())\n\n\n\n\n\n\n\nThe box plots show how the medians of the two distributions compare, but we can also compare the means of the distributions using the following to first group the data by the physical_3plus variable, and then calculate the mean weight in these groups using the mean function while ignoring missing values by setting the na.rm argument to TRUE.\n\nyrbss_select_phy %&gt;%\n  group_by(physical_3plus) %&gt;%\n  summarise(mean_weight = mean(weight, na.rm = TRUE))\n\n\n  \n\n\n\nThere is an observed difference, but is this difference large enough to deem it “statistically significant”? In order to answer this question we will conduct a hypothesis test. But before that a few more checks on the data:\nA.  Check for Normality\nAs stated before, statistical tests for means usually require a couple of checks:\n\nAre the data normally distributed?\n\nAre the data variances similar?\n\nLet us also complete a visual check for normality,with plots since we cannot do a shapiro.test:\nyrbss_select_phy %&gt;%\n  gf_density( ~ weight,\n              fill = ~ physical_3plus,\n              alpha = 0.5,\n              title = \"Highschoolers' Weights by Exercise Frequency\") %&gt;%\n  gf_facet_grid(~ physical_3plus) %&gt;% \n  gf_fitdistr(dist = \"dnorm\") %&gt;% \n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\n\n\nAgain, not normally distributed…\nWe can plot Q-Q plots for both variables, and also compare both data with normally-distributed data generated with the same means and standard deviations:\nyrbss_select_phy %&gt;% \n  gf_qq(~ weight | physical_3plus , color = ~ physical_3plus) %&gt;% \n  gf_qqline(ylab = \"Weight\") %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\n\n\nThe QQ-plots confirm that he tow data variables are not normally distributed.\nB.  Check for Variances\nLet us check if the two variables have similar variances: the var.test does this for us, with a NULL hypothesis that the variances are not significantly different:\nvar.test( weight ~ physical_3plus, data = yrbss_select_phy, \n          conf.int = TRUE,\n          conf.level = 0.95) %&gt;% \n  broom::tidy()\n\n# Critical F value\nqf(0.975,4021, 8341)\n\n\n\n\n  \n\n\n\n[1] 1.054398\n\n\n\nThe p.value states the probability of the data being what it is, assuming the NULL hypothesis that variances were similar. It being so small, we are able to reject this NULL Hypothesis that the variances of weight are nearly equal across the two exercise frequencies. (Compare the statistic in the var.test with the critical F-value)\n\n\n\n\n\n\nConditions\n\n\n\n\nThe two variables are not normally distributed.\nThe two variances are also significantly different.\n\n\n\nHence we will have to use non-parametric tests to infer if the means are similar."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#hypothesis-2",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#hypothesis-2",
    "title": "Inference for Two Independent Means",
    "section": "\n Hypothesis",
    "text": "Hypothesis\nBased on the graphs, how would we formulate our Hypothesis? We wish to infer whether there is difference in mean weight across physical_3plus. So accordingly:\n\\[\nH_0: \\mu_{physical-3plus-Yes} = \\mu_{physical-3plus-No}\\\\\n\\\\\\\nH_a: \\mu_{physical-3plus-Yes} \\ne \\mu_{physical-3plus-No}\\\\\n\\]"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#observed-and-test",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#observed-and-test",
    "title": "Inference for Two Independent Means",
    "section": "\n Observed and Test",
    "text": "Observed and Test\nStatistic\nWhat would be the test statistic we would use? The difference in means. Is the observed difference in the means between the two groups of scores non-zero? We use the diffmean function, from mosaic:\nobs_diff_phy &lt;- diffmean(weight ~ physical_3plus, data = yrbss_select_phy) \n\nobs_diff_phy\n\n\n\n diffmean \n-1.774584 \n\n\n\n\n Inference\n::: panel-tabset\nUsing parametric t.test\n\nWell, the variables are not normally distributed, and the variances are significantly different so a standard t.test is not advised. We can still try:\n\nmosaic::t_test(weight ~ physical_3plus,\n               var.equal = FALSE, # Welch Correction\n               data = yrbss_select_phy) %&gt;% \n  broom::tidy()\n\n\n  \n\n\n\nThe p.value is \\(8.9e-08\\) ! And the Confidence Interval is clear of \\(0\\). So the t.test gives us good reason to reject the Null Hypothesis that the means are similar. But can we really believe this, given the non-normality of data?\nUsing non-parametric paired Wilcoxon test\nHowever, we have seen that the data variables are not normally distributed. So a Wilcoxon Test, using signed-ranks, is indicated: (recall the model!)\n\n# For stability reasons, it may be advisable to use rounded data or to set digits.rank = 7, say, \n# such that determination of ties does not depend on very small numeric differences (see the example).\n\nwilcox.test(weight ~ physical_3plus,\n            conf.int = TRUE,\n            conf.level = 0.95,\n            data = yrbss_select_phy) %&gt;% \n  broom::tidy()\n\n\n  \n\n\n\nThe nonparametric wilcox.test also suggests that the means for weight across physical_3plus are significantly different.\nUsing the Linear Model Interpretation\nWe can apply the linear-model-as-inference interpretation to the ranked data data to implement the non-parametric test as a Linear Model:\n\\[\nlm(rank(weight) \\sim  physical.3plus) = \\beta_0 + \\beta_1 \\times physical.3plus\n\\\\\nH_0: \\beta_1 = 0\\\\\n\\\\\\\nH_a: \\beta_1 \\ne 0\\\\\n\\]\n\nlm(rank(weight) ~ physical_3plus, \n   data = yrbss_select_phy) %&gt;% \n  broom::tidy(conf.int = TRUE,\n              conf.level = 0.95)\n\n\n  \n\n\n\nHere too, the linear model using rank data arrives at a conclusion similar to that of the Mann-Whitney U test.\nUsing Permutation Tests\nWe will do this in two ways, just for fun: one using mosaic and the other using infer.\nBut first, we need to initialize the test, which we will save as obs_diff.\nobs_diff_infer &lt;- yrbss_select_phy %&gt;%\n  infer::specify(weight ~ physical_3plus) %&gt;%\n  infer::calculate(stat = \"diff in means\", order = c(\"yes\", \"no\"))\nobs_diff_infer\nobs_diff_mosaic &lt;- mosaic::diffmean(~ weight | physical_3plus, data = yrbss_select_phy)\nobs_diff_mosaic\nobs_diff_phy\n\n\n\n\n  \n\n\n\n diffmean \n-1.774584 \n\n\n diffmean \n-1.774584 \n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that obs_diff_infer is a 1 X 1 dataframe; obs_diff_mosaic is a scalar!!\n\n\n\nInference Using mosaic\n\n\nWe already have the observed difference, obs_diff_mosaic. Now we generate the null distribution using permutation, with mosaic:\n\nnull_dist_mosaic &lt;- do(999) * diffmean(~ weight | shuffle(physical_3plus), data = yrbss_select_phy)\n\nWe can also generate the histogram of the null distribution, compare that with the observed diffrence and compute the p-value and confidence intervals:\n\ngf_histogram(~ diffmean, data = null_dist_mosaic) %&gt;% \n  gf_vline(xintercept = obs_diff_mosaic, colour = \"red\")\n\n\n\n\n\n\n# p-value\nprop(~ diffmean != obs_diff_mosaic, data = null_dist_mosaic)\n\nprop_TRUE \n        1 \n\n# Confidence Intervals for p = 0.95\nmosaic::cdata(~ diffmean, p = 0.95, data = null_dist_mosaic)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#your-turn",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#your-turn",
    "title": "Inference for Two Independent Means",
    "section": "Your Turn",
    "text": "Your Turn\n\nCalculate a 95% confidence interval for the average height in meters (height) and interpret it in context.\nCalculate a new confidence interval for the same parameter at the 90% confidence level. Comment on the width of this interval versus the one obtained in the previous exercise.\nConduct a hypothesis test evaluating whether the average height is different for those who exercise at least three times a week and those who don’t.\nNow, a non-inference task: Determine the number of different options there are in the dataset for the hours_tv_per_school_day there are.\nCome up with a research question evaluating the relationship between height or weight and sleep. Formulate the question in a way that it can be answered using a hypothesis test and/or a confidence interval. Report the statistical results, and also provide an explanation in plain language. Be sure to check all assumptions, state your \\(\\alpha\\) level, and conclude in context."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/files/sampling-tutorial.html",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/files/sampling-tutorial.html",
    "title": "Sampling",
    "section": "",
    "text": "Continuing to treat the NHANES dataset as a population, We will try to replicate the process of sampling and CLT for another variable in the NHANES variable, AlcoholYear.\n\n\nTry sample sizes of 25, 50, 100, 500.\n\n\n\nWrite your observations here!"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/files/sampling-tutorial.html#sampling-alcoholyear",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/files/sampling-tutorial.html#sampling-alcoholyear",
    "title": "Sampling",
    "section": "",
    "text": "Try sample sizes of 25, 50, 100, 500."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/files/sampling-tutorial.html#conclusion",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/files/sampling-tutorial.html#conclusion",
    "title": "Sampling",
    "section": "",
    "text": "Write your observations here!"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/10-Intro/intro-inference.html#introduction",
    "href": "content/courses/Analytics/Inference/Modules/10-Intro/intro-inference.html#introduction",
    "title": "🧭 Basics of Statistical Inference",
    "section": " Introduction",
    "text": "Introduction\nIn this set of modules we will explore Data, understand what types of data variables there are, and the kinds of statistical tests and visualizations we can create with them.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🧭 Basics of Statistical Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/10-Intro/intro-inference.html#the-big-ideas-in-stats",
    "href": "content/courses/Analytics/Inference/Modules/10-Intro/intro-inference.html#the-big-ideas-in-stats",
    "title": "🧭 Basics of Statistical Inference",
    "section": "The Big Ideas in Stats",
    "text": "The Big Ideas in Stats\nSteven Stigler(Stigler 2016) is the author of the book “The Seven Pillars of Statistical Wisdom”. The Big Ideas in Statistics from that book are:\n\nAggregation\n\nThe first pillar I will call Aggregation, although it could just as well be given the nineteenth-century name, “The Combination of Observations,” or even reduced to the simplest example, taking a mean. Those simple names are misleading, in that I refer to an idea that is now old but was truly revolutionary in an earlier day—and it still is so today, whenever it reaches into a new area of application. How is it revolutionary? By stipulating that, given a number of observations, you can actually gain information by throwing information away! In taking a simple arithmetic mean, we discard the individuality of the measures, subsuming them to one summary.\n\n\n\nInformation\n\nIn the early eighteenth century it was discovered that in many situations the amount of information in a set of data was only proportional to the square root of the number n of observations, not the number n itself.\n\n\n\nLikelihood\n\nBy the name I give to the third pillar, Likelihood, I mean the calibration of inferences with the use of probability. The simplest form for this is in significance testing and the common P-value.\n\n\n\nIntercomparison\n\nIt represents what was also once a radical idea and is now commonplace: that statistical comparisons do not need to be made with respect to an exterior standard but can often be made in terms interior to the data themselves. The most commonly encountered examples of intercomparisons are Student’s t-tests and the tests of the analysis of variance.\n\n\n\nRegression\n\nI call the fifth pillar Regression, after Galton’s revelation of 1885, explained in terms of the bivariate normal distribution. Galton arrived at this by attempting to devise a mathematical framework for Charles Darwin’s theory of natural selection, overcoming what appeared to Galton to be an intrinsic contradiction in the theory: selection required increasing diversity, in contradiction to the appearance of the population stability needed for the definition of species.\n\n\n\nDesign of Experiments and Observations\n\nThe sixth pillar is Design, as in “Design of Experiments,” but conceived of more broadly, as an ideal that can discipline our thinking in even observational settings.Starting in the late nineteenth century, a new understanding of the topic appeared, as Charles S. Peirce and then Fisher discovered the extraordinary role randomization could play in inference.\n\n\n\nResiduals\n\nThe most common appearances in Statistics are our model diagnostics (plotting residuals), but more important is the way we explore high-dimensional spaces by fitting and comparing nested models.\n\nIn our work with Statistical Models, we will be working with all except Idea 6 above.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🧭 Basics of Statistical Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/10-Intro/intro-inference.html#what-is-a-statistical-model",
    "href": "content/courses/Analytics/Inference/Modules/10-Intro/intro-inference.html#what-is-a-statistical-model",
    "title": "🧭 Basics of Statistical Inference",
    "section": "What is a Statistical Model?",
    "text": "What is a Statistical Model?\nFrom Daniel Kaplan’s book:\n\n“Modeling” is a process of asking questions. “Statistical” refers in part to data – the statistical models you will construct will be rooted in data. But it refers also to a distinctively modern idea: that you can measure what you don’t know and that doing so contributes to your understanding.\n\nThe conclusions you reach from data depend on the specific questions you ask. The word “modeling” highlights that your goals, your beliefs, and your current state of knowledge all influence your analysis of data. You examine your data to see whether they are consistent with the hypotheses that frame your understanding of the system under study.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🧭 Basics of Statistical Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/10-Intro/intro-inference.html#types-of-statistical-models-based-on-purpose",
    "href": "content/courses/Analytics/Inference/Modules/10-Intro/intro-inference.html#types-of-statistical-models-based-on-purpose",
    "title": "🧭 Basics of Statistical Inference",
    "section": "Types of Statistical Models Based on Purpose",
    "text": "Types of Statistical Models Based on Purpose\nThere are three main uses for statistical models. They are closely related, but distinct enough to be worth enumerating.\nDescription. Sometimes you want to describe the range or typical values of a quantity. For example, what’s a “normal” white blood cell count? Sometimes you want to describe the relationship between things. Example: What’s the relationship between the price of gasoline and consumption by automobiles?\nClassification or Prediction. You often have information about some observable traits, qualities, or attributes of a system you observe and want to draw conclusions about other things that you can’t directly observe. For instance, you know a patient’s white blood-cell count and other laboratory measurements and want to diagnose the patient’s illness.\nAnticipating the consequences of interventions. Here, you intend to do something: you are not merely an observer but an active participant in the system. For example, people involved in setting or debating public policy have to deal with questions like these: To what extent will increasing the tax on gasoline reduce consumption? To what extent will paying teachers more increase student performance?\nThe appropriate form of a model depends on the purpose. For example, a model that diagnoses a patient as ill based on an observation of a high number of white blood cells can be sensible and useful. But that same model could give absurd predictions about intervention: Do you really think that lowering the white blood cell count by bleeding a patient will make the patient better?\nTo anticipate correctly the effects of an intervention you need to get the direction of cause (polarity) and effect (magnitude) correct in your models.\n\n\n\n\n\n\nNote\n\n\n\nAn effect size tells how the output of a model changes when a simple change is made to the input.Effect sizes always involve two variables: a response variable and a single explanatory variable. Effect size is always about a model. The model might have one explanatory variable or many explanatory variables. Each explanatory variable will have its own effect size, so a model with multiple explanatory variables will have multiple effect sizes.\n\n\n\n\n\n\n\n\nNote\n\n\n\nBut for a model used for classification or prediction, it may be unnecessary to represent causation correctly. Instead, other issues, e.g., the reliability of data, can be the most important. One of the thorniest issues in statistical modeling – with tremendous consequences for science, medicine, government, and commerce – is how you can legitimately draw conclusions about interventions from models based on data collected without performing these interventions.\n\n\n\nTypes of Models Based on Data Variables\nLet us look at the famous dataset pertaining to Francis Galton’s work on the heights of children and the heights of their parents. We can create 4 kinds of models based on the types of variables in that dataset.\n\n\n\nVariables and Models",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🧭 Basics of Statistical Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/10-Intro/intro-inference.html#linear-models-everywhere",
    "href": "content/courses/Analytics/Inference/Modules/10-Intro/intro-inference.html#linear-models-everywhere",
    "title": "🧭 Basics of Statistical Inference",
    "section": "Linear Models Everywhere",
    "text": "Linear Models Everywhere\nOur method in this set of modules is to take the modern view that all these models can be viewed from a standpoint of the Linear Model, also called Linear Regression \\(y = \\beta_1 *x + \\beta_0\\) . For example, it is relatively straightforward to imagine Plot B (Quant vs Quant ) as an example of a Linear Model, with the dependent variable modelled as \\(y\\) and the independent one as \\(x\\). We will try to work up to the intuition that this model can be used to understand all the models in the Figure.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🧭 Basics of Statistical Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/10-Intro/intro-inference.html#a-flowchart-of-statistical-inference-tests",
    "href": "content/courses/Analytics/Inference/Modules/10-Intro/intro-inference.html#a-flowchart-of-statistical-inference-tests",
    "title": "🧭 Basics of Statistical Inference",
    "section": "A Flowchart of Statistical Inference Tests",
    "text": "A Flowchart of Statistical Inference Tests\n\n\n\n\n\nflowchart TD\n    A[Inference for Means] --&gt;|Check Assumptions|B[Normality: Shapiro-Wilk Test shapiro.test\\n Variances: Fisher F-test var.test\\n Outliers: Box Plots]\n    B --&gt; M[Means]\n\n subgraph Means\n    direction TB\n      subgraph Single-Mean\n        direction LR\n        OM[Single Mean]&lt;--&gt;|p| TT[t.test]\n        TWT[t.test \\n Welch]&lt;--&gt;|p diff var|OM[Single Mean]\n        WT[wilcox.test]&lt;--&gt;|np|OM[Single Mean]\n      end\n      \n      subgraph Paired-Means\n        direction LR\n        TM[Paired Means]&lt;--&gt;|p| TTP[t.test with pairs]\n        WTP[wilcox.test with pairs\\n Mann-Whitney U Test]--&gt;|np|TM\n      end\n      \n      subgraph Multiple-Means\n        direction LR\n        MM[Multiple Means] --&gt;|p| ANO[ANOVA]\n        KW[kruskal.test]&lt;--&gt;|np indep| MM\n        FT[friedman.test]&lt;--&gt;|np dep| MM\n      end\n      \nM --&gt;Single-Mean\nSingle-Mean--&gt;Paired-Means\nPaired-Means--&gt;Multiple-Means\n\nend\n\n%%subgraph LM\n%%  direction BT\n%%  LM[Linear Model]--&gt;Means\n%%end",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🧭 Basics of Statistical Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/10-Intro/intro-inference.html#references",
    "href": "content/courses/Analytics/Inference/Modules/10-Intro/intro-inference.html#references",
    "title": "🧭 Basics of Statistical Inference",
    "section": "References",
    "text": "References\n\nhttp://drafts.jsvine.com/the-magic-criteria/\nTihamér von Ghyczy, The Fruitful Flaws of Strategy Metaphors. Harvard Business Review, 2003. https://hbr.org/2003/09/the-fruitful-flaws-of-strategy-metaphors\nDaniel T. Kaplan, Statistical Models (second edition). Available online. https://dtkaplan.github.io/SM2-bookdown/\nDaniel T. Kaplan, Compact Introduction to Classical Inference, 2020. Available Online. https://dtkaplan.github.io/CompactInference/\nDaniel T. Kaplan and Frank Shaw, Statistical Modeling: Computational Technique. Available online https://www.mosaic-web.org/go/SM2-technique/\nJonas Kristoffer Lindeløv, Common statistical tests are linear models (or: how to teach stats) https://lindeloev.github.io/tests-as-linear/",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🧭 Basics of Statistical Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/paired-means-tutorial.html",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/paired-means-tutorial.html",
    "title": "Tutorial on Inference for Two Paired Means",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\n\nlibrary(resampledata)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/paired-means-tutorial.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/paired-means-tutorial.html#setting-up-r-packages",
    "title": "Tutorial on Inference for Two Paired Means",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\n\nlibrary(resampledata)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/paired-means-tutorial.html#case-study-1-icecream",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/paired-means-tutorial.html#case-study-1-icecream",
    "title": "Tutorial on Inference for Two Paired Means",
    "section": "\n Case Study-1: IceCream!!",
    "text": "Case Study-1: IceCream!!\nWhat is there to not like about icecreams!! Here is a dataset that has data on Sugar and Calories between Vanilla and Chocolate icecreams, across several brands of icecreams. Is this a sample of paired data? Let us check:\n\n Inspecting and Charting Data\ndata(\"IceCream\")\nIceCream\ninspect(IceCream)\n\n\n\n\n  \n\n\n\n\n\n\ncategorical variables:  \n   name  class levels  n missing                                  distribution\n1 Brand factor     39 39       0 Baskin Robbins (2.6%) ...                    \n\nquantitative variables:  \n               name   class   min    Q1 median    Q3 max      mean        sd  n\n1   VanillaCalories integer 120.0 140.0    160 240.0 307 191.41026 58.644207 39\n2        VanillaFat numeric   4.5   7.5      9  15.5  21  11.28718  4.431655 39\n3      VanillaSugar numeric  10.0  12.5     17  21.0  27  17.13077  4.841333 39\n4 ChocolateCalories integer 120.0 140.0    170 260.0 320 198.74359 63.063342 39\n5      ChocolateFat numeric   5.0   7.5      9  14.7  21  11.12051  4.597378 39\n6    ChocolateSugar numeric  12.0  15.0     18  22.3  33  18.97436  5.402812 39\n  missing\n1       0\n2       0\n3       0\n4       0\n5       0\n6       0\n\n\n\nHmm…the data are about calories, fat, and sugar between two flavours of icecream sold by each brand. There are 39 brands.\nLet us plot the data first:\nIceCream %&gt;% \n  gf_col(fct_reorder(Brand, VanillaCalories) ~ VanillaCalories, \n         fill = \"red\") %&gt;% \n  gf_col(fct_reorder(Brand, VanillaCalories) ~ - ChocolateCalories, \n         fill = \"green\",\n         xlab = \"Calories\", ylab = \"Brand\", \n         title = \"Calories across Icecream Brands\",\n         subtitle = \"Vanilla = Red, Green = Chocolate\") %&gt;% \n  gf_theme(theme_classic())\nIceCream %&gt;% \n  gf_col(fct_reorder(Brand, VanillaFat) ~ VanillaFat, \n         fill = \"red\") %&gt;% \n  gf_col(fct_reorder(Brand, VanillaFat) ~ - ChocolateFat, \n         fill = \"green\",\n         xlab = \"Fat\", ylab = \"Brand\", \n         title = \"Calories across Icecream Brands\",\n         subtitle = \"Vanilla = Red, Green = Chocolate\") %&gt;% \n  gf_theme(theme_classic())\nIceCream %&gt;% \n  gf_col(fct_reorder(Brand, VanillaSugar) ~ VanillaSugar, \n         fill = \"red\") %&gt;% \n  gf_col(fct_reorder(Brand, VanillaSugar) ~ - ChocolateSugar, \n                  fill = \"green\",\n         xlab = \"Sugar\", ylab = \"Brand\", \n         title = \"Calories across Icecream Brands\",\n         subtitle = \"Vanilla = Red, Green = Chocolate\") %&gt;% \n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe may hypothesize that say, the fat content in the two flavours might be similar on a per brand basis. That is, if say Baskin Robbins has high sugar in the vanilla flavour, it is likely to have high sugar also in its chocolate flavour.\nLet us see what are the observed differences in the mean values of calories, sugar, and fat across brands:\n\nIceCream %&gt;% \n  mutate(diff_calories = VanillaCalories - ChocolateCalories,\n         diff_fat = VanillaFat - ChocolateFat,\n         diff_sugar = VanillaSugar - ChocolateSugar) %&gt;% \n  summarise(mean_diff_calories = mean(diff_calories),\n            mean_diff_fat = mean(diff_fat),\n            mean_diff_sugar = mean(diff_sugar))\n\n\n  \n\n\n\nHmm…while the numbers showing difference in means are quite different, we need to perform tests to infer whether these difference are statistically significant.\n\n Hypothesis\nHow do we specify our Hypotheses? (Of course, there is more than one!)\nWrite the Null and Alternate hypotheses here.\n\n Null Distribution Computations\nHow do we compute the NULL distributions, for each of the three components of the ice creams, using pair-wise analysis?"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/paired-means-tutorial.html#conclusions",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/paired-means-tutorial.html#conclusions",
    "title": "Tutorial on Inference for Two Paired Means",
    "section": "\n Conclusions",
    "text": "Conclusions\nSo are there significant differences in sugar, fat, and calorie content across the two flavours?\nIs this conclusion different if you don’t use paired-data, and just treat the data as independent readings?"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/60-SimTest/files/sim-tutorial.html",
    "href": "content/courses/Analytics/Inference/Modules/60-SimTest/files/sim-tutorial.html",
    "title": "Simulation",
    "section": "",
    "text": "In this module we will use simulation to solve several problems in Business Decision Making."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html",
    "title": "🃏 Permutation Test for Two Proportions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(ggmosaic) # plotting mosaic plots for Categorical Data\n\n### Dataset from Chihara and Hesterberg's book (Second Edition)\nlibrary(resampledata)\nlibrary(explore)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#setting-up-the-packages",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#setting-up-the-packages",
    "title": "🃏 Permutation Test for Two Proportions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(ggmosaic) # plotting mosaic plots for Categorical Data\n\n### Dataset from Chihara and Hesterberg's book (Second Edition)\nlibrary(resampledata)\nlibrary(explore)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#introduction",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#introduction",
    "title": "🃏 Permutation Test for Two Proportions",
    "section": "Introduction",
    "text": "Introduction\nWe saw from the diagram created by Allen Downey that there is only one test! We will now use this philosophy to develop a technique that allows us to mechanize several Statistical Models in that way, with nearly identical code.\nWe will use two packages in R, mosaic and the relatively new infer package, to develop our intuition for what are called permutation based statistical tests."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#testing-for-two-or-more-proportions",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#testing-for-two-or-more-proportions",
    "title": "🃏 Permutation Test for Two Proportions",
    "section": "Testing for Two or More Proportions",
    "text": "Testing for Two or More Proportions\nLet us try a dataset with Qualitative / Categorical data. This is the General Social Survey GSS dataset, and we have people with different levels of Education stating their opinion on the Death Penalty. We want to know if these two Categorical variables have a correlation, i.e. can the opinions in favour of the Death Penalty be explained by the Education level?\nSince data is Categorical ( both variables ), we need to take counts in a table, and then implement a chi-square test. In the test, we will permute the Education variable to see if we can see how significant its effect size is.\n\ndata(GSS2002)\ninspect(GSS2002)\n\n\ncategorical variables:  \n            name  class levels    n missing\n1         Region factor      7 2765       0\n2         Gender factor      2 2765       0\n3           Race factor      3 2765       0\n4      Education factor      5 2760       5\n5        Marital factor      5 2765       0\n6       Religion factor     13 2746      19\n7          Happy factor      3 1369    1396\n8         Income factor     24 1875     890\n9       PolParty factor      8 2729      36\n10      Politics factor      7 1331    1434\n11     Marijuana factor      2  851    1914\n12  DeathPenalty factor      2 1308    1457\n13        OwnGun factor      3  924    1841\n14        GunLaw factor      2  916    1849\n15 SpendMilitary factor      3 1324    1441\n16     SpendEduc factor      3 1343    1422\n17      SpendEnv factor      3 1322    1443\n18      SpendSci factor      3 1266    1499\n19        Pres00 factor      5 1749    1016\n20      Postlife factor      2 1211    1554\n                                    distribution\n1  North Central (24.7%) ...                    \n2  Female (55.6%), Male (44.4%)                 \n3  White (79.1%), Black (14.8%) ...             \n4  HS (53.8%), Bachelors (16.1%) ...            \n5  Married (45.9%), Never Married (25.6%) ...   \n6  Protestant (53.2%), Catholic (24.5%) ...     \n7  Pretty happy (57.3%) ...                     \n8  40000-49999 (9.1%) ...                       \n9  Ind (19.3%), Not Str Dem (18.9%) ...         \n10 Moderate (39.2%), Conservative (15.8%) ...   \n11 Not legal (64%), Legal (36%)                 \n12 Favor (68.7%), Oppose (31.3%)                \n13 No (65.5%), Yes (33.5%) ...                  \n14 Favor (80.5%), Oppose (19.5%)                \n15 About right (46.5%) ...                      \n16 Too little (73.9%) ...                       \n17 Too little (60%) ...                         \n18 About right (49.7%) ...                      \n19 Bush (50.6%), Gore (44.7%) ...               \n20 Yes (80.5%), No (19.5%)                      \n\nquantitative variables:  \n  name   class min  Q1 median   Q3  max mean       sd    n missing\n1   ID integer   1 692   1383 2074 2765 1383 798.3311 2765       0\n\n\nNote how all variables are Categorical !! Education has five levels:\n\nGSS2002 %&gt;% count(Education)\n\n\n  \n\n\nGSS2002 %&gt;% count(DeathPenalty)\n\n\n  \n\n\n\nLet us drop NA entries in Education and Death Penalty. And set up a table for the chi-square test.\n\ngss2002 &lt;- GSS2002 %&gt;% \n  dplyr::select(Education, DeathPenalty) %&gt;% \n  tidyr::drop_na(., c(Education, DeathPenalty))\ndim(gss2002)\n\n[1] 1307    2\n\ngss_summary &lt;- gss2002 %&gt;%\n  mutate(\n    Education = factor(\n      Education,\n      levels = c(\"Bachelors\", \"Graduate\", \"Jr Col\", \"HS\", \"Left HS\"),\n      labels = c(\"Bachelors\", \"Graduate\", \"Jr Col\", \"HS\", \"Left HS\")\n    ),\n    DeathPenalty = as.factor(DeathPenalty)\n  ) %&gt;%\n  group_by(Education, DeathPenalty) %&gt;%\n  summarise(count = n()) %&gt;% # This is good for a chisq test\n  \n  # Add two more columns to facilitate mosaic/Marrimekko Plot\n  # \n  mutate(edu_count = sum(count), \n         edu_prop = count / sum(count)) %&gt;%\n  ungroup() \n\ngss_summary"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#sec-table-plots",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#sec-table-plots",
    "title": "🃏 Permutation Test for Two Proportions",
    "section": "Table Plots",
    "text": "Table Plots\nWe can plot a heatmap-like mosaic chart for this table.\nUsing ggplot\n\n\n# https://stackoverflow.com/questions/19233365/how-to-create-a-marimekko-mosaic-plot-in-ggplot2\n\nggplot(data = gss_summary, aes( x = Education, y = edu_prop)) +\n  \n  geom_bar(aes(width = edu_count, fill = DeathPenalty), \n           stat = \"identity\", \n           position = \"fill\", \n           colour = \"black\") +\n  \n  geom_text(aes(label = scales::percent(edu_prop)), \n            position = position_stack(vjust = 0.5)) +\n\n\n# if labels are desired\n facet_grid(~ Education, scales = \"free_x\", space = \"free_x\") + \n  theme(scale_fill_brewer(palette = \"RdYlGn\")) + \n  # theme(panel.spacing.x = unit(0, \"npc\")) + # if no spacing preferred between bars\n  theme_void() \n\n\n\n\n\n\n\nUsing ggmosaic\n\n\n#library(ggmosaic)\n\nggplot(data = gss2002) +\n  geom_mosaic(aes(x = product(DeathPenalty, Education), fill = DeathPenalty))"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#section",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#section",
    "title": "🃏 Permutation Test for Two Proportions",
    "section": "",
    "text": "Observed Statistic: the X^2 metric\nWhen there are multiple proportions involved, the X^2 test is what is used.\nLet us now perform the base chisq test: We need a table and then the chisq test:\n\ngss_table &lt;- tally(DeathPenalty ~ Education, data = gss2002)\ngss_table\n\n            Education\nDeathPenalty Left HS  HS Jr Col Bachelors Graduate\n      Favor      117 511     71       135       64\n      Oppose      72 200     16        71       50\n\n# Get the observed chi-square statistic\nobservedChi2 &lt;- mosaic::chisq(tally(DeathPenalty ~ Education, data = gss2002))\nobservedChi2\n\nX.squared \n 23.45093 \n\n# Actual chi-square test\nstats::chisq.test(tally(DeathPenalty ~ Education, data = gss2002))\n\n\n    Pearson's Chi-squared test\n\ndata:  tally(DeathPenalty ~ Education, data = gss2002)\nX-squared = 23.451, df = 4, p-value = 0.0001029\n\n\nWhat would our Hypotheses be?\n$$ H_0: Education Does Not affect Votes on Death Penalty\\\nH_a: Education affects Votes on Death Penalty\n$$\nWe should now repeat the test with permutations on Education:\n\nnull_chisq &lt;- do(10000) * chisq.test(tally(DeathPenalty ~ shuffle(Education), data = gss2002))\n\nhead(null_chisq)\n\n\n  \n\n\ngf_histogram( ~ X.squared, data = null_chisq) %&gt;% \n  gf_vline(xintercept = observedChi2, color = \"red\")\n\n\n\n\n\n\nprop1(~ X.squared &gt;= observedChi2, data = null_chisq)\n\n prop_TRUE \n0.00029997 \n\n\nThe p-value is well below our threshold of \\(0.05%\\), so we would conclude that Education has a significant effect on DeathPenalty opinion!"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#conclusion",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#conclusion",
    "title": "🃏 Permutation Test for Two Proportions",
    "section": "Conclusion",
    "text": "Conclusion\nSo, what do you think?"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/150-Correlation/single_corr.html",
    "href": "content/courses/Analytics/Inference/Modules/150-Correlation/single_corr.html",
    "title": "Inference for Correlation",
    "section": "",
    "text": "# CRAN Packages\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(mosaic)\nlibrary(mosaicCore)\nlibrary(mosaicData)\n\nlibrary(openintro) # datasets and methods\nlibrary(resampledata3) # datasets\nlibrary(statsExpressions) # datasets and methods\nlibrary(ggstatsplot) # special stats plots\nlibrary(ggExtra)\n\n# Non-CRAN Packages\n# remotes::install_github(\"easystats/easystats\")\nlibrary(easystats)",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "Inference for Correlation"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/150-Correlation/single_corr.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Inference/Modules/150-Correlation/single_corr.html#setting-up-r-packages",
    "title": "Inference for Correlation",
    "section": "",
    "text": "# CRAN Packages\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(mosaic)\nlibrary(mosaicCore)\nlibrary(mosaicData)\n\nlibrary(openintro) # datasets and methods\nlibrary(resampledata3) # datasets\nlibrary(statsExpressions) # datasets and methods\nlibrary(ggstatsplot) # special stats plots\nlibrary(ggExtra)\n\n# Non-CRAN Packages\n# remotes::install_github(\"easystats/easystats\")\nlibrary(easystats)",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "Inference for Correlation"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/150-Correlation/single_corr.html#introduction",
    "href": "content/courses/Analytics/Inference/Modules/150-Correlation/single_corr.html#introduction",
    "title": "Inference for Correlation",
    "section": "\n Introduction",
    "text": "Introduction\nLet us recap a few basic definitions:\nWe have already encountered the variance of a variable:\n\\[\n\\begin{align*}\nvar_x &= \\frac{\\sum_{i=1}^{n}(x_i - \\mu_x)^2}{(n-1)}\\\\\nwhere ~ \\mu_x &= mean(x)\\\\\nn &= sample\\ size\n\\end{align*}\n\\] The standard deviation is: \\[\n\\sigma_x = \\sqrt{var_x}\\\\\n\\]\nThe covariance of two variables is defined as $$ \\[\\begin{align*}\ncov(x,y) &= \\frac{\\sum_{i = 1}^{n}(x_i - \\mu_x)*(y_i - \\mu_y)}{n-1}\\\\\n&= \\frac{\\sum{x_i *y_i}}{n-1} - \\frac{\\sum{x_i *\\mu_y}}{n-1} - \\frac{\\sum{y_i *\\mu_x}}{n-1} + \\frac{\\sum{\\mu_x *\\mu_y}}{n-1}\\\\\n&= \\frac{\\sum{x_i *y_i}}{n-1} - \\frac{\\sum{\\mu_x *\\mu_y}}{n-1}\\\\\n\n\\end{align*}\\] $$\nHence covariance is the expectation of the product minus the product of the expectations of the two variables.\n\n\n\n\n\n\nCovariance uses z-scores!\n\n\n\nNote that in both cases we are dealing with z-scores: variable minus its mean, \\(x_i - \\mu_x\\), which we have seen when dealing with the CLT and the Gaussian Distribution.\n\n\nSo, finally, the coefficient of correlation between two variables is defined as:\n\\[\n\\begin{align*}\ncorrelation ~ r &= \\frac{cov(x,y)}{\\sigma_x * \\sigma_y}\n\\\\\n&= \\frac{cov(x,y)}{\\sqrt{var_x} * \\sqrt{var_y}}\n\\end{align*}\n\\] Thus correlation coefficient is the covariance scaled by the geometric mean of the variances. Correlations define how one variables varies with another. One of the basic Questions we would have of our data is: Does some variable have a significant correlation score with another in some way? Does \\(y\\) vary with \\(x\\)? A Correlation Test is designed to answer exactly this question. The block diagram depicts the statistical procedures available to test for the significance of correlation scores between two variables.\n\n\n\n\n\nflowchart TD\n    A[Inference for Correlation] --&gt;|Check Assumptions| B[Normality: Shapiro-Wilk Test shapiro.test\\n Variances: Fisher F-test var.test]\n    B --&gt; C{OK?}\n    C --&gt;|Yes, both\\n Parametric| D[t.test]\n    D &lt;--&gt;F[Linear Model\\n Method] \n    C --&gt;|Yes, but not variance\\n Parametric| W[t.test with\\n Welch Correction]\n    W&lt;--&gt;F\n    C --&gt;|No\\n Non-Parametric| E[wilcox.test]\n    E &lt;--&gt; G[Linear Model\\n with\\n Ranked Data]\n    C --&gt;|No\\n Non-Parametric| P[Bootstrap\\n or\\n Permutation]\n    P &lt;--&gt; Q[Linear Model\\n with Ranked Data\\n and Permutation]",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "Inference for Correlation"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/150-Correlation/single_corr.html#case-study-1-a-simple-data-set-with-two-quant-variables",
    "href": "content/courses/Analytics/Inference/Modules/150-Correlation/single_corr.html#case-study-1-a-simple-data-set-with-two-quant-variables",
    "title": "Inference for Correlation",
    "section": "\n Case Study #1: A Simple Data set with Two Quant Variables",
    "text": "Case Study #1: A Simple Data set with Two Quant Variables\nLet us now see how a Correlation Test can be re-formulated as a Linear Model + Hypothesis Test.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "Inference for Correlation"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/150-Correlation/single_corr.html#the-linear-model",
    "href": "content/courses/Analytics/Inference/Modules/150-Correlation/single_corr.html#the-linear-model",
    "title": "Inference for Correlation",
    "section": "The Linear Model",
    "text": "The Linear Model\nThe premise here is that many common statistical tests are special cases of the linear model. A linear model estimates the relationship between dependent variable or “response” variable (\\(y\\)) and an explanatory variable or “predictor” (\\(x\\)). It is assumed that the relationship is linear. \\(\\beta_0\\) is the intercept and \\(\\beta_1\\) is the slope of the linear fit, that predicts the value of y based the value of x.\n\\[\ny = \\beta_0 + \\beta_1 *x\n\\]\nSome Toy Data\nMost examples in this exposition are based on three “imaginary” samples, \\(x, y1, y2\\). Each is normally distributed and made up of 50 observations. The means and the sds are, respectively:\n\nrnorm_fixed  &lt;- function(N, mu = 0, sd = 1) {\n  scale(rnorm(N))* sd + mu\n}\nparams &lt;- tibble(mu = c(0, 0.3, 0.5), sd = c(1,2,1.5))\nparams \n\n\n  \n\n\n\n\nset.seed(40) # for replication\n\n# Data as vectors ( for t.tests etc)\nx &lt;- rnorm_fixed(50, mu = 0.0, sd = 1) #explanatory\ny1 &lt;- rnorm_fixed(50, mu = 0.3, sd = 2) # dependent #1\ny2 &lt;- rnorm_fixed(50, mu = 0.5, sd = 1.5) # dependent #2\n\n# Make a tibble with all variables\nmydata_wide &lt;- tibble(x = x, y1 = y1, y2 = y2)\n\n# Long form data\nmydata_long &lt;- \n  mydata_wide %&gt;%\n  pivot_longer(., cols = c(x,y1,y2), \n               names_to = \"group\", \n               values_to = \"value\")\n\n# Long form data with only dependent variables\nmydata_long_y &lt;- \n  mydata_wide %&gt;% \n  select(-x) %&gt;% \n  pivot_longer(., cols = c(y1,y2), \n               names_to = \"group\", \n               values_to = \"value\")\n\nLet us look at our toy data in three ways:\n\nAll three variables:\n\n\nmydata_wide \n\n\n  \n\n\n\n\nVariables stacked and labelled (Note: group is now a Qual variable !!)\n\n\nmydata_long \n\n\n  \n\n\n\n\nSame as 2, but only for the dependent y variables:\n\n\nmydata_long_y",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "Inference for Correlation"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/150-Correlation/single_corr.html#pearson-correlation",
    "href": "content/courses/Analytics/Inference/Modules/150-Correlation/single_corr.html#pearson-correlation",
    "title": "Inference for Correlation",
    "section": "Pearson Correlation",
    "text": "Pearson Correlation\nModel\nThe model for Pearson Correlation tests is exactly the Linear Model:\n\\[\n\\begin{aligned}\ny = \\beta_0 + \\beta_1 \\times x\\\\\n\\\\\nH_0: Null\\ Hypothesis\\ =&gt; \\beta_1 = 0\\\\\\\nH_a: Alternate\\ Hypothesis\\ =&gt; \\beta_1 \\ne 0\\\\\n\\end{aligned}\n\\]\n\n# Pearson (built-in test)\ncor &lt;- cor.test(y1,x,method = \"pearson\") %&gt;% \n  broom::tidy() %&gt;% \n  mutate(term = \"Pearson Correlation r\") %&gt;% \n  select(term, estimate, p.value) \ncor \n\n\n  \n\n\n\nUsing the linear model method we get:\n\n# Linear Model\nlin &lt;- lm(y1 ~ 1 + x, data = mydata_wide) %&gt;% \n  broom::tidy() %&gt;% \n  mutate(term = c(\"beta_0\", \"beta_1\")) %&gt;% \n  select(term, estimate, p.value)\nlin \n\n\n  \n\n\n\nWhy are \\(r\\) and \\(\\beta_1\\) different, though the p-value is suspiciously the same!?\nDid we miss a factor of \\(\\frac{-0.463}{-0.231} = 2\\) somewhere…??\nLet us scale the variables to within {-1, +1} : (subtract the mean and divide by sd) and re-do the Linear Model with scaled versions \\(x\\) and \\(y\\):\n\n# Scaled linear model\nlin_scl &lt;- lm(scale(y1) ~ 1 + scale(x), data = mydata_wide) %&gt;% \n  broom::tidy() %&gt;% \n  mutate(term = c(\"beta_0\", \"beta_1\")) %&gt;% \n  select(term, estimate, p.value) %&gt;% \n  select(term, estimate, p.value)\nlin_scl \n\n\n  \n\n\n\nSo we conclude:\n\nWhen both x and y have the same standard deviation, the slope from the linear model and the Pearson correlation are the same. Here, since x has twice the sd of y, the ratio of slope = -0.4635533 to r = -0.2317767 is 0.5.\nThere is this relationship between the slope in the linear model and Pearson correlation:\n\n\\[\nSlope\\ \\beta_1 = \\frac{sd_y}{sd_x} * r\n\\]\nThe slope is usually much more interpretable and informative than the correlation coefficient.\n\nHence a linear model using scale() for both variables will show slope = r.\n\nSlope_Scaled: -0.2317767 = Correlation: -0.2317767\n\nFinally, the p-value for Pearson Correlation and that for the slope in the linear model is the same (\\(0.1053\\)). Which means we cannot reject the NULL hypothesis of “no relationship”.\nExample\nTBD",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "Inference for Correlation"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/150-Correlation/single_corr.html#spearman-correlation",
    "href": "content/courses/Analytics/Inference/Modules/150-Correlation/single_corr.html#spearman-correlation",
    "title": "Inference for Correlation",
    "section": "Spearman Correlation",
    "text": "Spearman Correlation\nModel\nIn some cases the LINE assumptions may not hold.\nNonlinear relationships, non-normally distributed data ( with large outliers ) and working with ordinal rather than continuous data: these situations necessitate the use of Spearman’s ranked correlation scores. (Ranked, not sign-ranked.).\nSee the example below: We choose to look at the gpa_study_hours dataset. It has two numeric columns gpa and study_hours:\n\nglimpse(gpa_study_hours)\n\nRows: 193\nColumns: 2\n$ gpa         &lt;dbl&gt; 4.000, 3.800, 3.930, 3.400, 3.200, 3.520, 3.680, 3.400, 3.…\n$ study_hours &lt;dbl&gt; 10, 25, 45, 10, 4, 10, 24, 40, 10, 10, 30, 7, 15, 60, 10, …\n\n\nWe can plot this:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nggplot(gpa_study_hours, aes(x = study_hours, y = gpa)) + \n  geom_point() + \n  geom_smooth()\n\n\n\n\n\n\n\nHmm…not normally distributed, and there is a sort of increasing relationship, however is it linear? And there is some evidence of heteroscedasticity, so the LINE assumptions are clearly in violation. Pearson correlation would not be the best idea here.\nLet us quickly try it anyway, using a Linear Model for the scaled gpa and study_hours variables, from where we get:\n\n# Pearson Correlation as Linear Model\nmodel_gpa &lt;-\n  lm(scale(gpa) ~ 1 + scale(study_hours), data = gpa_study_hours)\n##\nmodel_gpa %&gt;%\n  broom::tidy() %&gt;% \n  mutate(term = c(\"beta_0\", \"beta_1\")) %&gt;% \n  cbind(confint(model_gpa) %&gt;%                                              as_tibble()) %&gt;%  \n  select(term, estimate, p.value, `2.5 %`, `97.5 %`)\n\n\n  \n\n\n\nThe correlation estimate is \\(0.133\\); the p-value is \\(0.065\\) (and the confidence interval includes \\(0\\)).\nHence we fail to reject the NULL hypothesis that study_hours and gpa have no relationship. But can this be right?\nShould we use another test, that does not need the LINE assumptions?",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "Inference for Correlation"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/150-Correlation/single_corr.html#signed-rank-values",
    "href": "content/courses/Analytics/Inference/Modules/150-Correlation/single_corr.html#signed-rank-values",
    "title": "Inference for Correlation",
    "section": "“Signed Rank” Values",
    "text": "“Signed Rank” Values\nMost statistical tests use the actual values of the data variables. However, in some non-parametric statistical tests, the data are used in rank-transformed sense/order. (In some cases the signed-rank of the data values is used instead of the data itself.)\nSigned Rank is calculated as follows:\n\nTake the absolute value of each observation in a sample\nPlace the ranks in order of (absolute magnitude). The smallest number has rank = 1 and so on. This gives is ranked data.\nGive each of the ranks the sign of the original observation ( + or -). This gives us signed ranked data.\n\n\nsigned_rank &lt;- function(x) {sign(x) * rank(abs(x))}",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "Inference for Correlation"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/150-Correlation/single_corr.html#plotting-original-and-signed-rank-data",
    "href": "content/courses/Analytics/Inference/Modules/150-Correlation/single_corr.html#plotting-original-and-signed-rank-data",
    "title": "Inference for Correlation",
    "section": "Plotting Original and Signed Rank Data",
    "text": "Plotting Original and Signed Rank Data\nLet us see how this might work by comparing data and its signed-rank version…A quick set of plots:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n\np1 &lt;- ggplot(mydata_long, aes(x = group, y = value)) +\n  geom_jitter(width = 0.02,\n              height = 0,\n              aes(colour = group),\n              size = 4) +\n  geom_segment(data = mydata_wide,\n               aes(\n                 y = 0,\n                 yend = 0,\n                 x = .75,\n                 xend = 1.25\n               )) +\n  geom_text(aes(x = 1, y = 0.5, label = \"0\")) +\n  geom_segment(data = mydata_wide,\n               aes(\n                 y = 0.3,\n                 yend = 0.3,\n                 x = 1.75 ,\n                 xend = 2.25\n               )) +\n  geom_text(aes(x = 2, y = 0.6, label = \"0.3\")) +\n  geom_segment(data = mydata_wide,\n               aes(\n                 y = 0.5,\n                 yend = 0.5,\n                 x = 2.75,\n                 xend = 3.25\n               )) +\n  geom_text(aes(x = 3, y = 0.8, label = \"0.5\")) +\n  labs(title = \"Original Data\",\n       subtitle = \"Black Lines show Means\") +\n  ylab(\"Response Variable\")\n\np2 &lt;- mydata_long %&gt;%\n  group_by(group) %&gt;%\n  mutate(s_value = signed_rank(value)) %&gt;%\n  ggplot(., aes(x = group, y = s_value)) +\n  geom_jitter(width = 0.02,\n              height = 0,\n              aes(colour = group),\n              size = 4) +\n  stat_summary(\n    fun = \"mean\",\n    geom = \"point\",\n    colour = \"red\",\n    size = 8\n  ) +\n  labs(title = \"Signed Rank of Data\",\n       subtitle = \"Red Points are means of Ranks!\") +\n  ylab(\"Signed Rank of Response Variable\")\n\npatchwork::wrap_plots(p1, p2, nrow = 1, guides = \"collect\") \n\n\n\n\n\n\n\nSo the means of the ranks three separate variables seem to be in the same order as the means of the data variables themselves.\nHow about associations between data? Do ranks reflect well what the data might?\n\n# Plot the data\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n\np1 &lt;- ggplot(mydata_wide, aes(x, y1)) + \n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  ggtitle(\" Pearson Correlation\\n and Linear Models\")\n\n# Plot ranked data\np2 &lt;- mydata_wide %&gt;% \n  mutate(x_rank = rank(x),\n         y_rank = rank(y1)) %&gt;%\n  ggplot(.,aes(x_rank, y_rank)) + \n  geom_point(shape = 15, size = 2) +\n  geom_smooth(method = \"lm\") + \n  ggtitle(\" Spearman Ranked Correlation\\n and Linear Models\")\n\npatchwork::wrap_plots(p1,p2, nrow = 1, guides = \"collect\")\n\n\n\n\n\n\n\nThe slopes are almost identical, \\(0.25\\) for both original data and ranked data for \\(y1\\sim x\\). So maybe ranked and even sign_ranked data could work, and if it can work despite LINE assumptions not being satisfied, that would be nice!",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "Inference for Correlation"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/150-Correlation/single_corr.html#how-does-sign-rank-data-work",
    "href": "content/courses/Analytics/Inference/Modules/150-Correlation/single_corr.html#how-does-sign-rank-data-work",
    "title": "Inference for Correlation",
    "section": "How does Sign-Rank data work?",
    "text": "How does Sign-Rank data work?\nTBD: need to add some explanation here.\nSpearman correlation = Pearson correlation using the rank of the data observations. Let’s check how this holds for a our x and y1 data:\nSo the Linear Model for the Ranked Data would be:\n\\[\n\\begin{aligned}\ny = \\beta_0 + \\beta_1 \\times rank(x)\\\\\n\\\\\nH_0: Null\\ Hypothesis\\ =&gt; \\beta_1 = 0\\\\\\\nH_a: Alternate\\ Hypothesis\\ =&gt; \\beta_1 \\ne 0\\\\\n\\end{aligned}\n\\]\nCode\n\n# Spearman\ncor1 &lt;- cor.test(x, y1, method = \"spearman\") %&gt;%\n  broom::tidy() %&gt;% mutate(term = \"Spearman Correlation \") %&gt;% select(term, estimate, p.value)\ncor1\n\n\n  \n\n\n\n\n# Pearson using ranks\ncor2 &lt;- cor.test(rank(y1), rank(x), method = \"pearson\") %&gt;%\n  broom::tidy() %&gt;% select(estimate, p.value)\ncor2\n\n\n  \n\n\n\n\n# Linear Models using rank\ncor3 &lt;- lm(rank(y1) ~ 1 + rank(x),data = mydata_wide) %&gt;% \n  broom::tidy() %&gt;% select(estimate, p.value)\ncor3\n\n\n  \n\n\n\nNotes:\n\nWhen ranks are used, the slope of the linear model (\\(\\beta_1\\)) has the same value as the Spearman correlation coefficient ( \\(\\rho\\) ).\nNote that the slope from the linear model now has an intuitive interpretation: the number of ranks y changes for each change in rank of x. ( Ranks are “independent” of sd )\nExample\nWe examine the cars93 data, where the numeric variables of interest are weight and price.\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n\ncars93 %&gt;% \n  ggplot(aes(weight, price)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE, lty = 2) + \n  labs(title = \"Car Weight and Car Price have a nonlinear relationship\") + theme_classic()\n\n\n\n\n\n\n\nLet us try a Spearman Correlation score for these variables, since the data are not linearly related and the variance of price also is not constant over weight\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n\ncor.test(cars93$price, cars93$weight, method = \"spearman\") %&gt;% broom::tidy()\n\n\n  \n\n\n# Using linear Model\nlm(rank(price) ~ rank(weight), data = cars93) %&gt;% summary()\n\n\nCall:\nlm(formula = rank(price) ~ rank(weight), data = cars93)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-20.0676  -3.0135   0.7815   3.6926  20.4099 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   3.22074    2.05894   1.564    0.124    \nrank(weight)  0.88288    0.06514  13.554   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.46 on 52 degrees of freedom\nMultiple R-squared:  0.7794,    Adjusted R-squared:  0.7751 \nF-statistic: 183.7 on 1 and 52 DF,  p-value: &lt; 2.2e-16\n\n# Stats Plot\nggstatsplot::ggscatterstats(data = cars93, x = weight, \n                            y = price,\n                            type = \"nonparametric\",\n                            title = \"Cars93: Weight vs Price\",\n                            subtitle = \"Spearman Correlation\")\n\n\n\n\n\n\n\nWe see that using ranks of the price variable, we obtain a Spearman’s \\(\\rho = 0.882\\) with a p-value that is very small. Hence we are able to reject the NULL hypothesis and state that there is a relationship between these two variables. The linear relationship is evaluated as a correlation of 0.882.\n\n# Other ways using other packages\nmosaic::cor_test(gpa ~ study_hours, data = gpa_study_hours) %&gt;%\n  broom:: tidy() %&gt;% \n  select(estimate, p.value, conf.low, conf.high)\n\n\n  \n\n\n\n\nstatsExpressions::corr_test(data = gpa_study_hours, \n                            x = study_hours, \n                            y = gpa)",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "Inference for Correlation"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/150-Correlation/single_corr.html#references",
    "href": "content/courses/Analytics/Inference/Modules/150-Correlation/single_corr.html#references",
    "title": "Inference for Correlation",
    "section": "\n References",
    "text": "References\n\nCommon statistical tests are linear models (or: how to teach stats) by Jonas Kristoffer LindeløvCheatSheetCommon statistical tests are linear models: a work through by Steve DoogueJeffrey Walker “Elements of Statistical Modeling for Experimental Biology”\nDiez, David M & Barr, Christopher D & Çetinkaya-Rundel, Mine: OpenIntro Statistics\nModern Statistics with R: From wrangling and exploring data to inference and predictive modelling by Måns ThulinJeffrey Walker “A linear-model-can-be-fit-to-data-with-continuous-discrete-or-categorical-x-variables”\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\neasystats\n0.7.1\nLüdecke et al. (2022)\n\n\nggExtra\n0.10.1\nAttali and Baker (2023)\n\n\nggstatsplot\n0.12.3\nPatil (2021b)\n\n\nopenintro\n2.4.0\nÇetinkaya-Rundel et al. (2022)\n\n\nresampledata3\n1.0\nChihara and Hesterberg (2022)\n\n\nstatsExpressions\n1.5.4\nPatil (2021a)\n\n\n\n\n\n\nAttali, Dean, and Christopher Baker. 2023. ggExtra: Add Marginal Histograms to “ggplot2,” and More “ggplot2” Enhancements. https://CRAN.R-project.org/package=ggExtra.\n\n\nÇetinkaya-Rundel, Mine, David Diez, Andrew Bray, Albert Y. Kim, Ben Baumer, Chester Ismay, Nick Paterno, and Christopher Barr. 2022. openintro: Data Sets and Supplemental Functions from “OpenIntro” Textbooks and Labs. https://CRAN.R-project.org/package=openintro.\n\n\nChihara, Laura, and Tim Hesterberg. 2022. Resampledata3: Data Sets for “Mathematical Statistics with Resampling and R” (3rd Ed). https://CRAN.R-project.org/package=resampledata3.\n\n\nLüdecke, Daniel, Mattan S. Ben-Shachar, Indrajeet Patil, Brenton M. Wiernik, Etienne Bacher, Rémi Thériault, and Dominique Makowski. 2022. “easystats: Framework for Easy Statistical Modeling, Visualization, and Reporting.” CRAN. https://easystats.github.io/easystats/.\n\n\nPatil, Indrajeet. 2021a. “statsExpressions: R Package for Tidy Dataframes and Expressions with Statistical Details.” Journal of Open Source Software 6 (61): 3236. https://doi.org/10.21105/joss.03236.\n\n\n———. 2021b. “Visualizations with statistical details: The ‘ggstatsplot’ approach.” Journal of Open Source Software 6 (61): 3167. https://doi.org/10.21105/joss.03167.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "Inference for Correlation"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/50-Bootstrap/files/bootstrap.html",
    "href": "content/courses/Analytics/Inference/Modules/50-Bootstrap/files/bootstrap.html",
    "title": "Bootstrap Case Studies",
    "section": "",
    "text": "Example 5.2\n\nShow the Codemy.sample &lt;- rgamma(16, 1, 1/2)\n\nN &lt;- 10^5\nmy.boot &lt;- numeric(N)\nfor (i in 1:N)\n {\n  x &lt;- sample(my.sample, 16, replace = TRUE)  #draw resample\n  my.boot[i] &lt;- mean(x)                     #compute mean, store in my.boot\n  }\n\nggplot() + geom_histogram(aes(my.boot), bins=15)\n\n\n\n\n\n\nShow the Codemean(my.boot)  #mean\n\n[1] 1.581195\n\nShow the Codesd(my.boot)    #bootstrap SE\n\n[1] 0.3398244\n\n\nExample 5.3\nArsenic in wells in Bangladesh\n\nShow the CodeBangladesh &lt;- read.csv(\"../../../../../../materials/data/resampling/Bangladesh.csv\")\n\nggplot(Bangladesh, aes(Arsenic)) + geom_histogram(bins = 15)\n\n\n\n\n\n\nShow the Codeggplot(Bangladesh, aes(sample = Arsenic)) + stat_qq() + stat_qq_line()\n\n\n\n\n\n\nShow the CodeArsenic &lt;- pull(Bangladesh, Arsenic)\n#Alternatively\n#Arsenic &lt;- Bangladesh$Arsenic\n\nn &lt;- length(Arsenic)\nN &lt;- 10^4\n\narsenic.mean &lt;- numeric(N)\n\nfor (i in 1:N)\n{\n   x &lt;- sample(Arsenic, n, replace = TRUE)\n   arsenic.mean[i] &lt;- mean(x)\n}\n\nggplot() + geom_histogram(aes(arsenic.mean), bins = 15) + \n  labs(title=\"Bootstrap distribution of means\") + \n  geom_vline(xintercept = mean(Arsenic), colour = \"blue\")\n\n\n\n\n\n\nShow the Codedf &lt;- data.frame(x = arsenic.mean)\nggplot(df, aes(sample = x)) + stat_qq() + stat_qq_line()\n\n\n\n\n\n\nShow the Codemean(arsenic.mean)                 #bootstrap mean\n\n[1] 125.1958\n\nShow the Codemean(arsenic.mean) - mean(Arsenic) #bias\n\n[1] -0.12411\n\nShow the Codesd(arsenic.mean)                   #bootstrap SE\n\n[1] 18.20288\n\nShow the Codesum(arsenic.mean &gt; 161.3224)/N\n\n[1] 0.0312\n\nShow the Codesum(arsenic.mean &lt; 89.75262)/N\n\n[1] 0.0152\n\n\nExample 5.4 Skateboard\n\nShow the CodeSkateboard &lt;- read.csv(\"../../../../../../materials/data/resampling/Skateboard.csv\")\n\ntestF &lt;- Skateboard %&gt;% filter(Experimenter == \"Female\") %&gt;% pull(Testosterone)\ntestM &lt;- Skateboard %&gt;% filter(Experimenter == \"Male\") %&gt;% pull(Testosterone)\n\nobserved &lt;- mean(testF) - mean(testM)\n\nnf &lt;- length(testF)\nnm &lt;- length(testM)\n\nN &lt;- 10^4\n\nTestMean &lt;- numeric(N)\n\nfor (i in 1:N)\n{\n  sampleF &lt;- sample(testF, nf, replace = TRUE)\n  sampleM &lt;- sample(testM, nm, replace = TRUE)\n  TestMean[i] &lt;- mean(sampleF) - mean(sampleM)\n}\n\ndf &lt;- data.frame(TestMean)\nggplot(df) + geom_histogram(aes(TestMean), bins = 15) + \n  labs(title = \"Bootstrap distribution of difference in means\", xlab = \"means\") +\n  geom_vline(xintercept = observed, colour = \"blue\")\n\n\n\n\n\n\nShow the Codeggplot(df, aes(sample = TestMean))  + stat_qq() + stat_qq_line()\n\n\n\n\n\n\nShow the Codemean(testF) - mean(testM)\n\n[1] 83.0692\n\nShow the Codemean(TestMean)\n\n[1] 82.92373\n\nShow the Codesd(TestMean)\n\n[1] 29.35258\n\nShow the Codequantile(TestMean,c(0.025,0.975))\n\n     2.5%     97.5% \n 24.50547 140.15909 \n\nShow the Codemean(TestMean)- observed  #bias\n\n[1] -0.1454697\n\n\nPermutation test for Skateboard means\n\nShow the CodetestAll &lt;- pull(Skateboard, Testosterone)\n#testAll &lt;- Skateboard$Testosterone\n\nN &lt;- 10^4 - 1  #set number of times to repeat this process\n\n#set.seed(99)\nresult &lt;- numeric(N) # space to save the random differences\nfor(i in 1:N)\n  {\n  index &lt;- sample(71, size = nf, replace = FALSE) #sample of numbers from 1:71\n  result[i] &lt;- mean(testAll[index]) - mean(testAll[-index])\n}\n\n(sum(result &gt;= observed)+1)/(N + 1)  #P-value\n\n[1] 0.0059\n\nShow the Codeggplot() + geom_histogram(aes(result), bins = 15) + \n  labs(x = \"xbar1-xbar2\", title=\"Permutation distribution for testosterone levels\") +\n  geom_vline(xintercept = observed, colour = \"blue\")\n\n\n\n\n\n\nShow the Codedf &lt;- data.frame(result)\nggplot(df, aes(sample = result)) + stat_qq() + stat_qq_line()\n\n\n\n\n\n\n\nSection 5.4.1 Matched pairs for Diving data\n\nShow the CodeDiving2017 &lt;- read.csv(\"../../../../../../materials/data/resampling/Diving2017.csv\")\nDiff &lt;- Diving2017 %&gt;% mutate(Diff = Final - Semifinal) %&gt;% pull(Diff)\n#alternatively\n#Diff &lt;- Diving2017$Final - Diving2017$Semifinal\nn &lt;- length(Diff)\n\nN &lt;- 10^5\nresult &lt;- numeric(N)\n\nfor (i in 1:N)\n{\n  dive.sample &lt;- sample(Diff, n, replace = TRUE)\n  result[i] &lt;- mean(dive.sample)\n}\n\nggplot() + geom_histogram(aes(result), bins = 15)\n\n\n\n\n\n\nShow the Codequantile(result, c(0.025, 0.975))\n\n     2.5%     97.5% \n-6.691667 30.941667 \n\n\nExample 5.5 Verizon cont. Bootstrap means for the ILEC data and for the CLEC data\nBootstrap difference of means.\n\nShow the CodeVerizon &lt;- read.csv(\"../../../../../../materials/data/resampling/Verizon.csv\")\n\nTime.ILEC &lt;- Verizon %&gt;% filter(Group == \"ILEC\") %&gt;% pull(Time)\nTime.CLEC &lt;- Verizon %&gt;% filter(Group == \"CLEC\") %&gt;% pull(Time)\n\nobserved &lt;- mean(Time.ILEC) - mean(Time.CLEC)\n\nn.ILEC &lt;- length(Time.ILEC)\nn.CLEC &lt;- length(Time.CLEC)\n\nN &lt;- 10^4\n\ntime.ILEC.boot &lt;- numeric(N)\ntime.CLEC.boot &lt;- numeric(N)\ntime.diff.mean &lt;- numeric(N)\n\nset.seed(100)\nfor (i in 1:N)\n {\n  ILEC.sample &lt;- sample(Time.ILEC, n.ILEC, replace = TRUE)\n  CLEC.sample &lt;- sample(Time.CLEC, n.CLEC, replace = TRUE)\n  time.ILEC.boot[i] &lt;- mean(ILEC.sample)\n  time.CLEC.boot[i] &lt;- mean(CLEC.sample)\n  time.diff.mean[i] &lt;- mean(ILEC.sample) - mean(CLEC.sample)\n}\n\n#bootstrap for ILEC\nggplot() + geom_histogram(aes(time.ILEC.boot), bins = 15) + \n  labs(title = \"Bootstrap distribution of ILEC means\", x = \"means\") + \n  geom_vline(xintercept = mean(Time.ILEC), colour = \"blue\") + \n  geom_vline(xintercept = mean(time.ILEC.boot), colour = \"red\", lty=2)\n\n\n\n\n\n\nShow the Codesummary(time.ILEC.boot)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  7.036   8.156   8.400   8.406   8.642   9.832 \n\nShow the Codedf &lt;- data.frame(x = time.ILEC.boot)\nggplot(df, aes(sample = x)) + stat_qq() + stat_qq_line()\n\n\n\n\n\n\nShow the Code#bootstrap for CLEC\nggplot() + geom_histogram(aes(time.CLEC.boot), bins = 15) + \n  labs(title = \"Bootstrap distribution of CLEC means\", x = \"means\") + \n  geom_vline(xintercept = mean(Time.CLEC), colour = \"blue\") + \n  geom_vline(xintercept = mean(time.CLEC.boot), colour = \"red\", lty = 2)\n\n\n\n\n\n\nShow the Codedf &lt;- data.frame(x = time.CLEC.boot)\nggplot(df, aes(sample = x)) + stat_qq() + stat_qq_line()\n\n\n\n\n\n\nShow the Code#Different in means\nggplot() + geom_histogram(aes(time.diff.mean), bins = 15) + \n  labs(title = \"Bootstrap distribution of difference in means\", x = \"means\") +\n  geom_vline(xintercept = mean(time.diff.mean), colour = \"blue\") + \n  geom_vline(xintercept = mean(observed), colour = \"red\", lty = 2)\n\n\n\n\n\n\nShow the Codedf &lt;- data.frame(x = time.diff.mean)\nggplot(df, aes(sample = x)) + stat_qq() + stat_qq_line()\n\n\n\n\n\n\nShow the Codemean(time.diff.mean)\n\n[1] -8.096489\n\nShow the Codequantile(time.diff.mean, c(0.025, 0.975))\n\n      2.5%      97.5% \n-16.970068  -1.690859 \n\n\nSection 5.5 Verizon cont.\nBootstrap difference in trimmed means\n\nShow the CodeTime.ILEC &lt;- Verizon %&gt;% filter(Group == \"ILEC\") %&gt;% pull(Time)\nTime.CLEC &lt;- Verizon %&gt;% filter(Group == \"CLEC\") %&gt;% pull(Time)\nn.ILEC &lt;- length(Time.ILEC)\nn.CLEC &lt;- length(Time.CLEC)\n\nN &lt;- 10^4\ntime.diff.trim &lt;- numeric(N)\n\n#set.seed(100)\nfor (i in 1:N)\n{\n  x.ILEC &lt;- sample(Time.ILEC, n.ILEC, replace = TRUE)\n  x.CLEC &lt;- sample(Time.CLEC, n.CLEC, replace = TRUE)\n  time.diff.trim[i] &lt;- mean(x.ILEC, trim = .25) - mean(x.CLEC, trim = .25)\n}\n\nggplot() + geom_histogram(aes(time.diff.trim), bins = 15) + \n  labs(x = \"difference in trimmed means\") + \n  geom_vline(xintercept = mean(time.diff.trim),colour = \"blue\") + \n  geom_vline(xintercept = mean(Time.ILEC, trim = .25) - mean(Time.CLEC, trim = .25), colour = \"red\", lty = 2)\n\n\n\n\n\n\nShow the Codedf &lt;- data.frame(x = time.diff.trim)\nggplot(df, aes(sample = x)) + stat_qq() + stat_qq_line()\n\n\n\n\n\n\nShow the Codemean(time.diff.trim)\n\n[1] -10.32079\n\nShow the Codequantile(time.diff.trim, c(0.025,0.975))\n\n     2.5%     97.5% \n-15.47049  -4.97130 \n\n\nSection 5.5 Other statistics Verizon cont:\nBootstrap of the ratio of means\nTime.ILEC and Time.CLEC created above.\nn.ILEC, n.CLEC created above\n\nShow the CodeN &lt;- 10^4\ntime.ratio.mean &lt;- numeric(N)\n\n#set.seed(100)\nfor (i in 1:N)\n{\n  ILEC.sample &lt;- sample(Time.ILEC, n.ILEC, replace = TRUE)\n  CLEC.sample &lt;- sample(Time.CLEC, n.CLEC, replace = TRUE)\n  time.ratio.mean[i] &lt;- mean(ILEC.sample)/mean(CLEC.sample)\n}\n\nggplot() + geom_histogram(aes(time.ratio.mean), bins = 12) + \n  labs(title = \"bootstrap distribution of ratio of means\", x = \"ratio of means\") +\n  geom_vline(xintercept = mean(time.ratio.mean), colour = \"red\", lty = 2) + \n  geom_vline(xintercept  = mean(Time.ILEC)/mean(Time.CLEC), col = \"blue\")\n\n\n\n\n\n\nShow the Codedf &lt;- data.frame(x = time.ratio.mean)\nggplot(df, aes(sample = x)) + stat_qq() + stat_qq_line()\n\n\n\n\n\n\nShow the Codemean(time.ratio.mean)\n\n[1] 0.5429164\n\nShow the Codesd(time.ratio.mean)\n\n[1] 0.1354238\n\nShow the Codequantile(time.ratio.mean, c(0.025, 0.975))\n\n     2.5%     97.5% \n0.3283862 0.8517156 \n\n\nExample 5.7 Relative risk example\n\nShow the Codehighbp &lt;- rep(c(1,0),c(55,3283))   #high blood pressure\nlowbp &lt;- rep(c(1,0),c(21,2655))    #low blood pressure\n\nN &lt;- 10^4\nboot.rr &lt;- numeric(N)\nhigh.prop &lt;- numeric(N)\nlow.prop &lt;- numeric(N)\n\nfor (i in 1:N)\n{\n   x.high &lt;- sample(highbp,3338, replace = TRUE)\n   x.low  &lt;- sample(lowbp, 2676, replace = TRUE)\n   high.prop[i] &lt;- sum(x.high)/3338\n   low.prop[i]  &lt;- sum(x.low)/2676\n   boot.rr[i] &lt;- high.prop[i]/low.prop[i]\n}\n\nci &lt;- quantile(boot.rr, c(0.025, 0.975))\n\nggplot() + geom_histogram(aes(boot.rr), bins = 15) + \n  labs(title = \"Bootstrap distribution of relative risk\", x = \"relative risk\") +\n  geom_vline(aes(xintercept = mean(boot.rr), colour = \"mean of bootstrap\")) +\n  geom_vline(aes(xintercept = 2.12, colour=\"observed rr\"), lty = 2) + \n  scale_colour_manual(name=\"\", values = c(\"mean of bootstrap\"=\"blue\", \"observed rr\" = \"red\"))\n\n\n\n\n\n\nShow the Codetemp &lt;- ifelse(high.prop &lt; 1.31775*low.prop, 1, 0)\ntemp2 &lt;- ifelse(high.prop &gt; 3.687*low.prop, 1, 0)\ntemp3 &lt;- temp + temp2\n\ndf &lt;- data.frame(y=high.prop, x=low.prop, temp, temp2, temp3)\ndf1 &lt;- df %&gt;% filter(temp == 1)\ndf2 &lt;- df %&gt;% filter (temp2 == 1)\ndf3 &lt;- df %&gt;% filter(temp3 == 0)\n\nggplot(df, aes(x=x, y = y)) + \n  geom_point(data =df1, aes(x= x, y = y), colour = \"green\") + \n  geom_point(data = df2, aes(x = x, y = y), colour = \"green\") + \n  geom_point(data = df3, aes(x = x, y = y), colour = \"red\") + \n  geom_vline(aes(xintercept = mean(low.prop)), colour = \"red\") +\n  geom_hline(yintercept = mean(high.prop), colour = \"red\") + \n  geom_abline(aes(intercept = 0, slope = 2.12, colour = \"observed rr\"), lty = 2, lwd = 1) + \n  geom_abline(aes(intercept = 0, slope = ci[1], colour = \"bootstrap CI\"), lty = 2, lwd = 1) + \n  geom_abline(intercept = 0, slope = ci[2], colour = \"blue\", lty = 2, lwd = 1) +\n  scale_colour_manual(name=\"\", values=c(\"observed rr\"=\"black\", \"bootstrap CI\" = \"blue\")) +\n  labs(x = \"Proportion in low blood pressure group\", y = \"Proportion in high blood pressure group\")\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/courses/Analytics/Workflow/listing.html",
    "href": "content/courses/Analytics/Workflow/listing.html",
    "title": "Workflow",
    "section": "",
    "text": "Using FlexDashboard in R\n\n\nMaking Business Presentations using Flexdashboards in R\n\n\n\nflexdashboard\n\n\nDashboard Layouts\n\n\n\nMaking Business Presentations using Flexdashboards in R\n\n\n\n\n\nMar 10, 2023\n\n\nArvind Venkatadri\n\n\n\n\n\n\n\n\n\n\n\n\nDashboards in Quarto\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Workflow"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/30-SimplexMethodExcel/index.html",
    "href": "content/courses/Analytics/Prescriptive/Modules/30-SimplexMethodExcel/index.html",
    "title": "📅 The Simplex Method - In Excel",
    "section": "",
    "text": "Let us take the same problem as before:\n\\[\nMaximise\\ 7.75x_1 + 10x_2 \\\\\n\\] \\[\nSubject\\ to \\\\\n  \\begin{cases}\n    C1: -3x_1 + 2x_2 &&lt;= 3 \\\\\n    C2: 2x_1 + 4x_2 &&lt;= 27 \\\\\n    C3: 9x_1 + 10x_2 &&lt;= 90 \\\\\n    x_1, x_2 &gt;= 0\n  \\end{cases}\n\\]"
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/30-SimplexMethodExcel/index.html#using-the-excel-solver-add-in",
    "href": "content/courses/Analytics/Prescriptive/Modules/30-SimplexMethodExcel/index.html#using-the-excel-solver-add-in",
    "title": "📅 The Simplex Method - In Excel",
    "section": "",
    "text": "Let us take the same problem as before:\n\\[\nMaximise\\ 7.75x_1 + 10x_2 \\\\\n\\] \\[\nSubject\\ to \\\\\n  \\begin{cases}\n    C1: -3x_1 + 2x_2 &&lt;= 3 \\\\\n    C2: 2x_1 + 4x_2 &&lt;= 27 \\\\\n    C3: 9x_1 + 10x_2 &&lt;= 90 \\\\\n    x_1, x_2 &gt;= 0\n  \\end{cases}\n\\]"
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/30-SimplexMethodExcel/index.html#procedure",
    "href": "content/courses/Analytics/Prescriptive/Modules/30-SimplexMethodExcel/index.html#procedure",
    "title": "📅 The Simplex Method - In Excel",
    "section": "Procedure",
    "text": "Procedure\n\nSet up an Excel sheet as shown in the picture below. We enter in the objective function and the constraints in tabular form as shown:\n\n\n\n\n\n\n\n\n\n\nNext we invoke the Solver Add-in: (Data -&gt; Solver):\n\n\n\n\n\n\n\n\n\n\nWe set up the Solver for our problem as follows: Hit the SOLVE button.\n\n\n\n\n\n\n\n\n\n\nChoose to have all the three kinds of Reports from Solver (Answers, Sensitivity, and Limits).\n\n\n\n\n\n\n\n\n\nThis will create three new tabs which give additional information on:\n- How “centered” the solution is, or is it sensitive to variations of some parameters\n- How much slack do the individual constraints still have, at the end\nWe will discuss this in class!\nThe complete Excel file is here for your reference."
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/listing.html",
    "href": "content/courses/Analytics/Prescriptive/listing.html",
    "title": "Prescriptive Analytics",
    "section": "",
    "text": "📐 Intro to Linear Programming\n\n\n\n\n\n\n\n\n\n\n\nNov 10, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\n\n\n\n\n\n\n💭 The Simplex Method - Intuitively\n\n\n\n\n\nWe will look at developing an intuitive understanding of the Simplex Method for Linear Programming.\n\n\n\n\n\nNov 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n📅 The Simplex Method - In Excel\n\n\n\n\n\nWe will look at mechanizing the Simplex Method in Excel\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\nNo matching items\n\n Back to top",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Prescriptive Modelling"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html",
    "title": "Tutorial: Multiple Linear Regression with Backward Selection",
    "section": "",
    "text": "options(scipen = 1, digits = 3) #set to three decimal \nknitr::opts_chunk$set(echo = TRUE,warning = FALSE,message = FALSE)\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(mosaic)\nlibrary(infer)\n\n\n# Let us set a plot theme for Data visualisation\n\n# my_theme &lt;- function(){  # Creating a function\n#   theme_classic() +  # Using pre-defined theme as base\n#   theme(axis.text.x = element_text(size = 12, face = \"bold\"),  # Customizing axes text      \n#         axis.text.y = element_text(size = 12, face = \"bold\"),\n#         axis.title = element_text(size = 14, face = \"bold\"),  # Customizing axis title\n#         panel.grid = element_blank(),  # Taking off the default grid\n#         plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), units = , \"cm\"),\n#         legend.text = element_text(size = 12, face = \"italic\"),  # Customizing legend text\n#         legend.title = element_text(size = 12, face = \"bold\"),  # Customizing legend title\n#         legend.position = \"right\",  # Customizing legend position\n#         plot.caption = element_text(size = 12))  # Customizing plot caption\n# }   \n\nmy_theme &lt;- function(){  # Creating a function\n  theme_classic() +  # Using pre-defined theme as base\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        axis.text.x = element_text(size = 10, face = \"bold\"),  \n        # Customizing axes text      \n        axis.text.y = element_text(size = 10, face = \"bold\"),\n        axis.title = element_text(size = 12, face = \"bold\"),  \n        # Customizing axis title\n        panel.grid = element_blank(),  # Taking off the default grid\n        plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), units = , \"cm\"),\n        legend.text = element_text(size = 8, face = \"italic\"),  \n        # Customizing legend text\n        legend.title = element_text(size = 10, face = \"bold\"),  \n        # Customizing legend title\n        legend.position = \"right\",  # Customizing legend position\n        plot.caption = element_text(size = 8))  # Customizing plot caption\n}   \n\nIn this tutorial, we will use the Boston housing dataset. Our research question is:\n\n\n\n\n\n\nResearch Question\n\n\n\nHow do we predict the price of a house in Boston, based on other parameters Quantitative parameters such as area, location, rooms, and crime-rate in the neighbourhood?\nAnd how do we choose the “best” model, based on a tradeoff between Model Complexity and Model Accuracy?"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#setting-up-r-packages",
    "title": "Tutorial: Multiple Linear Regression with Backward Selection",
    "section": "",
    "text": "options(scipen = 1, digits = 3) #set to three decimal \nknitr::opts_chunk$set(echo = TRUE,warning = FALSE,message = FALSE)\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(mosaic)\nlibrary(infer)\n\n\n# Let us set a plot theme for Data visualisation\n\n# my_theme &lt;- function(){  # Creating a function\n#   theme_classic() +  # Using pre-defined theme as base\n#   theme(axis.text.x = element_text(size = 12, face = \"bold\"),  # Customizing axes text      \n#         axis.text.y = element_text(size = 12, face = \"bold\"),\n#         axis.title = element_text(size = 14, face = \"bold\"),  # Customizing axis title\n#         panel.grid = element_blank(),  # Taking off the default grid\n#         plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), units = , \"cm\"),\n#         legend.text = element_text(size = 12, face = \"italic\"),  # Customizing legend text\n#         legend.title = element_text(size = 12, face = \"bold\"),  # Customizing legend title\n#         legend.position = \"right\",  # Customizing legend position\n#         plot.caption = element_text(size = 12))  # Customizing plot caption\n# }   \n\nmy_theme &lt;- function(){  # Creating a function\n  theme_classic() +  # Using pre-defined theme as base\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        axis.text.x = element_text(size = 10, face = \"bold\"),  \n        # Customizing axes text      \n        axis.text.y = element_text(size = 10, face = \"bold\"),\n        axis.title = element_text(size = 12, face = \"bold\"),  \n        # Customizing axis title\n        panel.grid = element_blank(),  # Taking off the default grid\n        plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), units = , \"cm\"),\n        legend.text = element_text(size = 8, face = \"italic\"),  \n        # Customizing legend text\n        legend.title = element_text(size = 10, face = \"bold\"),  \n        # Customizing legend title\n        legend.position = \"right\",  # Customizing legend position\n        plot.caption = element_text(size = 8))  # Customizing plot caption\n}   \n\nIn this tutorial, we will use the Boston housing dataset. Our research question is:\n\n\n\n\n\n\nResearch Question\n\n\n\nHow do we predict the price of a house in Boston, based on other parameters Quantitative parameters such as area, location, rooms, and crime-rate in the neighbourhood?\nAnd how do we choose the “best” model, based on a tradeoff between Model Complexity and Model Accuracy?"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#workflow-read-the-data",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#workflow-read-the-data",
    "title": "Tutorial: Multiple Linear Regression with Backward Selection",
    "section": "\n Workflow: Read the Data",
    "text": "Workflow: Read the Data\n\ndata(\"BostonHousing2\", package = \"mlbench\")\nhousing &lt;- BostonHousing2\ninspect(housing)\n\n\ncategorical variables:  \n  name  class levels   n missing                                  distribution\n1 town factor     92 506       0 Cambridge (5.9%) ...                         \n2 chas factor      2 506       0 0 (93.1%), 1 (6.9%)                          \n\nquantitative variables:  \n      name   class       min       Q1   median       Q3      max     mean\n1    tract integer   1.00000 1303.250 3393.500 3739.750 5082.000 2700.356\n2      lon numeric -71.28950  -71.093  -71.053  -71.020  -70.810  -71.056\n3      lat numeric  42.03000   42.181   42.218   42.252   42.381   42.216\n4     medv numeric   5.00000   17.025   21.200   25.000   50.000   22.533\n5    cmedv numeric   5.00000   17.025   21.200   25.000   50.000   22.529\n6     crim numeric   0.00632    0.082    0.257    3.677   88.976    3.614\n7       zn numeric   0.00000    0.000    0.000   12.500  100.000   11.364\n8    indus numeric   0.46000    5.190    9.690   18.100   27.740   11.137\n9      nox numeric   0.38500    0.449    0.538    0.624    0.871    0.555\n10      rm numeric   3.56100    5.886    6.208    6.623    8.780    6.285\n11     age numeric   2.90000   45.025   77.500   94.075  100.000   68.575\n12     dis numeric   1.12960    2.100    3.207    5.188   12.127    3.795\n13     rad integer   1.00000    4.000    5.000   24.000   24.000    9.549\n14     tax integer 187.00000  279.000  330.000  666.000  711.000  408.237\n15 ptratio numeric  12.60000   17.400   19.050   20.200   22.000   18.456\n16       b numeric   0.32000  375.377  391.440  396.225  396.900  356.674\n17   lstat numeric   1.73000    6.950   11.360   16.955   37.970   12.653\n          sd   n missing\n1  1380.0368 506       0\n2     0.0754 506       0\n3     0.0618 506       0\n4     9.1971 506       0\n5     9.1822 506       0\n6     8.6015 506       0\n7    23.3225 506       0\n8     6.8604 506       0\n9     0.1159 506       0\n10    0.7026 506       0\n11   28.1489 506       0\n12    2.1057 506       0\n13    8.7073 506       0\n14  168.5371 506       0\n15    2.1649 506       0\n16   91.2949 506       0\n17    7.1411 506       0\n\n\nThe original data are 506 observations on 14 variables, medv being the target variable:\n\n\n\n\n\n\n\ncrim\nper capita crime rate by town\n\n\nzn\nproportion of residential land zoned for lots over 25,000 sq.ft\n\n\nindus\nproportion of non-retail business acres per town\n\n\nchas\nCharles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n\n\nnox\nnitric oxides concentration (parts per 10 million)\n\n\nrm\naverage number of rooms per dwelling\n\n\nage\nproportion of owner-occupied units built prior to 1940\n\n\ndis\nweighted distances to five Boston employment centres\n\n\nrad\nindex of accessibility to radial highways\n\n\ntax\nfull-value property-tax rate per USD 10,000\n\n\nptratio\npupil-teacher ratio by town\n\n\nb\n\n\\(1000(B - 0.63)^2\\) where B is the proportion of Blacks by town\n\n\nlstat\npercentage of lower status of the population\n\n\nmedv\nmedian value of owner-occupied homes in USD 1000’s\n\n\n\nThe corrected data set has the following additional columns:\n\n\ncmedv\ncorrected median value of owner-occupied homes in USD 1000’s\n\n\ntown\nname of town\n\n\ntract\ncensus tract\n\n\nlon\nlongitude of census tract\n\n\nlat\nlatitude of census tract\n\n\nOur response variable is cmedv, the corrected median value of owner-occupied homes in USD 1000’s. Their are many Quantitative feature variables that we can use to predict cmedv. And there are two Qualitative features, chas and tax."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#workflow-correlations",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#workflow-correlations",
    "title": "Tutorial: Multiple Linear Regression with Backward Selection",
    "section": "\n Workflow: Correlations",
    "text": "Workflow: Correlations\nWe can use purrr to evaluate all pair-wise correlations in one shot:\n\nall_corrs &lt;- housing %&gt;% \n  select(where(is.numeric)) %&gt;% \n  \n  # leave off cmedv/medv to get all the remaining ones\n  select(- cmedv, -medv) %&gt;%  \n  \n  # perform a cor.test for all variables against cmedv\n  purrr::map(.x = .,\n             .f = \\(x) cor.test(x, housing$cmedv)) %&gt;%\n  \n  # tidy up the cor.test outputs into a tidy data frame\n  map_dfr(broom::tidy, .id = \"predictor\") \n\nall_corrs\n\n\n  \n\n\nall_corrs %&gt;%\n  gf_hline(yintercept = 0,\n           color = \"grey\",\n           linewidth = 2) %&gt;%\n  gf_errorbar(\n    conf.high + conf.low ~ reorder(predictor, estimate),\n    colour = ~ estimate,\n    width = 0.5,\n    linewidth = ~ -log10(p.value),\n    caption = \"Significance = -log10(p.value)\"\n  ) %&gt;%\n  gf_point(estimate ~ reorder(predictor, estimate)) %&gt;%\n  gf_labs(x = \"Predictors\", y = \"Correlation with Median House Price\") %&gt;%\n  gf_theme(my_theme()) %&gt;%\n  gf_theme(theme(axis.text.x = element_text(angle = 45, hjust = 1))) %&gt;%\n  gf_refine(\n    scale_colour_distiller(\"Correlation\", type = \"div\", palette = \"RdBu\"),\n    scale_linewidth_continuous(\"Significance\", range = c(0.25, 3)),\n    guides(linewidth = guide_legend(reverse = TRUE)))\n\n\n\n\n\n\n\nThe variables rm, lstat seem to have high correlations with cmedv which are also statistically significant."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#workflow-maximal-multiple-regression-model",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#workflow-maximal-multiple-regression-model",
    "title": "Tutorial: Multiple Linear Regression with Backward Selection",
    "section": "\n Workflow: Maximal Multiple Regression Model",
    "text": "Workflow: Maximal Multiple Regression Model\nWe will create a regression model for cmedv using all the other numerical predictor features in the dataset.\n\nhousing_numeric &lt;- housing %&gt;% select(where(is.numeric), \n                                      \n                    # remove medv\n                    # an older version of cmedv\n                                      -c(medv))\nnames(housing_numeric) # 16 variables, one target, 15 predictors\n\n [1] \"tract\"   \"lon\"     \"lat\"     \"cmedv\"   \"crim\"    \"zn\"      \"indus\"  \n [8] \"nox\"     \"rm\"      \"age\"     \"dis\"     \"rad\"     \"tax\"     \"ptratio\"\n[15] \"b\"       \"lstat\"  \n\nhousing_maximal &lt;- lm(cmedv ~ ., data = housing_numeric)\nsummary(housing_maximal)\n\n\nCall:\nlm(formula = cmedv ~ ., data = housing_numeric)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-13.934  -2.752  -0.619   1.711  26.120 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -3.45e+02   3.23e+02   -1.07  0.28734    \ntract       -7.52e-04   4.46e-04   -1.69  0.09231 .  \nlon         -6.79e+00   3.44e+00   -1.98  0.04870 *  \nlat         -2.35e+00   5.36e+00   -0.44  0.66074    \ncrim        -1.09e-01   3.28e-02   -3.32  0.00097 ***\nzn           4.40e-02   1.39e-02    3.17  0.00164 ** \nindus        2.75e-02   6.20e-02    0.44  0.65692    \nnox         -1.55e+01   4.03e+00   -3.85  0.00014 ***\nrm           3.81e+00   4.20e-01    9.07  &lt; 2e-16 ***\nage          5.82e-03   1.34e-02    0.43  0.66416    \ndis         -1.38e+00   2.10e-01   -6.59  1.1e-10 ***\nrad          2.36e-01   8.47e-02    2.78  0.00558 ** \ntax         -1.48e-02   3.74e-03   -3.96  8.5e-05 ***\nptratio     -9.49e-01   1.41e-01   -6.73  4.7e-11 ***\nb            9.55e-03   2.67e-03    3.57  0.00039 ***\nlstat       -5.46e-01   5.06e-02  -10.80  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.73 on 490 degrees of freedom\nMultiple R-squared:  0.743, Adjusted R-squared:  0.735 \nF-statistic: 94.3 on 15 and 490 DF,  p-value: &lt;2e-16\n\n\nThe maximal model has an R.squared of \\(0.7426\\) which is much better than that we obtained for a simple model based on rm alone. How much can we simplify this maximal model, without losing out on R.squared?"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#workflow-model-reduction",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#workflow-model-reduction",
    "title": "Tutorial: Multiple Linear Regression with Backward Selection",
    "section": "\n Workflow: Model Reduction",
    "text": "Workflow: Model Reduction\nWe now proceed naively by removing one predictor after another. We will resort to what may amount to p-hacking by sorting the predictors based on their p-value1 in the maximal model and removing them in decreasing order of their p-value.\nWe will also use some powerful features from the purrr package (also part of the tidyverse), to create all these models all at once. Then we will be able to plot their R.squared values together and decide where we wish to trade off Explainability vs Complexity for our model.\n\n# No of Quant predictor variables in the dataset\nn_vars &lt;- housing %&gt;%\n  select(where(is.numeric), -c(cmedv, medv)) %&gt;%\n  length()\n\n# Maximal Model, now tidied\nhousing_maximal_tidy &lt;- \n  housing_maximal %&gt;% \n  broom::tidy() %&gt;% \n  \n# Obviously remove \"Intercept\" ;-D\n  filter(term != \"(Intercept)\") %&gt;% \n  \n# And horrors! Sort variables by p.value\n  arrange(p.value)\n\nhousing_maximal_tidy\n\n\n  \n\n\n\nThe last 5 variables are clearly statistically insignificant.\nAnd now we unleash the purrr package to create all the simplified models at once. We will construct a dataset containing three columns:\n\nA list of all quantitative predictor variables\nA sequence of numbers from 1 to N(predictor)\n\nA “list” column containing the housing data frame itself\n\nWe will use the iteration capability of purrr to sequentially drop one variable at a time from the maximal(15 predictor) model, build a new reduced model each time, and compute the r.squared:\nhousing_model_set &lt;- tibble(all_vars = \n                            list(housing_maximal_tidy$term), # p-hacked order!!\n                            keep_vars = seq(1, n_vars),\n                            data = list(housing_numeric))\nhousing_model_set\n# Unleash purrr in a series of mutates\nhousing_model_set &lt;- housing_model_set %&gt;%\n  \n# list of predictor variables for each model\n  mutate(mod_vars =\n           pmap(\n             .l = list(all_vars, keep_vars, data),\n             .f = \\(all_vars, keep_vars, data) all_vars[1:keep_vars]\n           )) %&gt;%\n  \n# build formulae with these for linear regression\n  mutate(formula = \n           map(.x = mod_vars,\n               .f = \\(mod_vars) as.formula(paste(\n                         \"cmedv ~\", paste(mod_vars, collapse = \"+\")\n                       )))) %&gt;%\n  \n# use the formulae to build multiple linear models\n  mutate(models =\n           pmap(\n             .l = list(data, formula),\n             .f = \\(data, formula) lm(formula, data = data)\n           ))\n# Check everything after the operation\nhousing_model_set\n# Tidy up the models using broom to expose their metrics\nhousing_models_tidy &lt;- housing_model_set %&gt;% \n  mutate(tidy_models =\n           map(\n             .x = models,\n             .f = \\(x) broom::glance(x,\n                                     conf.int = TRUE,\n                                     conf.lvel = 0.95)\n           )) %&gt;% \n\n  # Remove unwanted columns, keep model and predictor count\n  select(keep_vars, tidy_models) %&gt;%\n  unnest(tidy_models)\n\nhousing_models_tidy %&gt;%\n  gf_line(\n    r.squared ~ keep_vars,\n    ylab = \"R.Squared\",\n    xlab = \"No. params in the Linear Model\",\n    title = \"Model Explainability vs Complexity\",\n    subtitle = \"Model r.squared vs No. of Predictors\",\n    data = .\n  ) %&gt;%\n  \n  # Plot r.squared vs predictor count\n  gf_point(r.squared ~ keep_vars,\n           size = 3.5, color = \"grey\") %&gt;%\n  \n  # Show off the selected best model\n  gf_point(\n    r.squared ~ keep_vars,\n    size = 3.5,\n    color = \"red\",\n    data = housing_models_tidy %&gt;% filter(keep_vars == 4)\n  ) %&gt;%\n  \n  gf_hline(yintercept = 0.7, linetype = \"dashed\") %&gt;%\n  gf_theme(my_theme())\n\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nAt the loss of some 5% in the r.squared, we can drop our model complexity from 15 predictors to say 4! Our final model will then be:\n\nhousing_model_final &lt;- \n  housing_model_set %&gt;% \n  \n  # filter for best model, with 4 variables\n  filter(keep_vars == 4) %&gt;% \n  \n  # tidy up the model\n  mutate(tidy_models =\n           map(\n             .x = models,\n             .f = \\(x) broom::tidy(x,\n                                     conf.int = TRUE,\n                                     conf.lvel = 0.95)\n           )) %&gt;% \n  \n  # Remove unwanted columns, keep model and predictor count\n  select(keep_vars, models, tidy_models) %&gt;%\n  unnest(tidy_models)\n\nhousing_model_final\n\n\n  \n\n\nhousing_model_final %&gt;%  \n  # And plot the model\n  # Remove the intercept term\n  filter(term != \"(Intercept)\") %&gt;% \n  gf_col(estimate ~ term, fill = ~ term, width = 0.25) %&gt;% \n  gf_hline(yintercept = 0) %&gt;% \n  gf_errorbar(conf.low + conf.high ~ term, \n              width = 0.1, \n              title = \"Multiple Regression\",\n              subtitle = \"Model Estimates with Confidence Intervals\") %&gt;% \n  gf_theme(my_theme())\n\n\n\n\n\n\n\nOur current best model can be stated as:\n\\[\n\\widehat{cmedv} \\sim 24.459 - 0.563 * dis - 0.673 * lstat - 0.957 * ptratio  + 4.199 * rm\n\\]"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#workflow-diagnostics",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#workflow-diagnostics",
    "title": "Tutorial: Multiple Linear Regression with Backward Selection",
    "section": "\n Workflow: Diagnostics",
    "text": "Workflow: Diagnostics\nLet us use broom::augment to calculate residuals and predictions to arrive at a quick set of diagnostic plots.\n\nhousing_model_final_augment &lt;- \n  housing_model_set %&gt;% \n  filter(keep_vars == 4) %&gt;% \n  \n# augment the model\n  mutate(augment_models =\n           map(\n             .x = models,\n             .f = \\(x) broom::augment(x)\n           )) %&gt;% \n  unnest(augment_models) %&gt;% \n  select(cmedv:last_col())\n\nhousing_model_final_augment\n\n\n  \n\n\n\nhousing_model_final_augment %&gt;% \n  gf_point(.resid ~ .fitted, title = \"Residuals vs Fitted\") %&gt;%\n  gf_smooth() %&gt;% \n  gf_theme(my_theme)\nhousing_model_final_augment %&gt;% \n  gf_qq(~ .std.resid, title = \"Q-Q Residuals\") %&gt;% \n  gf_qqline() %&gt;%\n  gf_theme(my_theme)\nhousing_model_final_augment %&gt;% \n  gf_point(sqrt(.std.resid) ~ .fitted, \n           title = \"Scale-Location Plot\") %&gt;%\n    gf_smooth() %&gt;% \n  gf_theme(my_theme)\nhousing_model_final_augment %&gt;% \n  gf_point(.std.resid ~ .hat, \n           title = \"Residuals vs Leverage\") %&gt;%\n    gf_smooth() %&gt;% \n  gf_theme(my_theme)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe residuals plot shows a curved trend, and certainly does not resemble the stars at night, so it is possible that we have left out some possible richness in our model-making, a “systemic inadequacy”.\nThe Q-Q plot of residuals also shows a J-shape which indicates a non-normal distribution of residuals.\nThese could indicate that more complex model ( e.g. linear model with interactions between variables ( i.e. product terms ) may be necessary."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#conclusion",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#conclusion",
    "title": "Tutorial: Multiple Linear Regression with Backward Selection",
    "section": "\n Conclusion",
    "text": "Conclusion\nWe have used a multiple-regression workflow that takes all predictor variables into account in a linear model, and then systematically simplified that model such that the performance was just adequate.\nThe models we chose were all linear of course, but without interaction terms : each predictor was used only for its main effect. When the diagnostic plots were examined, we did see some shortcomings in the model. This could be overcome with a more complex model. These might include selected interactions, transformations of target(\\(cmedv^2\\), or \\(sqrt(cmedv)\\)) and some selected predictors."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#references",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#references",
    "title": "Tutorial: Multiple Linear Regression with Backward Selection",
    "section": "\n References",
    "text": "References\n\nJames, Witten, Hastie, Tibshirani, An Introduction to Statistical Learning. Chapter 3. Linear Regression. https://hastie.su.domains/ISLR2/Labs/Rmarkdown_Notebooks/Ch3-linreg-lab.html\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\ncorrgram\n1.14\nWright (2021)\n\n\ncorrplot\n0.92\nWei and Simko (2021)\n\n\nGGally\n2.2.1\nSchloerke et al. (2024)\n\n\ngt\n0.10.1\nIannone et al. (2024)\n\n\ninfer\n1.0.7\nCouch et al. (2021)\n\n\nISLR\n1.4\nJames et al. (2021)\n\n\njanitor\n2.2.0\nFirke (2023)\n\n\nreghelper\n1.1.2\nHughes and Beiner (2023)\n\n\n\n\n\n\nCouch, Simon P., Andrew P. Bray, Chester Ismay, Evgeni Chasnovski, Benjamin S. Baumer, and Mine Çetinkaya-Rundel. 2021. “infer: An R Package for Tidyverse-Friendly Statistical Inference.” Journal of Open Source Software 6 (65): 3661. https://doi.org/10.21105/joss.03661.\n\n\nFirke, Sam. 2023. janitor: Simple Tools for Examining and Cleaning Dirty Data. https://CRAN.R-project.org/package=janitor.\n\n\nHughes, Jeffrey, and David Beiner. 2023. reghelper: Helper Functions for Regression Analysis. https://CRAN.R-project.org/package=reghelper.\n\n\nIannone, Richard, Joe Cheng, Barret Schloerke, Ellis Hughes, Alexandra Lauer, and JooYoung Seo. 2024. gt: Easily Create Presentation-Ready Display Tables. https://CRAN.R-project.org/package=gt.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Rob Tibshirani. 2021. ISLR: Data for an Introduction to Statistical Learning with Applications in r. https://CRAN.R-project.org/package=ISLR.\n\n\nSchloerke, Barret, Di Cook, Joseph Larmarange, Francois Briatte, Moritz Marbach, Edwin Thoen, Amos Elberg, and Jason Crowley. 2024. GGally: Extension to “ggplot2”. https://CRAN.R-project.org/package=GGally.\n\n\nWei, Taiyun, and Viliam Simko. 2021. R Package “corrplot”: Visualization of a Correlation Matrix. https://github.com/taiyun/corrplot.\n\n\nWright, Kevin. 2021. corrgram: Plot a Correlogram. https://CRAN.R-project.org/package=corrgram."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#footnotes",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#footnotes",
    "title": "Tutorial: Multiple Linear Regression with Backward Selection",
    "section": "Footnotes",
    "text": "Footnotes\n\nJames, Witten, Hastie, Tibshirani,An Introduction to Statistical Learning. Chapter 3. Linear Regression https://hastie.su.domains/ISLR2/Labs/Rmarkdown_Notebooks/Ch3-linreg-lab.html↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "",
    "text": "options(scipen = 1, digits = 3) #set to three decimal \nknitr::opts_chunk$set(echo = TRUE,warning = FALSE,message = FALSE) \nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(mosaic)\nlibrary(GGally)\nlibrary(corrgram)\nlibrary(corrplot)\nlibrary(broom)\n\n# datasets\nlibrary(ISLR)\n\n\n# Let us set a plot theme for Data visualization\n\ntheme_set(theme_light(base_size = 11, base_family = \"Roboto Condensed\"))\n\ntheme_update(\n  panel.grid.minor = element_blank(),\n  plot.title = element_text(face = \"bold\"),\n  plot.title.position = \"plot\"\n)\n\nIn this tutorial, we will use the Boston housing Hitters dataset(s) from the ISLR package. Our research question is:\n\n\n\n\n\n\nResearch Question\n\n\n\nHow do we predict the Salary of baseball players based on other Quantitative parameters such as Hits, HmRun AtBat?\nAnd how do we choose the “best” model, based on a trade-off between Model Complexity and Model Accuracy?"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#setting-up-r-packages",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "",
    "text": "options(scipen = 1, digits = 3) #set to three decimal \nknitr::opts_chunk$set(echo = TRUE,warning = FALSE,message = FALSE) \nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(mosaic)\nlibrary(GGally)\nlibrary(corrgram)\nlibrary(corrplot)\nlibrary(broom)\n\n# datasets\nlibrary(ISLR)\n\n\n# Let us set a plot theme for Data visualization\n\ntheme_set(theme_light(base_size = 11, base_family = \"Roboto Condensed\"))\n\ntheme_update(\n  panel.grid.minor = element_blank(),\n  plot.title = element_text(face = \"bold\"),\n  plot.title.position = \"plot\"\n)\n\nIn this tutorial, we will use the Boston housing Hitters dataset(s) from the ISLR package. Our research question is:\n\n\n\n\n\n\nResearch Question\n\n\n\nHow do we predict the Salary of baseball players based on other Quantitative parameters such as Hits, HmRun AtBat?\nAnd how do we choose the “best” model, based on a trade-off between Model Complexity and Model Accuracy?"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#workflow-plan",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#workflow-plan",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "\n Workflow Plan",
    "text": "Workflow Plan\nOur target variable is Salary.\nWe will start with an examination of correlations between Salary and other Quant predictors.\nWe will use a null model for our Linear Regression at first, keeping just an intercept term. Based on the examination of the r-square improvement offered by each predictor individually, we will add another quantitative predictor. We will follow this process through up to a point where the gains in model accuracy are good enough to justify the additional model complexity.\n\n\n\n\n\n\nNote\n\n\n\nThis approach is the exact opposite of the earlier tutorial on multiple linear regression, where we started with a maximal model and trimmed it down based on an assessment of r.squared."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#workflow-eda",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#workflow-eda",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "\n Workflow: EDA",
    "text": "Workflow: EDA\nThe Hitters dataset has the following variables:\n\ndata(\"Hitters\")\ninspect(Hitters)\n\n\ncategorical variables:  \n       name  class levels   n missing\n1    League factor      2 322       0\n2  Division factor      2 322       0\n3 NewLeague factor      2 322       0\n                                   distribution\n1 A (54.3%), N (45.7%)                         \n2 W (51.2%), E (48.8%)                         \n3 A (54.7%), N (45.3%)                         \n\nquantitative variables:  \n      name   class  min    Q1 median     Q3   max    mean      sd   n missing\n1    AtBat integer 16.0 255.2  379.5  512.0   687  380.93  153.40 322       0\n2     Hits integer  1.0  64.0   96.0  137.0   238  101.02   46.45 322       0\n3    HmRun integer  0.0   4.0    8.0   16.0    40   10.77    8.71 322       0\n4     Runs integer  0.0  30.2   48.0   69.0   130   50.91   26.02 322       0\n5      RBI integer  0.0  28.0   44.0   64.8   121   48.03   26.17 322       0\n6    Walks integer  0.0  22.0   35.0   53.0   105   38.74   21.64 322       0\n7    Years integer  1.0   4.0    6.0   11.0    24    7.44    4.93 322       0\n8   CAtBat integer 19.0 816.8 1928.0 3924.2 14053 2648.68 2324.21 322       0\n9    CHits integer  4.0 209.0  508.0 1059.2  4256  717.57  654.47 322       0\n10  CHmRun integer  0.0  14.0   37.5   90.0   548   69.49   86.27 322       0\n11   CRuns integer  1.0 100.2  247.0  526.2  2165  358.80  334.11 322       0\n12    CRBI integer  0.0  88.8  220.5  426.2  1659  330.12  333.22 322       0\n13  CWalks integer  0.0  67.2  170.5  339.2  1566  260.24  267.06 322       0\n14 PutOuts integer  0.0 109.2  212.0  325.0  1378  288.94  280.70 322       0\n15 Assists integer  0.0   7.0   39.5  166.0   492  106.91  136.85 322       0\n16  Errors integer  0.0   3.0    6.0   11.0    32    8.04    6.37 322       0\n17  Salary numeric 67.5 190.0  425.0  750.0  2460  535.93  451.12 263      59\n\n\n\n Scatter Plots and Correlations\nWe should examine scatter plots and Correlations of Salary against these variables. Let us select a few sets of Quantitative and Qualitative features, along with the target variable Salary and do a pairs-plots with them:\nHitters %&gt;% \n  select(Salary, AtBat, Hits, HmRun) %&gt;% \n  GGally::ggpairs(title = \"Plot 1\", lower = list(continuous = wrap(\"smooth\", alpha = 0.2)))\nHitters %&gt;% \n  select(Salary, Runs, RBI, Walks,Years) %&gt;% \n  GGally::ggpairs(title = \"Plot 2\", lower = list(continuous = wrap(\"smooth\", alpha = 0.2)))\nHitters %&gt;% \n  select(Salary, CRBI, CAtBat, CHits, CHmRun, CRuns, CWalks) %&gt;% \n  GGally::ggpairs(title = \"Plot 3\", lower = list(continuous = wrap(\"smooth\", alpha = 0.2)))\nHitters %&gt;% \n  select(Salary, PutOuts,Assists,Errors) %&gt;% \n  GGally::ggpairs(title = \"Plot 4\", lower = list(continuous = wrap(\"smooth\", alpha = 0.2)))\nHitters %&gt;% \n  select(Salary, League,Division,NewLeague) %&gt;% \n  GGally::ggpairs(title = \"Plot 5\", lower = list(continuous = wrap(\"smooth\", alpha = 0.2)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAtBat and Hits seem relevant predictors for Salary. So are Runs, RBI,Walks, and Years. From Plot 2, both RBI and Walks are also inter-correlated with Runs. All the C* variables are well correlated with Salary and also among one another. (Plot3). Plot 4 has no significant correlations at all. Plot 5 shows Salary nearly equally distributed across League, Division, and NewLeague.\n\n Correlation Error-Bars\nWe can also plot all correlations in one graph using cor.test and purrr:\nall_corrs &lt;- \n  Hitters %&gt;% \n  select(where(is.numeric)) %&gt;% \n  \n  # leave off Salary and year to get all the remaining ones\n  select(- Salary) %&gt;% \n  \n  \n  # perform a cor.test for all variables against Salary\n  purrr::map(.x = .,\n             .f = \\(x) cor.test(x, Hitters$Salary)) %&gt;%\n  \n  # tidy up the cor.test outputs into a tidy data frame\n  map_dfr(broom::tidy, .id = \"predictor\") %&gt;% \n  arrange(desc(estimate))\n\nall_corrs\nall_corrs %&gt;%\n  gf_hline(yintercept = 0,\n           linewidth = 2,\n           color = \"grey\") %&gt;%\n  gf_errorbar(\n    conf.high + conf.low ~ reorder(predictor, estimate),\n    color = ~ estimate,\n    linewidth =  ~ -log10(p.value),\n    width = 0.5,\n    caption = \"Significance = -log10(p.value)\"\n  ) %&gt;%\n  gf_point(estimate ~ reorder(predictor, estimate)) %&gt;%\n  gf_labs(x = NULL, y = \"Correlation with Salary\") %&gt;%\n  #gf_theme(theme = my_theme()) %&gt;%\n  gf_refine(\n    scale_colour_distiller(\"Correlation\", type = \"div\",\n                           palette = \"RdBu\"),\n    scale_linewidth_continuous(\"Significance\", range = c(0.25, 3))\n  ) %&gt;%\n  gf_refine(guides(linewidth = guide_legend(reverse = TRUE)),\n            theme(axis.text.x = element_text(hjust = 1))) %&gt;%\n  gf_refine(guides(linewidth = guide_legend(reverse = TRUE)),\n            coord_flip())\n\n\n\n\n  \n\n\n\n\n\n\n\nThere are a good many predictors which have statistically significant correlations with Salary, such as CRuns , CHmRun. The darker the colour, the higher is the correlation score; the fatter the bar, the higher is the significance of the correlation.\nWe now start with setting up simple Linear Regressions with no predictors, only an intercept. We then fit separate Linear Models using each predictor individually. Then based on the the improvement in r.squared offered by each predictor, we progressively add it to the model, until we are “satisfied” with the quality of the model ( using rsquared and other means).\nLet us now do this."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#workflow-minimal-multiple-regression-model",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#workflow-minimal-multiple-regression-model",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "\n Workflow: Minimal Multiple Regression Model",
    "text": "Workflow: Minimal Multiple Regression Model\nNote the formula structure here: we want just and intercept.\n\nlm_min &lt;- lm(data = Hitters, Salary ~ 1)\nsummary(lm_min)\n\n\nCall:\nlm(formula = Salary ~ 1, data = Hitters)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n  -468   -346   -111    214   1924 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    535.9       27.8    19.3   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 451 on 262 degrees of freedom\n  (59 observations deleted due to missingness)\n\n\nlm_min %&gt;% broom::tidy()\nlm_min %&gt;% broom::glance()\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nOK, so the intercept is highly significant, the t-statistic is also high, but the intercept contributes nothing to the r.squared!! It is of no use at all!"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#workflow-predictor-addition-round1",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#workflow-predictor-addition-round1",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "\n Workflow: Predictor Addition (Round#1)",
    "text": "Workflow: Predictor Addition (Round#1)\nWe will now set up individual models for each predictor and look at the p.value and r.squared offered by each separate model:\nnames &lt;- names(Hitters %&gt;%\n  select(where(is.numeric), \n         -c(Salary)))\n\nn_vars &lt;- length(names)\n\nHitters_model_set &lt;- tibble(all_vars = list(names),\n                            keep_vars = seq(1, n_vars),\n                            data = list(Hitters))\n\n# Unleash purrr in a series of mutates\nHitters_model_set &lt;- Hitters_model_set %&gt;%\n  \n# Select Single Predictor for each Simple Model\n  mutate(mod_vars =\n           pmap(\n             .l = list(all_vars, keep_vars, data),\n             .f = \\(all_vars, keep_vars, data) all_vars[keep_vars]\n           )) %&gt;%\n  \n# build formulae with these for linear regression\n  mutate(formula = map(.x = mod_vars,\n                       .f = \\(mod_vars) as.formula(paste(\n                         \"Salary ~\", paste(mod_vars, collapse = \"+\")\n                       )))) %&gt;%\n  \n# use the formulae to build multiple linear models\n  mutate(models =\n           pmap(\n             .l = list(data, formula),\n             .f = \\(data, formula) lm(formula, data = data)\n           ))\n\n\n# Tidy up the models using broom to expose their metrics\nHitters_model_set &lt;- \n  Hitters_model_set %&gt;% \n  mutate(tidy_models =\n           map(\n             .x = models,\n             .f = \\(x) broom::glance(x,\n                                     conf.int = TRUE,\n                                     conf.lvel = 0.95)\n           ),\n         predictor_name = names[keep_vars]) %&gt;% \n\n  # Remove unwanted columns, keep model and predictor count\n  select(keep_vars,predictor_name, tidy_models) %&gt;%\n  unnest(tidy_models) %&gt;% \n  arrange(desc(r.squared))\n\n# Check everything after the operation\nHitters_model_set\n# Plot r.squared vs predictor count\nHitters_model_set %&gt;% \n  gf_point(r.squared ~ reorder(predictor_name, r.squared), \n           size = 3.5, \n           color = \"black\",\n           ylab = \"R.Squared\",\n           xlab = \"Params in the Linear Model\", data = .) %&gt;%\n  #gf_theme(my_theme()) %&gt;% \n  gf_refine(theme(axis.text.x = element_text(angle = 30,\n                                             hjust = 1)))\n\n\n\n\n  \n\n\n\n\n\n\n\n\n# Which is the winning Predictor?\nwinner &lt;- Hitters_model_set %&gt;% \n  arrange(desc(r.squared)) %&gt;% \n  select(predictor_name) %&gt;% \n  head(1) %&gt;% as.character()\nwinner\n\n[1] \"CRBI\"\n\n\n# Here is the Round 1 Model\n# Minimal model updated to included winning predictor\nlm_round1 &lt;- update(lm_min, ~. + CRBI)\nlm_round1 %&gt;% broom::tidy()\nlm_round1 %&gt;% broom::glance()\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nSo we can add CRBI as a predictor to our model as a predictor gives us an improved r.squared of \\(0.321\\), which is the square of the correlation between Salary and CRBI, \\(.567\\).\nAnd the model itself is: \\[\nSalary \\sim 274.580 + 0.791 \\times CRBI\n\\tag{1}\\]\nLet’s press on to Round 2."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#workflow-predictor-addition-round-2",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#workflow-predictor-addition-round-2",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "\n Workflow: Predictor Addition (Round #2)",
    "text": "Workflow: Predictor Addition (Round #2)\nWe will set up a round-2 model using CRBI as the predictor, and then proceed to add each of the other predictors as an update to the model.\n# Preliminaries\nnames &lt;- names(Hitters %&gt;%\n  select(where(is.numeric), -c(Salary, winner)))\n# names\n\nn_vars &lt;- length(names)\n# n_vars\n# names &lt;- names %&gt;% str_remove(winner)\n# names\n# n_vars &lt;- n_vars-1\n\n\n# Round 2 Iteration\nHitters_model_set &lt;- tibble(all_vars = list(names),\n                            keep_vars = seq(1, n_vars),\n                            data = list(Hitters))\n# Hitters_model_set \n\n# Unleash purrr in a series of mutates\nHitters_model_set &lt;- Hitters_model_set %&gt;%\n  \n# list of predictor variables for each model\n  mutate(mod_vars =\n           pmap(\n             .l = list(all_vars, keep_vars, data),\n             .f = \\(all_vars, keep_vars, data) all_vars[keep_vars]\n           )) %&gt;%\n  \n# build formulae with these for linear regression\n  mutate(formula = map(.x = mod_vars,\n                       .f = \\(mod_vars) as.formula(paste(\n                         \"Salary ~ CRBI +\", paste(mod_vars, collapse = \"+\")\n                       )))) %&gt;%\n  \n# use the formulae to build multiple linear models\n  mutate(models =\n           pmap(\n             .l = list(data, formula),\n             .f = \\(data, formula) lm(formula, data = data)\n           ))\n# Check everything after the operation\n# Hitters_model_set\n\n# Tidy up the models using broom to expose their metrics\nHitters_model_set &lt;- \n  Hitters_model_set %&gt;% \n  mutate(tidy_models =\n           map(\n             .x = models,\n             .f = \\(x) broom::glance(x,\n                                     conf.int = TRUE,\n                                     conf.lvel = 0.95)\n           ),\n         predictor_name = names[keep_vars]) %&gt;% \n\n  # Remove unwanted columns, keep model and predictor count\n  select(keep_vars,predictor_name, tidy_models) %&gt;%\n  unnest(tidy_models) %&gt;% \n  arrange(desc(r.squared))\n\nHitters_model_set\n# Plot r.squared vs predictor count\nHitters_model_set %&gt;% \n  gf_point(r.squared ~ reorder(predictor_name, r.squared), \n                               size = 3.5,\n                               ylab = \"R.Squared\",\n                               xlab = \"Param in the Linear Model\") %&gt;%\n  #gf_theme(my_theme()) %&gt;% \n  gf_refine(theme(axis.text.x = element_text(angle = 30, \n                                             hjust = 1)))\n\n\n\n\n  \n\n\n\n\n\n\n\n# Which is the winning Predictor?\n# \nwinner &lt;- Hitters_model_set %&gt;% \n  arrange(desc(r.squared)) %&gt;% \n  select(predictor_name) %&gt;% \n  head(1) %&gt;% as.character()\nwinner\n# Here is the Round 1 Model\nlm_round2 &lt;- update(lm_round1, ~. + Hits)\nlm_round2 %&gt;% broom::tidy()\nlm_round2 %&gt;% broom::glance()\n\n\n\n[1] \"Hits\"\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nAnd now the model itself is: \\[\nSalary \\sim -47.96 + 0.691 \\times CRBI + 3.30 \\times Hits\n\\tag{2}\\]\nNote the change in both intercept and the slope for CRBI when the new predictor Hits is added!!"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#workflow-visualization",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#workflow-visualization",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "\n Workflow: Visualization",
    "text": "Workflow: Visualization\nLet us quickly see how this model might look. We know that with simple regression, we obtain a straight line as our model. Here, with two (or more) predictors, we should obtain a ….(hyper)plane! Play with the interactive plot below!"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#discussion",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#discussion",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "\n Discussion",
    "text": "Discussion\nIt is interesting that the second variable to be added was Hits which has a lower correlation of \\(r = 0.439\\) with Salary compared to some other Quant predictors such as Chits( \\(r = 0.525\\) ). This is because CRBI is hugely correlated with all of these predictors, so CRBI effectively acts as a proxy for all of these. See Plot 3.\nWe see that adding Hits to the model gives us an improved r.squared of \\(0.425\\)."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#conclusion",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#conclusion",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "\n Conclusion",
    "text": "Conclusion\nWe can proceed in this way to subsequent rounds and decide to stop when the model complexity (no. of predictors ) and the resulting gain in r.squared does not seem worth it.\n\n\n\n\n\n\nAutomatic Iteration Method\n\n\n\nWe ought to convert the above code into an R function and run it that way for a specific number of rounds to see how things pan out. That is in the next version of this Tutorial! It appears that there is, what else, an R Package, called reghelper that allows us to do this! 😇 The reghelper::build_model() function can be used to:\n\nStart with only an intercept\n\nSequentially add each of the other predictor variables into the model “blocks”\n\nBlocks will be added in the order they are passed to the function, and variables from previous blocks will be included with each subsequent block, so they do not need to be repeated.\n\n\nType help(rehelper) in your Console.\nlibrary(reghelper)\n\nbig_model &lt;- build_model(\n  dv = Salary, \n  # Start with only an intercept lm(Salary ~ 1, data = .)\n  \n  # Sequentially add each of the other predictor variables\n  # Pass through variable names (or interaction terms) to add for each block. \n  # To add one term to a block, just pass it through directly; \n  # to add multiple terms at a time to a block, pass it through in a vector or list. \n  # Interaction Terms can be specified using the vector/list\n  # Blocks will be added in the order they are passed to the function\n  # Variables from previous blocks will be included with each subsequent block,  so they do not need to be repeated.\n  \n  1, AtBat, Hits, HmRun, Runs, RBI, Walks, Years, CAtBat, CHits, \n  CHmRun, CRuns, CRBI, CWalks, PutOuts, Assists, Errors,\n  \n  data = Hitters, \n  model = 'lm')\n\n\n\nThis multiple model is a list object with 4 items. Type summary(big_model) in your Console.\nWe can clean it up a wee bit:\n\nlibrary(gt)\n# big_model has 4 parts: formulas, residuals, coefficients, overall\n\noverall_clean &lt;- summary(big_model)$overall %&gt;% as_tibble() %&gt;% janitor::clean_names()\n\nformulas_clean &lt;- summary(big_model)$formulas %&gt;% \n  as.character() %&gt;% as_tibble() %&gt;% \n  rename(\"model_formula\" = value)\n\nall_models &lt;- cbind(formulas_clean, overall_clean) %&gt;% \n  dplyr::select(1,2,8)\nall_models %&gt;% \n  gt::gt() %&gt;% \n    tab_style(style = cell_fill(color = \"grey\"),\n            locations = cells_body(rows = seq(1, 18, 2)))\n\n\n\n\n\n\nmodel_formula\nr_squared\ndelta_r_sq\n\n\n\nSalary ~ 1\nNA\nNA\n\n\nSalary ~ 1 + AtBat\n0.156\nNA\n\n\nSalary ~ 1 + AtBat + Hits\n0.204\n0.047748\n\n\nSalary ~ 1 + AtBat + Hits + HmRun\n0.227\n0.023530\n\n\nSalary ~ 1 + AtBat + Hits + HmRun + Runs\n0.227\n0.000137\n\n\nSalary ~ 1 + AtBat + Hits + HmRun + Runs + RBI\n0.244\n0.016807\n\n\nSalary ~ 1 + AtBat + Hits + HmRun + Runs + RBI + Walks\n0.307\n0.062553\n\n\nSalary ~ 1 + AtBat + Hits + HmRun + Runs + RBI + Walks + Years\n0.409\n0.101919\n\n\nSalary ~ 1 + AtBat + Hits + HmRun + Runs + RBI + Walks + Years + CAtBat\n0.455\n0.046369\n\n\nSalary ~ 1 + AtBat + Hits + HmRun + Runs + RBI + Walks + Years + CAtBat + CHits\n0.471\n0.016234\n\n\nSalary ~ 1 + AtBat + Hits + HmRun + Runs + RBI + Walks + Years + CAtBat + CHits + CHmRun\n0.488\n0.016771\n\n\nSalary ~ 1 + AtBat + Hits + HmRun + Runs + RBI + Walks + Years + CAtBat + CHits + CHmRun + CRuns\n0.488\n0.000333\n\n\nSalary ~ 1 + AtBat + Hits + HmRun + Runs + RBI + Walks + Years + CAtBat + CHits + CHmRun + CRuns + CRBI\n0.489\n0.001157\n\n\nSalary ~ 1 + AtBat + Hits + HmRun + Runs + RBI + Walks + Years + CAtBat + CHits + CHmRun + CRuns + CRBI + CWalks\n0.498\n0.008575\n\n\nSalary ~ 1 + AtBat + Hits + HmRun + Runs + RBI + Walks + Years + CAtBat + CHits + CHmRun + CRuns + CRBI + CWalks + PutOuts\n0.522\n0.023739\n\n\nSalary ~ 1 + AtBat + Hits + HmRun + Runs + RBI + Walks + Years + CAtBat + CHits + CHmRun + CRuns + CRBI + CWalks + PutOuts + Assists\n0.527\n0.005401\n\n\nSalary ~ 1 + AtBat + Hits + HmRun + Runs + RBI + Walks + Years + CAtBat + CHits + CHmRun + CRuns + CRBI + CWalks + PutOuts + Assists + Errors\n0.528\n0.000814\n\n\n\n\n\n\n\nSo we have a list of all models with main effects only. We could play with the build_model function to develop interaction models too! Slightly weird that the NULL model of Salary~1 does not show an r.squared value with build_model…??"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#references",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#references",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "\n References",
    "text": "References\n\nhttps://ethanwicker.com/2021-01-11-multiple-linear-regression-002/\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\ncorrgram\n1.14\nWright (2021)\n\n\ncorrplot\n0.92\nWei and Simko (2021)\n\n\nGGally\n2.2.1\nSchloerke et al. (2024)\n\n\ngt\n0.10.1\nIannone et al. (2024)\n\n\ninfer\n1.0.7\nCouch et al. (2021)\n\n\nISLR\n1.4\nJames et al. (2021)\n\n\njanitor\n2.2.0\nFirke (2023)\n\n\nreghelper\n1.1.2\nHughes and Beiner (2023)\n\n\n\n\n\n\nCouch, Simon P., Andrew P. Bray, Chester Ismay, Evgeni Chasnovski, Benjamin S. Baumer, and Mine Çetinkaya-Rundel. 2021. “infer: An R Package for Tidyverse-Friendly Statistical Inference.” Journal of Open Source Software 6 (65): 3661. https://doi.org/10.21105/joss.03661.\n\n\nFirke, Sam. 2023. janitor: Simple Tools for Examining and Cleaning Dirty Data. https://CRAN.R-project.org/package=janitor.\n\n\nHughes, Jeffrey, and David Beiner. 2023. reghelper: Helper Functions for Regression Analysis. https://CRAN.R-project.org/package=reghelper.\n\n\nIannone, Richard, Joe Cheng, Barret Schloerke, Ellis Hughes, Alexandra Lauer, and JooYoung Seo. 2024. gt: Easily Create Presentation-Ready Display Tables. https://CRAN.R-project.org/package=gt.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Rob Tibshirani. 2021. ISLR: Data for an Introduction to Statistical Learning with Applications in r. https://CRAN.R-project.org/package=ISLR.\n\n\nSchloerke, Barret, Di Cook, Joseph Larmarange, Francois Briatte, Moritz Marbach, Edwin Thoen, Amos Elberg, and Jason Crowley. 2024. GGally: Extension to “ggplot2”. https://CRAN.R-project.org/package=GGally.\n\n\nWei, Taiyun, and Viliam Simko. 2021. R Package “corrplot”: Visualization of a Correlation Matrix. https://github.com/taiyun/corrplot.\n\n\nWright, Kevin. 2021. corrgram: Plot a Correlogram. https://CRAN.R-project.org/package=corrgram."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html",
    "href": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html",
    "title": "🕔 Modelling and Predicting Time Series",
    "section": "",
    "text": "library(tidyverse)\nlibrary(lubridate)\nlibrary(mosaic)\nlibrary(ggformula)\nlibrary(timetk)\n###\nlibrary(tsibble)\nlibrary(fpp3)\nlibrary(sweep) # Tidy forecast Model objects\n###\nlibrary(forecast)\nlibrary(prophet)",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Inferential Modelling",
      "🕔 Modelling and Predicting Time Series"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html#setup-the-packages",
    "href": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html#setup-the-packages",
    "title": "🕔 Modelling and Predicting Time Series",
    "section": "",
    "text": "library(tidyverse)\nlibrary(lubridate)\nlibrary(mosaic)\nlibrary(ggformula)\nlibrary(timetk)\n###\nlibrary(tsibble)\nlibrary(fpp3)\nlibrary(sweep) # Tidy forecast Model objects\n###\nlibrary(forecast)\nlibrary(prophet)",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Inferential Modelling",
      "🕔 Modelling and Predicting Time Series"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html#introduction",
    "href": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html#introduction",
    "title": "🕔 Modelling and Predicting Time Series",
    "section": "\n Introduction",
    "text": "Introduction\nIn this module we will look at modelling of time series. We will start with the simplest of exponential models and go all the way through ARIMA and forecasting with Prophet.\nFirst, some terminology!",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Inferential Modelling",
      "🕔 Modelling and Predicting Time Series"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html#additive-and-multiplicative-time-series-models",
    "href": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html#additive-and-multiplicative-time-series-models",
    "title": "🕔 Modelling and Predicting Time Series",
    "section": "\n Additive and Multiplicative Time Series Models",
    "text": "Additive and Multiplicative Time Series Models\nAdditive Time Series can be represented as:\n\\[\nY_t = S_t + T_t + ϵ_t\n\\]\nMultiplicative Time Series can be described as:\n\\[\nY_t = S_t × T_t × ϵ_t\n\\]\nLet us consider a Multiplicative Time Series, pertaining to sales of souvenirs at beaches in Australia: The time series looks like this:\n\n\n\n\n\n\nNote that along with the trend, the amplitude of both seasonal and noise components are also increasing in a multiplicative way here !! A multiplicative time series can be converted to additive by taking a log of the time series.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Inferential Modelling",
      "🕔 Modelling and Predicting Time Series"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html#stationarity",
    "href": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html#stationarity",
    "title": "🕔 Modelling and Predicting Time Series",
    "section": "Stationarity",
    "text": "Stationarity\nA time series is said to be stationary if it holds the following conditions true:\n\nThe mean value of time-series is constant over time, which implies, the trend component is nullified/constant.\nThe variance does not increase over time.\nSeasonality effect is minimal.\n\nThis means it is devoid of trend or seasonal patterns, which makes it looks like a random white noise irrespective of the observed time interval , i.e. zooming in on the time axis. ( i.e. self-similar and fractal)",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Inferential Modelling",
      "🕔 Modelling and Predicting Time Series"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html#a-bit-of-forecasting",
    "href": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html#a-bit-of-forecasting",
    "title": "🕔 Modelling and Predicting Time Series",
    "section": "A Bit of Forecasting?",
    "text": "A Bit of Forecasting?\nWe are always interested in the future. We will do this in three ways:\n\nuse Simple Exponential Smoothing\nuse a package called forecast to fit an ARIMA (Autoregressive Moving Average Integrated Model) model to the data and make predictions for weekly sales;\nAnd do the same using a package called prophet.\n\nForecasting using Exponential Smoothing\nFor example, the file contains total annual rainfall in inches for London, from 1813-1912 (original data from Hipel and McLeod, 1994).\n\nrain &lt;- scan(\"https://robjhyndman.com/tsdldata/hurst/precip1.dat\", skip = 2)\nrainseries &lt;- ts(rain, start = c(1813))\nplot(rainseries)\n\n\n\n\n\n\n\nThere is a nearly constant value of about 25 around which there are random fluctuations and it seems to be an additive model. How can we make forecasts with this time series?\nA deliberate detour:\nLet’s see some quick notation to aid understanding: Much of smoothing is based on the high school concept of a straight line, \\(y = m*x + c\\).\nIn the following, we choose to describe the models with:\n\n\n\\(y\\) : the actual values in the time series\n\n\\(\\hat y\\) : our predictions from whichever model we create\n\n\\(l\\) : a level or mean as forecast;\n\n\\(b\\) : a trend variable; akin to the slope in the straight line equation;\n\n\\(s\\) : seasonal component of the time series. Note that this is a set of values that stretch over one cycle of the time series.\n\nIn Exponential Smoothing and Forecasting, we make three models of increasing complexity:\n\nSimple Exponential Model: Here we deal only with the mean or level aspect of the (decomposed) time series and make predictions with that.\nHolt Model: Here we use the level and the trend from the decomposed time series for predictions\nHolt-Winters Model: Here we use the level, the trend, and the seasonal component from the decomposed time series for predictions.\n\n\n[&lt;start&gt;st]-&gt;[&lt;input&gt;input]\n[&lt;input&gt; input]-&gt;[&lt;package&gt; Time  Series|Decomposition]\n[&lt;package&gt; Time  Series|Decomposition]-&gt;[&lt;component&gt; Mean/Level]\n[&lt;package&gt; Time  Series|Decomposition]-&gt;[&lt;component&gt; Slope/Trend]\n[&lt;package&gt; Time  Series|Decomposition]-&gt;[&lt;component&gt; Seasonal]\n\n//Simple Exponential Smoothing\n[&lt;component&gt; Mean/Level]-&gt;[Delay A1]\n[Delay A1]-&gt;[Delay A2]\n[Delay A2]-&gt;[Delay A3]\n[Delay A3]...-&gt;...[Delay AN]\n[Delay A1]-&gt;[&lt;state&gt; A1]\n[Delay A2]-&gt;[&lt;state&gt; A2]\n[Delay A3]-&gt;[&lt;state&gt; A3]\n[Delay AN]-&gt;[&lt;state&gt; AN]\n[&lt;state&gt; AN]---([&lt;note&gt; $$alpha(1-alpha)^i$$]\n\n[&lt;state&gt; A1]-&gt;[&lt;state&gt; Add1]\n[&lt;state&gt; A2]-&gt;[&lt;state&gt; Add1]\n[&lt;state&gt; A3]-&gt;[&lt;state&gt; Add1]\n[&lt;state&gt; AN]-&gt;[&lt;state&gt; Add1]\n[&lt;state&gt; Add1]-&gt;[&lt;end&gt; Output]\n\n//Holt \n[&lt;component&gt; Slope/Trend]-&gt;[Delay B1]\n[Delay B1]-&gt;[Delay B2]\n[Delay B2]-&gt;[Delay B3]\n[Delay B3]...-&gt;...[Delay BN]\n[Delay B1]-&gt;[&lt;state&gt; B1]\n[Delay B2]-&gt;[&lt;state&gt; B2]\n[Delay B3]-&gt;[&lt;state&gt; B3]\n[Delay BN]-&gt;[&lt;state&gt; BN]\n[&lt;state&gt; BN]---([&lt;note&gt; $$beta(1-beta)^i$$]\n[&lt;state&gt; B1]-&gt;[&lt;state&gt; Add2]\n[&lt;state&gt; B2]-&gt;[&lt;state&gt; Add2]\n[&lt;state&gt; B3]-&gt;[&lt;state&gt; Add2]\n[&lt;state&gt; BN]-&gt;[&lt;state&gt; Add2]\n[&lt;state&gt; Add2]-&gt;[&lt;end&gt; Output]\n\n// Holt Winters\n[&lt;component&gt; Seasonal]-&gt;[Delay C1]\n[Delay C1]-&gt;[Delay C2]\n[Delay C2]-&gt;[Delay C3]\n[Delay C3]...-&gt;...[Delay CN]\n[Delay C1]-&gt;[&lt;state&gt; C1]\n[Delay C2]-&gt;[&lt;state&gt; C2]\n[Delay C3]-&gt;[&lt;state&gt; C3]\n[Delay CN]-&gt;[&lt;state&gt; CN]\n[&lt;state&gt; CN]---([&lt;note&gt; $$gamma(1-gamma)^i$$]\n[&lt;state&gt; C1]-&gt;[&lt;state&gt; Add3]\n[&lt;state&gt; C2]-&gt;[&lt;state&gt; Add3]\n[&lt;state&gt; C3]-&gt;[&lt;state&gt; Add3]\n[&lt;state&gt; CN]-&gt;[&lt;state&gt; Add3]\n[&lt;state&gt; Add3]-&gt;[&lt;end&gt; Output]\n\n// Final Output\n[&lt;end&gt; Output]-&gt;[&lt;receiver&gt; Forecast]\n\n\n\n\n\nSimple Smoothing is smoothing based forecasting using just the level ( i.e. mean) of the Time Series to make forecasts.\nDouble exponential smoothing, or Holt Smoothing Model, is just exponential smoothing applied to both level and trend.\nThe idea behind triple exponential smoothing, or the Holt-Winters Smoothing Model, is to apply exponential smoothing to the seasonal components in addition to level and trend.\nWhat does “Exponential” mean?\nAll three models use memory: at each time instant in the Time Series, a set of past values, along with the present sample is used to make a prediction of the relevant parameter ( level / slope / seasonal). These are then added together to make the forecast.\nThe memory in each case controlled by a parameter: alpha for the estimate of the level beta for the slope estimate, and gamma for the seasonal component estimate at the current time point. All these parameters are between 0 and 1. The model takes a weighted average of past values of each parameter. The weights are derived in the form of \\(\\alpha(1-\\alpha)^i\\), where \\(i\\) defines how old the sample is compared to the present one, thus forming a set of weights that decrease exponentially with delay. Values of \\(\\alpha, \\beta. \\gamma\\) that are close to 0 mean that significant weightage is placed on observations in the past.(Memory is “stronger”). To express this in mathematical notation we now need three equations: one for level, one for the trend and one to combine the level and trend to get the expected \\(\\hat y\\).\nTo make forecasts using simple exponential smoothing in R, we can use the HoltWinters() function in R, or the forecast::ets() function from forecasts. This latter function is more powerful.\n\nargs(HoltWinters)\n\nfunction (x, alpha = NULL, beta = NULL, gamma = NULL, seasonal = c(\"additive\", \n    \"multiplicative\"), start.periods = 2, l.start = NULL, b.start = NULL, \n    s.start = NULL, optim.start = c(alpha = 0.3, beta = 0.1, \n        gamma = 0.1), optim.control = list()) \nNULL\n\nargs(forecast::ets)\n\nfunction (y, model = \"ZZZ\", damped = NULL, alpha = NULL, beta = NULL, \n    gamma = NULL, phi = NULL, additive.only = FALSE, lambda = NULL, \n    biasadj = FALSE, lower = c(rep(1e-04, 3), 0.8), upper = c(rep(0.9999, \n        3), 0.98), opt.crit = c(\"lik\", \"amse\", \"mse\", \"sigma\", \n        \"mae\"), nmse = 3, bounds = c(\"both\", \"usual\", \"admissible\"), \n    ic = c(\"aicc\", \"aic\", \"bic\"), restrict = TRUE, allow.multiplicative.trend = FALSE, \n    use.initial.values = FALSE, na.action = c(\"na.contiguous\", \n        \"na.interp\", \"na.fail\"), ...) \nNULL\n\n\nTo use HoltWinters() for simple exponential smoothing, we need to set the parameters beta=FALSE and gamma=FALSE in the HoltWinters() function (the beta and gamma parameters are used for double exponential smoothing, or triple exponential smoothing.\nTo use forecast::ets, we set the model argument to “ANN”, “AAN”, and “AAA” respectively for each of the three smoothing models.\nNote: The HoltWinters() function returns a list variable, that contains several named elements.\n\nrainseriesforecasts &lt;- forecast::ets(rainseries, model = \"ANN\")\n# class(rainseriesforecasts)\n# str(rainseriesforecasts)\nplot(rainseriesforecasts)\n\n\n\n\n\n\nplot(forecast(rainseriesforecasts, 10))\n\n\n\n\n\n\n\nARIMA\nWe can also use past trends and seasonality in the data to make predictions about the future using the forecast package. Here we use an auto ARIMA model to guess at the trend in the time series. Then we use that model to forecast a few periods into the future.\nMathematically an ARIMA model can be shown as follows:\n\n\n\n\n\n\nWe will use the familiar Walmart Sales dataset, and try to predict weekly sales for one of the Departments.\n\ndata(\"walmart_sales_weekly\")\nwalmart_wide &lt;- walmart_sales_weekly %&gt;% \n  pivot_wider(., id_cols = c(Date), \n              names_from = Dept, \n              values_from = Weekly_Sales,\n              names_prefix = \"Sales_\")\n\n## forecast::auto.arima needs a SINGLE time series, so we pick one, Dept95\nsales_95_ts &lt;- walmart_wide %&gt;% \n  select(Sales_95) %&gt;% \n  ts(start = c(2010,1), end = c(2012,52),frequency = 52)\nsales_95_ts\n\nTime Series:\nStart = c(2010, 1) \nEnd = c(2012, 52) \nFrequency = 52 \n  [1] 106690.06 111390.36 107952.07 103652.58 112807.75 112048.41 117716.13\n  [8] 113117.35 111466.37 116770.82 126341.84 110204.77 107648.14 125592.28\n [15] 120247.90 120036.99 121902.19 133056.97 131995.00 134118.05 120172.47\n [22] 124821.44 126241.20 121386.73 116256.35 108781.57 131128.96 131288.83\n [29] 124601.48 117929.58 124220.10 125027.49 124372.90 114702.69 113009.41\n [36] 120764.22 123510.99 110052.15 105793.40 110332.92 110209.31 107544.02\n [43] 106015.41 100834.31 111384.36 116521.67 121695.13  93676.95 107317.32\n [50] 109955.90 103724.16  99043.34 114270.08 117548.75 112165.80 107742.95\n [57] 116225.68 120621.32 123405.41 122280.13 112905.09 126746.25 126834.30\n [64] 118632.26 111764.31 120882.84 124953.94 112581.20 119815.67 135260.49\n [71] 136364.46 135197.63 121814.84 128054.88 133213.04 127906.50 121483.11\n [78] 117284.94 138538.47 138567.10 133260.84 122721.92 130446.34 133762.77\n [85] 133939.40 116165.28 115663.78 132805.42 125954.30 116931.34 108018.21\n [92] 114793.92 115047.16 113966.34 112688.97 102798.99 119053.80 120721.07\n [99] 125041.39  93358.91 116427.93 118685.12 113021.23 102202.04 115507.25\n[106] 125038.09 119807.63 110870.94 118406.27 125840.82 132318.50 117030.73\n[113] 127706.00 137958.76 129438.22 123172.79 118589.44 130920.36 131341.85\n[120] 129031.19 127603.00 130573.37 139857.10 140806.36 124594.40 131935.56\n[127] 148798.05 129724.74 126861.49 121030.79 134832.22 137408.20 136264.68\n[134] 118845.34 124741.33 140657.40 128542.73 119121.35 115326.47 127009.22\n[141] 124559.93 123346.24 117375.38 106690.06 111390.36 107952.07 103652.58\n[148] 112807.75 112048.41 117716.13 113117.35 111466.37 116770.82 126341.84\n[155] 110204.77 107648.14\n\narima_dept_95 &lt;- forecast::auto.arima(y = sales_95_ts)\narima_dept_95\n\nSeries: sales_95_ts \nARIMA(0,1,1)(0,1,0)[52] \n\nCoefficients:\n          ma1\n      -0.8842\ns.e.   0.0530\n\nsigma^2 = 29974424:  log likelihood = -1033.02\nAIC=2070.03   AICc=2070.15   BIC=2075.3\n\nplot(arima_dept_95)\n\n\n\n\n\n\n# Use the model to forecast 12 weeks into the future\nsales95_forecast &lt;- forecast(arima_dept_95, h = 12)\n\n# Plot the forecast. Again, we can use autoplot.\nautoplot(sales95_forecast) +\n  theme_minimal()\n\n\n\n\n\n\n\nWe’re fairly limited in what we can actually tweak when using autoplot(), so instead we can convert the forecast object to a data frame and use ggplot() like normal:\n\n# Get data out of this weird sales95_forecast object\nsales95_forecast\n\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n2013.000       116571.1 109554.8 123587.5 105840.6 127301.7\n2013.019       126102.0 119038.7 133165.2 115299.7 136904.3\n2013.038       120871.5 113761.7 127981.4 109998.0 131745.1\n2013.058       111934.8 104778.7 119091.0 100990.5 122879.2\n2013.077       119470.2 112268.0 126672.3 108455.5 130484.9\n2013.096       126904.7 119656.9 134152.5 115820.1 137989.3\n2013.115       133382.4 126089.2 140675.6 122228.3 144536.5\n2013.135       118094.6 110756.3 125433.0 106871.6 129317.7\n2013.154       128769.9 121386.7 136153.1 117478.2 140061.6\n2013.173       139022.7 131594.8 146450.5 127662.8 150382.5\n2013.192       130502.1 123030.0 137974.3 119074.5 141929.8\n2013.212       124236.7 116720.5 131752.9 112741.7 135731.7\n\nsales95_forecast_tidy &lt;- sweep::sw_sweep(sales95_forecast, \n                                         fitted = TRUE, \n                                         timetk_idx = TRUE)\n\nsales95_forecast_tidy\n\n\n  \n\n\n# For whatever reason, the date column here is a special type of variable called\n# \"yearmon\", which ggplot doesn't know how to deal with (like, we can't zoom in\n# on the plot with coord_cartesian). We use zoo::as.Date() to convert the\n# yearmon variable into a regular date\nsales95_forecast_tidy_real_date &lt;- \n  sales95_forecast_tidy %&gt;% \n  mutate(actual_date = zoo::as.Date(index, frac = 1))\nsales95_forecast_tidy_real_date\n\n\n  \n\n\n# Plot this puppy!\nggplot(sales95_forecast_tidy, aes(x = index, y = value, color = key)) +\n  geom_ribbon(aes(ymin = lo.95, ymax = hi.95), \n              fill = \"#3182bd\", color = NA) +\n  geom_ribbon(aes(ymin = lo.80, ymax = hi.80, fill = key), \n              fill = \"#deebf7\", color = NA, alpha = 0.8) +\n  geom_line(size = 1) + \n  geom_point(size = 0.5) +\n  labs(x = NULL, y = \"sales95\") +\n  scale_y_continuous(labels = scales::comma) +\n  # Zoom in on 2012-2016\n  #coord_cartesian(xlim = ymd(c(\"2004-07-01\", \"2007-07-31\"))) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\nplot_time_series(.data = sales95_forecast_tidy,.date_var = index,.value = value,.color_var = key,.smooth = FALSE)\n\n\n\n\n\nA Bit of Forecasting?\nWe are always interested in the future. We will do this in three ways:\n\nuse Simple Exponential Smoothing\nuse a package called forecast to fit an ARIMA (Auto-regressive Moving Average Integrated Model) model to the data and make predictions for weekly sales;\nAnd do the same using a package called ’prophet`.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Inferential Modelling",
      "🕔 Modelling and Predicting Time Series"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html#conclusion",
    "href": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html#conclusion",
    "title": "🕔 Modelling and Predicting Time Series",
    "section": "Conclusion",
    "text": "Conclusion",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Inferential Modelling",
      "🕔 Modelling and Predicting Time Series"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html#references",
    "href": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html#references",
    "title": "🕔 Modelling and Predicting Time Series",
    "section": "References",
    "text": "References\n1, Shampoo Dataset Brownlee: https://raw.githubusercontent.com/jbrownlee/Datasets/master/shampoo.csv",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Inferential Modelling",
      "🕔 Modelling and Predicting Time Series"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/listing.html",
    "href": "content/courses/Analytics/Modelling/listing.html",
    "title": "Inferential Modelling",
    "section": "",
    "text": "William G. Hunter, Six Statistical Tales. Journal of the Royal Statistical Society. Series D (The Statistician), Vol. 30, No. 2, Jun., 1981, pp. 107-117 https://sci-hub.ru/10.2307/2987563\nAndrew Gelman, Jennifer Hill, Aki Vehtari. Regression and Other Stories, Cambridge University Press, 2023.Available Online",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Inferential Modelling"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/listing.html#references",
    "href": "content/courses/Analytics/Modelling/listing.html#references",
    "title": "Inferential Modelling",
    "section": "",
    "text": "William G. Hunter, Six Statistical Tales. Journal of the Royal Statistical Society. Series D (The Statistician), Vol. 30, No. 2, Jun., 1981, pp. 107-117 https://sci-hub.ru/10.2307/2987563\nAndrew Gelman, Jennifer Hill, Aki Vehtari. Regression and Other Stories, Cambridge University Press, 2023.Available Online",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Inferential Modelling"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/20-Intro-to-Radiant/radiant.html",
    "href": "content/courses/Analytics/Tools/20-Intro-to-Radiant/radiant.html",
    "title": "🐉 Introduction to Radiant",
    "section": "",
    "text": "Radiant is an open-source platform-independent browser-based interface for business analytics in R.\nKey features\n\nExplore: Quickly and easily summarize, visualize, and analyze your data\nCross-platform: It runs in a browser on Windows, Mac, and Linux\nReproducible: Recreate results and share work with others as a state file or an Rmarkdown report\nProgramming: Integrate Radiant’s analysis functions with your own R-code\nContext: Data and examples focus on business applications\n\nRadiant can be used for a variety of tasks\n\nProbability and Stats\nData visualization\nMachine Learning\nData mining\nReport Generation.\n\nRadiant Workflows can also be exported to R/RStudio easily.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Tools",
      "🐉 Introduction to Radiant"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/20-Intro-to-Radiant/radiant.html#introduction-to-radiant",
    "href": "content/courses/Analytics/Tools/20-Intro-to-Radiant/radiant.html#introduction-to-radiant",
    "title": "🐉 Introduction to Radiant",
    "section": "",
    "text": "Radiant is an open-source platform-independent browser-based interface for business analytics in R.\nKey features\n\nExplore: Quickly and easily summarize, visualize, and analyze your data\nCross-platform: It runs in a browser on Windows, Mac, and Linux\nReproducible: Recreate results and share work with others as a state file or an Rmarkdown report\nProgramming: Integrate Radiant’s analysis functions with your own R-code\nContext: Data and examples focus on business applications\n\nRadiant can be used for a variety of tasks\n\nProbability and Stats\nData visualization\nMachine Learning\nData mining\nReport Generation.\n\nRadiant Workflows can also be exported to R/RStudio easily.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Tools",
      "🐉 Introduction to Radiant"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/20-Intro-to-Radiant/radiant.html#installing-radiant",
    "href": "content/courses/Analytics/Tools/20-Intro-to-Radiant/radiant.html#installing-radiant",
    "title": "🐉 Introduction to Radiant",
    "section": "Installing Radiant",
    "text": "Installing Radiant\nYou can download and install Radiant from here:\nhttps://radiant-rstats.github.io/docs/install.html\n\n\n\n\n\n\nImportant\n\n\n\nNOTE: This automatically installs R, RStudio, and Radiant on your machine. This is going to be convenient when we start working in R too!\nIt also installs Latex, which allows us to create crisp PDF reports of our analyses.\nThe version of R may not be the latest one, though…",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Tools",
      "🐉 Introduction to Radiant"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/20-Intro-to-Radiant/radiant.html#basic-tutorials-with-radiant",
    "href": "content/courses/Analytics/Tools/20-Intro-to-Radiant/radiant.html#basic-tutorials-with-radiant",
    "title": "🐉 Introduction to Radiant",
    "section": "Basic Tutorials with Radiant",
    "text": "Basic Tutorials with Radiant\nAll the Tutorials are available on Youtube; the links to individual videos are on the page below\nhttps://radiant-rstats.github.io/docs/radiant-tutorial-series.html",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Tools",
      "🐉 Introduction to Radiant"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html",
    "title": "🐉 Introduction to R and RStudio",
    "section": "",
    "text": "At the end of this Lab, we will:\n\nhave installed R and RStudio on our machines\nunderstood how to add additional R-packages for specific features and graphic capability\nrun code within RStudio and interpret the results\nhave learnt to look for help within R and RStudio\nlearnt to use Quarto in R, which a document format for reproducible report generation",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Tools",
      "🐉 Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#goals",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#goals",
    "title": "🐉 Introduction to R and RStudio",
    "section": "",
    "text": "At the end of this Lab, we will:\n\nhave installed R and RStudio on our machines\nunderstood how to add additional R-packages for specific features and graphic capability\nrun code within RStudio and interpret the results\nhave learnt to look for help within R and RStudio\nlearnt to use Quarto in R, which a document format for reproducible report generation",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Tools",
      "🐉 Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#introduction-to-r-and-rstudio",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#introduction-to-r-and-rstudio",
    "title": "🐉 Introduction to R and RStudio",
    "section": "\n Introduction to R and RStudio",
    "text": "Introduction to R and RStudio\nThis guide will lead you through the steps to install and use R, a free and open-source software environment for statistical computing and graphics.\nWhat is R?\n\n\nR is the name of the programming language itself, based off S from Bell Labs, which users access through a command-line interpreter (&gt;)\n\nWhat is RStudio?\n\n\nRStudio is a powerful and convenient user interface that allows you to access the R programming language along with a lot of other bells and whistles that enhance functionality (and sanity).\n\nOur end goal is to get you looking at a screen like this:\n\n\nRStudio Default Window",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Tools",
      "🐉 Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#install-r",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#install-r",
    "title": "🐉 Introduction to R and RStudio",
    "section": "\n Install R",
    "text": "Install R\nInstall R from CRAN, the Comprehensive R Archive Network. Please choose a precompiled binary distribution for your operating system.\n\n Check in\nLaunch R by clicking this logo  in your Applications. You should see one console with a command line interpreter. Try typing 2 + 2 and check !\nClose R.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Tools",
      "🐉 Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#install-rstudio",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#install-rstudio",
    "title": "🐉 Introduction to R and RStudio",
    "section": "\n Install RStudio",
    "text": "Install RStudio\nInstall the free, open-source edition of RStudio: http://www.rstudio.com/products/rstudio/download/\nRStudio provides a powerful user interface for R, called an integrated development environment. RStudio includes:\n\na console (the standard command line interface: &gt;),\na syntax-highlighting editor that supports direct code execution, and\ntools for plotting, history, debugging and work space management.\n\n\n Check in\nLaunch RStudio. You should get a window similar to the screenshot you see here, but yours will be empty. Look at the bottom left pane: this is the same console window you saw when you opened R in step @Check-In-R\n\nPlace your cursor where you see &gt; and type x &lt;- 2 + 2 again hit enter or return, then type x, and hit enter/return again.\nIf [1] 4 prints to the screen, you have successfully installed R and RStudio, and you can move onto installing packages.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Tools",
      "🐉 Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#install-packages",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#install-packages",
    "title": "🐉 Introduction to R and RStudio",
    "section": "\n Install packages",
    "text": "Install packages\nThe version of R that you just downloaded is considered base R, which provides you with good but basic statistical computing and graphics powers. For analytical and graphical super-powers, you’ll need to install add-on packages, which are user-written, to extend/expand your R capabilities. Packages can live in one of two places:\n\nThey may be carefully curated by CRAN (which involves a thorough submission and review process), and thus are easy install using install.packages(\"name_of_package\", dependencies = TRUE) in your CONSOLE.\nPersonal repositories of packages created by practitioners, which are usually in Github.\n\nPlace your cursor in the CONSOLE again (where you last typed x and [4] printed on the screen). You can use the first method to install the following packages directly from CRAN, all of which we will use:\n\nknitr\ntidyverse\nggformula\n\nbabynames\n\n\n\n\n\n\n\n\nInstallation and Usage of R Packages!\n\n\n\n\nTo install a package, you put the name of the package in quotes as in install.packages(\"name_of_package\"). Mind your use of quotes carefully with packages.\nTo use an already installed package, you must load it first, as in library(name_of_package), leaving the name of the package bare. You only need to do this once per RStudio session.\n\n\n\n\nIf you want help, no quotes are needed: help(name_of_package) or ?name_of_package.\nIf you want the citation for a package (and you should give credit where credit is due), ask R as in citation(\"name_of_package\").",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Tools",
      "🐉 Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#using-quarto",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#using-quarto",
    "title": "🐉 Introduction to R and RStudio",
    "section": "\n Using Quarto",
    "text": "Using Quarto\nWe will get acquainted with the Quarto Document format, which allows us to mix text narrative, code, code-developed figures and items from the web in a seamless document. Quarto can be used to generate multiple formats such as HTML, Word, PDF from the same text/code file. Something that can:\n\nprovide a visualization\nprovide insight\ntell a story\nis reproducible\nbe a call to action or a recommendation\n\n\n\n Setting up Quarto\nQuarto is already installed along with RStudio!! We can check if all is in order by running a check in the Terminal in RStudio. \nThe commands are:\n\nquarto check install\nquarto check knitr\n\nIf these come out with no errors then we are ready to fire up our first Quarto document.\n\n\n\n Practice\nLet us now create a brand new Quarto document, create some graphs in R and add some narrative text and see how we can generate our first report!\n\nFire up a new Quarto document by going to: File -&gt; New File -&gt; Quarto Document.\n\nGive a title to your document ( “My First Quarto Document”, for example.\nChange the author name to your own! Keep HTML as your output format\nSwitch to Visual mode, if it is not already there. Use the visual mode tool bar.\n\n\n\n\nClick on the various buttons to see what happens. Try to create Sections, code chunks, embedding images and tables.\n\n\n\n\n\n\nAdd Anything Shortcut\n\n\n\nTry the “add anything” shortcut! Type “/” anywhere in your Quarto Doc, while in Visual Mode, and choose what you want to add from the drop-down menu!\n\n\n\nCreate a code chunk as shown below. You can either use the visual tool bar to create it, or simply hit the copy button in the code chunk display on this website and paste the results into your Quarto document. Check every step!\n\n\n```{r}\n#| label: setup\nknitr::opts_chunk$set(warnings = TRUE, errors = FALSE, messages = TRUE)\nlibrary(knitr) # to use this….document! More later!!\nlibrary(tidyverse) # Data Management and Plotting!!\nlibrary(babynames) # A package containing, yes, Baby Names\n```\n\n\nHit the green “play” button to run this “setup” chunk to include in your R session all the installed packages you need.\nLet us greet our data first !!\n\n\n```{r}\n#| label: babynames data\nglimpse(babynames)\nhead(babynames) \ntail(babynames)\nnames(babynames)\n```\n\nRows: 1,924,665\nColumns: 5\n$ year &lt;dbl&gt; 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880,…\n$ sex  &lt;chr&gt; \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", …\n$ name &lt;chr&gt; \"Mary\", \"Anna\", \"Emma\", \"Elizabeth\", \"Minnie\", \"Margaret\", \"Ida\",…\n$ n    &lt;int&gt; 7065, 2604, 2003, 1939, 1746, 1578, 1472, 1414, 1320, 1288, 1258,…\n$ prop &lt;dbl&gt; 0.07238359, 0.02667896, 0.02052149, 0.01986579, 0.01788843, 0.016…\n\n\n\n  \n\n\n  \n\n\n\n[1] \"year\" \"sex\"  \"name\" \"n\"    \"prop\"\n\n\n\nIf you have done the above and produced sane-looking output, you are ready for the next bit. Use the code below to create a new data frame called my_name_data.\n\n\n```{r}\n#| label: manipulate name data\nmy_name_data &lt;- babynames %&gt;%\n  filter(name == \"Arvind\" | name == \"Aravind\") %&gt;% \n  filter(sex == \"M\") \n```\n\n\nThe first bit makes a new dataset called my_name_data that is a copy of the babynames dataset\nthe %&gt;% (pipe) tells you we are doing some other stuff to it later.1\n\nThe second bit filters our babynames to only keep rows where the name is either Arvind or Aravind (read | as “or”.)\nThe third bit applies another filter to keep only those where sex is male.\n\nLet’s check out the data.\n\n```{r}\nmy_name_data\nglimpse(my_name_data)\n```\n\n\n  \n\n\n\nRows: 61\nColumns: 5\n$ year &lt;dbl&gt; 1970, 1972, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983,…\n$ sex  &lt;chr&gt; \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", …\n$ name &lt;chr&gt; \"Arvind\", \"Arvind\", \"Arvind\", \"Arvind\", \"Arvind\", \"Arvind\", \"Arvi…\n$ n    &lt;int&gt; 5, 8, 7, 5, 9, 6, 7, 6, 8, 6, 7, 7, 7, 13, 8, 11, 6, 8, 12, 10, 1…\n$ prop &lt;dbl&gt; 2.620e-06, 4.780e-06, 4.310e-06, 3.060e-06, 5.260e-06, 3.510e-06,…\n\n\n\nAgain, if you have sane-looking output here, move along to plotting the data!\n\n\n```{r}\n#| label:  plot_name_data\n\nplot &lt;- gf_line(prop ~ year, color = ~ name, \n        data = my_name_data)\n```\n\nNow if you did this right, you will not see your plot!\n\nBecause we saved the ggplot with a name (plot), R just saved the object for you. But check out the top right pane in RStudio again: under the Environment pane you should see plot, so it is there, you just have to ask for it. Here’s how:\n\n\n```{r}\nplot \n```\n\n\n\n\n\n\n\n\nNow hit the Render button on your Visual toolbar and see what happens!! Try to use the drop down menu next to it and see if there are more output file options!!\n\n Make a new name plot!\n\nEdit my code above to create a new dataset. Pick 2 names to compare how popular they each are (these could be different spellings of your own name, like I did, but you can choose any 2 names that are present in the dataset), and create a new data object with a new name. Make the new plot, changing the name of the data= argument my_name_data in gf_line to the name of your new dataset.\nWrite narratives comments wherever suitable in your Quarto document. Make sure you don’t type inside your code chunks. See if you can write your comments in sections which you can create with your visual tool bar, or by using the “add anything” shortcut.\nSave your work ( your Quarto document itself) so you can share your favorite plot.\nShare your Plot: You will not like the looks of your plot if you mouse over to Export and save it. Instead, use ggplot2’s command for saving a plot with sensible defaults:\n\nType help(ggsave) in your Console.\n```{r}\n#| label: Saving\n\nggsave(\"file_name_here.pdf\", plot) # please make the filename unique!\n```",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Tools",
      "🐉 Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#conclusions",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#conclusions",
    "title": "🐉 Introduction to R and RStudio",
    "section": "\n Conclusions",
    "text": "Conclusions\nWe have installed R, RStudio and created our Quarto document, complete with graphs and narrative text. We also rendered our Quarto doc into HTML and other formats!",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Tools",
      "🐉 Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#readings",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#readings",
    "title": "🐉 Introduction to R and RStudio",
    "section": "\n Readings",
    "text": "Readings\n\nR for Data Science, Workflow: Basics Chapter: http://r4ds.had.co.nz/workflow-basics.html\nModern Dive, Getting Started Chapter: http://moderndive.com/2-getting-started.html\nR & RStudio Basics: https://bookdown.org/chesterismay/rbasics/3-rstudiobasics.html\nRStudio IDE Cheatsheet: https://github.com/rstudio/cheatsheets/blob/master/rstudio-ide.pdf",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Tools",
      "🐉 Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#references",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#references",
    "title": "🐉 Introduction to R and RStudio",
    "section": "\n References",
    "text": "References\n\nhttps://rmarkdown.rstudio.com/index.html\n\nhttps://ysc-rmarkdown.netlify.app/slides/01-basics.html Nice RMarkdown presentation and “code movies” !\nhttps://www.markdowntutorial.com\nhttps://quarto.org/docs/get-started/hello/rstudio.html\n\nhttps://quarto.org/docs/authoring/markdown-basics.html How to do more with Quarto HTML format",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Tools",
      "🐉 Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#assignments",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#assignments",
    "title": "🐉 Introduction to R and RStudio",
    "section": "\n Assignment(s)",
    "text": "Assignment(s)\n\nComplete the markdown tutorial in [reference 1]\nLook through the Slides in [reference 2]\nCreate a fresh Quarto document and use as many as possible of the RMarkdown constructs from the Cheatsheet [reference 1]",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Tools",
      "🐉 Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#footnotes",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/r-intro.html#footnotes",
    "title": "🐉 Introduction to R and RStudio",
    "section": "Footnotes",
    "text": "Footnotes\n\nInsert the pipe character using the keyboard shortcutCTRL + SHIFT + M.↩︎",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Tools",
      "🐉 Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html",
    "href": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html",
    "title": "ML - Clustering",
    "section": "",
    "text": "Quoting from http://baoqiang.org/?p=579\n\n\nThese two are arguably the two commonly used cluster methods. One of the reasons is that they are easy to use and also somehow straightforward. So how do they work?\nk-Nearest-Neighbour: Provide N n-dimension entries with known associated classes for each entry, the number of classes is k, that is, \\[\n\\{\\vec{x_i}, y_i\\} ,\\ \\vec{x_i} \\in\\ {\\Re^{n}}\\ , y_i\\ = \\{c_1,...c_k\\},\ni = 1...N\n\\]\nFor a new entry \\(\\vec{v_j}\\), to which class should it belong? We need use a distance measure to get the k closest entries of the new entry , the final decision is simple majority vote based the closest k neighbors. The distance metric could be euclidean or other similar ones.\n\n\nK-means: Given N n-dimension entries and classify them in k classes. At first, we randomly choose k entries and assign them to k clusters. They are the seed classes. Then we calculate the distance between each entry and each class. Each entry will be assigned into one class in terms of the its distance to each class, i.e., assign the entry to its closest class. After the assignment is complete, we then calculate the centroid of each class based on their new members. After the centroid calculation, we go back to the distance calculation and therefore new round classification. We stop the iteration when there is convergence,i.e,, no new centroid and classification.\nThe two methods are all semi-supervised learning algorithms because they do need we provide the number of clusters prior the clustering.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Predictive Modelling",
      "ML - Clustering"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#introduction",
    "href": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#introduction",
    "title": "ML - Clustering",
    "section": "",
    "text": "Quoting from http://baoqiang.org/?p=579\n\n\nThese two are arguably the two commonly used cluster methods. One of the reasons is that they are easy to use and also somehow straightforward. So how do they work?\nk-Nearest-Neighbour: Provide N n-dimension entries with known associated classes for each entry, the number of classes is k, that is, \\[\n\\{\\vec{x_i}, y_i\\} ,\\ \\vec{x_i} \\in\\ {\\Re^{n}}\\ , y_i\\ = \\{c_1,...c_k\\},\ni = 1...N\n\\]\nFor a new entry \\(\\vec{v_j}\\), to which class should it belong? We need use a distance measure to get the k closest entries of the new entry , the final decision is simple majority vote based the closest k neighbors. The distance metric could be euclidean or other similar ones.\n\n\nK-means: Given N n-dimension entries and classify them in k classes. At first, we randomly choose k entries and assign them to k clusters. They are the seed classes. Then we calculate the distance between each entry and each class. Each entry will be assigned into one class in terms of the its distance to each class, i.e., assign the entry to its closest class. After the assignment is complete, we then calculate the centroid of each class based on their new members. After the centroid calculation, we go back to the distance calculation and therefore new round classification. We stop the iteration when there is convergence,i.e,, no new centroid and classification.\nThe two methods are all semi-supervised learning algorithms because they do need we provide the number of clusters prior the clustering.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Predictive Modelling",
      "ML - Clustering"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#workflow-using-orange",
    "href": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#workflow-using-orange",
    "title": "ML - Clustering",
    "section": "Workflow using Orange",
    "text": "Workflow using Orange",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Predictive Modelling",
      "ML - Clustering"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#workflow-using-radiant",
    "href": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#workflow-using-radiant",
    "title": "ML - Clustering",
    "section": "Workflow using Radiant",
    "text": "Workflow using Radiant",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Predictive Modelling",
      "ML - Clustering"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#workflow-using-r",
    "href": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#workflow-using-r",
    "title": "ML - Clustering",
    "section": "Workflow using R",
    "text": "Workflow using R",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Predictive Modelling",
      "ML - Clustering"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#conclusion",
    "href": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#conclusion",
    "title": "ML - Clustering",
    "section": "Conclusion",
    "text": "Conclusion",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Predictive Modelling",
      "ML - Clustering"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#references",
    "href": "content/courses/Analytics/Predictive/Modules/40-Clustering/index.html#references",
    "title": "ML - Clustering",
    "section": "References",
    "text": "References\n\nK-means Cluster Analysis. UC Business Analytics R Programming Guide https://uc-r.github.io/kmeans_clustering#optimal\nThean C Lim. Clustering: k-means, k-means ++ and gganimate. https://theanlim.rbind.io/post/clustering-k-means-k-means-and-gganimate/",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Predictive Modelling",
      "ML - Clustering"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html",
    "title": "ML - Classification",
    "section": "",
    "text": "Have you played a Childhood Game called 20 Questions? Someone has a “target” entity in mind ( a person or a thing or a literary character) and the others need to discover that entity by asking 20 questions.\n\nHow does one create questions in the game?\n\nCategories?\nNumbers? How?\nComparisons?\n\n\nWhat sort of answers can you expect for each question?",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Predictive Modelling",
      "ML - Classification"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#a-childhood-game",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#a-childhood-game",
    "title": "ML - Classification",
    "section": "",
    "text": "Have you played a Childhood Game called 20 Questions? Someone has a “target” entity in mind ( a person or a thing or a literary character) and the others need to discover that entity by asking 20 questions.\n\nHow does one create questions in the game?\n\nCategories?\nNumbers? How?\nComparisons?\n\n\nWhat sort of answers can you expect for each question?",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Predictive Modelling",
      "ML - Classification"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#twenty-questions-game-as-a-play-with-data",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#twenty-questions-game-as-a-play-with-data",
    "title": "ML - Classification",
    "section": "Twenty Questions Game as a Play with Data…",
    "text": "Twenty Questions Game as a Play with Data…\nAssuming we think of a 20Q Target as say, celebrity singer like Taylor Swift, or a cartoon character like Thomas the Tank Engine, what would an underlying “data structure” look like? We would ask Questions for instance in the following order to find the target of Taylor Swift:\n\nHuman?(Yes)\nLiving?(Yes)\nMale?(No)\nCelebrity?(Yes)\nMusic?(Yes)\nUSA?(Yes)\n\nOh…Taylor Swift!!!\nLet us try to construct the “datasets” underlying this game!\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nOccupation\nSex\nLiving\nNationality\ngenre\npet\n\n\nTaylor Swift\nSinger\nF\nTRUE\nUSA\ncountry/rock\nScottish Fold Cats\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nType\nLiving\nhuman\nNationality\ncolour\nmaterial\n\n\nThomas, the Tank Engine\nCartoon Character\nFALSE\nFALSE\nUK\nblue\nmetal\n\n\n\n\nIt should be fairly clear that the Questions we ask are based on the COLUMNs in the respective 1-row datasets! The TARGET Column in both cases is the name column.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Predictive Modelling",
      "ML - Classification"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#what-is-a-decision-tree",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#what-is-a-decision-tree",
    "title": "ML - Classification",
    "section": "What is a Decision Tree?",
    "text": "What is a Decision Tree?\nCan you imagine how the 20 Questions Game can be shown as a tree?\n\n\n\n\n\n\nEach Question we ask, based on one of the Feature columns, begets a Yes/NO answer and we turn the left or right accordingly. When we arrive at the leaf, we should be in a position to guess the answer !",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Predictive Modelling",
      "ML - Classification"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#twenty-times-20-questions",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#twenty-times-20-questions",
    "title": "ML - Classification",
    "section": "Twenty times 20 Questions !!",
    "text": "Twenty times 20 Questions !!\nWhat if the dataset we had contained many rows, instead of just one row? How would we play the 20Q Game in this situation? Here is a sample of the famous penguins dataset:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\nAdelie\nTorgersen\n41.1\n18.6\n189\n3325\nmale\n2009\n\n\nAdelie\nTorgersen\n38.7\n19.0\n195\n3450\nfemale\n2007\n\n\nChinstrap\nDream\n50.9\n19.1\n196\n3550\nmale\n2008\n\n\nAdelie\nBiscoe\n37.9\n18.6\n193\n2925\nfemale\n2009\n\n\nGentoo\nBiscoe\n44.5\n14.7\n214\n4850\nfemale\n2009\n\n\nAdelie\nDream\n32.1\n15.5\n188\n3050\nfemale\n2009\n\n\nChinstrap\nDream\n45.9\n17.1\n190\n3575\nfemale\n2007\n\n\nAdelie\nDream\n40.8\n18.4\n195\n3900\nmale\n2007\n\n\nGentoo\nBiscoe\n45.3\n13.7\n210\n4300\nfemale\n2008\n\n\nGentoo\nBiscoe\n48.5\n14.1\n220\n5300\nmale\n2008\n\n\nAdelie\nDream\n37.0\n16.9\n185\n3000\nfemale\n2007\n\n\nAdelie\nTorgersen\n39.2\n19.6\n195\n4675\nmale\n2007\n\n\n\n\n\nAs before, we would need to look at the dataset as containing a TARGET column which we want to predict using several other FEATURE columns. Let us choose species.\nWhen we look at the FEATURE columns, We would need to formulate questions based on entire columns at a time. For instance:\n\n\n“Is the bill_length_mm* greater than 45mm?” considers the entire bill_length_mm* FEATURE column\nIs the sex female? considers the entire sex column\n\nIf the specific FEATURE column is a Numerical (N) variable, the question would use some “thresholding” as shown in the question above, to convert the Numerical Variable into a Categorical variable.\nIf a specific FEATURE column is a Categorical (C) variable, the question would be like a filter operation in Excel.\nEither way, we end up answering with a smaller and smaller subset of rows in the dataset, to which the questions are answered with a Yes. It is as if we played many 20 Questions games in parallel, since there are so many simultaneous “answers”!\nOnce we exhaust all the FEATURE columns, then what remains is a subset (i.e. rows) of the original dataset and we read off the TARGET column, which should now contain a set of identical entries, e.g. “Adelie”. Thus we can extend a single-target 20Q game to a multiple-target one using a larger dataset. ( Note how the multiple targets are all the same: “Adelie”, or “Gentoo”, or “Chinstrap”)\nThis forms the basic intuition for a Machine Learning Algorithm called a Decision Tree.\nDecision Tree in Orange\nLet us visualize this Decision Tree in Orange. Look at the now famous penguins dataset, available here:\nhttps://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\nWe see that there are three species of penguins, that live on three islands. The measurements for each penguin are flipper_length_mm, bill_length_mm, bill_depth_mm, and body_mass_g.\n\nTask 1: Create a few data visualizations for the variables, and pairs of variables from this dataset.\nTask 2: Can you inspect the visualizations and imagine how each of this dataset can be used in a 20 Questions Game, to create a Decision Tree for this dataset as shown below?\n\n\n\nPenguins Decision Tree!\n\nWhat did we learn?\n\nThe 20Q Game can be viewed as a “Decision Tree” of Questions and Answers,\nEach fork in the game is a Question.\nDepending upon whether the current answer is yes or no, we turn in one direction or the other.\nEach of our questions is based on the information available in one or other of the columns!!\nWe arrive at a final “answer” or “target” after a particular sequence of yes/no answers. This is the one of the leaf nodes in the Tree.\nThe island and the species columns are categories and are especially suited to being the targets for a 20 Questions Game.\nWe can therefore use an entire column of data as our 20Questions target, rather than just one entity, person.\n\nThis is how we will use this Game as a Model for our first ML algorithm, classification using Decision Trees.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Predictive Modelling",
      "ML - Classification"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#how-do-we-make-predictions-using-our-decision-tree",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#how-do-we-make-predictions-using-our-decision-tree",
    "title": "ML - Classification",
    "section": "How do we Make Predictions using our Decision Tree",
    "text": "How do we Make Predictions using our Decision Tree\nOur aim is to make predictions. Predictions of what? When we are given new unseen data in the same format, we should be able to predict TARGET variable using the same FEATURE columns.\nNOTE: This that is usually a class/category (We CAN also predict a numerical value with a Decision Tree; but we will deal with that later.)\nIn order to make predictions with completely unseen data, we need to first check if the algorithm is working well with known data. The way to do this is to use a large portion of data to design the tree, and then use the tree to predict some aspect of the remaining, but similar, data. Let us split the penguins dataset into two pieces: a training set to design our tree, and a test set to check how it is working.\nDownload this penguin tree file and open it in Orange.\nHow good are the Predictions? What is the Classification Error Rate?",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Predictive Modelling",
      "ML - Classification"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#how-many-trees-do-we-need-enter-the-random-forest",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#how-many-trees-do-we-need-enter-the-random-forest",
    "title": "ML - Classification",
    "section": "How Many Trees do we Need? Enter the Random Forest!",
    "text": "How Many Trees do we Need? Enter the Random Forest!\nCheck all your individual Decision Trees: do they ask the same Questions? Do they fork in the same way? Yes, they all seem to use the same set of parameters to reach the target. So they are capable of being “biased” and make the same mistakes. So we ask: Does it help to use more than one tree, if all the questions/forks in the Trees are similar?\nNo…we need different Trees to be able to ask different questions, based on different variables or features in the data. That will make the Trees as different as possible and so…unbiased. This is what we also saw when we played 20Q: offbeat questions opened up some avenues for predicting the answer/target.\nA forest of such trees is called the Wild Wood a Random Forest !",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Predictive Modelling",
      "ML - Classification"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#an-introduction-to-random-forests",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#an-introduction-to-random-forests",
    "title": "ML - Classification",
    "section": "An Introduction to Random Forests",
    "text": "An Introduction to Random Forests\nIn the Random Forest method, we do as follows:\n\nSplit the dataset into training and test subsets (70::30 proportion is very common). Keep aside the testing dataset for final testing.\nDecide on a number of trees, say 100-500 in the forest.\nTake the training dataset and repeatedly sample some of the rows in it. Rows can be repeated too; this is called bootstrap sampling.\nGive this sampled training set to each tree. Each tree develops a question from this dataset, in a random fashion, using a randomly chosen variable. E.g. with penguins, if our target is species, then some trees will will use island, some others will use body_mass_g and some others may use bill_length_mm.\nEach tree will “grow its questions” in a unique way !! Since the questions are possibly based on a different variable at each time, the trees will grow in very different ways.\nStop when the required accuracy has been achieved (the sets contain observations/rows from only one species predominantly)\nWith the test set let each tree vote on which species it has decided upon. Take the majority vote.\n\nPhew!!\nLet’s get a visual sense of how all this works:\nhttps://waternova.github.io/random-forest-viz/",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Predictive Modelling",
      "ML - Classification"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#random-forest-classification-for-heart-patients",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#random-forest-classification-for-heart-patients",
    "title": "ML - Classification",
    "section": "Random Forest Classification for Heart Patients",
    "text": "Random Forest Classification for Heart Patients\nDo you want to develop an ML model for heart patients? We have a dataset of heart patients at the University of California, Arvind Irvine ML Dataset Repository\nHeart Patient Data. Import into Orange !!\nWhat are the variables?\n\n(age): age in years\n(sex): 1 = male; 0 = female\n(cp): chest-pain type( 4 types, 1/2/3/4)\n(trestbps): resting blood pressure (in mm Hg on admission to the hospital)\n(chol) : serum cholesterol in mg/dl\n(fbs): (fasting blood sugar &gt; 120 mg/dl) (1 = true; 0 = false)\n(restecg): resting electrocardiograph results (0 = normal; 1= ST-T wave abnormality; 3 = LV hypertrophy)\n(thalach): maximum heart rate achieved\n(exang): exercise induced angina (1 = yes; 0 = no) (remember Puneet Rajkumar)\n(oldpeak): ST depression induced by exercise relative to rest\n(slope): the slope of the peak exercise ST segment\n\nValue 1: upsloping\nValue 2: flat\nValue 3: downsloping\n\n\n(ca): number of major vessels (0-3) colored by fluoroscopy\n(thal): 3 = normal; 6 = fixed defect; 7 = reversible defect\n(num) : the target attribute, diagnosis of heart disease (angiographic disease status)\n\nValue 0: &lt; 50% diameter narrowing\nValue 1: &gt; 50% diameter narrowing\n(in any major vessel: attributes 59 through 68 are vessels)\n\n\n\nWe will create a Random Forest Model for this dataset, and compare with the Desision Tree for the same dataset.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Predictive Modelling",
      "ML - Classification"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#how-good-is-my-random-forest",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#how-good-is-my-random-forest",
    "title": "ML - Classification",
    "section": "How good is my Random Forest?",
    "text": "How good is my Random Forest?\n\nClassification Error\n\nGini Impurity\n\nCross Entropy",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Predictive Modelling",
      "ML - Classification"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#references",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/index.html#references",
    "title": "ML - Classification",
    "section": "References",
    "text": "References\n\nhttps://towardsdatascience.com/data-science-made-easy-data-modeling-and-prediction-using-orange-f451f17061fa\nThe beauty of Random Forests: https://orangedatamining.com/blog/2016/12/22/the-beauty-of-random-forest/\nPythagorean Trees for Random Forests: https://orangedatamining.com/blog/2016/07/29/pythagorean-trees-and-forests/\ndata.tree sample applications, Christoph Glur, 2020-07-31. https://cran.r-project.org/web/packages/data.tree/vignettes/applications.html\nhttps://ryjohnson09.netlify.app/post/caret-and-tidymodels/",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Predictive Modelling",
      "ML - Classification"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html",
    "href": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html",
    "title": "🐉 Intro to Orange",
    "section": "",
    "text": "Orange is a visual drag-and-drop tool for\n\nData visualization\n\nMachine Learning\n\nData mining\n\nand much more. Here is what it looks like:\n\nAll operations are done using a visual menu-driven interface. The visuals can be exported to PNG/SVG/PDF and the entire workflow can be exported into a Report Form and edited for presentation and sharing.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Predictive Modelling",
      "🐉 Intro to Orange"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#introduction-to-orange",
    "href": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#introduction-to-orange",
    "title": "🐉 Intro to Orange",
    "section": "",
    "text": "Orange is a visual drag-and-drop tool for\n\nData visualization\n\nMachine Learning\n\nData mining\n\nand much more. Here is what it looks like:\n\nAll operations are done using a visual menu-driven interface. The visuals can be exported to PNG/SVG/PDF and the entire workflow can be exported into a Report Form and edited for presentation and sharing.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Predictive Modelling",
      "🐉 Intro to Orange"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#installing-orange",
    "href": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#installing-orange",
    "title": "🐉 Intro to Orange",
    "section": "Installing Orange",
    "text": "Installing Orange\nYou can download and install Orange from here:\nhttps://orangedatamining.com/download/",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Predictive Modelling",
      "🐉 Intro to Orange"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#basic-usage-of-orange",
    "href": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#basic-usage-of-orange",
    "title": "🐉 Intro to Orange",
    "section": "Basic Usage of Orange",
    "text": "Basic Usage of Orange",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Predictive Modelling",
      "🐉 Intro to Orange"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#orange-workflows",
    "href": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#orange-workflows",
    "title": "🐉 Intro to Orange",
    "section": "Orange Workflows",
    "text": "Orange Workflows",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Predictive Modelling",
      "🐉 Intro to Orange"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#widgets-and-channels",
    "href": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#widgets-and-channels",
    "title": "🐉 Intro to Orange",
    "section": "Widgets and Channels",
    "text": "Widgets and Channels",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Predictive Modelling",
      "🐉 Intro to Orange"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#loading-data-into-orange",
    "href": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#loading-data-into-orange",
    "title": "🐉 Intro to Orange",
    "section": "Loading data into Orange",
    "text": "Loading data into Orange\n\n\nWe are good to get started with Orange!!",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Predictive Modelling",
      "🐉 Intro to Orange"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#simple-visuals-using-orange",
    "href": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#simple-visuals-using-orange",
    "title": "🐉 Intro to Orange",
    "section": "Simple Visuals using Orange",
    "text": "Simple Visuals using Orange\nLet us create some simple visualizations using Orange.\n\nUse the File Widget to import the iris dataset into your session\nUse the Data Table Widget to look at the data, and note its variable names\nUse the Visualization Widgets ( Scatter Plot, Bar Plot, and Distributions) to look at the properties of the variables, and examine relationships between them.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Predictive Modelling",
      "🐉 Intro to Orange"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#reference",
    "href": "content/courses/Analytics/Predictive/Modules/10-IntroOrange/index.html#reference",
    "title": "🐉 Intro to Orange",
    "section": "Reference",
    "text": "Reference\n\nIntroduction to Data Mining-Working notes for the hands-on course with Orange Data Mining. (Download file)",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Predictive Modelling",
      "🐉 Intro to Orange"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/30-Japan-HisayeYamamoto/index.html",
    "href": "content/courses/ISTW/Modules/30-Japan-HisayeYamamoto/index.html",
    "title": "Japan - Hisaye Yamamoto",
    "section": "",
    "text": "From https://encyclopedia.densho.org/Hisaye_Yamamoto/\n\nA Southern California Nisei writer of short stories, Hisaye Yamamoto (1921–2011) was among the first Japanese American writers to win national renown after World War II. Yamamoto’s upbringing in an immigrant farming community and her incarceration in a World War II U.S. government prison camp formed the basis for some of her best-known stories, notable for their sensitive portrayal of the emotionally and artistically constricted lives of Issei women and intergenerational family dynamics. Oblique, often deadpan in delivery and told with quiet humor and bracing candor, they reveal the love affairs, madness, psychic and physical brutality that lay beneath the placid surface of Issei and Nisei life. The subject matter, precision and grace of Yamamoto’s works have led critics to compare her to short story masters Katherine Mansfield, Flannery O’Connor, and Grace Paley."
  },
  {
    "objectID": "content/courses/ISTW/Modules/30-Japan-HisayeYamamoto/index.html#hisaye-yamamoto",
    "href": "content/courses/ISTW/Modules/30-Japan-HisayeYamamoto/index.html#hisaye-yamamoto",
    "title": "Japan - Hisaye Yamamoto",
    "section": "",
    "text": "From https://encyclopedia.densho.org/Hisaye_Yamamoto/\n\nA Southern California Nisei writer of short stories, Hisaye Yamamoto (1921–2011) was among the first Japanese American writers to win national renown after World War II. Yamamoto’s upbringing in an immigrant farming community and her incarceration in a World War II U.S. government prison camp formed the basis for some of her best-known stories, notable for their sensitive portrayal of the emotionally and artistically constricted lives of Issei women and intergenerational family dynamics. Oblique, often deadpan in delivery and told with quiet humor and bracing candor, they reveal the love affairs, madness, psychic and physical brutality that lay beneath the placid surface of Issei and Nisei life. The subject matter, precision and grace of Yamamoto’s works have led critics to compare her to short story masters Katherine Mansfield, Flannery O’Connor, and Grace Paley."
  },
  {
    "objectID": "content/courses/ISTW/Modules/30-Japan-HisayeYamamoto/index.html#story",
    "href": "content/courses/ISTW/Modules/30-Japan-HisayeYamamoto/index.html#story",
    "title": "Japan - Hisaye Yamamoto",
    "section": "Story",
    "text": "Story\nWe will read Yamamoto’s story Seventeen Syllables."
  },
  {
    "objectID": "content/courses/ISTW/Modules/30-Japan-HisayeYamamoto/index.html#themes",
    "href": "content/courses/ISTW/Modules/30-Japan-HisayeYamamoto/index.html#themes",
    "title": "Japan - Hisaye Yamamoto",
    "section": "Themes",
    "text": "Themes\n\nInter-generational Conflict\nLiving as an Expat\nFirst Love\nTeenage Romance\n“Arranged Marriage” (picture marriage)\nDeception…"
  },
  {
    "objectID": "content/courses/ISTW/Modules/30-Japan-HisayeYamamoto/index.html#additional-material",
    "href": "content/courses/ISTW/Modules/30-Japan-HisayeYamamoto/index.html#additional-material",
    "title": "Japan - Hisaye Yamamoto",
    "section": "Additional Material",
    "text": "Additional Material\n\n\nNotes and References\n\nA Beautiful Scrolly Story about Yamamoto and her influence. https://artsandculture.google.com/story/hisaye-yamamoto-an-american-story-american-writers-museum/OAVRaqAwV3tpLA?hl=en\nReading Yamamoto. https://faculty.georgetown.edu/bassr/heath/syllabuild/iguide/yamamoto.html\nGangs of Wasseypur: “Permission Leni Chahiye, Na?” https://youtu.be/To54dv2jnJg"
  },
  {
    "objectID": "content/courses/ISTW/Modules/30-Japan-HisayeYamamoto/index.html#song-for-the-story",
    "href": "content/courses/ISTW/Modules/30-Japan-HisayeYamamoto/index.html#song-for-the-story",
    "title": "Japan - Hisaye Yamamoto",
    "section": "Song for the Story",
    "text": "Song for the Story\nA song from 70 years ago? For Teens!?? You must be joking!!! But just maybe it could work….so here goes!\nStarring: Bharat Bhushan & Madhubala & Pradeep Kumar\nArtist: Mohammed Rafi & Lata Mangeshkar\nLyrics: Rajendra Krishan\nComposed: Madan Mohan Kohli\nMovie/Album: Gateway Of India (1957)\n\n\nHere is the audio track alone:\n Your browser does not support the audio tag;  for browser support, please see:  https://www.w3schools.com/tags/tag_audio.asp \n\n\n\n\n\n\nHindi (and some Urdu!) lyrics\nEnglish Translation\n\n\n\nDo ghadi wo jo paas aa baithe (2)\nFor Two Moments, when They sat beside me\n\n\nHam zamane se dur ja baithe(2)\nWe were far away from everybody\n\n\n—–\n—-\n\n\nBhul ki unka hamnashi ho ke (2)\n’Twas a mistake to share a drink\n\n\nRoyege dil ko umar bhar kho ke (2)\nI will cry all my life for my Lost Heart\n\n\nHaay kya chiz thi luta baithe\nAlas, what a Thing it was, That I have Lost…\n\n\nDo Ghadi…\nTwo Moments…\n\n\n—-\n—-\n\n\nDil ko ek din zarur jana tha (2)\nThe Heart had to leave One Day\n\n\nVahi pahucha jaha thikana tha (2)\nThere It Reached, where it was Right\n\n\nDil vahi dil jo dil me ja baithe\nThat is a Heart, that Resides in a Heart\n\n\nDo Ghadi…\nTwo Moments…\n\n\n—-\n—–\n\n\nEk dil hi tha gham gusaar (2)\nThe Heart was my One Solace\n\n\nmeharbaan khaas raazdaar apna (2)\nMy Patron, my Confidant…\n\n\nghair ka kyun use banaa baithe\nWhy Did I Give it Away to a Stranger !!\n\n\nDo Ghadi…\nTwo Moments…\n\n\n—-\n—-\n\n\nGhair bhi to koi haseen hogaa (2)\nThe Stranger must also have been Lovely\n\n\nDil yoon hi de diya nahin hoga (2)\nYou would not have parted with your Heart Just Like That\n\n\nDekhkar kuchh to chot khaa baithe\nYou Saw Them, and were Wounded\n\n\nDo ghadi wo jo paas aa baithe\nTwo Moments…"
  },
  {
    "objectID": "content/courses/ISTW/Modules/30-Japan-HisayeYamamoto/index.html#writing-prompts",
    "href": "content/courses/ISTW/Modules/30-Japan-HisayeYamamoto/index.html#writing-prompts",
    "title": "Japan - Hisaye Yamamoto",
    "section": "Writing Prompts",
    "text": "Writing Prompts\n\nYour Mama’s “Arranged Marriage”\nA Rant in GenZ language about almost anything (please create a Glossary in an Appendix!)\nGetting “(ab)used” to Dad’s / Mom’s taste in Music\nThe communication between parents and child in “Seventeen Syllables” and in Grace Paley’s “The Loudest Voice”."
  },
  {
    "objectID": "content/courses/ISTW/Modules/50-Russia-MaximGorky/index.html",
    "href": "content/courses/ISTW/Modules/50-Russia-MaximGorky/index.html",
    "title": "Russia-Maxim Gorky",
    "section": "",
    "text": "Maxim Gorky (born March 16, 1868, Nizhny Novgorod, Russia—died June 14, 1936) Russian short-story writer and novelist who first attracted attention with his naturalistic and sympathetic stories of tramps and social outcasts and later wrote other stories, novels, and plays, including his famous The Lower Depths.\n\n\n(He) was a Russian author considered the father of Soviet revolutionary literature and founder of the doctrine of socialist realism. After having a difficult childhood, he roamed across the Russian empire, frequently changing jobs for about fifteen years before he became a successful writer. The experiences he had during those fifteen years deeply influenced his writing. Initially, he wrote stories mainly based on the lives of tramps and social outcasts, and he became known for his naturalistic style of writing. One of his greatest works is ‘The Mother,’ which Lenin praised as “a very timely book.”\n\n\nGorky was deeply associated with fellow Russian writers, Anton Chekhov and Leo Tolstoy and later wrote memoirs on them. Gorky was not only a great writer but also an influential figure in the political thinking. He was active with the emerging Marxist social-democrat movement. Initially a Bolshevik supporter, he became a critic when Vladimir Lenin seized power. However, later Gorky served as a Soviet advocate and headed the Union of Soviet Writers. His life was marked with a number of politically forced and sometimes self-imposed exiles.\n\n\nHe was nominated five times for the Nobel Prize in Literature."
  },
  {
    "objectID": "content/courses/ISTW/Modules/50-Russia-MaximGorky/index.html#maxim-gorky",
    "href": "content/courses/ISTW/Modules/50-Russia-MaximGorky/index.html#maxim-gorky",
    "title": "Russia-Maxim Gorky",
    "section": "",
    "text": "Maxim Gorky (born March 16, 1868, Nizhny Novgorod, Russia—died June 14, 1936) Russian short-story writer and novelist who first attracted attention with his naturalistic and sympathetic stories of tramps and social outcasts and later wrote other stories, novels, and plays, including his famous The Lower Depths.\n\n\n(He) was a Russian author considered the father of Soviet revolutionary literature and founder of the doctrine of socialist realism. After having a difficult childhood, he roamed across the Russian empire, frequently changing jobs for about fifteen years before he became a successful writer. The experiences he had during those fifteen years deeply influenced his writing. Initially, he wrote stories mainly based on the lives of tramps and social outcasts, and he became known for his naturalistic style of writing. One of his greatest works is ‘The Mother,’ which Lenin praised as “a very timely book.”\n\n\nGorky was deeply associated with fellow Russian writers, Anton Chekhov and Leo Tolstoy and later wrote memoirs on them. Gorky was not only a great writer but also an influential figure in the political thinking. He was active with the emerging Marxist social-democrat movement. Initially a Bolshevik supporter, he became a critic when Vladimir Lenin seized power. However, later Gorky served as a Soviet advocate and headed the Union of Soviet Writers. His life was marked with a number of politically forced and sometimes self-imposed exiles.\n\n\nHe was nominated five times for the Nobel Prize in Literature."
  },
  {
    "objectID": "content/courses/ISTW/Modules/50-Russia-MaximGorky/index.html#story",
    "href": "content/courses/ISTW/Modules/50-Russia-MaximGorky/index.html#story",
    "title": "Russia-Maxim Gorky",
    "section": "Story",
    "text": "Story\nWe will read Gorky’s story The Clown."
  },
  {
    "objectID": "content/courses/ISTW/Modules/50-Russia-MaximGorky/index.html#themes",
    "href": "content/courses/ISTW/Modules/50-Russia-MaximGorky/index.html#themes",
    "title": "Russia-Maxim Gorky",
    "section": "Themes",
    "text": "Themes\n\nBeing Pretentious and Being Genuine\nHumour as way to examine Life\nWhat does a Court Jester/Stand-Up Comedian do for a living?\nA Joker in a pack of cards?\nStreet Events, The Bystander Effect, and Jane Jacobs’ idea “Eyes on the Street”\nSignalling and How to Speak without Talking\nRisk Taking for the Benefit of Society\nHating Oneself for Cowardice?\nEnvy / Jealousy and Anger"
  },
  {
    "objectID": "content/courses/ISTW/Modules/50-Russia-MaximGorky/index.html#additional-material",
    "href": "content/courses/ISTW/Modules/50-Russia-MaximGorky/index.html#additional-material",
    "title": "Russia-Maxim Gorky",
    "section": "Additional Material",
    "text": "Additional Material\nNotes and References\n\nHingley, Ronald Francis. “Maxim Gorky”. Encyclopedia Britannica, 30 Nov. 2023, https://www.britannica.com/biography/Maxim-Gorky. Accessed 6 January 2024.\nGuzeva, Alexandra. Russia Beyond, March 28, 2018. 5 reasons why Soviet writer Maxim Gorky is so great. https://www.rbth.com/arts/327885-why-soviet-writer-gorky-great. Accessed 6 January 2024.\nJane Jacobs. 1961. “The Death and Life of Great American Cities”. PDF.\n\n\nThe Death and Life of Great American Cities is a 1961 book by writer and activist Jane Jacobs. The book is a critique of 1950s urban planning policy, which it holds responsible for the decline of many city neighborhoods in the United States. The book is Jacobs’ best-known and most influential work.\n\n\nRobert Kanigel’s biography of Jacobs is called Eyes on the Street, a phrase that Jacobs herself coined about the crucial importance of a vibrant street life to neighborhood safety and community.\n\n\n\n\nCarlheim-Gyllensköld, Monika. March 11, 2016. An open letter to Roger Mogert https://larsgustafssonblog.blogspot.com/2016/03/gastartikelmonika-gyllenskolddet.html\nOn Not Speaking Out and the Bystander Effect\n\n\nFirst they came for the Communists\nAnd I did not speak out\nBecause I was not a Communist\nThen they came for the Socialists\nAnd I did not speak out\nBecause I was not a Socialist\nThen they came for the trade unionists\nAnd I did not speak out\nBecause I was not a trade unionist\nThen they came for the Jews\nAnd I did not speak out\nBecause I was not a Jew\nThen they came for me\nAnd there was no one left\nTo speak out for me\n\n-PASTOR MARTIN NIEMÖLLER\n\n\n\nIs Signalling Costly? Penn DJ, Számadó S. The Handicap Principle: how an erroneous hypothesis became a scientific principle - PDF. Biol Rev Camb Philos Soc. 2020;95(1):267-290. doi:10.1111/brv.12563\n\nSong(s) for the Story\n\nPerhaps, after listening to this Hindi song, we will understand why Raj Kapoor was so popular in Soviet Russia !!\n\nhttps://youtu.be/OXPmxcFgXvY\n Your browser does not support the audio tag;  for browser support, please see:  https://www.w3schools.com/tags/tag_audio.asp \nSinger: Mukesh\nMusic Director: Shankar-Jaikishan\nFilm: Mera Naam Joker\nStarring: Raj Kapoor, Simi Garewal, Manoj Kumar, Rishi Kapoor, Dharmendra, Dara Singh, Padmini, Rajendra Kumar.\nYear: 1970\n\n\n\n\n\n\nHindi Lyrics\nEnglish Translation\n\n\n\nKehta hai joker saara zamaana\nSays the Joker, that the whole World\n\n\nAadhi haqikat aadha fasana\n(is) Half Truth, Half Story\n\n\nChashma utaaro phir dekho yaaro\nRemove your specs, and then see, my friends\n\n\nduniya nayee hai, chehra purana\nThe World is New, the Face is Old\n\n\nkehta hai joker…\nSays the Joker…\n\n\n—–\n—–\n\n\n( Apne pe hans kar jag ko hansaya\nHe laughs at Himself to make the World laugh\n\n\nBan ke tamasha mele main aaya ) (x2)\nBecoming a Show, he Comes to this gathering\n\n\nmele main aaya\nComes to this Gathering\n\n\n(hindu na muslim, poorab na pashchim) (x2)\nNot Hindu not Muslim, nor East nor West\n\n\nMazhub hai apna, hansna hansaana\nMy Religion is to Laugh and make others Laugh\n\n\nkehta hai joker …\nSays the Joker…\n\n\n——\n—–\n\n\n(Dhakke pe dhakka, rele pe rela\nPushing and Shoving all around\n\n\nHai bheed itni, par dil akela ) (x2)\nSuch a Crowd, but the Heart is Lonely\n\n\npar dil akela\nbut the Heart is Lonely\n\n\n(gam jab sataye, seeti bajaana ) (x2)\nWhen Sadness troubles you, Whistle!!\n\n\npar maskhare se dil na lagana\nBut never Become Attached to Joker / Humour!\n\n\nkehta hai joker …\nSays the Joker…\n\n\nchasma utaro …\nRemove your specs,….\n\n\nkehta hai joker …\nSays the Joker…\n\n\n\n\nReference to the “circus life, we all need a clown” in Steve Perry/Journey’s classic rock song, Faithfully.\nReference to “….When the jester sang for the king and queen, In a coat he borrowed from James Dean….Oh, and while the king was looking down, The jester stole his thorny crown, The courtroom was adjourned” in Don McClean’s rock anthem, American Pie."
  },
  {
    "objectID": "content/courses/ISTW/Modules/50-Russia-MaximGorky/index.html#writing-prompts",
    "href": "content/courses/ISTW/Modules/50-Russia-MaximGorky/index.html#writing-prompts",
    "title": "Russia-Maxim Gorky",
    "section": "Writing Prompts",
    "text": "Writing Prompts\n\nOn being funny at the “wrong time”\nOn being a bystander in a street crime / Actions Speak Louder than Words?\nOn the Design of an ad/campaign for the Police to prevent neighbourhood crime\nOn a cringe-worthy act on your part (autobiographical first person writing)\nA discussion of jobs and professions that seem to take risks for the benefit of society\nComparing this story with Peter Carey’s The Last Days of the Famous Mime."
  },
  {
    "objectID": "content/courses/ISTW/Modules/150-Ireland-WilliamTrevor/index.html",
    "href": "content/courses/ISTW/Modules/150-Ireland-WilliamTrevor/index.html",
    "title": "Ireland - William Trevor",
    "section": "",
    "text": "William Trevor"
  },
  {
    "objectID": "content/courses/ISTW/Modules/150-Ireland-WilliamTrevor/index.html#ireland-william-trevor",
    "href": "content/courses/ISTW/Modules/150-Ireland-WilliamTrevor/index.html#ireland-william-trevor",
    "title": "Ireland - William Trevor",
    "section": "",
    "text": "William Trevor"
  },
  {
    "objectID": "content/courses/ISTW/Modules/150-Ireland-WilliamTrevor/index.html#story",
    "href": "content/courses/ISTW/Modules/150-Ireland-WilliamTrevor/index.html#story",
    "title": "Ireland - William Trevor",
    "section": "Story",
    "text": "Story\nWe will read a sombre but enlightening story by this great Writer.\nA Choice of Butchers"
  },
  {
    "objectID": "content/courses/ISTW/Modules/150-Ireland-WilliamTrevor/index.html#additional-material",
    "href": "content/courses/ISTW/Modules/150-Ireland-WilliamTrevor/index.html#additional-material",
    "title": "Ireland - William Trevor",
    "section": "Additional Material",
    "text": "Additional Material\nQuoted from the LitHub Website\n\nOne of his stories that has always spoken to me is “A Choice of Butchers.” Reading it again, I prepared myself in advance for the emotional impact it would have on me, only to experience its tragic force more acutely. He handles the intersection of childhood with the adult world so beautifully. To me this is a story both about the loss of innocence and, at the same time, the ongoing state of innocence that can betray us when we are young. I cherish the dreary details of the house, the oatmeal wallpaper, the way he describes domestic spaces and habits, and people’s physical traits. And that uncommon, perfect word at the end to describe the father: “rumbustiousness.” The story is layered and ambiguous, elegiac, brilliantly understated, impossible not to read in one sitting. It articulates the terrifying resentment, disappointment, and anger we can feel towards our parents, and the confusion and distress evoked by those very feelings.\n\n–Jhumpa Lahiri"
  },
  {
    "objectID": "content/courses/ISTW/Modules/150-Ireland-WilliamTrevor/index.html#themes",
    "href": "content/courses/ISTW/Modules/150-Ireland-WilliamTrevor/index.html#themes",
    "title": "Ireland - William Trevor",
    "section": "Themes",
    "text": "Themes\n\nFamilies\nAdults and Children\nParents are human too\nPetty Jealousy\nPersonal Folly\n“From the Mouths of Babes”"
  },
  {
    "objectID": "content/courses/ISTW/Modules/150-Ireland-WilliamTrevor/index.html#additional-material-1",
    "href": "content/courses/ISTW/Modules/150-Ireland-WilliamTrevor/index.html#additional-material-1",
    "title": "Ireland - William Trevor",
    "section": "Additional Material",
    "text": "Additional Material\nNotes and References\n\n7 Writers share their favourite William Trevor story\nAnother good “Dad” story is this one by Grace Paley: A Conversation with My Father\nSong for the Story\nBilly Currington’s Country hit from 2003, Walk a Little Straighter.\n\n“Walk a Little Straighter” was written by Billy Currington, making it a very personal composition. This song is a highly autobiographical track, inspired by Currington’s own childhood experiences. The song narrates the story of a young boy addressing his father’s alcohol addiction. “Walk a Little Straighter,” tells about the pain and hope a child feels towards his father who is struggling with alcoholism. Upon its release in 2003, “Walk a Little Straighter” became a significant success, reaching No. 8 on the US Billboard Hot Country Songs chart. It marked the beginning of Billy Currington’s successful career in country music.\n\n\n\n\nRead “Walk A Little Straighter” by Billy Currington on Genius"
  },
  {
    "objectID": "content/courses/ISTW/Modules/150-Ireland-WilliamTrevor/index.html#writing-prompts",
    "href": "content/courses/ISTW/Modules/150-Ireland-WilliamTrevor/index.html#writing-prompts",
    "title": "Ireland - William Trevor",
    "section": "Writing Prompts",
    "text": "Writing Prompts\n\nWhen Adults goof up\nWhat is “Good Parenting”?\n“ChhoTa mooh aur Badi Baat: Out of the Mouths of Babes”.\nCompare this story with Rudyard Kipling’s story, Tods’ Amendment @Kipling Society. See notes on the same web page."
  },
  {
    "objectID": "content/courses/ISTW/Modules/100-CzechRepublic-MilanKundera/index.html#story",
    "href": "content/courses/ISTW/Modules/100-CzechRepublic-MilanKundera/index.html#story",
    "title": "Czech Republic - Milan Kundera",
    "section": "Story",
    "text": "Story\nWe will read this terrifying story by this great Writer. The Hitchhiking Game"
  },
  {
    "objectID": "content/courses/ISTW/Modules/100-CzechRepublic-MilanKundera/index.html#themes",
    "href": "content/courses/ISTW/Modules/100-CzechRepublic-MilanKundera/index.html#themes",
    "title": "Czech Republic - Milan Kundera",
    "section": "Themes",
    "text": "Themes\n\nYoung Love\nGames and Teasing\nEgo and Pain\nRules in the Game\nConsent"
  },
  {
    "objectID": "content/courses/ISTW/Modules/100-CzechRepublic-MilanKundera/index.html#notes-and-references",
    "href": "content/courses/ISTW/Modules/100-CzechRepublic-MilanKundera/index.html#notes-and-references",
    "title": "Czech Republic - Milan Kundera",
    "section": "Notes and References",
    "text": "Notes and References\nAdditional Material\nSong for the Story\nSong: Chalo Ek Baar Phir Se\nAlbum: Gumrah\nYear: 1963\nArtist: Mahendra Kapoor\nDirector: B.R.Chopra\nStar Cast: Sunil Dutt, Mala Sinha, Ashok Kumar, Shashikala, Nirupa Roy, Nana Plasekar\nMusic Director: Ravi\nLyricist: Sahir Ludhianvi\n\n\n\nChalo ek baar phirse\n\n\n\n\n\nHindi (and quite some Urdu!) Lyrics\nEnglish Translation\n\n\n\nchalo ek baar phir se, ajnabii ban jaayein ham donon(2)\nCome, let us become strangers once again\n\n\n—–\n—–\n\n\nna main tumse koi ummiid rakhuun dil-navaazii kii\nI shall no longer maintain hopes of compassion from you\n\n\nna tum merii taraf dekho ghalat andaaz nazaron se\nNor shall you gaze at me with your deceptive glances.\n\n\nna mere dil ki dhaDkan laDkhaDaaye merii baaton mein\nMy heart shall no longer tremble when I speak,\n\n\n\nna zaahir ho tumhaari kashm-kash ka raaz nazaron se.\nNor shall your glances reveal the secret of your torment.\n\n\n—-\n—-\n\n\ntumhein bhii koii uljhan roktii hai pesh-qadmii se\nComplications prevent you from advancing further,\n\n\nmujhe bhii log kahte hain ki yeh jalve paraaye hain\nI too am told that I wear disguises.\n\n\nmere hamraah bhi rusvaayiaan hain mere maazii kii\nThe disgraces of my past are now my companions,\n\n\ntumhaare saath bhii guzrii huii raaton ke saaye hain\nwhile the shadows of bygone nights are with you too.\n\n\n—–\n—–\n\n\ntaarruf rog ho jaaye to usko bhuulnaa bahtar\nShould knowing one another become a disease, then it is best to forget it.\n\n\ntaalluq bojh ban jaaye to usko toDnaa achhaa\nShould a relationship become a burden, then it is best to end it.\n\n\nvoh afsaana jise anjaam tak laanaa na ho mumkin\nFor that tale which cannot culminate in a conclusion,\n\n\nuse ek khuubsuurat moD de kar chhoDna achhaa\nit is best to give it a beautiful turn and leave it be.\n\n\n—–\n—–\n\n\nchalo ek baar phir se, ajnabii ban jaayein ham donon\nCome, let us become strangers once again."
  },
  {
    "objectID": "content/courses/ISTW/Modules/100-CzechRepublic-MilanKundera/index.html#writing-prompts",
    "href": "content/courses/ISTW/Modules/100-CzechRepublic-MilanKundera/index.html#writing-prompts",
    "title": "Czech Republic - Milan Kundera",
    "section": "Writing Prompts",
    "text": "Writing Prompts\n\nBeing a conservatively brought up young person at a bar\nOn a deep desire to commit an act of violence and how you would resist it, or not\nOn finding hidden depths of personality on your best friend ( and how you found out)"
  },
  {
    "objectID": "content/courses/ISTW/Modules/270-Spain-MerceRodoreda/index.html",
    "href": "content/courses/ISTW/Modules/270-Spain-MerceRodoreda/index.html",
    "title": "Spain-Merce Rodoreda",
    "section": "",
    "text": "Mercé Rodoreda i Gurguí was born on 10 October 1908 in Barcelona. She is the most translated author from Catalan into any other language, and her novel La plaça del Diamant (In Diamond Square) is one of the most celebrated novels of the Spanish Civil War."
  },
  {
    "objectID": "content/courses/ISTW/Modules/270-Spain-MerceRodoreda/index.html#merce-rodoreda",
    "href": "content/courses/ISTW/Modules/270-Spain-MerceRodoreda/index.html#merce-rodoreda",
    "title": "Spain-Merce Rodoreda",
    "section": "",
    "text": "Mercé Rodoreda i Gurguí was born on 10 October 1908 in Barcelona. She is the most translated author from Catalan into any other language, and her novel La plaça del Diamant (In Diamond Square) is one of the most celebrated novels of the Spanish Civil War."
  },
  {
    "objectID": "content/courses/ISTW/Modules/270-Spain-MerceRodoreda/index.html#story",
    "href": "content/courses/ISTW/Modules/270-Spain-MerceRodoreda/index.html#story",
    "title": "Spain-Merce Rodoreda",
    "section": "Story",
    "text": "Story\nWe will read Rodoreda’s story Rain."
  },
  {
    "objectID": "content/courses/ISTW/Modules/270-Spain-MerceRodoreda/index.html#themes",
    "href": "content/courses/ISTW/Modules/270-Spain-MerceRodoreda/index.html#themes",
    "title": "Spain-Merce Rodoreda",
    "section": "Themes",
    "text": "Themes\n\nIndependent Single Women\nMaking Decisions\nIs Marriage crumbling as an Institution?\nLuxury Beliefs (after Rob Henderson)"
  },
  {
    "objectID": "content/courses/ISTW/Modules/270-Spain-MerceRodoreda/index.html#additional-material",
    "href": "content/courses/ISTW/Modules/270-Spain-MerceRodoreda/index.html#additional-material",
    "title": "Spain-Merce Rodoreda",
    "section": "Additional Material",
    "text": "Additional Material\nNotes and References\n\n\n#RivetingReviews: An Authentic, Brilliant, Catalan Life: A Profile Of Mercè Rodoreda by West Camel. 20 June, 2023. https://www.eurolitnetwork.com/rivetingreviews-an-authentic-brilliant-catalan-life-a-profile-of-merce-rodoreda-by-west-camel/\n\nHenderson, Ron. 12 June 2022. Luxury Beliefs are Status Symbols: The struggle for distinction. https://www.robkhenderson.com/p/status-symbols-and-the-struggle-for?utm_campaign=post&utm_medium=web Accessed 07 Jan 2024.\n\n\n\n\n\nSong for the Story\nArtistes: Dragon ( pop group from NZ)\nTitle: Rain\nDate: July 1983\nChart Position: US(88); Aus(2)\nLyrics: https://www.lyrics.com/lyric/1406915/Rain"
  },
  {
    "objectID": "content/courses/ISTW/Modules/270-Spain-MerceRodoreda/index.html#writing-prompts",
    "href": "content/courses/ISTW/Modules/270-Spain-MerceRodoreda/index.html#writing-prompts",
    "title": "Spain-Merce Rodoreda",
    "section": "Writing Prompts",
    "text": "Writing Prompts\n\nOn Ghosting\nOn Being Indecisive\n“Is Marriage Crumbling”, and other Luxury Beliefs."
  },
  {
    "objectID": "content/courses/ISTW/Modules/90-Russia-IvanBunin/index.html",
    "href": "content/courses/ISTW/Modules/90-Russia-IvanBunin/index.html",
    "title": "Russia-Ivan Bunin",
    "section": "",
    "text": "IVAN BUNIN, the first Russian writer to win the Nobel Prize (1931), was born in 1870 to an aristocratic family in Vorornezh. After attending the University of Moscow briefly, he brought out his first book, a volume of verse. For this and his realistic accounts of the decay of the Russian nobility, he was awarded the Pushkin Prize for Literature and elected to the Russian Academy. He fled to western Europe, following the Revolution, and lived mainly in Paris, sometimes nearly destitute, until his death at the age of eightythree. His study of the dying patriarchy among Russian peasants raises him into the front rank of European novelists, but his present reputation rests on his short stories, in such collections as The Gentleman from San Francisco and The Grammar of Love. In many of his stories he contrasts the transitoriness of human life with the endurance of beauty and nature. Somerset Maugham has called “Sunstroke” one of the world’s best stories.\n\n\nWe will read Ivan Bunin’s short story, Sunstroke\n\n\nFirst Love\nShipboard Romances ( cliche )\nTime and Memory, Senses\n“River of Life” situation\nOne Night Stands?\nAdultery: Bad for Life but good for Literature?\nMen Don’t Cry?\n\nThe Map of the Story\n\n\n\n\n\n\n\n\nIvan Bunin – Biographical. NobelPrize.org. Nobel Prize Outreach AB 2022. Sat. 5 Mar 2022. https://www.nobelprize.org/prizes/literature/1933/bunin/biographical/"
  },
  {
    "objectID": "content/courses/ISTW/Modules/90-Russia-IvanBunin/index.html#ivan-bunin",
    "href": "content/courses/ISTW/Modules/90-Russia-IvanBunin/index.html#ivan-bunin",
    "title": "Russia-Ivan Bunin",
    "section": "",
    "text": "IVAN BUNIN, the first Russian writer to win the Nobel Prize (1931), was born in 1870 to an aristocratic family in Vorornezh. After attending the University of Moscow briefly, he brought out his first book, a volume of verse. For this and his realistic accounts of the decay of the Russian nobility, he was awarded the Pushkin Prize for Literature and elected to the Russian Academy. He fled to western Europe, following the Revolution, and lived mainly in Paris, sometimes nearly destitute, until his death at the age of eightythree. His study of the dying patriarchy among Russian peasants raises him into the front rank of European novelists, but his present reputation rests on his short stories, in such collections as The Gentleman from San Francisco and The Grammar of Love. In many of his stories he contrasts the transitoriness of human life with the endurance of beauty and nature. Somerset Maugham has called “Sunstroke” one of the world’s best stories.\n\n\nWe will read Ivan Bunin’s short story, Sunstroke\n\n\nFirst Love\nShipboard Romances ( cliche )\nTime and Memory, Senses\n“River of Life” situation\nOne Night Stands?\nAdultery: Bad for Life but good for Literature?\nMen Don’t Cry?\n\nThe Map of the Story\n\n\n\n\n\n\n\n\nIvan Bunin – Biographical. NobelPrize.org. Nobel Prize Outreach AB 2022. Sat. 5 Mar 2022. https://www.nobelprize.org/prizes/literature/1933/bunin/biographical/"
  },
  {
    "objectID": "content/courses/ISTW/Modules/90-Russia-IvanBunin/index.html#notes-and-references",
    "href": "content/courses/ISTW/Modules/90-Russia-IvanBunin/index.html#notes-and-references",
    "title": "Russia-Ivan Bunin",
    "section": "Notes and References",
    "text": "Notes and References\n\nMahAkavi KAlidAsa, “raghuvaMsham” (Dynasty of Emperor Raghu, 8th chapter, 95 verses). https://sanskritdocuments.org/sites/giirvaani/giirvaani/rv/sargas/08_rv.htm\n\n\n\nThe Lament of Aja\n\n\nविललाप स बाष्पगद्गदम् सहजामप्यपहाय धीरताम्| अभितप्तमयोऽपि मार्दवम् भजते कैव कथा शरीरिषु॥ ८-४\n\n\nvilalāpa sa bāṣpagadgadam sahajāmapyapahāya dhīratām| abhitaptamayo’pi mārdavam bhajate kaiva kathā śarīriṣu || 8-43\n\n\n\nsaH sahajAm api dhIratAm apahAya = he, naturally though, firmness, on forgoing;\n\nbAShpa gadgadam vilalApa = with tears, stammer, bewailed;\n\nabhitaptam ayaH api mArdavam bhajate = when excessively heated, iron, even, softness, acquires;\n\nsharIriShu kaiva kathA = of those possessing bodies, what, can be said.\n\n\n\nHaving even given up his natural fortitude, Aja bewailed stammering on account of his being choked with tears. Even iron when excessively heated acquires softness; what then can be said in respect of those possessing bodies. [8-43]\n\n\nPorter, Richard N. “Bunin’s ‘A Sunstroke’ and Chekhov’s ‘The Lady with the Dog.’” South Atlantic Bulletin 42, no. 4 (1977): 51–56. https://doi.org/10.2307/3199025.\n\n\nChekhov’s “The Lady with the Dog”and Bunin’s “A Sunstroke” have much in common, are frequently mentioned in connection with each other, and lend themselves to comparison. By discerning what features of the stories are alike and unlike one can learn much about the overall similarities and differences of the authors.\nThe plots of both stories are familiar. “The Lady with the Dog” is about Dmitry Dmitrich Gurov, a banker from Moscow, not yet forty, married, and the father of three children, and Anna Sergeevna von Dideritz, who has married two years before and now lives in the provincial city of S. They meet in Yalta, where they are spending their vacations alone. Soon they have an affair. Despite qualms on Anna’s part, they are fairly happy, but Gurov is relieved when she goes. At home in Moscow, he is surprised find that he does not forget her quickly. Instead, he misses her more and more and decides to go to S. to see her. She is surprised but admits that she has thought of him often and arranges to visit him occasionally in Moscow. On her visits, they meet in her hotel room. Although they find some happiness, they realize that the most difficult part of their affair is just beginning.\n\n\n“A Sunstroke” is about a lieutenant and a young married woman, both of them anonymous, who meet on a Volga river boat. They are immediately drawn to each other and agree to get off at a small town, where they spend the night. When the woman leaves the next morning, the lieutenant does not mind her going; but later in the day he realizes that he misses her desperately. He cannot go after her because she has not told him her name. He tries unsuccessfully in various ways to overcome his sense of loss, and, when he takes the boat that evening, he feels that he has grown ten years older.\n\nSongs for the Story !!\nA torch ballad by Phil Collins!\n{{% youtube \"Cq7Je8uveKE\" %}}\nAnd an equally good lament by the lady:\n{{% youtube \"s24reWXydp8\" %}}"
  },
  {
    "objectID": "content/courses/ISTW/Modules/90-Russia-IvanBunin/index.html#writing-prompts",
    "href": "content/courses/ISTW/Modules/90-Russia-IvanBunin/index.html#writing-prompts",
    "title": "Russia-Ivan Bunin",
    "section": "Writing Prompts",
    "text": "Writing Prompts\n\nDon’t be a Crybaby\n“Tere Bina Zindagi se Koi” story in English\n…"
  },
  {
    "objectID": "content/courses/ISTW/Modules/70-Brazil-ClariceLispector/index.html",
    "href": "content/courses/ISTW/Modules/70-Brazil-ClariceLispector/index.html",
    "title": "Brazil-Clarice Lispector",
    "section": "",
    "text": "A Woman, a Jew,and one born in Ukraine, and one of Brazil’s greatest writers…..Clarice Lispector."
  },
  {
    "objectID": "content/courses/ISTW/Modules/70-Brazil-ClariceLispector/index.html#clarice-lispector",
    "href": "content/courses/ISTW/Modules/70-Brazil-ClariceLispector/index.html#clarice-lispector",
    "title": "Brazil-Clarice Lispector",
    "section": "",
    "text": "A Woman, a Jew,and one born in Ukraine, and one of Brazil’s greatest writers…..Clarice Lispector."
  },
  {
    "objectID": "content/courses/ISTW/Modules/70-Brazil-ClariceLispector/index.html#story",
    "href": "content/courses/ISTW/Modules/70-Brazil-ClariceLispector/index.html#story",
    "title": "Brazil-Clarice Lispector",
    "section": "Story",
    "text": "Story\nWe will read her story, “Beauty and the Beast, or, The Wound too Great”, p. 291 in the Oxford Anthology of the Brazilian Short Story, edited by K. David Jackson."
  },
  {
    "objectID": "content/courses/ISTW/Modules/70-Brazil-ClariceLispector/index.html#themes",
    "href": "content/courses/ISTW/Modules/70-Brazil-ClariceLispector/index.html#themes",
    "title": "Brazil-Clarice Lispector",
    "section": "Themes",
    "text": "Themes\n\nBeauty and Ugliness\nBody as Power\nEphiphany\nWhere, When, and from Whom do We Learn?\nReligion as “Lived Life” (Practical Spirituality)\nMetaphors\nMaterialism…"
  },
  {
    "objectID": "content/courses/ISTW/Modules/70-Brazil-ClariceLispector/index.html#additional-material",
    "href": "content/courses/ISTW/Modules/70-Brazil-ClariceLispector/index.html#additional-material",
    "title": "Brazil-Clarice Lispector",
    "section": "Additional Material",
    "text": "Additional Material\nNotes and References\n\nhttps://www.newyorker.com/books/page-turner/the-true-glamour-of-clarice-lispector\nSuryakant Tripathi “Nirala”. 1923. Bhikshuk. http://kavitakosh.org\n\nवह आता–\nदो टूक कलेजे को करता, पछताता पथ पर आता।\nपेट पीठ दोनों मिलकर हैं एक,\nचल रहा लकुटिया टेक,\nमुट्ठी भर दाने को —भूख मिटाने को\nमुँह फटी पुरानी झोली का फैलाता —\nदो टूक कलेजे के करता पछताता पथ पर आता।\nसाथ दो बच्चे भी हैं सदा हाथ फैलाए,\nबाएँ से वे मलते हुए पेट को चलते,\nऔर दाहिना दया दृष्टि-पाने की ओर बढ़ाए।\nभूख से सूख ओठ जब जाते\nदाता-भाग्य विधाता से क्या पाते?\nघूँट आँसुओं के पीकर रह जाते।\nचाट रहे जूठी पत्तल वे सभी सड़क पर खड़े हुए,\nऔर झपट लेने को उनसे कुत्ते भी हैं अड़े हुए !\nठहरो ! अहो मेरे हृदय में है अमृत, मैं सींच दूँगा\nअभिमन्यु जैसे हो सकोगे तुम\nतुम्हारे दुख मैं अपने हृदय में खींच लूँगा।\n-Suryakant Tripathi “Nirala”"
  },
  {
    "objectID": "content/courses/ISTW/Modules/70-Brazil-ClariceLispector/index.html#song-for-the-story",
    "href": "content/courses/ISTW/Modules/70-Brazil-ClariceLispector/index.html#song-for-the-story",
    "title": "Brazil-Clarice Lispector",
    "section": "Song for the Story",
    "text": "Song for the Story\nThe roles are the reversed in this song are they? But that is possibly what we saw in the story…who is the beggar?\n\n\n\nAnother not un-related song, is this one by England Dan Seals and John Ford Coley, especially if you think the lady had a boyfriend earlier."
  },
  {
    "objectID": "content/courses/ISTW/Modules/70-Brazil-ClariceLispector/index.html#writing-prompts",
    "href": "content/courses/ISTW/Modules/70-Brazil-ClariceLispector/index.html#writing-prompts",
    "title": "Brazil-Clarice Lispector",
    "section": "Writing Prompts",
    "text": "Writing Prompts\n\nOn Growing Older by 5 years in 5 minutes\nOn How Others seem to Define Us\nExplain something using Religous Metaphors (any religion, any scripture) !!"
  },
  {
    "objectID": "content/courses/ISTW/Modules/160-USA-RaymondCarver/index.html#story",
    "href": "content/courses/ISTW/Modules/160-USA-RaymondCarver/index.html#story",
    "title": "USA-Raymond Carver",
    "section": "Story",
    "text": "Story\nWe will read Carver’s luminous story, Cathedral. PDF"
  },
  {
    "objectID": "content/courses/ISTW/Modules/160-USA-RaymondCarver/index.html#themes",
    "href": "content/courses/ISTW/Modules/160-USA-RaymondCarver/index.html#themes",
    "title": "USA-Raymond Carver",
    "section": "Themes",
    "text": "Themes\n\nBlindness\nJealousy\nIrony / Miscomprehension\nCheerfulness of the Blind"
  },
  {
    "objectID": "content/courses/ISTW/Modules/160-USA-RaymondCarver/index.html#additional-material",
    "href": "content/courses/ISTW/Modules/160-USA-RaymondCarver/index.html#additional-material",
    "title": "USA-Raymond Carver",
    "section": "Additional Material",
    "text": "Additional Material\nNotes and References\n\nRaymond Carver on How to Write\nShort Film on this story: \n\n\n   \n\n\nH.G. Wells’ story, The Country of the Blind\n\n\n\n“The Country of the Blind” is a short story written by H. G. Wells. It was first published in the April 1904 issue of The Strand Magazine and included in a 1911 collection of Wells’s short stories, The Country of the Blind and Other Stories. It is one of Wells’s best known short stories, and features prominently in literature dealing with blindness.-Wikipedia\n\n 4. Sparsh, starring Naseeruddin Shah and Shabana Azmi.\n\n\nhttps://www.youtube.com/watch?v=8yB4IwsnyQ8\n\nSparsh (English: Touch) is a 1980 Indian Hindi feature film directed by Sai Paranjpye. It stars Naseeruddin Shah and Shabana Azmi playing the characters of a visually impaired principal and a sighted teacher in a school for the blind, where they fall in love though soon their complexes tag along and they struggle to get past them to reconnect with the “touch” of love.\nThe film remains most memorable for the subtle acting of its leads, plus the handling of the issue of relationships with the visually handicapped, revealing the emotional and perception divide between the worlds of the “blind” and the “sighted”, epitomized by the characters. The film won the National Film Award for Best Feature Film in Hindi. However, the film’s release was delayed by almost 4 years.\n\nShorter Summary:\nhttps://www.dailymotion.com/video/x3nv3l5\n\n\nAnd Then there was Light is the autobiography of Jacques Lusseyran, the blind hero of the French Resistance during WWII.\n\n\nhttps://angelusnews.com/news/us-world/jacques-lusseyran-blind-hero-of-the-french-resistance/\n\n“And There Was Light” is the strange and beautiful autobiography of Jacques Lusseyran, “blind hero of the French Resistance.”\nBorn in Paris in 1924, Lusseyran lost his sight at the age of 8 in a schoolroom incident. Even at that age, he was groping toward the transcendent.\nTrying to navigate his way around a world he could no longer see, he came to learn that inanimate things are alive, and of the sympathetic current that runs between the branches of a tree in springtime, and that if you press the little stone you’ve secreted in your pocket, it will press back.\nHe wrote, “The seeing commit a strange error. They believe that we know the world only through our eyes. For my part, I discovered that the universe consists of pressure, that every object and every living being reveals itself to us at first by a kind of quiet yet unmistakable pressure that indicates its intention and its form. I even experienced the following wonderful fact: A voice, the voice of a person, permits him to appear in a picture. When the voice of a man reaches me, I immediately perceive his figure, his rhythm, and most of his intentions.\n\nSong(s) for the Story !!\n\nMahendra Kapoor from the movie Sambandh(1969):\n\n\n\n\n\nDeep Purple, When a Blind Man Cries"
  },
  {
    "objectID": "content/courses/ISTW/Modules/160-USA-RaymondCarver/index.html#writing-prompts",
    "href": "content/courses/ISTW/Modules/160-USA-RaymondCarver/index.html#writing-prompts",
    "title": "USA-Raymond Carver",
    "section": "Writing Prompts",
    "text": "Writing Prompts"
  },
  {
    "objectID": "content/courses/ISTW/Modules/170-Italy-PrimoLevi/index.html",
    "href": "content/courses/ISTW/Modules/170-Italy-PrimoLevi/index.html",
    "title": "Italy-Primo Levi",
    "section": "",
    "text": "Levi, a 23-year old chemist, was arrested in December 1943 and transported to Auschwitz in February 1944. There he remained until the camp was liberated on 27 January 1945. He arrived back home in Turin in October, unrecognisable to the concierge who had seen him only a couple of years earlier.\n\n\nHydrogen from Primo Levi’s collection The Periodic Table https://archive.org/download/ThePeriodicTable-PrimoLevi/periodic-primo.pdf\n\n\nEpiphany\n\nCuriosity, the Desire to Know, and Perseverance\n\nRule Breaking\n\n“Other Worldliness”\n\nMetaphors: Describing an idea using vocabulary from another domain\nTRIZ, aka Teoriya Resheniya Izobretatelskhikh Zadatch"
  },
  {
    "objectID": "content/courses/ISTW/Modules/170-Italy-PrimoLevi/index.html#italy-primo-levi",
    "href": "content/courses/ISTW/Modules/170-Italy-PrimoLevi/index.html#italy-primo-levi",
    "title": "Italy-Primo Levi",
    "section": "",
    "text": "Levi, a 23-year old chemist, was arrested in December 1943 and transported to Auschwitz in February 1944. There he remained until the camp was liberated on 27 January 1945. He arrived back home in Turin in October, unrecognisable to the concierge who had seen him only a couple of years earlier.\n\n\nHydrogen from Primo Levi’s collection The Periodic Table https://archive.org/download/ThePeriodicTable-PrimoLevi/periodic-primo.pdf\n\n\nEpiphany\n\nCuriosity, the Desire to Know, and Perseverance\n\nRule Breaking\n\n“Other Worldliness”\n\nMetaphors: Describing an idea using vocabulary from another domain\nTRIZ, aka Teoriya Resheniya Izobretatelskhikh Zadatch"
  },
  {
    "objectID": "content/courses/ISTW/Modules/170-Italy-PrimoLevi/index.html#additional-material",
    "href": "content/courses/ISTW/Modules/170-Italy-PrimoLevi/index.html#additional-material",
    "title": "Italy-Primo Levi",
    "section": "Additional Material",
    "text": "Additional Material\nNotes and References\n\nThe last story in The Periodic Table, titled Carbon is also a fantastic metaphoric journey of a single Carbon atom. Read it!! It has been described as the most accessible piece of science writing!\n\nMore Holocaust Reading:\n\nTadeuscz Borowski’s “Postal Indiscretions” is a series of letters written from a concentration camp.\n\nHis short story, This Way for the Gas, Ladies and Gentlemen is also a harrowing read. Weblink to PDF\n\nThe World of Tadeuscz Borowski’s Auschwitz https://www.nybooks.com/daily/2021/09/12/the-world-of-tadeusz-borowskis-auschwitz/\n\n\n\nhttps://www.theguardian.com/books/2017/apr/22/primo-levi-auschwitz-if-this-is-a-man-memoir-70-years\nA Student Reflection on Primo Levi’s Hydrogen: https://ocw.mit.edu/courses/literature/21l-325-small-wonders-staying-alive-spring-2007/assignments/periodic2.pdf\nExtract from “The key to the highest truths”: Primo Levi and the beauty of chemistry\n\n\nNonetheless, reading Levi’s writing over lockdown, I was reminded that he also witnessed chemistry’s most detestable side at Auschwitz, as part of the Chemical Kommando transporting magnesium chloride, and at the IG-Farben laboratory. Despite this, Levi never lost sight of the beauty of chemistry: for me, found in the sublimation of brilliant emerald-green crystals of nickelocene; in the jagged, imperfect trace of an action potential on the electromyograph; in the faint rainbow of lines emitted by potassium under a sodium discharge lamp. If Levi were to observe us in these practical classes, complete with our rash deductions, amateurish mistakes and shattered glassware, I like to think he would be pleased.\n\nSong for the Story !!\nSong: Flying Sorcery Artiste: Al Stewart\nAlastair(“Al”) Ian Stewart (born 5 September 1945) is a Scottish singer-songwriter and folk-rock musician who rose to prominence as part of the British folk revival in the 1960s and 1970s. He developed a unique style of combining folk-rock songs with delicately woven tales of characters and events from history.\n\n\n\nRead “Flying Sorcery” by Al Stewart on Genius"
  },
  {
    "objectID": "content/courses/ISTW/Modules/170-Italy-PrimoLevi/index.html#writing-prompts",
    "href": "content/courses/ISTW/Modules/170-Italy-PrimoLevi/index.html#writing-prompts",
    "title": "Italy-Primo Levi",
    "section": "Writing Prompts",
    "text": "Writing Prompts\n\nMy Genius Friend\nWhat I learnt by Breaking Rules\nDescribing a Scientific Concept using everyday objects"
  },
  {
    "objectID": "content/courses/ISTW/Modules/180-India-RuthPrawerJhabvala/index.html#india-ruth-prawer-jhabvala",
    "href": "content/courses/ISTW/Modules/180-India-RuthPrawerJhabvala/index.html#india-ruth-prawer-jhabvala",
    "title": "India - Ruth Prawer Jhabvala",
    "section": "India : Ruth Prawer Jhabvala",
    "text": "India : Ruth Prawer Jhabvala\nFrom: https://www.britannica.com/biography/Ruth-Prawer-Jhabvala\n\nRuth Prawer Jhabvala, original name Ruth Prawer, (born May 7, 1927, Cologne, Germany—died April 3, 2013, New York, New York, U.S.), novelist and screenwriter, well known for her witty and insightful portrayals of contemporary Indian lives and, especially, for her 46 years as a pivotal member of Ismail Merchant and James Ivory’s filmmaking team.\n\n\nJhabvala’s family was Jewish, and in 1939 they emigrated from Germany to England; she was made a naturalized British citizen in 1948. After receiving an M.A. in English (1951) from Queen Mary College, London, she married an Indian architect and moved to India, where she lived for the next 24 years. After 1975 she lived in New York City, becoming a U.S. citizen in 1986.\n\nStory\nWe will read this beautiful, beautifully written story, with a completely cringe-worthy character in it.\nThe Interview\nThemes\n\nJoint Family\n“Victim Mentality”\nIncest\nNotes and References\n\nThe Difficult Genius of Ruth Prawer Jhabvala\nAdditional Material\n\nThe Great Indian Family: New Roles, Old Responsibilities, https://www.amazon.in/Gitanjali-Prasad/e/B001HPLWHW/ref=dp_byline_cont_pop_book_1\nMukul Kesavan, The ugly Indian man: Of hygiene, hair and horrible habits, https://www.telegraphindia.com/opinion/the-ugly-indian-man-of-hygiene-hair-and-horrible-habits/cid/1026680#\nSong for the Story\nSong: Yeh Jeevan Hai Is Jeevan Ka Yahi Hai\nMovie: Piya Ka Ghar\nYear: 1972\nSinger: Kishore Kumar Music: Laxmikant Pyarelal\nLyrics: Anand Bakshi\nCast: Anil Dhawan, Jaya Bhaduri\nDirector: Basu Chatterjee\n\n\nWriting Prompts\n\nCritical Reflection on the Story!\n\nSarcastic Piece on the Indian Male\nNav-Vadhu describing the first week after an arranged marriage\n\nAn analysis of a regional movie that has the Joint Family as a Theme\nThe Joys of Living in a Joint Family"
  },
  {
    "objectID": "content/courses/ISTW/Modules/80-England-VSPritchett/index.html",
    "href": "content/courses/ISTW/Modules/80-England-VSPritchett/index.html",
    "title": "England - V S Pritchett",
    "section": "",
    "text": "V S Pritchett\n\n\nV.S. Pritchett (1900–1997)was the best short story writer in English during the twentieth century. There should be no argument about this. (emphasis mine)-David Miller, That Glimpse of Truth: The 100 Finest Short Stories Ever Written, Head of Zeus, 2014. ISBN:9781784080037\n\n Extracted from https://www.theguardian.com/books/booksblog/2008/feb/22/vspritchett\n\nVictor Sawdon Pritchett, or VSP, as he preferred to be known (he loathed his Christian name), exemplifies the gap that can yawn between reputation and readership. Hugely productive throughout his 97-year life as a short story writer, essayist, biographer, autobiographer and novelist, he is little read just 11 years after his death. In his short fiction Pritchett is one of the English writers who most clearly exhibits the mark of Chekhov’s influence. The significance the Russian placed on the commonplace thing and apparently incidental aside is there, as is the deceptively simple expression of complex emotional processes. But the chief Chekhovian element which Pritchett makes his own is the way he subsumes himself within the story. To borrow from drama, Pritchett should be seen, not as a director with a signature style, but as an actor with the ability to lose himself entirely in whatever role he is playing. His stories situate the reader in direct relation to their characters, with little or no authorial filter between them.\n\n\nWe will read his story A Family Man.\n\n\nLies and Lying\nAffairs\n“Collateral” damage to other people\n“Satyam bruyat, Priyam Bruyaat..\n\nसत्यं ब्रूयात् प्रियं ब्रूयात् न ब्रूयात् सत्यमप्रियम्\nप्रियं च नानृतं ब्रूयात् एष धर्मः सनातनः ॥"
  },
  {
    "objectID": "content/courses/ISTW/Modules/80-England-VSPritchett/index.html#v.-s.-pritchett",
    "href": "content/courses/ISTW/Modules/80-England-VSPritchett/index.html#v.-s.-pritchett",
    "title": "England - V S Pritchett",
    "section": "",
    "text": "V S Pritchett\n\n\nV.S. Pritchett (1900–1997)was the best short story writer in English during the twentieth century. There should be no argument about this. (emphasis mine)-David Miller, That Glimpse of Truth: The 100 Finest Short Stories Ever Written, Head of Zeus, 2014. ISBN:9781784080037\n\n Extracted from https://www.theguardian.com/books/booksblog/2008/feb/22/vspritchett\n\nVictor Sawdon Pritchett, or VSP, as he preferred to be known (he loathed his Christian name), exemplifies the gap that can yawn between reputation and readership. Hugely productive throughout his 97-year life as a short story writer, essayist, biographer, autobiographer and novelist, he is little read just 11 years after his death. In his short fiction Pritchett is one of the English writers who most clearly exhibits the mark of Chekhov’s influence. The significance the Russian placed on the commonplace thing and apparently incidental aside is there, as is the deceptively simple expression of complex emotional processes. But the chief Chekhovian element which Pritchett makes his own is the way he subsumes himself within the story. To borrow from drama, Pritchett should be seen, not as a director with a signature style, but as an actor with the ability to lose himself entirely in whatever role he is playing. His stories situate the reader in direct relation to their characters, with little or no authorial filter between them.\n\n\nWe will read his story A Family Man.\n\n\nLies and Lying\nAffairs\n“Collateral” damage to other people\n“Satyam bruyat, Priyam Bruyaat..\n\nसत्यं ब्रूयात् प्रियं ब्रूयात् न ब्रूयात् सत्यमप्रियम्\nप्रियं च नानृतं ब्रूयात् एष धर्मः सनातनः ॥"
  },
  {
    "objectID": "content/courses/ISTW/Modules/80-England-VSPritchett/index.html#additional-material",
    "href": "content/courses/ISTW/Modules/80-England-VSPritchett/index.html#additional-material",
    "title": "England - V S Pritchett",
    "section": "Additional Material",
    "text": "Additional Material\n\nAdultery makes for good fiction!\n\nFord Madox Ford, The Good Soldier\n\nJane Smiley, The Age of Grief\n\nKate Chopin, The Awakening\n\n\n\n\nNotes and References\n\nhttps://truthultimate.com/satyam-bruyat-priyam-bruyat/\nSongs for the Story\n\nWhitney Houston (1985) : Saving All my Love for You\n\n\n\n\nFleetwood Mac (1987) : Tell Me Lies"
  },
  {
    "objectID": "content/courses/ISTW/Modules/80-England-VSPritchett/index.html#writing-prompts",
    "href": "content/courses/ISTW/Modules/80-England-VSPritchett/index.html#writing-prompts",
    "title": "England - V S Pritchett",
    "section": "Writing Prompts",
    "text": "Writing Prompts\n\nLying for the sake of goodness"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/80-htmlwidgets/htmlwidgets.html#setting-up-r-packages",
    "href": "content/courses/R4Artists/Modules/80-htmlwidgets/htmlwidgets.html#setting-up-r-packages",
    "title": "Lab-8: Did you ever see such a thing as a drawing of a muchness?",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\n# Data manipulation and data sources\noptions(htmltools.preserve.raw = FALSE, echo = TRUE)\nlibrary(tidyverse)\nlibrary(tidygraph)\nlibrary(igraph)\nlibrary(palmerpenguins)\nlibrary(igraphdata)\n\n\n# To render htmlwidgets as iframe widgets\n# https://communicate-data-with-r.netlify.app/docs/communicate/htmlwidgets-in-documents/\n#library(widgetframe)\n\n# htmlwidget related libraries\nlibrary(htmlwidgets)\n\n\n# Widget Libraries\nlibrary(leaflet)\nlibrary(plotly)\nlibrary(DT)\nlibrary(echarts4r)\nlibrary(echarts4r.assets)\nlibrary(canvasXpress)\nlibrary(rgl)\nlibrary(networkD3)\nlibrary(threejs)\nlibrary(slickR)\nlibrary(crosstalk)\n# Linkable widgets in crosstalk - github repo only\n#devtools::install_github(\"kent37/summarywidget\")\nlibrary(summarywidget)",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-8: Did you ever see such a thing as a drawing of a muchness?"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/80-htmlwidgets/htmlwidgets.html#introduction",
    "href": "content/courses/R4Artists/Modules/80-htmlwidgets/htmlwidgets.html#introduction",
    "title": "Lab-8: Did you ever see such a thing as a drawing of a muchness?",
    "section": "Introduction",
    "text": "Introduction\nThere are very many great JavaScript libraries for creating eye-popping and even interactive charts! And these are now available in R, and can be invoked using R code! So we can “use JavaScript” in R, as it were, without knowing JavaScript! And create something like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnlike the Dormouse, no complaints!\nWe will explore a few them, as an alternative to ggplot !!\nThis may be too much of a good thing, or a much of muchness but then, we can always use more then one way of telling our stories!!",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-8: Did you ever see such a thing as a drawing of a muchness?"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/80-htmlwidgets/htmlwidgets.html#htmlwidgets-usage",
    "href": "content/courses/R4Artists/Modules/80-htmlwidgets/htmlwidgets.html#htmlwidgets-usage",
    "title": "Lab-8: Did you ever see such a thing as a drawing of a muchness?",
    "section": "\nhtmlwidgets usage",
    "text": "htmlwidgets usage\nhtmlwidgets creates, well, widgets, that can visualize data in many ways. HTML widgets work just like R plots except they produce interactive web visualizations. These can be used in RMarkdown, in flexdashboards, and in shiny apps.\nAll the possible widgets ( 127 of them on CRAN ) are listed here: https://gallery.htmlwidgets.org/\nSome packages that offer widgets for use in htmlwidgets:\n\n\nnetworkD3:\n\n\nForce directed networks with simpleNetwork and forceNetwork\n\nSankey diagrams with sankeyNetwork\n\nRadial networks with radialNetwork\n\nDendro networks with dendroNetwork\n\n\nUsing networkD3\n\n\nlibrary(networkD3)\ndata(\"karate\")\n\n# Make separate data frames for edges and nodes\n# networkD3 needs indices starting from 1\nkarate_edges &lt;- karate %&gt;% \n  as_tbl_graph() %&gt;% \n  tidygraph::activate(edges) %&gt;% \n  as.data.frame() %&gt;% \n  dplyr::mutate(source = from - 1, target = to - 1) %&gt;% \n  rename(value = weight) %&gt;% \n  select(source, target, value)\nkarate_edges\n\n\n  \n\n\nkarate_nodes &lt;- karate %&gt;% \n  as_tbl_graph() %&gt;% \n  tidygraph::activate(nodes) %&gt;% \n  as.data.frame() %&gt;% \n  dplyr::mutate(group = as.character(Faction)) %&gt;% \n  select(name, label, group, color)\nkarate_nodes\n\n\n  \n\n\nsimpleNetwork(karate_edges, \n              charge = -50, \n              fontSize = 12, zoom = FALSE, \n              fontFamily = \"serif\")\n\n\n\n\n\n\n\nforceNetwork(\n  Links = karate_edges,\n  Nodes = karate_nodes,\n  Value = \"value\",\n  # width of edges, dbl\n  NodeID = \"name\",\n  # chr\n  Group = \"group\",\n  # Node group, chr\n  # Nodesize = \"label\" # chr !!!\n  # linkColour = \"value\"\n)\n\n\n\n\n\n\nCreating a Sankey Diagram.\n\n# Code is not working need to fix\n# No nodes showing up...\nUCB_graph &lt;-\n  UCBAdmissions %&gt;% \n  as.data.frame() %&gt;% \n  # select(Gender, Admit, Dept, Freq) %&gt;% \n  as_tbl_graph()\nUCB_graph\n\nUCB_nodes &lt;- UCB_graph %&gt;% activate(nodes) %&gt;% as_tibble()\nUCB_nodes\n\nUCB_links &lt;- UCB_graph %&gt;% \n  activate(edges) %&gt;% \n  as_tibble() %&gt;%\n  dplyr::mutate(from = from - 1, to = to - 1)\nUCB_links\n\nsankeyNetwork(\n  Links = UCB_links,\n  Nodes = UCB_nodes,\n  Source = \"from\",\n  Target = \"to\",\n  Value = \"Freq\",\n  LinkGroup = \"Dept\",\n  fontSize = 20,\n  fontFamily = \"Arial\"\n)\n\nsimpleNetwork, forceNetwork and sankeyNetwork use a similar node and link data structure, organized as two data frames (not tibbles)\nchordNetwork uses an association matrix type of matrix or a data frame organized in the same way, where entry (n,m) represents the strength of the link from group n to group m. Matrix needs to be square !! “Column names” and “Row names” need to be the same if the data is a data.frame.\n\ndata &lt;- matrix(rpois(n = 16, lambda = 50), \n               nrow = 4, ncol = 4)\nchordNetwork(data,\n             labels = c(\"A\", \"B\", \"C\", \"D\"))",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-8: Did you ever see such a thing as a drawing of a muchness?"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/80-htmlwidgets/htmlwidgets.html#using-threejs-wip",
    "href": "content/courses/R4Artists/Modules/80-htmlwidgets/htmlwidgets.html#using-threejs-wip",
    "title": "Lab-8: Did you ever see such a thing as a drawing of a muchness?",
    "section": "Using threejs ( WIP )",
    "text": "Using threejs ( WIP )\n\nhttps://bwlewis.github.io/rthreejs/\nhttps://bwlewis.github.io/rthreejs/crosstalk.html\n\n\ngraphjs usage\n\ndata(\"LeMis\") # igraph object\nLeMis\n\nIGRAPH b7852d9 U--- 77 254 -- \n+ attr: label (v/c), color (v/c), size (v/n)\n+ edges from b7852d9:\n [1]  1-- 2  1-- 3  1-- 4  3-- 4  1-- 5  1-- 6  1-- 7  1-- 8  1-- 9  1--10\n[11] 11--12  4--12  3--12  1--12 12--13 12--14 12--15 12--16 17--18 17--19\n[21] 18--19 17--20 18--20 19--20 17--21 18--21 19--21 20--21 17--22 18--22\n[31] 19--22 20--22 21--22 17--23 18--23 19--23 20--23 21--23 22--23 17--24\n[41] 18--24 19--24 20--24 21--24 22--24 23--24 13--24 12--24 24--25 12--25\n[51] 25--26 24--26 12--26 25--27 12--27 17--27 26--27 12--28 24--28 26--28\n[61] 25--28 27--28 12--29 28--29 24--30 28--30 12--30 24--31 31--32 12--32\n[71] 24--32 28--32 12--33 12--34 28--34 12--35 30--35 12--36 35--36 30--36\n+ ... omitted several edges\n\n#V(LeMis)$label\n#V(LeMis)$color\ngraphjs(LeMis,\n    layout=list( # animates between a list of layouts\n      # layouts need to be 3D layouts\n      # Or each can be a 3 column matrix with n(rows) = n(vertices)\n    layout_randomly(LeMis, dim=3),\n    layout_on_sphere(LeMis),\n    layout_with_drl(LeMis, dim=3),  # note! somewhat slow...\n    layout_with_fr(LeMis, dim=3, niter=30)\n    ),\n  main=list(\"random layout\", \"sphere layout\", \"drl layout\", \"fr layout\"),\n  fpl=300)",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-8: Did you ever see such a thing as a drawing of a muchness?"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/80-htmlwidgets/htmlwidgets.html#globejs-usage",
    "href": "content/courses/R4Artists/Modules/80-htmlwidgets/htmlwidgets.html#globejs-usage",
    "title": "Lab-8: Did you ever see such a thing as a drawing of a muchness?",
    "section": "\nglobejs usage",
    "text": "globejs usage\nPlot points, arcs and images on a globe in 3D using three.js. The globe can be rotated and and zoomed. Great Circles and historical routes are a good idea for this perhaps.\nRefer to this page for more ideas http://bwlewis.github.io/rthreejs/globejs.html\n\n# Random Lats and Longs\nlat &lt;- rpois(10, 60) + rnorm(10, 80)\nlong &lt;- rpois(10, 60) + rnorm(10, 10)\n\n# Random \"Spike\" heights for each location. Population? Tourists? GDP?\nvalue &lt;- rpois(10, lambda = 80)\n \nglobejs(lat = lat, long = long)\n\n\n\n\n\nglobejs(\n  lat = lat,\n  long = long,\n  \n  # random heights of the Spikes (!!) at lat-long combo\n  value = value,\n  color = \"red\",\n  # Zoom factor, default is 35\n  fov = 50\n)\n\n\n\n\n\nglobejs(\n  lat = lat,\n  long = long,\n  value = value,\n  color = \"red\",\n  pointsize = 4, # width of the columns\n  # Zoom position\n  fov = 35,\n  # initial position of the globe\n  rotationlat = 0.6, #  in RADIANS !!! Good Heavens!!\n  rotationlong = 0.2 #  in RADIANS !!! Good Heavens!!\n)\n\n\n\n\n\nglobejs(\n  lat = lat,\n  long = long,\n  value = value,\n  color = \"red\",\n  pointsize = 4,\n  fov = 35,\n  rotationlat = 0.6,\n  rotationlong = 0.2,\n  lightcolor = \"#aaeeff\",\n  emissive = \"#0000ee\",\n  bodycolor = \"#ffffff\",\n  bg = \"grey\"\n)\n\n\n\n\n\n\n\n# Plotting arcs on the globe\n# Requires 4 column data frame for start and end lat and long\n\narcs &lt;- data.frame(start_lat = runif(10, min = -20, max = 20), \n                   start_lon = runif(10, min = -20, max = 20), \n                   end_lat = runif(10, min = -20, max = 20),\n                   end_lon = runif(10, min = -20, max = 20) + 60)\narcs\n\n\n  \n\n\nglobejs(arcs = arcs,\n        arcsColor = \"gold\",\n        arcsLwd = 4,\n        arcsHeight = 0.6,\n        arcsOpacity = 1,\n        rotationlat = 0,\n        rotationlong = -2.2,\n        bg = \"lightblue\",atmosphere = FALSE,\n        pointsize = 2)\n\n\n\n\n\n\nthreejs contains a dataset called flights obtained from Callum Prentice’s FlightStream page: http://callumprentice.github.io/apps/flight_stream/index.html\n\ndata(\"flights\",package = \"threejs\")\nflights %&gt;% head()\n\n\n  \n\n\nfrequent_destinations &lt;-\n  flights %&gt;% group_by(dest_lat, dest_long) %&gt;% \n  count(sort = TRUE) %&gt;% \n  ungroup() %&gt;% \n  slice_max(n = 10, order_by = n)\nfrequent_destinations\n\n\n  \n\n\nfrequent_flights &lt;- flights %&gt;% \n  semi_join(frequent_destinations,\n            by = c(\"dest_lat\" = \"dest_lat\", \"dest_long\" = \"dest_long\")) %&gt;% unique() \n\nfrequent_flights %&gt;% \n  kableExtra::kbl() %&gt;%\n  kableExtra::kable_paper(full_width = TRUE) %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"condensed\", \"responsive\")) %&gt;%\n  kableExtra::scroll_box(width = \"700px\", height = \"500px\")\n\n\n\n\n\n\n\norigin_lat\norigin_long\ndest_lat\ndest_long\n\n\n\n1\n40.657633\n17.947033\n47.464722\n8.549167\n\n\n2\n44.828335\n-0.715556\n47.464722\n8.549167\n\n\n3\n51.382669\n-2.719089\n47.464722\n8.549167\n\n\n4\n34.875117\n33.624850\n47.464722\n8.549167\n\n\n5\n27.931886\n-15.386586\n47.464722\n8.549167\n\n\n6\n25.557111\n34.583711\n47.464722\n8.549167\n\n\n7\n40.783200\n-91.125500\n41.978603\n-87.904842\n\n\n8\n39.834564\n-88.865689\n41.978603\n-87.904842\n\n\n9\n13.681108\n100.747283\n1.350189\n103.994433\n\n\n10\n-12.408333\n130.872660\n1.350189\n103.994433\n\n\n11\n19.934856\n110.458961\n1.350189\n103.994433\n\n\n12\n30.229503\n120.434453\n1.350189\n103.994433\n\n\n13\n22.308919\n113.914603\n1.350189\n103.994433\n\n\n14\n8.113200\n98.316872\n1.350189\n103.994433\n\n\n15\n34.434722\n135.244167\n25.077731\n121.232822\n\n\n16\n2.745578\n101.709917\n1.350189\n103.994433\n\n\n17\n14.508647\n121.019581\n1.350189\n103.994433\n\n\n18\n5.297139\n100.276864\n1.350189\n103.994433\n\n\n19\n-31.940278\n115.966944\n1.350189\n103.994433\n\n\n20\n11.546556\n104.844139\n1.350189\n103.994433\n\n\n21\n13.410666\n103.812840\n1.350189\n103.994433\n\n\n22\n16.907305\n96.133222\n1.350189\n103.994433\n\n\n23\n10.818797\n106.651856\n1.350189\n103.994433\n\n\n24\n1.350189\n103.994433\n25.077731\n121.232822\n\n\n25\n51.132767\n13.767161\n47.464722\n8.549167\n\n\n26\n42.760277\n10.239445\n47.464722\n8.549167\n\n\n27\n46.991067\n15.439628\n47.464722\n8.549167\n\n\n28\n47.793304\n13.004333\n47.464722\n8.549167\n\n\n29\n21.539400\n109.294000\n34.447119\n108.751592\n\n\n30\n29.719217\n106.641678\n40.080111\n116.584556\n\n\n31\n29.719217\n106.641678\n31.143378\n121.805214\n\n\n32\n29.719217\n106.641678\n34.447119\n108.751592\n\n\n33\n30.578528\n103.947086\n40.080111\n116.584556\n\n\n34\n30.578528\n103.947086\n31.143378\n121.805214\n\n\n35\n30.578528\n103.947086\n34.447119\n108.751592\n\n\n36\n25.935064\n119.663272\n34.447119\n108.751592\n\n\n37\n34.490900\n102.371900\n34.447119\n108.751592\n\n\n38\n45.623403\n126.250328\n34.447119\n108.751592\n\n\n39\n24.992364\n102.743536\n25.077731\n121.232822\n\n\n40\n24.992364\n102.743536\n34.447119\n108.751592\n\n\n41\n29.297778\n90.911944\n34.447119\n108.751592\n\n\n42\n35.046100\n118.412000\n34.447119\n108.751592\n\n\n43\n36.857214\n117.215992\n34.447119\n108.751592\n\n\n44\n43.907106\n87.474244\n34.447119\n108.751592\n\n\n45\n38.269200\n109.731000\n34.447119\n108.751592\n\n\n46\n36.898731\n30.800461\n47.464722\n8.549167\n\n\n47\n27.178317\n33.799436\n47.464722\n8.549167\n\n\n48\n42.572778\n21.035833\n47.464722\n8.549167\n\n\n50\n41.961622\n21.621381\n47.464722\n8.549167\n\n\n51\n50.865917\n7.142744\n51.477500\n-0.461389\n\n\n52\n50.865917\n7.142744\n48.110278\n16.569722\n\n\n53\n50.865917\n7.142744\n47.464722\n8.549167\n\n\n54\n52.461056\n9.685078\n48.110278\n16.569722\n\n\n55\n53.630389\n9.988228\n51.477500\n-0.461389\n\n\n56\n53.630389\n9.988228\n48.110278\n16.569722\n\n\n57\n53.630389\n9.988228\n47.464722\n8.549167\n\n\n58\n48.689878\n9.221964\n48.110278\n16.569722\n\n\n59\n52.559686\n13.287711\n48.110278\n16.569722\n\n\n60\n10.307542\n123.979439\n1.350189\n103.994433\n\n\n61\n15.185833\n120.560278\n1.350189\n103.994433\n\n\n62\n10.713044\n122.545297\n1.350189\n103.994433\n\n\n63\n14.508647\n121.019581\n40.080111\n116.584556\n\n\n64\n14.508647\n121.019581\n31.143378\n121.805214\n\n\n66\n14.508647\n121.019581\n25.077731\n121.232822\n\n\n67\n12.994414\n80.180517\n1.350189\n103.994433\n\n\n68\n40.560000\n109.997000\n34.447119\n108.751592\n\n\n69\n31.780019\n117.298436\n31.143378\n121.805214\n\n\n70\n24.992364\n102.743536\n40.080111\n116.584556\n\n\n71\n24.992364\n102.743536\n31.143378\n121.805214\n\n\n73\n26.538522\n106.800703\n1.350189\n103.994433\n\n\n74\n26.883333\n100.233330\n34.447119\n108.751592\n\n\n75\n24.401100\n98.531700\n40.080111\n116.584556\n\n\n77\n5.937208\n116.051181\n31.143378\n121.805214\n\n\n78\n13.681108\n100.747283\n31.143378\n121.805214\n\n\n79\n43.541200\n125.120100\n31.143378\n121.805214\n\n\n80\n33.511306\n126.493028\n31.143378\n121.805214\n\n\n82\n18.766847\n98.962644\n31.143378\n121.805214\n\n\n83\n38.965667\n121.538600\n31.143378\n121.805214\n\n\n84\n29.102800\n110.443000\n31.143378\n121.805214\n\n\n85\n22.308919\n113.914603\n31.143378\n121.805214\n\n\n86\n8.113200\n98.316872\n31.143378\n121.805214\n\n\n87\n45.623403\n126.250328\n31.143378\n121.805214\n\n\n88\n33.149700\n130.302000\n31.143378\n121.805214\n\n\n89\n36.181083\n140.415444\n31.143378\n121.805214\n\n\n90\n22.577094\n120.350006\n31.143378\n121.805214\n\n\n91\n34.434722\n135.244167\n31.143378\n121.805214\n\n\n93\n25.218106\n110.039197\n31.143378\n121.805214\n\n\n94\n36.117000\n103.617000\n31.143378\n121.805214\n\n\n95\n22.149556\n113.591558\n31.143378\n121.805214\n\n\n96\n31.143378\n121.805214\n1.350189\n103.994433\n\n\n97\n31.143378\n121.805214\n25.077731\n121.232822\n\n\n98\n31.197875\n121.336319\n34.447119\n108.751592\n\n\n99\n41.382400\n123.290100\n34.447119\n108.751592\n\n\n100\n38.280686\n114.697300\n25.077731\n121.232822\n\n\n102\n22.006400\n113.376000\n31.143378\n121.805214\n\n\n103\n19.088686\n72.867919\n51.477500\n-0.461389\n\n\n104\n19.088686\n72.867919\n1.350189\n103.994433\n\n\n105\n50.901389\n4.484444\n43.677223\n-79.630556\n\n\n106\n28.566500\n77.103088\n51.477500\n-0.461389\n\n\n107\n28.566500\n77.103088\n1.350189\n103.994433\n\n\n109\n37.936358\n23.944467\n51.477500\n-0.461389\n\n\n110\n37.936358\n23.944467\n48.110278\n16.569722\n\n\n111\n35.339719\n25.180297\n48.110278\n16.569722\n\n\n112\n41.669167\n44.954722\n48.110278\n16.569722\n\n\n113\n35.040222\n-106.609194\n41.978603\n-87.904842\n\n\n114\n57.201944\n-2.197778\n51.477500\n-0.461389\n\n\n115\n5.605186\n-0.166786\n51.477500\n-0.461389\n\n\n116\n36.674900\n-4.499106\n51.477500\n-0.461389\n\n\n117\n42.557100\n-92.400300\n41.978603\n-87.904842\n\n\n118\n31.722556\n35.993214\n41.978603\n-87.904842\n\n\n119\n52.308613\n4.763889\n51.477500\n-0.461389\n\n\n120\n59.651944\n17.918611\n51.477500\n-0.461389\n\n\n121\n43.991922\n-76.021739\n41.978603\n-87.904842\n\n\n123\n33.636719\n-84.428067\n51.477500\n-0.461389\n\n\n124\n33.636719\n-84.428067\n41.978603\n-87.904842\n\n\n125\n24.432972\n54.651138\n51.477500\n-0.461389\n\n\n126\n24.432972\n54.651138\n41.978603\n-87.904842\n\n\n127\n30.194528\n-97.669889\n51.477500\n-0.461389\n\n\n128\n30.194528\n-97.669889\n41.978603\n-87.904842\n\n\n129\n42.234875\n-85.552058\n41.978603\n-87.904842\n\n\n130\n26.270834\n50.633610\n51.477500\n-0.461389\n\n\n131\n41.297078\n2.078464\n51.477500\n-0.461389\n\n\n132\n41.938889\n-72.683222\n41.978603\n-87.904842\n\n\n133\n60.293386\n5.218142\n51.477500\n-0.461389\n\n\n134\n54.618056\n-5.872500\n51.477500\n-0.461389\n\n\n135\n44.535444\n11.288667\n51.477500\n-0.461389\n\n\n136\n12.949986\n77.668206\n51.477500\n-0.461389\n\n\n137\n40.477111\n-88.915917\n41.978603\n-87.904842\n\n\n138\n36.124472\n-86.678194\n41.978603\n-87.904842\n\n\n140\n42.364347\n-71.005181\n51.477500\n-0.461389\n\n\n141\n42.364347\n-71.005181\n41.978603\n-87.904842\n\n\n142\n50.901389\n4.484444\n51.477500\n-0.461389\n\n\n143\n47.590000\n7.529167\n51.477500\n-0.461389\n\n\n144\n47.436933\n19.255592\n51.477500\n-0.461389\n\n\n145\n42.940525\n-78.732167\n41.978603\n-87.904842\n\n\n146\n39.175361\n-76.668333\n51.477500\n-0.461389\n\n\n147\n39.175361\n-76.668333\n41.978603\n-87.904842\n\n\n148\n49.012779\n2.550000\n51.477500\n-0.461389\n\n\n149\n49.012779\n2.550000\n41.978603\n-87.904842\n\n\n150\n35.035278\n-85.203808\n41.978603\n-87.904842\n\n\n151\n38.138639\n-78.452861\n41.978603\n-87.904842\n\n\n152\n41.884694\n-91.710806\n41.978603\n-87.904842\n\n\n153\n41.411689\n-81.849794\n41.978603\n-87.904842\n\n\n154\n35.214000\n-80.943139\n51.477500\n-0.461389\n\n\n155\n35.214000\n-80.943139\n41.978603\n-87.904842\n\n\n156\n35.214000\n-80.943139\n43.677223\n-79.630556\n\n\n157\n39.997972\n-82.891889\n41.978603\n-87.904842\n\n\n158\n40.039250\n-88.278056\n41.978603\n-87.904842\n\n\n159\n38.818094\n-92.219631\n41.978603\n-87.904842\n\n\n160\n55.617917\n12.655972\n51.477500\n-0.461389\n\n\n161\n-33.964806\n18.601667\n51.477500\n-0.461389\n\n\n162\n21.036528\n-86.877083\n41.978603\n-87.904842\n\n\n163\n39.048836\n-84.667822\n41.978603\n-87.904842\n\n\n164\n44.772726\n-89.646635\n41.978603\n-87.904842\n\n\n165\n39.902375\n-84.219375\n41.978603\n-87.904842\n\n\n166\n42.402000\n-90.709472\n41.978603\n-87.904842\n\n\n167\n38.852083\n-77.037722\n41.978603\n-87.904842\n\n\n168\n38.852083\n-77.037722\n43.677223\n-79.630556\n\n\n170\n39.861656\n-104.673178\n51.477500\n-0.461389\n\n\n171\n39.861656\n-104.673178\n41.978603\n-87.904842\n\n\n172\n32.896828\n-97.037997\n51.477500\n-0.461389\n\n\n173\n32.896828\n-97.037997\n41.978603\n-87.904842\n\n\n174\n32.896828\n-97.037997\n43.677223\n-79.630556\n\n\n175\n25.261125\n51.565056\n41.978603\n-87.904842\n\n\n176\n41.533972\n-93.663083\n41.978603\n-87.904842\n\n\n177\n42.212444\n-83.353389\n41.978603\n-87.904842\n\n\n178\n53.421333\n-6.270075\n51.477500\n-0.461389\n\n\n179\n53.421333\n-6.270075\n41.978603\n-87.904842\n\n\n180\n51.289453\n6.766775\n51.477500\n-0.461389\n\n\n181\n51.289453\n6.766775\n41.978603\n-87.904842\n\n\n182\n25.252778\n55.364444\n51.477500\n-0.461389\n\n\n183\n0.042386\n32.443503\n51.477500\n-0.461389\n\n\n184\n55.950000\n-3.372500\n51.477500\n-0.461389\n\n\n185\n31.807250\n-106.377583\n41.978603\n-87.904842\n\n\n186\n38.036997\n-87.532364\n41.978603\n-87.904842\n\n\n187\n40.692500\n-74.168667\n51.477500\n-0.461389\n\n\n188\n40.692500\n-74.168667\n41.978603\n-87.904842\n\n\n189\n46.920650\n-96.815764\n41.978603\n-87.904842\n\n\n190\n41.804475\n12.250797\n51.477500\n-0.461389\n\n\n191\n41.804475\n12.250797\n41.978603\n-87.904842\n\n\n192\n26.072583\n-80.152750\n41.978603\n-87.904842\n\n\n193\n42.965424\n-83.743629\n41.978603\n-87.904842\n\n\n194\n50.026421\n8.543125\n51.477500\n-0.461389\n\n\n195\n43.582014\n-96.741914\n41.978603\n-87.904842\n\n\n196\n40.978472\n-85.195139\n41.978603\n-87.904842\n\n\n197\n36.151219\n-5.349664\n51.477500\n-0.461389\n\n\n198\n55.871944\n-4.433056\n51.477500\n-0.461389\n\n\n199\n57.662836\n12.279819\n51.477500\n-0.461389\n\n\n200\n44.485072\n-88.129589\n41.978603\n-87.904842\n\n\n201\n42.880833\n-85.522806\n41.978603\n-87.904842\n\n\n202\n46.238064\n6.108950\n51.477500\n-0.461389\n\n\n203\n52.461056\n9.685078\n51.477500\n-0.461389\n\n\n205\n60.317222\n24.963333\n51.477500\n-0.461389\n\n\n206\n22.308919\n113.914603\n41.978603\n-87.904842\n\n\n207\n41.066959\n-73.707575\n41.978603\n-87.904842\n\n\n208\n34.637194\n-86.775056\n41.978603\n-87.904842\n\n\n209\n17.453117\n78.467586\n51.477500\n-0.461389\n\n\n210\n38.944533\n-77.455811\n51.477500\n-0.461389\n\n\n211\n29.984433\n-95.341442\n51.477500\n-0.461389\n\n\n212\n29.984433\n-95.341442\n41.978603\n-87.904842\n\n\n213\n37.649944\n-97.433056\n41.978603\n-87.904842\n\n\n214\n39.717331\n-86.294383\n41.978603\n-87.904842\n\n\n215\n40.976922\n28.814606\n51.477500\n-0.461389\n\n\n216\n30.494056\n-81.687861\n41.978603\n-87.904842\n\n\n217\n40.639751\n-73.778925\n51.477500\n-0.461389\n\n\n218\n40.639751\n-73.778925\n41.978603\n-87.904842\n\n\n219\n40.639751\n-73.778925\n43.677223\n-79.630556\n\n\n220\n40.639751\n-73.778925\n47.464722\n8.549167\n\n\n221\n-26.139166\n28.246000\n51.477500\n-0.461389\n\n\n222\n2.745578\n101.709917\n51.477500\n-0.461389\n\n\n224\n29.226567\n47.968928\n51.477500\n-0.461389\n\n\n225\n36.080056\n-115.152250\n51.477500\n-0.461389\n\n\n226\n36.080056\n-115.152250\n41.978603\n-87.904842\n\n\n227\n36.080056\n-115.152250\n43.677223\n-79.630556\n\n\n228\n33.942536\n-118.408075\n51.477500\n-0.461389\n\n\n229\n33.942536\n-118.408075\n41.978603\n-87.904842\n\n\n230\n33.942536\n-118.408075\n31.143378\n121.805214\n\n\n231\n33.942536\n-118.408075\n43.677223\n-79.630556\n\n\n232\n53.865897\n-1.660569\n51.477500\n-0.461389\n\n\n233\n34.875117\n33.624850\n51.477500\n-0.461389\n\n\n234\n38.036500\n-84.605889\n41.978603\n-87.904842\n\n\n235\n40.777245\n-73.872608\n41.978603\n-87.904842\n\n\n236\n40.777245\n-73.872608\n43.677223\n-79.630556\n\n\n237\n51.477500\n-0.461389\n41.978603\n-87.904842\n\n\n238\n51.477500\n-0.461389\n48.110278\n16.569722\n\n\n239\n51.477500\n-0.461389\n43.677223\n-79.630556\n\n\n240\n51.477500\n-0.461389\n47.464722\n8.549167\n\n\n241\n34.729444\n-92.224306\n41.978603\n-87.904842\n\n\n242\n43.878986\n-91.256711\n41.978603\n-87.904842\n\n\n243\n40.493556\n-3.566764\n41.978603\n-87.904842\n\n\n244\n53.353744\n-2.274950\n41.978603\n-87.904842\n\n\n245\n18.503717\n-77.913358\n41.978603\n-87.904842\n\n\n246\n39.297606\n-94.713905\n41.978603\n-87.904842\n\n\n247\n28.429394\n-81.308994\n41.978603\n-87.904842\n\n\n248\n28.429394\n-81.308994\n43.677223\n-79.630556\n\n\n249\n40.193494\n-76.763403\n41.978603\n-87.904842\n\n\n250\n35.042417\n-89.976667\n41.978603\n-87.904842\n\n\n251\n19.436303\n-99.072097\n41.978603\n-87.904842\n\n\n252\n39.140972\n-96.670833\n41.978603\n-87.904842\n\n\n253\n25.793250\n-80.290556\n41.978603\n-87.904842\n\n\n254\n25.793250\n-80.290556\n43.677223\n-79.630556\n\n\n255\n42.947222\n-87.896583\n41.978603\n-87.904842\n\n\n256\n41.448528\n-90.507539\n41.978603\n-87.904842\n\n\n257\n46.353611\n-87.395278\n41.978603\n-87.904842\n\n\n258\n43.139858\n-89.337514\n41.978603\n-87.904842\n\n\n259\n44.881956\n-93.221767\n41.978603\n-87.904842\n\n\n260\n29.993389\n-90.258028\n41.978603\n-87.904842\n\n\n261\n35.764722\n140.386389\n41.978603\n-87.904842\n\n\n262\n35.764722\n140.386389\n1.350189\n103.994433\n\n\n263\n35.764722\n140.386389\n25.077731\n121.232822\n\n\n264\n35.393089\n-97.600733\n41.978603\n-87.904842\n\n\n265\n41.303167\n-95.894069\n41.978603\n-87.904842\n\n\n266\n41.978603\n-87.904842\n40.080111\n116.584556\n\n\n267\n41.978603\n-87.904842\n31.143378\n121.805214\n\n\n268\n41.978603\n-87.904842\n43.677223\n-79.630556\n\n\n269\n39.871944\n-75.241139\n43.677223\n-79.630556\n\n\n270\n39.871944\n-75.241139\n47.464722\n8.549167\n\n\n271\n28.945464\n-13.605225\n47.464722\n8.549167\n\n\n272\n36.674900\n-4.499106\n48.110278\n16.569722\n\n\n273\n38.282169\n-0.558156\n47.464722\n8.549167\n\n\n274\n31.722556\n35.993214\n48.110278\n16.569722\n\n\n275\n59.651944\n17.918611\n48.110278\n16.569722\n\n\n276\n24.432972\n54.651138\n40.080111\n116.584556\n\n\n277\n24.432972\n54.651138\n1.350189\n103.994433\n\n\n278\n41.297078\n2.078464\n48.110278\n16.569722\n\n\n280\n-27.384167\n153.117500\n1.350189\n103.994433\n\n\n282\n49.012779\n2.550000\n48.110278\n16.569722\n\n\n283\n39.601944\n19.911667\n48.110278\n16.569722\n\n\n284\n35.531747\n24.149678\n48.110278\n16.569722\n\n\n285\n55.617917\n12.655972\n48.110278\n16.569722\n\n\n286\n37.466781\n15.066400\n47.464722\n8.549167\n\n\n288\n51.289453\n6.766775\n48.110278\n16.569722\n\n\n289\n51.289453\n6.766775\n47.464722\n8.549167\n\n\n290\n41.804475\n12.250797\n48.110278\n16.569722\n\n\n291\n43.809953\n11.205100\n48.110278\n16.569722\n\n\n292\n32.697889\n-16.774453\n48.110278\n16.569722\n\n\n293\n32.697889\n-16.774453\n47.464722\n8.549167\n\n\n295\n50.026421\n8.543125\n48.110278\n16.569722\n\n\n296\n28.452717\n-13.863761\n48.110278\n16.569722\n\n\n297\n28.452717\n-13.863761\n47.464722\n8.549167\n\n\n298\n54.913250\n8.340472\n47.464722\n8.549167\n\n\n303\n60.317222\n24.963333\n48.110278\n16.569722\n\n\n304\n60.317222\n24.963333\n47.464722\n8.549167\n\n\n306\n35.339719\n25.180297\n47.464722\n8.549167\n\n\n307\n38.872858\n1.373117\n47.464722\n8.549167\n\n\n308\n37.435128\n25.348103\n48.110278\n16.569722\n\n\n309\n36.399169\n25.479333\n48.110278\n16.569722\n\n\n310\n37.068319\n22.025525\n48.110278\n16.569722\n\n\n311\n34.875117\n33.624850\n48.110278\n16.569722\n\n\n312\n51.505278\n0.055278\n47.464722\n8.549167\n\n\n315\n27.931886\n-15.386586\n48.110278\n16.569722\n\n\n317\n40.493556\n-3.566764\n48.110278\n16.569722\n\n\n318\n40.493556\n-3.566764\n47.464722\n8.549167\n\n\n319\n35.857497\n14.477500\n48.110278\n16.569722\n\n\n320\n48.353783\n11.786086\n48.110278\n16.569722\n\n\n321\n45.630606\n8.728111\n48.110278\n16.569722\n\n\n322\n40.886033\n14.290781\n47.464722\n8.549167\n\n\n323\n43.658411\n7.215872\n48.110278\n16.569722\n\n\n324\n49.498700\n11.066897\n48.110278\n16.569722\n\n\n325\n40.898661\n9.517628\n48.110278\n16.569722\n\n\n326\n40.898661\n9.517628\n47.464722\n8.549167\n\n\n327\n39.553610\n2.727778\n48.110278\n16.569722\n\n\n328\n39.553610\n2.727778\n47.464722\n8.549167\n\n\n329\n38.925467\n20.765311\n48.110278\n16.569722\n\n\n330\n36.405419\n28.086192\n48.110278\n16.569722\n\n\n331\n40.519725\n22.970950\n48.110278\n16.569722\n\n\n332\n38.905394\n16.242269\n47.464722\n8.549167\n\n\n333\n47.793304\n13.004333\n48.110278\n16.569722\n\n\n334\n28.044475\n-16.572489\n48.110278\n16.569722\n\n\n335\n28.044475\n-16.572489\n47.464722\n8.549167\n\n\n337\n52.559686\n13.287711\n47.464722\n8.549167\n\n\n338\n48.110278\n16.569722\n47.464722\n8.549167\n\n\n339\n17.136749\n-61.792667\n43.677223\n-79.630556\n\n\n340\n33.636719\n-84.428067\n43.677223\n-79.630556\n\n\n341\n12.501389\n-70.015221\n43.677223\n-79.630556\n\n\n342\n24.432972\n54.651138\n43.677223\n-79.630556\n\n\n343\n19.267000\n-69.742000\n43.677223\n-79.630556\n\n\n344\n41.297078\n2.078464\n43.677223\n-79.630556\n\n\n345\n32.364042\n-64.678703\n43.677223\n-79.630556\n\n\n346\n41.938889\n-72.683222\n43.677223\n-79.630556\n\n\n347\n13.074603\n-59.492456\n43.677223\n-79.630556\n\n\n348\n36.124472\n-86.678194\n43.677223\n-79.630556\n\n\n349\n4.701594\n-74.146947\n43.677223\n-79.630556\n\n\n350\n42.364347\n-71.005181\n43.677223\n-79.630556\n\n\n352\n39.175361\n-76.668333\n43.677223\n-79.630556\n\n\n353\n22.513200\n-78.511000\n43.677223\n-79.630556\n\n\n354\n49.012779\n2.550000\n43.677223\n-79.630556\n\n\n355\n41.411689\n-81.849794\n43.677223\n-79.630556\n\n\n357\n39.997972\n-82.891889\n43.677223\n-79.630556\n\n\n358\n55.617917\n12.655972\n43.677223\n-79.630556\n\n\n359\n21.036528\n-86.877083\n43.677223\n-79.630556\n\n\n360\n39.048836\n-84.667822\n43.677223\n-79.630556\n\n\n362\n39.861656\n-104.673178\n43.677223\n-79.630556\n\n\n364\n42.212444\n-83.353389\n43.677223\n-79.630556\n\n\n365\n53.421333\n-6.270075\n43.677223\n-79.630556\n\n\n366\n40.692500\n-74.168667\n43.677223\n-79.630556\n\n\n367\n41.804475\n12.250797\n43.677223\n-79.630556\n\n\n368\n26.072583\n-80.152750\n43.677223\n-79.630556\n\n\n369\n50.026421\n8.543125\n43.677223\n-79.630556\n\n\n370\n19.292778\n-81.357750\n43.677223\n-79.630556\n\n\n371\n23.562631\n-75.877958\n43.677223\n-79.630556\n\n\n372\n12.004247\n-61.786192\n43.677223\n-79.630556\n\n\n373\n-23.432075\n-46.469511\n43.677223\n-79.630556\n\n\n374\n22.989153\n-82.409086\n43.677223\n-79.630556\n\n\n375\n22.308919\n113.914603\n43.677223\n-79.630556\n\n\n376\n20.785589\n-76.315108\n43.677223\n-79.630556\n\n\n377\n38.944533\n-77.455811\n43.677223\n-79.630556\n\n\n378\n29.984433\n-95.341442\n43.677223\n-79.630556\n\n\n379\n39.717331\n-86.294383\n43.677223\n-79.630556\n\n\n380\n40.976922\n28.814606\n43.677223\n-79.630556\n\n\n382\n17.935667\n-76.787500\n43.677223\n-79.630556\n\n\n387\n-12.021889\n-77.114319\n43.677223\n-79.630556\n\n\n388\n10.593289\n-85.544408\n43.677223\n-79.630556\n\n\n389\n18.503717\n-77.913358\n43.677223\n-79.630556\n\n\n390\n39.297606\n-94.713905\n43.677223\n-79.630556\n\n\n392\n40.193494\n-76.763403\n43.677223\n-79.630556\n\n\n393\n19.436303\n-99.072097\n43.677223\n-79.630556\n\n\n395\n42.947222\n-87.896583\n43.677223\n-79.630556\n\n\n396\n44.881956\n-93.221767\n43.677223\n-79.630556\n\n\n398\n29.993389\n-90.258028\n43.677223\n-79.630556\n\n\n399\n48.353783\n11.786086\n43.677223\n-79.630556\n\n\n400\n25.038958\n-77.466231\n43.677223\n-79.630556\n\n\n401\n35.764722\n140.386389\n43.677223\n-79.630556\n\n\n403\n40.080111\n116.584556\n43.677223\n-79.630556\n\n\n405\n33.434278\n-112.011583\n43.677223\n-79.630556\n\n\n406\n40.491467\n-80.232872\n43.677223\n-79.630556\n\n\n407\n21.773625\n-72.265886\n43.677223\n-79.630556\n\n\n408\n19.757900\n-70.570033\n43.677223\n-79.630556\n\n\n409\n18.567367\n-68.363431\n43.677223\n-79.630556\n\n\n410\n31.143378\n121.805214\n43.677223\n-79.630556\n\n\n411\n35.877639\n-78.787472\n43.677223\n-79.630556\n\n\n412\n43.118866\n-77.672389\n43.677223\n-79.630556\n\n\n413\n26.536167\n-81.755167\n43.677223\n-79.630556\n\n\n414\n32.733556\n-117.189667\n43.677223\n-79.630556\n\n\n415\n-33.392975\n-70.785803\n43.677223\n-79.630556\n\n\n416\n47.449000\n-122.309306\n43.677223\n-79.630556\n\n\n417\n37.618972\n-122.374889\n43.677223\n-79.630556\n\n\n418\n9.993861\n-84.208806\n43.677223\n-79.630556\n\n\n419\n22.492192\n-79.943611\n43.677223\n-79.630556\n\n\n420\n38.748697\n-90.370028\n43.677223\n-79.630556\n\n\n421\n43.111187\n-76.106311\n43.677223\n-79.630556\n\n\n422\n32.011389\n34.886667\n43.677223\n-79.630556\n\n\n423\n27.975472\n-82.533250\n43.677223\n-79.630556\n\n\n424\n13.733194\n-60.952597\n43.677223\n-79.630556\n\n\n425\n48.110278\n16.569722\n43.677223\n-79.630556\n\n\n426\n23.034445\n-81.435278\n43.677223\n-79.630556\n\n\n427\n52.165750\n20.967122\n43.677223\n-79.630556\n\n\n428\n46.485001\n-84.509445\n43.677223\n-79.630556\n\n\n429\n49.210833\n-57.391388\n43.677223\n-79.630556\n\n\n430\n53.309723\n-113.579722\n43.677223\n-79.630556\n\n\n431\n45.868889\n-66.537222\n43.677223\n-79.630556\n\n\n432\n44.225277\n-76.596944\n43.677223\n-79.630556\n\n\n433\n44.880833\n-63.508610\n43.677223\n-79.630556\n\n\n434\n56.653333\n-111.221944\n43.677223\n-79.630556\n\n\n435\n45.322500\n-75.669167\n43.677223\n-79.630556\n\n\n436\n46.791111\n-71.393333\n43.677223\n-79.630556\n\n\n437\n42.275556\n-82.955556\n43.677223\n-79.630556\n\n\n438\n46.112221\n-64.678611\n43.677223\n-79.630556\n\n\n439\n50.431944\n-104.665833\n43.677223\n-79.630556\n\n\n440\n48.371944\n-89.323889\n43.677223\n-79.630556\n\n\n441\n46.161388\n-60.047779\n43.677223\n-79.630556\n\n\n442\n46.625000\n-80.798889\n43.677223\n-79.630556\n\n\n443\n45.316111\n-65.890278\n43.677223\n-79.630556\n\n\n444\n48.569721\n-81.376667\n43.677223\n-79.630556\n\n\n445\n45.470556\n-73.740833\n43.677223\n-79.630556\n\n\n446\n45.470556\n-73.740833\n47.464722\n8.549167\n\n\n447\n49.193889\n-123.184444\n43.677223\n-79.630556\n\n\n448\n49.910036\n-97.239886\n43.677223\n-79.630556\n\n\n449\n52.170834\n-106.699722\n43.677223\n-79.630556\n\n\n450\n43.033056\n-81.151111\n43.677223\n-79.630556\n\n\n451\n46.363611\n-79.422778\n43.677223\n-79.630556\n\n\n452\n51.113888\n-114.020278\n43.677223\n-79.630556\n\n\n453\n46.290001\n-63.121111\n43.677223\n-79.630556\n\n\n454\n48.646944\n-123.425833\n43.677223\n-79.630556\n\n\n455\n47.618610\n-52.751945\n43.677223\n-79.630556\n\n\n456\n43.677223\n-79.630556\n47.464722\n8.549167\n\n\n457\n34.519672\n113.840889\n25.077731\n121.232822\n\n\n458\n28.189158\n113.219633\n25.077731\n121.232822\n\n\n459\n26.883333\n100.233330\n25.077731\n121.232822\n\n\n460\n29.826683\n121.461906\n25.077731\n121.232822\n\n\n461\n31.742042\n118.862025\n25.077731\n121.232822\n\n\n462\n41.382400\n123.290100\n25.077731\n121.232822\n\n\n463\n52.308613\n4.763889\n25.077731\n121.232822\n\n\n468\n23.392436\n113.298786\n1.350189\n103.994433\n\n\n471\n49.012779\n2.550000\n40.080111\n116.584556\n\n\n472\n49.012779\n2.550000\n31.143378\n121.805214\n\n\n473\n49.012779\n2.550000\n1.350189\n103.994433\n\n\n476\n49.012779\n2.550000\n47.464722\n8.549167\n\n\n477\n42.212444\n-83.353389\n51.477500\n-0.461389\n\n\n479\n45.726387\n5.090833\n48.110278\n16.569722\n\n\n481\n36.691014\n3.215408\n51.477500\n-0.461389\n\n\n482\n36.691014\n3.215408\n40.080111\n116.584556\n\n\n483\n36.691014\n3.215408\n48.110278\n16.569722\n\n\n486\n19.088686\n72.867919\n47.464722\n8.549167\n\n\n487\n22.654739\n88.446722\n1.350189\n103.994433\n\n\n489\n28.566500\n77.103088\n41.978603\n-87.904842\n\n\n490\n28.566500\n77.103088\n31.143378\n121.805214\n\n\n492\n28.566500\n77.103088\n48.110278\n16.569722\n\n\n493\n28.566500\n77.103088\n47.464722\n8.549167\n\n\n495\n37.469075\n126.450517\n1.350189\n103.994433\n\n\n500\n5.937208\n116.051181\n1.350189\n103.994433\n\n\n501\n5.937208\n116.051181\n25.077731\n121.232822\n\n\n502\n6.166850\n102.293014\n1.350189\n103.994433\n\n\n503\n1.484697\n110.346933\n1.350189\n103.994433\n\n\n505\n6.329728\n99.728667\n1.350189\n103.994433\n\n\n506\n4.322014\n113.986806\n1.350189\n103.994433\n\n\n508\n20.521800\n-103.311167\n41.978603\n-87.904842\n\n\n510\n37.466781\n15.066400\n48.110278\n16.569722\n\n\n511\n61.174361\n-149.996361\n41.978603\n-87.904842\n\n\n514\n33.367467\n-7.589967\n51.477500\n-0.461389\n\n\n515\n33.367467\n-7.589967\n47.464722\n8.549167\n\n\n518\n13.440947\n-89.055728\n43.677223\n-79.630556\n\n\n535\n60.317222\n24.963333\n40.080111\n116.584556\n\n\n536\n60.317222\n24.963333\n31.143378\n121.805214\n\n\n537\n60.317222\n24.963333\n1.350189\n103.994433\n\n\n539\n60.317222\n24.963333\n34.447119\n108.751592\n\n\n551\n41.248055\n-8.681389\n47.464722\n8.549167\n\n\n557\n28.566500\n77.103088\n25.077731\n121.232822\n\n\n560\n41.804475\n12.250797\n31.143378\n121.805214\n\n\n563\n41.804475\n12.250797\n47.464722\n8.549167\n\n\n565\n45.445103\n9.276739\n48.110278\n16.569722\n\n\n566\n53.882469\n28.030731\n48.110278\n16.569722\n\n\n569\n29.719217\n106.641678\n25.077731\n121.232822\n\n\n570\n38.965667\n121.538600\n25.077731\n121.232822\n\n\n571\n25.935064\n119.663272\n25.077731\n121.232822\n\n\n575\n22.639258\n113.810664\n25.077731\n121.232822\n\n\n576\n36.266108\n120.374436\n25.077731\n121.232822\n\n\n577\n25.077731\n121.232822\n34.447119\n108.751592\n\n\n578\n9.006792\n7.263172\n51.477500\n-0.461389\n\n\n582\n43.352072\n77.040508\n51.477500\n-0.461389\n\n\n583\n31.722556\n35.993214\n51.477500\n-0.461389\n\n\n593\n33.820931\n35.488389\n51.477500\n-0.461389\n\n\n596\n13.681108\n100.747283\n51.477500\n-0.461389\n\n\n597\n55.740322\n9.151778\n47.464722\n8.549167\n\n\n606\n30.121944\n31.405556\n51.477500\n-0.461389\n\n\n611\n30.578528\n103.947086\n51.477500\n-0.461389\n\n\n615\n55.408611\n37.906111\n51.477500\n-0.461389\n\n\n624\n-34.822222\n-58.535833\n51.477500\n-0.461389\n\n\n625\n37.014425\n-7.965911\n51.477500\n-0.461389\n\n\n628\n8.616444\n-13.195489\n51.477500\n-0.461389\n\n\n631\n-22.808903\n-43.243647\n51.477500\n-0.461389\n\n\n634\n-23.432075\n-46.469511\n51.477500\n-0.461389\n\n\n636\n40.467500\n50.046667\n51.477500\n-0.461389\n\n\n640\n22.308919\n113.914603\n51.477500\n-0.461389\n\n\n641\n35.552258\n139.779694\n51.477500\n-0.461389\n\n\n645\n38.872858\n1.373117\n51.477500\n-0.461389\n\n\n646\n37.469075\n126.450517\n51.477500\n-0.461389\n\n\n648\n21.679564\n39.156536\n51.477500\n-0.461389\n\n\n650\n37.435128\n25.348103\n51.477500\n-0.461389\n\n\n652\n36.399169\n25.479333\n51.477500\n-0.461389\n\n\n653\n50.345000\n30.894722\n51.477500\n-0.461389\n\n\n655\n-8.858375\n13.231178\n51.477500\n-0.461389\n\n\n661\n59.800292\n30.262503\n51.477500\n-0.461389\n\n\n663\n51.477500\n-0.461389\n40.080111\n116.584556\n\n\n664\n51.477500\n-0.461389\n31.143378\n121.805214\n\n\n665\n51.477500\n-0.461389\n1.350189\n103.994433\n\n\n672\n23.843333\n90.397781\n51.477500\n-0.461389\n\n\n673\n23.843333\n90.397781\n1.350189\n103.994433\n\n\n674\n4.944200\n114.928353\n31.143378\n121.805214\n\n\n675\n4.944200\n114.928353\n1.350189\n103.994433\n\n\n676\n24.550560\n55.103174\n51.477500\n-0.461389\n\n\n677\n28.189158\n113.219633\n34.447119\n108.751592\n\n\n678\n22.608267\n108.172442\n34.447119\n108.751592\n\n\n679\n39.124353\n117.346183\n34.447119\n108.751592\n\n\n681\n43.670833\n142.447500\n25.077731\n121.232822\n\n\n683\n13.681108\n100.747283\n25.077731\n121.232822\n\n\n684\n13.681108\n100.747283\n48.110278\n16.569722\n\n\n685\n-27.384167\n153.117500\n25.077731\n121.232822\n\n\n686\n23.392436\n113.298786\n25.077731\n121.232822\n\n\n687\n49.012779\n2.550000\n25.077731\n121.232822\n\n\n688\n-6.125567\n106.655897\n25.077731\n121.232822\n\n\n690\n42.775200\n141.692283\n25.077731\n121.232822\n\n\n691\n30.578528\n103.947086\n25.077731\n121.232822\n\n\n692\n-8.748169\n115.167172\n25.077731\n121.232822\n\n\n693\n33.585942\n130.450686\n25.077731\n121.232822\n\n\n694\n13.483450\n144.795983\n25.077731\n121.232822\n\n\n695\n21.221192\n105.807178\n25.077731\n121.232822\n\n\n696\n40.851422\n111.824103\n25.077731\n121.232822\n\n\n697\n30.229503\n120.434453\n25.077731\n121.232822\n\n\n698\n41.770000\n140.821944\n25.077731\n121.232822\n\n\n699\n22.308919\n113.914603\n25.077731\n121.232822\n\n\n700\n45.623403\n126.250328\n25.077731\n121.232822\n\n\n701\n37.469075\n126.450517\n25.077731\n121.232822\n\n\n702\n40.639751\n-73.778925\n25.077731\n121.232822\n\n\n705\n36.394611\n136.406544\n25.077731\n121.232822\n\n\n706\n2.745578\n101.709917\n25.077731\n121.232822\n\n\n707\n25.218106\n110.039197\n25.077731\n121.232822\n\n\n708\n33.942536\n-118.408075\n25.077731\n121.232822\n\n\n709\n22.149556\n113.591558\n25.077731\n121.232822\n\n\n712\n34.756944\n133.855278\n25.077731\n121.232822\n\n\n713\n40.080111\n116.584556\n25.077731\n121.232822\n\n\n714\n11.546556\n104.844139\n25.077731\n121.232822\n\n\n716\n38.139722\n140.916944\n25.077731\n121.232822\n\n\n717\n47.449000\n-122.309306\n25.077731\n121.232822\n\n\n718\n37.618972\n-122.374889\n25.077731\n121.232822\n\n\n719\n10.818797\n106.651856\n25.077731\n121.232822\n\n\n721\n-7.379831\n112.786858\n25.077731\n121.232822\n\n\n722\n36.857214\n117.215992\n25.077731\n121.232822\n\n\n723\n25.077731\n121.232822\n43.677223\n-79.630556\n\n\n724\n56.923611\n23.971111\n48.110278\n16.569722\n\n\n725\n56.923611\n23.971111\n47.464722\n8.549167\n\n\n726\n6.498553\n-58.254119\n43.677223\n-79.630556\n\n\n729\n10.595369\n-61.337242\n43.677223\n-79.630556\n\n\n730\n35.179528\n128.938222\n25.077731\n121.232822\n\n\n731\n35.179528\n128.938222\n34.447119\n108.751592\n\n\n732\n8.977889\n38.799319\n40.080111\n116.584556\n\n\n733\n-37.008056\n174.791667\n31.143378\n121.805214\n\n\n734\n59.651944\n17.918611\n40.080111\n116.584556\n\n\n735\n40.560000\n109.997000\n40.080111\n116.584556\n\n\n737\n21.539400\n109.294000\n40.080111\n116.584556\n\n\n738\n13.681108\n100.747283\n40.080111\n116.584556\n\n\n739\n30.121944\n31.405556\n40.080111\n116.584556\n\n\n740\n23.392436\n113.298786\n40.080111\n116.584556\n\n\n741\n23.392436\n113.298786\n31.143378\n121.805214\n\n\n742\n23.392436\n113.298786\n34.447119\n108.751592\n\n\n745\n34.519672\n113.840889\n40.080111\n116.584556\n\n\n746\n43.541200\n125.120100\n40.080111\n116.584556\n\n\n747\n41.538100\n120.435000\n40.080111\n116.584556\n\n\n748\n42.235000\n118.908000\n40.080111\n116.584556\n\n\n749\n36.716600\n127.499119\n40.080111\n116.584556\n\n\n754\n18.766847\n98.962644\n40.080111\n116.584556\n\n\n755\n55.617917\n12.655972\n40.080111\n116.584556\n\n\n756\n28.189158\n113.219633\n40.080111\n116.584556\n\n\n757\n42.775200\n141.692283\n40.080111\n116.584556\n\n\n760\n30.578528\n103.947086\n1.350189\n103.994433\n\n\n763\n31.941667\n119.711667\n40.080111\n116.584556\n\n\n764\n40.060300\n113.482000\n40.080111\n116.584556\n\n\n765\n31.300000\n107.500000\n40.080111\n116.584556\n\n\n766\n40.025500\n124.286600\n40.080111\n116.584556\n\n\n767\n28.566500\n77.103088\n40.080111\n116.584556\n\n\n768\n38.965667\n121.538600\n40.080111\n116.584556\n\n\n770\n38.965667\n121.538600\n34.447119\n108.751592\n\n\n771\n39.850000\n110.033000\n40.080111\n116.584556\n\n\n772\n51.289453\n6.766775\n40.080111\n116.584556\n\n\n773\n25.252778\n55.364444\n40.080111\n116.584556\n\n\n774\n29.102800\n110.443000\n40.080111\n116.584556\n\n\n775\n40.692500\n-74.168667\n40.080111\n116.584556\n\n\n776\n40.692500\n-74.168667\n31.143378\n121.805214\n\n\n777\n41.804475\n12.250797\n40.080111\n116.584556\n\n\n778\n39.224061\n125.670150\n40.080111\n116.584556\n\n\n779\n25.935064\n119.663272\n40.080111\n116.584556\n\n\n780\n50.026421\n8.543125\n40.080111\n116.584556\n\n\n781\n50.026421\n8.543125\n31.143378\n121.805214\n\n\n782\n32.900000\n115.816667\n40.080111\n116.584556\n\n\n783\n33.585942\n130.450686\n31.143378\n121.805214\n\n\n784\n37.558311\n126.790586\n40.080111\n116.584556\n\n\n785\n46.238064\n6.108950\n40.080111\n116.584556\n\n\n786\n32.391100\n105.702000\n40.080111\n116.584556\n\n\n787\n19.934856\n110.458961\n40.080111\n116.584556\n\n\n790\n40.851422\n111.824103\n40.080111\n116.584556\n\n\n791\n40.851422\n111.824103\n31.143378\n121.805214\n\n\n793\n31.780019\n117.298436\n40.080111\n116.584556\n\n\n794\n30.229503\n120.434453\n40.080111\n116.584556\n\n\n796\n30.229503\n120.434453\n34.447119\n108.751592\n\n\n797\n22.308919\n113.914603\n40.080111\n116.584556\n\n\n798\n8.113200\n98.316872\n40.080111\n116.584556\n\n\n799\n49.204997\n119.825000\n40.080111\n116.584556\n\n\n800\n46.083000\n122.017000\n40.080111\n116.584556\n\n\n801\n42.841400\n93.669200\n40.080111\n116.584556\n\n\n802\n35.552258\n139.779694\n40.080111\n116.584556\n\n\n803\n21.318681\n-157.922428\n40.080111\n116.584556\n\n\n804\n45.623403\n126.250328\n40.080111\n116.584556\n\n\n806\n28.562200\n121.429000\n40.080111\n116.584556\n\n\n807\n38.944533\n-77.455811\n40.080111\n116.584556\n\n\n808\n29.984433\n-95.341442\n40.080111\n116.584556\n\n\n809\n37.469075\n126.450517\n40.080111\n116.584556\n\n\n810\n38.481944\n106.009167\n40.080111\n116.584556\n\n\n811\n38.481944\n106.009167\n31.143378\n121.805214\n\n\n812\n38.481944\n106.009167\n34.447119\n108.751592\n\n\n813\n40.976922\n28.814606\n40.080111\n116.584556\n\n\n814\n40.976922\n28.814606\n31.143378\n121.805214\n\n\n815\n29.338600\n117.176000\n40.080111\n116.584556\n\n\n816\n29.338600\n117.176000\n34.447119\n108.751592\n\n\n817\n40.639751\n-73.778925\n40.080111\n116.584556\n\n\n818\n26.899700\n114.737500\n40.080111\n116.584556\n\n\n819\n29.733000\n115.983000\n40.080111\n116.584556\n\n\n820\n24.796400\n118.590000\n31.143378\n121.805214\n\n\n821\n46.843394\n130.465389\n40.080111\n116.584556\n\n\n822\n32.857000\n103.683000\n31.143378\n121.805214\n\n\n823\n28.865000\n115.900000\n40.080111\n116.584556\n\n\n824\n28.865000\n115.900000\n31.143378\n121.805214\n\n\n825\n34.434722\n135.244167\n40.080111\n116.584556\n\n\n829\n25.825800\n114.912000\n40.080111\n116.584556\n\n\n830\n26.538522\n106.800703\n40.080111\n116.584556\n\n\n831\n26.538522\n106.800703\n31.143378\n121.805214\n\n\n832\n26.538522\n106.800703\n34.447119\n108.751592\n\n\n833\n25.218106\n110.039197\n40.080111\n116.584556\n\n\n835\n25.218106\n110.039197\n34.447119\n108.751592\n\n\n836\n33.942536\n-118.408075\n40.080111\n116.584556\n\n\n838\n51.148056\n-0.190278\n40.080111\n116.584556\n\n\n841\n36.117000\n103.617000\n40.080111\n116.584556\n\n\n842\n36.117000\n103.617000\n34.447119\n108.751592\n\n\n843\n26.883333\n100.233330\n40.080111\n116.584556\n\n\n844\n29.297778\n90.911944\n40.080111\n116.584556\n\n\n845\n24.207500\n109.391000\n40.080111\n116.584556\n\n\n846\n40.493556\n-3.566764\n40.080111\n116.584556\n\n\n847\n44.523889\n129.568889\n40.080111\n116.584556\n\n\n848\n-37.673333\n144.843333\n31.143378\n121.805214\n\n\n849\n22.149556\n113.591558\n40.080111\n116.584556\n\n\n851\n31.428100\n104.741000\n40.080111\n116.584556\n\n\n853\n48.353783\n11.786086\n40.080111\n116.584556\n\n\n854\n48.353783\n11.786086\n31.143378\n121.805214\n\n\n855\n34.991389\n126.382778\n40.080111\n116.584556\n\n\n856\n45.630606\n8.728111\n40.080111\n116.584556\n\n\n857\n45.630606\n8.728111\n31.143378\n121.805214\n\n\n858\n47.239628\n123.918131\n40.080111\n116.584556\n\n\n859\n29.826683\n121.461906\n40.080111\n116.584556\n\n\n860\n34.858414\n136.805408\n31.143378\n121.805214\n\n\n861\n31.742042\n118.862025\n40.080111\n116.584556\n\n\n862\n31.742042\n118.862025\n34.447119\n108.751592\n\n\n863\n22.608267\n108.172442\n40.080111\n116.584556\n\n\n865\n35.764722\n140.386389\n40.080111\n116.584556\n\n\n866\n35.764722\n140.386389\n31.143378\n121.805214\n\n\n867\n32.070800\n120.976000\n40.080111\n116.584556\n\n\n868\n26.195814\n127.645869\n40.080111\n116.584556\n\n\n871\n40.080111\n116.584556\n31.143378\n121.805214\n\n\n872\n40.080111\n116.584556\n1.350189\n103.994433\n\n\n874\n40.080111\n116.584556\n48.110278\n16.569722\n\n\n875\n40.080111\n116.584556\n34.447119\n108.751592\n\n\n878\n31.143378\n121.805214\n34.447119\n108.751592\n\n\n881\n22.639258\n113.810664\n34.447119\n108.751592\n\n\n882\n36.266108\n120.374436\n34.447119\n108.751592\n\n\n886\n27.912200\n120.852000\n34.447119\n108.751592\n\n\n887\n31.494400\n120.429000\n34.447119\n108.751592\n\n\n898\n50.026421\n8.543125\n25.077731\n121.232822\n\n\n899\n34.796111\n138.189444\n25.077731\n121.232822\n\n\n902\n19.934856\n110.458961\n25.077731\n121.232822\n\n\n904\n34.436111\n132.919444\n25.077731\n121.232822\n\n\n906\n21.318681\n-157.922428\n25.077731\n121.232822\n\n\n908\n24.344525\n124.186983\n25.077731\n121.232822\n\n\n909\n22.577094\n120.350006\n40.080111\n116.584556\n\n\n911\n22.577094\n120.350006\n1.350189\n103.994433\n\n\n912\n28.865000\n115.900000\n25.077731\n121.232822\n\n\n914\n31.877222\n131.448611\n25.077731\n121.232822\n\n\n915\n31.803397\n130.719408\n25.077731\n121.232822\n\n\n919\n34.858414\n136.805408\n25.077731\n121.232822\n\n\n921\n26.195814\n127.645869\n25.077731\n121.232822\n\n\n923\n5.297139\n100.276864\n25.077731\n121.232822\n\n\n927\n16.907305\n96.133222\n25.077731\n121.232822\n\n\n928\n7.367303\n134.544278\n25.077731\n121.232822\n\n\n932\n-33.946111\n151.177222\n25.077731\n121.232822\n\n\n933\n18.302897\n109.412272\n25.077731\n121.232822\n\n\n935\n34.214167\n134.015556\n25.077731\n121.232822\n\n\n937\n36.648333\n137.187500\n25.077731\n121.232822\n\n\n938\n25.077731\n121.232822\n48.110278\n16.569722\n\n\n940\n9.071364\n-79.383453\n43.677223\n-79.630556\n\n\n946\n21.420428\n-77.847433\n43.677223\n-79.630556\n\n\n950\n7.180756\n79.884117\n1.350189\n103.994433\n\n\n958\n22.308919\n113.914603\n34.447119\n108.751592\n\n\n966\n52.308613\n4.763889\n40.080111\n116.584556\n\n\n967\n52.308613\n4.763889\n31.143378\n121.805214\n\n\n968\n23.392436\n113.298786\n51.477500\n-0.461389\n\n\n974\n28.918900\n111.640000\n40.080111\n116.584556\n\n\n976\n34.519672\n113.840889\n31.143378\n121.805214\n\n\n980\n43.541200\n125.120100\n25.077731\n121.232822\n\n\n981\n36.247500\n113.126000\n40.080111\n116.584556\n\n\n985\n28.189158\n113.219633\n31.143378\n121.805214\n\n\n990\n40.025500\n124.286600\n31.143378\n121.805214\n\n\n995\n40.094000\n94.481800\n34.447119\n108.751592\n\n\n996\n42.212444\n-83.353389\n40.080111\n116.584556\n\n\n997\n42.212444\n-83.353389\n31.143378\n121.805214\n\n\n999\n29.102800\n110.443000\n25.077731\n121.232822\n\n\n1001\n25.935064\n119.663272\n1.350189\n103.994433\n\n\n1005\n19.934856\n110.458961\n31.143378\n121.805214\n\n\n1006\n21.221192\n105.807178\n40.080111\n116.584556\n\n\n1007\n31.780019\n117.298436\n34.447119\n108.751592\n\n\n1015\n29.934200\n122.362000\n40.080111\n116.584556\n\n\n1017\n37.469075\n126.450517\n31.143378\n121.805214\n\n\n1020\n33.616653\n73.099233\n40.080111\n116.584556\n\n\n1021\n24.796400\n118.590000\n40.080111\n116.584556\n\n\n1022\n46.843394\n130.465389\n31.143378\n121.805214\n\n\n1023\n28.865000\n115.900000\n34.447119\n108.751592\n\n\n1027\n2.745578\n101.709917\n31.143378\n121.805214\n\n\n1030\n26.538522\n106.800703\n25.077731\n121.232822\n\n\n1037\n44.523889\n129.568889\n31.143378\n121.805214\n\n\n1038\n30.754000\n106.062000\n40.080111\n116.584556\n\n\n1039\n42.088056\n127.548889\n40.080111\n116.584556\n\n\n1040\n47.239628\n123.918131\n31.143378\n121.805214\n\n\n1045\n22.608267\n108.172442\n25.077731\n121.232822\n\n\n1054\n18.302897\n109.412272\n34.447119\n108.751592\n\n\n1060\n37.746897\n112.628428\n34.447119\n108.751592\n\n\n1064\n30.783758\n114.208100\n34.447119\n108.751592\n\n\n1065\n30.836100\n108.406000\n34.447119\n108.751592\n\n\n1067\n2.745578\n101.709917\n40.080111\n116.584556\n\n\n1071\n11.679431\n122.376294\n1.350189\n103.994433\n\n\n1072\n52.308613\n4.763889\n41.978603\n-87.904842\n\n\n1073\n52.308613\n4.763889\n43.677223\n-79.630556\n\n\n1077\n33.636719\n-84.428067\n47.464722\n8.549167\n\n\n1094\n37.469075\n126.450517\n41.978603\n-87.904842\n\n\n1097\n40.639751\n-73.778925\n31.143378\n121.805214\n\n\n1115\n-8.858375\n13.231178\n40.080111\n116.584556\n\n\n1116\n60.193917\n11.100361\n48.110278\n16.569722\n\n\n1121\n53.421333\n-6.270075\n48.110278\n16.569722\n\n\n1123\n53.421333\n-6.270075\n47.464722\n8.549167\n\n\n1128\n25.252778\n55.364444\n31.143378\n121.805214\n\n\n1129\n25.252778\n55.364444\n1.350189\n103.994433\n\n\n1130\n25.252778\n55.364444\n25.077731\n121.232822\n\n\n1131\n25.252778\n55.364444\n48.110278\n16.569722\n\n\n1132\n25.252778\n55.364444\n43.677223\n-79.630556\n\n\n1133\n25.252778\n55.364444\n47.464722\n8.549167\n\n\n1134\n-37.673333\n144.843333\n1.350189\n103.994433\n\n\n1136\n8.977889\n38.799319\n51.477500\n-0.461389\n\n\n1138\n8.977889\n38.799319\n31.143378\n121.805214\n\n\n1139\n-34.945000\n138.530556\n1.350189\n103.994433\n\n\n1140\n-37.008056\n174.791667\n1.350189\n103.994433\n\n\n1143\n-43.489358\n172.532225\n1.350189\n103.994433\n\n\n1146\n50.026421\n8.543125\n47.464722\n8.549167\n\n\n1157\n24.432972\n54.651138\n31.143378\n121.805214\n\n\n1160\n44.818444\n20.309139\n51.477500\n-0.461389\n\n\n1165\n43.809953\n11.205100\n47.464722\n8.549167\n\n\n1166\n46.238064\n6.108950\n47.464722\n8.549167\n\n\n1167\n51.432447\n12.241633\n47.464722\n8.549167\n\n\n1168\n45.200761\n7.649631\n47.464722\n8.549167\n\n\n1170\n42.695194\n23.406167\n48.110278\n16.569722\n\n\n1171\n42.695194\n23.406167\n47.464722\n8.549167\n\n\n1172\n13.912583\n100.606750\n1.350189\n103.994433\n\n\n1173\n13.912583\n100.606750\n34.447119\n108.751592\n\n\n1175\n8.095969\n98.988764\n1.350189\n103.994433\n\n\n1176\n31.780019\n117.298436\n25.077731\n121.232822\n\n\n1177\n38.481944\n106.009167\n25.077731\n121.232822\n\n\n1180\n63.985000\n-22.605556\n51.477500\n-0.461389\n\n\n1181\n63.985000\n-22.605556\n43.677223\n-79.630556\n\n\n1182\n63.985000\n-22.605556\n47.464722\n8.549167\n\n\n1189\n39.850000\n110.033000\n34.447119\n108.751592\n\n\n1198\n41.101400\n121.062000\n31.143378\n121.805214\n\n\n1203\n31.428100\n104.741000\n31.143378\n121.805214\n\n\n1204\n35.179528\n128.938222\n31.143378\n121.805214\n\n\n1208\n4.567972\n101.092194\n1.350189\n103.994433\n\n\n1209\n3.775389\n103.209056\n1.350189\n103.994433\n\n\n1210\n35.417000\n116.533000\n34.447119\n108.751592\n\n\n1211\n-6.900625\n107.576294\n1.350189\n103.994433\n\n\n1213\n-1.268272\n116.894478\n1.350189\n103.994433\n\n\n1214\n-6.125567\n106.655897\n40.080111\n116.584556\n\n\n1215\n-6.125567\n106.655897\n31.143378\n121.805214\n\n\n1216\n-6.125567\n106.655897\n1.350189\n103.994433\n\n\n1219\n-8.748169\n115.167172\n1.350189\n103.994433\n\n\n1221\n40.976922\n28.814606\n1.350189\n103.994433\n\n\n1222\n-7.788181\n110.431758\n1.350189\n103.994433\n\n\n1223\n-8.757322\n116.276675\n1.350189\n103.994433\n\n\n1225\n1.549447\n124.925878\n1.350189\n103.994433\n\n\n1226\n0.460786\n101.444539\n1.350189\n103.994433\n\n\n1227\n-2.898250\n104.699903\n1.350189\n103.994433\n\n\n1231\n33.511306\n126.493028\n25.077731\n121.232822\n\n\n1232\n18.766847\n98.962644\n25.077731\n121.232822\n\n\n1239\n43.041000\n144.193000\n25.077731\n121.232822\n\n\n1244\n13.410666\n103.812840\n25.077731\n121.232822\n\n\n1247\n30.582200\n117.050200\n40.080111\n116.584556\n\n\n1249\n28.918900\n111.640000\n34.447119\n108.751592\n\n\n1252\n40.060300\n113.482000\n34.447119\n108.751592\n\n\n1254\n32.900000\n115.816667\n34.447119\n108.751592\n\n\n1255\n40.851422\n111.824103\n34.447119\n108.751592\n\n\n1257\n33.777200\n119.147800\n34.447119\n108.751592\n\n\n1258\n42.841400\n93.669200\n34.447119\n108.751592\n\n\n1259\n35.799700\n107.603000\n34.447119\n108.751592\n\n\n1264\n28.852200\n105.393000\n34.447119\n108.751592\n\n\n1265\n30.754000\n106.062000\n34.447119\n108.751592\n\n\n1267\n40.926389\n107.738889\n34.447119\n108.751592\n\n\n1274\n36.898731\n30.800461\n48.110278\n16.569722\n\n\n1280\n55.408611\n37.906111\n48.110278\n16.569722\n\n\n1287\n27.178317\n33.799436\n48.110278\n16.569722\n\n\n1302\n25.557111\n34.583711\n48.110278\n16.569722\n\n\n1306\n32.011389\n34.886667\n48.110278\n16.569722\n\n\n1308\n40.560000\n109.997000\n31.143378\n121.805214\n\n\n1309\n21.539400\n109.294000\n31.143378\n121.805214\n\n\n1318\n37.271600\n118.281900\n31.143378\n121.805214\n\n\n1319\n25.935064\n119.663272\n31.143378\n121.805214\n\n\n1329\n26.195814\n127.645869\n31.143378\n121.805214\n\n\n1334\n43.352072\n77.040508\n40.080111\n116.584556\n\n\n1339\n50.901389\n4.484444\n40.080111\n116.584556\n\n\n1354\n37.271600\n118.281900\n40.080111\n116.584556\n\n\n1355\n36.636900\n109.554000\n40.080111\n116.584556\n\n\n1374\n48.528044\n135.188361\n40.080111\n116.584556\n\n\n1375\n56.180000\n92.475000\n40.080111\n116.584556\n\n\n1381\n59.800292\n30.262503\n40.080111\n116.584556\n\n\n1384\n36.117000\n103.617000\n25.077731\n121.232822\n\n\n1387\n4.191833\n73.529128\n40.080111\n116.584556\n\n\n1392\n49.566667\n117.329444\n40.080111\n116.584556\n\n\n1395\n55.012622\n82.650656\n40.080111\n116.584556\n\n\n1408\n39.794444\n106.799444\n34.447119\n108.751592\n\n\n1410\n51.956944\n4.437222\n48.110278\n16.569722\n\n\n1424\n41.297078\n2.078464\n47.464722\n8.549167\n\n\n1426\n43.301097\n-2.910608\n51.477500\n-0.461389\n\n\n1450\n43.302061\n-8.377256\n51.477500\n-0.461389\n\n\n1467\n35.416111\n51.152222\n51.477500\n-0.461389\n\n\n1468\n35.416111\n51.152222\n40.080111\n116.584556\n\n\n1469\n35.416111\n51.152222\n48.110278\n16.569722\n\n\n1471\n40.467500\n50.046667\n40.080111\n116.584556\n\n\n1472\n40.467500\n50.046667\n48.110278\n16.569722\n\n\n1473\n5.765280\n103.007000\n1.350189\n103.994433\n\n\n1474\n44.941444\n17.297501\n47.464722\n8.549167\n\n\n1476\n29.102800\n110.443000\n34.447119\n108.751592\n\n\n1478\n19.934856\n110.458961\n34.447119\n108.751592\n\n\n1484\n45.306110\n130.996670\n40.080111\n116.584556\n\n\n1490\n38.280686\n114.697300\n34.447119\n108.751592\n\n\n1495\n29.733300\n118.256000\n34.447119\n108.751592\n\n\n1500\n42.775200\n141.692283\n31.143378\n121.805214\n\n\n1501\n34.796111\n138.189444\n31.143378\n121.805214\n\n\n1504\n34.436111\n132.919444\n31.143378\n121.805214\n\n\n1508\n35.552258\n139.779694\n1.350189\n103.994433\n\n\n1509\n37.571100\n139.064600\n31.143378\n121.805214\n\n\n1513\n36.394611\n136.406544\n31.143378\n121.805214\n\n\n1514\n31.803397\n130.719408\n31.143378\n121.805214\n\n\n1515\n33.827222\n132.699722\n31.143378\n121.805214\n\n\n1518\n32.916944\n129.913611\n31.143378\n121.805214\n\n\n1525\n34.756944\n133.855278\n31.143378\n121.805214\n\n\n1527\n46.223686\n14.457611\n48.110278\n16.569722\n\n\n1528\n46.223686\n14.457611\n47.464722\n8.549167\n\n\n1536\n44.818444\n20.309139\n48.110278\n16.569722\n\n\n1537\n44.818444\n20.309139\n47.464722\n8.549167\n\n\n1539\n54.377569\n18.466222\n47.464722\n8.549167\n\n\n1540\n50.077731\n19.784836\n47.464722\n8.549167\n\n\n1541\n51.102683\n16.885836\n47.464722\n8.549167\n\n\n1542\n11.546556\n104.844139\n31.143378\n121.805214\n\n\n1550\n51.022222\n71.466944\n48.110278\n16.569722\n\n\n1551\n33.511306\n126.493028\n40.080111\n116.584556\n\n\n1559\n37.469075\n126.450517\n48.110278\n16.569722\n\n\n1560\n37.469075\n126.450517\n34.447119\n108.751592\n\n\n1561\n37.469075\n126.450517\n43.677223\n-79.630556\n\n\n1568\n52.308613\n4.763889\n1.350189\n103.994433\n\n\n1570\n52.308613\n4.763889\n48.110278\n16.569722\n\n\n1572\n52.308613\n4.763889\n47.464722\n8.549167\n\n\n1588\n35.857497\n14.477500\n47.464722\n8.549167\n\n\n1593\n19.292778\n-81.357750\n41.978603\n-87.904842\n\n\n1597\n49.626575\n6.211517\n48.110278\n16.569722\n\n\n1612\n50.026421\n8.543125\n41.978603\n-87.904842\n\n\n1615\n50.026421\n8.543125\n1.350189\n103.994433\n\n\n1619\n52.461056\n9.685078\n47.464722\n8.549167\n\n\n1625\n51.432447\n12.241633\n48.110278\n16.569722\n\n\n1628\n48.353783\n11.786086\n41.978603\n-87.904842\n\n\n1633\n48.353783\n11.786086\n47.464722\n8.549167\n\n\n1634\n-1.319167\n36.927500\n47.464722\n8.549167\n\n\n1635\n49.498700\n11.066897\n47.464722\n8.549167\n\n\n1637\n48.689878\n9.221964\n47.464722\n8.549167\n\n\n1643\n50.077731\n19.784836\n48.110278\n16.569722\n\n\n1645\n52.165750\n20.967122\n47.464722\n8.549167\n\n\n1646\n53.353744\n-2.274950\n48.110278\n16.569722\n\n\n1648\n4.191833\n73.529128\n31.143378\n121.805214\n\n\n1650\n36.674900\n-4.499106\n47.464722\n8.549167\n\n\n1652\n59.651944\n17.918611\n47.464722\n8.549167\n\n\n1653\n37.936358\n23.944467\n47.464722\n8.549167\n\n\n1657\n52.453856\n-1.748028\n47.464722\n8.549167\n\n\n1658\n13.681108\n100.747283\n47.464722\n8.549167\n\n\n1660\n42.364347\n-71.005181\n47.464722\n8.549167\n\n\n1661\n50.901389\n4.484444\n47.464722\n8.549167\n\n\n1662\n47.590000\n7.529167\n48.110278\n16.569722\n\n\n1663\n47.436933\n19.255592\n47.464722\n8.549167\n\n\n1664\n30.121944\n31.405556\n47.464722\n8.549167\n\n\n1667\n55.617917\n12.655972\n47.464722\n8.549167\n\n\n1669\n42.561353\n18.268244\n47.464722\n8.549167\n\n\n1671\n55.408611\n37.906111\n47.464722\n8.549167\n\n\n1675\n40.692500\n-74.168667\n47.464722\n8.549167\n\n\n1681\n-23.432075\n-46.469511\n47.464722\n8.549167\n\n\n1683\n46.238064\n6.108950\n48.110278\n16.569722\n\n\n1687\n22.989153\n-82.409086\n47.464722\n8.549167\n\n\n1689\n22.308919\n113.914603\n47.464722\n8.549167\n\n\n1691\n38.944533\n-77.455811\n47.464722\n8.549167\n\n\n1692\n40.976922\n28.814606\n47.464722\n8.549167\n\n\n1694\n-26.139166\n28.246000\n47.464722\n8.549167\n\n\n1695\n50.345000\n30.894722\n47.464722\n8.549167\n\n\n1696\n36.080056\n-115.152250\n47.464722\n8.549167\n\n\n1697\n33.942536\n-118.408075\n47.464722\n8.549167\n\n\n1700\n59.800292\n30.262503\n47.464722\n8.549167\n\n\n1704\n46.004275\n8.910578\n47.464722\n8.549167\n\n\n1705\n49.626575\n6.211517\n47.464722\n8.549167\n\n\n1706\n45.726387\n5.090833\n47.464722\n8.549167\n\n\n1708\n53.353744\n-2.274950\n47.464722\n8.549167\n\n\n1709\n25.793250\n-80.290556\n47.464722\n8.549167\n\n\n1712\n45.630606\n8.728111\n47.464722\n8.549167\n\n\n1714\n43.658411\n7.215872\n47.464722\n8.549167\n\n\n1715\n35.764722\n140.386389\n47.464722\n8.549167\n\n\n1718\n41.978603\n-87.904842\n47.464722\n8.549167\n\n\n1719\n60.193917\n11.100361\n47.464722\n8.549167\n\n\n1720\n44.572161\n26.102178\n47.464722\n8.549167\n\n\n1721\n40.080111\n116.584556\n47.464722\n8.549167\n\n\n1723\n50.100833\n14.260000\n47.464722\n8.549167\n\n\n1725\n18.567367\n-68.363431\n47.464722\n8.549167\n\n\n1726\n31.143378\n121.805214\n47.464722\n8.549167\n\n\n1727\n31.606886\n-8.036300\n47.464722\n8.549167\n\n\n1729\n37.618972\n-122.374889\n47.464722\n8.549167\n\n\n1730\n1.350189\n103.994433\n47.464722\n8.549167\n\n\n1732\n43.538944\n16.297964\n47.464722\n8.549167\n\n\n1735\n32.011389\n34.886667\n47.464722\n8.549167\n\n\n1736\n27.975472\n-82.533250\n47.464722\n8.549167\n\n\n1738\n45.505278\n12.351944\n47.464722\n8.549167\n\n\n1740\n39.489314\n-0.481625\n47.464722\n8.549167\n\n\n1745\n45.742931\n16.068778\n47.464722\n8.549167\n\n\n1766\n24.796400\n118.590000\n25.077731\n121.232822\n\n\n1794\n12.949986\n77.668206\n1.350189\n103.994433\n\n\n1798\n11.030031\n77.043383\n1.350189\n103.994433\n\n\n1799\n29.719217\n106.641678\n1.350189\n103.994433\n\n\n1800\n18.766847\n98.962644\n1.350189\n103.994433\n\n\n1801\n10.155556\n76.391389\n1.350189\n103.994433\n\n\n1802\n28.189158\n113.219633\n1.350189\n103.994433\n\n\n1804\n16.043917\n108.199370\n1.350189\n103.994433\n\n\n1805\n-8.546553\n125.524719\n1.350189\n103.994433\n\n\n1807\n7.125522\n125.645778\n1.350189\n103.994433\n\n\n1808\n21.221192\n105.807178\n1.350189\n103.994433\n\n\n1810\n17.453117\n78.467586\n1.350189\n103.994433\n\n\n1813\n24.992364\n102.743536\n1.350189\n103.994433\n\n\n1814\n27.696583\n85.359100\n1.350189\n103.994433\n\n\n1828\n-20.430235\n57.683600\n40.080111\n116.584556\n\n\n1829\n-20.430235\n57.683600\n31.143378\n121.805214\n\n\n1834\n30.121944\n31.405556\n48.110278\n16.569722\n\n\n1835\n30.121944\n31.405556\n43.677223\n-79.630556\n\n\n1838\n25.088200\n104.958700\n31.143378\n121.805214\n\n\n1852\n43.541200\n125.120100\n34.447119\n108.751592\n\n\n1864\n31.941667\n119.711667\n34.447119\n108.751592\n\n\n1865\n40.060300\n113.482000\n31.143378\n121.805214\n\n\n1866\n31.300000\n107.500000\n31.143378\n121.805214\n\n\n1872\n-8.748169\n115.167172\n31.143378\n121.805214\n\n\n1884\n34.633000\n98.867000\n34.447119\n108.751592\n\n\n1886\n21.221192\n105.807178\n31.143378\n121.805214\n\n\n1887\n36.524000\n114.430000\n31.143378\n121.805214\n\n\n1895\n33.777200\n119.147800\n40.080111\n116.584556\n\n\n1900\n42.841400\n93.669200\n31.143378\n121.805214\n\n\n1902\n21.318681\n-157.922428\n31.143378\n121.805214\n\n\n1906\n29.934200\n122.362000\n31.143378\n121.805214\n\n\n1914\n39.856900\n98.341400\n34.447119\n108.751592\n\n\n1915\n26.899700\n114.737500\n34.447119\n108.751592\n\n\n1916\n29.515000\n108.830000\n40.080111\n116.584556\n\n\n1917\n35.417000\n116.533000\n40.080111\n116.584556\n\n\n1921\n32.857000\n103.683000\n34.447119\n108.751592\n\n\n1922\n39.542922\n76.019956\n34.447119\n108.751592\n\n\n1948\n34.410000\n112.280000\n40.080111\n116.584556\n\n\n1949\n34.550000\n119.250000\n40.080111\n116.584556\n\n\n1950\n35.046100\n118.412000\n40.080111\n116.584556\n\n\n1951\n35.046100\n118.412000\n31.143378\n121.805214\n\n\n1952\n24.207500\n109.391000\n31.143378\n121.805214\n\n\n1953\n28.852200\n105.393000\n40.080111\n116.584556\n\n\n1954\n28.852200\n105.393000\n31.143378\n121.805214\n\n\n1960\n34.991389\n126.382778\n31.143378\n121.805214\n\n\n1962\n30.754000\n106.062000\n31.143378\n121.805214\n\n\n1965\n29.826683\n121.461906\n34.447119\n108.751592\n\n\n1969\n31.742042\n118.862025\n31.143378\n121.805214\n\n\n1972\n22.608267\n108.172442\n1.350189\n103.994433\n\n\n2010\n33.585942\n130.450686\n1.350189\n103.994433\n\n\n2018\n34.434722\n135.244167\n1.350189\n103.994433\n\n\n2023\n34.858414\n136.805408\n1.350189\n103.994433\n\n\n2029\n35.764722\n140.386389\n48.110278\n16.569722\n\n\n2036\n39.457583\n-74.577167\n41.978603\n-87.904842\n\n\n2047\n33.679750\n-78.928333\n41.978603\n-87.904842\n\n\n2048\n37.721278\n-122.220722\n41.978603\n-87.904842\n\n\n2050\n31.428100\n104.741000\n34.447119\n108.751592\n\n\n2074\n44.535444\n11.288667\n48.110278\n16.569722\n\n\n2075\n50.901389\n4.484444\n48.110278\n16.569722\n\n\n2077\n47.436933\n19.255592\n48.110278\n16.569722\n\n\n2082\n46.785167\n23.686167\n48.110278\n16.569722\n\n\n2085\n42.561353\n18.268244\n48.110278\n16.569722\n\n\n2087\n36.713056\n28.792500\n48.110278\n16.569722\n\n\n2089\n48.357222\n35.100556\n48.110278\n16.569722\n\n\n2092\n36.237611\n43.963158\n48.110278\n16.569722\n\n\n2093\n40.147275\n44.395881\n48.110278\n16.569722\n\n\n2098\n46.991067\n15.439628\n48.110278\n16.569722\n\n\n2105\n49.924786\n36.289986\n48.110278\n16.569722\n\n\n2106\n38.944533\n-77.455811\n48.110278\n16.569722\n\n\n2107\n47.178492\n27.620631\n48.110278\n16.569722\n\n\n2109\n47.260219\n11.343964\n48.110278\n16.569722\n\n\n2110\n40.976922\n28.814606\n48.110278\n16.569722\n\n\n2111\n40.639751\n-73.778925\n48.110278\n16.569722\n\n\n2113\n50.345000\n30.894722\n48.110278\n16.569722\n\n\n2114\n46.927744\n28.930978\n48.110278\n16.569722\n\n\n2115\n46.642514\n14.337739\n48.110278\n16.569722\n\n\n2117\n45.034689\n39.170539\n48.110278\n16.569722\n\n\n2118\n48.663055\n21.241112\n48.110278\n16.569722\n\n\n2120\n59.800292\n30.262503\n48.110278\n16.569722\n\n\n2123\n38.781311\n-9.135919\n48.110278\n16.569722\n\n\n2125\n48.233219\n14.187511\n48.110278\n16.569722\n\n\n2128\n49.812500\n23.956111\n48.110278\n16.569722\n\n\n2134\n40.886033\n14.290781\n48.110278\n16.569722\n\n\n2137\n46.426767\n30.676464\n48.110278\n16.569722\n\n\n2139\n41.978603\n-87.904842\n48.110278\n16.569722\n\n\n2141\n44.572161\n26.102178\n48.110278\n16.569722\n\n\n2144\n38.175958\n13.091019\n48.110278\n16.569722\n\n\n2145\n50.100833\n14.260000\n48.110278\n16.569722\n\n\n2146\n42.572778\n21.035833\n48.110278\n16.569722\n\n\n2149\n47.258208\n39.818089\n48.110278\n16.569722\n\n\n2150\n45.785597\n24.091342\n48.110278\n16.569722\n\n\n2151\n43.824583\n18.331467\n48.110278\n16.569722\n\n\n2153\n41.961622\n21.621381\n48.110278\n16.569722\n\n\n2155\n43.538944\n16.297964\n48.110278\n16.569722\n\n\n2157\n38.905394\n16.242269\n48.110278\n16.569722\n\n\n2161\n42.359392\n19.251894\n48.110278\n16.569722\n\n\n2162\n41.414742\n19.720561\n48.110278\n16.569722\n\n\n2166\n43.232072\n27.825106\n48.110278\n16.569722\n\n\n2167\n45.505278\n12.351944\n48.110278\n16.569722\n\n\n2187\n-26.139166\n28.246000\n1.350189\n103.994433\n\n\n2191\n40.128082\n32.995083\n48.110278\n16.569722\n\n\n2192\n40.898553\n29.309219\n48.110278\n16.569722\n\n\n2193\n40.898553\n29.309219\n47.464722\n8.549167\n\n\n2194\n47.485033\n9.560775\n48.110278\n16.569722\n\n\n2197\n33.616653\n73.099233\n51.477500\n-0.461389\n\n\n2199\n33.616653\n73.099233\n43.677223\n-79.630556\n\n\n2200\n24.906547\n67.160797\n51.477500\n-0.461389\n\n\n2201\n24.906547\n67.160797\n43.677223\n-79.630556\n\n\n2202\n31.521564\n74.403594\n51.477500\n-0.461389\n\n\n2203\n31.521564\n74.403594\n43.677223\n-79.630556\n\n\n2206\n11.679431\n122.376294\n25.077731\n121.232822\n\n\n2217\n-9.443383\n147.220050\n1.350189\n103.994433\n\n\n2232\n25.261125\n51.565056\n51.477500\n-0.461389\n\n\n2234\n25.261125\n51.565056\n40.080111\n116.584556\n\n\n2235\n25.261125\n51.565056\n31.143378\n121.805214\n\n\n2236\n25.261125\n51.565056\n1.350189\n103.994433\n\n\n2237\n25.261125\n51.565056\n48.110278\n16.569722\n\n\n2238\n25.261125\n51.565056\n47.464722\n8.549167\n\n\n2251\n31.722556\n35.993214\n47.464722\n8.549167\n\n\n2262\n38.781311\n-9.135919\n43.677223\n-79.630556\n\n\n2263\n41.248055\n-8.681389\n43.677223\n-79.630556\n\n\n2264\n37.741184\n-25.697870\n43.677223\n-79.630556\n\n\n2266\n52.268028\n104.388975\n40.080111\n116.584556\n\n\n2283\n27.701900\n118.001000\n34.447119\n108.751592\n\n\n2284\n49.207947\n-2.195508\n47.464722\n8.549167\n\n\n2286\n59.651944\n17.918611\n41.978603\n-87.904842\n\n\n2291\n55.617917\n12.655972\n41.978603\n-87.904842\n\n\n2293\n55.617917\n12.655972\n31.143378\n121.805214\n\n\n2294\n55.617917\n12.655972\n1.350189\n103.994433\n\n\n2300\n50.901389\n4.484444\n41.978603\n-87.904842\n\n\n2306\n23.077242\n72.634650\n1.350189\n103.994433\n\n\n2308\n41.297078\n2.078464\n1.350189\n103.994433\n\n\n2334\n55.408611\n37.906111\n1.350189\n103.994433\n\n\n2339\n41.804475\n12.250797\n1.350189\n103.994433\n\n\n2363\n4.191833\n73.529128\n1.350189\n103.994433\n\n\n2365\n48.353783\n11.786086\n1.350189\n103.994433\n\n\n2366\n45.630606\n8.728111\n1.350189\n103.994433\n\n\n2378\n24.957640\n46.698776\n1.350189\n103.994433\n\n\n2382\n53.047500\n8.786667\n48.110278\n16.569722\n\n\n2385\n55.972642\n37.414589\n48.110278\n16.569722\n\n\n2386\n55.972642\n37.414589\n43.677223\n-79.630556\n\n\n2387\n55.972642\n37.414589\n47.464722\n8.549167\n\n\n2389\n21.679564\n39.156536\n1.350189\n103.994433\n\n\n2390\n21.679564\n39.156536\n43.677223\n-79.630556\n\n\n2392\n46.914100\n7.497153\n48.110278\n16.569722\n\n\n2393\n37.986814\n58.360967\n51.477500\n-0.461389\n\n\n2408\n40.976922\n28.814606\n41.978603\n-87.904842\n\n\n2436\n6.933206\n100.392975\n1.350189\n103.994433\n\n\n2445\n26.883333\n100.233330\n1.350189\n103.994433\n\n\n2447\n22.149556\n113.591558\n1.350189\n103.994433\n\n\n2450\n29.826683\n121.461906\n1.350189\n103.994433\n\n\n2458\n37.936358\n23.944467\n43.677223\n-79.630556\n\n\n2465\n55.871944\n-4.433056\n43.677223\n-79.630556\n\n\n2467\n51.148056\n-0.190278\n43.677223\n-79.630556\n\n\n2469\n53.353744\n-2.274950\n43.677223\n-79.630556\n\n\n2474\n45.505278\n12.351944\n43.677223\n-79.630556\n\n\n2477\n33.875031\n10.775461\n47.464722\n8.549167\n\n\n2478\n36.075833\n10.438611\n47.464722\n8.549167\n\n\n2479\n36.851033\n10.227217\n48.110278\n16.569722\n\n\n2480\n36.851033\n10.227217\n47.464722\n8.549167\n\n\n2485\n31.742042\n118.862025\n1.350189\n103.994433\n\n\n2487\n-28.164444\n153.504722\n1.350189\n103.994433\n\n\n2490\n51.148056\n-0.190278\n48.110278\n16.569722\n\n\n2491\n51.148056\n-0.190278\n47.464722\n8.549167\n\n\n2492\n51.874722\n-0.368333\n47.464722\n8.549167\n\n\n2493\n40.652083\n-75.440806\n41.978603\n-87.904842\n\n\n2496\n42.748267\n-73.801692\n41.978603\n-87.904842\n\n\n2501\n44.257526\n-88.507576\n41.978603\n-87.904842\n\n\n2503\n35.436194\n-82.541806\n41.978603\n-87.904842\n\n\n2504\n41.338478\n-75.723403\n41.978603\n-87.904842\n\n\n2507\n33.562942\n-86.753550\n41.978603\n-87.904842\n\n\n2509\n43.564361\n-116.222861\n41.978603\n-87.904842\n\n\n2514\n44.471861\n-73.153278\n41.978603\n-87.904842\n\n\n2517\n45.777643\n-111.160151\n41.978603\n-87.904842\n\n\n2518\n33.938833\n-81.119528\n41.978603\n-87.904842\n\n\n2519\n40.916083\n-81.442194\n41.978603\n-87.904842\n\n\n2521\n32.898647\n-80.040528\n41.978603\n-87.904842\n\n\n2528\n47.168400\n-88.489100\n41.978603\n-87.904842\n\n\n2529\n38.805805\n-104.700778\n41.978603\n-87.904842\n\n\n2539\n46.842091\n-92.193649\n41.978603\n-87.904842\n\n\n2546\n44.865800\n-91.484300\n41.978603\n-87.904842\n\n\n2547\n42.159889\n-76.891611\n41.978603\n-87.904842\n\n\n2557\n38.950944\n-95.663611\n41.978603\n-87.904842\n\n\n2563\n-23.432075\n-46.469511\n41.978603\n-87.904842\n\n\n2564\n36.097750\n-79.937306\n41.978603\n-87.904842\n\n\n2565\n34.895556\n-82.218889\n41.978603\n-87.904842\n\n\n2568\n21.318681\n-157.922428\n41.978603\n-87.904842\n\n\n2573\n38.944533\n-77.455811\n41.978603\n-87.904842\n\n\n2584\n32.311167\n-90.075889\n41.978603\n-87.904842\n\n\n2589\n42.778700\n-84.587357\n41.978603\n-87.904842\n\n\n2603\n40.850971\n-96.759250\n41.978603\n-87.904842\n\n\n2604\n43.532913\n-84.079647\n41.978603\n-87.904842\n\n\n2610\n42.932556\n-71.435667\n41.978603\n-87.904842\n\n\n2614\n43.169500\n-86.238200\n41.978603\n-87.904842\n\n\n2616\n30.691231\n-88.242814\n41.978603\n-87.904842\n\n\n2622\n25.778489\n-100.106878\n41.978603\n-87.904842\n\n\n2625\n25.038958\n-77.466231\n41.978603\n-87.904842\n\n\n2629\n20.898650\n-156.430458\n41.978603\n-87.904842\n\n\n2644\n7.180756\n79.884117\n51.477500\n-0.461389\n\n\n2645\n7.180756\n79.884117\n40.080111\n116.584556\n\n\n2646\n7.180756\n79.884117\n31.143378\n121.805214\n\n\n2648\n6.284467\n81.124128\n40.080111\n116.584556\n\n\n2649\n6.284467\n81.124128\n31.143378\n121.805214\n\n\n2650\n23.593278\n58.284444\n47.464722\n8.549167\n\n\n2659\n55.591531\n37.261486\n43.677223\n-79.630556\n\n\n2783\n16.043917\n108.199370\n31.143378\n121.805214\n\n\n2817\n53.047500\n8.786667\n47.464722\n8.549167\n\n\n2818\n52.134642\n7.684831\n47.464722\n8.549167\n\n\n2819\n35.416111\n51.152222\n31.143378\n121.805214\n\n\n2843\n33.679750\n-78.928333\n43.677223\n-79.630556\n\n\n2851\n18.439417\n-66.001833\n43.677223\n-79.630556\n\n\n2853\n18.040953\n-63.108900\n43.677223\n-79.630556\n\n\n2860\n49.956112\n-119.377778\n43.677223\n-79.630556\n\n\n2877\n38.292392\n27.156953\n48.110278\n16.569722\n\n\n2878\n38.292392\n27.156953\n47.464722\n8.549167\n\n\n2885\n42.359392\n19.251894\n47.464722\n8.549167\n\n\n\n\n\n\n\n\n# Get a nice image of the globe from NASA\nearth &lt;- \"http://eoimages.gsfc.nasa.gov/images/imagerecords/73000/73909/world.topo.bathy.200412.3x5400x2700.jpg\"\n\nglobejs(\n  img = earth,\n  lat = frequent_flights$dest_lat,\n  long = frequent_flights$dest_long,\n  arcs = frequent_flights,\n  #value = frequent_destinations$n,\n  color = \"red\",\n  #bodycolor = \"#aaaaff\",\n  arcsHeight = 0.3,\n  arcsLwd = 2,\n  arcsColor = \"#ffff00\",\n  arcsOpacity = 0.35,\n  atmosphere = FALSE,\n  #color=\"#00aaff\",\n  pointsize = 2,\n  rotationlat = 0.6,\n  rotationlong = 0.2,\n  # lightcolor = \"#aaeeff\",\n  # emissive = \"#0000ee\",\n  # bodycolor = \"#ffffff\",\n  bg = \"grey\"\n        )",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-8: Did you ever see such a thing as a drawing of a muchness?"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/80-htmlwidgets/htmlwidgets.html#using-scatterplot3js-and-friends",
    "href": "content/courses/R4Artists/Modules/80-htmlwidgets/htmlwidgets.html#using-scatterplot3js-and-friends",
    "title": "Lab-8: Did you ever see such a thing as a drawing of a muchness?",
    "section": "Using scatterplot3js and friends",
    "text": "Using scatterplot3js and friends\n3D scatter plots with points and lines can be achieved using scatterplot3js, points3D, and lines3D.\n\nlibrary(palmerpenguins)\npenguins &lt;- penguins %&gt;% drop_na()\nscatterplot3js(x = penguins$bill_length_mm, \n               y = penguins$flipper_length_mm, \n               z = penguins$body_mass_g)\n\n\n\n\n\n\n\nlibrary(palmerpenguins)\npenguins &lt;- penguins %&gt;% drop_na()\nscatterplot3js(x = penguins$bill_length_mm, \n               y = penguins$flipper_length_mm, \n               z = penguins$body_mass_g,\n               cex.symbols = 0.2) # Smaller Points\n\n\n\n\n\n\n\nlibrary(palmerpenguins)\npenguins &lt;- penguins %&gt;% drop_na()\n\nscatterplot3js(x = penguins$bill_length_mm, \n               y = penguins$flipper_length_mm, \n               z = penguins$body_mass_g,\n               cex.symbols = 0.2) # Smaller Points",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-8: Did you ever see such a thing as a drawing of a muchness?"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/80-htmlwidgets/htmlwidgets.html#references",
    "href": "content/courses/R4Artists/Modules/80-htmlwidgets/htmlwidgets.html#references",
    "title": "Lab-8: Did you ever see such a thing as a drawing of a muchness?",
    "section": "References",
    "text": "References\n\nBring the best of JavaScript data visualization to R, https://www.htmlwidgets.org/\nUsing htmlwidgets in Rmarkdown, https://communicate-data-with-r.netlify.app/docs/communicate/htmlwidgets-in-documents/\nKarambelkar et al, htmlwidgets and knitr , https://cran.r-project.org/web/packages/widgetframe/vignettes/widgetframe_and_knitr.html\nThe patchwork package to combine plots. https://patchwork.data-imaginist.com/\nThe threejs package: three.js widgets for R https://bwlewis.github.io/rthreejs/",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-8: Did you ever see such a thing as a drawing of a muchness?"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/120-time/index.html#introduction",
    "href": "content/courses/R4Artists/Modules/120-time/index.html#introduction",
    "title": "Lab-12: Time is a Him!!",
    "section": "Introduction",
    "text": "Introduction\nTime Series data are important in data visualization where events have a temporal dimension, such as with finance, transportation, music, telecommunications for example.\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(ggformula)\n##########################################\n# Install core TimeSeries Packages\n# library(ctv)\n# ctv::install.views(\"TimeSeries\", coreOnly = TRUE)\n# To update core TimeSeries packages\n# ctv::update.views(\"TimeSeries\")\n# Time Series Core Packages\n##########################################\nlibrary(tsibble)\nlibrary(feasts) # Feature Extraction and Statistics for Time Series\nlibrary(fable) # Forecasting Models for Tidy Time Series\nlibrary(tseries) # Time Series Analysis and Computational Finance\nlibrary(forecast)\nlibrary(zoo)\n##########################################\nlibrary(tsibbledata) # Time Series Demo Datasets",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-12: Time is a Him!!"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/120-time/index.html#introduction-to-time-series-data-formats",
    "href": "content/courses/R4Artists/Modules/120-time/index.html#introduction-to-time-series-data-formats",
    "title": "Lab-12: Time is a Him!!",
    "section": "Introduction to Time Series: Data Formats",
    "text": "Introduction to Time Series: Data Formats\nThere are multiple formats for time series data.\n\nThe base ts format: The stats::ts() function will convert a numeric vector into an R time series object. The format is ts(vector, start=, end=, frequency=) where start and end are the times of the first and last observation and frequency is the number of observations per unit time (1=annual, 4=quarterly, 12=monthly, etc.). Used by established packages like forecast\n\nTibble format: the simplest is of course the standard tibble / dataframe, with a time variable to indicate that the other variables vary with time. Used by more recent packages such as timetk & modeltime\n\nThe modern tsibble (time series tibble) format: this is a new format for time series analysis, and is used by the tidyverts set of packages (fable, feasts and others).\nThere is also a tsbox package from ROpenScience that allows easy inter-conversion between these ( and other! ) formats!",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-12: Time is a Him!!"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/120-time/index.html#creating-time-series",
    "href": "content/courses/R4Artists/Modules/120-time/index.html#creating-time-series",
    "title": "Lab-12: Time is a Him!!",
    "section": "Creating time series",
    "text": "Creating time series\nIn this first example, we will use simple ts data, and then do another with a tibble dataset, and then a third example with an tsibble formatted dataset.\n\nts format data\nThere are a few datasets in base R that are in ts format already.\n\nAirPassengers\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1949 112 118 132 129 121 135 148 148 136 119 104 118\n1950 115 126 141 135 125 149 170 170 158 133 114 140\n1951 145 150 178 163 172 178 199 199 184 162 146 166\n1952 171 180 193 181 183 218 230 242 209 191 172 194\n1953 196 196 236 235 229 243 264 272 237 211 180 201\n1954 204 188 235 227 234 264 302 293 259 229 203 229\n1955 242 233 267 269 270 315 364 347 312 274 237 278\n1956 284 277 317 313 318 374 413 405 355 306 271 306\n1957 315 301 356 348 355 422 465 467 404 347 305 336\n1958 340 318 362 348 363 435 491 505 404 359 310 337\n1959 360 342 406 396 420 472 548 559 463 407 362 405\n1960 417 391 419 461 472 535 622 606 508 461 390 432\n\nstr(AirPassengers)\n\n Time-Series [1:144] from 1949 to 1961: 112 118 132 129 121 135 148 148 136 119 ...\n\n\nThis can be easily plotted using base R:\n\nplot(AirPassengers)\n\n\n\n\n\n\n\nLet us take data that is “time oriented” but not in ts format, and convert it to ts: the syntax of ts() is:\nSyntax: objectName &lt;- ts(data, start, end, frequency) where  - data represents the data vector - start represents the first observation in time series\n- end represents the last observation in time series\n- frequency represents number of observations per unit time. For example, frequency=1 for monthly data.\nWe will pick simple numerical vector data ( i.e. not a timeseries ) ChickWeight:\n\nChickWeight %&gt;% head()\n\n\n  \n\n\nChickWeight_ts &lt;- ts(ChickWeight$weight, frequency = 2)\nplot(ChickWeight_ts)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe ts format\n\n\n\nThe ts format is not recommended for new analysis since it does not permit inclusion of multiple time series in one dataset, nor other categorical variables for grouping etc.\n\n\n\ntibble format data\nSome “time-oriented” datasets are available in tibble form. Let us try to plot one, the walmart_sales_weekly dataset from the timetk package:\n\ndata(walmart_sales_weekly, package = \"timetk\")\nwalmart_sales_weekly\n\n\n  \n\n\n\nThis dataset is a tibble with a Date column. The Dept column is clearly a categorical column that allows us to distinguish separate time series, i.e. one for each value of Dept. We will convert that to a factor( it is an double precision number ) and then plot the data using this column on the Date on the \\(x\\)-axis:\n\nwalmart_sales_weekly %&gt;% \n  \n  # convert Dept number to a **categorical factor**\n  mutate(Dept = forcats::as_factor(Dept)) %&gt;% \n  \n  gf_point(Weekly_Sales ~ Date, \n           group = ~ Dept, \n           colour = ~ Dept, data = .) %&gt;% \n  gf_line() %&gt;% \n  gf_theme(theme_minimal())\n\n\n\n\n\n\n\nFor more analysis and forecasting etc., it is useful to convert this tibble into a tsibble:\n\nwalmart_tsibble &lt;- as_tsibble(walmart_sales_weekly,\n                         index = Date,\n                         key = c(id, Dept))\nwalmart_tsibble\n\n\n  \n\n\n\nThe 7D states the data is weekly. There is a Date column and all the other numerical variables are time-varying quantities. The categorical variables such as id, and Dept allow us to identify separate time series in the data, and these have 7 combinations hence are 7 time series in this data, as indicated.\n\n\nLet us plot Weekly_Sales, colouring the time series by Dept:\n\nwalmart_tsibble %&gt;% \n  gf_line(Weekly_Sales ~ Date, \n          colour = ~ as_factor(Dept), data = .) %&gt;% \n  gf_point() %&gt;%\n  gf_theme(theme_minimal()) %&gt;% \n  labs(title = \"Weekly Sales by Dept at Walmart\")\n\n[[1]]\n\n\n\n$title\n[1] \"Weekly Sales by Dept at Walmart\"\n\nattr(,\"class\")\n[1] \"labels\"\n\n\n\n\n\n\n\nFigure 1: Walmart Time Series\n\n\n\n\n\nWe can also do a quick autoplot that seems to offer less control and is also not interactive.\n\nwalmart_tsibble %&gt;% \n  dplyr::group_by(Dept) %&gt;% \n  autoplot(Weekly_Sales) %&gt;% \n  gf_theme(theme_minimal())\n\n\n\n\n\n\n\n\n\ntsibble format data\nIn the packages tsibbledata and fpp3 we have a good choice of tsibble format data. Let us pick one:\n\nhh_budget\n\n\n  \n\n\n\nThere are 4 keys ( id variables ) here, one for each country. Six other quantitative columns are the individual series. Let us plot (some of) the timeseries:\nggplot2::theme_set(theme_classic())\nhh_budget %&gt;% \n  gf_path(Debt ~ Year, colour = ~ Country,\n          title = \"Debt over Time\")\n##\nhh_budget %&gt;% \n  gf_path(Savings ~ Year, colour = ~ Country,\n          title = \"Savings over Time\")\n##\nhh_budget %&gt;% \n  gf_path(Expenditure ~ Year, colour = ~ Country,\n          title = \"Expenditure over Time\")\n##\nhh_budget %&gt;% \n  gf_path(Wealth ~ Year, colour = ~ Country,\n          title = \"Wealth over Time\")",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-12: Time is a Him!!"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/120-time/index.html#one-more-example",
    "href": "content/courses/R4Artists/Modules/120-time/index.html#one-more-example",
    "title": "Lab-12: Time is a Him!!",
    "section": "One more example",
    "text": "One more example\nOften we have data in table form, that is time-oriented, with a date like column, and we need to convert it into a tsibble for analysis:\n\nprison &lt;- readr::read_csv(\"https://OTexts.com/fpp3/extrafiles/prison_population.csv\")\nglimpse(prison)\n\nRows: 3,072\nColumns: 6\n$ Date       &lt;date&gt; 2005-03-01, 2005-03-01, 2005-03-01, 2005-03-01, 2005-03-01…\n$ State      &lt;chr&gt; \"ACT\", \"ACT\", \"ACT\", \"ACT\", \"ACT\", \"ACT\", \"ACT\", \"ACT\", \"NS…\n$ Gender     &lt;chr&gt; \"Female\", \"Female\", \"Female\", \"Female\", \"Male\", \"Male\", \"Ma…\n$ Legal      &lt;chr&gt; \"Remanded\", \"Remanded\", \"Sentenced\", \"Sentenced\", \"Remanded…\n$ Indigenous &lt;chr&gt; \"ATSI\", \"Non-ATSI\", \"ATSI\", \"Non-ATSI\", \"ATSI\", \"Non-ATSI\",…\n$ Count      &lt;dbl&gt; 0, 2, 0, 5, 7, 58, 5, 101, 51, 131, 145, 323, 355, 1617, 12…\n\n\nWe have a Date column for the time index, and we have unique key variables like State, Gender, Legal and Indigenous. Count is the value that is variable over time. It also appears that the data is quarterly, since mosaic::inspect reports the max_diff in the Date column as \\(92\\). .Run mosaic::inspect(prison) in your Console\n\nprison_tsibble &lt;- prison %&gt;% \n  mutate(quarter = yearquarter(Date)) %&gt;% \n  select(-Date) %&gt;% # Remove the Date column now that we have quarters\n  as_tsibble(index = quarter, key = c(State, Gender, Legal, Indigenous))\n\nprison_tsibble\n\n\n  \n\n\n\n(Here, ATSI stands for Aboriginal or Torres Strait Islander.). We have \\(64\\) time series here, organized quarterly.\nLet us examine the key variables:\n\nprison_tsibble %&gt;% distinct(Indigenous)\n\n\n  \n\n\nprison_tsibble %&gt;% distinct(State)\n\n\n  \n\n\n\nSo we can plot the time series, faceted / coloured by State:\n\nprison_tsibble %&gt;% \n  tsibble::index_by() %&gt;% \n  group_by(Indigenous, State) %&gt;% \n  #filter(State == \"NSW\") %&gt;% \n  summarise(Total = sum(Count))  %&gt;%\n  gf_point(Total ~quarter, colour = ~ Indigenous, \n             shape = ~ Indigenous) %&gt;% \n  gf_line() %&gt;% \n  \n  # Note that the y-axes are all. different!!\n  gf_facet_wrap(vars(State), scale = \"free_y\") %&gt;% \n  \n  gf_theme(theme_minimal())\n\n\n\n\n\n\n\nHmm…looks like New South Wales(NSW) as something different going on compared to the rest of the states in Aus. Because of the large cities there…",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-12: Time is a Him!!"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/120-time/index.html#decomposing-time-series",
    "href": "content/courses/R4Artists/Modules/120-time/index.html#decomposing-time-series",
    "title": "Lab-12: Time is a Him!!",
    "section": "Decomposing Time Series",
    "text": "Decomposing Time Series\nWe can decompose the Weekly_Sales into components representing trends, seasonal events that repeat, and irregular noise. Since each Dept could have a different set of trends, we will do this first for one Dept, say Dept #95:\n\nwalmart_decomposed_season &lt;- walmart_tsibble %&gt;% \n  dplyr::filter(Dept == \"95\") %&gt;% # filter for Dept 95\n  #\n  # feasts depends upon fabletools.\n  # \n  fabletools::model(\n    season = STL(Weekly_Sales ~ season(window = \"periodic\"))) \nwalmart_decomposed_season %&gt;% fabletools::components()\n\n\n  \n\n\n###\nwalmart_decomposed_ets &lt;- walmart_tsibble %&gt;% \n  dplyr::filter(Dept == \"95\") %&gt;% # filter for Dept 95\n  #\n  # feasts depends upon fabletools.\n  # \n  fabletools::model(\n    ets = ETS(box_cox(Weekly_Sales, 0.3)))\n###\nwalmart_decomposed_ets %&gt;% fabletools::components()\n\n\n  \n\n\n###\nwalmart_decomposed_arima &lt;- walmart_tsibble %&gt;%\n  dplyr::filter(Dept == \"95\") %&gt;% # filter for Dept 95\n  fabletools::model(arima = ARIMA(log(Weekly_Sales)))\nwalmart_decomposed_arima %&gt;% broom::tidy()\n\n\n  \n\n\n\n\nwalmart_decomposed_season %&gt;% \n  components() %&gt;% \n  autoplot() + \n  labs( title = \"Seasonal Variations in Weekly Sales, Dept #95\")\n\n\n\n\n\n\nwalmart_decomposed_ets %&gt;% \n  components() %&gt;% \n  autoplot() + \n  labs( title = \"ETS Variations in Weekly Sales, Dept #95\")",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-12: Time is a Him!!"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/120-time/index.html#conclusion",
    "href": "content/courses/R4Artists/Modules/120-time/index.html#conclusion",
    "title": "Lab-12: Time is a Him!!",
    "section": "Conclusion",
    "text": "Conclusion\nTBW",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-12: Time is a Him!!"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/120-time/index.html#references",
    "href": "content/courses/R4Artists/Modules/120-time/index.html#references",
    "title": "Lab-12: Time is a Him!!",
    "section": "References",
    "text": "References\n\nForecasting: Principles and Practice (3rd ed) Rob J Hyndman and George Athanasopoulos",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-12: Time is a Him!!"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/10-Basics/index.html#where-does-data-come-from",
    "href": "content/courses/R4Artists/Modules/10-Basics/index.html#where-does-data-come-from",
    "title": "🕶 Lab-1: Science, Human Experience, Experiments, and Data",
    "section": "\n Where does Data come from?",
    "text": "Where does Data come from?\nWe will need to form a basic understanding of basic scientific enterprise. Let us look at the slides.",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "🕶 Lab-1: Science, Human Experience, Experiments, and Data"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/10-Basics/index.html#why-visualize",
    "href": "content/courses/R4Artists/Modules/10-Basics/index.html#why-visualize",
    "title": "🕶 Lab-1: Science, Human Experience, Experiments, and Data",
    "section": "\n Why Visualize?",
    "text": "Why Visualize?\n\nWe can digest information more easily when it is pictorial\nOur Working Memories are both short-term and limited in capacity. So a picture abstracts the details and presents us with an overall summary, an insight, or a story that is both easy to recall and easy on retention.\n\nData Viz includes shapes that carry strong cultural memories and impressions for us. These cultural memories help us to use data viz in a universal way to appeal to a wide variety of audiences. (Do humans have a gene for geometry?)\nIt helps sift facts and mere statements: for example:\n\n\n\nRape Capital\n\n\n\nWhat does Data Reveal?",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "🕶 Lab-1: Science, Human Experience, Experiments, and Data"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/10-Basics/index.html#what-are-data-types",
    "href": "content/courses/R4Artists/Modules/10-Basics/index.html#what-are-data-types",
    "title": "🕶 Lab-1: Science, Human Experience, Experiments, and Data",
    "section": "\n What are Data Types?",
    "text": "What are Data Types?\n\n\n\nIn more detail:",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "🕶 Lab-1: Science, Human Experience, Experiments, and Data"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/10-Basics/index.html#sec-data-types",
    "href": "content/courses/R4Artists/Modules/10-Basics/index.html#sec-data-types",
    "title": "🕶 Lab-1: Science, Human Experience, Experiments, and Data",
    "section": "\n How do we Spot Data Variable Types?",
    "text": "How do we Spot Data Variable Types?\nBy asking questions!\n\n\n\n\n\n\nNo\nPronoun\nAnswer\nVariable/Scale\nExample\nWhat Operations?\n\n\n\n1\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities, with Scale and a Zero Value.Differences and Ratios /Products are meaningful.\nQuantitative/Ratio\nLength,Height,Temperature in Kelvin,Activity,Dose Amount,Reaction Rate,Flow Rate,Concentration,Pulse,Survival Rate\nCorrelation\n\n\n2\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities with Scale. Differences are meaningful, but not products or ratios\nQuantitative/Interval\npH,SAT score(200-800),Credit score(300-850),SAT score(200-800),Year of Starting College\nMean,Standard Deviation\n\n\n3\nHow, What Kind, What Sort\nA Manner / Method, Type or Attribute from a list, with list items in some \" order\" ( e.g. good, better, improved, best..)\nQualitative/Ordinal\nSocioeconomic status (Low income, Middle income, High income),Education level (HighSchool, BS, MS, PhD),Satisfaction rating(Very much Dislike, Dislike, Neutral, Like, Very Much Like)\nMedian,Percentile\n\n\n4\nWhat, Who, Where, Whom, Which\nName, Place, Animal, Thing\nQualitative/Nominal\nName\nCount no. of cases,Mode\n\n\n\n\n\n\n\nAs you go from Qualitative to Quantitative data types in the table, I hope you can detect a movement from fuzzy groups/categories to more and more crystallized numbers.\n\n\nType of Variables\n\nEach variable/scale can be subjected to the operations of the previous group. In the words of S.S. Stevens\n\nthe basic operations needed to create each type of scale is cumulative: to an operation listed opposite a particular scale must be added all those operations preceding it.",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "🕶 Lab-1: Science, Human Experience, Experiments, and Data"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/10-Basics/index.html#sec-data-viz",
    "href": "content/courses/R4Artists/Modules/10-Basics/index.html#sec-data-viz",
    "title": "🕶 Lab-1: Science, Human Experience, Experiments, and Data",
    "section": "What Are the Parts of a Data Viz?",
    "text": "What Are the Parts of a Data Viz?",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "🕶 Lab-1: Science, Human Experience, Experiments, and Data"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/10-Basics/index.html#how-to-pick-a-data-viz",
    "href": "content/courses/R4Artists/Modules/10-Basics/index.html#how-to-pick-a-data-viz",
    "title": "🕶 Lab-1: Science, Human Experience, Experiments, and Data",
    "section": "How to pick a Data Viz?",
    "text": "How to pick a Data Viz?\nMost Data Visualizations use one or more of the following geometric attributes or aesthetics. These geometric aesthetics are used to represent qualitative or quantitative variables from your data.\n\n\nCommon Geometric Aesthetics in Charts\n\nWhat does this mean? We can think of simple visualizations as combinations of these aesthetics. Some examples:\n\n\nGeometries , Combinations, and Graphs\n\n\n\n\n\n\nAesthetic #1\nAesthetic #2\nShape\n\n\n\nPosition X = Quant Variable\nPosition Y = Quant Variable\nP oints/Circles with Fixed Size\n\n\nPosition X = Qual Variable\nPosition Y = Count of Qual Variable\nColumns\n\n\nPosition X = Qual Variable\nPosition Y = Qual Variable\nRectangles, with area proportional to joint(X,Y) count\n\n\n\nPosition X = Qual Variable\nPosition Y = Rank-Ordered Quant Variable\nBox + Whisker, Box length proportional to I nter-Quartile Range, w hisker-length proportional to upper and lower quartile resp.\n\n\nPosition X = Quant Variable\nPosition Y = Quant Variable + Qual Variable\n\nLine and Area\nColour for Area",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "🕶 Lab-1: Science, Human Experience, Experiments, and Data"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/90-GoM/spatial.html",
    "href": "content/courses/R4Artists/Modules/90-GoM/spatial.html",
    "title": "Lab-9: If you please sir…which way to the Secret Garden?",
    "section": "",
    "text": "Alice asks the Catterpillar the Way",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-9: If you please sir...which way to the Secret Garden?"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/90-GoM/spatial.html#slides-and-tutorials",
    "href": "content/courses/R4Artists/Modules/90-GoM/spatial.html#slides-and-tutorials",
    "title": "Lab-9: If you please sir…which way to the Secret Garden?",
    "section": "\n Slides and Tutorials",
    "text": "Slides and Tutorials\n\n\n\n\n\n\n\n\n R Tutorial\n Slides \n Leaflet Tutorial\n MapviewTutorial",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-9: If you please sir...which way to the Secret Garden?"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/90-GoM/spatial.html#introduction",
    "href": "content/courses/R4Artists/Modules/90-GoM/spatial.html#introduction",
    "title": "Lab-9: If you please sir…which way to the Secret Garden?",
    "section": "\n Introduction",
    "text": "Introduction\nChoropleth Map\nWhat does choropleth1 mean? And what kind of information could this map represent?\n\n\nBubble Map\nWhat information could this map below represent?\n\n\nWhat is there to not like about maps!!! Let us now look at the slides. Then we will understand how the R packages sf, tmap work to create maps, using data downloadable into R using osmdata, osmplotr and rnaturalearth.\nWe will learn to make static and interactive maps and to show off different kinds of data on them, data that have an inherently “spatial” spread or significance! sf + ggplot and tmap give us great static maps. Interactive maps we will make with leaflet and mapview; tmap is also capable of creating interactive maps.\nTrade Routes? Populations? Street Crime hotspots? Theatre and Food districts and popular Restaurants? Literary Paris, London and Barcelona?\nAll possible !!",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-9: If you please sir...which way to the Secret Garden?"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/90-GoM/spatial.html#embedded-tutorials",
    "href": "content/courses/R4Artists/Modules/90-GoM/spatial.html#embedded-tutorials",
    "title": "Lab-9: If you please sir…which way to the Secret Garden?",
    "section": "Embedded Tutorials",
    "text": "Embedded Tutorials\n\nknitr::include_url(\"../../../../labs/r-labs/maps/gram-maps.qmd\")\n\n\n\n\nknitr::include_url(\"../../../../slides/r-slides/spatial/Spatial-Data-in-R.html\")",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-9: If you please sir...which way to the Secret Garden?"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/90-GoM/spatial.html#references",
    "href": "content/courses/R4Artists/Modules/90-GoM/spatial.html#references",
    "title": "Lab-9: If you please sir…which way to the Secret Garden?",
    "section": "References",
    "text": "References\n\nOSM Basic Maps Vignette\nNikita Voevodin, R, Not the Best Practices\nNico Hahn, Making Maps with R\nEmine Fidan, Interactive Maps in R\nLovelace et al, Geocomputation in R",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-9: If you please sir...which way to the Secret Garden?"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/90-GoM/spatial.html#footnotes",
    "href": "content/courses/R4Artists/Modules/90-GoM/spatial.html#footnotes",
    "title": "Lab-9: If you please sir…which way to the Secret Garden?",
    "section": "Footnotes",
    "text": "Footnotes\n\nEtymology. From Ancient Greek χώρα (khṓra, “location”) + πλῆθος (plêthos, “a great number”) + English map. First proposed in 1938 by American geographer John Kirtland Wright to mean “quantity in area,” although maps of the type have been used since the early 19th century.↩︎",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-9: If you please sir...which way to the Secret Garden?"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/30-quarto/index.html",
    "href": "content/courses/R4Artists/Modules/30-quarto/index.html",
    "title": "Lab-3: Drink Me!",
    "section": "",
    "text": "We will get acquainted with the RMarkdown Quarto Document format, which allows us to mix text narrative, code, code-developed figures and items from the web in a seamless document.Then we can try to create something in the lines of what Hans Rosling did.\nSomething that can:\n\nprovide a visualization\nprovide insight\ntell a story\nis reproducible",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-3: Drink Me!"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/30-quarto/index.html#introduction",
    "href": "content/courses/R4Artists/Modules/30-quarto/index.html#introduction",
    "title": "Lab-3: Drink Me!",
    "section": "",
    "text": "We will get acquainted with the RMarkdown Quarto Document format, which allows us to mix text narrative, code, code-developed figures and items from the web in a seamless document.Then we can try to create something in the lines of what Hans Rosling did.\nSomething that can:\n\nprovide a visualization\nprovide insight\ntell a story\nis reproducible",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-3: Drink Me!"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/30-quarto/index.html#setting-up-quarto",
    "href": "content/courses/R4Artists/Modules/30-quarto/index.html#setting-up-quarto",
    "title": "Lab-3: Drink Me!",
    "section": "Setting up Quarto",
    "text": "Setting up Quarto\nQuarto is installed along with RStudio. We can check if all is in order by running a check in the Terminal in RStudio. \nThe commands are:\n\nquarto check install\nquarto check knitr\n\nIf these come out with no errors then we are ready to fire up our first Quarto document.",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-3: Drink Me!"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/30-quarto/index.html#practice",
    "href": "content/courses/R4Artists/Modules/30-quarto/index.html#practice",
    "title": "Lab-3: Drink Me!",
    "section": "Practice",
    "text": "Practice\nFire up a new Quarto document by going to: File -&gt; New File -&gt; Quarto Document.Switch to Visual mode, if it is not already there.\nUse the visual mode tool bar.\nTry to create Sections, code chunks, embedding images and tables.\nHit the Render button to see how the documents is converted into an html document.",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-3: Drink Me!"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/30-quarto/index.html#references",
    "href": "content/courses/R4Artists/Modules/30-quarto/index.html#references",
    "title": "Lab-3: Drink Me!",
    "section": "References",
    "text": "References\n\nhttps://www.markdowntutorial.com\nhttps://quarto.org/docs/authoring/markdown-basics.html\nhttps://rmarkdown.rstudio.com/index.html\nhttps://ysc-rmarkdown.netlify.app/slides/01-basics.html Nice RMarkdown presentation and “code movies” !",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-3: Drink Me!"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/30-quarto/index.html#assignments",
    "href": "content/courses/R4Artists/Modules/30-quarto/index.html#assignments",
    "title": "Lab-3: Drink Me!",
    "section": "Assignment(s)",
    "text": "Assignment(s)\n\nComplete the markdown tutorial in\nCreate a fresh Quarto document and use as many as possible of the RMarkdown constructs from the Cheatsheet [reference 1]",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-3: Drink Me!"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/30-quarto/index.html#fun-stuff",
    "href": "content/courses/R4Artists/Modules/30-quarto/index.html#fun-stuff",
    "title": "Lab-3: Drink Me!",
    "section": "Fun Stuff",
    "text": "Fun Stuff\n\nhttps://rmarkdown.rstudio.com/lesson-1.html\nDesirée De Leon, Alison Hill: rstudio4edu: A Handbook for Teaching and Learning with R and RStudio, https://rstudio4edu.github.io/rstudio4edu-book/",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-3: Drink Me!"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "",
    "text": "This Quarto document is part of my Course, R for Artists and Managers. The material is based on A Layered Grammar of Graphics by Hadley Wickham.\nThe intent of this Tutorial is to build Skill in coding in R, and also appreciate R as a way to metaphorically visualize information of various kinds, using predominantly geometric figures and structures.\nAll RMarkdown files combine code, text, web-images, and figures developed using code. Everything is text; code chunks are enclosed in fences (```)"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#setting-up-r-packages",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#setting-up-r-packages",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\n## packages\nlibrary(tidyverse)   ## data science package collection (incl. the ggplot2 package)\nlibrary(systemfonts) ## use custom fonts (need to be installed on your OS)  \nlibrary(paletteer)   ## scico  and many other colour palettes palettes(http://www.fabiocrameri.ch/colourmaps.php) in R \nlibrary(ggtext)      ## add improved text rendering to ggplot2\nlibrary(ggforce)     ## add missing functionality to ggplot2\nlibrary(ggdist)      ## add uncertainty visualizations to ggplot2\nlibrary(ggformula)   ## Formula interface to ggplot\nlibrary(magick)      ## load images into R\nlibrary(patchwork)   ## combine outputs from ggplot2\nlibrary(palmerpenguins)\n\nlibrary(showtext)   ## add google fonts to plots\n\nknitr::opts_chunk$set(\n  error = TRUE,\n  comment = NA,\n  warning = FALSE,\n  errors = FALSE,\n  message = FALSE,\n  tidy = FALSE,\n  cache = FALSE,\n  echo = TRUE,\n  warning = FALSE,\n# from the vignette for the showtext package\n  fig.showtext = TRUE,\n  fig.retina = 1,\n\n  fig.width = 9,\n  fig.height = 8,\n  fig.path = \"06a-figs/\"\n)"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#using-google-fonts",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#using-google-fonts",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "Using Google Fonts",
    "text": "Using Google Fonts\nWe will want to add a few new fonts to our graphs. The best way (currently) is to use the showtext package ( which we loaded above) to bring into our work fonts from Google. To view and select the fonts you might want to work with, spend some time looking over:\n\nGoogle Webfonts Helper App\nGoogle Fonts\n\n\nsysfonts::font_add_google(\"Gochi Hand\", \"gochi\")\nfont_add_google(\"Schoolbell\", \"bell\")\nfont_add_google(\"Galada\", \"galada\")\nfont_add_google(\"Schoolbell\", \"bell\")\nfont_add_google(\"Roboto\", \"roboto\")\nfont_add_google(\"Noto Sans\", \"noto\")\nfont_add_google(\"Uchen\", \"uchen\")\nfont_add_google(\"Ibarra Real Nova\", \"ibarra\")\nfont_add_google(\"Open Sans\", \"open\")\nfont_add_google(\"Anton\", \"anton\")\nfont_add_google(\"Tangerine\", \"tangerine\")\n\n # set the google fonts as default\nshowtext::showtext_auto()\n\nWe will work with a familiar dataset, so that we can concentrate on the chart aesthetics, without having to spend time getting used to the data: the penguins dataset again, from the palmerpenguins package.\n\n\n\n\n\n\nggformula and ggplot worlds\n\n\n\nIt seems we can mix `ggformula` code with `ggtext` code, using the `+` sign!! What joy !!! Need to find out if this works for other `ggplot` extensions as well !!!"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#data",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#data",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "Data",
    "text": "Data\nAlways start your work with a table of the data:\n\npenguins &lt;- penguins %&gt;% drop_na() # remove data containing missing data\npenguins"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#basic-plot",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#basic-plot",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "Basic Plot",
    "text": "Basic Plot\nA basic scatter plot, which we will progressively dress up.\n\n\nUsing ggformula\nUsing ggplot\n\n\n\n\n## simple plot: data + mappings + geometry\n## no colour or fill yet\ngf &lt;-  gf_point(bill_depth_mm ~ bill_length_mm, \n                 data = penguins, \n                 alpha = 0.6, size = 3.5)\ngf\n\n\n\n\n\n\n\n\n\n\n## simple plot: data + mappings + geometry\n## no colour or fill yet\ngg &lt;- ggplot(penguins, aes(x = bill_length_mm,\n                     y = bill_depth_mm)) +\n  geom_point(alpha = .6, size = 3.5)\ngg"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#customized-plot",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#customized-plot",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "Customized Plot",
    "text": "Customized Plot\nLet us set some ggplot theme aspects now!! Here is a handy picture showing (most of) the theme-able aspects of a ggplot plot.\n\n\nRosana Ferrero (@RosanaFerrero) on Twitter Sept 11, 2022\n\nFor more info, type ?theme in your console.\n\n## change global theme settings (for all following plots)\ntheme_set(theme_minimal(base_size = 12, base_family = \"open\"))\n\n## modify plot elements globally (for all following plots)\ntheme_update(\n  axis.ticks = element_line(color = \"grey92\"),\n  axis.ticks.length = unit(.5, \"lines\"),\n  panel.grid.minor = element_blank(),\n  legend.title = element_text(size = 12),\n  legend.text = element_text(color = \"grey30\"),\n  plot.title = element_text(size = 18, face = \"bold\"),\n  plot.subtitle = element_text(size = 12, color = \"grey30\"),\n  plot.caption = element_text(size = 9, margin = margin(t = 15))\n)\n\nSince we know what the basic plot looks like, let’s add titles, labels and colours. We will also set limits and scales.\n\n\nUsing ggformula\nUsing ggplot\n\n\n\n\ngf1 &lt;- penguins %&gt;% \n  gf_point(bill_depth_mm ~ bill_length_mm, \n           \n           # colour by continuous variable\n           color =  ~ body_mass_g, \n           alpha = .6, size = 3.5) %&gt;% \n\n  \n  ## custom axes scaling\n  gf_refine(\n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)),\n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5), \n                     limits = c(12.5, 22.5)), \n  \n  ## custom colors from the scico package\n  ## using the paletteer super package\n  paletteer:: scale_color_paletteer_c(`\"scico::bamako\"`, direction = -1),\n  \n  ## custom labels\n  labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (Pygoscelis)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) PLoS ONE',\n    x = 'Bill Length (mm)', \n    y = 'Bill Depth (mm)',\n    color = 'Body mass (g)' # Neat Way of naming a scale and legend\n  ))\n\ngf1\n\n\n\n\n\n\n\n\n\n\ngg1 &lt;- penguins %&gt;% \n  ggplot(aes(y = bill_depth_mm, x = bill_length_mm), alpha = .6, \n         size = 3.5) +\n  geom_point(aes(colour = body_mass_g)) + \n\n  \n  ## custom axes scaling\n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) + \n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5), \n                     limits = c(12.5, 22.5)) + \n  \n  ## custom colors from the scico package\n  paletteer:: scale_color_paletteer_c(`\"scico::bamako\"`, \n                                      direction = -1) + \n  \n  ## custom labels\n  labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (Pygoscelis)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) PLoS ONE',\n    x = 'Bill Length (mm)', \n    y = 'Bill Depth (mm)',\n    color = 'Body mass (g)' \n  )\ngg1"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#using-element_markdown",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#using-element_markdown",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "Using element_markdown()\n",
    "text": "Using element_markdown()\n\n\n\nUsing ggformula\nUsing ggplot\n\n\n\nWe can use our familiar markdown syntax right inside the titles and captions of the plot. element_markdown() is a theme-ing command made available by the ggtext package.\nelement_markdown() → formatted text elements, e.g. titles, caption, axis text, striptext\n\ngf2 &lt;- penguins %&gt;% gf_point(bill_depth_mm ~ bill_length_mm, \n                            color = ~ body_mass_g, \n                            alpha = 0.6, size = 3.5) %&gt;% \n gf_refine(\n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)),\n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5), \n                     limits = c(12.5, 22.5)), \n  \n  ## custom colors from the scico package\n  paletteer:: scale_color_paletteer_c(\"scico::bamako\", \n                                      direction = -1),\n  \n  ## custom labels using element_markdown()\n   labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) *PLoS ONE*',\n    x = '**Bill Length** (mm)', \n    y = '**Bill Depth** (mm)',\n    color = 'Body mass (g)')\n  ) %&gt;% \n  \n  # New code from here\n  # Enables markdown titles, captions and labels\n  gf_theme(theme(\n    plot.title = ggtext::element_markdown(),\n    plot.caption = ggtext::element_markdown(),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown()\n  ))\n\n gf2\n\n\n\n\n\n\n\n\n\n\ngg2 &lt;- ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5) + \n  \n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5), \n                     limits = c(12.5, 22.5)) +\n   paletteer:: scale_color_paletteer_c(`\"scico::bamako\"`, direction = -1) +\n   \n  ## New code starts here: Two Step Procedure with ggtext\n  ## 1. Markdown formatting of labels and title, using asterisks\n  labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) *PLoS ONE*',\n    x = '**Bill Length** (mm)', \n    y = '**Bill Depth** (mm)',\n    color = 'Body mass (g)'\n  ) +\n   \n  ## 2. Add theme related commands from ggtext\n  ## render respective text elements\n  theme(\n    plot.title = ggtext::element_markdown(),\n    plot.caption = ggtext::element_markdown(),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown()\n  )\ngg2"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#element_markdown-in-combination-with-html",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#element_markdown-in-combination-with-html",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "\nelement_markdown() in combination with HTML",
    "text": "element_markdown() in combination with HTML\nThis allows us to change fonts in titles, labels, and captions.\n\n\nUsing ggformula\nUsing ggplot\n\n\n\n\n## use HTML syntax to change text color\n## \ngf2 %&gt;% \n  \n  # html in labels\n  gf_labs(title = 'Bill Dimensions of Brush-Tailed Penguins\n          &lt;i style = \"color:#28A87D;\"&gt;Pygoscelis &lt;/i&gt;'\n            ) \n\n\n\n\n\n\n## use HTML syntax to change font and text size\ngf2 %&gt;% \n  gf_labs(title = 'Bill Dimensions of Brush-Tailed Penguins &lt;b style=\"font-size:32pt;font-family:tangerine;\"&gt;Pygoscelis&lt;/b&gt;') \n\n\n\n\n\n\n\n\n\n\n## use HTML syntax to change text color\ngg2 +\n  labs(title = 'Bill Dimensions of Brush-Tailed Penguins &lt;i style=\"color:#28A87D;\"&gt;Pygoscelis&lt;/i&gt;') +\n  theme(plot.margin = margin(t = 25))\n\n\n\n\n\n\n## use HTML syntax to change font and text size\ngg2 +\n  labs(title = 'Bill Dimensions of Brush-Tailed Penguins &lt;b style=\"font-size:32pt;font-family:tangerine;\"&gt;Pygoscelis&lt;/b&gt;')"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#adding-images-to-ggplot",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#adding-images-to-ggplot",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "Adding images to ggplot",
    "text": "Adding images to ggplot\nSave an image from the web in the same folder as your RMarkdown. Use html tags to include it, say as part of your plot title, as shown below.\n\n\nUsing ggformula\nUsing ggplot\n\n\n\n\n## use HTML syntax to add images to text elements\ngf2 %&gt;% \n  gf_labs(title = 'Bill Dimensions of Brush-Tailed Penguins &nbsp;&nbsp;&nbsp; &lt;img src=\"../images/culmen_depth.png\"‚ width=\"480\"/&gt;') \n\n\n\n\n\n\n\n\n\n\n## use HTML syntax to add images to text elements\ngg2 + \n  labs(title = 'Bill Dimensions of Brush-Tailed Penguins &nbsp;&nbsp;&nbsp; &lt;img src=\"../images/culmen_depth.png\"‚ width=\"480\"/&gt;')"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#annotations-with-geom_richtext-and-geom_textbox",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#annotations-with-geom_richtext-and-geom_textbox",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "Annotations with geom_richtext() and geom_textbox()\n",
    "text": "Annotations with geom_richtext() and geom_textbox()\n\nFurther ggplot annotations can be achieved using geom_richtext() and geom_textbox(). geom_richtext() also allows formatted text labels with 360° rotation. One needs to pass a dataframe to geom_richtext() giving the location, colour, rotation etc of the label annotation.\n\n\nUsing ggformula\nUsing ggplot\n\n\n\n\n# Create a label tibble\n# Three rich text labels, \n# so three sets of locations x and y, and angle of rotation\nlabels &lt;- tibble(\n      x = c(34, 56, 54), \n      y = c(20, 18.5, 14.5),\n      angle = c(12, 20, 335),\n      species = c(\"Adelie\", \"Chinstrap\", \"Gentoo\"),\n      \n      lab = c(\"&lt;b style='font-family:anton;font-size:24pt;'&gt;Adélie&lt;/b&gt;&lt;br&gt;&lt;i style='color:darkgrey;'&gt;P. adéliae&lt;/i&gt;\", \n              \"&lt;b style='font-family:anton;font-size:24pt;'&gt;Chinstrap&lt;/b&gt;&lt;br&gt;&lt;i style='color:darkgrey;'&gt;P. antarctica&lt;/i&gt;\", \n              \"&lt;b style='font-family:anton;font-size:24pt;'&gt;Gentoo&lt;/b&gt;&lt;br&gt;&lt;i style='color:darkgrey;'&gt;P. papua&lt;/i&gt;\"))\nlabels\n\n\n  \n\n\ngf_rich &lt;- penguins %&gt;% \n  gf_point(bill_depth_mm ~ bill_length_mm, \n                      color = ~ species, \n                      alpha = 0.6, size = 3.5, data = penguins) +\n  \n  \n    ## add text annotations for each species\n    ggtext::geom_richtext(\n    data = labels,\n    # Now pass the data variables as aesthetics\n    aes(x, y, label = lab, color = species, angle = angle), \n    size = 4, fill = NA, label.color = NA,\n    lineheight = .3) +\n    # show.legend = FALSE else we get some unusual legends!\n    # fill = NA makes the labels' fill transparent\n  \n  \n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5), \n                     limits = c(12.5, 22.5)) +\n  scale_colour_paletteer_d(palette = `\"rcartocolor::Bold\"`, guide = \"none\") +\n  \n  labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) *PLoS ONE*',\n    x = '**Bill Length** (mm)', \n    y = '**Bill Depth** (mm)',\n    color = 'Body mass (g)'\n  ) +\n  \n  # Use theme and element_markdown() to format axes and titles as usual\n  theme(\n    plot.title = ggtext::element_markdown(),\n    plot.caption = ggtext::element_markdown(),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown(),\n    plot.margin = margin(25, 6, 15, 6)\n  )\n\n\ngf_rich\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote the plus sign usage here!!We are combining the ggformula and ggplot syntax, and it works!\n\n\n\n\n\ngg_rich &lt;- ggplot(penguins, aes(x = bill_length_mm, \n                                y = bill_depth_mm)) +\n  geom_point(aes(color = species), alpha = .6, size = 3.5) + \n  \n  ## add text annotations for each species\n  ggtext::geom_richtext(\n    data = labels,\n    # Now pass the data variables as aesthetics\n    aes(x, y, label = lab, color = species, angle = angle), \n    size = 4, fill = NA, label.color = NA,\n    lineheight = .3) +\n  \n  \n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5), \n                     limits = c(12.5, 22.5)) +\n  scale_colour_paletteer_d(`\"rcartocolor::Bold\"`, guide = \"none\")+\n  \n  labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) *PLoS ONE*',\n    x = '**Bill Length** (mm)', \n    y = '**Bill Depth** (mm)',\n    color = 'Body mass (g)'\n  ) +\n  \n  # Use theme and element_markdown() to format axes and titles as usual\n  theme(\n    plot.title = ggtext::element_markdown(),\n    plot.caption = ggtext::element_markdown(),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown(),\n    plot.margin = margin(25, 6, 15, 6)\n  )\n  \ngg_rich"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#formatted-text-boxes-on-plots",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#formatted-text-boxes-on-plots",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "Formatted Text boxes on plots",
    "text": "Formatted Text boxes on plots\nelement_textbox() and element_textbox_simple() → formatted text boxes with word wrapping.\n\n\nUsing ggformula\nUsing ggplot\n\n\n\n\ngf_box &lt;- penguins %&gt;% \n  gf_point(bill_depth_mm ~ bill_length_mm, \n                      color = ~ species, \n                      alpha = 0.6, size = 3.5, data = penguins) +\n  \n  \n    ## add text annotations for each species\n    ggtext::geom_richtext(\n    data = labels,\n    # Now pass the data variables as aesthetics\n    aes(x, y, label = lab, color = species, angle = angle), \n    size = 4, fill = NA, label.color = NA,\n    lineheight = .3) +\n    # show.legend = FALSE else we get some unusual legends!\n    # fill = NA makes the labels' fill transparent\n  \n  \n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5), \n                     limits = c(12.5, 22.5)) +\n  scale_colour_paletteer_d(palette = `\"rcartocolor::Bold\"`, guide = \"none\") +\n  \n  \n# Now for the Plot Titles and Labels, as before  \n  labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) *PLoS ONE*',\n    x = '**Bill Length** (mm)',\n    y = '**Bill Depth** (mm)',\n    color = 'Body mass (g)'\n  ) +\n  \n# Add the ggtext theme related commands\n  theme(\n    ## turn title into filled textbox\n    plot.title = ggtext::element_textbox_simple(\n      color = \"white\",\n      fill = \"#28A78D\",\n      size = 32,\n      padding = margin(8, 4, 8, 4),\n      margin = margin(b = 5),\n      lineheight = .9\n    ),\n    plot.subtitle = ggtext::element_textbox_simple(\n      size = 10,\n      padding = margin(5.5, 5.5, 5.5, 5.5),\n      margin = margin(0, 0, 5.5, 0),\n      fill = \"orange1\"\n    ),\n    \n    ## add round outline to caption\n    plot.caption = ggtext::element_textbox_simple(\n      width = NULL,\n      linetype = 1,\n      fill = \"grey\",\n      padding = margin(4, 8, 4, 8),\n      margin = margin(t = 15),\n      r = grid::unit(8, \"pt\")\n    ),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown(),\n    plot.margin = margin(25, 6, 15, 6)\n  )\n\ngf_box\n\n\n\n\n\n\n\nNote again the use of the plus sign with ggformula. This works!\n\n\n\ngg_box &lt;- ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(aes(color = species), alpha = .6, size = 3.5) +\n  \n     ## add text annotations for each species\n    ggtext::geom_richtext(\n    data = labels,\n    # Now pass the data variables as aesthetics\n    aes(x, y, label = lab, color = species, angle = angle), \n    size = 4, fill = NA, label.color = NA,\n    lineheight = .3) +\n    # show.legend = FALSE else we get some unusual legends!\n    # fill = NA makes the labels' fill transparent\n  \n  \n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5), \n                     limits = c(12.5, 22.5)) +\n  scale_colour_paletteer_d(palette = `\"rcartocolor::Bold\"`, guide = \"none\") +\n  \n  \n# Now for the Plot Titles and Labels, as before  \n  labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) *PLoS ONE*',\n    x = '**Bill Length** (mm)',\n    y = '**Bill Depth** (mm)',\n    color = 'Body mass (g)'\n  ) +\n  \n# Add the ggtext theme related commands\n  theme(\n    ## turn title into filled textbox\n    plot.title = ggtext::element_textbox_simple(\n      color = \"white\",\n      fill = \"#28A78D\",\n      size = 32,\n      padding = margin(8, 4, 8, 4),\n      margin = margin(b = 5),\n      lineheight = .9\n    ),\n    plot.subtitle = ggtext::element_textbox_simple(\n      size = 10,\n      padding = margin(5.5, 5.5, 5.5, 5.5),\n      margin = margin(0, 0, 5.5, 0),\n      fill = \"orange1\"\n    ),\n    \n    ## add round outline to caption\n    plot.caption = ggtext::element_textbox_simple(\n      width = NULL,\n      linetype = 1,\n      fill = \"grey\",\n      padding = margin(4, 8, 4, 8),\n      margin = margin(t = 15),\n      r = grid::unit(8, \"pt\")\n    ),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown(),\n    plot.margin = margin(25, 6, 15, 6)\n  )\n\ngg_box"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#using-geom_texbox-for-formatted-text-boxes-with-word-wrapping",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#using-geom_texbox-for-formatted-text-boxes-with-word-wrapping",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "Using geom_texbox() for formatted text boxes with word wrapping",
    "text": "Using geom_texbox() for formatted text boxes with word wrapping\n\n\nUsing ggformula\nUsing ggplot\n\n\n\n\ntext_box &lt;- tibble(x = 34, y = 13.7, label = \"&lt;span style='font-size:12pt;font-family:anton;'&gt;Lorem Ipsum Dolor Sit Amet&lt;/span&gt;&lt;br&gt;&lt;br&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\")\n\n\ngf_box +\n  ## add textbox with long paragraphs\n  ggtext::geom_textbox(\n    data = text_box,\n    aes(x, y, \n        label = label),\n    size = 2.2, family = \"sans\",\n    fill = \"cornsilk\", \n    \n    colour = \"black\",\n# This is ESSENTIAL !!!\n# It appears that the original colour aesthetic mapping in `gf_box` and a possible colour aesthetic with `geom_textbox` have a clash, *only* with ggformula. No such issues below with the ggplot.\n# So declaring a colour here is essential\n\n    box.color = \"cornsilk3\",\n    #box.padding = c(2,2,2,2),\n    width = unit(11, \"lines\")\n  ) +\n  coord_cartesian(clip = \"off\") # ensure no clipping of labels near the edge\n\n\n\n\n\n\n\n\n\n\ngg_box +\n  ## add textbox with long paragraphs\n  ggtext::geom_textbox(\n    data = text_box,\n    aes(x, y, label = label),\n    size = 2.2, family = \"sans\",\n    fill = \"cornsilk\", box.color = \"cornsilk3\",\n    width = unit(11, \"lines\")\n  ) +\n  coord_cartesian(clip = \"off\") # ensure no clipping of labels near the edge"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#using-ggforce",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#using-ggforce",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "Using {ggforce}\n",
    "text": "Using {ggforce}\n\nFrom Thomas Lin Pedersen’s website → www.ggforce.data-imaginist.com\n\nggforce is a package aimed at providing missing functionality to ggplot2 through the extension system introduced with ggplot2 v2.0.0. Broadly speaking ggplot2 has been aimed primarily at explorative data visualization in order to investigate the data at hand, and less at providing utilities for composing custom plots a la D3.js. ggforce is mainly an attempt to address these “shortcoming” (design choices might be a better description). The goal is to provide a repository of geoms, stats, etc. that are as well documented and implemented as the official ones found in ggplot2.\n\nWe will start with the basic plot, with the ggtext related work done up to now:\n\n## use ggtext rendering for the following plots\ntheme_update(\n  plot.title = ggtext::element_markdown(),\n  plot.caption = ggtext::element_markdown(),\n  axis.title.x = ggtext::element_markdown(),\n  axis.title.y = ggtext::element_markdown()\n)\n\n\n\nUsing ggformula and ggforce\nUsing ggplot and ggforce\n\n\n\n\n## plot that we will annotate with ggforce afterwards\ngf3 &lt;- penguins %&gt;% \n  gf_point(bill_depth_mm ~ bill_length_mm,\n           color = ~ body_mass_g,\n           alpha = .6, \n           size = 3.5) + \n  \n  coord_cartesian(xlim = c(25, 65), ylim = c(10, 25)) +\n  \n  # Add Colour scales\n  scale_color_paletteer_c(`\"grDevices::Lajolla\"`, direction = -1) +\n  \n  # Add labels\n  labs(\n    title = \"Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)\",\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE*\",\n    x = \"**Bill Length** (mm)\",\n    y = \"**Bill Depth** (mm)\",\n    color = \"Body mass (g)\",\n    fill = \"Species\"\n  )\n\n\n\n## ellipsoids for all groups\ngf3 +\n  ggforce::geom_mark_ellipse(\n    aes(fill = species, \n        label = species), \n    \n    color = \"black\", \n    # This is good to include\n    # Else ellipses get coloured too\n    \n    alpha = .15, \n    show.legend = FALSE\n  ) \n\n\n\n\n\n\n\n\n\n\n## plot that we will annotate with ggforce afterwards\ngg3 &lt;- ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(aes(color = body_mass_g), \n             alpha = .6, \n             size = 3.5) + \n\n  coord_cartesian(xlim = c(25, 65), ylim = c(10, 25)) +\n  \n  # Add Colour scales\n  scale_color_paletteer_c(`\"grDevices::Lajolla\"`, direction = -1) +\n  # rcartocolor::scale_fill_carto_d(palette = \"Bold\") +\n  \n  # Add labels\n  labs(\n    title = \"Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)\",\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE*\",\n    x = \"**Bill Length** (mm)\",\n    y = \"**Bill Depth** (mm)\",\n    color = \"Body mass (g)\",\n    fill = \"Species\"\n  )\n\n\n## ellipsoids for all groups\ngg3 +\n  ggforce::geom_mark_ellipse(\n    aes(fill = species, \n        label = species), \n    alpha = .15, \n    show.legend = FALSE\n  ) \n\n\n\n\n\n\n\n\n## ellipsoids for specific subset\ngg3 +\n  ggforce::geom_mark_ellipse(\n    aes(fill = species, label = species, filter = species == 'Gentoo'), \n    alpha = 0, show.legend = FALSE\n  ) +\n geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5)\n\n\n\n\n\n\n\n\n## circles\ngg3 +\n  ggforce::geom_mark_circle(\n    aes(fill = species, label = species, filter = species == 'Gentoo'), \n    alpha = 0, show.legend = FALSE\n  ) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5)\n\n\n\n\n\n\n\n\n## rectangles\ngg3 +\n  ggforce::geom_mark_rect(\n    aes(fill = species, label = species, filter = species == 'Gentoo'), \n    alpha = 0, show.legend = FALSE\n  ) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5) \n\n\n\n\n\n\n\n\nlibrary(concaveman)\n## hull\ngg3 +\n  ggforce::geom_mark_hull(\n    aes(fill = species, label = species, filter = species == 'Gentoo'), \n    alpha = 0, show.legend = FALSE\n  ) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5)"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#ggplot-tricks",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#ggplot-tricks",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "ggplot tricks",
    "text": "ggplot tricks\n\ngg0 &lt;- \n  ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n    ggforce::geom_mark_ellipse(\n    aes(fill = species, \n        label = species), \n    alpha = .15, \n    show.legend = FALSE\n  ) +\n    geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5) + \n    scale_x_continuous(breaks = seq(25, 65, by = 5), limits = c(25, 65)) +\n    scale_y_continuous(breaks = seq(12, 24, by = 2), limits = c(12, 24)) +\n    scico::scale_color_scico(palette = \"bamako\", direction = -1) +\n    labs(\n      title = \"Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)\",\n      subtitle = 'A scatter plot of bill depth versus bill length.',\n      caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE*\",\n      x = \"Bill Length (mm)\", \n      y = \"Bill Depth (mm)\",\n      color = \"Body mass (g)\"\n    )\ngg0\n\n\n\n\n\n\n\nLeft-Aligned Title\n\n(gg1 &lt;- gg0 + theme(plot.title.position = \"plot\"))\n\n\n\n\n\n\n\nRight-Aligned Caption\n\ngg1b &lt;- gg1 +  theme(plot.caption.position = \"panel\")\ngg1b\n\n\n\n\n\n\n\nLegend Design\n\ngg1b + theme(legend.position = \"top\")\n\n\n\n\n\n\n#ggsave(\"06a_legend_position.pdf\", width = 9, height = 8, device = cairo_pdf)\n\ngg1b + \n  theme(legend.position = \"top\") +\n  guides(color = guide_colorbar(title.position = \"top\", \n                                title.hjust = .5, \n                                barwidth = unit(20, \"lines\"), \n                                barheight = unit(.5, \"lines\")))\n\n\n\n\n\n\n\nAdd Images\n\n## read PNG file from web\npng &lt;- magick::image_read(\"../images/culmen_depth.png\")\n## turn image into `rasterGrob`\nimg &lt;- grid::rasterGrob(png, interpolate = TRUE)\n\ngg5 &lt;- gg2 + \n  annotation_custom(img, ymin = 18, ymax = 28, xmin = 58, xmax = 65) +\n    labs(caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE* &bull; Illustration: Allison Horst\")\ngg5"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#using-patchwork",
    "href": "content/courses/R4Artists/Modules/70-wizardy/files/07-wizardy.html#using-patchwork",
    "title": "Fonts, Themes, and other Wizardy in ggplot",
    "section": "Using {patchwork}\n",
    "text": "Using {patchwork}\n\n\nThe goal of patchwork is to make it ridiculously simple to combine separate ggplots into the same graphic. As such it tries to solve the same problem as gridExtra::grid.arrange() and cowplot::plot_grid but using an API that incites exploration and iteration, and scales to arbitrily complex layouts.\n\n→ https://patchwork.data-imaginist.com/\nLet us make two plots and combine them into a single patchwork plot.\n\n## calculate bill ratio\npenguins_stats &lt;- penguins %&gt;% \n  mutate(bill_ratio = bill_length_mm / bill_depth_mm) %&gt;% \n  filter(!is.na(bill_ratio))\n\n## create a second chart\ngg6 &lt;- ggplot(penguins_stats, aes(y = bill_ratio, x = species, fill = species, color = species)) + geom_violin() + \n  labs(\n    y = \"Bill ratio\",\n    x = \"Species\",\n    subtitle = \"\",\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE* &bull; Illustration: Allison Horst\"\n  ) +\n  theme(\n    panel.grid.major.x = element_line(size = .35),\n    panel.grid.major.y = element_blank(),\n    axis.text.y = element_text(size = 13),\n    axis.ticks.length = unit(0, \"lines\"),\n    plot.title.position = 'plot',\n    plot.subtitle = element_text(margin = margin(t = 5, b = 10)),\n    plot.margin = margin(10, 25, 10, 25)\n  )\n\nNow to combine both plots into one using simple operators:\n\nFor the special case of putting plots besides each other or on top of each other patchwork provides 2 shortcut operators. | will place plots next to each other while / will place them on top of each other.\n\nFirst we stack up the graphs side by side:\n\n## combine both plots\ngg5 | (gg6 + labs(title = \"Bill Ratios of Brush-Tailed Penguins\",\n                  subtitle = \"Violin Plots of Bill Ration versus species\"))\n\n\n\n\n\n\n\nWe can place them in one column:\n\ngg5 / (gg6 + labs(title = \"Bill Ratios of Brush-Tailed Penguins\",\n                  subtitle = \"Violin Plots of Bill Ration versus species\")) +\n  plot_layout(heights = c(0.4, 0.4))"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/50-working-in-tidyverse/index.html",
    "href": "content/courses/R4Artists/Modules/50-working-in-tidyverse/index.html",
    "title": "Lab-5: Twas brillig, and the slithy toves…",
    "section": "",
    "text": "dplyr Tutorial",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-5: Twas brillig, and the slithy toves..."
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/50-working-in-tidyverse/index.html#slides-and-tutorials",
    "href": "content/courses/R4Artists/Modules/50-working-in-tidyverse/index.html#slides-and-tutorials",
    "title": "Lab-5: Twas brillig, and the slithy toves…",
    "section": "",
    "text": "dplyr Tutorial",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-5: Twas brillig, and the slithy toves..."
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/50-working-in-tidyverse/index.html#introduction",
    "href": "content/courses/R4Artists/Modules/50-working-in-tidyverse/index.html#introduction",
    "title": "Lab-5: Twas brillig, and the slithy toves…",
    "section": " Introduction",
    "text": "Introduction\nWe meet the most important idea in R: tidy data. Once data is tidy, there is a great deal of insight to be obtained from it, by way of tables, graphs and explorations!\nWe will get hands on with dplyr, the R-package that belongs in the tidyverse and is a terrific toolbox to clean, transform, reorder, and summarize your data. And we will be ready to ask Questions of our data and embark on analyzing it.",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-5: Twas brillig, and the slithy toves..."
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/50-working-in-tidyverse/index.html#readings",
    "href": "content/courses/R4Artists/Modules/50-working-in-tidyverse/index.html#readings",
    "title": "Lab-5: Twas brillig, and the slithy toves…",
    "section": "Readings",
    "text": "Readings\n\nR4DS dplyr chapter\nModernDive dplyr chapter\nRStudio dplyr Cheatsheet",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-5: Twas brillig, and the slithy toves..."
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/20-intro/index.html#introduction",
    "href": "content/courses/R4Artists/Modules/20-intro/index.html#introduction",
    "title": "Lab-2: Down the R-abbit Hole…",
    "section": "Introduction",
    "text": "Introduction\nWelcome!\nLet’s start our journey to the Garden of Data Visualization, with this terrific presentation by the great ( and sadly late..) Hans Rosling.\nThe best stats you’ve ever seen by Hans Rosling:\n\nWe will run some boiler-plate R code today! Nothing ( almost! ) to code! We will get used to the tools and words of the trade: R, RStudio, installation, packages, libraries….",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-2: Down the R-abbit Hole..."
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/index.html#sequence-diagram",
    "href": "content/courses/R4Artists/Modules/110-GoD/index.html#sequence-diagram",
    "title": "Lab-11: The Queen of Hearts, She Made some Tarts",
    "section": "Sequence Diagram",
    "text": "Sequence Diagram\nLook at the code below: What do you think it represents?\n\nDiagrammeR(\"\nsequenceDiagram\nArvind -&gt;&gt; Anamika: Why are you late today?\nAnamika -&gt;&gt; Anamika: Ulp...\nAnamika -&gt;&gt; Arvind: I am sorry... &lt;br&gt; may I come in please?\n\nArvind -&gt;&gt; Komal: And you? What kept you?\nKomal -&gt;&gt; Anamika: (Quietly) He's having a bad day, dude...\nAnamika -&gt;&gt; Komal: (Whisper) Boomer...\n\")\n\n\n\n\n\nThis is a simple Sequence Diagram! Shows a strictly imaginary exchange between a pair of students and an unknown Faculty Member.\nLet us now see how we can embellish this kind of diagram. Can we have a Garden of Forking Paths?\n\nDiagrammeR(\"\n  graph LR\n    A--&gt;B\n    A--&gt;C\n    C--&gt;E\n    B--&gt;D\n    C--&gt;D\n    D--&gt;F\n    E--&gt;F\n\")\n\n\n\n\n\n\nDiagrammeR(\"\n        sequenceDiagram\n        \n        alt Anamika is always punctual\n        Arvind -&gt;&gt; Anamika: Why haven't you put up your Daily Reflection?\n        Anamika -&gt;&gt; Anamika: Ulp...\n        Note right of Anamika : I have had it today..\n        Anamika -&gt;&gt; Arvind: I am sorry... \n        Arvind -&gt;&gt; Anamika: Ok write it today\n        \n        else Anamika is usually tardy\n        Arvind -&gt;&gt; Anamika: Why haven't you put up your Daily Reflection?\n        Anamika -&gt;&gt; Anamika: Ulp...\n        Anamika -&gt;&gt; Arvind: I am sorry... \n        Arvind -&gt;&gt; Anamika: This is not acceptable and will reflect in your grade\n        end\n        \n        Arvind -&gt;&gt; Komal: And you? What kept you?\n        Komal -&gt;&gt; Anamika: (Quietly) He's having a bad day, dude...\n        Anamika -&gt;&gt; Komal: (Whisper) Boomer...\n        Note over Anamika,Komal: Giggle...\n\")\n\n\n\n\n\nFrom here: https://cyberhelp.sesync.org/blog/visualization-with-diagrammeR.html\n\ngrViz(\"digraph{\n\n      graph[rankdir = LR]\n  \n      node[shape = rectangle, style = filled]\n  \n      node[fillcolor = Coral, margin = 0.2]\n      A[label = 'Figure 1: Map']\n      B[label = 'Figure 2: Metrics']\n  \n      node[fillcolor = Cyan, margin = 0.2]\n      C[label = 'Figures.Rmd']\n  \n      node[fillcolor = Violet, margin = 0.2]\n      D[label = 'Analysis_1.R']\n      E[label = 'Analysis_2.R']\n  \n      subgraph cluster_0 {\n        graph[shape = rectangle]\n        style = rounded\n        bgcolor = Gold\n    \n        label = 'Data Source 1'\n        node[shape = rectangle, fillcolor = LemonChiffon, margin = 0.25]\n        F[label = 'my_dataframe_1.csv']\n        G[label = 'my_dataframe_2.csv']\n      }\n  \n      subgraph cluster_1 {\n         graph[shape = rectangle]\n         style = rounded\n         bgcolor = Gold\n    \n         label = 'Data Source 2'\n         node[shape = rectangle, fillcolor = LemonChiffon, margin = 0.25]\n         H[label = 'my_dataframe_3.csv']\n         I[label = 'my_dataframe_4.csv']\n      }\n  \n      edge[color = black, arrowhead = vee, arrowsize = 1.25]\n      C -&gt; {A B}\n      D -&gt; C\n      E -&gt; C\n      F -&gt; D\n      G -&gt; D\n      H -&gt; E\n      I -&gt; E\n      \n      }\")\n\n\n\n\n\n\nmermaid(\"\n        graph BT\n        A((Salinity))\n        A--&gt;B(Barnacles)\n        B-.-&gt;|-0.10|B1{Mussels}\n        A-- 0.30 --&gt;B1\n\n        C[Air Temp]\n        C--&gt;B\n        C-.-&gt;E(Macroalgae)\n        E--&gt;B1\n        C== 0.89 ==&gt;B1\n\n        style A fill:#FFF, stroke:#333, stroke-width:4px\n        style B fill:#9AA, stroke:#9AA, stroke-width:2px\n        style B1 fill:#879, stroke:#333, stroke-width:1px\n        style C fill:#ADF, stroke:#333, stroke-width:2px\n        style E fill:#9C2, stroke:#9C2, stroke-width:2px\n\n        \")\n\n\n\n\n\n\nDiagrammeR(\"\nsequenceDiagram\n  Arvind -&gt;&gt;ticket seller: ask ticket\n  ticket seller-&gt;&gt;database: seats\n  alt tickets available\n    database-&gt;&gt;ticket seller: ok\n    ticket seller-&gt;&gt;customer: confirm\n    Arvind -&gt;&gt;ticket seller: ok\n    ticket seller-&gt;&gt;database: book a seat\n    ticket seller-&gt;&gt;printer: print ticket\n  else sold out\n    database-&gt;&gt;ticket seller: none left\n    ticket seller-&gt;&gt;customer: sorry\n  end\n\")\n\n\n\n\n\n\nDiagrammeR(\n\"graph TB;\nA(Rounded)--&gt;B[Squared];\nB--&gt;C{A Decision};\nC--&gt;D[Square One];\nC--&gt;E[Square Two];\n\n%% Now styling these blocks\nstyle A fill:#E5E25F;  \nstyle B fill:#87AB51; \nstyle C fill:#3C8937;\nstyle D fill:#23772C;  \nstyle E fill:#B6E6E6;\n\"\n)\n\n\n\n\n\n\n  grViz(\"\ndigraph boxes_and_circles {\n\n  # a 'graph' statement\n  graph [overlap = true, fontsize = 10,forcelabels = true]\n\n  # several 'node' statements\n  node [shape = box,fontname = Helvetica, color = red, style = filled]\n  A[label = 'This is \\\\n an internal \\\\n label', xlabel = 'This is \\\\nan external \\\\nlabel']; B; C; D; E; F\n\n  node [shape = circle, fixedsize = true, color = palegreen, width = 0.9] // sets as circles\n  1; 2; 3; 4; 5; 6; 7; 8\n\n  # several 'edge' statements\n  A-&gt;{1,2,3,4} B-&gt;2 B-&gt;3 B-&gt;4 C-&gt;A\n  1-&gt;D E-&gt;A 2-&gt;4 1-&gt;5 1-&gt;F\n  E-&gt;6 4-&gt;6 5-&gt;7 6-&gt;7 3-&gt;8 3-&gt;1\n}\n\")",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-11: The Queen of Hearts, She Made some Tarts"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/index.html#sequence-diagram-2",
    "href": "content/courses/R4Artists/Modules/110-GoD/index.html#sequence-diagram-2",
    "title": "Lab-11: The Queen of Hearts, She Made some Tarts",
    "section": "Sequence Diagram-2",
    "text": "Sequence Diagram-2",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-11: The Queen of Hearts, She Made some Tarts"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/index.html#sequence-diagram-3",
    "href": "content/courses/R4Artists/Modules/110-GoD/index.html#sequence-diagram-3",
    "title": "Lab-11: The Queen of Hearts, She Made some Tarts",
    "section": "Sequence Diagram 3",
    "text": "Sequence Diagram 3",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-11: The Queen of Hearts, She Made some Tarts"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/index.html#mindmap",
    "href": "content/courses/R4Artists/Modules/110-GoD/index.html#mindmap",
    "title": "Lab-11: The Queen of Hearts, She Made some Tarts",
    "section": "Mindmap",
    "text": "Mindmap",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-11: The Queen of Hearts, She Made some Tarts"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/index.html#gantt-chart",
    "href": "content/courses/R4Artists/Modules/110-GoD/index.html#gantt-chart",
    "title": "Lab-11: The Queen of Hearts, She Made some Tarts",
    "section": "Gantt Chart",
    "text": "Gantt Chart",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-11: The Queen of Hearts, She Made some Tarts"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/index.html#flow-chart",
    "href": "content/courses/R4Artists/Modules/110-GoD/index.html#flow-chart",
    "title": "Lab-11: The Queen of Hearts, She Made some Tarts",
    "section": "Flow chart",
    "text": "Flow chart",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-11: The Queen of Hearts, She Made some Tarts"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/index.html#some-definitions-on-the-grammar-of-shapes-in-nomnoml",
    "href": "content/courses/R4Artists/Modules/110-GoD/index.html#some-definitions-on-the-grammar-of-shapes-in-nomnoml",
    "title": "Lab-11: The Queen of Hearts, She Made some Tarts",
    "section": "Some definitions on the “grammar of shapes” in nomnoml\n",
    "text": "Some definitions on the “grammar of shapes” in nomnoml\n\n\nAssociation Types: Connectors between blocks( i.e. Classifiers)\nClassifier Types: Kinds of blocks.\nDirective Types: Directives change the nature of the diagram rendered, by affective parameters like colour, direction and margins. ( Ha! VC people!!)\n\nCSS colours https://www.w3schools.com/cssref/css_colors.asp Only these colours are permitted, so use either the names or these specific colour hash codes. Any general hash code will not render.\n\n//association-1\n[a] - [b] \n\n//association-2\n[b] -&gt; [c] \n\n//association_3\n[c] &lt;-&gt; [a]\n\n//dependency-1\n[a] &lt;--&gt;[d]\n\n//dependency-2\n#.ell: visual=ellipse fill=#fbfb09 bold\n#.arvind: visual=rhomb fill=#ff2234 bold\n[&lt;ell&gt;e]--&gt;[a]\n//generalization-1\n[c]-:&gt;[&lt;arvind&gt;k]\n\n//implementation --:&gt;\n[k]--:&gt;[d]\n\n\n\n\n\n\n//composition +-\n[a]+-[b]\n//composition +-&gt;\n[b]-+[c]\n//aggregation o-\n[c]o-&gt;[d]\n//aggregation o-&gt;\n[d]o-&gt;[a]\n//note --\n[d]--[everything happens;here]\n//hidden -/-\n[d]-/-[f]\n////////////////////////\n//weightless edge _&gt;\n//[k]_&gt;[d] //not working\n//weightless dashed__\n//[d]__[j] //not working\n\n\n\n\n\nClassifier Types\nThese are different kinds of blocks.\n\n[class]-&gt;[&lt;abstract&gt; abstract]\n[&lt;abstract&gt; abstract]-:&gt;[&lt;instance&gt; instance]\n[&lt;instance&gt; instance]-:&gt;[&lt;note&gt; note]\n[&lt;note&gt; note]--&gt;[&lt;reference&gt; reference]\n\n\n\n\n\n\n[&lt;package&gt; package|components]--&gt;[&lt;frame&gt; frame|]\n[&lt;database&gt; database]--&gt;[&lt;start&gt; start]\n[&lt;end&gt; end]-o&gt;[&lt;state&gt; state]\n\n\n\n\n\n\n[&lt;choice&gt; choice]---&gt;[&lt;sync&gt; sync]\n[&lt;input&gt; input]-&gt;[&lt;sender&gt; sender]\n[&lt;receiver&gt; receiver]o-[&lt;transceiver&gt; transceiver]\n\n\n\n\n\n\n#direction:down\n#background:lightgrey\n#fill: fuchsia; green; purple\n#fillArrows: false\n#font: Courier\n[class]-&gt;[&lt;abstract&gt; abstract]\n[&lt;abstract&gt; abstract]-:&gt;[&lt;instance&gt; instance]\n[&lt;instance&gt; instance]-:&gt;[&lt;note&gt; note]\n[&lt;note&gt; note]--&gt;[&lt;reference&gt; reference]\n\n\n\n\n\n\n#font: CenturySchoolbook\n#fill: lightyellow\n#stroke: green\n\n[&lt;actor&gt; actor]---[&lt;usecase&gt; usecase]\n[&lt;usecase&gt; usecase]&lt;--&gt;[&lt;label&gt; label]\n[&lt;usecase&gt; usecase]-/-[&lt;hidden&gt; hidden]\n\n\n\n\n\n\n[&lt;table&gt; table| a | 5 || b | 7]\n\n\n\n\n\n\n[&lt;table&gt; table| c | 9 ]",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-11: The Queen of Hearts, She Made some Tarts"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/index.html#directives",
    "href": "content/courses/R4Artists/Modules/110-GoD/index.html#directives",
    "title": "Lab-11: The Queen of Hearts, She Made some Tarts",
    "section": "Directives",
    "text": "Directives\nDirectives change the nature of the diagram rendered, by affective parameters like colour, direction and margins.",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-11: The Queen of Hearts, She Made some Tarts"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/index.html#custom-classifier-styles",
    "href": "content/courses/R4Artists/Modules/110-GoD/index.html#custom-classifier-styles",
    "title": "Lab-11: The Queen of Hearts, She Made some Tarts",
    "section": "Custom classifier styles",
    "text": "Custom classifier styles\nA directive that starts with “.” define a classifier’s style. The style is written as a space separated list of modifiers and key/value pairs.\n\n#.box: fill=#8f8 dashed\n#.blob: visual=ellipse title=bold\n#.arvind: visual=rhomb title=bold dashed fill=CornFlowerBlue\n[&lt;box&gt; GreenBox]\n[&lt;blob&gt; Blobby]\n[&lt;arvind&gt; Someone]",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-11: The Queen of Hearts, She Made some Tarts"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/index.html#nomnoml-keyvalue-pairs",
    "href": "content/courses/R4Artists/Modules/110-GoD/index.html#nomnoml-keyvalue-pairs",
    "title": "Lab-11: The Queen of Hearts, She Made some Tarts",
    "section": "\nnomnoml Key/value pairs",
    "text": "nomnoml Key/value pairs\n\nfill=(any css color)\nstroke=(any css color)\nalign=center align=left\ndirection=right direction=down\nvisual=actor\nvisual=class\nvisual=database\nvisual=ellipse\nvisual=end\nvisual=frame\nvisual=hidden\nvisual=input\nvisual=none\nvisual=note\nvisual=package\nvisual=receiver\nvisual=rhomb\nvisual=roundrect\nvisual=sender\nvisual=start\nvisual=sync\nvisual=table\nvisual=transceiver",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-11: The Queen of Hearts, She Made some Tarts"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/index.html#text-modifiers",
    "href": "content/courses/R4Artists/Modules/110-GoD/index.html#text-modifiers",
    "title": "Lab-11: The Queen of Hearts, She Made some Tarts",
    "section": "Text modifiers",
    "text": "Text modifiers\nbold center italic left underline\n\n# .box: fill=#8f8 dashed\n# .blob: visual=rhomb title=bold fill=#8f8 dashed\n\n[A]-[B]\n[B]--[&lt;usecase&gt;C]\n[C]-[&lt;box&gt; D]\n[B]--[&lt;blob&gt; Jabba;TheHut]\n\n\n\n\n\n\n[a] -&gt;[b]\n[b] -:&gt; [c]\n[c]o-&gt;[d]\n[d]-/-[e]\n\n\n\n\n\n\n#fill: lightgreen; lightblue; lightyellow; grey; white\n\n[&lt;table&gt; table | c | 9 ]\n\n[R | [&lt;table&gt; Packages |\n         Base R |\n         [ &lt;table&gt; tidyverse| ggplot | tidyr | readr |\n             [&lt;table&gt; dplyr|\n                 magrittr | Others]]]]\n\n\n\n\n\n\n#fill: lightgreen; lightblue; lightyellow; pink; white\n\n[RStudio | [R | [&lt;table&gt; Packages |\n                   Base R | [ tidyverse |\n                               ggplot | tidyr | readr |\n                               [dplyr]--[magrittr]\n                               [dplyr]--[Others]\n                             | tibble\n                             ]\n                 | lubridate | DiagrammeR | Lattice]]]\n\n\n\n\n\n\n[Linux]+-[Ubuntu]\n[Linux]+-[Mint]\n[Ubuntu]--[Mint]\n[Linux]+-[Rosa Linux]\n[Linux]+-[Mx Linux]\n[Debian]-+[Linux]\n\n\n[Fedora]-+[Linux]\n[Puppy Linux]-+[Linux]\n[Personal Pups]-+[Puppy Linux]",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-11: The Queen of Hearts, She Made some Tarts"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/120-WorkingWithText/index.html",
    "href": "content/courses/MathModelsDesign/Modules/120-WorkingWithText/index.html",
    "title": "Working with Text",
    "section": "",
    "text": "Topics that may be covered here: - Text Analysis for Authenticity - Detection of Authorship - Sentiment Analysis",
    "crumbs": [
      "Teaching",
      "Math Models in Design",
      "Working with Text"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/120-WorkingWithText/index.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/120-WorkingWithText/index.html#introduction",
    "title": "Working with Text",
    "section": "",
    "text": "Topics that may be covered here: - Text Analysis for Authenticity - Detection of Authorship - Sentiment Analysis",
    "crumbs": [
      "Teaching",
      "Math Models in Design",
      "Working with Text"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/120-WorkingWithText/index.html#resources",
    "href": "content/courses/MathModelsDesign/Modules/120-WorkingWithText/index.html#resources",
    "title": "Working with Text",
    "section": "Resources",
    "text": "Resources\n\nhttps://worrydream.com/Tangle/ Reactive text documents. Brett Victor( “Worrydream”)",
    "crumbs": [
      "Teaching",
      "Math Models in Design",
      "Working with Text"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/160-WorkingWithThoughts/index.html",
    "href": "content/courses/MathModelsDesign/Modules/160-WorkingWithThoughts/index.html",
    "title": "Working With Thoughts",
    "section": "",
    "text": "We will play several short games followed by discussions. These games may bring to light some of our Cognitive Biases and see how they affect us, and especially as we try to function as Artists/Designers/Creators.\nThere will be short readings that follow after each game.\nBut first, let us see how frail/fragile/fallible… we all are:\n\n\n\nRight! On to our first little fallibility!\n\n\n\n\nTest: PPT\n\nShort Reading: PDF\n\nTool: PDF\n\nReading: Here is a short reading on Exaggerated Emotional Coherence, also known as the Halo Effect** Download PDF **\n\nYou have to Stick the lighted candle to the Wall in such a way that the melting wax does not drop on to the floor.\n\nLook at the graph below: does it remind you of something you know very well?\n\n\n\n\n\n\n\n\nWhat does this graph represent?\nLet us pretend we are part of this graph and see where our Problem Formulating Skills take us!",
    "crumbs": [
      "Teaching",
      "Math Models in Design",
      "Working With Thoughts"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/160-WorkingWithThoughts/index.html#can-you-see-straight",
    "href": "content/courses/MathModelsDesign/Modules/160-WorkingWithThoughts/index.html#can-you-see-straight",
    "title": "Working With Thoughts",
    "section": "",
    "text": "We will play several short games followed by discussions. These games may bring to light some of our Cognitive Biases and see how they affect us, and especially as we try to function as Artists/Designers/Creators.\nThere will be short readings that follow after each game.\nBut first, let us see how frail/fragile/fallible… we all are:\n\n\n\nRight! On to our first little fallibility!\n\n\n\n\nTest: PPT\n\nShort Reading: PDF\n\nTool: PDF\n\nReading: Here is a short reading on Exaggerated Emotional Coherence, also known as the Halo Effect** Download PDF **\n\nYou have to Stick the lighted candle to the Wall in such a way that the melting wax does not drop on to the floor.\n\nLook at the graph below: does it remind you of something you know very well?\n\n\n\n\n\n\n\n\nWhat does this graph represent?\nLet us pretend we are part of this graph and see where our Problem Formulating Skills take us!",
    "crumbs": [
      "Teaching",
      "Math Models in Design",
      "Working With Thoughts"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/160-WorkingWithThoughts/index.html#discussion",
    "href": "content/courses/MathModelsDesign/Modules/160-WorkingWithThoughts/index.html#discussion",
    "title": "Working With Thoughts",
    "section": "Discussion",
    "text": "Discussion\n\nProblems and Contradictions\nAll Available Resources\n\nAssumptions and Functional Fixedness\n\n\n\nA comparable switch of attention occurs in an old joke about a worker in a high security factory, in which the employees were carefully watched when they left at the end of their work day. On a particular day, this worker was stopped at the factory gate as he walked out with a wheelbarrow full of styrofoam packing peanuts. He explained that he had salvaged these from the trash, and was planning to use them in shipping gifts to his grandchildren. Searching through this packing material, the guards found nothing, and so they let the man go home. The following week the same thing happened, and the worker was again stopped. But he offered the very same story, and when the guards searched through the packing peanuts and found nothing, he was allowed to leave. But this continued, week after week, until the guards could no longer believe that one person would want or could make use of so much packing material. Finally, the man was held for interrogation, at which time he admitted that he had absolutely no use for packing peanuts - and that, all these weeks, he had been stealing wheelbarrows.\n\n\nHearing this joke, I am reminded of the phrase “part and parcel”, which is a rough equivalent of “figure and ground”, the Gestalt Principles. Throughout most of it, the packing peanuts occupy center stage as figure (part), while the wheelbarrows (which function merely as containers) are completely ignored as innocuous ground (parcel). At the end of the joke, there is an unexpected twist, a switch of emphasis, a recentering, when we learn that the parcel is really the part.\n\nThis should also remind us of the Guilford Alternative Uses Exercise that we did, where we forced ourselves to leave the “regular use” of an object behind and think of it as serving quite another function.\nBias on TV\nLet’s find some of these ideas in our favourite Episode of one Season of your favourite show and tell everybody with a poster!",
    "crumbs": [
      "Teaching",
      "Math Models in Design",
      "Working With Thoughts"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/160-WorkingWithThoughts/index.html#bayesian-estimation",
    "href": "content/courses/MathModelsDesign/Modules/160-WorkingWithThoughts/index.html#bayesian-estimation",
    "title": "Working With Thoughts",
    "section": "Bayesian Estimation",
    "text": "Bayesian Estimation\nTaxicab Accident problem\nDisease Problem\nBaseball score prediction in R ( David Robinson)",
    "crumbs": [
      "Teaching",
      "Math Models in Design",
      "Working With Thoughts"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/160-WorkingWithThoughts/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/160-WorkingWithThoughts/index.html#references",
    "title": "Working With Thoughts",
    "section": "References",
    "text": "References\n\nThe Halo Effect, https://explorable.com/halo-effect\nNisbett, R. E., & Wilson, T. D. (1977). The halo effect: Evidence for unconscious alteration of judgments. Journal of Personality and Social Psychology, 35(4), 250–256. https://doi.org/10.1037/0022-3514.35.4.250 Download PDF\nBayesian Thinking Tutorial https://arbital.com/p/bayes_frequency_diagram/?l=55z&pathId=86923\nhttp://ndl.ethernet.edu.et/bitstream/123456789/37455/1/Max_Marchi.pdf",
    "crumbs": [
      "Teaching",
      "Math Models in Design",
      "Working With Thoughts"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/80-WorkingWithChance/index.html",
    "href": "content/courses/MathModelsDesign/Modules/80-WorkingWithChance/index.html",
    "title": "Working with Chance",
    "section": "",
    "text": "Topics that may be covered here:\n- Monte Carlo Simulations\n- Bayesian Thinking",
    "crumbs": [
      "Teaching",
      "Math Models in Design",
      "Working with Chance"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/80-WorkingWithChance/index.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/80-WorkingWithChance/index.html#introduction",
    "title": "Working with Chance",
    "section": "",
    "text": "Topics that may be covered here:\n- Monte Carlo Simulations\n- Bayesian Thinking",
    "crumbs": [
      "Teaching",
      "Math Models in Design",
      "Working with Chance"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/80-WorkingWithChance/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/80-WorkingWithChance/index.html#references",
    "title": "Working with Chance",
    "section": "References",
    "text": "References\n\nhttps://californiaglobe.com/fr/the-story-of-arnold-schwarzeneggers-infamous-hidden-expletive-veto/\n\nPhilip B. Stark (2010). Null and Vetoed: “Chance Coincidence”?, Chance, November 2010, v23(4), 43–46.). https://www.stat.berkeley.edu/~stark/Preprints/acrosticVeto09.htm\nLo Bello, A. (1991). Ask Marilyn: The Mathematical Controversy in Parade Magazine. The Mathematical Gazette, 75(473), 275–277. https://doi.org/10.2307/3619484.PDF.",
    "crumbs": [
      "Teaching",
      "Math Models in Design",
      "Working with Chance"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/40-WorkingWithNetworks/index.html",
    "href": "content/courses/MathModelsDesign/Modules/40-WorkingWithNetworks/index.html",
    "title": "Working with Networks",
    "section": "",
    "text": "Do you think your friends have more friends than you have? Do you think that you are outside the herd, and that what you think or do is from your own mind?\nAnyways…",
    "crumbs": [
      "Teaching",
      "Math Models in Design",
      "Working with Networks"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/40-WorkingWithNetworks/index.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/40-WorkingWithNetworks/index.html#introduction",
    "title": "Working with Networks",
    "section": "",
    "text": "Do you think your friends have more friends than you have? Do you think that you are outside the herd, and that what you think or do is from your own mind?\nAnyways…",
    "crumbs": [
      "Teaching",
      "Math Models in Design",
      "Working with Networks"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/40-WorkingWithNetworks/index.html#activities",
    "href": "content/courses/MathModelsDesign/Modules/40-WorkingWithNetworks/index.html#activities",
    "title": "Working with Networks",
    "section": "Activities",
    "text": "Activities\nActivity-1: Secret Santa Game\nLet us play this in the vanilla way: Paper chits with names in a bin and drawing them in turn. What can go wrong with this?\nShould we use this instead? https://www.drawnames.com.sg/secret-santa-generator\nDiscussion: Nodes, Links, Link Directionality, Connected and Disconnected Networks\nActivity-2: Barabasi Cocktail Party Game\nThis is a game “invented” by Alberto-Laszlo Barabasi, a Network Science pioneer and expert, who has written a wonderful, and wonderfully acessible, book on Network Science, available online http://networksciencebook.com/\nDiscussion: Network Mechanisms, Information Flow, Giant Component,Emergence\nActivity-3: Indian Surnames Game\n\nHow many common India surnames do we know? Let us write them on the board.\nEach of us will now look at each surname and recollect how many people they know with that surname.\nWrite down the score for each surname.\nLet’s plot this on (yet another ) network!!\nTry also to create a map using this website: The Memory Underground https://memoryunderground.com\n\n\nDiscussion: Node Degree, Giant Component?  Small Worlds?  Multi-Link network, Link Values or Costs\nActivity-4: Way-Spotting Game\nNow that we have an idea of nodes, links and costs, let us get an experience of some more network science ideas:\n\nMake groups of 3.\nHead over to http://www.wayspotting.com/index.html\n\nPlay!! Make a note of your route each time ( your “traversal” of the network)\nNote if you can see the following:\n\n\n\nFrequently Used Nodes\nFrequently used Links\n\n\n\nSee here for more info: https://medium.com/@ran_katzir/teaching-network-science-using-board-games-f78489a3b3bd\n\n\nDiscussion: Network Traversal, Node Degree, Centrality, Betweenness, Link Values or Costs\nActivity-5: Hi, I am Kevin Bacon, SMI Foundation 2022 Batch\nLet us find a Keven Bacon in SMI Foundation Studies Programme!!\n\nCollect friends Data from across college/class, import and plot, analyze and comment\nUse this online tool at DataBasic.io https://databasic.io to Connect the Dots, OR\nEven more fun at at GraphCommons https://graphcommons.com/graphs/new\n\n\nDiscussion: Node Degree, Centrality, Betweenness, Link Values or Costs\nActivity-6: Can you Introduce me to Chandler, again?\n\nTake your favourite Literary Work / TV Serial / Movie and create a Network Database for it.\nVisualize it either with or without tech tools From Teach Engineering, this Activity Sheet https://www.teachengineering.org/activities/view/uno_graphtheory_lesson01_activity2\n\nCan also use Graph Comicshttps://aviz.fr/~bbach/graphcomics/\n\n\nDiscussion: Networks are everywhere, Cannot \"unsee\" them, You are a node and you are a link..are you?\nActivity-7: Why are all Metro Maps at 45 degrees?\n\nhttps://artsandculture.google.com/asset/the-tate-gallery-by-tube-david-booth/PAG-Gx_SV2jNiw?hl=en\nhttps://search.r-project.org/CRAN/refmans/ggraph/html/layout_tbl_graph_metro.html\nHenry Beck’s London Underground Map. https://artsandculture.google.com/asset/underground-map-henry-c-beck-london-transport-and-waterlow-sons-ltd/fAHJweSexswKxw?hl=en and\n\nhttps://g.co/arts/UNfBm6QL5kukYm4r9",
    "crumbs": [
      "Teaching",
      "Math Models in Design",
      "Working with Networks"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/40-WorkingWithNetworks/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/40-WorkingWithNetworks/index.html#references",
    "title": "Working with Networks",
    "section": "References",
    "text": "References\n\nDmitry Zinoniev, Network Science Intro Slides, https://www.slideshare.net/DmitryZinoviev/workshop-20212296\nMark Newman, The Physics of Networks,Read the PDF\nA Network oriented short story. Frigyes Karinthy, “Chains”. Read PDF\nWho told you about Srishti? Where? Read Mark Granovetter, The Strength of Weak Ties, https://www.cs.cmu.edu/~jure/pub/papers/granovetter73ties.pdf",
    "crumbs": [
      "Teaching",
      "Math Models in Design",
      "Working with Networks"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/listing.html",
    "href": "content/courses/MathModelsDesign/listing.html",
    "title": "Math Models in Design",
    "section": "",
    "text": "We will study several Mathematical Models and apply them to Art and Design. The algorithms will be examined and then coded in R; however other open source tools may also be introduced as and when needed.",
    "crumbs": [
      "Teaching",
      "Math Models in Design"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/listing.html#introduction",
    "href": "content/courses/MathModelsDesign/listing.html#introduction",
    "title": "Math Models in Design",
    "section": "",
    "text": "We will study several Mathematical Models and apply them to Art and Design. The algorithms will be examined and then coded in R; however other open source tools may also be introduced as and when needed.",
    "crumbs": [
      "Teaching",
      "Math Models in Design"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/listing.html#ideas",
    "href": "content/courses/MathModelsDesign/listing.html#ideas",
    "title": "Math Models in Design",
    "section": "Ideas",
    "text": "Ideas\n\n6 Equations that changed the world: Each Model is an Equation\nA mechanical engineer, a chemical engineer, and a computer scientist are driving down the street. The car breaks down and all three get out to see what’s wrong. The mechanical engineer says, “I know what’s wrong, it sounds like the the piston rods are misaligned, if we just fix that it’ll work again.” She takes out her tools, starts messing with the engine, and 30 minutes later the car still isn’t running. The chemical engineer says, “No, no, it sounds like we’re having a problem with the oxygen mixture, it’s too rich and we just need to tune that.” She pulls out the oxygen sensor and starts tweaking, but 30 minutes later the car still doesn’t go. The computer scientist says, “I don’t know what you two are talking about. Just turn the car off, slam the left passenger door three times, walk around the car twice, turn it back on and it will work again.” They do that, and lo and behold, the car starts. CS Command Line Bulshittery",
    "crumbs": [
      "Teaching",
      "Math Models in Design"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/listing.html#references",
    "href": "content/courses/MathModelsDesign/listing.html#references",
    "title": "Math Models in Design",
    "section": "References",
    "text": "References\n\nMaking Explanations (tools): https://explorabl.es/tools/",
    "crumbs": [
      "Teaching",
      "Math Models in Design"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/90-TRIZ-Contradiction-Matrix/index.html#project-tetris-triz-videos",
    "href": "content/courses/TRIZ4ProbSolving/Modules/90-TRIZ-Contradiction-Matrix/index.html#project-tetris-triz-videos",
    "title": "TRIZ - The Contradiction Matrix Workflow",
    "section": "Project TETRIS TRIZ Videos",
    "text": "Project TETRIS TRIZ Videos\nLet us get an actual experience of TRIZ methods with these short videos:\n\nHistory of TRIZ\n\n\n\n\n\n\nTRIZ Tales: Nina at School\n \n\n\nTRIZ Tales: Nina at University\n\n\n\n\nTRIZ Tales: Nina at Work #1 \n\n\n\n\nTRIZ Tales: Nina at Work #2 \n\n\n\nHow to use the Contradiction Matrix:(Coursera Video)",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "TRIZ - The Contradiction Matrix Workflow"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/90-TRIZ-Contradiction-Matrix/index.html#a-triz-workflow",
    "href": "content/courses/TRIZ4ProbSolving/Modules/90-TRIZ-Contradiction-Matrix/index.html#a-triz-workflow",
    "title": "TRIZ - The Contradiction Matrix Workflow",
    "section": "A TRIZ Workflow",
    "text": "A TRIZ Workflow\nWe have seen\n\nHow to examine a Situation\n\nUse the Ishikawa Diagram to find Knobs and Settings\nFind one or more Administrative Contradictions\n\nState the Ideal Final Result\n\nConvert the AC into a TC with 39 TRIZ Parameters\n\n(Convert the AC into a PC)\n\nNow we will use the TC and find out solutions for it using what is called the Contradiction Matrix. Here is a workflow presentation below:https://arvindvenkatadri.com/slides/playandinvent/triz-basics/#1\"",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "TRIZ - The Contradiction Matrix Workflow"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/90-TRIZ-Contradiction-Matrix/index.html#triz-stories-and-case-studies",
    "href": "content/courses/TRIZ4ProbSolving/Modules/90-TRIZ-Contradiction-Matrix/index.html#triz-stories-and-case-studies",
    "title": "TRIZ - The Contradiction Matrix Workflow",
    "section": "TRIZ Stories and Case Studies!!",
    "text": "TRIZ Stories and Case Studies!!\nWant to create a good protest or strike? Or simply want to clean up the oceans?\nhttps://arvindvenkatadri.com/slides/playandinvent/triz-database/#1\"",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "TRIZ - The Contradiction Matrix Workflow"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/90-TRIZ-Contradiction-Matrix/index.html#the-triz-contradiction-matrix",
    "href": "content/courses/TRIZ4ProbSolving/Modules/90-TRIZ-Contradiction-Matrix/index.html#the-triz-contradiction-matrix",
    "title": "TRIZ - The Contradiction Matrix Workflow",
    "section": "The TRIZ Contradiction Matrix",
    "text": "The TRIZ Contradiction Matrix\n\nThe TRIZ Contradiction Matrix (2003) (Matrix)",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "TRIZ - The Contradiction Matrix Workflow"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/90-TRIZ-Contradiction-Matrix/index.html#references",
    "href": "content/courses/TRIZ4ProbSolving/Modules/90-TRIZ-Contradiction-Matrix/index.html#references",
    "title": "TRIZ - The Contradiction Matrix Workflow",
    "section": "References",
    "text": "References\n\nPizza Box https://www.metodolog.ru/triz-journal/archives/2008/03/04/\nTRIZ for Dummies Cheat Sheet (weblink)\nTRIZ Inventive Principles in Pics @instagram: https://www.instagram.com/stories/highlights/17857566926043828/\nStan Kaplan, An Introduction to TRIZ(PDF) This is a simple and short introduction to all aspects of Classical TRIZ.\nJack Hipple, “The Ideal Result: What it is and how to achive it”, Springer, 2012.\nProject TETRIS: Chapter 1: Introduction to Classical TRIZ (PDF)\nProject TETRIS: Chapter 5: Techniques to Resolve Contradictions / Resources / Effects (PDF)\nProject TETRIS: Examples of inventive problems: Example 1-5 (RAR Archive File)",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "TRIZ - The Contradiction Matrix Workflow"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/50-Metaphoric-Thinking/index.html#introduction",
    "href": "content/courses/TRIZ4ProbSolving/Modules/50-Metaphoric-Thinking/index.html#introduction",
    "title": "A Year of Metaphoric Thinking",
    "section": "Introduction",
    "text": "Introduction\n\nThe Year of Magical Thinking is Joan Didion’s account of the year following the death of her husband, writer John Gregory Dunne, and her attempts to make sense of her grief while tending to the severe illness of her adopted daughter, Quintana. As she tries to make sense of John’s death and her own changed identity, Didion discovers that grief is not what she expected it to be. Consumed by memories of the years they lived in Los Angeles, shortly after they married and adopted Quintana, Didion feels that she has entered a state of temporary insanity. Though cool and collected on the surface, she begins to believe that her wishes might have the power to bring John back. To this end, she refuses to give away his clothes and shoes, believing that her husband will need them when he returns to her. She calls this childlike belief that her thoughts and wishes can alter reality “magical thinking.”\n\nOf course we are not embarked on anything like this, but we do want to generate some “magic” in our thoughts! So taking some inspiration from her “childlike belief” that can “alter reality”, let us hark back to our childhoods and see what we can do with these objects below:",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "A Year of Metaphoric Thinking"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/50-Metaphoric-Thinking/index.html#activities",
    "href": "content/courses/TRIZ4ProbSolving/Modules/50-Metaphoric-Thinking/index.html#activities",
    "title": "A Year of Metaphoric Thinking",
    "section": "Activities",
    "text": "Activities\nActivity-1: Some Objects to Contemplate\nHow many different uses can you imagine for each of these objects? Can you briefly describe and quickly sketch a few ideas?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nActivity-2: Some Shapes to Contemplate\nWhere do you reckon you can “see” these shapes ? Can you briefly describe and quickly sketch a few ideas?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScoring your Ideas\nScoring is comprised of four components:\n\n\nFluency - total. Just add up all the responses. In this example it is 6.\n\nFlexibility - or different categories of ideas or, as Csikszentmihalyi would have us say, DOMAINS. Is your brick a Toy? Can it be used as…a Horticultural support thing? That is two domains, so two points.\n\n\n\nFrom https://houseplantjoy.com/bonsai-styles-fascinating-facts-about-bonsai-forms/\n\n\n\nElaboration - amount of detail (for Example: “in a bonsai” = 0, whereas “in a bonsai to create a root-over-rock structure as an island” = 2 (one for root-over-rock, two for further detail about the island structure).\n\nOriginality - each response it compared to the total amount of responses from all of the people you gave the test to. Responses that were given by only 5% of your group are unusual (1 point), responses that were given by only 1% of your group are unique - 2 points). Total all the points. Higher scores indicate creativity*\n\n\n\n\n\n\n\nNote\n\n\n\nYou might have noticed that the higher fluency the higher the originality (if you did “good for you!”) This is a contamination problem and can be corrected by using a corrective calculation for originality (originality = originality/fluency).\n\n\nDiscussion\nBoth these activities are examples of exercises in divergent thinking. See the references for more information.\n\nDid you use the words “as” and “like” to describe your ideas?\nDid you not use these words to describe your ideas?\nWere there, in your opinion, any outrageous ideas presented? Why were they outrageous?\nAre metaphors more interesting when they are surprising?\nHow did the attributes of the objects ( shape , texture, size, weight, orientation…) get embedded in the ideas presented?\nWere these “embeddings” meaningful? How and why so? (Ask Bourdieu!)\nActivity-3: Gangs of Wasseypur\nWe will divide into two groups (four if necessary) and contemplate a brief description of the town of Wasseypur. There are 4 short questions / problems for you to consider at the end.\nActivity-4: Seymour Papert Constructionism Game\nhttps://arvindvenkatadri.com/courses/1-play-and-invent/modules/50-metaphoric-thinking/#activity-4-seymour-papert-constructionism-game-1\nActivity-5: C’est ne une Pipe\n\nWe will break up into groups of 4-5.\nEach group will be given a household object, perhaps an unusual one.\nYou need to imagine a use for it that is not what is the common known one.\nMarket it as a product that serves this new purpose. Make an ad.\nUse only Gen Z language in your ad.\nAd = Performance/Jingle + Poster\n\n(Articles: Book End made of Wood; Aristo Slide Rule; 80-year-old brass mortar and pestle; Node from Elephant Bamboo stem)\nActivity-6: Metaphorical Story re-Writing\n\nRead the story in Ref. 7 below.\nUnderstand the metaphors in the story:\n\nWhat is the story about?\nWhat “metaphorical language” has Joshua Ferris used to describe the characters, their actions, and the results of their actions?\nWhat is the domain of these metaphors in the story?\n\n\nNow, choose a (short!) story that you know really well, something that you may have encountered in school.\nRe-write this story using language, metaphors, and images from any domain (other than what may be in the original story).\n\nFor example, in Primo Levi’s lyrical story Carbon, a lot of metaphors from Chemistry and atomic physics are used to describe life, and connections with people.",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "A Year of Metaphoric Thinking"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/50-Metaphoric-Thinking/index.html#references",
    "href": "content/courses/TRIZ4ProbSolving/Modules/50-Metaphoric-Thinking/index.html#references",
    "title": "A Year of Metaphoric Thinking",
    "section": "References",
    "text": "References\n\nGuilford Test for Divergent Thinking: (Weblink)\nWallas-Kogan Test for Divergent Thinking: (Weblink)\nThibodeau, Paul & Boroditsky, Lera. (2011). Metaphors We Think With: The Role of Metaphor in Reasoning. PloS one. 6. e16782. 10.1371/journal.pone.0016782. (PDF)\nBobo Hjort. (2003) Drawing, Knowledge, and Intuitive Thinking: Drawing as a Way to Understand and Solve Complex Problems in Art and Complexity. J. Casti and A. Karlqvist (editors) © 2003 Published by Elsevier Science B.V.(PDF)\nDavid Chen, Creative Paradoxical Thinking and Its Implications for Teaching and Learning Motor Skills (PDF)\nHolyoak and Thagard, The Analogical Mind, (PDF)\nJoshua Ferris, The Market Value of my Father, a metaphorical Short Story. (Web Link)",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "A Year of Metaphoric Thinking"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/35-Domain-Field-Self/index.html#the-creativity-systems-model",
    "href": "content/courses/TRIZ4ProbSolving/Modules/35-Domain-Field-Self/index.html#the-creativity-systems-model",
    "title": "I Connect therefore I am",
    "section": "The Creativity Systems Model",
    "text": "The Creativity Systems Model\n\n\n\n\nMihaly Csikszentmihalyi\n\n\n\nMihaly Csikszentmihalyi created what he called a Systems Model for Creativity, shown in the Figure below.\n\n\n\n\n\n\n\n\nHere are some brief extracts from his book[Reference 1]\n\nMany of these books and articles have tried to answer what has been thought to be the most fundamental question: What is creativity? But no one has raised the simple question that should precede attempts at defining, measuring, or enhancing, namely: Where is creativity? On hearing this question, most people would answer “Why, in the creative person’s head, of course”.\n\n\nAfter studying creativity on and off for almost a quarter of a century, I have come to the reluctant conclusion that this is not the case. We cannot study creativity by isolating individuals and their works from the social and historical milieu in which their actions are carried out.\n\n\nThis is because what we call creative is never the result of individual action alone; it is the product of three main shaping forces:\n\na set of social institutions, or field, that selects from the variations produced by individuals those that are worth preserving;\n\na stable cultural domain that will preserve and transmit the selected new ideas or forms to the following generations;\n\nand finally the individual, who brings about some change in the domain, a change that the field will consider to be creative.\n\n\n\nSo the question “Where is creativity?” cannot be answered solely with reference to the person and the person’s work. Creativity is a phenomenon that results from interaction between these three systems. Without a culturally defined domain of action in which innovation is possible, the person cannot even get started. And without a group of peers to evaluate and confirm the adaptiveness of the innovation, it is impossible to differentiate what is creative from what is simply statistically improbable or bizarre.\n\n\nIf we think about it, the reason we believe that Leonardo or Einstein was creative is that we have read that that is the case, we have been told it is true; our opinions about who is creative and why ultimately are based on faith. We have faith in the domains of art and science, and we trust the judgment of the field, that is, of the artistic and scientific establishments. There is nothing wrong with this, because it is an inevitable situation. But by recognizing it, we must also accept some of its consequences, namely, that any attribution of creativity must be relative, grounded only in social agreement.",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "I Connect therefore I am"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/35-Domain-Field-Self/index.html#discussion",
    "href": "content/courses/TRIZ4ProbSolving/Modules/35-Domain-Field-Self/index.html#discussion",
    "title": "I Connect therefore I am",
    "section": "Discussion",
    "text": "Discussion\nWhat are the salient points in this model?\n\nA rather “societal” view of creativity! Rather than a psychic view!\n\nThe influence of environment and people around the individual\n\nA defined domain of work, with a defined language to discuss its ideas and concepts.\nThe “evaluation” of the creative act by field members, who act as gatekeepers\n\nIs this nonplussing? Contrary to what you believe? Outright disturbing?\nI think this is immensely reassuring and humbling at the same time: reassuring because a creative act can be performed by anybody, not some arbitrarily defined “gifted individual”, and humbling, because if field members don’t like your act, or if you don’t have access to the field, you might not make it. It is fair and unfair, but immensely real.",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "I Connect therefore I am"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/35-Domain-Field-Self/index.html#why-this-model-matters-to-us",
    "href": "content/courses/TRIZ4ProbSolving/Modules/35-Domain-Field-Self/index.html#why-this-model-matters-to-us",
    "title": "I Connect therefore I am",
    "section": "Why this Model matters to us",
    "text": "Why this Model matters to us\nAs we will see later in this course, classical TRIZ was developed by Genrikh Altshuller based on an evaluation of more than 25000 patents. He found “design patterns” that had been repeatedly used across patents and classified them for our use. There is a strong sense of history and society right at the heart of TRIZ.\nFurther, the patents the study of which helped Altshuller create TRIZ in the first place belong with domains that we may not necessarily encounter. And yet, the Principles of TRIZ straddle domains and help “cross-domain” solutions to happen: the use of principles in one domain in another, unrelated, domain. (More when we enter TRIZ proper)",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "I Connect therefore I am"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/35-Domain-Field-Self/index.html#some-examples-from-history",
    "href": "content/courses/TRIZ4ProbSolving/Modules/35-Domain-Field-Self/index.html#some-examples-from-history",
    "title": "I Connect therefore I am",
    "section": "Some Examples from History",
    "text": "Some Examples from History\n\nBotticelli was for centuries considered to be a coarse painter, and the women he painted “sickly” and “clumsy”. Only in the mid-nineteenth century did some critics begin to reevaluate his work and see in it creative anticipations of modern sensibility. To what extent was creativity contained in Botticelli’s canvases, and to what extent did it emerge from the interpretive efforts of critics like Ruskin?\n\n\n…our view of Mendel’s contribution to genetics is generally quite wrong. The impression we have is that Mendel made a series of epochal experiments in the genetic transmission of traits in the 1860s, but that his creativity was not recognized by the scientific community until about 40 years later. This view is radically mistaken in a subtle but essential respect. (Brannigan) argues that Mendel’s experiments were not and could not have been contributions to genetics at the time they were made. Their implications for the theory of variation and natural selection were discovered only in 1900 by William Bateson and other evolutionists looking for a mechanism that explained discontinuous inheritance. Within their theoretical framework, Mendel’s work suddenly acquired an importance that it had lacked before, even in the mind of Mendel himself.",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "I Connect therefore I am"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/35-Domain-Field-Self/index.html#references",
    "href": "content/courses/TRIZ4ProbSolving/Modules/35-Domain-Field-Self/index.html#references",
    "title": "I Connect therefore I am",
    "section": "References",
    "text": "References\n\nMihaly Csikszentmihalyi, The Systems Model of Creativity: The Collected Works of Mihaly Csikszentmihalyi, ISBN:9789401790857, Springer, 2015.\nOliver Ding, “Platform Creativity: Domain, Field, and Person”, https://medium.com/call4/domain-8a22b6b486f4\nJessica Wolf,UCLA Newsroom, The truth about Galileo and his conflict with the Catholic Church, https://newsroom.ucla.edu/releases/the-truth-about-galileo-and-his-conflict-with-the-catholic-church",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "I Connect therefore I am"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/20-Cultural-Capital/index.html#introduction",
    "href": "content/courses/TRIZ4ProbSolving/Modules/20-Cultural-Capital/index.html#introduction",
    "title": "I am What I yam",
    "section": "Introduction",
    "text": "Introduction\nWe will understand Pierre Bourdieu’s Concept of Cultural Capital(CC).\nCC affects our view of the world and helps us identify Problem Situations and Contradictions in Life.\n\nWhat is Cultural Capital? https://culturallearningalliance.org.uk/what-is-cultural-capital\n\nCultural Capital",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "I am What I yam"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/20-Cultural-Capital/index.html#activities",
    "href": "content/courses/TRIZ4ProbSolving/Modules/20-Cultural-Capital/index.html#activities",
    "title": "I am What I yam",
    "section": "Activities",
    "text": "Activities\nSharing\n\nSharing of CC: we will each of us take turns to reveal some aspect of Cultural Capital that we directly possess: Objectified, Institutional, and Embodied\n\n\n\nWhat is something that replicates from generation to generation, biologically?\nIs there something that replicates “non-biologically”?\nHow would you measure that? In what units, so to speak?\nRe-Inventing your Own Game\n\nYou will be divided into Teams. Each Team needs to chose a wellknown parlour/household or board game from your childhood. Set it up and play it.\nThen, you will be given a piece of Electronic Hardware called a Makey-Makey. You will need to include this hardware in your Game and extend the Game in a certain way.\n\n\n\n\n\n\n\nTRIZ Concept: Su-Field Analysis\n\n\n\nWe will later tie this activity to the TRIZ idea of Substance-Field Analysis. Perhaps you have a premonitory idea about this already?\n\n\nInventing your City\n\nMaking a Street Intersection Logo\n\nLet us take inspiration from this website: https://www.cartogrammar.com/blog/maps-make-the-best-logos/\n\nHere are some instructions:\n\n\n\nVisit your home town on Google Maps\nSelect a specific STREET INTERSECTION there.\nCopy paste the Shape of the Street Intersection into your favourite image making software\nCreate A LOGO for your city, using the Shape, such that you show off some interesting aspects of your City, based on your own CULTURAL CAPITAL.\nThree sentences to describe your Logo.\nSee examples at the link above for inspiration\n\n\n\n\n\n\n\nSerendipity\n\n\n\nThis course is about nothing if not about serendipity. https://www.dictionary.com/browse/serendipity. We will connect this logo building idea with the idea of Metaphors in TRIZ.",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "I am What I yam"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/20-Cultural-Capital/index.html#references",
    "href": "content/courses/TRIZ4ProbSolving/Modules/20-Cultural-Capital/index.html#references",
    "title": "I am What I yam",
    "section": "References",
    "text": "References\n\nWeb Apps for your Makey-Makey. https://makeymakey.com/blogs/how-to-instructions/apps-for-plug-and-play\nMake your own game with a Makey-Makey. https://www.instructables.com/Using-Scratch-and-Makey-Makey-to-make-your-own-gam\nRichard Dawkins. The Selfish Gene. https://richarddawkins.net/2014/02/whats-in-a-meme/",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "I am What I yam"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/files/Cognitive-Bias-Games.html",
    "href": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/files/Cognitive-Bias-Games.html",
    "title": "I Think, Therefore I am",
    "section": "",
    "text": "In this Module, we will acquaint ourselves with a few of our Cognitive Biases, in gamefied fashion. We will then discuss how these biases inform our efforts in the practice of Creativity and Innovation and discuss ways of overcoming them."
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/files/Cognitive-Bias-Games.html#bias-1-cognitive-miserliness",
    "href": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/files/Cognitive-Bias-Games.html#bias-1-cognitive-miserliness",
    "title": "I Think, Therefore I am",
    "section": "Bias #1: Cognitive Miserliness",
    "text": "Bias #1: Cognitive Miserliness\nGame\nLet us start with a set of relatively easy games. Here PDF are some questions for you. Please write or call out the answers as you see them.\nInterpretation\nReferences for Further Reading"
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/files/Cognitive-Bias-Games.html#bias-2-halo-effect",
    "href": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/files/Cognitive-Bias-Games.html#bias-2-halo-effect",
    "title": "I Think, Therefore I am",
    "section": "Bias #2 Halo Effect",
    "text": "Bias #2 Halo Effect\nGame\nLook at the set of words given here PDF. We have a set of words arranged in a series; each word-set describing a person. Please look at these and then write a brief description of that person.\nLet us now compare what different people wrote about the persons described the respective word-sets.\nInterpretation\nWhat we saw was that the word-set are the same but in reverse order. The SEQUENCE in which we encounter them colours our opinion of the person whom they describe. So first impressions do seem to be last impressions! This is a Cognitive Bias the Halo Effect at work. We formulate decisions based on first impressions. What can we do to over come this bias?\nReferences for Further Reading"
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/files/Cognitive-Bias-Games.html#bias-3-priming",
    "href": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/files/Cognitive-Bias-Games.html#bias-3-priming",
    "title": "I Think, Therefore I am",
    "section": "Bias #3 Priming",
    "text": "Bias #3 Priming\nGame\nYou will be given a small questionnaire PDF, with a single multiple-choice question. Please enter your answer at the appropriate location.\nIntepretation\nThe Marvels of Priming\nAs is common in science, the first big breakthrough in our understanding of the mechanism of association was an improvement in a method of measurement. Until a few decades ago, the only way to study associations was to ask many people questions such as, “What is the first word that comes to your mind when you hear the word DAY?” The researchers tallied the frequency of responses, such as “night,” “sunny,” or “long.” In the 1980s, psychologists discovered that exposure to a word causes immediate and measurable changes in the ease with which many related words can be evoked. If you have recently seen or heard the word EAT, you are temporarily more likely to complete the word fragment SO_P as SOUP than as SOAP. The opposite would happen, of course, if you had just seen WASH. We call this a priming effect and say that the idea of EAT primes the idea of SOUP, and that WASH primes SOAP.\nPriming effects take many forms. If the idea of EAT is currently on your mind (whether or not you are conscious of it), you will be quicker than usual to recognize the word SOUP when it is spoken in a whisper or presented in a blurry font. And of course you are primed not only for the idea of soup but also for a multitude of food-related ideas, including fork, hungry, fat, diet, and cookie. If for your most recent meal you sat at a wobbly restaurant table, you will be primed for wobbly as well. Furthermore, the primed ideas have some ability to prime other ideas, although more weakly. Like ripples on a pond, activation spreads through a small part of the vast network of associated ideas. The mapping of these ripples is now one of the most exciting pursuits in psychological research.\nThe Ideomotor Effect Another major advance in our understanding of memory was the discovery that priming is not restricted to concepts and words. You cannot know this from conscious experience, of course, but you must accept the alien idea that your actions and your emotions can be primed by events of which you are not even aware.\nIn an experiment that became an instant classic, the psychologist John Bargh and his collaborators asked students at New York University — most aged eighteen to twentytwo — to assemble four-word sentences from a set of five words (for example, “finds he it yellow instantly”). For one group of students, half the scrambled sentences contained words associated with the elderly, such as Florida, forgetful, bald, gray, or wrinkle. When they had completed that task, the young participants were sent out to do another experiment in an office down the hall. That short walk was what the experiment was about. The researchers unobtrusively measured the time it took people to get from one end of the corridor to the other. As Bargh had predicted, the young people who had fashioned a sentence from words with an elderly theme walked down the hallway significantly more slowly than the others.\nReferences for Further Reading"
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/files/Cognitive-Bias-Games.html#bias-4",
    "href": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/files/Cognitive-Bias-Games.html#bias-4",
    "title": "I Think, Therefore I am",
    "section": "Bias #4",
    "text": "Bias #4\nGame\nIntepretation\nReferences for Further Reading"
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/files/Cognitive-Bias-Games.html#conclusions",
    "href": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/files/Cognitive-Bias-Games.html#conclusions",
    "title": "I Think, Therefore I am",
    "section": "Conclusions",
    "text": "Conclusions\nSo what should we do to try to overcome these ( and other ) Cognitive Biases?\n\nBe aware: Awareness is the first step towards overcoming errors in judgement. Keeping in mind the potentially harmful consequences of first impressions is helpful when meeting new people.\n\n\nSlow down: The second step is to deliberately slow down your judgement and any subsequent decisions. For example, never make a recruitment choice straight after the interview.\n\n\nBe systematic: Finally, try to engage your analytical reasoning skills by taking a systematic approach. This sounds trickier than it is. In the context of interviewing, you could prepare a list of essential criteria and force yourself to consider each one carefully before making a choice.\n\nThis helps us to not arrive at flawed ideas to pursue but ones that we are reasonably sure we can justify to….our Field, as conceptualized by Mihaly Csikszentmihalyi."
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/75-TRIZ-Ideal-Final-Result/index.html#ideal-final-result-game",
    "href": "content/courses/TRIZ4ProbSolving/Modules/75-TRIZ-Ideal-Final-Result/index.html#ideal-final-result-game",
    "title": "TRIZ - The Ideal Final Result",
    "section": "Ideal Final Result: Game",
    "text": "Ideal Final Result: Game\nTBD",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "TRIZ - The Ideal Final Result"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/75-TRIZ-Ideal-Final-Result/index.html#discussion",
    "href": "content/courses/TRIZ4ProbSolving/Modules/75-TRIZ-Ideal-Final-Result/index.html#discussion",
    "title": "TRIZ - The Ideal Final Result",
    "section": "Discussion",
    "text": "Discussion\nLike the Queen in Alice in Wonderland, it is important to conceptualize the Ideal Final Result as one of the six impossible things before breakfast. As Jack Hipple says in his book The Ideal Result ( Reference 3), it’s when something performs its function, and does not exist.\nIn this sense, TRIZ is a foretaste of what was said much later after TRIZ was invented, that the best Technologies are those that disappear.",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "TRIZ - The Ideal Final Result"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/75-TRIZ-Ideal-Final-Result/index.html#references",
    "href": "content/courses/TRIZ4ProbSolving/Modules/75-TRIZ-Ideal-Final-Result/index.html#references",
    "title": "TRIZ - The Ideal Final Result",
    "section": "References",
    "text": "References\n\nTitanic Game (PDF)\nStan Kaplan, An Introduction to TRIZ(PDF) This is a simple and short introduction to all aspects of Classical TRIZ.\nJack Hipple, “The Ideal Result: What it is and how to achieve it”, Springer, 2012.",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "TRIZ - The Ideal Final Result"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/100-TRIZ-Laws-of-Evolution/index.html#introduction",
    "href": "content/courses/TRIZ4ProbSolving/Modules/100-TRIZ-Laws-of-Evolution/index.html#introduction",
    "title": "TRIZ - The Laws of Evolution",
    "section": "Introduction",
    "text": "Introduction\n\nLaws of Product Evolution\n8 directions\nRadar Chart in R\nFind Opportunities based on Radar Chart",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "TRIZ - The Laws of Evolution"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/100-TRIZ-Laws-of-Evolution/index.html#laws-of-product-evolution",
    "href": "content/courses/TRIZ4ProbSolving/Modules/100-TRIZ-Laws-of-Evolution/index.html#laws-of-product-evolution",
    "title": "TRIZ - The Laws of Evolution",
    "section": "Laws of Product Evolution",
    "text": "Laws of Product Evolution\n\nLaw of completeness of the system: Systems derive from synthesis of separate parts into a functional system.\nLaw of energy transfer in the system: Shaft, gears, magnetic fields, charged particles, which are the heart of many inventive problems.\nLaw of increasing ideality: Function is created with minimum complexity, which can be considered a ratio of system usefulness to its harmful effects. The ideal system has the desired outputs with no harmful effects, i.e., no machine, just the function(s).\nLaw of harmonization: Transferring energy more efficiently.\nLaw of uneven development of parts: Not all parts evolve at the same pace. The least will limit the overall system\nLaw of transition to a super system: Solution system becomes subsystem of larger system.\nLaw of Transition from Macro to Micro: Using physically smaller solutions, e.g., electronic tubes to chips.\nLaw of Increasing Substance-Field Involvement: Viewing and modeling systems as composed of two substances interacting through a field.",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "TRIZ - The Laws of Evolution"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/100-TRIZ-Laws-of-Evolution/index.html#how-would-you-redesign-this-product",
    "href": "content/courses/TRIZ4ProbSolving/Modules/100-TRIZ-Laws-of-Evolution/index.html#how-would-you-redesign-this-product",
    "title": "TRIZ - The Laws of Evolution",
    "section": "How Would you Redesign this Product",
    "text": "How Would you Redesign this Product\nTBD. Plot a set of Radar charts showing different stages of evolution of well known product lines. Obsolete products are even more interesting.",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "TRIZ - The Laws of Evolution"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/100-TRIZ-Laws-of-Evolution/index.html#references",
    "href": "content/courses/TRIZ4ProbSolving/Modules/100-TRIZ-Laws-of-Evolution/index.html#references",
    "title": "TRIZ - The Laws of Evolution",
    "section": "References",
    "text": "References\n\nProject TETRIS: Chapter 2: Laws of System Evolution (PDF)\nProject TETRIS: Chapter 5: Techniques to Resolve Contradictions / Resources / Effects (PDF)\nProject TETRIS: Examples of inventive problems: Example 1-5 (RAR Archive File)",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "TRIZ - The Laws of Evolution"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/listing.html",
    "href": "content/courses/TRIZ4ProbSolving/listing.html",
    "title": "TRIZ for Problem Solving",
    "section": "",
    "text": "Title\n\n\nReading Time\n\n\n\n\n\n\n\n\n\nI am Water\n\n\n2 min\n\n\n\n\n\n\n\nI am What I yam\n\n\n2 min\n\n\n\n\n\n\n\nBirds of Different Feathers\n\n\n2 min\n\n\n\n\n\n\n\nI Connect therefore I am\n\n\n5 min\n\n\n\n\n\n\n\nI Think, Fast and Slow\n\n\n1 min\n\n\n\n\n\n\n\nThe Art of Parallel Thinking\n\n\n2 min\n\n\n\n\n\n\n\nA Year of Metaphoric Thinking\n\n\n5 min\n\n\n\n\n\n\n\nTRIZ - Problems and Contradictions\n\n\n6 min\n\n\n\n\n\n\n\nTRIZ - The Unreasonable Effectiveness of Available Resources\n\n\n6 min\n\n\n\n\n\n\n\nTRIZ - The Ideal Final Result\n\n\n2 min\n\n\n\n\n\n\n\nTRIZ - A Contradictory Language\n\n\n13 min\n\n\n\n\n\n\n\nTRIZ - The Contradiction Matrix Workflow\n\n\n2 min\n\n\n\n\n\n\n\nTRIZ - The Laws of Evolution\n\n\n2 min\n\n\n\n\n\n\n\nTRIZ - Substance Field Analysis, and ARIZ\n\n\n1 min\n\n\n\n\n\nNo matching items\n\n Back to top",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/10-SummaryStats/index.html",
    "href": "content/courses/NoCode/Modules/10-SummaryStats/index.html",
    "title": "🕶 Summary Statistics",
    "section": "",
    "text": "Jorge Luis Borges, in a fantasy short story published in 1942, “Funes the Memorious,” he described a man, Ireneo Funes, who found after an accident that he could remember absolutely everything. He could reconstruct every day in the smallest detail, and he could even later reconstruct the reconstruction, but he was incapable of understanding. Borges wrote, “To think is to forget details, generalize, make abstractions. In the teeming world of Funes there were only details.”\nAggregation can yield great gains above the individual components. Funes was big data without Statistics."
  },
  {
    "objectID": "content/courses/NoCode/Modules/10-SummaryStats/index.html#whats-the-story-today",
    "href": "content/courses/NoCode/Modules/10-SummaryStats/index.html#whats-the-story-today",
    "title": "🕶 Summary Statistics",
    "section": "",
    "text": "Jorge Luis Borges, in a fantasy short story published in 1942, “Funes the Memorious,” he described a man, Ireneo Funes, who found after an accident that he could remember absolutely everything. He could reconstruct every day in the smallest detail, and he could even later reconstruct the reconstruction, but he was incapable of understanding. Borges wrote, “To think is to forget details, generalize, make abstractions. In the teeming world of Funes there were only details.”\nAggregation can yield great gains above the individual components. Funes was big data without Statistics."
  },
  {
    "objectID": "content/courses/NoCode/Modules/10-SummaryStats/index.html#what-numbers-will-we-see-today",
    "href": "content/courses/NoCode/Modules/10-SummaryStats/index.html#what-numbers-will-we-see-today",
    "title": "🕶 Summary Statistics",
    "section": "\n What numbers will we see today?",
    "text": "What numbers will we see today?\nBefore we plot a single chart, it is wise to take a look at several numbers that summarize the dataset under consideration. What might these be? Some obviously useful numbers are:\n\nDataset length: How many rows/observations?\nDataset breadth: How many columns/variables?\nHow many Quant variables?\nHow many Qual variables?\nQuant variables: min, max, mean, median, sd\nQual variables: levels, counts per level\nBoth: means, medians for each level of a Qual variable…"
  },
  {
    "objectID": "content/courses/NoCode/Modules/10-SummaryStats/index.html#what-kind-of-data-variables-will-we-choose",
    "href": "content/courses/NoCode/Modules/10-SummaryStats/index.html#what-kind-of-data-variables-will-we-choose",
    "title": "🕶 Summary Statistics",
    "section": "\n What kind of Data Variables will we choose?",
    "text": "What kind of Data Variables will we choose?\n\n\n\n\n\n\nNo\nPronoun\nAnswer\nVariable/Scale\nExample\nWhat Operations?\n\n\n\n1\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities, with Scale and a Zero Value.Differences and Ratios /Products are meaningful.\nQuantitative/Ratio\nLength,Height,Temperature in Kelvin,Activity,Dose Amount,Reaction Rate,Flow Rate,Concentration,Pulse,Survival Rate\nCorrelation\n\n\n2\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities with Scale. Differences are meaningful, but not products or ratios\nQuantitative/Interval\npH,SAT score(200-800),Credit score(300-850),SAT score(200-800),Year of Starting College\nMean,Standard Deviation\n\n\n3\nHow, What Kind, What Sort\nA Manner / Method, Type or Attribute from a list, with list items in some \" order\" ( e.g. good, better, improved, best..)\nQualitative/Ordinal\nSocioeconomic status (Low income, Middle income, High income),Education level (HighSchool, BS, MS, PhD),Satisfaction rating(Very much Dislike, Dislike, Neutral, Like, Very Much Like)\nMedian,Percentile\n\n\n4\nWhat, Who, Where, Whom, Which\nName, Place, Animal, Thing\nQualitative/Nominal\nName\nCount no. of cases,Mode\n\n\n\n\n\n\n\nWe will obviously choose all variables in the dataset, unless they are unrelated ones such as row number or ID which we know we don’t want to use."
  },
  {
    "objectID": "content/courses/NoCode/Modules/10-SummaryStats/index.html#how-do-these-summaries-work",
    "href": "content/courses/NoCode/Modules/10-SummaryStats/index.html#how-do-these-summaries-work",
    "title": "🕶 Summary Statistics",
    "section": "\n How do these Summaries Work?",
    "text": "How do these Summaries Work?\nInspecting the min, max,mean, median and sd of each of the Quant variables tells us straightaway what the ranges of the variables are, and if there are some outliers, which could be normal, or maybe due to data entry error! Comparing two Quant variables for their ranges also tells us that we may have to scale/normalize them, if one variable has large numbers and the other has very small ones.\nWith Qual variables, we understand the levels within each, and understand the total number of combinations of the levels across these. Counts across levels, and across combinations of levels tells us whether the data has sufficient readings for graphing, inference, and decision-making, of if certain classes of data are under or over represented.\nFor both types of variables, we need to keep an eye open for data entries that are missing! We will have to take a decision to let go of that entire observation ( i.e. row) or do what is called imputation to fill in values that are based on the other values in the same column."
  },
  {
    "objectID": "content/courses/NoCode/Modules/10-SummaryStats/index.html#obtaining-quant-summaries",
    "href": "content/courses/NoCode/Modules/10-SummaryStats/index.html#obtaining-quant-summaries",
    "title": "🕶 Summary Statistics",
    "section": "\n Obtaining Quant Summaries",
    "text": "Obtaining Quant Summaries\n\n\nUsing Orange\nUsing RAWgraphs\nUsing DataWrapper\n\n\n\nLet us rapidly make some histograms in Orange, so that we know how the tool works here. We start with the iris dataset: Download this Orange workflow file and open it in Orange.\n Download the Histogram Workflow \nYou can see the effect of modifying the bin widths, and of fitting a standard distribution for comparison.\n\n\n\n\n\nhttps://academy.datawrapper.de/article/136-histogram-min-max-median-mean\nDataWrapper does not offer a separate histogram-making tool. Histograms in DataWrapper are available as a part of the data-inspection part of the work flow, as a small thumbnail-sized plot."
  },
  {
    "objectID": "content/courses/NoCode/Modules/10-SummaryStats/index.html#dataset-netflix-original-series",
    "href": "content/courses/NoCode/Modules/10-SummaryStats/index.html#dataset-netflix-original-series",
    "title": "🕶 Summary Statistics",
    "section": "\n Dataset: Netflix Original Series",
    "text": "Dataset: Netflix Original Series\nWe are now ready for a more detailed example. Here is a look at this data on Netflix Original Series. Download it to your machine by clicking on the button below.\n Download the Netflix Dataset \n\n Examine the Data\n\n\n\n\n\nFigure 1: Netflix Data Table\n\n\nFigure 1 states that there are 109 movies, 6 variables in the dataset.\n\n Data Dictionary\n\n\n\n\n\n\nQuantitative Data\n\n\n\n\n\nPremiere_Year: (int) Year the movie premiered\n\nSeasons: (int) No. of Seasons\n\nEpisodes: (int) No. of Episodes\n\nIMDB_Rating: (int) IMDB Rating!!\n\n\n\n\n\n\n\n\n\nQualitative Data\n\n\n\n\n\nGenere: (chr) types of Genres\n\nTitle: (chr) 109 titles\n\nSubgenre: (chr) types of sub-Genres\n\nStatus: (chr) status on Netflix\n\n\n\n\n Research Questions\nLet’s try a few questions and see if they are answerable with Histograms.\n\n\n\n\n\n\nNote\n\n\n\nQ1. What is the distribution of IMDB_Rating? If we split/colour by movie Genere?\n\n\n\n\n\n\n\n\n\n(a) IMDB Ratings Histogram\n\n\n\n\n\n\n\n\n\n(b) IMDB Rating vs Genere\n\n\n\n\n\n\nFigure 2: Netflix Data Histograms\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQ2. Are IMDB_Rating affected by the number of Seasons or Episodes?\n\n\n\n\n\n\n\n\n\n(a) Reformatting “Seasons”\n\n\n\n\n\n\n\n\n\n(b) IMDB Rating vs Seasons\n\n\n\n\n\n\nFigure 3: Plotting with Seasons\n\n\nWe first need to reformat the Seasons variable from N to C in the data file view. This converts it to Qual. Then we split the IMDB histogram by this new variable.\n\n\n\n What is the Story Here?\nMost movies have decent IMDB scores; the distribution is left-skewed. Some of course have been trashed!! Splitting IMDBRating by Genere is not too illuminating…\nNot much wisdom to be gleaned either from splitting IMDBRating by Seasons…"
  },
  {
    "objectID": "content/courses/NoCode/Modules/10-SummaryStats/index.html#dataset-the-old-faithful-geyser-in-the-usa",
    "href": "content/courses/NoCode/Modules/10-SummaryStats/index.html#dataset-the-old-faithful-geyser-in-the-usa",
    "title": "🕶 Summary Statistics",
    "section": "\n Dataset: the Old Faithful geyser in the USA",
    "text": "Dataset: the Old Faithful geyser in the USA\nHere is a dataset about the eruption durations, and wait times between eruptions of the Old Faithful geyser in Yellowstone National Park, USA.\nDownload this data to your machine and import it into Orange.\n Download the Geyser Dataset \n\n Examine the Data\n\n\n\n\n\nFigure 4: Old Faithful Data Table\n\n\nFigure 4 states that we have 272 data points, and three variables. All variables are Quantitative!\n\n Data Dictionary\n\n\n\n\n\n\nQuantitative Data\n\n\n\n\n\neruptions: (dbl Duration Times of Eruptions\n\nwaiting: (dbl) Waiting Times between Eruptions\n\ndensity: (dbl) (Ignore this for now)\n\n\n\n\n\n\n\n\n\nQualitative Data\n\n\n\n\nNo Qual variables!!\n\n\n\n\n Research Questions\n\n\n\n\n\n\nNote\n\n\n\nQ1. How are eruptions (durations) and waiting (times) distributed?\n\n\n\n\n\n\n\n\n\n(a) Eruption Durations Histogram\n\n\n\n\n\n\n\n\n\n(b) Waiting Times Histogram\n\n\n\n\n\n\nFigure 5: Old Faithful Data Histograms\n\n\n\n\n\n What is the Story Here?\n\nBoth durations have a “double-humped” distribution…\nThere are therefore two distinct ranges for both durations.\nAre there two different mechanisms at work in the geyser, that randomly kick in?"
  },
  {
    "objectID": "content/courses/NoCode/Modules/10-SummaryStats/index.html#your-turn",
    "href": "content/courses/NoCode/Modules/10-SummaryStats/index.html#your-turn",
    "title": "🕶 Summary Statistics",
    "section": "\n Your Turn",
    "text": "Your Turn\nTry your hand at these datasets. Look at the data table, state the data dictionary, contemplate a few Research Questions and answer them with graphs in Orange!\n\nAirbnb Price Data on the French Riviera\n\n\n\n\n Airbnb data\n\n\n\n1. Wage and Education Data from Canada\n Download the Wages/Education Dataset"
  },
  {
    "objectID": "content/courses/NoCode/Modules/10-SummaryStats/index.html#wait-but-why",
    "href": "content/courses/NoCode/Modules/10-SummaryStats/index.html#wait-but-why",
    "title": "🕶 Summary Statistics",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nHistograms are used to study the distribution of one or a few variables. Checking the distribution of your variables one by one is probably the first task you should do when you get a new dataset. It delivers a good quantity of information. Several distribution shapes exist, here is an illustration of the 6 most common ones:\n\n\n\n\n\n\n\n\n\nIn your Design-Project-related research, you will collect data from or about your target audience. The Quantitative parts of that data may obtain with any of these distributions. Inspecting these may give you an insight into the population of your target audience, something that may likely be true, a hunch, which you could verify and convert into …opportunity."
  },
  {
    "objectID": "content/courses/NoCode/Modules/10-SummaryStats/index.html#references",
    "href": "content/courses/NoCode/Modules/10-SummaryStats/index.html#references",
    "title": "🕶 Summary Statistics",
    "section": "\n References",
    "text": "References\n\nSee the scrolly animation for a histogram at this website: Exploring Histograms, an essay by Aran Lunzer and Amelia McNamara https://tinlizzie.org/histograms/?s=09\nhttps://www.data-to-viz.com/graph/histogram.html\n\n\n\n\nFigure 1: Netflix Data Table\nFigure 2 (a): IMDB Ratings Histogram\nFigure 2 (b): IMDB Rating vs Genere\nFigure 3 (a): Reformatting “Seasons”\nFigure 3 (b): IMDB Rating vs Seasons\nFigure 4: Old Faithful Data Table\nFigure 5 (a): Eruption Durations Histogram\nFigure 5 (b): Waiting Times Histogram"
  },
  {
    "objectID": "content/courses/NoCode/Modules/28-BoxPlotsViolins/index.html",
    "href": "content/courses/NoCode/Modules/28-BoxPlotsViolins/index.html",
    "title": "🐢 Box Plots",
    "section": "",
    "text": "Variable #1\nVariable #2\nChart Names\nChart Shape\n\n\nQuant\nNone\nBox-Whisker Plots and Violin Plots\n\n\n\n\n\n\n\n\n\nNo\nPronoun\nAnswer\nVariable/Scale\nExample\nWhat Operations?\n\n\n\n2\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities with Scale. Differences are meaningful, but not products or ratios\nQuantitative/Interval\npH,SAT score(200-800),Credit score(300-850),SAT score(200-800),Year of Starting College\nMean,Standard Deviation\n\n\n3\nHow, What Kind, What Sort\nA Manner / Method, Type or Attribute from a list, with list items in some \" order\" ( e.g. good, better, improved, best..)\nQualitative/Ordinal\nSocioeconomic status (Low income, Middle income, High income),Education level (HighSchool, BS, MS, PhD),Satisfaction rating(Very much Dislike, Dislike, Neutral, Like, Very Much Like)\nMedian,Percentile",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🐢 Box Plots"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/28-BoxPlotsViolins/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/NoCode/Modules/28-BoxPlotsViolins/index.html#what-graphs-will-we-see-today",
    "title": "🐢 Box Plots",
    "section": "",
    "text": "Variable #1\nVariable #2\nChart Names\nChart Shape\n\n\nQuant\nNone\nBox-Whisker Plots and Violin Plots\n\n\n\n\n\n\n\n\n\nNo\nPronoun\nAnswer\nVariable/Scale\nExample\nWhat Operations?\n\n\n\n2\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities with Scale. Differences are meaningful, but not products or ratios\nQuantitative/Interval\npH,SAT score(200-800),Credit score(300-850),SAT score(200-800),Year of Starting College\nMean,Standard Deviation\n\n\n3\nHow, What Kind, What Sort\nA Manner / Method, Type or Attribute from a list, with list items in some \" order\" ( e.g. good, better, improved, best..)\nQualitative/Ordinal\nSocioeconomic status (Low income, Middle income, High income),Education level (HighSchool, BS, MS, PhD),Satisfaction rating(Very much Dislike, Dislike, Neutral, Like, Very Much Like)\nMedian,Percentile",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🐢 Box Plots"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/28-BoxPlotsViolins/index.html#inspiration",
    "href": "content/courses/NoCode/Modules/28-BoxPlotsViolins/index.html#inspiration",
    "title": "🐢 Box Plots",
    "section": "\n Inspiration",
    "text": "Inspiration\n\n\n\n\n\nFigure 1: Box Plot Inspiration\n\n\nAlice said, “I say what I mean and I mean what I say!” Are the rest of us so sure? What do we mean when we use any of the phrases above? How definite are we? There is a range of “sureness” and “unsureness”…and this is where we can use box plots like Figure 1 to show that range of opinion.\nMaybe it is time for a box plot on shades of meaning for Gen-Z phrases!",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🐢 Box Plots"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/28-BoxPlotsViolins/index.html#how-do-these-charts-work",
    "href": "content/courses/NoCode/Modules/28-BoxPlotsViolins/index.html#how-do-these-charts-work",
    "title": "🐢 Box Plots",
    "section": "\n How do these Chart(s) Work?",
    "text": "How do these Chart(s) Work?\nBox Plots are an extremely useful data visualization that gives us an idea of the distribution of a Quant variable, for each level of another Qual variable. The internal process of this plot is as follows:\n\nmake groups of the Quant variable for each level of the Qual\nin each group, rank the Quant variable values in increasing order\nCalculate: median, IQR, outliers\n\nplot these as a vertical or horizontal box structure\n\nThe box can also be asymmetric “half plots” if needed…\n\n\n\n\n\n\nHistograms and Box Plots\n\n\n\nNote how the histogram that dwells upon the mean and standard deviation, whereas the boxplot focuses on the median and quartiles. The former uses the values of the Quant variable, whereas the latter uses their sequence number or ranks.\n\n\nBox plots are often used for example in HR operations to understand Salary distributions across grades of employees. Marks of students in competitive exams are also declared using Quartiles.\n\n\n\n\n\nFigure 2: Box Plot and Density\n\n\nIn the Figure 2, the boxplot is compared with a density plot, which shows a symmetric normal density. Since the latter is symmetric, the median and the mean are identical, as seen by the correspondence with the boxplot in the figure above.\n\n\n\n\n\n\n\n\n\n(a) Box Plot and Skewness\n\n\n\n\n\n\n\n\n\n(b) Density and Skewness\n\n\n\n\n\n\nFigure 3: Box Plot Discussions\n\n\nIn the Figure 3 (a), we see the difference between boxplots that show symmetric and skewed distributions. The “lid” and the “bottom” of the box are not of similar width in distributions with significant skewness.\nCompare these with the corresponding Figure 3 (b).",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🐢 Box Plots"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/28-BoxPlotsViolins/index.html#creating-box-plots",
    "href": "content/courses/NoCode/Modules/28-BoxPlotsViolins/index.html#creating-box-plots",
    "title": "🐢 Box Plots",
    "section": "\n Creating Box Plots",
    "text": "Creating Box Plots\n\n\nUsing Orange\nUsing RAWgraphs\nUsing DataWrapper\n\n\n\nHere is the BoxPlot Widget description.\nLet us first plot a set of boxplots for the familiar iris dataset and then investigate other datasets using the same Orange workflow.\n Download the BoxPlot Workflow file \n\n\n\n\n\nFigure 4: Iris Box Plot\n\n\nFigure 4 shows the three horizontal box-plots for the chosen Quant variable, one for each level of iris(species). The IQR is also shown for each fo the groups. One can selectively compare either medians or means across these groups of measurements.\n\n\nhttps://youtu.be/Cax0cQ6caI8\n\n\nThere does not seem to be a way of creating Box Plots in DataWrapper .",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🐢 Box Plots"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/28-BoxPlotsViolins/index.html#dataset-salaries-in-academia",
    "href": "content/courses/NoCode/Modules/28-BoxPlotsViolins/index.html#dataset-salaries-in-academia",
    "title": "🐢 Box Plots",
    "section": "\n Dataset: Salaries in Academia",
    "text": "Dataset: Salaries in Academia\nLet us examine this dataset in Orange.\n Download the Salaries data \n\n Examine the Data\n\n\n\n\n\nFigure 5: Salaries Data Table\n\n\nFigure 5 states that there are 397 teachers, with 6 variables in the dataset.\n\n\n\n\n\nFigure 6: SalariesData Table\n\n\n\n Data Dictionary\n\n\n\n\n\n\nQuantitative Data\n\n\n\n\n\nsalary: (int) (Annual) Salary!\n\nyrs_service: (int) No. of Years they have served as teachers\n\nyrs_since_phd: (int) No. of Years after their PhD. (sigh)\n\n\n\n\n\n\n\n\n\nQualitative Data\n\n\n\n\n\ndiscipline: (chr) Nature of Expertise\n\nrank: (chr) Nature of Appointment\n\nsex: (chr) Male / Female. Note the imbalance in the counts!!\n\n\n\n\n\n\n\n\n\nQual and Quant…\n\n\n\nCan any of the Quant variables be thought of as Quant variables? When, under what circumstances?\n\n\n\n Research Questions\nLet’s try a few questions and see if they are answerable with Box Plots and Violins\n\n\n\n\n\n\nQuestion\n\n\n\nQ1. What is the distribution of salary? If we split by sex?\n\n\n\n\n\n\n\n\n\n(a) Salaries Box Plot\n\n\n\n\n\n\n\n\n\n(b) Salaries Box Plot by Sex\n\n\n\n\n\n\nFigure 7: Salaries Data Box Plots\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ2. What is the distribution of salary, when we split by other Qual variables, such as rank?\n\n\n\n\n\nFigure 8: Salaries Box Plot by Sex\n\n\n\n\n\n What is the Story Here?\nSalaries have quite a wide distribution with some very highly paid individuals ( ~ $240K), while the median is still $107K. So some people are paid than 2X the median!\nWhen split by sex, we get two box plots that show the differences between group salaries. The means and medians are quite different between the two groups, an important inference that needs to be completely verified by a statistical t-test.\nWhen split by rank, we get three box plots that show the differences between group salaries, again an important inference that needs to be completely verified by a statistical ANOVA test.",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🐢 Box Plots"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/28-BoxPlotsViolins/index.html#are-the-differences-significant",
    "href": "content/courses/NoCode/Modules/28-BoxPlotsViolins/index.html#are-the-differences-significant",
    "title": "🐢 Box Plots",
    "section": "\n Are the Differences Significant?",
    "text": "Are the Differences Significant?\n\n\n\n\n\n\nHunches and Hypotheses\n\n\n\nIn data analysis, we always want to know1, as in life, how important things are, whether they matter. To do this, we make up hunches, or more precisely, Hypotheses. We make two in fact:\n\\(H_0\\): Nothing is happening;\\(H_a\\): (“a” for Alternate): Something is happening and it is important enough to pay attention to.\nWe then pretend that \\(H_0\\) is true and ask that our data prove us wrong; if it does, we reject \\(H_0\\) in favour of \\(H_a\\).\nWe will use this very common idea of Hypothesis Testing again in this course.\n\n\n\n\n\n\n\n\nt-test for two categories\n\n\n\nWhen comparing mean salaries vs sex in Figure 7 (b), note the annotation below the graph. This is the result of the t-test:\n\\[ Student's ~ t: 3.198~(p = 0.002, ~ N = 397) \\]\nThis indicates several things:\n\nThat the t-statistic is 3.198;\nIf we assume sex makes no difference to salary, then the probability that this difference could arise merely by chance is low \\(p = 0.002\\);\nAnd of course that there \\(397\\) data points to vouch for this estimate.\n\nThe test states that this difference is statistically significant and could be used to justify further actions based upon it. Look at the references below to get a fascinating history of statistical testing and its origins in …beer.\n\n\n\n\n\n\n\n\nANOVA test for more than 2 levels\n\n\n\nNow observe the boxplots and annotations in Figure 8, where again we compare mean salaries vs rank. This is the result of the ANOVA-test:\n\\[ ANOVA: ~ 128.217~~(p = 0.000, ~ N = 397) \\]\nThis indicates several things:\n\nThat the ANOVA F-statistic is 128.217;\nIf we assume rank makes no difference to salary, then the probability that this difference could arise merely by chance is negligible \\(p = 0.000\\);\nAnd again that there \\(397\\) data points to vouch for this estimate.\n\nThe ANOVA test states that the (multiple) differences are statistically significant and could be used to justify further actions based upon it.\n\n\n\n\n\n\n\n\n ANOVA for the Cat-egorically Curious\n\n\n\nFor the intrepid, here is a brief, diagrammed, hand-calculated, and intuitive walk-through of ANOVA. Note that the t-test and ANOVA are identical tests, the former being used for 2-level comparisons of means, and the latter for comparisons of more than 2 means. Again, means, not medians.",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🐢 Box Plots"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/28-BoxPlotsViolins/index.html#your-turn",
    "href": "content/courses/NoCode/Modules/28-BoxPlotsViolins/index.html#your-turn",
    "title": "🐢 Box Plots",
    "section": "\n Your Turn",
    "text": "Your Turn\nHere are a couple of datasets that you might want to analyze with box plots, and even perform t-tests and ANOVA-tests:\n\nInsurance Data\n\n Download the Insurance data \n\nPolitical Donations\n\n Download the Donations data \n\nUFO Encounters The data dictionary for this dataset is here at the TidyTuesday Website.\n\n\n(The TidyTuesday Website is a treasure trove of interesting datasets!)\n\n\n UFO Sighting data",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🐢 Box Plots"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/28-BoxPlotsViolins/index.html#wait-but-why",
    "href": "content/courses/NoCode/Modules/28-BoxPlotsViolins/index.html#wait-but-why",
    "title": "🐢 Box Plots",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nBox plots are a powerful statistical graphic that give us a combined view of data ranges, quartiles, medians, and outliers.\nBox plots can compare groups within our Quant variable, based on levels of a Qual variable. This is a very common and important task in research! In your design research, you would have numerical Quant data that is accompanied by categorical Qual data pertaining to your target audience. Analyzing for differences in the Quant across levels of the Qual (e.g household expenditure across groups of people) is a vital step in justifying time, effort, and money for further actions in your project. Don’t faff this.\nThey are ideal for visualizing statistical tests for difference in mean values across groups (t-test and ANOVA).",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🐢 Box Plots"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/28-BoxPlotsViolins/index.html#references",
    "href": "content/courses/NoCode/Modules/28-BoxPlotsViolins/index.html#references",
    "title": "🐢 Box Plots",
    "section": "\n References",
    "text": "References\n\nBevans, R. (2023, June 22). An Introduction to t Tests | Definitions, Formula and Examples. Scribbr. https://www.scribbr.com/statistics/t-test/\nBrown, Angus. (2008). The Strange Origins of the t-test. Physiology News | No. 71 | Summer 2008| https://static.physoc.org/app/uploads/2019/03/22194755/71-a.pdf\nStephen T. Ziliak.(2008). Guinnessometrics: The Economic Foundation of “Student’s” t. Journal of Economic Perspectives—Volume 22, Number 4—Fall 2008—Pages 199–216. https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.22.4.199\n\n\n\n\nFigure 1: Box Plot Inspiration\nFigure 2: Box Plot and Density\nFigure 3 (a): Box Plot and Skewness\nFigure 3 (b): Density and Skewness\nFigure 4: Iris Box Plot\nFigure 5: Salaries Data Table\nFigure 6: SalariesData Table\nFigure 7 (a): Salaries Box Plot\nFigure 7 (b): Salaries Box Plot by Sex\nFigure 8: Salaries Box Plot by Sex",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🐢 Box Plots"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/28-BoxPlotsViolins/index.html#footnotes",
    "href": "content/courses/NoCode/Modules/28-BoxPlotsViolins/index.html#footnotes",
    "title": "🐢 Box Plots",
    "section": "Footnotes",
    "text": "Footnotes\n\n“Ah, Misha, he has a stormy spirit. His mind is in bondage. He is haunted by a great, unsolved doubt. He is one of those who don’t want millions, but an answer to their questions.” ― Fyodor Dostoevsky, The Brothers Karamazov: A Novel in Four Parts With Epilogue↩︎",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🐢 Box Plots"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/30-ScatterPlots/index.html",
    "href": "content/courses/NoCode/Modules/30-ScatterPlots/index.html",
    "title": "🏃 Scatter Plots",
    "section": "",
    "text": "Variable #1\nVariable #2\nChart Names\nChart Shape\n\n\nQuant\nQuant\nScatter Plot",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🏃 Scatter Plots"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/30-ScatterPlots/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/NoCode/Modules/30-ScatterPlots/index.html#what-graphs-will-we-see-today",
    "title": "🏃 Scatter Plots",
    "section": "",
    "text": "Variable #1\nVariable #2\nChart Names\nChart Shape\n\n\nQuant\nQuant\nScatter Plot",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🏃 Scatter Plots"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/30-ScatterPlots/index.html#what-kind-of-data-variables-will-we-choose",
    "href": "content/courses/NoCode/Modules/30-ScatterPlots/index.html#what-kind-of-data-variables-will-we-choose",
    "title": "🏃 Scatter Plots",
    "section": "\n What kind of Data Variables will we choose?",
    "text": "What kind of Data Variables will we choose?\n\n\n\n\n\n\nNo\nPronoun\nAnswer\nVariable/Scale\nExample\nWhat Operations?\n\n\n1\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities, with Scale and a Zero Value.Differences and Ratios /Products are meaningful.\nQuantitative/Ratio\nLength,Height,Temperature in Kelvin,Activity,Dose Amount,Reaction Rate,Flow Rate,Concentration,Pulse,Survival Rate\nCorrelation",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🏃 Scatter Plots"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/30-ScatterPlots/index.html#inspiration",
    "href": "content/courses/NoCode/Modules/30-ScatterPlots/index.html#inspiration",
    "title": "🏃 Scatter Plots",
    "section": "\n Inspiration",
    "text": "Inspiration\n\n\n\n\n\nFigure 1: ScatterPlot Inspiration http://www.calamitiesofnature.com/archive/?c=559\n\n\nDoes belief in Evolution depend upon the GSP of of the country? Where is the US in all of this? Does the Bible Belt tip the scales here?\nAnd India?",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🏃 Scatter Plots"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/30-ScatterPlots/index.html#how-do-these-charts-work",
    "href": "content/courses/NoCode/Modules/30-ScatterPlots/index.html#how-do-these-charts-work",
    "title": "🏃 Scatter Plots",
    "section": "\n How do these Chart(s) Work?",
    "text": "How do these Chart(s) Work?\nScatter Plots take two separate Quant variables as inputs. Each of the variables is mapped to a position, or coordinate: one for the X-axis, and the other for the Y-axis. Each pair of observations from the two Quant variables ( which would be in one row!) give us a point in the Scatter Plot.\nLooking at these clouds of points gives us an intuitive sense of the relationship between the two Quant variables, how one varies with the other. A cloud that slopes upward to the right indicates a positive relationship between the two; a cloud that slopes down to the right indicates a negative one. An amorphous cloud that does not discernibly slope in either way would lead us to infer that there is little or no relationship between the variables.\n\n\n\n\n\n\nSlope and the Correlation Coefficient are Related\n\n\n\nUnder the assumption of a linear relationship between the two Quant variables, we plot a straight trend line, or regression line through the cloud of points, as a line that best represents that linear relationship. The slope of the regression line is directly linked to the Pearson Correlation Coefficient between the two variables.",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🏃 Scatter Plots"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/30-ScatterPlots/index.html#plotting-a-scatter-plot",
    "href": "content/courses/NoCode/Modules/30-ScatterPlots/index.html#plotting-a-scatter-plot",
    "title": "🏃 Scatter Plots",
    "section": "\n Plotting a Scatter Plot",
    "text": "Plotting a Scatter Plot\n\n\nUsing Orange\nUsing RAWgraphs\nUsing DataWrapper\n\n\n\nWe can use the now (overly) familiar iris dataset to plot our first scatter plot. Download the workflow file below:\n Download the Scatter Plot Workflow \n\n\n\n\n\nFigure 2: Iris Scatter Plot\n\n\n\nTry setting shapes and colours, and try plotting a “regression line”. Do you get one line, or several? Why, or why not? How can you switch between the two “methods”?\nTry other pairs of Quant variables in the dataset.\nWhich plot is the most informative? Why?\n\n\n\nhttps://youtu.be/FafXHMmau1M\n\n\nhttps://academy.datawrapper.de/article/65-how-to-create-a-scatter-plot",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🏃 Scatter Plots"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/30-ScatterPlots/index.html#what-is-the-story-here",
    "href": "content/courses/NoCode/Modules/30-ScatterPlots/index.html#what-is-the-story-here",
    "title": "🏃 Scatter Plots",
    "section": "What is the Story here?",
    "text": "What is the Story here?\n\nThere are three species of iris flowers and they are “separable” based on combinations of their quantitative measurements.\nSome pairs of Quant variables create Scatter Plots that are quite disjoint and allow easy identification of the species variable.\nIn a ML model for this dataset, the species variable is most likely to be the target variable while the rest are predictors.",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🏃 Scatter Plots"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/30-ScatterPlots/index.html#dataset-cancer",
    "href": "content/courses/NoCode/Modules/30-ScatterPlots/index.html#dataset-cancer",
    "title": "🏃 Scatter Plots",
    "section": "\n Dataset: Cancer",
    "text": "Dataset: Cancer\nLet us examine a fairly complex dataset pertaining to cancer, and analyze that with scatter plots.\n Download the Cancer dataset \nWe can use the same Workflow as before.\n\n Examine the Data\n\n\n\n\n\nFigure 3: Cancer Data Table\n\n\nFrom Figure 3, we see that there is one Qual column Diagnosis, and all the remaining 31 columns seem to be some Quant measurements of a total of 569 tumours. (Not all columns are visible)\n\n\n\n\n\nFigure 4: Cancer Data Summary Table #1\n\n\nFigure 4 gives is histograms and statistics of all the 32 columns. Most histograms seem roughly symmetric, but a detailed look must be taken.\n\n\n\n\n\nFigure 5: Cancer Data Summary Table #2\n\n\nIn Figure 5, we see that there is some imbalance between the counts for the one Qual variable, Diagnosis.\n\n Data Dictionary\n\n\n\n\n\n\nQuantitative Data\n\n\n\n\n\n“Id”\n\n\n\n“Radius (mean)”\n“Texture (mean)”\n\n\n“Perimeter (mean)”\n“Area (mean)”\n\n\n“Smoothness (mean)”\n“Compactness (mean)”\n\n\n“Concavity (mean)”\n“Concave points (mean)”\n\n\n“Symmetry (mean)”\n“Fractal dimension (mean)”\n\n\n“Radius (se)”\n“Texture (se)”\n\n\n“Perimeter (se)”\n“Area (se)”\n\n\n“Smoothness (se)”\n“Compactness (se)”\n\n\n“Concavity (se)”\n“Concave points (se)”\n\n\n“Symmetry (se)”\n“Fractal dimension (se)”\n\n\n“Radius (worst)”\n“Texture (worst)”\n\n\n“Perimeter (worst)”\n“Area (worst)”\n\n\n“Smoothness (worst)”\n“Compactness (worst)”\n\n\n“Concavity (worst)”\n“Concave points (worst)”\n\n\n“Symmetry (worst)”\n“Fractal dimension (worst)”\n\n\n\n\n\nMany of the Quant variables seem to be mean measurements, with the mean presumably taken over several “sites” within the same tumour.\nAlong with the mean, there are also measurements of se or standard error which is, roughly speaking, a measure of the standard deviation of the multiple measurements made. So for instance, Area(mean) and Area(se) are pairs of measurements created using multiple “sites” or “cross-sections” on one tumour.\nSome other variables are labelled as worst, which may be either the max or min of such a set of “multi-site” tumour measurements.\n\n\n\n\n\n\n\nMay the (data) Source be with you\n\n\n\nIt is important to note that these are (educated?) guesses; one is best off connecting with the person/agency that provided the data for a precise understanding of variables. This will prevent nonsensical plots/models and inferences from showing up in your work.\n\n\n\n\n\n\n\n\nQualitative Data\n\n\n\n\n\nDiagnosis: (text) (B)enign, or (M)alignant\n\n\n\n\n Research Questions\n\n\n\n\n\n\nQuestion\n\n\n\nQ1. Are the mean and se observations correlated, for a particular variable?\n\n\n\n\n\n\n\n\n\n(a) Cancer Scatter Plot #1\n\n\n\n\n\n\n\n\n\n(b) Cancer Scatter Plot #2\n\n\n\n\n\n\n\n\n\n\n\n(c) Cancer Scatter Plot #3\n\n\n\n\n\n\n\n\n\n(d) Cancer Scatter Plot #4\n\n\n\n\n\n\nFigure 6\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ2. Are the mean and worst observations correlated, for a particular variable?\n\n\n\n\n\n\n\n\n\n(a) Cancer Scatter Plot #5\n\n\n\n\n\n\n\n\n\n(b) Cancer Scatter Plot #6\n\n\n\n\n\n\n\n\n\n\n\n(c) Cancer Scatter Plot #7\n\n\n\n\n\n\n\n\n\n(d) Cancer Scatter Plot #8\n\n\n\n\n\n\nFigure 7\n\n\n\n\n\n What is the Story Here?\nFrom Figure 7 (a), we see that the area(mean) and area(se) are somewhat correlated; moreover the correlation is slightly higher for the malignant tumours ( red dots, appropriately…). This trend shows up also for radius in Figure 7 (b), and for fractaldimension in Figure 7 (d). However, for smoothness, we see much lower correlation {#fig-cancer-smoothness-mean-se}.\nFor the mean vs worst scatter plots, we see decent correlations all around, with each of the graphs showing clouds tilted upward to the right.\n\n\n\n\n\n\nSimpson’s Paradox\n\n\n\nTry to remove colours and then plot a regression line. This usually gives a more clear idea of the correlation, without running into problems such as the Simpson’s Paradox:\n\n\n\n\n\nFigure 8: Simpson’s Paradox",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🏃 Scatter Plots"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/30-ScatterPlots/index.html#your-turn",
    "href": "content/courses/NoCode/Modules/30-ScatterPlots/index.html#your-turn",
    "title": "🏃 Scatter Plots",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nTry to play this online Correlation Game\nTry this dataset on School Expenditure and Grades.\n\n Download the School Data \n\nGas Prices and Consumption, described here. Note the log-transformed Quant data…why do you reckon this was done in the data set itself?\n\n Download the Gas Consumption Data \n\nHorror Movies ( oh, you awful people..)\n\n Download the Horrow Movie Data",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🏃 Scatter Plots"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/30-ScatterPlots/index.html#wait-but-why",
    "href": "content/courses/NoCode/Modules/30-ScatterPlots/index.html#wait-but-why",
    "title": "🏃 Scatter Plots",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nScatter Plots, when they show “linear” clouds, tell us that there is some relationship between two Quant variables we have just plotted\nIf so, then if one is the target variable you are trying to design for, then the other independent, or controllable, variable is something you might want to design with.\nAlways, always, plot and test your data! Both numerical summaries and graphical summaries are necessary! See below!!\n\n\n\n\n\n\n\nAnd How about these datasets?\n\n\n\n\n\n\n\n\ndataset\nmean_x\nmean_y\nstd_dev_x\nstd_dev_y\ncorr_x_y\n\n\n\naway\n54.26610\n47.83472\n16.76983\n26.93974\n-0.0641284\n\n\nbullseye\n54.26873\n47.83082\n16.76924\n26.93573\n-0.0685864\n\n\ncircle\n54.26732\n47.83772\n16.76001\n26.93004\n-0.0683434\n\n\ndino\n54.26327\n47.83225\n16.76514\n26.93540\n-0.0644719\n\n\ndots\n54.26030\n47.83983\n16.76774\n26.93019\n-0.0603414\n\n\nh_lines\n54.26144\n47.83025\n16.76590\n26.93988\n-0.0617148\n\n\nhigh_lines\n54.26881\n47.83545\n16.76670\n26.94000\n-0.0685042\n\n\nslant_down\n54.26785\n47.83590\n16.76676\n26.93610\n-0.0689797\n\n\nslant_up\n54.26588\n47.83150\n16.76885\n26.93861\n-0.0686092\n\n\nstar\n54.26734\n47.83955\n16.76896\n26.93027\n-0.0629611\n\n\nv_lines\n54.26993\n47.83699\n16.76996\n26.93768\n-0.0694456\n\n\nwide_lines\n54.26692\n47.83160\n16.77000\n26.93790\n-0.0665752\n\n\nx_shape\n54.26015\n47.83972\n16.76996\n26.93000\n-0.0655833\n\n\n\n\n\n\n\n\n\n\n\n\nYes, you did want to plot these in Orange, didn’t you? Here is the data then!!\n\n\n DataSaurus Dirty Dozen\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\nCan selling more ice-cream make people drown?\nUse your head about pairs of variables. Do not fall into this trap)",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🏃 Scatter Plots"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/30-ScatterPlots/index.html#references",
    "href": "content/courses/NoCode/Modules/30-ScatterPlots/index.html#references",
    "title": "🏃 Scatter Plots",
    "section": "\n References",
    "text": "References\n\nRohrer JM. Thinking Clearly About Correlations and Causation: Graphical Causal Models for Observational Data. Advances in Methods and Practices in Psychological Science. 2018;1(1):27-42. https://doi.org/10.1177/2515245917745629 PDF\nCase Study on Horror Movies https://notawfulandboring.blogspot.com/2024/04/using-pulse-rates-to-determine-scariest.html\nThe Datasaurus Package: https://cran.r-project.org/web/packages/datasauRus/vignettes/Datasaurus.html\nA superb web-scrolly on Sustainable Development Goals (SDGs)s! Go and see!!https://datatopics.worldbank.org/sdgatlas/goal-1-no-poverty?lang=en\nHunter, W. G. (1981). Six Statistical Tales. The Statistician, 30(2), 107. doi:10.2307/2987563. https://sci-hub.ru/10.2307/2987563\n\n\n\n\nFigure 1: ScatterPlot Inspiration http://www.calamitiesofnature.com/archive/?c=559\nFigure 2: Iris Scatter Plot\nFigure 3: Cancer Data Table\nFigure 4: Cancer Data Summary Table #1\nFigure 5: Cancer Data Summary Table #2\nFigure 6 (a): Cancer Scatter Plot #1\nFigure 6 (b): Cancer Scatter Plot #2\nFigure 6 (c): Cancer Scatter Plot #3\nFigure 6 (d): Cancer Scatter Plot #4\nFigure 7 (a): Cancer Scatter Plot #5\nFigure 7 (b): Cancer Scatter Plot #6\nFigure 7 (c): Cancer Scatter Plot #7\nFigure 7 (d): Cancer Scatter Plot #8\nFigure 8: Simpson’s Paradox",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🏃 Scatter Plots"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/35-MosaicCharts/files/mosaic-tutorial.html",
    "href": "content/courses/NoCode/Modules/35-MosaicCharts/files/mosaic-tutorial.html",
    "title": "Mosaic Chart Step by Step Tutorial",
    "section": "",
    "text": "library(\"vcd\")\nlibrary(\"scatterplot3d\")\nlibrary(colorspace)\n## random seed\nmyseed &lt;- 1071\nset.seed(myseed)"
  },
  {
    "objectID": "content/courses/NoCode/Modules/35-MosaicCharts/files/mosaic-tutorial.html#data",
    "href": "content/courses/NoCode/Modules/35-MosaicCharts/files/mosaic-tutorial.html#data",
    "title": "Mosaic Chart Step by Step Tutorial",
    "section": "Data",
    "text": "Data\n\ndata(\"Hospital\")\ndata(\"Arthritis\")\nart &lt;- xtabs(~ Treatment + Improved, data = Arthritis, subset = Sex == \"Female\")\nnames(dimnames(art))[2] &lt;- \"Improvement\"\n\n\ndata(\"UCBAdmissions\")\nnames(dimnames(UCBAdmissions)) &lt;- c(\"Admission\", \"Gender\", \"Department\")\n\n\ndata(\"Punishment\")\npunish &lt;- xtabs(Freq ~ memory + attitude + age + education, data = Punishment)\ndimnames(punish)[[3]][3] &lt;- \"40+\""
  },
  {
    "objectID": "content/courses/NoCode/Modules/35-MosaicCharts/files/mosaic-tutorial.html#inference",
    "href": "content/courses/NoCode/Modules/35-MosaicCharts/files/mosaic-tutorial.html#inference",
    "title": "Mosaic Chart Step by Step Tutorial",
    "section": "Inference",
    "text": "Inference\n\nset.seed(myseed)\nart_max &lt;- coindep_test(art, n = 5000)\n\nset.seed(myseed)\nucb_max &lt;- coindep_test(aperm(UCBAdmissions, c(3,2,1)), margin = \"Department\", n = 5000)\n\n\nset.seed(myseed)\nart_chisq &lt;- coindep_test(art, n = 5000, indepfun = function(x) sum(x^2))\n\n\nset.seed(myseed)\nhos_chisq &lt;- coindep_test(Hospital, n = 5000, indepfun = function(x) sum(x^2))"
  },
  {
    "objectID": "content/courses/NoCode/Modules/35-MosaicCharts/files/mosaic-tutorial.html#figures",
    "href": "content/courses/NoCode/Modules/35-MosaicCharts/files/mosaic-tutorial.html#figures",
    "title": "Mosaic Chart Step by Step Tutorial",
    "section": "Figures",
    "text": "Figures\n\nFigure 12.1: Grouped bar chart for the hospital data\n\n\nbarplot(Hospital, legend = rownames(Hospital), beside = TRUE,\n        xlab = \"Length of stay\", ylab = \"Number of patients\")\n\n\n\n\n\n\n###################################################\n### Figure 12.2: 3D bar chart for the hospital data\n###################################################\nmyHospital = t(Hospital)[,3:1]\nmydat = data.frame(\"Length of stay\" = as.vector(row(myHospital)),\n                   \"Visit frequency\" = as.vector(col(myHospital)),\n                   \"Number of patients\" = as.vector(myHospital))\nscatterplot3d(mydat, type = \"h\", pch = \" \", lwd = 10,\nx.ticklabs = c(\"2-9\",\"\",\"10-19\",\"\",\"20+\"),\n              y.ticklabs = c(\"Never\",\"\",\"Less than monthly\",\"\",\"Regular\"),\n              xlab = \"Length of stay\", ylab = \"Visit frequency\", zlab = \"Number of patients\",\n              y.margin.add = 0.2,\n              color = \"black\", box = FALSE)\n\n\n\n\n\n\n############################################################################\n### Figure 12.3: Construction of a mosaic plot for the hospital data, step 1\n############################################################################\nmosaic(margin.table(Hospital, 2), split = TRUE)\n\n\n\n\n\n\n\nFigure 12.4: Construction of a mosaic plot for the hospital data, step 2\n\nmosaic(t(Hospital), split = TRUE, mar = c(left = 3.5),\n       labeling_args = list(offset_labels = c(left = 0.5),\n         offset_varnames = c(left = 1, top = 0.5), set_labels =\n         list(\"Visit frequency\" = c(\"Regular\",\"Less than\\nmonthly\",\"Never\"))))\n\n\n\n\n\n\n###\n### Figure 12.5: Mosaic plot for the Hospital data, alternative splitting\n\nmosaic(Hospital)\n\n\n\n\n\n\n\nFigure 12.6: Sieve plot for the hospital data\n\nsieve(t(Hospital), split = TRUE, pop = FALSE, gp = gpar(lty = \"dotted\", col = \"black\"))\nlabeling_cells(text = t(Hospital), clip = FALSE, gp = gpar(fontface = 2, fontsize = 15))(t(Hospital))\n\n\n\n\n\n\n\nFigure 12.7: Association plot for the hospital data\n\nassoc(t(Hospital), split = TRUE)\n\n\n\n\n\n\n\nFigure 12.8: Qualitative color palette for HSV and HCL space\n\npar(mfrow = c(1,2), mar = c(1,1,1,1), oma = c(0,0,0,0))\npie(rep(1,9), radius = 1, col = rainbow(9), labels = 360 * 0:8/9)\npie(rep(1,9), radius = 1, col = rainbow_hcl(9), labels = 360 * 0:8/9)\n\n\n\n\n\n\n\nFigure 12.9: Diverging color palettes for HSV and HCL space\n\npar(mfrow = c(1,1), mar = c(1,1,1,1), oma = c(0,0,0,0))\nplot.new()\nrect(0:4/5, 0.2, 1:5/5, 0.5, border = 0, col = diverge_hcl(5))\nrect(0, 0.2, 1, 0.5, border = 1, col = NULL)\nrect(0:4/5, 0.55, 1:5/5, 0.85, border = 0, col = diverge_hsv(5))\nrect(0, 0.55, 1, 0.85, border = 1, col = NULL)\ntext(c(1:5/5 - 0.1), 0.11, c(\"(260, 100, 50)\", \"(260, 50, 70)\", \"(H, 0, 90)\", \"(0, 50, 70)\", \"(0, 100, 50)\"))\ntext(c(1:5/5 - 0.1), 0.91, c(\"(240, 100, 100)\", \"(240, 50, 100)\", \"(H, 0, 100)\", \"(0, 50, 100)\",\"(0, 100, 100)\"))\n\n\n\n\n\n\n\nFigure 12.10: Spine plot with highlighting for the hospital data\n\nmycol &lt;- rep(grey.colors(2)[2:1], 1:2)\nmosaic(t(Hospital),\n       mar = c(left = 3.5),\n       labeling_args = list(offset_labels = c(left = 0.5),\n         offset_varnames = c(left = 1, top = 0.5), set_labels =\n         list(\"Visit frequency\" = c(\"Regular\",\"Less than\\nmonthly\",\"Never\"))),\n       split = TRUE, highlighting = 2, gp = gpar(fill = mycol, col = mycol))\n\n\n\n\n\n\n\nFigure 12.11: Mosaic display of the hospital data with Friendly-like\n\n### color coding of the residuals\n\nmosaic(t(Hospital), split = TRUE, shade = TRUE,\n       mar = c(left = 3.5),\n       gp_args = list(p.value = hos_chisq$p.value),\n       labeling_args = list(offset_labels = c(left = 0.5),\n         offset_varnames = c(left = 1, top = 0.5), set_labels =\n         list(\"Visit frequency\" = c(\"Regular\",\"Less than\\nmonthly\",\"Never\"))))\n\n\n\n\n\n\n\nFigure 12.12: Association plot of the hospital data with\n\n### Friendly-like color coding of the residuals\n##########################################################################\nassoc(t(Hospital), split = TRUE, shade = TRUE, keep = TRUE,\n      gp_args = list(p.value = hos_chisq$p.value))\n\n\n\n\n\n\n\nFigure 12.13: Mosaic plot for the arthritis data, using the chi-squared test and fixed cut-off points for the shading\n\n###########################################################################\nmosaic(art, shade = TRUE, gp_args = list(p.value = art_chisq$p.value))\n\n\n\n\n\n\n\nFigure 12.14: Mosaic plot for the arthritis data, using the maximum test and data-driven cut-off points for the residuals\n\nset.seed(myseed)\nmosaic(art, gp = shading_max, gp_args = list(n = 5000))\n\n\n\n\n\n\n\nFigure 12.15: Pairs-plot for the UCB admissions data\n\npairs(UCBAdmissions)\n\n\n\n\n\n\n\nFigure 12.16: Doubledecker plot for the UCB admissions data\n\ndoubledecker(aperm(aperm(UCBAdmissions, c(1,3,2))[2:1,,], c(2,3,1)),\n             margins = c(left = 0, right = 5), col = rev(grey.colors(2)))\n\n\n\n\n\n\n\nFigure 12.17: Mosic plot for the UCB admissions data\n\nmosaic(aperm(UCBAdmissions, c(3,2,1)), data = UCBAdmissions, split = TRUE,\n       shade = TRUE, keep = FALSE, residuals = ucb_max$residuals,\n       gp_args = list(p.value = ucb_max$p.value), rep = c(Admission = FALSE))\n\n\n\n\n\n\n\nFigure 12.18: Association plot for the UCB admissions data\n\nassoc(aperm(UCBAdmissions, c(3,2,1)), split = c(TRUE,FALSE,TRUE), shade = TRUE,\n      residuals_type = \"Pearson\", residuals = ucb_max$residuals,\n      gp_args = list(p.value = ucb_max$p.value), rep = c(Admission = FALSE))\n\n\n\n\n\n\n\nFigure 12.19: Conditional association plot for the UCB admissions data\n\ncotabplot(aperm(UCBAdmissions, c(2,1,3)), panel = cotab_coindep, shade = TRUE,\n          legend = FALSE,\n          panel_args = list(type = \"assoc\", margins = c(2,1,1,2), varnames = FALSE))\n\n\n\n\n\n\n### Figure 12.20: Mosic plot with highlighting for the punishment data\n\n\nmosaic(~ age + education + memory + attitude, data = punish, keep = FALSE,\n       gp = gpar(fill = grey.colors(2)), spacing = spacing_highlighting,\n       rep = c(attitude = FALSE))\n\n\n\n\n\n\n\nFigure 12.21: Conditional mosaic plot for the punishment data\n\nset.seed(123)\ncotabplot(punish, panel = cotab_coindep, panel_args = list(varnames = FALSE, margins = c(2, 1, 1, 2)))"
  },
  {
    "objectID": "content/courses/NoCode/Modules/20-Histograms/index.html",
    "href": "content/courses/NoCode/Modules/20-Histograms/index.html",
    "title": "🕶 Histograms",
    "section": "",
    "text": "Variable #1\nVariable #2\nChart Names\nChart Shape\n\n\n\nQuant\nNone\nHistogram",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🕶 Histograms"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/20-Histograms/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/NoCode/Modules/20-Histograms/index.html#what-graphs-will-we-see-today",
    "title": "🕶 Histograms",
    "section": "",
    "text": "Variable #1\nVariable #2\nChart Names\nChart Shape\n\n\n\nQuant\nNone\nHistogram",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🕶 Histograms"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/20-Histograms/index.html#what-kind-of-data-variables-will-we-choose",
    "href": "content/courses/NoCode/Modules/20-Histograms/index.html#what-kind-of-data-variables-will-we-choose",
    "title": "🕶 Histograms",
    "section": "\n What kind of Data Variables will we choose?",
    "text": "What kind of Data Variables will we choose?\n\n\n\n\n\n\nNo\nPronoun\nAnswer\nVariable/Scale\nExample\nWhat Operations?\n\n\n1\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities, with Scale and a Zero Value.Differences and Ratios /Products are meaningful.\nQuantitative/Ratio\nLength,Height,Temperature in Kelvin,Activity,Dose Amount,Reaction Rate,Flow Rate,Concentration,Pulse,Survival Rate\nCorrelation",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🕶 Histograms"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/20-Histograms/index.html#inspiration",
    "href": "content/courses/NoCode/Modules/20-Histograms/index.html#inspiration",
    "title": "🕶 Histograms",
    "section": "\n Inspiration",
    "text": "Inspiration\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Golf Drive Distance over the years\n\n\nWhat do we see here? In about two-and-a-half decades, golf drive distances have increased, on the average, by 35 yards. The maximum distance has also gone up by 30 yards, and the minimum is now at 250 yards, which was close to average in 1983! What was a decent average in 1983 is just the bare minimum in 2017!!\nIs it the dimples that the golf balls have? But these have been around a long time…or is it the clubs, and the swing technique invented by more recent players?",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🕶 Histograms"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/20-Histograms/index.html#how-do-these-charts-work",
    "href": "content/courses/NoCode/Modules/20-Histograms/index.html#how-do-these-charts-work",
    "title": "🕶 Histograms",
    "section": "\n How do these Chart(s) Work?",
    "text": "How do these Chart(s) Work?\nHistograms are best to show the distribution of values of a quantitative variable. A distribution shows how often the variable in question lies within specific value ranges. We plot the histogram by displaying the how often vs defined ranges, often called buckets or bins. For example, in 2017, 8.5% of all drive distances were at the then average distance of 292.1 yards. One can create histogram buckets from Quant variables, such as 0-5, 6-10, 11-15…etc.\n\n\n\n\n\n\nHistograms vs Bar/Column Charts\n\n\n\nAs we will see shortly, Bar/Column charts show categorical data, such as the number of apples, bananas, carrots, etc. Visually speaking, histograms do not usually show spaces between buckets because these are continuous values, while column charts must show spaces to separate each category. More later.",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🕶 Histograms"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/20-Histograms/index.html#plotting-a-histograms",
    "href": "content/courses/NoCode/Modules/20-Histograms/index.html#plotting-a-histograms",
    "title": "🕶 Histograms",
    "section": "\n Plotting a Histograms",
    "text": "Plotting a Histograms\n\n\nUsing Orange\nUsing RAWgraphs\nUsing DataWrapper\n\n\n\nLet us rapidly make some histograms in Orange, so that we know how the tool works here. We start with the iris dataset: Download this Orange workflow file and open it in Orange.\n Download the Histogram Workflow \nYou can see the effect of modifying the bin widths, and of fitting a standard distribution for comparison.\n\n\n\n\n\nhttps://academy.datawrapper.de/article/136-histogram-min-max-median-mean\nDataWrapper does not offer a separate histogram-making tool. Histograms in DataWrapper are available as a part of the data-inspection part of the work flow, as a small thumbnail-sized plot.",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🕶 Histograms"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/20-Histograms/index.html#dataset-netflix-original-series",
    "href": "content/courses/NoCode/Modules/20-Histograms/index.html#dataset-netflix-original-series",
    "title": "🕶 Histograms",
    "section": "\n Dataset: Netflix Original Series",
    "text": "Dataset: Netflix Original Series\nWe are now ready for a more detailed example. Here is a look at this data on Netflix Original Series. Download it to your machine by clicking on the button below.\n Download the Netflix Dataset \n\n Examine the Data\n\n\n\n\n\nFigure 2: Netflix Data Table\n\n\nFigure 2 states that there are 109 movies, 6 variables in the dataset.\n\n Data Dictionary\n\n\n\n\n\n\nQuantitative Data\n\n\n\n\n\nPremiere_Year: (int) Year the movie premiered\n\nSeasons: (int) No. of Seasons\n\nEpisodes: (int) No. of Episodes\n\nIMDB_Rating: (int) IMDB Rating!!\n\n\n\n\n\n\n\n\n\nQualitative Data\n\n\n\n\n\nGenere: (chr) types of Genres\n\nTitle: (chr) 109 titles\n\nSubgenre: (chr) types of sub-Genres\n\nStatus: (chr) status on Netflix\n\n\n\n\n Research Questions\nLet’s try a few questions and see if they are answerable with Histograms.\n\n\n\n\n\n\nNote\n\n\n\nQ1. What is the distribution of IMDB_Rating? If we split/colour by movie Genere?\n\n\n\n\n\n\n\n\n\n(a) IMDB Ratings Histogram\n\n\n\n\n\n\n\n\n\n(b) IMDB Rating vs Genere\n\n\n\n\n\n\nFigure 3: Netflix Data Histograms\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQ2. Are IMDB_Rating affected by the number of Seasons or Episodes?\n\n\n\n\n\n\n\n\n\n(a) Reformatting “Seasons”\n\n\n\n\n\n\n\n\n\n(b) IMDB Rating vs Seasons\n\n\n\n\n\n\nFigure 4: Plotting with Seasons\n\n\nWe first need to reformat the Seasons variable from N to C in the data file view. This converts it to Qual. Then we split the IMDB histogram by this new variable.\n\n\n\n What is the Story Here?\nMost movies have decent IMDB scores; the distribution is left-skewed. Some of course have been trashed!! Splitting IMDBRating by Genere is not too illuminating…\nNot much wisdom to be gleaned either from splitting IMDBRating by Seasons…",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🕶 Histograms"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/20-Histograms/index.html#dataset-the-old-faithful-geyser-in-the-usa",
    "href": "content/courses/NoCode/Modules/20-Histograms/index.html#dataset-the-old-faithful-geyser-in-the-usa",
    "title": "🕶 Histograms",
    "section": "\n Dataset: the Old Faithful geyser in the USA",
    "text": "Dataset: the Old Faithful geyser in the USA\nHere is a dataset about the eruption durations, and wait times between eruptions of the Old Faithful geyser in Yellowstone National Park, USA.\nDownload this data to your machine and import it into Orange.\n Download the Geyser Dataset \n\n Examine the Data\n\n\n\n\n\nFigure 5: Old Faithful Data Table\n\n\nFigure 5 states that we have 272 data points, and three variables. All variables are Quantitative!\n\n Data Dictionary\n\n\n\n\n\n\nQuantitative Data\n\n\n\n\n\neruptions: (dbl Duration Times of Eruptions\n\nwaiting: (dbl) Waiting Times between Eruptions\n\ndensity: (dbl) (Ignore this for now)\n\n\n\n\n\n\n\n\n\nQualitative Data\n\n\n\n\nNo Qual variables!!\n\n\n\n\n Research Questions\n\n\n\n\n\n\nNote\n\n\n\nQ1. How are eruptions (durations) and waiting (times) distributed?\n\n\n\n\n\n\n\n\n\n(a) Eruption Durations Histogram\n\n\n\n\n\n\n\n\n\n(b) Waiting Times Histogram\n\n\n\n\n\n\nFigure 6: Old Faithful Data Histograms\n\n\n\n\n\n What is the Story Here?\n\nBoth durations have a “double-humped” distribution…\nThere are therefore two distinct ranges for both durations.\nAre there two different mechanisms at work in the geyser, that randomly kick in?",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🕶 Histograms"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/20-Histograms/index.html#your-turn",
    "href": "content/courses/NoCode/Modules/20-Histograms/index.html#your-turn",
    "title": "🕶 Histograms",
    "section": "\n Your Turn",
    "text": "Your Turn\nTry your hand at these datasets. Look at the data table, state the data dictionary, contemplate a few Research Questions and answer them with graphs in Orange!\n\nAirbnb Price Data on the French Riviera\n\n\n\n\n Airbnb data\n\n\n\n1. Wage and Education Data from Canada\n Download the Wages/Education Dataset",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🕶 Histograms"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/20-Histograms/index.html#wait-but-why",
    "href": "content/courses/NoCode/Modules/20-Histograms/index.html#wait-but-why",
    "title": "🕶 Histograms",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nHistograms are used to study the distribution of one or a few variables. Checking the distribution of your variables one by one is probably the first task you should do when you get a new dataset. It delivers a good quantity of information. Several distribution shapes exist, here is an illustration of the 6 most common ones:\n\n\n\n\n\n\n\n\n\nIn your Design-Project-related research, you will collect data from or about your target audience. The Quantitative parts of that data may obtain with any of these distributions. Inspecting these may give you an insight into the population of your target audience, something that may likely be true, a hunch, which you could verify and convert into …opportunity.",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🕶 Histograms"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/20-Histograms/index.html#references",
    "href": "content/courses/NoCode/Modules/20-Histograms/index.html#references",
    "title": "🕶 Histograms",
    "section": "\n References",
    "text": "References\n\nSee the scrolly animation for a histogram at this website: Exploring Histograms, an essay by Aran Lunzer and Amelia McNamara https://tinlizzie.org/histograms/?s=09\nhttps://www.data-to-viz.com/graph/histogram.html\n\n\n\n\nFigure 1: Golf Drive Distance over the years\nFigure 1: Golf Drive Distance over the years\nFigure 2: Netflix Data Table\nFigure 3 (a): IMDB Ratings Histogram\nFigure 3 (b): IMDB Rating vs Genere\nFigure 4 (a): Reformatting “Seasons”\nFigure 4 (b): IMDB Rating vs Seasons\nFigure 5: Old Faithful Data Table\nFigure 6 (a): Eruption Durations Histogram\nFigure 6 (b): Waiting Times Histogram",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🕶 Histograms"
    ]
  },
  {
    "objectID": "content/courses/NoCode/listing.html",
    "href": "content/courses/NoCode/listing.html",
    "title": "Data Viz with No Code!",
    "section": "",
    "text": "It is a truth universally acknowledged, that a Srishti Art and Design student, in possession of a good Mac, must be terrified of coding.\n\n-Code and Prejudice, Jane Austen, 1870\n\n\nThis Unit takes SMI peasants students on a journey of using data to tell stories and make decisions. Without doing an iota of work writing one byte of code. Bah.\nDatasets from various domains of human enterprise and activity are introduced. The datasets are motivated from the point of view of the Types of Information they contain: Quantities, Attributes, Changes, Portions, Ranking, Relations, and related to Space, and Time, for example.\nThe human contexts from these datasets are used to appreciate the specifics of data formats, and the nature of variables within the data. Student will relate the data variables to Data/Information Visualizations, making decisions on how geometric shapes and other aspects of different Data Types and Visualizations can be metaphorically matched to the contexts. These information-to-geometry metaphors will lead us to Insights, Questions, and eventually to Stories and good Decisions.\nStudents will then be prompted to work in groups, or as a whole, to conduct a complete data gathering experiment on campus, visualize the data and tell a Story that pertains to their immediate surroundings.",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code"
    ]
  },
  {
    "objectID": "content/courses/NoCode/listing.html#abstract",
    "href": "content/courses/NoCode/listing.html#abstract",
    "title": "Data Viz with No Code!",
    "section": "",
    "text": "It is a truth universally acknowledged, that a Srishti Art and Design student, in possession of a good Mac, must be terrified of coding.\n\n-Code and Prejudice, Jane Austen, 1870\n\n\nThis Unit takes SMI peasants students on a journey of using data to tell stories and make decisions. Without doing an iota of work writing one byte of code. Bah.\nDatasets from various domains of human enterprise and activity are introduced. The datasets are motivated from the point of view of the Types of Information they contain: Quantities, Attributes, Changes, Portions, Ranking, Relations, and related to Space, and Time, for example.\nThe human contexts from these datasets are used to appreciate the specifics of data formats, and the nature of variables within the data. Student will relate the data variables to Data/Information Visualizations, making decisions on how geometric shapes and other aspects of different Data Types and Visualizations can be metaphorically matched to the contexts. These information-to-geometry metaphors will lead us to Insights, Questions, and eventually to Stories and good Decisions.\nStudents will then be prompted to work in groups, or as a whole, to conduct a complete data gathering experiment on campus, visualize the data and tell a Story that pertains to their immediate surroundings.",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code"
    ]
  },
  {
    "objectID": "content/courses/NoCode/listing.html#what-you-will-learn",
    "href": "content/courses/NoCode/listing.html#what-you-will-learn",
    "title": "Data Viz with No Code!",
    "section": "What you will learn",
    "text": "What you will learn\n\nData Basics:\n\nLearn to find data and appreciate how it was gathered in the first place\nWhat does data look like and why should we care?\nHow to Spot a good Variable\nWhat can we do with the data, visually? How do geometric attributes such as location, size, and colour, lend themselves to representing data?\n\nCreating Graphs and Data Visualizations\n\nRapidly make different kinds of graphs\nUse Graphs and Tables as a way of getting answers to your Questions\nDevelop intuition that matches data and graph types\n\nTell stories with data and graphs\n\nAnnotate Graphs with text and insights\nExport these to create crisp and readable documents that you can share",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code"
    ]
  },
  {
    "objectID": "content/courses/NoCode/listing.html#introduction",
    "href": "content/courses/NoCode/listing.html#introduction",
    "title": "Data Viz with No Code!",
    "section": "Introduction",
    "text": "Introduction\nTake a look at the graph visualization below:\n\n\n\n\nWhat information does the graph convey? How ?\nWhat aspects of the Visual convey “human” information, such as Number and Relation?\nWhat could the sloping dotted line in the picture depict?\n\nWe will form our intuition about shapes and data and learn to create some evocative information graphics that tell stories.",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code"
    ]
  },
  {
    "objectID": "content/courses/NoCode/listing.html#references",
    "href": "content/courses/NoCode/listing.html#references",
    "title": "Data Viz with No Code!",
    "section": "References",
    "text": "References\n\nJack Dougherty and Ilya Ilyankou, Hands-On Data Visualization: Interactive Storytelling from Spreadsheets to Code, https://handsondataviz.org/. Available free Online.\nClaus O. Wilke, Fundamentals of Data Visualization, https://clauswilke.com/dataviz/. Available free Online.\nJonathan Schwabish, Better Data Visualizations: A Guide for Scholars, Researchers, and Wonks, Columbia University Press, 2021.\nAlberto Cairo, The Functional Art:An introduction to information graphics and visualization, New Riders. 2013. ISBN-9780133041361.\nCole Nussbaumer Knaflic, Storytelling With Data: A Data Visualization Guide for Business Professionals, Wiley 2015. ISBN-9781119002253.",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code"
    ]
  },
  {
    "objectID": "content/courses/NoCode/listing.html#quick-lookup",
    "href": "content/courses/NoCode/listing.html#quick-lookup",
    "title": "Data Viz with No Code!",
    "section": "Quick Lookup",
    "text": "Quick Lookup\n\nCharts and Data\n\nData Vis Project https://datavizproject.com/ Allows you to match data types and data-vis types!! Perfect!!\nData Viz Catalogue https://datavizcatalogue.com/ Another good place to look for graphs that match your data!\nFrom Data-to-Viz https://www.data-to-viz.com/#explore\nFinancial Times Visual Vocabulary Chart. A great chart to match data to data-viz. PDF here and Web version https://ft-interactive.github.io/visual-vocabulary/\n72 types of Visualization for Data Stories https://blog.gramener.com/types-of-data-visualization-for-data-stories/\n\n\n\nCharting in R\n\nR Charts https://r-charts.com/\nR Graph Gallery https://r-graph-gallery.com/index.html",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code"
    ]
  },
  {
    "objectID": "content/courses/NoCode/listing.html#dataset-resources",
    "href": "content/courses/NoCode/listing.html#dataset-resources",
    "title": "Data Viz with No Code!",
    "section": "Dataset Resources",
    "text": "Dataset Resources\n\nA wide variety of graphics and datasets on global issues at Our World in Data https://ourworldindata.org/\nDatasets at calmcode.io https://calmcode.io/datasets.html. Simple datasets that you should begin with.\nData.World https://data.world. A very well organized easily searchable database of datasets and visualizations!\nThe Harvard Dataverse https://dataverse.harvard.edu/. A very large searchable database of datasets on a very wode set of topics.\nIPUMS https://www.ipums.org/ The Integrated Public Use Microdata Series (IPUMS) is the world’s largest individual-level population database. IPUMS consists of microdata samples from United States (IPUMS-USA) and international (IPUMS-International) census records, as well as data from U.S. and international surveys. Data provided is integrated across time and space. Health, Economics, Higher Education, Historical Data and much more.\nKaggle Datasets https://www.kaggle.com/datasets E.g. Netflix Shows\nData Is Plural https://www.data-is-plural.com/. This a weekly newsletter of useful/curious datasets by Jeremy Singer-Vine.\nInformation is Beautiful https://informationisbeautiful.net/ David McCandless’ terrific information visualization site. All datasets used here are also available for download.\nIndia Data by Sector https://data.gov.in/sector\nThe FBI’s Crime Data Explorer (very US-centric) https://crime-data-explorer.app.cloud.gov/pages/home\nDatasets at 538 ( very US-centric) https://data.fivethirtyeight.com/\nOpen Data Network ( again very US-centric) https://www.opendatanetwork.com/\n311-data.org https://www.311-data.org/. Data about 311 calls in different parts of the US. (#311 is a complaints service that deals with non-crime / non-emergency related neighbourhood issues in the US)\nGoogle Dataset Search https://datasetsearch.research.google.com/\nGithub dataset search https://github.com/search?q=datasets\nWorld Inequality Database, https://wid.world/. Global data on income and wealth inequality. India specific data also available.\nWorld Bank Open Data https://data.worldbank.org/. A global collection of economic development data .\nJonathan Schwabish’s PolicyViz DataViz Catalogue. https://policyviz.com/resources/policyviz-data-visualization-catalog/ This is a spreadsheet that has links to data and images of visualizations that have been achieved with each of the datasets. Over 800 entries…see table below! (US centric, but very inspirational visualizations!), See the emebedded version below:\n\n\n\n\n\n\nWork With Data. https://www.workwithdata.com/data A good selection of datasets on a wide set of topics. Check the neat network diagram there!\nVincent Arel-Bundock’s RDatasets webpage: https://vincentarelbundock.github.io/Rdatasets/index.html",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code"
    ]
  },
  {
    "objectID": "content/courses/NoCode/listing.html#our-tools",
    "href": "content/courses/NoCode/listing.html#our-tools",
    "title": "Data Viz with No Code!",
    "section": "Our Tools",
    "text": "Our Tools\n\nChart Creation and Export\n\nOrange Data Mining https://orangedatamining.com/ Free software. Very intuitive, point-and-click, goes all the way from simple data-viz to ML!\nDatawrapper https://academy.datawrapper.de/ A free browser-based tool, requires registration and login.\nRAWGraphs https://app.rawgraphs.io/ Another Free browser-based tool, no registration, no login. Simple interface too.\n\n\n\nStory Telling with Charts\n\nFlourish Studio https://flourish.studio/ Beautiful and easy data visualization and storytelling\nInfogram https://infogram.com/ Create engaging infographics and reports in minutes\nVisme https://www.visme.co/ Yet another…",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code"
    ]
  },
  {
    "objectID": "content/courses/NoCode/listing.html#modules",
    "href": "content/courses/NoCode/listing.html#modules",
    "title": "Data Viz with No Code!",
    "section": "Modules",
    "text": "Modules",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code"
    ]
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/30-Classification/files/Random-Forests.html",
    "href": "content/courses/ML4Artists/Modules/30-Classification/files/Random-Forests.html",
    "title": "Random Forests",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(broom)\nlibrary(prettydoc)\nlibrary(corrplot)\n\ncorrplot 0.92 loaded\n\nlibrary(ggformula)\n\nLoading required package: scales\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\nLoading required package: ggridges\n\nNew to ggformula?  Try the tutorials: \n    learnr::run_tutorial(\"introduction\", package = \"ggformula\")\n    learnr::run_tutorial(\"refining\", package = \"ggformula\")\n\nlibrary(palmerpenguins) # Allison Horst's `penguins` data.\n##\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ dials        1.2.1      ✔ rsample      1.2.1 \n✔ infer        1.0.7      ✔ tune         1.2.1 \n✔ modeldata    1.3.0      ✔ workflows    1.1.4 \n✔ parsnip      1.2.1      ✔ workflowsets 1.1.0 \n✔ recipes      1.0.10     ✔ yardstick    1.3.1 \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(dials)\nlibrary(modeldata)\nlibrary(rsample)\nlibrary(recipes)\nlibrary(yardstick)\nlibrary(parsnip)"
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/30-Classification/files/Random-Forests.html#penguin-random-forest-model-withrandomforest",
    "href": "content/courses/ML4Artists/Modules/30-Classification/files/Random-Forests.html#penguin-random-forest-model-withrandomforest",
    "title": "Random Forests",
    "section": "Penguin Random Forest Model withrandomForest\n",
    "text": "Penguin Random Forest Model withrandomForest\n\nUsing the penguins dataset and Random Forest Classification.\n\npenguins\n\n\n  \n\n\nsummary(penguins)\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex     \n Min.   :172.0     Min.   :2700   female:165  \n 1st Qu.:190.0     1st Qu.:3550   male  :168  \n Median :197.0     Median :4050   NA's  : 11  \n Mean   :200.9     Mean   :4202               \n 3rd Qu.:213.0     3rd Qu.:4750               \n Max.   :231.0     Max.   :6300               \n NA's   :2         NA's   :2                  \n\npenguins %&gt;% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n344\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nspecies\n0\n1.00\nFALSE\n3\nAde: 152, Gen: 124, Chi: 68\n\n\nisland\n0\n1.00\nFALSE\n3\nBis: 168, Dre: 124, Tor: 52\n\n\nsex\n11\n0.97\nFALSE\n2\nmal: 168, fem: 165\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nbill_length_mm\n2\n0.99\n43.92\n5.46\n32.1\n39.23\n44.45\n48.5\n59.6\n▃▇▇▆▁\n\n\nbill_depth_mm\n2\n0.99\n17.15\n1.97\n13.1\n15.60\n17.30\n18.7\n21.5\n▅▅▇▇▂\n\n\nflipper_length_mm\n2\n0.99\n200.92\n14.06\n172.0\n190.00\n197.00\n213.0\n231.0\n▂▇▃▅▂\n\n\nbody_mass_g\n2\n0.99\n4201.75\n801.95\n2700.0\n3550.00\n4050.00\n4750.0\n6300.0\n▃▇▆▃▂\n\n\n\n\npenguins &lt;- penguins %&gt;% tidyr::drop_na()\n# Spent one hour trying to find `drop-na()` (14 June 2020)\n\n\n# library(corrplot)\ncor &lt;- penguins %&gt;% select(where(is.numeric)) %&gt;% cor() \ncor %&gt;% corrplot(., method = \"ellipse\", order = \"hclust\",tl.cex = 1.0,)\n\n\n\n\n\n\n# try these too:\n# cor %&gt;% corrplot(., method = \"square\", order = \"hclust\",tl.cex = 0.5)\n# cor %&gt;% corrplot(., method = \"color\", order = \"hclust\",tl.cex = 0.5)\n# cor %&gt;% corrplot(., method = \"shade\", order = \"hclust\",tl.cex = 0.5)\n\nNotes: - flipper_length_mm and culmen_depth_mm are negatively correlated at approx (-0.7) - flipper_length_mm and body_mass_g are positively correlated at approx 0.8\nSo we will use steps in the recipe to remove correlated variables.\nPenguin Data Sampling and Recipe\n\n# Data Split\npenguin_split &lt;- initial_split(penguins, prop = 0.6)\npenguin_train &lt;- training(penguin_split)\npenguin_test &lt;- testing(penguin_split)\npenguin_split\n\n&lt;Training/Testing/Total&gt;\n&lt;199/134/333&gt;\n\nhead(penguin_train)\n\n\n  \n\n\n# Recipe\npenguin_recipe &lt;- penguins %&gt;% \n  recipe(species ~ .) %&gt;% \n  step_normalize(all_numeric()) %&gt;% # Scaling and Centering\n  step_corr(all_numeric()) %&gt;%  # Handling correlated variables\n  prep()\n\n# Baking the data\npenguin_train_baked &lt;-  penguin_train %&gt;% \n  bake(object = penguin_recipe, new_data = .)\n\npenguin_test_baked &lt;-  penguin_test %&gt;% \n  bake(object = penguin_recipe, new_data = .)\n\nhead(penguin_train_baked)\n\n\n  \n\n\n\nPenguin Random Forest Model\n\npenguin_model &lt;- \n  rand_forest(trees = 100) %&gt;% \n  set_engine(\"randomForest\") %&gt;% \n  set_mode(\"classification\")\npenguin_model\n\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  trees = 100\n\nComputational engine: randomForest \n\npenguin_fit &lt;- \n  penguin_model %&gt;% \n  fit(species ~ .,penguin_train_baked)\npenguin_fit\n\nparsnip model object\n\n\nCall:\n randomForest(x = maybe_data_frame(x), y = y, ntree = ~100) \n               Type of random forest: classification\n                     Number of trees: 100\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 2.01%\nConfusion matrix:\n          Adelie Chinstrap Gentoo class.error\nAdelie        83         1      0  0.01190476\nChinstrap      2        38      1  0.07317073\nGentoo         0         0     74  0.00000000\n\n# iris_ranger &lt;- \n#   rand_forest(trees = 100) %&gt;% \n#   set_mode(\"classification\") %&gt;% \n#   set_engine(\"ranger\") %&gt;% \n#   fit(Species ~ ., data = iris_training_baked)\n\nMetrics for the Penguin Random Forest Model\n\n# Predictions\npredict(object = penguin_fit, new_data = penguin_test_baked) %&gt;%  \n  dplyr::bind_cols(penguin_test_baked) %&gt;% \n  glimpse()\n\nRows: 134\nColumns: 8\n$ .pred_class       &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; -0.8946955, -0.8215515, -0.6752636, -0.8764095, -1.7…\n$ bill_depth_mm     &lt;dbl&gt; 0.77955895, 0.11940428, 0.42409105, 1.23658911, 1.99…\n$ flipper_length_mm &lt;dbl&gt; -1.4246077, -1.0678666, -0.4257325, -0.4257325, -0.2…\n$ body_mass_g       &lt;dbl&gt; -0.567620576, -0.505525421, -1.188572125, 0.58113979…\n$ sex               &lt;fct&gt; male, female, female, male, male, female, male, fema…\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n\n# Prediction Accuracy Metrics\npredict(object = penguin_fit, new_data = penguin_test_baked) %&gt;%  \n  dplyr::bind_cols(penguin_test_baked) %&gt;% \n  yardstick::metrics(truth = species, estimate = .pred_class)\n\n\n  \n\n\n# Prediction Probabilities\npenguin_fit_probs &lt;- \n  predict(penguin_fit, penguin_test_baked, type = \"prob\") %&gt;%\n  dplyr::bind_cols(penguin_test_baked)\nglimpse(penguin_fit_probs)\n\nRows: 134\nColumns: 10\n$ .pred_Adelie      &lt;dbl&gt; 1.00, 0.99, 0.98, 0.97, 1.00, 1.00, 0.64, 1.00, 1.00…\n$ .pred_Chinstrap   &lt;dbl&gt; 0.00, 0.01, 0.02, 0.02, 0.00, 0.00, 0.35, 0.00, 0.00…\n$ .pred_Gentoo      &lt;dbl&gt; 0.00, 0.00, 0.00, 0.01, 0.00, 0.00, 0.01, 0.00, 0.00…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; -0.8946955, -0.8215515, -0.6752636, -0.8764095, -1.7…\n$ bill_depth_mm     &lt;dbl&gt; 0.77955895, 0.11940428, 0.42409105, 1.23658911, 1.99…\n$ flipper_length_mm &lt;dbl&gt; -1.4246077, -1.0678666, -0.4257325, -0.4257325, -0.2…\n$ body_mass_g       &lt;dbl&gt; -0.567620576, -0.505525421, -1.188572125, 0.58113979…\n$ sex               &lt;fct&gt; male, female, female, male, male, female, male, fema…\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n\n# Confusion Matrix\npenguin_fit$fit$confusion %&gt;% tidy()\n\n\n  \n\n\n# Gain Curves\npenguin_fit_probs %&gt;% \n  yardstick::gain_curve(species, .pred_Adelie:.pred_Gentoo) %&gt;%\n  autoplot()\n\n\n\n\n\n\n# ROC Plot\npenguin_fit_probs%&gt;%\n  roc_curve(species, .pred_Adelie:.pred_Gentoo) %&gt;%\n  autoplot()\n\n\n\n\n\n\n\nUsing broom on the penguin model\n\npenguin_split\n\n&lt;Training/Testing/Total&gt;\n&lt;199/134/333&gt;\n\npenguin_split %&gt;% broom::tidy()\n\n\n  \n\n\npenguin_recipe %&gt;% broom::tidy()\n\n\n  \n\n\n# Following do not work for `random forest models` !! ;-()\n#penguin_model %&gt;% tidy()\n#penguin_fit %&gt;% tidy() \npenguin_model %&gt;% str()\n\nList of 7\n $ args                 :List of 3\n  ..$ mtry : language ~NULL\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_EmptyEnv&gt; \n  ..$ trees: language ~100\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_EmptyEnv&gt; \n  ..$ min_n: language ~NULL\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_EmptyEnv&gt; \n $ eng_args             : Named list()\n  ..- attr(*, \"class\")= chr [1:2] \"quosures\" \"list\"\n $ mode                 : chr \"classification\"\n $ user_specified_mode  : logi TRUE\n $ method               : NULL\n $ engine               : chr \"randomForest\"\n $ user_specified_engine: logi TRUE\n - attr(*, \"class\")= chr [1:2] \"rand_forest\" \"model_spec\"\n\npenguin_test_baked"
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/30-Classification/files/Random-Forests.html#iris-random-forest-model-with-ranger",
    "href": "content/courses/ML4Artists/Modules/30-Classification/files/Random-Forests.html#iris-random-forest-model-with-ranger",
    "title": "Random Forests",
    "section": "Iris Random Forest Model with ranger\n",
    "text": "Iris Random Forest Model with ranger\n\nUsing the iris dataset and Random Forest Classification. This part uses rsample to split the data and the recipes to prep the data for model making.\n\n#set.seed(100)\niris_split &lt;- rsample::initial_split(iris, prop = 0.6)\niris_split\n\n&lt;Training/Testing/Total&gt;\n&lt;90/60/150&gt;\n\niris_split %&gt;% training() %&gt;% glimpse()\n\nRows: 90\nColumns: 5\n$ Sepal.Length &lt;dbl&gt; 5.6, 5.4, 5.1, 4.4, 6.4, 6.0, 7.9, 4.9, 5.5, 5.0, 5.5, 5.…\n$ Sepal.Width  &lt;dbl&gt; 2.5, 3.7, 3.8, 2.9, 2.8, 2.9, 3.8, 3.6, 2.4, 2.3, 2.5, 3.…\n$ Petal.Length &lt;dbl&gt; 3.9, 1.5, 1.6, 1.4, 5.6, 4.5, 6.4, 1.4, 3.7, 3.3, 4.0, 1.…\n$ Petal.Width  &lt;dbl&gt; 1.1, 0.2, 0.2, 0.2, 2.2, 1.5, 2.0, 0.1, 1.0, 1.0, 1.3, 0.…\n$ Species      &lt;fct&gt; versicolor, setosa, setosa, setosa, virginica, versicolor…\n\niris_split %&gt;% testing() %&gt;% glimpse()\n\nRows: 60\nColumns: 5\n$ Sepal.Length &lt;dbl&gt; 5.1, 4.7, 5.0, 4.9, 4.3, 5.7, 5.7, 5.4, 5.1, 4.6, 5.0, 5.…\n$ Sepal.Width  &lt;dbl&gt; 3.5, 3.2, 3.4, 3.1, 3.0, 4.4, 3.8, 3.4, 3.7, 3.6, 3.4, 3.…\n$ Petal.Length &lt;dbl&gt; 1.4, 1.3, 1.5, 1.5, 1.1, 1.5, 1.7, 1.7, 1.5, 1.0, 1.6, 1.…\n$ Petal.Width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.1, 0.1, 0.4, 0.3, 0.2, 0.4, 0.2, 0.4, 0.…\n$ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n\nIris Data Pre-Processing: Creating the Recipe\nThe recipes package provides an interface that specializes in data pre-processing. Within the package, the functions that start, or execute, the data transformations are named after cooking actions. That makes the interface more user-friendly. For example:\n\nrecipe() - Starts a new set of transformations to be applied, similar to the ggplot() command. Its main argument is the model’s formula.\nprep() - Executes the transformations on top of the data that is supplied (typically, the training data). Each data transformation is a step() function. ( Recall what we did with the caret package: Centering, Scaling, Removing Correlated variables…)\n\nNote that in order to avoid data leakage (e.g: transferring information from the train set into the test set), data should be “prepped” using the train_tbl only. https://towardsdatascience.com/modelling-with-tidymodels-and-parsnip-bae2c01c131c CRAN: The idea is that the preprocessing operations will all be created using the training set and then these steps will be applied to both the training and test set.\n\n# Pre Processing the Training Data\n\niris_recipe &lt;- \n  training(iris_split) %&gt;% # Note: Using TRAINING data !!\n  recipe(Species ~ .)      # Note: Outcomes ~ Predictors !!\n\n# The data contained in the `data` argument need not be the training set; this data is only used to catalog the names of the variables and their types (e.g. numeric, etc.).\n\nQ: How does the recipe “figure” out which are the outcomes and which are the predictors? A.The recipe command defines Outcomes and Predictors using the formula interface. Not clear how this recipe “figures” out which are the outcomes and which are the predictors, when we have not yet specified them…\nQ. Why is the recipe not agnostic to data set? Is that a meaningful question? A. The use of the training set in the recipe command is just to declare the variables and specify the roles of the data, nothing else. Roles are open-ended and extensible. From https://cran.r-project.org/web/packages/recipes/vignettes/Simple_Example.html :\n\nThis document demonstrates some basic uses of recipes. First, some definitions are required: - variables are the original (raw) data columns in a data frame or tibble. For example, in a traditional formula Y ~ A + B + A:B, the variables are A, B, and Y. - roles define how variables will be used in the model. Examples are: predictor (independent variables), response, and case weight. This is meant to be open-ended and extensible. - terms are columns in a design matrix such as A, B, and A:B. These can be other derived entities that are grouped, such as a set of principal components or a set of columns, that define a basis function for a variable. These are synonymous with features in machine learning. Variables that have predictor roles would automatically be main effect terms.\n\n\n# Apply the transformation steps\niris_recipe &lt;- iris_recipe %&gt;% \n  step_corr(all_predictors()) %&gt;% \n  step_center(all_predictors(), -all_outcomes()) %&gt;% \n  step_scale(all_predictors(), -all_outcomes()) %&gt;% \n  prep()\n\nThis has created the recipe() and prepped it too. We now need to apply it to our datasets:\n\nTake training data and bake() it to prepare it for modelling.\nDo the same for the testing set.\n\n\niris_training_baked &lt;- \n  iris_split %&gt;% \n  training() %&gt;% \n  bake(iris_recipe,.)\niris_training_baked\n\n\n  \n\n\niris_testing_baked &lt;- \n  iris_split %&gt;% \n  testing() %&gt;% \n  bake(iris_recipe,.)\niris_testing_baked \n\n\n  \n\n\n\nIris Model Training using parsnip\n\nDifferent ML packages provide different interfaces (APIs ) to do the same thing (e.g random forests). The tidymodels package provides a consistent interface to invoke a wide variety of packages supporting a wide variety of models.\nThe parsnip package is a successor to caret.\nTo model with parsnip: 1. Pick a model : 2. Set the engine 3. Set the mode (if needed): Classification or Regression\nCheck here for models available in parsnip.\n\nMode: classification and regression in parsnip, each using a variety of models. ( Which Way). This defines the form of the output.\nEngine: The engine is the R package that is invoked by parsnip to execute the model. E.g glm, glmnet,keras.( How ) parsnip provides wrappers for models from these packages.\nModel: is the specific technique used for the modelling task. E.g linear_reg(), logistic_reg(), mars, decision_tree, nearest_neighbour…(What model).\n\nand models have: - hyperparameters: that are numerical or factor variables that tune the model ( Like the alpha beta parameters for Bayesian priors)\nWe can use the random forest model to classify the iris into species. Here Species is the Outcome variable and the rest are predictor variables. The random forest model is provided by the ranger package, to which tidymodels/parsnip provides a simple and consistent interface.\n\nlibrary(ranger)\niris_ranger &lt;- \n  rand_forest(trees = 100) %&gt;% \n  set_mode(\"classification\") %&gt;% \n  set_engine(\"ranger\") %&gt;% \n  fit(Species ~ ., data = iris_training_baked)\n\nranger can generate random forest models for classification, regression, survival( time series, time to event stuff). Extreme Forests are also supported, wherein all points in the dataset are used ( instead of bootstrap samples) along with feature bagging. We can also run the same model using the randomForest package:\n\nlibrary(randomForest,quietly = TRUE)\niris_rf &lt;- \n  rand_forest(trees = 100) %&gt;% \n  set_mode(\"classification\") %&gt;% \n  set_engine(\"randomForest\") %&gt;% \n  fit(Species ~ ., data = iris_training_baked)\n\nIris Predictions\nThe predict() function run against a parsnip model returns a prediction tibble. By default, the prediction variable is called .pred_class.\n\npredict(object = iris_ranger, new_data = iris_testing_baked) %&gt;%  \n  dplyr::bind_cols(iris_testing_baked) %&gt;% \n  glimpse()\n\nRows: 60\nColumns: 5\n$ .pred_class  &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n$ Sepal.Length &lt;dbl&gt; -0.9035455, -1.3763309, -1.0217418, -1.1399382, -1.849116…\n$ Sepal.Width  &lt;dbl&gt; 1.17909073, 0.44386094, 0.93401413, 0.19878435, -0.046292…\n$ Petal.Width  &lt;dbl&gt; -1.35400849, -1.35400849, -1.35400849, -1.48532396, -1.48…\n$ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n\nIris Classification Model Validation\nWe use metrics() function from the yardstick package to evaluate how good the model is.\n\npredict(iris_ranger, iris_testing_baked) %&gt;%\n  dplyr::bind_cols(iris_testing_baked) %&gt;% \n  yardstick::metrics(truth = Species, estimate = .pred_class)\n\n\n  \n\n\n\nWe can also check the metrics for randomForest model:\n\npredict(iris_rf, iris_testing_baked) %&gt;%\n  dplyr::bind_cols(iris_testing_baked) %&gt;% \n  yardstick::metrics(truth = Species, estimate = .pred_class)\n\n\n  \n\n\n\nIris Per-Classifier Metrics\nWe can use the parameter type = \"prob\" in the predict() function to obtain a probability score on each prediction. TBD: How is this prob calculated? Possible answer: the Random Forest model outputs its answer by majority voting across n trees. Each of the possible answers( i.e. predictions) for a particular test datum gets a share of the vote, that represents its probability. Hence each dataum in the test vector can show a probability for the “winning” answer. ( Quite possibly we can get the probabilities for all possible outcomes for each test datum)\n\niris_ranger_probs &lt;- \n  predict(iris_ranger, iris_testing_baked, type = \"prob\") %&gt;%\n  dplyr::bind_cols(iris_testing_baked)\nglimpse(iris_ranger_probs)\n\nRows: 60\nColumns: 7\n$ .pred_setosa     &lt;dbl&gt; 0.9944603, 0.9782222, 0.9856349, 0.9532222, 0.9516659…\n$ .pred_versicolor &lt;dbl&gt; 0.005539683, 0.011361111, 0.013365079, 0.035111111, 0…\n$ .pred_virginica  &lt;dbl&gt; 0.000000000, 0.010416667, 0.001000000, 0.011666667, 0…\n$ Sepal.Length     &lt;dbl&gt; -0.9035455, -1.3763309, -1.0217418, -1.1399382, -1.84…\n$ Sepal.Width      &lt;dbl&gt; 1.17909073, 0.44386094, 0.93401413, 0.19878435, -0.04…\n$ Petal.Width      &lt;dbl&gt; -1.35400849, -1.35400849, -1.35400849, -1.48532396, -…\n$ Species          &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos…\n\niris_rf_probs &lt;- \n  predict(iris_rf, iris_testing_baked, type = \"prob\") %&gt;%\n  dplyr::bind_cols(iris_testing_baked)\nglimpse(iris_rf_probs)\n\nRows: 60\nColumns: 7\n$ .pred_setosa     &lt;dbl&gt; 1.00, 1.00, 1.00, 0.98, 0.98, 0.80, 0.79, 0.98, 1.00,…\n$ .pred_versicolor &lt;dbl&gt; 0.00, 0.00, 0.00, 0.01, 0.01, 0.20, 0.20, 0.02, 0.00,…\n$ .pred_virginica  &lt;dbl&gt; 0.00, 0.00, 0.00, 0.01, 0.01, 0.00, 0.01, 0.00, 0.00,…\n$ Sepal.Length     &lt;dbl&gt; -0.9035455, -1.3763309, -1.0217418, -1.1399382, -1.84…\n$ Sepal.Width      &lt;dbl&gt; 1.17909073, 0.44386094, 0.93401413, 0.19878435, -0.04…\n$ Petal.Width      &lt;dbl&gt; -1.35400849, -1.35400849, -1.35400849, -1.48532396, -…\n$ Species          &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos…\n\n# Tabulating the probabilities\nftable(iris_rf_probs$.pred_versicolor)\n\n  0 0.01 0.02 0.04 0.06 0.07 0.09 0.11 0.13 0.17 0.2 0.22 0.35 0.45 0.53 0.54 0.56 0.67 0.71 0.72 0.82 0.88 0.89 0.93 0.94 0.95 0.97 0.98  1\n                                                                                                                                            \n 10    8    4    3    2    2    2    1    1    2   2    1    1    1    1    1    1    1    1    1    1    1    1    1    5    1    1    1  2\n\nftable(iris_rf_probs$.pred_virginica)\n\n  0 0.01 0.02 0.03 0.05 0.06 0.07 0.08 0.09 0.17 0.26 0.32 0.43 0.45 0.46 0.8 0.81 0.84 0.88 0.9 0.91 0.93 0.94 0.95 0.96 0.98  1\n                                                                                                                                 \n 19    6    1    4    2    1    1    1    1    1    1    1    1    1    1   1    1    1    1   1    1    2    2    1    3    2  2\n\nftable(iris_rf_probs$.pred_setosa)\n\n  0 0.01 0.02 0.03 0.04 0.05 0.26 0.55 0.65 0.78 0.79 0.8 0.98 0.99  1\n                                                                      \n 19    7    5    3    2    1    1    1    1    1    1   1    6    3  8\n\n\nIris Classifier: Gain and ROC Curves\nWe can plot gain and ROC curves for each of these models\n\niris_ranger_probs %&gt;% \n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %&gt;% \n  glimpse()\n\nRows: 141\nColumns: 5\n$ .level          &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"set…\n$ .n              &lt;dbl&gt; 0, 1, 2, 3, 4, 6, 7, 9, 10, 13, 15, 17, 19, 20, 21, 22…\n$ .n_events       &lt;dbl&gt; 0, 1, 2, 3, 4, 6, 7, 9, 10, 13, 15, 17, 19, 20, 21, 22…\n$ .percent_tested &lt;dbl&gt; 0.000000, 1.666667, 3.333333, 5.000000, 6.666667, 10.0…\n$ .percent_found  &lt;dbl&gt; 0.000000, 4.545455, 9.090909, 13.636364, 18.181818, 27…\n\niris_ranger_probs %&gt;% \n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %&gt;% \n  autoplot()\n\n\n\n\n\n\niris_ranger_probs %&gt;% \n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %&gt;% \n  glimpse()\n\nRows: 144\nColumns: 4\n$ .level      &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\"…\n$ .threshold  &lt;dbl&gt; -Inf, 0.000000000, 0.001000000, 0.002428571, 0.003360324, …\n$ specificity &lt;dbl&gt; 0.0000000, 0.0000000, 0.1315789, 0.2105263, 0.2631579, 0.2…\n$ sensitivity &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n\niris_ranger_probs %&gt;% \n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n\niris_rf_probs %&gt;% \n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %&gt;% \n  glimpse()\n\nRows: 74\nColumns: 5\n$ .level          &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"set…\n$ .n              &lt;dbl&gt; 0, 8, 11, 17, 18, 19, 20, 21, 22, 23, 24, 26, 29, 34, …\n$ .n_events       &lt;dbl&gt; 0, 8, 11, 17, 18, 19, 20, 21, 22, 22, 22, 22, 22, 22, …\n$ .percent_tested &lt;dbl&gt; 0.000000, 13.333333, 18.333333, 28.333333, 30.000000, …\n$ .percent_found  &lt;dbl&gt; 0.000000, 36.363636, 50.000000, 77.272727, 81.818182, …\n\niris_rf_probs %&gt;% \n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %&gt;% \n  autoplot()\n\n\n\n\n\n\niris_rf_probs %&gt;% \n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %&gt;% \n  glimpse()\n\nRows: 77\nColumns: 4\n$ .level      &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\"…\n$ .threshold  &lt;dbl&gt; -Inf, 0.00, 0.01, 0.02, 0.03, 0.04, 0.05, 0.26, 0.55, 0.65…\n$ specificity &lt;dbl&gt; 0.0000000, 0.0000000, 0.5000000, 0.6842105, 0.8157895, 0.8…\n$ sensitivity &lt;dbl&gt; 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0…\n\niris_rf_probs %&gt;% \n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\nIris Classifier: Metrics\n\npredict(iris_ranger, iris_testing_baked, type = \"prob\") %&gt;% \n  bind_cols(predict(iris_ranger,iris_testing_baked)) %&gt;% \n  bind_cols(select(iris_testing_baked,Species)) %&gt;% \n  glimpse()\n\nRows: 60\nColumns: 5\n$ .pred_setosa     &lt;dbl&gt; 0.9944603, 0.9782222, 0.9856349, 0.9532222, 0.9516659…\n$ .pred_versicolor &lt;dbl&gt; 0.005539683, 0.011361111, 0.013365079, 0.035111111, 0…\n$ .pred_virginica  &lt;dbl&gt; 0.000000000, 0.010416667, 0.001000000, 0.011666667, 0…\n$ .pred_class      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos…\n$ Species          &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos…\n\n# predict(iris_ranger, iris_testing_baked, type = \"prob\") %&gt;% \n#   bind_cols(predict(iris_ranger,iris_testing_baked)) %&gt;% \n#   bind_cols(select(iris_testing_baked,Species)) %&gt;% \n#   yardstick::metrics(data = ., truth = Species, estimate = .pred_class, ... = .pred_setosa:.pred_virginica)\n\n\n# And for the `randomForest`method\n\npredict(iris_rf, iris_testing_baked, type = \"prob\") %&gt;% \n  bind_cols(predict(iris_ranger,iris_testing_baked)) %&gt;% \n  bind_cols(select(iris_testing_baked,Species)) %&gt;% \n  glimpse()\n\nRows: 60\nColumns: 5\n$ .pred_setosa     &lt;dbl&gt; 1.00, 1.00, 1.00, 0.98, 0.98, 0.80, 0.79, 0.98, 1.00,…\n$ .pred_versicolor &lt;dbl&gt; 0.00, 0.00, 0.00, 0.01, 0.01, 0.20, 0.20, 0.02, 0.00,…\n$ .pred_virginica  &lt;dbl&gt; 0.00, 0.00, 0.00, 0.01, 0.01, 0.00, 0.01, 0.00, 0.00,…\n$ .pred_class      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos…\n$ Species          &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos…\n\n# predict(iris_rf, iris_testing_baked, type = \"prob\") %&gt;% \n#   bind_cols(predict(iris_ranger,iris_testing_baked)) %&gt;% \n#   bind_cols(select(iris_testing_baked,Species)) %&gt;% \n#   yardstick::metrics(data = ., truth = Species, estimate = .pred_class, ... = .pred_setosa:.pred_virginica)"
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/30-Classification/files/Random-Forests.html#references",
    "href": "content/courses/ML4Artists/Modules/30-Classification/files/Random-Forests.html#references",
    "title": "Random Forests",
    "section": "References",
    "text": "References\n\nMachine Learning Basics - Random Forest at Shirin’s Playground"
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/20-Regression/index.html",
    "href": "content/courses/ML4Artists/Modules/20-Regression/index.html",
    "title": "ML - Regression",
    "section": "",
    "text": "Interpolation:\n\nbetween TWO colours, both colours inclusive using a straight line between them\nbetween several different colours?\n\nby mixing “equal proportions” of each\nProportions based on “distance” from each colour\nOn a “plane” with these points",
    "crumbs": [
      "Teaching",
      "ML for Artists and Managers",
      "ML - Regression"
    ]
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/20-Regression/index.html#workflow-in-orange",
    "href": "content/courses/ML4Artists/Modules/20-Regression/index.html#workflow-in-orange",
    "title": "ML - Regression",
    "section": "Workflow in Orange",
    "text": "Workflow in Orange\nLet us “draw inspiration” from the picture above, and see if we can replicate it. We will fire up Orange, paint some data and see if we can fit a linear regression ML model to it.\nHere is the Orange file for you to download. Open this file in Orange.",
    "crumbs": [
      "Teaching",
      "ML for Artists and Managers",
      "ML - Regression"
    ]
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/20-Regression/index.html#workflow-in-radiant",
    "href": "content/courses/ML4Artists/Modules/20-Regression/index.html#workflow-in-radiant",
    "title": "ML - Regression",
    "section": "Workflow in Radiant",
    "text": "Workflow in Radiant",
    "crumbs": [
      "Teaching",
      "ML for Artists and Managers",
      "ML - Regression"
    ]
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/20-Regression/index.html#workflow-in-r",
    "href": "content/courses/ML4Artists/Modules/20-Regression/index.html#workflow-in-r",
    "title": "ML - Regression",
    "section": "Workflow in R",
    "text": "Workflow in R",
    "crumbs": [
      "Teaching",
      "ML for Artists and Managers",
      "ML - Regression"
    ]
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/20-Regression/index.html#conclusion",
    "href": "content/courses/ML4Artists/Modules/20-Regression/index.html#conclusion",
    "title": "ML - Regression",
    "section": "Conclusion",
    "text": "Conclusion",
    "crumbs": [
      "Teaching",
      "ML for Artists and Managers",
      "ML - Regression"
    ]
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/10-IntroOrange/index.html",
    "href": "content/courses/ML4Artists/Modules/10-IntroOrange/index.html",
    "title": "🐉 Intro to Orange",
    "section": "",
    "text": "Orange is a visual drag-and-drop tool for\n\nData visualization\n\nMachine Learning\n\nData mining\n\nand much more. Here is what it looks like:\n\nAll operations are done using a visual menu-driven interface. The visuals can be exported to PNG/SVG/PDF and the entire workflow can be exported into a Report Form and edited for presentation and sharing.",
    "crumbs": [
      "Teaching",
      "ML for Artists and Managers",
      "🐉 Intro to Orange"
    ]
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/10-IntroOrange/index.html#introduction-to-orange",
    "href": "content/courses/ML4Artists/Modules/10-IntroOrange/index.html#introduction-to-orange",
    "title": "🐉 Intro to Orange",
    "section": "",
    "text": "Orange is a visual drag-and-drop tool for\n\nData visualization\n\nMachine Learning\n\nData mining\n\nand much more. Here is what it looks like:\n\nAll operations are done using a visual menu-driven interface. The visuals can be exported to PNG/SVG/PDF and the entire workflow can be exported into a Report Form and edited for presentation and sharing.",
    "crumbs": [
      "Teaching",
      "ML for Artists and Managers",
      "🐉 Intro to Orange"
    ]
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/10-IntroOrange/index.html#installing-orange",
    "href": "content/courses/ML4Artists/Modules/10-IntroOrange/index.html#installing-orange",
    "title": "🐉 Intro to Orange",
    "section": "Installing Orange",
    "text": "Installing Orange\nYou can download and install Orange from here:\nhttps://orangedatamining.com/download/",
    "crumbs": [
      "Teaching",
      "ML for Artists and Managers",
      "🐉 Intro to Orange"
    ]
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/10-IntroOrange/index.html#basic-usage-of-orange",
    "href": "content/courses/ML4Artists/Modules/10-IntroOrange/index.html#basic-usage-of-orange",
    "title": "🐉 Intro to Orange",
    "section": "Basic Usage of Orange",
    "text": "Basic Usage of Orange",
    "crumbs": [
      "Teaching",
      "ML for Artists and Managers",
      "🐉 Intro to Orange"
    ]
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/10-IntroOrange/index.html#orange-workflows",
    "href": "content/courses/ML4Artists/Modules/10-IntroOrange/index.html#orange-workflows",
    "title": "🐉 Intro to Orange",
    "section": "Orange Workflows",
    "text": "Orange Workflows",
    "crumbs": [
      "Teaching",
      "ML for Artists and Managers",
      "🐉 Intro to Orange"
    ]
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/10-IntroOrange/index.html#widgets-and-channels",
    "href": "content/courses/ML4Artists/Modules/10-IntroOrange/index.html#widgets-and-channels",
    "title": "🐉 Intro to Orange",
    "section": "Widgets and Channels",
    "text": "Widgets and Channels",
    "crumbs": [
      "Teaching",
      "ML for Artists and Managers",
      "🐉 Intro to Orange"
    ]
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/10-IntroOrange/index.html#loading-data-into-orange",
    "href": "content/courses/ML4Artists/Modules/10-IntroOrange/index.html#loading-data-into-orange",
    "title": "🐉 Intro to Orange",
    "section": "Loading data into Orange",
    "text": "Loading data into Orange\n\n\nWe are good to get started with Orange!!",
    "crumbs": [
      "Teaching",
      "ML for Artists and Managers",
      "🐉 Intro to Orange"
    ]
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/10-IntroOrange/index.html#simple-visuals-using-orange",
    "href": "content/courses/ML4Artists/Modules/10-IntroOrange/index.html#simple-visuals-using-orange",
    "title": "🐉 Intro to Orange",
    "section": "Simple Visuals using Orange",
    "text": "Simple Visuals using Orange\nLet us create some simple visualizations using Orange.\n\nUse the File Widget to import the iris dataset into your session\nUse the Data Table Widget to look at the data, and note its variable names\nUse the Visualization Widgets ( Scatter Plot, Bar Plot, and Distributions) to look at the properties of the variables, and examine relationships between them.",
    "crumbs": [
      "Teaching",
      "ML for Artists and Managers",
      "🐉 Intro to Orange"
    ]
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/10-IntroOrange/index.html#reference",
    "href": "content/courses/ML4Artists/Modules/10-IntroOrange/index.html#reference",
    "title": "🐉 Intro to Orange",
    "section": "Reference",
    "text": "Reference\n\nIntroduction to Data Mining-Working notes for the hands-on course with Orange Data Mining. (Download file)",
    "crumbs": [
      "Teaching",
      "ML for Artists and Managers",
      "🐉 Intro to Orange"
    ]
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#image-right",
    "href": "content/slides/projects-slides/portfolio/index.html#image-right",
    "title": "Introduction to Networks in R",
    "section": ".image-right",
    "text": ".image-right\n\n\nWe can use the .image-right and .image-left classes to insert images in the background\nThese images will be placed behind most other content"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#image-left",
    "href": "content/slides/projects-slides/portfolio/index.html#image-left",
    "title": "Introduction to Networks in R",
    "section": ".image-left",
    "text": ".image-left\n\n\n\n\nIt is therefore recommened that you use multiple columns to only have text on the background area"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#section",
    "href": "content/slides/projects-slides/portfolio/index.html#section",
    "title": "Introduction to Networks in R",
    "section": "",
    "text": "background images"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#setting-background-colors",
    "href": "content/slides/projects-slides/portfolio/index.html#setting-background-colors",
    "title": "Introduction to Networks in R",
    "section": "Setting background colors",
    "text": "Setting background colors\nyou can set your background as you normally would"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#video-slide-title",
    "href": "content/slides/projects-slides/portfolio/index.html#video-slide-title",
    "title": "Introduction to Networks in R",
    "section": "Video Slide Title",
    "text": "Video Slide Title\nThis slides’s background video will play in a loop with audio muted."
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#slide-title",
    "href": "content/slides/projects-slides/portfolio/index.html#slide-title",
    "title": "Introduction to Networks in R",
    "section": "Slide Title",
    "text": "Slide Title"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#further-modifying-theme",
    "href": "content/slides/projects-slides/portfolio/index.html#further-modifying-theme",
    "title": "Introduction to Networks in R",
    "section": "Further Modifying theme",
    "text": "Further Modifying theme\nIf you want to modify theme, you can specify the .scss my modifying the yaml to look like this\nformat: \n  letterbox-revealjs:\n    theme: [default, style.scss]"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#modifying-letterbox-background",
    "href": "content/slides/projects-slides/portfolio/index.html#modifying-letterbox-background",
    "title": "Introduction to Networks in R",
    "section": "Modifying letterbox background",
    "text": "Modifying letterbox background\nThe background colors can be with with, where #444444 represents the new background color and #222222 represents the color of the shadow\n.quarto-light {\n  background-color: #444444;\n}\n\n.quarto-dark {\n  background-color: #444444;\n}\n\n.slides {\n  box-shadow: #222222 0px 0px 30px 0px;\n}"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#quarto",
    "href": "content/slides/projects-slides/portfolio/index.html#quarto",
    "title": "Introduction to Networks in R",
    "section": "Quarto",
    "text": "Quarto\nQuarto enables you to weave together content and executable code into a finished presentation. To learn more about Quarto presentations see https://quarto.org/docs/presentations/."
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#bullets",
    "href": "content/slides/projects-slides/portfolio/index.html#bullets",
    "title": "Introduction to Networks in R",
    "section": "Bullets",
    "text": "Bullets\nWhen you click the Render button a document will be generated that includes:\n\nContent authored with markdown\nOutput from executable code"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/index.html#code",
    "href": "content/slides/projects-slides/portfolio/index.html#code",
    "title": "Introduction to Networks in R",
    "section": "Code",
    "text": "Code\nWhen you click the Render button a presentation will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "content/slides/listing.html",
    "href": "content/slides/listing.html",
    "title": "Applied Metaphors: Learning TRIZ, Complexity, Data/Stats/ML using Metaphors",
    "section": "",
    "text": "Introduction to Networks in R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Networks in R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMIT License\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMIT License\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlaying with Leaflet\n\n\n\n\n\n\nArvind Venkatadri\n\n\nMay 13, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nThe Nature of Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorking in R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "content/slides/r-slides/r-slides-listing.html",
    "href": "content/slides/r-slides/r-slides-listing.html",
    "title": "Applied Metaphors: Learning TRIZ, Complexity, Data/Stats/ML using Metaphors",
    "section": "",
    "text": "Introduction to Networks in R\n\n\nUsing tidygraph and visNetwork\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMIT License\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlaying with Leaflet\n\n\n\n\n\n\n\n\n\n\n\nMay 13, 2017\n\n\nArvind Venkatadri\n\n\n\n\n\n\n\n\n\n\n\n\nThe Nature of Data\n\n\nHow does Human Experience link with Data?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorking in R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "content/slides/r-slides/networks/LICENSE.html",
    "href": "content/slides/r-slides/networks/LICENSE.html",
    "title": "MIT License",
    "section": "",
    "text": "MIT License\nCopyright (c) 2022 quarto-letterbox authors\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/slides/r-slides/working-in-R/index.html#introduction",
    "href": "content/slides/r-slides/working-in-R/index.html#introduction",
    "title": "Working in R",
    "section": "Introduction",
    "text": "Introduction"
  },
  {
    "objectID": "content/slides/r-slides/working-in-R/index.html#data-structures",
    "href": "content/slides/r-slides/working-in-R/index.html#data-structures",
    "title": "Working in R",
    "section": "Data Structures",
    "text": "Data Structures"
  },
  {
    "objectID": "content/projects/fsp-doe/index.html",
    "href": "content/projects/fsp-doe/index.html",
    "title": "A Design of Experiments Class",
    "section": "",
    "text": "Code Lab"
  },
  {
    "objectID": "content/projects/fsp-doe/index.html#slides-and-code-links",
    "href": "content/projects/fsp-doe/index.html#slides-and-code-links",
    "title": "A Design of Experiments Class",
    "section": "",
    "text": "Code Lab"
  },
  {
    "objectID": "content/projects/fsp-doe/index.html#introduction",
    "href": "content/projects/fsp-doe/index.html#introduction",
    "title": "A Design of Experiments Class",
    "section": " Introduction",
    "text": "Introduction\nThis is a brief description and analysis of a Design of Experiments module conducted as a part of the Order and Chaos course, in the Foundation Studies Program (FSP 2021-2022) at SMI, MAHE, Bangalore."
  },
  {
    "objectID": "content/projects/fsp-doe/index.html#context",
    "href": "content/projects/fsp-doe/index.html#context",
    "title": "A Design of Experiments Class",
    "section": "Context",
    "text": "Context\nA Short Term Memory(STM) Test was the investigative tool used to verify several Hypotheses that were documented on the subject of STM.\nThis article describes the statistical analysis that was done with the readings. In particular, Permutations Tests were used to verify the effect size for each of three parameters that were hypothesized.\nFor more information, please click on the icon above to look at the Lab document."
  },
  {
    "objectID": "content/projects/fsp-doe/index.html#references",
    "href": "content/projects/fsp-doe/index.html#references",
    "title": "A Design of Experiments Class",
    "section": "References",
    "text": "References\n\nLawrance, A. J. 1996. “A Design of Experiments Workshop as an Introduction to Statistics.” American Statistician 50 (2): 156–58. doi:10.1080/00031305.1996.10474364.\nErnst, Michael D. 2004. “Permutation Methods: A Basis for Exact Inference.” Statistical Science 19 (4): 676–85. doi:10.1214/088342304000000396.\nPruim R, Kaplan DT, Horton NJ (2017). “The mosaic Package: Helping Students to ‘Think with Data’ Using R.” The R Journal, 9(1), 77–102. https://journal.r-project.org/archive/2017/RJ-2017-024/index.html."
  },
  {
    "objectID": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html",
    "href": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html",
    "title": "The TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad",
    "section": "",
    "text": "Here we go with another of my TRIZ Chronicles. The first edition analysing the famous movie Lawrence of Arabia is here.\nThe recent series of ads by Spotify have been very well received, by the ad industry as well. There have been several of these ads, each with a solid little story and a great punch line. There is one popular one that I have selected here, featuring a Mother shopping for veggies, while her son watches (and “listens”) to her bargaining with the shopkeeper. Here I take just that highly effective ad and interpret it from a TRIZ viewpoint.\nThis piece stems from my teaching a course on Creative Thinking and Problem Solving based on TRIZ, titled Play and Invent, over the past 8 years or more at the Srishti Manipal Institute of Art, Design, and Technology, Bangalore, India. (https://srishtimanipalinstitute.in)."
  },
  {
    "objectID": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#introduction",
    "href": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#introduction",
    "title": "The TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad",
    "section": "",
    "text": "Here we go with another of my TRIZ Chronicles. The first edition analysing the famous movie Lawrence of Arabia is here.\nThe recent series of ads by Spotify have been very well received, by the ad industry as well. There have been several of these ads, each with a solid little story and a great punch line. There is one popular one that I have selected here, featuring a Mother shopping for veggies, while her son watches (and “listens”) to her bargaining with the shopkeeper. Here I take just that highly effective ad and interpret it from a TRIZ viewpoint.\nThis piece stems from my teaching a course on Creative Thinking and Problem Solving based on TRIZ, titled Play and Invent, over the past 8 years or more at the Srishti Manipal Institute of Art, Design, and Technology, Bangalore, India. (https://srishtimanipalinstitute.in)."
  },
  {
    "objectID": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#the-famous-spotify-ad",
    "href": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#the-famous-spotify-ad",
    "title": "The TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad",
    "section": "The Famous Spotify Ad",
    "text": "The Famous Spotify Ad\nLet us watch the Spotify ad first, before analyzing it!"
  },
  {
    "objectID": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#the-young-mans-problem",
    "href": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#the-young-mans-problem",
    "title": "The TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad",
    "section": "The Young Man’s Problem",
    "text": "The Young Man’s Problem\nIn order to make a story out of this, I want make a Protagonist out the young man in the ad. It is he who has the problem and he who is going to apply TRIZ to solve it. I discuss the source of his Problem and give an analysis of the Problem from a (classical) TRIZ perspective, including the formulation of the Contradiction, Identification of Causes, the statement of the Ideal Final Result, and finally using the TRIZ Contradiction Matrix to find Inventive Principles, that lead to the solution, which of course, is meant to unerringly include Spotify !\nFirst a philosophical digression:—\nSeveral authors have taken a Game View of life. James P Carse’s famous book titled Finite and Infinite Games speaks of Play, Types of Games, Rules, Winning and our own aims in the Game itself. A similar articulation is, in my opinion, that of Mihaly Csikszentlmihalyi in his concept of Flow, shown here below:\n\n\nFrom Sketchplanations\n\nWhen the Game presents very little Challenge, we are bored. When the Game demands extreme skills the challenge is too much for us and we experience anxiety. When the Challenge presented is just barely matched by our Skill, we are in the zone of Flow, or what I call Play.\nA good metaphoric image for this experience is as follows:— that we live in a space where the Floor of Boredom is always rising and would crush us against our Ceiling of Anxiety. One Way to deal with this is to develop more Skills and push the Ceiling away, effectively moving into the zone of Flow. Another Way of looking at this is what Carse suggests: When Play is no longer possible, change the Game.\nSo what does all this have to do with getting veggies?\nThe ad is, in my opinion, all about Boredom, and how to avoid it. And not offend anybody. The Young Man (hereinafter, “YM”) simply has to accompany his Mom, and be there while she gets the veggies. I will exaggerate his irritation and his boredom at the risk of offending young people likely to read this, and say that he would rather not be there but he does not want to hurt Mom.\nWe are now ready for the TRIZ based Analysis of this Problem!"
  },
  {
    "objectID": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#a-triz-analysis-of-a-visit-to-the-subzi-mandi",
    "href": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#a-triz-analysis-of-a-visit-to-the-subzi-mandi",
    "title": "The TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad",
    "section": "A TRIZ Analysis of a Visit to the Subzi Mandi",
    "text": "A TRIZ Analysis of a Visit to the Subzi Mandi\nFor a TRIZ workflow, we proceed as before:\nFirst, using the method described in Open Source TRIZ, we identify knobs or parameters within the situation and see how turning these could lead to identifying a Cause for a Problem in the form of a Contradiction.\nHere below is a quick Ishikawa Diagram to help us identify the Parameters of this Problem:\n\n\n\n\n\n\n\n\nTurning the knobs/parameters in the Ishikawa Diagram, it seems that if the YM goes to the market with Mom, he would most likely get bored, but would please Mom. If he doesn’t go, then he chills at home, but Mom is going to justifiably furious. Herein lies the Contradiction, which we can now specify as an Administrative Contradiction(AC) in plain English:\n\n\nAC: The YM wants to chill at home but Mom wants him to take her veggie shopping. He has to put up with the Waste of Time, and being bored, and Stress at being away from friends.\n\n\nNext, based on this Contradiction and the inspection of the Ishikawa Diagram above, we are now ready to define a TRIZ Ideal Final Result:\n\n\nIFR: The YM must go to the Market and not be bored.\n\n\nNote again the impossible sounding way of expressing the IFR! One needs practice, like the Queen in Alice in Wonderland, who could think of Six Impossible Things before Breakfast ! Also note there could be other ways of specifying the IFR. See below, section Alternative Ideas for IFR.\nLet us take the AC and convert it into a Technical Contradiction(TC), keeping this IFR in mind. We will look at the 48 TRIZ Parameters in the TRIZ Contradiction Matrix (PDF) and see which Parameter we want to improve, while not worsening another. Here is what we can obtain. We will analyze each Contradiction both ways1:\n\n- TC 1: Improve Loss of Time (26) and not worsen Effect of External Harmful Factors (30)\n- TC 2: Improve Increase Productivity (44) and not worsen Stress (19)\n\nHere we choose these Parameters based on our IFR that while going to the Market may be unavoidable, Boredom need not ensue. Parameters chosen from the TRIZ Matrix can be thought of as metaphors for the knobs that lie within our AC. Going from the AC to the TC is an act of making metaphors. We could easily have chosen the Parameter Noise(29) as the “metaphoric thing” to avoid, but the current IFR doesn’t quite support that. There is here a considerable flexibility and possibility for imaginative interpretations of the AC, but using the language of TRIZ.\nWe could also formulate a Physical Contradiction(PC)2:\n\n\nPC: The YM must be in the market and not be in the market at the same time.\n\n\nwhich is aimed squarely at one of the Assumptions in the Problem, that the YM simply has to go. Again, if the IFR is formulated differently we could obtain a very different set of AC and PC. See below, section Alternative Ideas for IFR.\nIn a future post, we will deal with using the PC and the TRIZ Separation Principles to solve Problems."
  },
  {
    "objectID": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#solving-the-technical-contradiction",
    "href": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#solving-the-technical-contradiction",
    "title": "The TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad",
    "section": "Solving the Technical Contradiction",
    "text": "Solving the Technical Contradiction\nLet us take the both the TC-s into the Contradiction Matrix and arrive at the list of TRIZ Inventive Principles. Here is the Matrix solution for TC-1 in the figure below:\n\n\n\n\n\n\n\n\n\nThe two squares for the TC1 have been circled in red, solving TC-1.\nThe Inventive Principles are:(TC1, TC2, both ways)\n\n1(Segmentation)\n35(Parameter Change)\n21(Skipping)\n18(Mechanical Vibration) (!!)\n2(Taking Out/Separation)\n10(Prior Action)\n\n36(Phase Transitions)\nand with TC2:\n\n3(Local Quality)\n14(Spheroidality/Curvature)\n9(Preliminary Anti-Action)\n37(Thermal Expansion)\n40(Composite Materials)\n25(Self Service)\n24(Intermediary)\n\nThat is a considerable list for us to try to use!! Let us apply some these Inventive Principles! Viewing these Inventive Principles as we Generalized Solutions we try to map these back into the Problem at hand:\n\n\n35(Parameter Change): Which Parameter to change? Location? No. Sound? Change the “Bargaining Talk” into what? Sweet Musical Lyrics!!🎵🤣\n\n18(Mechanical Vibration) : What, make noise of your own? Yes! Play Music !!🔉 🤣\n\n14(Spheroidality): Wear “spherical” headphones!!🎧! Create a “sound sphere”! This is a long shot!!\n\n3(Local Quality): also indicates the creation of a “local” cocoon around the YM, but needs to be combined with 18(Mechanical Vibration) to truly arrive at the musical solution!\n\nOne could make decent interpretations of 2(Taking Out/Separation), and 24(Intermediary), but we are already there! The rest are perhaps (at least to me!) not very evocative, unless 37(Thermal Expansion) means “throw a temper tantrum at Mom”? Never! So there you have it! The Cinderella song played on Spotify becomes not just a noise canceller but actually seems to substitute the very conversation between Mom and the vendor. And the YM has successfully attained Flow ! And the IFR too, since with the music in his head, he is effectively “in the marketplace and not in the marketplace at the same time!\nAnd I attained Flow in writing this!!"
  },
  {
    "objectID": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#alternative-ideas-for-ifr",
    "href": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#alternative-ideas-for-ifr",
    "title": "The TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad",
    "section": "Alternative Ideas for IFR",
    "text": "Alternative Ideas for IFR\nWe note in passing that there is more than one way of formulating the Ideal Final Result. Here are two more examples:\n\n\nIFR2: The veggies should arrive without (the YM) going to the Market\nIFR3: Food should be prepared without having to go buy veggies.\n\n\nClearly these are at least as good as the one we have chosen, sounding nicely “impossible” in their own right! The point is that in the analysis of the Problem, we do need to ask Who has the Problem, as we did, and the IFR needs to stem from there. These alternative IFRs could well be the Voice of (another) Customer.\nIf there is any interesting situation that could be analyzed with TRIZ, please send me a DM! Thanks !"
  },
  {
    "objectID": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#references",
    "href": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#references",
    "title": "The TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad",
    "section": "References",
    "text": "References\n\nJames P Carse, Finite and Infinite Games, Free Press, 1986. ISBN: 0-02-905980-1\nMihaly Csikszentmihalyi, Creativity, Flow, and the Psychology of Discovery and Invention. Harper Perennial; Reprint edition (August 6, 2013)\nJack Hipple, The Ideal Result and How to Achieve It. Springer; 2012th edition (June 26, 2012)\nValery Souchkov, Defining Contradictions. http://www.xtriz.com/Training/TRIZ_DefineContradiction_Tutorial.pdf"
  },
  {
    "objectID": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#footnotes",
    "href": "content/projects/2023-01-09-TRIZ-Spotify/index.en.html#footnotes",
    "title": "The TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad",
    "section": "Footnotes",
    "text": "Footnotes\n\nThe Contradiction Matrix is not quite symmetric, so stating the Contradiction both ways allows us to access a slightly larger set of Inventive Principles from two cells of the Matrix.↩︎\nArriving at Physical Contradictions is not always easy! If we can, then there are a very crisp set of TRIZ Separation Principles that we can apply to solve the Problem.↩︎"
  },
  {
    "objectID": "content/projects/2023-11-03-Owind/index.html",
    "href": "content/projects/2023-11-03-Owind/index.html",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the O-Wind Turbine",
    "section": "",
    "text": "Here we go with another of my TRIZ Chronicles ! The earlier editions are here: Lawrence of Arabia, Spotify, and the Great Bubble Barrier.\nThis is another piece stems from my teaching a course on Creative Thinking and Problem Solving based on TRIZ, titled Play and Invent, over the past 8 years or more at the Srishti Manipal Institute of Art, Design, and Technology and now at DSU School of Commerce & Management Studies, both in Bangalore, INDIA."
  },
  {
    "objectID": "content/projects/2023-11-03-Owind/index.html#a-triz-analysis-of-the-dyson-o-wind-generator",
    "href": "content/projects/2023-11-03-Owind/index.html#a-triz-analysis-of-the-dyson-o-wind-generator",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the O-Wind Turbine",
    "section": "A TRIZ Analysis of the Dyson O-Wind Generator",
    "text": "A TRIZ Analysis of the Dyson O-Wind Generator\nFor a TRIZ workflow, we proceed as before:\n\nFirst, using the method described in Open Source TRIZ, we identify knobs or parameters within the situation\nWe see how turning these could lead to identifying a Statement / Cause for a Problem in the form of a Contradiction.\nRe-word the plain English Contradiction into TRIZ Parameters and look it up in the Contradiction Matrix. Obtain the Inventive Principles.\nApply these Inventive Principles into your Problem and solve it.\n\nIn the video itself, we heard about how electrical power consumption centers are the urban areas and these are far away from the generation sites. This leads to capital costs in HT Transmission equipment; we go to HT transmission to reduce losses on the way. This is already a Contradiction, which we might solve using Segmentation to arrive at Local Generation of Power. Local generation is a good idea to reduce these costs. This leads easily to Solar Panels on rooftops for example. Again while this may be cheaper than the electrical distribution system, it still uses a fair bit of capex and space and is centralized per building. Can we take Segmentation even further and think of a hyper-local household-based power generation unit, using the Wind?\nWhat would be the problems with using Wind based power generation around the home? Here below is a quick Ishikawa Diagram to help us identify the Parameters of this Problem:\n\n\n\n\n\n\n\n\nLooking at this Diagram, with the aspects identified, we could pair them off and see how they affect one another. In doing so, we could make up several problem. Let us state at some of our Problems: I have marked some of these with question marks since I am using imagination here and not direct primary research or information to formulate these. Note that some these may sound naive, but that is exactly way to start!\n\nI would like to have access my generator, but it needs to be not too close to the walls for it to harness the wind.\nHow to tap the power from the generator? What if the connection wires get twisted?\nDo I need a conventional Commutator? Won’t that be heavy?\nWhat voltage and current will I get? Will it be compatible with my 230V AC mains?\n\nAs you can see, many different problems and contradictions await our attention. Let us cut to the chase and state perhaps the most interesting problem (to me!) that the inventors have solved as demonstrated in the video above. We will state this as an Administrative Contradiction(AC) in plain English:\n\n\n\n\n\n\nAdministrative Contradiction\n\n\n\nAC: Winds help to generate power by making something rotate, but winds can change direction and slow down the existing rotation.\n\n\nWhat would an IFR be in this situation? How “unreasonable” can we be? Let us try:\n\n\n\n\n\n\nIdeal Final Result\n\n\n\nTorque must be in one direction only (irrespective of wind direction)\n\n\n\n\n\n\n\n\nUnidirectional Assumption\n\n\n\nI have made a strong assumption here about the the unidirectional movement: the main intent is for the rotating generator to be able to harness winds from any direction to establish or continue rotation in one direction (CW or CCW). Alternating current power generation is in principle immune to direction of rotation.\n\n\nLet us take our AC and convert it into a Technical Contradiction(TC), keeping this IFR in mind. We will look at the 48 TRIZ Parameters in the TRIZ Contradiction Matrix(PDF) and see which Parameter we want to improve, while not worsening another. Here is what we can obtain. We will analyze the Contradiction both ways1:\n\n\n\n\n\n\nTechnical Contradictions\n\n\n\n\n\nTC 1: Improve (15)Force/Torque while not worsening (3)Angle/Length of Moving Object\n\n\nTC 2: Improve (3)Angle/Length of Moving Object while not worsening (15)Force/Torque\n\n\n\n\nAgain we have chosen the TRIZ Parameters based on our IFR. Other metaphoric TRIZ Parameters that may suggest themselves are 12(Duration of Action on a Moving Object), 14(Speed), and (40)Harmful Effects Acting on the System.\nIs there a Physical Contradiction(PC)2 possible here?\n\n\n\n\n\n\nPhysical Contradiction\n\n\n\nThe Rotor must yield and not yield to the Wind at the same time. In other words, the rotor must be “porous and non-porous”3 to the wind at the same time.\n\n\nLet us now apply the TCs to the Contradiction Matrix and obtain the TRIZ Inventive Principles."
  },
  {
    "objectID": "content/projects/2023-11-03-Owind/index.html#solving-the-technical-contradiction",
    "href": "content/projects/2023-11-03-Owind/index.html#solving-the-technical-contradiction",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the O-Wind Turbine",
    "section": "Solving the Technical Contradiction",
    "text": "Solving the Technical Contradiction\nLet us take the both the TC-s into the Contradiction Matrix and arrive at the list of TRIZ Inventive Principles. Here is what the Matrix suggests:\nFor TC-1:\n\n17(Another Dimension) !!\n4( Asymmetry)\n14(Curvature) !!!\n\n10(Preliminary Action)\nand with TC-2:\n\n3(Local Quality)\n9(Preliminary Anti-Action)\n35(Parameter Change)\n\nHmm…based on the PC, we may have expected a Separation in Space solution, suggested by Curvature, Another Dimension and Asymmetry. Viewing these Inventive Principles as we Generalized Solutions, we try to map these back into the Problem at hand. In keeping with the metaphoric/analogic way of thinking that TRIZ embodies, I deliberately use many visual hints here from math, physics, geography, and biology.\n\n(14)Curvature: Hmm…nothing new here, or is there? Of course the rotor has to be curved and kind of sphere-like….\n17(Another Dimension): A near-spherical thing has really only one dimension..the radius. And that points in all directions / dimensions! Should there be changes in radius then? Should the radius change create bumps ( positive change ) or depressions ( negative change?) Should the bump be like a welt, and the depression like a groove? How can a bump or a depression itself be curved, as 14(Curvature) suggests?\n4(Asymmetry): The bumps or depressions…..they have to be asymmetric? So….not like longitudes and nor latitudes, but may be like those great circles.\n\n3(Local Quality): OK, the bumps or depressions are already “local”….can we go further? Here is where I stretch and go hyper-local: Should there be structures on or inside them, like flaps or fins or vanes? How can these be asymmetric, then? By acting like miniature flaps or trapdoors, that yield / fall flat when pushed in one direction and stand up / resist when pushed in the other direction…somewhat like a dog or cat’s fur? Then push and pull work differently…\n\n\n\n\n\n\nFrom Flaps to ….Funnels!\n\n\n\nMaking these flaps movable as the above paragraph seems to suggest would probably not be a good idea, from an engineering standpoint. But once we have the image of wind + flaps / fins / vanes and differences in pressure or movement, the Bernoulli Principle and Venturi effect suggest themselves immediately!! So what could this vane-fin-fur-flap thingy be then? Oh good heavens, a funnel !!!\n\n\n{HappyApple, Public domain, via Wikimedia Commons}\nSo each of those bumps are segmented into funnel-like structures that cause differences in air pressure when the wind blow. These differences are unidirectional and create movement/rotation! And because the bumps are curved along the surface of the sphere, and they are not parallel to one another (asymmetry), at least some of the internal funnels will always be “in the wind” 4, and capable of creating rotation using Bernoulli/Venturi effect!\n\n9(Preliminary Anti-Action): What do we wish to guard against? Counter acting wind forces. Well, the funnel structures work only with wind blowing into the broad opening and so we are fine!\n\nSo finally we could just imagine a spherical object, mounted on a spindle, with spiral arc-like bumps at different places on the surfaces. Within the arc-like bumps are funnel-like structures that create differentials in pressure when subject to the wind, and that creates rotation. Since the funnels are asymmetric by nature, our final rotation is unidirectional. Whew! ( Yes, that “whew” is also very suggestive here 😃!)"
  },
  {
    "objectID": "content/projects/2023-11-03-Owind/index.html#using-triz-separation-principles",
    "href": "content/projects/2023-11-03-Owind/index.html#using-triz-separation-principles",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the O-Wind Turbine",
    "section": "Using TRIZ Separation Principles",
    "text": "Using TRIZ Separation Principles\nAs Hipple explains, there is frequently an underlying physical parameter, such as length, breadth, weight, or energy, or speed for example that lies at the root of our Technical Contradiction.\nOur IFR states that we want the rotor to yield one way and to not yield when pushed the other way so it needs to be both hard and soft at the same time. This is a Physical Contradiction! In this case we can easily see and application of Separation in Space and also Separation on Condition. However I think in this case, it would not be easy to arrive at the Solution using just these.\nThat’s a wrap! In the next episode of the #TRIZ Chronicles, I wish to step even further out of my area of expertise and dabble in HR! I think looking at some of the institution-building ideas in Ricardo Semler’s book, Maverick would be a good idea!"
  },
  {
    "objectID": "content/projects/2023-11-03-Owind/index.html#references",
    "href": "content/projects/2023-11-03-Owind/index.html#references",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the O-Wind Turbine",
    "section": "References",
    "text": "References\n\nJack Hipple, The Ideal Result and How to Achieve It. Springer; 2012th edition (June 26, 2012)\nValery Souchkov, Defining Contradictions. http://www.xtriz.com/Training/TRIZ_DefineContradiction_Tutorial.pdf\n\nOpen Source TRIZ: Making Contradictions. https://www.youtube.com/watch?v=cah0OhCH55k"
  },
  {
    "objectID": "content/projects/2023-11-03-Owind/index.html#footnotes",
    "href": "content/projects/2023-11-03-Owind/index.html#footnotes",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the O-Wind Turbine",
    "section": "Footnotes",
    "text": "Footnotes\n\nThe Contradiction Matrix is not quite symmetric, so stating the Contradiction both ways allows us to access a slightly larger set of Inventive Principles from two cells of the Matrix.↩︎\nArriving at Physical Contradictions is not always easy! If we can, then there are a very crisp set of TRIZ Separation Principles that we can apply to solve the Problem.↩︎\nSo the Rotor must have…holes? How do holes “work in one direction only”? We will see…↩︎\nMathematically, the Wind direction vector will be (nearly) normal to the aperture of some funnel.↩︎"
  },
  {
    "objectID": "content/projects/listing.html",
    "href": "content/projects/listing.html",
    "title": "Applied Metaphors: Learning TRIZ, Complexity, Data/Stats/ML using Metaphors",
    "section": "",
    "text": "A Design of Experiments Class\n\n\nA Design of Experiments Class\n\n\n\nArvind Venkatadri\n\n\nJan 31, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA TRIZ Analysis of Lawrence of Arabia\n\n\nThe Attack on Aqaba: A TRIZ Analysis\n\n\n\nArvind V.\n\n\nDec 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nA Tidygraph version of a Popular Network Science Tutorial\n\n\n\n\n\n\nArvind Venkatadri\n\n\nJun 16, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResources for Order and Chaos\n\n\n\n\n\n\n\n\n\nDec 12, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nRmd Project for Quarto Website\n\n\n\n\n\n\nArvind V\n\n\nApr 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTRIZ – An Inventive Problem-Solving Method\n\n\n\n\n\n\nArvind V.\n\n\nNov 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nThe TRIZ Chronicles: A TRIZ Analysis of a Recent Spotify Ad\n\n\nPunjabi Pop and Getting the Veggies\n\n\n\nArvind Venkatadri\n\n\nJan 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nThe TRIZ Chronicles: TRIZ Analysis of the Banff Wildlife Crossings\n\n\nIs there a solution to wildlife roadkill?\n\n\n\nArvind Venkatadri\n\n\nJul 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nThe TRIZ Chronicles: TRIZ Analysis of the O-Wind Turbine\n\n\nIs there a cheap and effective way to generate power using the Wind, right in your home?\n\n\n\nArvind Venkatadri\n\n\nMar 1, 2023\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top",
    "crumbs": [
      "Blogs"
    ]
  },
  {
    "objectID": "content/projects/2023-07-02-Wildlife-Crossing/index.html",
    "href": "content/projects/2023-07-02-Wildlife-Crossing/index.html",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the Banff Wildlife Crossings",
    "section": "",
    "text": "Here we go with another of my TRIZ Chronicles ! The earlier editions are here: Lawrence of Arabia, Spotify, Great Bubble Barrier, and the OWind Turbine.\nThis is another piece stems from my teaching a course on Creative Thinking and Problem Solving based on TRIZ, titled Play and Invent, over the past 8 years or more at the Srishti Manipal Institute of Art, Design, and Technology and now at DSU School of Commerce & Management Studies, both in Bangalore, INDIA."
  },
  {
    "objectID": "content/projects/2023-07-02-Wildlife-Crossing/index.html#introduction",
    "href": "content/projects/2023-07-02-Wildlife-Crossing/index.html#introduction",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the Banff Wildlife Crossings",
    "section": "",
    "text": "Here we go with another of my TRIZ Chronicles ! The earlier editions are here: Lawrence of Arabia, Spotify, Great Bubble Barrier, and the OWind Turbine.\nThis is another piece stems from my teaching a course on Creative Thinking and Problem Solving based on TRIZ, titled Play and Invent, over the past 8 years or more at the Srishti Manipal Institute of Art, Design, and Technology and now at DSU School of Commerce & Management Studies, both in Bangalore, INDIA."
  },
  {
    "objectID": "content/projects/2023-07-02-Wildlife-Crossing/index.html#where-did-the-bear-cross-the-road",
    "href": "content/projects/2023-07-02-Wildlife-Crossing/index.html#where-did-the-bear-cross-the-road",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the Banff Wildlife Crossings",
    "section": "\n Where did the Bear cross the Road?",
    "text": "Where did the Bear cross the Road?\n\nWildlife roadkill was a serious problem in Banff National Park in Canada, for both wildlife and motorists. The problem was tackled beautifully by Parks Canada using a system of tunnels and stunning natural-looking overpasses.\nWithout further ado, let us do a TRIZ Analysis of this remarkable set of inventions."
  },
  {
    "objectID": "content/projects/2023-07-02-Wildlife-Crossing/index.html#a-triz-analysis-of-the-banff-wildlife-crossings",
    "href": "content/projects/2023-07-02-Wildlife-Crossing/index.html#a-triz-analysis-of-the-banff-wildlife-crossings",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the Banff Wildlife Crossings",
    "section": "\n A TRIZ Analysis of the Banff Wildlife Crossings",
    "text": "A TRIZ Analysis of the Banff Wildlife Crossings\nFor a TRIZ workflow, we proceed as before:\n\nFirst, using the method described in Open Source TRIZ, we identify knobs or parameters within the situation\nWe see how turning these could lead to identifying a Statement / Cause for a Problem in the form of a Contradiction.\nRe-word the plain English Contradiction into TRIZ Parameters and look it up in the Contradiction Matrix. Obtain the Inventive Principles.\nApply these Inventive Principles into your Problem and solve it.\n\nHere below is a quick Ishikawa Diagram to help us identify the Parameters of this Problem:\n\n\n\n\n\n\n\n\nLooking at this Diagram, with the aspects identified, we could pair them off and see how they affect one another. In doing so, we could make up several problem statements. Let us state some of our Problems\n\n\nSignage would help drivers slow down, but slowing down may make journey less enjoyable.\n\nSlowing Down may improve animal movement but may endanger humans.\n\nClearing the Vegetation may make animals more visible, but may also make vehicles visible to animals and affect their movement\n\nAs you can see, many different problems and contradictions await our attention. Let us cut to the chase and state our Administrative Contradiction(AC) in plain English:\n\n\n\n\n\n\n Administrative Contradiction\n\n\n\nAC: We wish to drive at high speeds, but not kill migrating wild animals nor endanger our vehicles.\n\n\nWhat would an IFR be in this situation? How “unreasonable” can we be? Let us try:\n\n\n\n\n\n\n Ideal Final Result\n\n\n\nThe Animals and Humans should both use the Road whenever they want without being mutually affected!\n\n\nLet us take our AC and convert it into a Technical Contradiction(TC), keeping this IFR in mind. We will look at the 48 TRIZ Parameters in the TRIZ Contradiction Matrix and see which Parameter we want to improve, while not worsening another. Here is what we can obtain. We will analyze the Contradictions both ways1:\n\n\n\n\n\n\n Technical Contradictions\n\n\n\n\n\nTC1: Improve Loss of Time(26) and not worsen Duration of Action by Moving Object(12)\n\n\n\n\nSince our IFR is all about time, we have chosen the TRIZ Parameters that are time-oriented. We could have also tried the following:\n\n\nTC2: Improve Volume of Moving Object(7) and not worsen Loss of Time(26)\n\n\nTC3: Improve Other harmful factors Acting on the System(40) and not worsen Duration of Action by Stationary Object(13)2\n\nThese include Volume and External Factors which are not quite there in out IFR. Is there a Physical Contradiction(PC)3 possible here?\n\n\n\n\n\n\n\n Physical Contradiction\n\n\n\nIn fact our IFR is nearly worded as a PC: The Vehicles and the Animals must use the Road at the Same Time."
  },
  {
    "objectID": "content/projects/2023-07-02-Wildlife-Crossing/index.html#solving-the-technical-contradictions",
    "href": "content/projects/2023-07-02-Wildlife-Crossing/index.html#solving-the-technical-contradictions",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the Banff Wildlife Crossings",
    "section": "\n Solving the Technical Contradictions",
    "text": "Solving the Technical Contradictions\nLet us take the set of TC-s into the Contradiction Matrix and arrive at the list of TRIZ Inventive Principles. Here is what the Matrix suggests:\nFor TC1:\n\n3(Local Quality)\n17(Another Dimension)\n28(Mechanics Substitution)\n8(Anti-Weight); and\n19(Periodic Action),\n10(Preliminary Action)\n\nHmm…based on the PC, we may have expected a Separation in Space solution, suggested by Another Dimension and Local Quality. Viewing these Inventive Principles as we Generalized Solutions, we try to map these back into the Problem at hand. In keeping with the metaphoric/analogic way of thinking that TRIZ embodies, I deliberately use many visual hints here from math, physics, geography, and biology.\n\n3(Local Quality): So something that is local…local where? Well, along the highway, of course. So something that is located a specific points along the highway. Nice but not really clear enough to be actionable, yet.\n17(Another Dimension): Well, well. The Road is a linear thing and has length and breadth. What would we use for another dimension? Height, of course! So, we need to go either above the road or below! And that leads us to a …bridge and a tunnel !!!\n\nThe other Inventive Principles are, to me, not evocative enough in this instance. But we already have a decent idea: we could just imagine a set of Local bridges and tunnels that occur at Periodic Intervals along the highway. And that is exactly what Parks Canada have done.\n\nHere is the solution in action:"
  },
  {
    "objectID": "content/projects/2023-07-02-Wildlife-Crossing/index.html#using-triz-separation-principles",
    "href": "content/projects/2023-07-02-Wildlife-Crossing/index.html#using-triz-separation-principles",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the Banff Wildlife Crossings",
    "section": "\n Using TRIZ Separation Principles",
    "text": "Using TRIZ Separation Principles\nAs Hipple explains, there is frequently an underlying physical parameter, such as length, breadth, weight, or energy, or speed for example that lies at the root of our Technical Contradiction. Our IFR states that we want the humans and animals to use the road at the same TIME, and hence Separation in Space becomes a nice way to think of a solution.\nThat’s a wrap! In the next episode of the #TRIZ Chronicles, I wish to step even further out of my area of expertise and dabble in HR! I think looking at some of the institution-building ideas in Ricardo Semler’s book, Maverick would be a good idea!"
  },
  {
    "objectID": "content/projects/2023-07-02-Wildlife-Crossing/index.html#readings",
    "href": "content/projects/2023-07-02-Wildlife-Crossing/index.html#readings",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the Banff Wildlife Crossings",
    "section": "\n Readings",
    "text": "Readings\n\nhttps://discoverapega.ca/stories/wildlife-crossings-key-to-highway-safety-in-banff/\nJack Hipple, The Ideal Result and How to Achieve It. Springer; 2012th edition (June 26, 2012)\nValery Souchkov, Defining Contradictions. http://www.xtriz.com/Training/TRIZ_DefineContradiction_Tutorial.pdf\n\nOpen Source TRIZ: Making Contradictions. https://www.youtube.com/watch?v=cah0OhCH55k\n\nScrucca (2004)\nCano, Moguerza, and Redchuk (2012)"
  },
  {
    "objectID": "content/projects/2023-07-02-Wildlife-Crossing/index.html#footnotes",
    "href": "content/projects/2023-07-02-Wildlife-Crossing/index.html#footnotes",
    "title": "The TRIZ Chronicles: TRIZ Analysis of the Banff Wildlife Crossings",
    "section": "Footnotes",
    "text": "Footnotes\n\nThe Contradiction Matrix is not quite symmetric, so stating the Contradiction both ways allows us to access a slightly larger set of Inventive Principles from two cells of the Matrix.↩︎\nAnimals are nearly stationary compared to the vehicles.↩︎\nArriving at Physical Contradictions is not always easy! If we can, then there are a very crisp set of TRIZ Separation Principles that we can apply to solve the Problem.↩︎"
  },
  {
    "objectID": "content/posts/listing.html",
    "href": "content/posts/listing.html",
    "title": "Applied Metaphors: Learning TRIZ, Complexity, Data/Stats/ML using Metaphors",
    "section": "",
    "text": "Using Lordicons, Fontawesome Icons,Academicons, and Iconify Icons\n\n\n\n\n\n\nArvind V.\n\n\nJan 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nNutshell: Expandable Explanations\n\n\n\n\n\n\nDavid Schoch\n\n\n\n\n\n\n\n\n\n\n\n\nUsing sketch\n\n\n\n\n\n\nArvind Venkatadri\n\n\nJan 9, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nGenerating Fake Data in R\n\n\n\n\n\n\nArvind V\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top",
    "crumbs": [
      "Posts"
    ]
  },
  {
    "objectID": "content/posts/16-FakeData/index.html",
    "href": "content/posts/16-FakeData/index.html",
    "title": "Generating Fake Data in R",
    "section": "",
    "text": "Often we need to generate fake data for teaching and demo purposes. This post uncovers several different packages for this purpose."
  },
  {
    "objectID": "content/posts/16-FakeData/index.html#introduction",
    "href": "content/posts/16-FakeData/index.html#introduction",
    "title": "Generating Fake Data in R",
    "section": "",
    "text": "Often we need to generate fake data for teaching and demo purposes. This post uncovers several different packages for this purpose."
  },
  {
    "objectID": "content/posts/16-FakeData/index.html#set-up-the-r-packages",
    "href": "content/posts/16-FakeData/index.html#set-up-the-r-packages",
    "title": "Generating Fake Data in R",
    "section": "Set Up the R Packages",
    "text": "Set Up the R Packages\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(ggformula)\nlibrary(vtable)\n\n# Generate Data\n# library(simulate) TO BE FOUND AND INSTALLED!!!!\nlibrary(regressinator)\nlibrary(holodeck)\nlibrary(explore)\nlibrary(charlatan)\nlibrary(ids) # animals, adjectives, sentences, and proquints\nlibrary(rcorpora)\nlibrary(simstudy)"
  },
  {
    "objectID": "content/posts/16-FakeData/index.html#using-simulate",
    "href": "content/posts/16-FakeData/index.html#using-simulate",
    "title": "Generating Fake Data in R",
    "section": "Using simulate",
    "text": "Using simulate\n\nsim_bernoulli(prob = 0.2, params = NULL, data = df)\nsim_beta(shape1 = 0.2, shape2 = 0.8, params = NULL)"
  },
  {
    "objectID": "content/posts/16-FakeData/index.html#using-regressinator",
    "href": "content/posts/16-FakeData/index.html#using-regressinator",
    "title": "Generating Fake Data in R",
    "section": "Using regressinator",
    "text": "Using regressinator\nhttps://www.refsmmat.com/regressinator/\n\nThe regressinator is a pedagogical tool for conducting simulations of regression analyses and diagnostics. It can:\n\nSimulate populations with predictor variables from arbitrary distributions\nSimulate response variables that are functions of the predictor variables plus error, or are drawn from a distribution related to the predictors\nGiven a model, simulate from the population sampling distribution of that model’s estimates\nGiven a model fit to data, generate new simulated data based on the model fit\nFacilitate lineup plots comparing diagnostics on the fitted model to diagnostics where all model assumptions are met.\n\n\n\nlibrary(regressinator)\n\nlinear_pop &lt;- population(\n  x1 = predictor(\"rnorm\", mean = 4, sd = 10),\n  x2 = predictor(\"runif\", min = 0, max = 10),\n  y = response(\n    0.7 + 2.2 * x1 - 0.2 * x2, # relationship between X and Y\n    family = gaussian(),       # link function and response distribution\n    error_scale = 1.5          # sd; errors are scaled by this amount\n  )\n)\n\nIn general, population() defines a population according to the following relationship:\n\\[\nY ∼ Some ~ Distribution\\\\\n\\] \\[\n~g(E[Y | X = x]) = \\mu(x)\\\\\n\\] \\[\nwhere ~ μ(x)=any~function~of~x\\\\\n\\]\nIf family is not specified the default is Gaussian, and the link function g is identity.\nWe can create a population with binary outcomes and a logistic link function:\n\nlogistic_pop &lt;- population(\n  x1 = predictor(\"rnorm\", mean = 0, sd = 10),\n  x2 = predictor(\"runif\", min = 0, max = 10),\n  y = response(0.7 + 2.2 * x1 - 0.2 * x2,\n               family = binomial(link = \"logit\"))\n)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html",
    "href": "content/labs/r-labs/graphics/wizardy.html",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "",
    "text": "This Quarto document is part of my Workshop in R. The material is based on A Layered Grammar of Graphics by Hadley Wickham, and more specifically on the tutorials by Cedric Scherer. The course is meant for First Year students pursuing a Degree in Art and Design. The intent is to build Skill in coding in R, and also appreciate R as a way to metaphorically visualize information of various kinds, using predominantly geometric figures and structures.\nAll Quarto files combine code, text, web-images, and figures developed using code. Everything is text; code chunks are enclosed in fences (```)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#introduction",
    "href": "content/labs/r-labs/graphics/wizardy.html#introduction",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "",
    "text": "This Quarto document is part of my Workshop in R. The material is based on A Layered Grammar of Graphics by Hadley Wickham, and more specifically on the tutorials by Cedric Scherer. The course is meant for First Year students pursuing a Degree in Art and Design. The intent is to build Skill in coding in R, and also appreciate R as a way to metaphorically visualize information of various kinds, using predominantly geometric figures and structures.\nAll Quarto files combine code, text, web-images, and figures developed using code. Everything is text; code chunks are enclosed in fences (```)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#goals",
    "href": "content/labs/r-labs/graphics/wizardy.html#goals",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "Goals",
    "text": "Goals\n\n(Re)Understand different kinds of data variables\nAppreciate how they can be identified based on the Interrogative Pronouns they answer to\nUnderstand how each kind of variable lends itself to a specific choice of colour scale in the data visualization."
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#pedagogical-note",
    "href": "content/labs/r-labs/graphics/wizardy.html#pedagogical-note",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "Pedagogical Note",
    "text": "Pedagogical Note\nThe method followed will be based on PRIMM:\n\n\nPREDICT Inspect the code and guess at what the code might do, write predictions\n\n\nRUN the code provided and check what happens\n\nINFER what the parameters of the code do and write comments to explain. What bells and whistles can you see?\n\nMODIFY the parameters code provided to understand the options available. Write comments to show what you have aimed for and achieved.\n\nMAKE : take an idea/concept of your own, and graph it.\n\nIn the following, there is some boiler plate code demonstrating the use of colour palettes in R. There are places where YOUR TURN is mention; copy and play with the boiler plate code to see what happens !"
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#setting-up-r-packages",
    "href": "content/labs/r-labs/graphics/wizardy.html#setting-up-r-packages",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\nLet’s load up a few packages that we need to start:\n\nlibrary(tidyverse)   ## data science package collection (incl. the ggplot2 package)\nlibrary(systemfonts) ## use custom fonts (need to be installed on your OS)  \nlibrary(scico)       ## scico color palettes(http://www.fabiocrameri.ch/colourmaps.php) in R \nlibrary(ggtext)      ## add improved text rendering to ggplot2\nlibrary(ggforce)     ## add missing functionality to ggplot2\nlibrary(ggdist)      ## add uncertainty visualizations to ggplot2\nlibrary(magick)      ## load images into R\nlibrary(patchwork)   ## combine outputs from ggplot2\nlibrary(kableExtra)  ## Produces attractive tables\nlibrary(palmerpenguins)\n\nlibrary(showtext)   ## add google fonts to plots\n\nWe will want to add a few new fonts to our graphs. The best way (currently) is to use the showtext package ( which we loaded above) to bring into our work fonts from Google. To view and select the fonts you might want to work with, spend some time looking over:\n\nGoogle Webfonts Helper App\nGoogle Fonts\n\n\nfont_add_google(\"Gochi Hand\", \"gochi\")\nfont_add_google(\"Schoolbell\", \"bell\")\nfont_add_google(\"Galada\", \"galada\")\nfont_add_google(\"Schoolbell\", \"bell\")\nfont_add_google(\"Roboto\", \"roboto\")\nfont_add_google(\"Noto Sans\", \"noto\")\nfont_add_google(\"Uchen\", \"uchen\")\nfont_add_google(\"Ibarra Real Nova\", \"ibarra\")\nfont_add_google(\"Open Sans\", \"open\")\nfont_add_google(\"Anton\", \"anton\")\nfont_add_google(\"Tangerine\", \"tangerine\")\n\nshowtext_auto() # set the google fonts as default\n\nWe will work with a familiar dataset, so that we can concentrate on the chart aesthetics, without having to spend time getting used to the data: the penguins dataset again, from the palmerpenguins package."
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#data",
    "href": "content/labs/r-labs/graphics/wizardy.html#data",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "Data",
    "text": "Data\nAlways start your work with a table of the data:\n\npenguins &lt;- penguins %&gt;% drop_na() # remove data containing missing data\n\n## Create a nicely formatted table\n## uses `kableExtra` package\n## \npenguins %&gt;% \n  kableExtra::kbl() %&gt;%\n  kableExtra::kable_paper(full_width = TRUE) %&gt;%\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"condensed\", \"responsive\")) %&gt;%\n  kableExtra::scroll_box(width = \"700px\", height = \"500px\")\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n2007\n\n\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n2007\n\n\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nfemale\n2007\n\n\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n2007\n\n\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmale\n2007\n\n\nAdelie\nTorgersen\n38.9\n17.8\n181\n3625\nfemale\n2007\n\n\nAdelie\nTorgersen\n39.2\n19.6\n195\n4675\nmale\n2007\n\n\nAdelie\nTorgersen\n41.1\n17.6\n182\n3200\nfemale\n2007\n\n\nAdelie\nTorgersen\n38.6\n21.2\n191\n3800\nmale\n2007\n\n\nAdelie\nTorgersen\n34.6\n21.1\n198\n4400\nmale\n2007\n\n\nAdelie\nTorgersen\n36.6\n17.8\n185\n3700\nfemale\n2007\n\n\nAdelie\nTorgersen\n38.7\n19.0\n195\n3450\nfemale\n2007\n\n\nAdelie\nTorgersen\n42.5\n20.7\n197\n4500\nmale\n2007\n\n\nAdelie\nTorgersen\n34.4\n18.4\n184\n3325\nfemale\n2007\n\n\nAdelie\nTorgersen\n46.0\n21.5\n194\n4200\nmale\n2007\n\n\nAdelie\nBiscoe\n37.8\n18.3\n174\n3400\nfemale\n2007\n\n\nAdelie\nBiscoe\n37.7\n18.7\n180\n3600\nmale\n2007\n\n\nAdelie\nBiscoe\n35.9\n19.2\n189\n3800\nfemale\n2007\n\n\nAdelie\nBiscoe\n38.2\n18.1\n185\n3950\nmale\n2007\n\n\nAdelie\nBiscoe\n38.8\n17.2\n180\n3800\nmale\n2007\n\n\nAdelie\nBiscoe\n35.3\n18.9\n187\n3800\nfemale\n2007\n\n\nAdelie\nBiscoe\n40.6\n18.6\n183\n3550\nmale\n2007\n\n\nAdelie\nBiscoe\n40.5\n17.9\n187\n3200\nfemale\n2007\n\n\nAdelie\nBiscoe\n37.9\n18.6\n172\n3150\nfemale\n2007\n\n\nAdelie\nBiscoe\n40.5\n18.9\n180\n3950\nmale\n2007\n\n\nAdelie\nDream\n39.5\n16.7\n178\n3250\nfemale\n2007\n\n\nAdelie\nDream\n37.2\n18.1\n178\n3900\nmale\n2007\n\n\nAdelie\nDream\n39.5\n17.8\n188\n3300\nfemale\n2007\n\n\nAdelie\nDream\n40.9\n18.9\n184\n3900\nmale\n2007\n\n\nAdelie\nDream\n36.4\n17.0\n195\n3325\nfemale\n2007\n\n\nAdelie\nDream\n39.2\n21.1\n196\n4150\nmale\n2007\n\n\nAdelie\nDream\n38.8\n20.0\n190\n3950\nmale\n2007\n\n\nAdelie\nDream\n42.2\n18.5\n180\n3550\nfemale\n2007\n\n\nAdelie\nDream\n37.6\n19.3\n181\n3300\nfemale\n2007\n\n\nAdelie\nDream\n39.8\n19.1\n184\n4650\nmale\n2007\n\n\nAdelie\nDream\n36.5\n18.0\n182\n3150\nfemale\n2007\n\n\nAdelie\nDream\n40.8\n18.4\n195\n3900\nmale\n2007\n\n\nAdelie\nDream\n36.0\n18.5\n186\n3100\nfemale\n2007\n\n\nAdelie\nDream\n44.1\n19.7\n196\n4400\nmale\n2007\n\n\nAdelie\nDream\n37.0\n16.9\n185\n3000\nfemale\n2007\n\n\nAdelie\nDream\n39.6\n18.8\n190\n4600\nmale\n2007\n\n\nAdelie\nDream\n41.1\n19.0\n182\n3425\nmale\n2007\n\n\nAdelie\nDream\n36.0\n17.9\n190\n3450\nfemale\n2007\n\n\nAdelie\nDream\n42.3\n21.2\n191\n4150\nmale\n2007\n\n\nAdelie\nBiscoe\n39.6\n17.7\n186\n3500\nfemale\n2008\n\n\nAdelie\nBiscoe\n40.1\n18.9\n188\n4300\nmale\n2008\n\n\nAdelie\nBiscoe\n35.0\n17.9\n190\n3450\nfemale\n2008\n\n\nAdelie\nBiscoe\n42.0\n19.5\n200\n4050\nmale\n2008\n\n\nAdelie\nBiscoe\n34.5\n18.1\n187\n2900\nfemale\n2008\n\n\nAdelie\nBiscoe\n41.4\n18.6\n191\n3700\nmale\n2008\n\n\nAdelie\nBiscoe\n39.0\n17.5\n186\n3550\nfemale\n2008\n\n\nAdelie\nBiscoe\n40.6\n18.8\n193\n3800\nmale\n2008\n\n\nAdelie\nBiscoe\n36.5\n16.6\n181\n2850\nfemale\n2008\n\n\nAdelie\nBiscoe\n37.6\n19.1\n194\n3750\nmale\n2008\n\n\nAdelie\nBiscoe\n35.7\n16.9\n185\n3150\nfemale\n2008\n\n\nAdelie\nBiscoe\n41.3\n21.1\n195\n4400\nmale\n2008\n\n\nAdelie\nBiscoe\n37.6\n17.0\n185\n3600\nfemale\n2008\n\n\nAdelie\nBiscoe\n41.1\n18.2\n192\n4050\nmale\n2008\n\n\nAdelie\nBiscoe\n36.4\n17.1\n184\n2850\nfemale\n2008\n\n\nAdelie\nBiscoe\n41.6\n18.0\n192\n3950\nmale\n2008\n\n\nAdelie\nBiscoe\n35.5\n16.2\n195\n3350\nfemale\n2008\n\n\nAdelie\nBiscoe\n41.1\n19.1\n188\n4100\nmale\n2008\n\n\nAdelie\nTorgersen\n35.9\n16.6\n190\n3050\nfemale\n2008\n\n\nAdelie\nTorgersen\n41.8\n19.4\n198\n4450\nmale\n2008\n\n\nAdelie\nTorgersen\n33.5\n19.0\n190\n3600\nfemale\n2008\n\n\nAdelie\nTorgersen\n39.7\n18.4\n190\n3900\nmale\n2008\n\n\nAdelie\nTorgersen\n39.6\n17.2\n196\n3550\nfemale\n2008\n\n\nAdelie\nTorgersen\n45.8\n18.9\n197\n4150\nmale\n2008\n\n\nAdelie\nTorgersen\n35.5\n17.5\n190\n3700\nfemale\n2008\n\n\nAdelie\nTorgersen\n42.8\n18.5\n195\n4250\nmale\n2008\n\n\nAdelie\nTorgersen\n40.9\n16.8\n191\n3700\nfemale\n2008\n\n\nAdelie\nTorgersen\n37.2\n19.4\n184\n3900\nmale\n2008\n\n\nAdelie\nTorgersen\n36.2\n16.1\n187\n3550\nfemale\n2008\n\n\nAdelie\nTorgersen\n42.1\n19.1\n195\n4000\nmale\n2008\n\n\nAdelie\nTorgersen\n34.6\n17.2\n189\n3200\nfemale\n2008\n\n\nAdelie\nTorgersen\n42.9\n17.6\n196\n4700\nmale\n2008\n\n\nAdelie\nTorgersen\n36.7\n18.8\n187\n3800\nfemale\n2008\n\n\nAdelie\nTorgersen\n35.1\n19.4\n193\n4200\nmale\n2008\n\n\nAdelie\nDream\n37.3\n17.8\n191\n3350\nfemale\n2008\n\n\nAdelie\nDream\n41.3\n20.3\n194\n3550\nmale\n2008\n\n\nAdelie\nDream\n36.3\n19.5\n190\n3800\nmale\n2008\n\n\nAdelie\nDream\n36.9\n18.6\n189\n3500\nfemale\n2008\n\n\nAdelie\nDream\n38.3\n19.2\n189\n3950\nmale\n2008\n\n\nAdelie\nDream\n38.9\n18.8\n190\n3600\nfemale\n2008\n\n\nAdelie\nDream\n35.7\n18.0\n202\n3550\nfemale\n2008\n\n\nAdelie\nDream\n41.1\n18.1\n205\n4300\nmale\n2008\n\n\nAdelie\nDream\n34.0\n17.1\n185\n3400\nfemale\n2008\n\n\nAdelie\nDream\n39.6\n18.1\n186\n4450\nmale\n2008\n\n\nAdelie\nDream\n36.2\n17.3\n187\n3300\nfemale\n2008\n\n\nAdelie\nDream\n40.8\n18.9\n208\n4300\nmale\n2008\n\n\nAdelie\nDream\n38.1\n18.6\n190\n3700\nfemale\n2008\n\n\nAdelie\nDream\n40.3\n18.5\n196\n4350\nmale\n2008\n\n\nAdelie\nDream\n33.1\n16.1\n178\n2900\nfemale\n2008\n\n\nAdelie\nDream\n43.2\n18.5\n192\n4100\nmale\n2008\n\n\nAdelie\nBiscoe\n35.0\n17.9\n192\n3725\nfemale\n2009\n\n\nAdelie\nBiscoe\n41.0\n20.0\n203\n4725\nmale\n2009\n\n\nAdelie\nBiscoe\n37.7\n16.0\n183\n3075\nfemale\n2009\n\n\nAdelie\nBiscoe\n37.8\n20.0\n190\n4250\nmale\n2009\n\n\nAdelie\nBiscoe\n37.9\n18.6\n193\n2925\nfemale\n2009\n\n\nAdelie\nBiscoe\n39.7\n18.9\n184\n3550\nmale\n2009\n\n\nAdelie\nBiscoe\n38.6\n17.2\n199\n3750\nfemale\n2009\n\n\nAdelie\nBiscoe\n38.2\n20.0\n190\n3900\nmale\n2009\n\n\nAdelie\nBiscoe\n38.1\n17.0\n181\n3175\nfemale\n2009\n\n\nAdelie\nBiscoe\n43.2\n19.0\n197\n4775\nmale\n2009\n\n\nAdelie\nBiscoe\n38.1\n16.5\n198\n3825\nfemale\n2009\n\n\nAdelie\nBiscoe\n45.6\n20.3\n191\n4600\nmale\n2009\n\n\nAdelie\nBiscoe\n39.7\n17.7\n193\n3200\nfemale\n2009\n\n\nAdelie\nBiscoe\n42.2\n19.5\n197\n4275\nmale\n2009\n\n\nAdelie\nBiscoe\n39.6\n20.7\n191\n3900\nfemale\n2009\n\n\nAdelie\nBiscoe\n42.7\n18.3\n196\n4075\nmale\n2009\n\n\nAdelie\nTorgersen\n38.6\n17.0\n188\n2900\nfemale\n2009\n\n\nAdelie\nTorgersen\n37.3\n20.5\n199\n3775\nmale\n2009\n\n\nAdelie\nTorgersen\n35.7\n17.0\n189\n3350\nfemale\n2009\n\n\nAdelie\nTorgersen\n41.1\n18.6\n189\n3325\nmale\n2009\n\n\nAdelie\nTorgersen\n36.2\n17.2\n187\n3150\nfemale\n2009\n\n\nAdelie\nTorgersen\n37.7\n19.8\n198\n3500\nmale\n2009\n\n\nAdelie\nTorgersen\n40.2\n17.0\n176\n3450\nfemale\n2009\n\n\nAdelie\nTorgersen\n41.4\n18.5\n202\n3875\nmale\n2009\n\n\nAdelie\nTorgersen\n35.2\n15.9\n186\n3050\nfemale\n2009\n\n\nAdelie\nTorgersen\n40.6\n19.0\n199\n4000\nmale\n2009\n\n\nAdelie\nTorgersen\n38.8\n17.6\n191\n3275\nfemale\n2009\n\n\nAdelie\nTorgersen\n41.5\n18.3\n195\n4300\nmale\n2009\n\n\nAdelie\nTorgersen\n39.0\n17.1\n191\n3050\nfemale\n2009\n\n\nAdelie\nTorgersen\n44.1\n18.0\n210\n4000\nmale\n2009\n\n\nAdelie\nTorgersen\n38.5\n17.9\n190\n3325\nfemale\n2009\n\n\nAdelie\nTorgersen\n43.1\n19.2\n197\n3500\nmale\n2009\n\n\nAdelie\nDream\n36.8\n18.5\n193\n3500\nfemale\n2009\n\n\nAdelie\nDream\n37.5\n18.5\n199\n4475\nmale\n2009\n\n\nAdelie\nDream\n38.1\n17.6\n187\n3425\nfemale\n2009\n\n\nAdelie\nDream\n41.1\n17.5\n190\n3900\nmale\n2009\n\n\nAdelie\nDream\n35.6\n17.5\n191\n3175\nfemale\n2009\n\n\nAdelie\nDream\n40.2\n20.1\n200\n3975\nmale\n2009\n\n\nAdelie\nDream\n37.0\n16.5\n185\n3400\nfemale\n2009\n\n\nAdelie\nDream\n39.7\n17.9\n193\n4250\nmale\n2009\n\n\nAdelie\nDream\n40.2\n17.1\n193\n3400\nfemale\n2009\n\n\nAdelie\nDream\n40.6\n17.2\n187\n3475\nmale\n2009\n\n\nAdelie\nDream\n32.1\n15.5\n188\n3050\nfemale\n2009\n\n\nAdelie\nDream\n40.7\n17.0\n190\n3725\nmale\n2009\n\n\nAdelie\nDream\n37.3\n16.8\n192\n3000\nfemale\n2009\n\n\nAdelie\nDream\n39.0\n18.7\n185\n3650\nmale\n2009\n\n\nAdelie\nDream\n39.2\n18.6\n190\n4250\nmale\n2009\n\n\nAdelie\nDream\n36.6\n18.4\n184\n3475\nfemale\n2009\n\n\nAdelie\nDream\n36.0\n17.8\n195\n3450\nfemale\n2009\n\n\nAdelie\nDream\n37.8\n18.1\n193\n3750\nmale\n2009\n\n\nAdelie\nDream\n36.0\n17.1\n187\n3700\nfemale\n2009\n\n\nAdelie\nDream\n41.5\n18.5\n201\n4000\nmale\n2009\n\n\nGentoo\nBiscoe\n46.1\n13.2\n211\n4500\nfemale\n2007\n\n\nGentoo\nBiscoe\n50.0\n16.3\n230\n5700\nmale\n2007\n\n\nGentoo\nBiscoe\n48.7\n14.1\n210\n4450\nfemale\n2007\n\n\nGentoo\nBiscoe\n50.0\n15.2\n218\n5700\nmale\n2007\n\n\nGentoo\nBiscoe\n47.6\n14.5\n215\n5400\nmale\n2007\n\n\nGentoo\nBiscoe\n46.5\n13.5\n210\n4550\nfemale\n2007\n\n\nGentoo\nBiscoe\n45.4\n14.6\n211\n4800\nfemale\n2007\n\n\nGentoo\nBiscoe\n46.7\n15.3\n219\n5200\nmale\n2007\n\n\nGentoo\nBiscoe\n43.3\n13.4\n209\n4400\nfemale\n2007\n\n\nGentoo\nBiscoe\n46.8\n15.4\n215\n5150\nmale\n2007\n\n\nGentoo\nBiscoe\n40.9\n13.7\n214\n4650\nfemale\n2007\n\n\nGentoo\nBiscoe\n49.0\n16.1\n216\n5550\nmale\n2007\n\n\nGentoo\nBiscoe\n45.5\n13.7\n214\n4650\nfemale\n2007\n\n\nGentoo\nBiscoe\n48.4\n14.6\n213\n5850\nmale\n2007\n\n\nGentoo\nBiscoe\n45.8\n14.6\n210\n4200\nfemale\n2007\n\n\nGentoo\nBiscoe\n49.3\n15.7\n217\n5850\nmale\n2007\n\n\nGentoo\nBiscoe\n42.0\n13.5\n210\n4150\nfemale\n2007\n\n\nGentoo\nBiscoe\n49.2\n15.2\n221\n6300\nmale\n2007\n\n\nGentoo\nBiscoe\n46.2\n14.5\n209\n4800\nfemale\n2007\n\n\nGentoo\nBiscoe\n48.7\n15.1\n222\n5350\nmale\n2007\n\n\nGentoo\nBiscoe\n50.2\n14.3\n218\n5700\nmale\n2007\n\n\nGentoo\nBiscoe\n45.1\n14.5\n215\n5000\nfemale\n2007\n\n\nGentoo\nBiscoe\n46.5\n14.5\n213\n4400\nfemale\n2007\n\n\nGentoo\nBiscoe\n46.3\n15.8\n215\n5050\nmale\n2007\n\n\nGentoo\nBiscoe\n42.9\n13.1\n215\n5000\nfemale\n2007\n\n\nGentoo\nBiscoe\n46.1\n15.1\n215\n5100\nmale\n2007\n\n\nGentoo\nBiscoe\n47.8\n15.0\n215\n5650\nmale\n2007\n\n\nGentoo\nBiscoe\n48.2\n14.3\n210\n4600\nfemale\n2007\n\n\nGentoo\nBiscoe\n50.0\n15.3\n220\n5550\nmale\n2007\n\n\nGentoo\nBiscoe\n47.3\n15.3\n222\n5250\nmale\n2007\n\n\nGentoo\nBiscoe\n42.8\n14.2\n209\n4700\nfemale\n2007\n\n\nGentoo\nBiscoe\n45.1\n14.5\n207\n5050\nfemale\n2007\n\n\nGentoo\nBiscoe\n59.6\n17.0\n230\n6050\nmale\n2007\n\n\nGentoo\nBiscoe\n49.1\n14.8\n220\n5150\nfemale\n2008\n\n\nGentoo\nBiscoe\n48.4\n16.3\n220\n5400\nmale\n2008\n\n\nGentoo\nBiscoe\n42.6\n13.7\n213\n4950\nfemale\n2008\n\n\nGentoo\nBiscoe\n44.4\n17.3\n219\n5250\nmale\n2008\n\n\nGentoo\nBiscoe\n44.0\n13.6\n208\n4350\nfemale\n2008\n\n\nGentoo\nBiscoe\n48.7\n15.7\n208\n5350\nmale\n2008\n\n\nGentoo\nBiscoe\n42.7\n13.7\n208\n3950\nfemale\n2008\n\n\nGentoo\nBiscoe\n49.6\n16.0\n225\n5700\nmale\n2008\n\n\nGentoo\nBiscoe\n45.3\n13.7\n210\n4300\nfemale\n2008\n\n\nGentoo\nBiscoe\n49.6\n15.0\n216\n4750\nmale\n2008\n\n\nGentoo\nBiscoe\n50.5\n15.9\n222\n5550\nmale\n2008\n\n\nGentoo\nBiscoe\n43.6\n13.9\n217\n4900\nfemale\n2008\n\n\nGentoo\nBiscoe\n45.5\n13.9\n210\n4200\nfemale\n2008\n\n\nGentoo\nBiscoe\n50.5\n15.9\n225\n5400\nmale\n2008\n\n\nGentoo\nBiscoe\n44.9\n13.3\n213\n5100\nfemale\n2008\n\n\nGentoo\nBiscoe\n45.2\n15.8\n215\n5300\nmale\n2008\n\n\nGentoo\nBiscoe\n46.6\n14.2\n210\n4850\nfemale\n2008\n\n\nGentoo\nBiscoe\n48.5\n14.1\n220\n5300\nmale\n2008\n\n\nGentoo\nBiscoe\n45.1\n14.4\n210\n4400\nfemale\n2008\n\n\nGentoo\nBiscoe\n50.1\n15.0\n225\n5000\nmale\n2008\n\n\nGentoo\nBiscoe\n46.5\n14.4\n217\n4900\nfemale\n2008\n\n\nGentoo\nBiscoe\n45.0\n15.4\n220\n5050\nmale\n2008\n\n\nGentoo\nBiscoe\n43.8\n13.9\n208\n4300\nfemale\n2008\n\n\nGentoo\nBiscoe\n45.5\n15.0\n220\n5000\nmale\n2008\n\n\nGentoo\nBiscoe\n43.2\n14.5\n208\n4450\nfemale\n2008\n\n\nGentoo\nBiscoe\n50.4\n15.3\n224\n5550\nmale\n2008\n\n\nGentoo\nBiscoe\n45.3\n13.8\n208\n4200\nfemale\n2008\n\n\nGentoo\nBiscoe\n46.2\n14.9\n221\n5300\nmale\n2008\n\n\nGentoo\nBiscoe\n45.7\n13.9\n214\n4400\nfemale\n2008\n\n\nGentoo\nBiscoe\n54.3\n15.7\n231\n5650\nmale\n2008\n\n\nGentoo\nBiscoe\n45.8\n14.2\n219\n4700\nfemale\n2008\n\n\nGentoo\nBiscoe\n49.8\n16.8\n230\n5700\nmale\n2008\n\n\nGentoo\nBiscoe\n49.5\n16.2\n229\n5800\nmale\n2008\n\n\nGentoo\nBiscoe\n43.5\n14.2\n220\n4700\nfemale\n2008\n\n\nGentoo\nBiscoe\n50.7\n15.0\n223\n5550\nmale\n2008\n\n\nGentoo\nBiscoe\n47.7\n15.0\n216\n4750\nfemale\n2008\n\n\nGentoo\nBiscoe\n46.4\n15.6\n221\n5000\nmale\n2008\n\n\nGentoo\nBiscoe\n48.2\n15.6\n221\n5100\nmale\n2008\n\n\nGentoo\nBiscoe\n46.5\n14.8\n217\n5200\nfemale\n2008\n\n\nGentoo\nBiscoe\n46.4\n15.0\n216\n4700\nfemale\n2008\n\n\nGentoo\nBiscoe\n48.6\n16.0\n230\n5800\nmale\n2008\n\n\nGentoo\nBiscoe\n47.5\n14.2\n209\n4600\nfemale\n2008\n\n\nGentoo\nBiscoe\n51.1\n16.3\n220\n6000\nmale\n2008\n\n\nGentoo\nBiscoe\n45.2\n13.8\n215\n4750\nfemale\n2008\n\n\nGentoo\nBiscoe\n45.2\n16.4\n223\n5950\nmale\n2008\n\n\nGentoo\nBiscoe\n49.1\n14.5\n212\n4625\nfemale\n2009\n\n\nGentoo\nBiscoe\n52.5\n15.6\n221\n5450\nmale\n2009\n\n\nGentoo\nBiscoe\n47.4\n14.6\n212\n4725\nfemale\n2009\n\n\nGentoo\nBiscoe\n50.0\n15.9\n224\n5350\nmale\n2009\n\n\nGentoo\nBiscoe\n44.9\n13.8\n212\n4750\nfemale\n2009\n\n\nGentoo\nBiscoe\n50.8\n17.3\n228\n5600\nmale\n2009\n\n\nGentoo\nBiscoe\n43.4\n14.4\n218\n4600\nfemale\n2009\n\n\nGentoo\nBiscoe\n51.3\n14.2\n218\n5300\nmale\n2009\n\n\nGentoo\nBiscoe\n47.5\n14.0\n212\n4875\nfemale\n2009\n\n\nGentoo\nBiscoe\n52.1\n17.0\n230\n5550\nmale\n2009\n\n\nGentoo\nBiscoe\n47.5\n15.0\n218\n4950\nfemale\n2009\n\n\nGentoo\nBiscoe\n52.2\n17.1\n228\n5400\nmale\n2009\n\n\nGentoo\nBiscoe\n45.5\n14.5\n212\n4750\nfemale\n2009\n\n\nGentoo\nBiscoe\n49.5\n16.1\n224\n5650\nmale\n2009\n\n\nGentoo\nBiscoe\n44.5\n14.7\n214\n4850\nfemale\n2009\n\n\nGentoo\nBiscoe\n50.8\n15.7\n226\n5200\nmale\n2009\n\n\nGentoo\nBiscoe\n49.4\n15.8\n216\n4925\nmale\n2009\n\n\nGentoo\nBiscoe\n46.9\n14.6\n222\n4875\nfemale\n2009\n\n\nGentoo\nBiscoe\n48.4\n14.4\n203\n4625\nfemale\n2009\n\n\nGentoo\nBiscoe\n51.1\n16.5\n225\n5250\nmale\n2009\n\n\nGentoo\nBiscoe\n48.5\n15.0\n219\n4850\nfemale\n2009\n\n\nGentoo\nBiscoe\n55.9\n17.0\n228\n5600\nmale\n2009\n\n\nGentoo\nBiscoe\n47.2\n15.5\n215\n4975\nfemale\n2009\n\n\nGentoo\nBiscoe\n49.1\n15.0\n228\n5500\nmale\n2009\n\n\nGentoo\nBiscoe\n46.8\n16.1\n215\n5500\nmale\n2009\n\n\nGentoo\nBiscoe\n41.7\n14.7\n210\n4700\nfemale\n2009\n\n\nGentoo\nBiscoe\n53.4\n15.8\n219\n5500\nmale\n2009\n\n\nGentoo\nBiscoe\n43.3\n14.0\n208\n4575\nfemale\n2009\n\n\nGentoo\nBiscoe\n48.1\n15.1\n209\n5500\nmale\n2009\n\n\nGentoo\nBiscoe\n50.5\n15.2\n216\n5000\nfemale\n2009\n\n\nGentoo\nBiscoe\n49.8\n15.9\n229\n5950\nmale\n2009\n\n\nGentoo\nBiscoe\n43.5\n15.2\n213\n4650\nfemale\n2009\n\n\nGentoo\nBiscoe\n51.5\n16.3\n230\n5500\nmale\n2009\n\n\nGentoo\nBiscoe\n46.2\n14.1\n217\n4375\nfemale\n2009\n\n\nGentoo\nBiscoe\n55.1\n16.0\n230\n5850\nmale\n2009\n\n\nGentoo\nBiscoe\n48.8\n16.2\n222\n6000\nmale\n2009\n\n\nGentoo\nBiscoe\n47.2\n13.7\n214\n4925\nfemale\n2009\n\n\nGentoo\nBiscoe\n46.8\n14.3\n215\n4850\nfemale\n2009\n\n\nGentoo\nBiscoe\n50.4\n15.7\n222\n5750\nmale\n2009\n\n\nGentoo\nBiscoe\n45.2\n14.8\n212\n5200\nfemale\n2009\n\n\nGentoo\nBiscoe\n49.9\n16.1\n213\n5400\nmale\n2009\n\n\nChinstrap\nDream\n46.5\n17.9\n192\n3500\nfemale\n2007\n\n\nChinstrap\nDream\n50.0\n19.5\n196\n3900\nmale\n2007\n\n\nChinstrap\nDream\n51.3\n19.2\n193\n3650\nmale\n2007\n\n\nChinstrap\nDream\n45.4\n18.7\n188\n3525\nfemale\n2007\n\n\nChinstrap\nDream\n52.7\n19.8\n197\n3725\nmale\n2007\n\n\nChinstrap\nDream\n45.2\n17.8\n198\n3950\nfemale\n2007\n\n\nChinstrap\nDream\n46.1\n18.2\n178\n3250\nfemale\n2007\n\n\nChinstrap\nDream\n51.3\n18.2\n197\n3750\nmale\n2007\n\n\nChinstrap\nDream\n46.0\n18.9\n195\n4150\nfemale\n2007\n\n\nChinstrap\nDream\n51.3\n19.9\n198\n3700\nmale\n2007\n\n\nChinstrap\nDream\n46.6\n17.8\n193\n3800\nfemale\n2007\n\n\nChinstrap\nDream\n51.7\n20.3\n194\n3775\nmale\n2007\n\n\nChinstrap\nDream\n47.0\n17.3\n185\n3700\nfemale\n2007\n\n\nChinstrap\nDream\n52.0\n18.1\n201\n4050\nmale\n2007\n\n\nChinstrap\nDream\n45.9\n17.1\n190\n3575\nfemale\n2007\n\n\nChinstrap\nDream\n50.5\n19.6\n201\n4050\nmale\n2007\n\n\nChinstrap\nDream\n50.3\n20.0\n197\n3300\nmale\n2007\n\n\nChinstrap\nDream\n58.0\n17.8\n181\n3700\nfemale\n2007\n\n\nChinstrap\nDream\n46.4\n18.6\n190\n3450\nfemale\n2007\n\n\nChinstrap\nDream\n49.2\n18.2\n195\n4400\nmale\n2007\n\n\nChinstrap\nDream\n42.4\n17.3\n181\n3600\nfemale\n2007\n\n\nChinstrap\nDream\n48.5\n17.5\n191\n3400\nmale\n2007\n\n\nChinstrap\nDream\n43.2\n16.6\n187\n2900\nfemale\n2007\n\n\nChinstrap\nDream\n50.6\n19.4\n193\n3800\nmale\n2007\n\n\nChinstrap\nDream\n46.7\n17.9\n195\n3300\nfemale\n2007\n\n\nChinstrap\nDream\n52.0\n19.0\n197\n4150\nmale\n2007\n\n\nChinstrap\nDream\n50.5\n18.4\n200\n3400\nfemale\n2008\n\n\nChinstrap\nDream\n49.5\n19.0\n200\n3800\nmale\n2008\n\n\nChinstrap\nDream\n46.4\n17.8\n191\n3700\nfemale\n2008\n\n\nChinstrap\nDream\n52.8\n20.0\n205\n4550\nmale\n2008\n\n\nChinstrap\nDream\n40.9\n16.6\n187\n3200\nfemale\n2008\n\n\nChinstrap\nDream\n54.2\n20.8\n201\n4300\nmale\n2008\n\n\nChinstrap\nDream\n42.5\n16.7\n187\n3350\nfemale\n2008\n\n\nChinstrap\nDream\n51.0\n18.8\n203\n4100\nmale\n2008\n\n\nChinstrap\nDream\n49.7\n18.6\n195\n3600\nmale\n2008\n\n\nChinstrap\nDream\n47.5\n16.8\n199\n3900\nfemale\n2008\n\n\nChinstrap\nDream\n47.6\n18.3\n195\n3850\nfemale\n2008\n\n\nChinstrap\nDream\n52.0\n20.7\n210\n4800\nmale\n2008\n\n\nChinstrap\nDream\n46.9\n16.6\n192\n2700\nfemale\n2008\n\n\nChinstrap\nDream\n53.5\n19.9\n205\n4500\nmale\n2008\n\n\nChinstrap\nDream\n49.0\n19.5\n210\n3950\nmale\n2008\n\n\nChinstrap\nDream\n46.2\n17.5\n187\n3650\nfemale\n2008\n\n\nChinstrap\nDream\n50.9\n19.1\n196\n3550\nmale\n2008\n\n\nChinstrap\nDream\n45.5\n17.0\n196\n3500\nfemale\n2008\n\n\nChinstrap\nDream\n50.9\n17.9\n196\n3675\nfemale\n2009\n\n\nChinstrap\nDream\n50.8\n18.5\n201\n4450\nmale\n2009\n\n\nChinstrap\nDream\n50.1\n17.9\n190\n3400\nfemale\n2009\n\n\nChinstrap\nDream\n49.0\n19.6\n212\n4300\nmale\n2009\n\n\nChinstrap\nDream\n51.5\n18.7\n187\n3250\nmale\n2009\n\n\nChinstrap\nDream\n49.8\n17.3\n198\n3675\nfemale\n2009\n\n\nChinstrap\nDream\n48.1\n16.4\n199\n3325\nfemale\n2009\n\n\nChinstrap\nDream\n51.4\n19.0\n201\n3950\nmale\n2009\n\n\nChinstrap\nDream\n45.7\n17.3\n193\n3600\nfemale\n2009\n\n\nChinstrap\nDream\n50.7\n19.7\n203\n4050\nmale\n2009\n\n\nChinstrap\nDream\n42.5\n17.3\n187\n3350\nfemale\n2009\n\n\nChinstrap\nDream\n52.2\n18.8\n197\n3450\nmale\n2009\n\n\nChinstrap\nDream\n45.2\n16.6\n191\n3250\nfemale\n2009\n\n\nChinstrap\nDream\n49.3\n19.9\n203\n4050\nmale\n2009\n\n\nChinstrap\nDream\n50.2\n18.8\n202\n3800\nmale\n2009\n\n\nChinstrap\nDream\n45.6\n19.4\n194\n3525\nfemale\n2009\n\n\nChinstrap\nDream\n51.9\n19.5\n206\n3950\nmale\n2009\n\n\nChinstrap\nDream\n46.8\n16.5\n189\n3650\nfemale\n2009\n\n\nChinstrap\nDream\n45.7\n17.0\n195\n3650\nfemale\n2009\n\n\nChinstrap\nDream\n55.8\n19.8\n207\n4000\nmale\n2009\n\n\nChinstrap\nDream\n43.5\n18.1\n202\n3400\nfemale\n2009\n\n\nChinstrap\nDream\n49.6\n18.2\n193\n3775\nmale\n2009\n\n\nChinstrap\nDream\n50.8\n19.0\n210\n4100\nmale\n2009\n\n\nChinstrap\nDream\n50.2\n18.7\n198\n3775\nfemale\n2009"
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#basic-plot",
    "href": "content/labs/r-labs/graphics/wizardy.html#basic-plot",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "Basic Plot",
    "text": "Basic Plot\nA basic scatter plot, which we will progressively dress up:\n\n## simple plot: data + mappings + geometry\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(alpha = .6, size = 3.5)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#customized-plot",
    "href": "content/labs/r-labs/graphics/wizardy.html#customized-plot",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "Customized Plot",
    "text": "Customized Plot\nLet us set some ggplot theme aspects now!! Here is a handy picture showing ( most of ) the theme-able aspects of a ggplot plot.\nFor more info, type ?theme in your console.\n\n\nggplot Theme Elements\n\n\n## change global theme settings (for all following plots)\ntheme_set(theme_minimal(base_size = 12, base_family = \"open\"))\n\n## modify plot elements globally (for all following plots)\ntheme_update(\n  axis.ticks = element_line(color = \"grey92\"),\n  axis.ticks.length = unit(.5, \"lines\"),\n  panel.grid.minor = element_blank(),\n  legend.title = element_text(size = 12),\n  legend.text = element_text(color = \"grey30\"),\n  plot.title = element_text(size = 18, face = \"bold\"),\n  plot.subtitle = element_text(size = 12, color = \"grey30\"),\n  plot.caption = element_text(size = 9, margin = margin(t = 15))\n)\n\nSince we know what the basic plot looks like, let’s add titles, labels and colours. We will also set limits and scales:\n\nggplot(penguins, aes(x = bill_length_mm, \n                     y = bill_depth_mm)) +\n  \n  geom_point(aes(color = body_mass_g), alpha = .6, \n             size = 3.5) + \n  \n  ## custom axes scaling\n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  \n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5), \n                     limits = c(12.5, 22.5)) +\n  \n  ## custom colors from the scico package\n  scico::scale_color_scico(palette = \"bamako\", direction = -1) +\n  \n  ## custom labels\n  labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (Pygoscelis)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) PLoS ONE',\n    x = 'Bill Length (mm)', \n    y = 'Bill Depth (mm)',\n    color = 'Body mass (g)'\n  )"
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#using-ggtext",
    "href": "content/labs/r-labs/graphics/wizardy.html#using-ggtext",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "Using {ggtext}\n",
    "text": "Using {ggtext}\n\nFrom Claus Wilke’s website (www.wilkelab.org/ggtext)\n\nThe ggtext package provides simple Markdown and HTML rendering for ggplot2. Under the hood, the package uses the gridtext package for the actual rendering, and consequently it is limited to the feature set provided by gridtext.\nSupport is provided for Markdown both in theme elements (plot titles, subtitles, captions, axis labels, legends, etc.) and in geoms (similar to geom_text()). In both cases, there are two alternatives, one for creating simple text labels and one for creating text boxes with word wrapping.\n\n\n\n\n\n\n\nImportant\n\n\n\nNOTE: on some machines, the ggtext package may not work as expected. In this case, please do as follows, using your Console:\n\nremove gridtext: remove.packages(gridtext).\nInstall development version of gridtext: remotes::install_github(\"wilkelab/gridtext\")\n\n\n\n\nelement_markdown()\nWe can use our familiar markdown syntax right inside the titles and captions of the plot. element_markdown() is a theming command made available by the ggtext package.\nelement_markdown() → formatted text elements, e.g. titles, caption, axis text, strip text\n\n## assign plot to `gt` - we can add new things to this plot later\n## (wrapped in parenthesis so it is assigned and plotted in one step)\n\n(gt &lt;- ggplot(penguins, aes(x = bill_length_mm, \n                            y = bill_depth_mm)) +\n    \n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5) +\n    \n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n    \n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5), \n                     limits = c(12.5, 22.5)) +\n    \n  scico::scale_color_scico(palette = \"bamako\", direction = -1) +\n   \n# New code starts here: Two Step Procedure with ggtext\n# 1. Markdown formatting of labels and title, using asterisks\n# To create italics and bold text in titles\n    \n    \n  labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) *PLoS ONE*',\n    \n    x = '**Bill Length** (mm)', \n    y = '**Bill Depth** (mm)',\n    \n    color = 'Body mass (g)'\n  ) +\n   \n  ## 2. Add theme related commands from ggtext\n  ## render respective text elements\n    \n  theme(\n    plot.title = ggtext::element_markdown(),\n    plot.caption = ggtext::element_markdown(),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown()\n  )\n)\n\n\n\n\n\n\n\n\nelement_markdown() in combination with HTML\nThis allows us to change fonts in titles, labels, and captions:\n\n## use HTML syntax to change text color\ngt_mar &lt;- gt +\n  labs(title = 'Bill Dimensions of Brush-Tailed Penguins &lt;i style=\"color:#28A87D;\"&gt;Pygoscelis&lt;/i&gt;') +\n  theme(plot.margin = margin(t = 25))\n\n\n## use HTML syntax to change font and text size\ngt_mar +\n  labs(title = 'Bill Dimensions of Brush-Tailed Penguins &lt;b style=\"font-size:32pt;font-family:tangerine;\"&gt;Pygoscelis&lt;/b&gt;')\n\n\n\n\n\n\n\nAdding images to ggplot\nSave an image from the web in the same folder as your RMarkdown. Use html tags to include it, say as part of your plot title, as shown below.\n\n## use HTML syntax to add images to text elements\ngt_mar + \n  labs(title = 'Bill Dimensions of Brush-Tailed Penguins &nbsp;&nbsp;&nbsp; &lt;img src=\"images/culmen_depth.png\"‚ width=\"480\"/&gt;')\n\n\n\n\n\n\n\nAnnotations with geom_richtext() and geom_textbox()\n\nFurther ggplot annotations can be achieved using geom_richtext() and geom_textbox().\ngeom_richtext() also allows formatted text labels with 360° rotation. One needs to pass a dataframe to geom_richtext() giving the location, colour, rotation etc of the label annotation.\n\ngt_rich &lt;- ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(aes(color = species), alpha = .6, size = 3.5) + \n  \n  ## add text annotations for each species\n  ggtext::geom_richtext(\n    data = tibble(\n      \n      # Three rich text labels, so three sets of locations x and y, and angle of rotation\n      x = c(34, 56, 54), y = c(20, 18.5, 14.5),\n            angle = c(12, 20, 335),\n      species = c(\"Adélie\", \"Chinstrap\", \"Gentoo\"),\n      lab = c(\"&lt;b style='font-family:anton;font-size:24pt;'&gt;Adélie&lt;/b&gt;&lt;br&gt;&lt;i style='color:darkgrey;'&gt;P. adéliae&lt;/i&gt;\", \n              \"&lt;b style='font-family:anton;font-size:24pt;'&gt;Chinstrap&lt;/b&gt;&lt;br&gt;&lt;i style='color:darkgrey;'&gt;P. antarctica&lt;/i&gt;\", \n              \"&lt;b style='font-family:anton;font-size:24pt;'&gt;Gentoo&lt;/b&gt;&lt;br&gt;&lt;i style='color:darkgrey;'&gt;P. papua&lt;/i&gt;\"),\n\n    ),\n    \n    \n    # Now pass these data variables as aesthetics\n    aes(x, y, label = lab, color = species, angle = angle), \n    \n    size = 4, fill = NA, label.color = NA,\n    lineheight = .3\n  ) +\n  \n  \n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5), \n                     limits = c(12.5, 22.5)) +\n  \n  rcartocolor::scale_color_carto_d(palette = \"Bold\", \n                                   guide = \"none\") +\n  \n  labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) *PLoS ONE*',\n    x = '**Bill Length** (mm)', \n    y = '**Bill Depth** (mm)',\n    color = 'Body mass (g)'\n  ) +\n  \n  # Use theme and element_markdown() to format axes and titles as usual\n  theme(\n    plot.title = ggtext::element_markdown(),\n    plot.caption = ggtext::element_markdown(),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown(),\n    plot.margin = margin(25, 6, 15, 6)\n  )\n  \ngt_rich\n\n\n\n\n\n\n\nFormatted Text boxes on ggplots\nelement_textbox() and element_textbox_simple() → formatted text boxes with word wrapping\n\ngt_box &lt;- ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(aes(color = species), alpha = .6, size = 3.5) +\n  \n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  \n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5),\n                     limits = c(12.5, 22.5)) +\n  \n  rcartocolor::scale_color_carto_d(palette = \"Bold\", guide = \"none\") +\n  \n  ## add text annotations for each species\n  ## Creating a tibble for the labels!\n  ggtext::geom_richtext(\n    data = tibble(\n      # Three rich text labels\n      # So three sets of locations x and y, and angle of rotation\n      x = c(34, 56, 54),\n      y = c(20, 18.5, 14.5),\n      angle = c(12, 20, 335),\n      species = c(\"Adélie\", \"Chinstrap\", \"Gentoo\"),\n      notes = c(\n        \"&lt;b style='font-family:anton;font-size:24pt;'&gt;Adélie&lt;/b&gt;&lt;br&gt;&lt;i style='color:darkgrey;'&gt;P. adéliae&lt;/i&gt;\",\n        \"&lt;b style='font-family:anton;font-size:24pt;'&gt;Chinstrap&lt;/b&gt;&lt;br&gt;&lt;i style='color:darkgrey;'&gt;P. antarctica&lt;/i&gt;\",\n        \"&lt;b style='font-family:anton;font-size:24pt;'&gt;Gentoo&lt;/b&gt;&lt;br&gt;&lt;i style='color:darkgrey;'&gt;P. papua&lt;/i&gt;\"\n      )\n      ),\n    \n    \n    # Now pass these data variables as aesthetics\n    aes(\n      x,\n      y,\n      label = notes,\n      color = species,\n      angle = angle\n    ),\n    \n    size = 4,\n    fill = NA,\n    label.color = NA,\n    lineheight = .3\n  ) +\n  \n  \n# Now for the Plot Titles and Labels, as before  \n  labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) *PLoS ONE*',\n    x = '**Bill Length** (mm)',\n    y = '**Bill Depth** (mm)',\n    color = 'Body mass (g)'\n  ) +\n  \n# Add the ggtext theme related commands\n  theme(\n    ## turn title into filled textbox\n    plot.title = ggtext::element_textbox_simple(\n      color = \"white\",\n      fill = \"#28A78D\",\n      size = 32,\n      padding = margin(8, 4, 8, 4),\n      margin = margin(b = 5),\n      lineheight = .9\n    ),\n    ## add round outline to caption\n    plot.caption = ggtext::element_textbox_simple(\n      width = NULL,\n      linetype = 1,\n      padding = margin(4, 8, 4, 8),\n      margin = margin(t = 15),\n      r = grid::unit(8, \"pt\")\n    ),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown(),\n    plot.margin = margin(25, 6, 15, 6)\n  )\n\ngt_box\n\n\n\n\n\n\n\ngeom_textbox() → formatted text boxes with word wrapping\n\ngt_box +\n  ## add textbox with long paragraphs\n  ggtext::geom_textbox(\n    data = tibble(x = 34, y = 13.7, label = \"&lt;span style='font-size:12pt;font-family:anton;'&gt;Lorem Ipsum Dolor Sit Amet&lt;/span&gt;&lt;br&gt;&lt;br&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\"),\n    aes(x, y, label = label),\n    size = 2.2, family = \"sans\",\n    fill = \"cornsilk\", box.color = \"cornsilk3\",\n    width = unit(11, \"lines\")\n  ) +\n  coord_cartesian(clip = \"off\") # ensure no clipping of labels near the edge"
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#using-ggforce",
    "href": "content/labs/r-labs/graphics/wizardy.html#using-ggforce",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "Using {ggforce}\n",
    "text": "Using {ggforce}\n\nFrom Thomas Lin Pedersen’s website → www.ggforce.data-imaginist.com\n\nggforce is a package aimed at providing missing functionality to ggplot2 through the extension system introduced with ggplot2 v2.0.0. Broadly speaking ggplot2 has been aimed primarily at explorative data visualization in order to investigate the data at hand, and less at providing utilities for composing custom plots a la D3.js. ggforce is mainly an attempt to address these “shortcoming” (design choices might be a better description). The goal is to provide a repository of geoms, stats, etc. that are as well documented and implemented as the official ones found in ggplot2.\n\nWe will start with the basic plot, with the ggtext related work done up to now:\n\n## use ggtext rendering for the following plots\ntheme_update(\n  plot.title = ggtext::element_markdown(),\n  plot.caption = ggtext::element_markdown(),\n  axis.title.x = ggtext::element_markdown(),\n  axis.title.y = ggtext::element_markdown()\n)\n\n\n## plot that we will annotate with ggforce afterwards\n(gf &lt;- ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) + \n  scico::scale_color_scico(palette = \"bamako\", direction = -1) +\n  coord_cartesian(xlim = c(25, 65), ylim = c(10, 25)) +\n  rcartocolor::scale_fill_carto_d(palette = \"Bold\") +\n  labs(\n    title = \"Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)\",\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE*\",\n    x = \"**Bill Length** (mm)\", \n    y = \"**Bill Depth** (mm)\",\n    color = \"Body mass (g)\",\n    fill = \"Species\"\n  ))\n\n\n\n\n\n\n\n\n## ellipsoids for all groups\n(gf +\n  ggforce::geom_mark_ellipse(\n    aes(fill = species, label = species), \n    alpha = .15, show.legend = FALSE\n  ) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5)\n)\n\n\n\n\n\n\n\n\n## ellipsoids for specific subset\n(gf +\n  ggforce::geom_mark_ellipse(\n    aes(fill = species, label = species, filter = species == 'Gentoo'), \n    alpha = 0, show.legend = FALSE\n  ) +\n geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5)\n#   coord_cartesian(xlim = c(25, 65), ylim = c(10, 25))\n)\n\n\n\n\n\n\n\n\n## circles\n(gf +\n  ggforce::geom_mark_circle(\n    aes(fill = species, label = species, filter = species == 'Gentoo'), \n    alpha = 0, show.legend = FALSE\n  ) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5)\n)\n\n\n\n\n\n\n\n\n## rectangles\n(gf +\n  ggforce::geom_mark_rect(\n    aes(fill = species, label = species, filter = species == 'Gentoo'), \n    alpha = 0, show.legend = FALSE\n  ) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5) \n)\n\n\n\n\n\n\n\n\nlibrary(concaveman)\n## hull\n(gf +\n  ggforce::geom_mark_hull(\n    aes(fill = species, label = species, filter = species == 'Gentoo'), \n    alpha = 0, show.legend = FALSE\n  ) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5)\n)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#ggplot-tricks",
    "href": "content/labs/r-labs/graphics/wizardy.html#ggplot-tricks",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "ggplot tricks",
    "text": "ggplot tricks\n\n(gg0 &lt;- \n  ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n    ggforce::geom_mark_ellipse(\n      aes(fill = species, label = species), \n      alpha = 0, show.legend = FALSE\n    ) +\n    geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5) + \n    scale_x_continuous(breaks = seq(25, 65, by = 5), limits = c(25, 65)) +\n    scale_y_continuous(breaks = seq(12, 24, by = 2), limits = c(12, 24)) +\n    scico::scale_color_scico(palette = \"bamako\", direction = -1) +\n    labs(\n      title = \"Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)\",\n      subtitle = 'A scatter plot of bill depth versus bill length.',\n      caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE*\",\n      x = \"Bill Length (mm)\", \n      y = \"Bill Depth (mm)\",\n      color = \"Body mass (g)\"\n    )\n)\n\n\n\n\n\n\n\nLeft-Aligned Title\n\n(gg1 &lt;- gg0 + theme(plot.title.position = \"plot\"))\n\n\n\n\n\n\n\nRight-Aligned Caption\n\n(gg1b &lt;- gg1 +  theme(plot.caption.position = \"panel\"))\n\n\n\n\n\n\n\nLegend Design\n\n(gg2 &lt;- gg1b + theme(legend.position = \"top\"))\n\n\n\n\n\n\nggsave(\"06a_legend_position.pdf\", width = 9, height = 8, device = cairo_pdf)\n\n(gg2b &lt;- gg2 + \n  guides(color = guide_colorbar(title.position = \"top\", \n                                title.hjust = .5, \n                                barwidth = unit(20, \"lines\"), \n                                barheight = unit(.5, \"lines\"))))\n\n\n\n\n\n\n\nAdd Images\n\n## read PNG file from web\npng &lt;- magick::image_read(\"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/man/figures/culmen_depth.png\")\n## turn image into `rasterGrob`\nimg &lt;- grid::rasterGrob(png, interpolate = TRUE)\n\ngg3 &lt;- gg2b +\n  annotation_custom(img, ymin = 18.5, ymax = 30.5, xmin = 55, xmax = 65.5) +\n    labs(caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE* &bull; Illustration: Allison Horst\")\ngg3"
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#using-patchwork",
    "href": "content/labs/r-labs/graphics/wizardy.html#using-patchwork",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "Using {patchwork}\n",
    "text": "Using {patchwork}\n\n\nThe goal of patchwork is to make it ridiculously simple to combine separate ggplots into the same graphic. As such it tries to solve the same problem as gridExtra::grid.arrange() and cowplot::plot_grid but using an API that incites exploration and iteration, and scales to arbitrily complex layouts.\n\n→ https://patchwork.data-imaginist.com/\nLet us make two plots and combine them into a single patchwork plot.\n\n## calculate bill ratio\npenguins_stats &lt;- penguins %&gt;% \n  mutate(bill_ratio = bill_length_mm / bill_depth_mm) %&gt;% \n  filter(!is.na(bill_ratio))\n\n## create a second chart\ngg4 &lt;- ggplot(penguins_stats, aes(y = bill_ratio, x = species, fill = species, color = species)) + geom_violin() + \n  labs(\n    y = \"Bill ratio\",\n    x = \"Species\",\n    subtitle = \"\",\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE* &bull; Illustration: Allison Horst\"\n  ) +\n  theme(\n    panel.grid.major.x = element_line(size = .35),\n    panel.grid.major.y = element_blank(),\n    axis.text.y = element_text(size = 13),\n    axis.ticks.length = unit(0, \"lines\"),\n    plot.title.position = 'plot',\n    plot.subtitle = element_text(margin = margin(t = 5, b = 10)),\n    plot.margin = margin(10, 25, 10, 25)\n  )\n\nNow to combine both plots into one using simple operators:\n\nFor the special case of putting plots besides each other or on top of each other patchwork provides 2 shortcut operators. | will place plots next to each other while / will place them on top of each other.\n\nFirst we stack up the graphs side by side:\n\n## combine both plots\ngg3 | (gg4 + labs(title = \"Bill Ratios of Brush-Tailed Penguins\",\n                  subtitle = \"Violin Plots of Bill Ration versus species\"))\n\n\n\n\n\n\n\nWe can place them in one column:\n\ngg3 / (gg3 + labs(title = \"Bill Ratios of Brush-Tailed Penguins\",\n                  subtitle = \"Violin Plots of Bill Ration versus species\")) +\n  plot_layout(heights = c(0.4, 0.4))"
  },
  {
    "objectID": "content/labs/r-labs/graphics/wizardy.html#resources",
    "href": "content/labs/r-labs/graphics/wizardy.html#resources",
    "title": "Lab 06a: Fonts, Themes, and other Wizardy in ggplot",
    "section": "Resources",
    "text": "Resources\n\n\nIntro to R (one of many good online tutorials)\n“R for Data Science” book (open-access)\nggplot2 Book (open-access)\nR Graph Gallery\nSlides of Cedric Scherer’s talk\nExtensive ggplot2 tutorial\n“Evolution of a ggplot” blog post by Cedric Scherer\n\n#TidyTuesday project (#TidyTuesday on Twitter)\n\n#TidyTuesday Contributions by Cedric Scherer incl. all codes\n\nR4DS learning community (huge Slack community for people learning R incl. a mentoring program)\n\nIllustrations by Allison Horst (more general about data and stats + R-related)\nR Packages:\n\nggplot2\nggtext\nggforce\nggdist\nggraph\nggstream\nggbump\ngggibous\nwaffle\ngeofacet\ncartogram\npatchwork\nsf"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html",
    "href": "content/labs/r-labs/graphics/colors.html",
    "title": "Lab 05: Colors with Penguins",
    "section": "",
    "text": "knitr::opts_chunk$set(error = TRUE, comment = NA, warning = FALSE, errors = FALSE, message = FALSE, tidy = FALSE, cache = FALSE, fig.path= \"03-figs/\")\n\nlibrary(tidyverse) # Manage data\nlibrary(scales) # Create special ( % or $ ) scales\n#\nlibrary(palmerpenguins) # source of our data\n#\nlibrary(RColorBrewer) # Colour Palettes\nlibrary(wesanderson) # Colour Palettes\n#library(gameofthrones) # You all know this!\n#\nlibrary(paletteer) # Colour Palettes\nlibrary(colorspace) # Colour Palettes\n#\nlibrary(patchwork) # arranges plots on Row-Col \nlibrary(ggthemes)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#setting-up-r-packages",
    "href": "content/labs/r-labs/graphics/colors.html#setting-up-r-packages",
    "title": "Lab 05: Colors with Penguins",
    "section": "",
    "text": "knitr::opts_chunk$set(error = TRUE, comment = NA, warning = FALSE, errors = FALSE, message = FALSE, tidy = FALSE, cache = FALSE, fig.path= \"03-figs/\")\n\nlibrary(tidyverse) # Manage data\nlibrary(scales) # Create special ( % or $ ) scales\n#\nlibrary(palmerpenguins) # source of our data\n#\nlibrary(RColorBrewer) # Colour Palettes\nlibrary(wesanderson) # Colour Palettes\n#library(gameofthrones) # You all know this!\n#\nlibrary(paletteer) # Colour Palettes\nlibrary(colorspace) # Colour Palettes\n#\nlibrary(patchwork) # arranges plots on Row-Col \nlibrary(ggthemes)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#introduction",
    "href": "content/labs/r-labs/graphics/colors.html#introduction",
    "title": "Lab 05: Colors with Penguins",
    "section": "\n Introduction",
    "text": "Introduction\nThis Quarto document is part of my Workshop in R. The material is based on A Layered Grammar of Graphics by Hadley Wickham. The course is meant for First Year students pursuing a Degree in Art and Design. The intent is to build Skill in coding in R, and also appreciate R as a way to metaphorically visualize information of various kinds, using predominantly geometric figures and structures.\nAll Quarto files combine code, text, web-images, and figures developed using code. Everything is text; code chunks are enclosed in fences (```)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#goals",
    "href": "content/labs/r-labs/graphics/colors.html#goals",
    "title": "Lab 05: Colors with Penguins",
    "section": "Goals",
    "text": "Goals\n\n(Re)Understand different kinds of data variables\nAppreciate how they can be identified based on the Interrogative Pronouns they answer to\nUnderstand how each kind of variable lends itself to a specific choice of colour scale in the data visualization."
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#pedagogical-note",
    "href": "content/labs/r-labs/graphics/colors.html#pedagogical-note",
    "title": "Lab 05: Colors with Penguins",
    "section": "Pedagogical Note",
    "text": "Pedagogical Note\nThe method followed will be based on PRIMM:\n\n\nPREDICT Inspect the code and guess at what the code might do, write predictions\n\n\nRUN the code provided and check what happens\n\nINFER what the parameters of the code do and write comments to explain. What bells and whistles can you see?\n\nMODIFY the parameters code provided to understand the options available. Write comments to show what you have aimed for and achieved.\n\nMAKE : take an idea/concept of your own, and graph it.\n\nIn the following, there is some boiler plate code demonstrating the use of colour palettes in R. There are places where YOUR TURN is mention; copy and play with the boiler plate code to see what happens !"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#data",
    "href": "content/labs/r-labs/graphics/colors.html#data",
    "title": "Lab 05: Colors with Penguins",
    "section": "Data",
    "text": "Data\nWe will use the penguins dataset built into the palmerpenguins package. Your should try other datasets too!\nHere is a glimpse of the data:\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nNote that the unit of observation here is one-row-per-penguin.\nVariables you need for this lab:"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#colour-vs-fill-aesthetic",
    "href": "content/labs/r-labs/graphics/colors.html#colour-vs-fill-aesthetic",
    "title": "Lab 05: Colors with Penguins",
    "section": "Colour vs fill aesthetic",
    "text": "Colour vs fill aesthetic\nFill and colour scales in ggplot2 can use the same palettes. Some shapes such as lines only accept the colour aesthetic, while others, such as polygons, accept both colour and fill aesthetics. In the latter case, the colour refers to the border of the shape, and the fill to the interior.\n\n## A look at all 25 symbols\ndf &lt;- data.frame(x = 1:5,\n                 y = rep(rev(seq(0, 24, by = 5)), each = 5),\n                 z = 1:25)\ns &lt;- ggplot(df, aes(x = x, y = y)) +\n  geom_text(aes(label = z, y = y - 1)) +\n  theme_void()\ns + geom_point(aes(shape = z), size = 4) + scale_shape_identity()\n\n\n\n\n\n\n\nAll symbols have a foreground colour, so if we add color = \"navy\", they all are affected.\n\ns + geom_point(aes(shape = z), size = 4, colour = \"blue\")  + scale_shape_identity()\n\n\n\n\n\n\n\nWhile all symbols have a foreground colour, symbols 21-25 also take a background colour (fill). So if we add fill = \"orchid\", only the last row of symbols are affected.\n\ns + geom_point(aes(shape = z), size = 4, colour = \"blue\", fill = \"orchid\")  + scale_shape_identity()"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#discrete-vs-continuous-variables",
    "href": "content/labs/r-labs/graphics/colors.html#discrete-vs-continuous-variables",
    "title": "Lab 05: Colors with Penguins",
    "section": "Discrete vs continuous variables",
    "text": "Discrete vs continuous variables\nWHAT IS THE DIFFERENCE BETWEEN CATEGORICAL, ORDINAL AND INTERVAL VARIABLES?\nIn order to use color with your data, most importantly, you need to know if you’re dealing with discrete or continuous variables."
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#some-colour-palette-packages-in-r",
    "href": "content/labs/r-labs/graphics/colors.html#some-colour-palette-packages-in-r",
    "title": "Lab 05: Colors with Penguins",
    "section": "Some Colour Palette Packages in R",
    "text": "Some Colour Palette Packages in R\nWe have the following example packages that offer palettes in R:\n\nRColorBrewer\nwesanderson\npaletteer\ncolorspace\n\nSee Appendix for a detailed graphical analysis of these palette packages."
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#colour-palette-types",
    "href": "content/labs/r-labs/graphics/colors.html#colour-palette-types",
    "title": "Lab 05: Colors with Penguins",
    "section": "Colour Palette Types",
    "text": "Colour Palette Types\nThese palettes can be:\n\nSequential (type = “seq”) palettes are suited to ordered data that progress from low to high. Lightness steps dominate the look of these schemes, with light colors for low data values to dark colors for high data values. (for numerical data, that are ordered)\n\n\nDiverging (type = “div”) palettes put equal emphasis on mid-range critical values and extremes at both ends of the data range. The critical class or break in the middle of the legend is emphasized with light colors and low and high extremes are emphasized with dark colors that have contrasting hues.(for numerical data that can be positive or negative, often representing deviations from some norm or baseline)\n\n\nQualitative (type = “qual”) palettes do not imply magnitude differences between legend classes, and hues are used to create the primary visual differences between classes. Qualitative schemes are best suited to representing nominal or categorical data. (for qualitative unordered data)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#create-a-simple-set-of-scatter-plots",
    "href": "content/labs/r-labs/graphics/colors.html#create-a-simple-set-of-scatter-plots",
    "title": "Lab 05: Colors with Penguins",
    "section": "Create a simple set of scatter plots",
    "text": "Create a simple set of scatter plots\nWe will create simple base plots in ggplot and see how we may alter the colour scales using palettes.\n\nnames(penguins)\n\n[1] \"species\"           \"island\"            \"bill_length_mm\"   \n[4] \"bill_depth_mm\"     \"flipper_length_mm\" \"body_mass_g\"      \n[7] \"sex\"               \"year\"             \n\n\n\np1 &lt;- penguins %&gt;% \n  drop_na() %&gt;% \n  # pipe data into ggplot\n  # after removing data rows that have missing ( NA ) values\n  ggplot(aes(y = body_mass_g, x = flipper_length_mm, \n           color = species # COLOUR = DISCRETE/QUAL VARIABLE\n           )) +\n           geom_point() + \n           labs(title = \"Default Colours in ggplot\", \n                subtitle = \"P1: DISCRETE/QUAL Colour Palette\")\n\n\np2 &lt;- \npenguins %&gt;% \n  drop_na() %&gt;% \n  # pipe the data into ggplot, \n  # after removing data rows that have missing ( NA ) values\n  ggplot(aes(y = body_mass_g, x = flipper_length_mm, \n           color = bill_length_mm # COLOUR = CONT/QUANT VARIABLE\n           )) +\n           geom_point() + \n           labs(title = \"Default Colours in ggplot\", \n                subtitle = \"P2: CONTINUOUS/QUANT Colour Palette\")\n\np1\n\n\n\n\n\n\np2\n\n\n\n\n\n\n\nNote that these use the default colours in R."
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#colours-for-discrete-qual-variables",
    "href": "content/labs/r-labs/graphics/colors.html#colours-for-discrete-qual-variables",
    "title": "Lab 05: Colors with Penguins",
    "section": "Colours for Discrete (QUAL) Variables",
    "text": "Colours for Discrete (QUAL) Variables\nThe commands below are used to fill colours based on Qualitative Variables:\n\nscale_colour/fill_discrete\n\nscale_colour/fill_brewer # RColorBrewer\n….\n\nNow to use these!"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#plotting-colours-based-on-discrete-variables",
    "href": "content/labs/r-labs/graphics/colors.html#plotting-colours-based-on-discrete-variables",
    "title": "Lab 05: Colors with Penguins",
    "section": "Plotting Colours based on Discrete Variables",
    "text": "Plotting Colours based on Discrete Variables"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#discrete-n-colour-palettes-from-rcolorbrewer",
    "href": "content/labs/r-labs/graphics/colors.html#discrete-n-colour-palettes-from-rcolorbrewer",
    "title": "Lab 05: Colors with Penguins",
    "section": "Discrete n-Colour palettes from RColorBrewer\n",
    "text": "Discrete n-Colour palettes from RColorBrewer\n\n\nRColorBrewer::brewer.pal.info\n\n\n  \n\n\nRColorBrewer::display.brewer.all()\n\n\n\n\n\n\n\n\np1 +\n  # default palette = \"Blues\"\n  scale_colour_brewer() +\n  labs(title = \"Brewer Palette = Blues\")\n\n\n\n\n\n\np1 +\n  scale_color_brewer(palette = \"Spectral\") +\n  labs(title = \"Brewer Palette = Spectral\")"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#discrete-colour-scales-using-wesanderson-palettes",
    "href": "content/labs/r-labs/graphics/colors.html#discrete-colour-scales-using-wesanderson-palettes",
    "title": "Lab 05: Colors with Penguins",
    "section": "Discrete Colour scales using wesanderson palettes",
    "text": "Discrete Colour scales using wesanderson palettes\n\nwesanderson::wes_palettes %&gt;% names()\n\n [1] \"BottleRocket1\"     \"BottleRocket2\"     \"Rushmore1\"        \n [4] \"Rushmore\"          \"Royal1\"            \"Royal2\"           \n [7] \"Zissou1\"           \"Zissou1Continuous\" \"Darjeeling1\"      \n[10] \"Darjeeling2\"       \"Chevalier1\"        \"FantasticFox1\"    \n[13] \"Moonrise1\"         \"Moonrise2\"         \"Moonrise3\"        \n[16] \"Cavalcanti1\"       \"GrandBudapest1\"    \"GrandBudapest2\"   \n[19] \"IsleofDogs1\"       \"IsleofDogs2\"       \"FrenchDispatch\"   \n[22] \"AsteroidCity1\"     \"AsteroidCity2\"     \"AsteroidCity3\"    \n\n\n\np1 +\n  scale_colour_discrete(type = wes_palette(name = \"GrandBudapest1\",\n                                           n = 3)) +\n  labs(title = \"Wes Anderson Palette: GrandBudapest\")\n\n\n\n\n\n\n# We can also specify colour codes ourselves with scale_x_discrete.\n# Use argument \"values\" instead of \"type\"\nmanual_colours &lt;- c(\"#afc4b8\", \"#f1a4b2\", \"#ffb1e1\") \nmanual_colours\n\n[1] \"#afc4b8\" \"#f1a4b2\" \"#ffb1e1\"\n\np1 +\n  scale_colour_manual(values =  manual_colours) +\n  labs(title = \"Manual Colours\")"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#discrete-n-colour-palettes-from-rcolorbrewer-1",
    "href": "content/labs/r-labs/graphics/colors.html#discrete-n-colour-palettes-from-rcolorbrewer-1",
    "title": "Lab 05: Colors with Penguins",
    "section": "Discrete n-Colour palettes from RColorBrewer\n",
    "text": "Discrete n-Colour palettes from RColorBrewer\n\n\n# scale_x_brewer() for DISCRETE data\np1 +\n  scale_colour_brewer(palette = \"Spectral\") +\n  \n  labs(title = \"RColorBrewer Palette = Spectral\")"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#discrete-colour-scales-using-paletteer-palettes",
    "href": "content/labs/r-labs/graphics/colors.html#discrete-colour-scales-using-paletteer-palettes",
    "title": "Lab 05: Colors with Penguins",
    "section": "Discrete Colour scales using paletteer palettes",
    "text": "Discrete Colour scales using paletteer palettes\n\npalettes_d_names\n\n\n  \n\n\npalettes_dynamic_names\n\n\n  \n\n\npaletteer_d(\"dutchmasters::pearl_earring\")\n\n&lt;colors&gt;\n#A65141FF #E7CDC2FF #80A0C7FF #394165FF #FCF9F0FF #B1934AFF #DCA258FF #100F14FF #8B9DAFFF #EEDA9DFF #E8DCCFFF \n\npaletteer_dynamic(\"ggthemes_ptol::qualitative\", n = 3)\n\n&lt;colors&gt;\n#4477AAFF #DDCC77FF #CC6677FF \n\np1 +\n  scale_colour_paletteer_d(\"ggthemes_ptol::qualitative\", \n                           dynamic = TRUE) +\n  \n  labs(title = \"ggThemes Palette: Qualitative\", \n          subtitle = \"\")\n\n\n\n\n\n\n# I like Vermeer's \"Girl with the Pearl Earring\"!\np1 +\n  scale_colour_paletteer_d(\"dutchmasters::pearl_earring\",\n                           dynamic = FALSE) +\n  \n  labs(title = \"Palettes from `paletteer`\", \n          subtitle = \" Palette from Vermeer: Girl with Pearl Earring\")"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#colours-for-continuous-quant-variables",
    "href": "content/labs/r-labs/graphics/colors.html#colours-for-continuous-quant-variables",
    "title": "Lab 05: Colors with Penguins",
    "section": "Colours for Continuous (QUANT) Variables",
    "text": "Colours for Continuous (QUANT) Variables\nThe commands below are used to fill colours based on Quantitative Variables:\n\n\nscale_colour/fill_gradient (Two colour gradient)\n\nscale_colour/fill_gradient2 (Three colour gradient)\n\nscale_colour/fill_gradientn (Specify Palette, from other packages also, like wesanderson )\n\nscale_colour/fill_distiller (Palettes from RColorBrewer)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#plotting-colours-based-on-continuous-variables",
    "href": "content/labs/r-labs/graphics/colors.html#plotting-colours-based-on-continuous-variables",
    "title": "Lab 05: Colors with Penguins",
    "section": "Plotting Colours based on Continuous Variables",
    "text": "Plotting Colours based on Continuous Variables"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#continuous-two-colour-gradients",
    "href": "content/labs/r-labs/graphics/colors.html#continuous-two-colour-gradients",
    "title": "Lab 05: Colors with Penguins",
    "section": "Continuous Two Colour Gradients",
    "text": "Continuous Two Colour Gradients\nCreates a pallete containing continuous shades between two colours:\n\np2 +\n    scale_color_gradient(\n      low = \"yellow\", # Play with this in the chunk below\n      high = \"purple\") + # Play with this in the chnk below\n  \n  labs(title = \"Two Colour Gradients\",\n          subtitle = \"P2: Continuous 2-Colour Pallete\")"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#continuous-three-colour-gradients",
    "href": "content/labs/r-labs/graphics/colors.html#continuous-three-colour-gradients",
    "title": "Lab 05: Colors with Penguins",
    "section": "Continuous Three Colour Gradients",
    "text": "Continuous Three Colour Gradients\nSometimes we want a palette this way: a midpoint colour, and colours for the two extremes of a continuous variable:\n\ncolour_midpoint &lt;- mean(penguins$bill_length_mm, \n                         na.rm = TRUE) # remove missing values\n# Struggled all morning on 22 Aug 2020 to get at this ;-D\n\n# Play with the function: 0/mean/median/mode/max/min\n\np2 +\n  scale_colour_gradient2(\n  low = \"brown\", # Play with this in the chunk below\n  mid = \"white\", # Play with this in the chunk below\n  high = \"purple\", # Play with this in the chunk below\n  midpoint = colour_midpoint, # see above\n  space = \"Lab\", # don't mess with this!\n  na.value = \"grey50\")  +\n  labs(title = \"Three colour continuous gradient\", \n          subtitle = \"Mid Colour mapped to midpoint of data variable\",\n          caption = \"Colours inspired by my favourite cocker spaniel, Lord Chestnut\") # Play with these"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#continuous-n-colour-gradients---grdevices-package",
    "href": "content/labs/r-labs/graphics/colors.html#continuous-n-colour-gradients---grdevices-package",
    "title": "Lab 05: Colors with Penguins",
    "section": "Continuous n-Colour Gradients - grDevices package",
    "text": "Continuous n-Colour Gradients - grDevices package\n\n# grDevices Palettes\np2 +\n  scale_colour_gradientn(\n    colours = terrain.colors(10)) +\n  # Try these:\n  # heat.colors() / topo.colors() / cm.colors() / rainbow()\n  \n  labs(title = \"N-colour continuous gradients\", \n          subtitle = \"Palettes from grDevices\",\n          caption = \"Palette: terrain.colors\")"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#continuous-n-colour-gradients---wesanderson-palettes",
    "href": "content/labs/r-labs/graphics/colors.html#continuous-n-colour-gradients---wesanderson-palettes",
    "title": "Lab 05: Colors with Penguins",
    "section": "Continuous n-Colour Gradients - wesanderson Palettes",
    "text": "Continuous n-Colour Gradients - wesanderson Palettes\n\nwes_palettes\n\n$BottleRocket1\n[1] \"#A42820\" \"#5F5647\" \"#9B110E\" \"#3F5151\" \"#4E2A1E\" \"#550307\" \"#0C1707\"\n\n$BottleRocket2\n[1] \"#FAD510\" \"#CB2314\" \"#273046\" \"#354823\" \"#1E1E1E\"\n\n$Rushmore1\n[1] \"#E1BD6D\" \"#EABE94\" \"#0B775E\" \"#35274A\" \"#F2300F\"\n\n$Rushmore\n[1] \"#E1BD6D\" \"#EABE94\" \"#0B775E\" \"#35274A\" \"#F2300F\"\n\n$Royal1\n[1] \"#899DA4\" \"#C93312\" \"#FAEFD1\" \"#DC863B\"\n\n$Royal2\n[1] \"#9A8822\" \"#F5CDB4\" \"#F8AFA8\" \"#FDDDA0\" \"#74A089\"\n\n$Zissou1\n[1] \"#3B9AB2\" \"#78B7C5\" \"#EBCC2A\" \"#E1AF00\" \"#F21A00\"\n\n$Zissou1Continuous\n [1] \"#3A9AB2\" \"#6FB2C1\" \"#91BAB6\" \"#A5C2A3\" \"#BDC881\" \"#DCCB4E\" \"#E3B710\"\n [8] \"#E79805\" \"#EC7A05\" \"#EF5703\" \"#F11B00\"\n\n$Darjeeling1\n[1] \"#FF0000\" \"#00A08A\" \"#F2AD00\" \"#F98400\" \"#5BBCD6\"\n\n$Darjeeling2\n[1] \"#ECCBAE\" \"#046C9A\" \"#D69C4E\" \"#ABDDDE\" \"#000000\"\n\n$Chevalier1\n[1] \"#446455\" \"#FDD262\" \"#D3DDDC\" \"#C7B19C\"\n\n$FantasticFox1\n[1] \"#DD8D29\" \"#E2D200\" \"#46ACC8\" \"#E58601\" \"#B40F20\"\n\n$Moonrise1\n[1] \"#F3DF6C\" \"#CEAB07\" \"#D5D5D3\" \"#24281A\"\n\n$Moonrise2\n[1] \"#798E87\" \"#C27D38\" \"#CCC591\" \"#29211F\"\n\n$Moonrise3\n[1] \"#85D4E3\" \"#F4B5BD\" \"#9C964A\" \"#CDC08C\" \"#FAD77B\"\n\n$Cavalcanti1\n[1] \"#D8B70A\" \"#02401B\" \"#A2A475\" \"#81A88D\" \"#972D15\"\n\n$GrandBudapest1\n[1] \"#F1BB7B\" \"#FD6467\" \"#5B1A18\" \"#D67236\"\n\n$GrandBudapest2\n[1] \"#E6A0C4\" \"#C6CDF7\" \"#D8A499\" \"#7294D4\"\n\n$IsleofDogs1\n[1] \"#9986A5\" \"#79402E\" \"#CCBA72\" \"#0F0D0E\" \"#D9D0D3\" \"#8D8680\"\n\n$IsleofDogs2\n[1] \"#EAD3BF\" \"#AA9486\" \"#B6854D\" \"#39312F\" \"#1C1718\"\n\n$FrenchDispatch\n[1] \"#90D4CC\" \"#BD3027\" \"#B0AFA2\" \"#7FC0C6\" \"#9D9C85\"\n\n$AsteroidCity1\n[1] \"#0A9F9D\" \"#CEB175\" \"#E54E21\" \"#6C8645\" \"#C18748\"\n\n$AsteroidCity2\n[1] \"#C52E19\" \"#AC9765\" \"#54D8B1\" \"#b67c3b\" \"#175149\" \"#AF4E24\"\n\n$AsteroidCity3\n[1] \"#FBA72A\" \"#D3D4D8\" \"#CB7A5C\" \"#5785C1\"\n\nnames(wes_palettes)\n\n [1] \"BottleRocket1\"     \"BottleRocket2\"     \"Rushmore1\"        \n [4] \"Rushmore\"          \"Royal1\"            \"Royal2\"           \n [7] \"Zissou1\"           \"Zissou1Continuous\" \"Darjeeling1\"      \n[10] \"Darjeeling2\"       \"Chevalier1\"        \"FantasticFox1\"    \n[13] \"Moonrise1\"         \"Moonrise2\"         \"Moonrise3\"        \n[16] \"Cavalcanti1\"       \"GrandBudapest1\"    \"GrandBudapest2\"   \n[19] \"IsleofDogs1\"       \"IsleofDogs2\"       \"FrenchDispatch\"   \n[22] \"AsteroidCity1\"     \"AsteroidCity2\"     \"AsteroidCity3\"    \n\n\n\np2 +\n    scale_colour_gradientn(\n      colors = wes_palette(name = \"GrandBudapest1\", \n                           n = 4), # Keep an eye on \"n\".\n      na.value = \"grey\") +\n  # Try these:\n  # \"BottleRocket1\"  \"BottleRocket2\"  \"Rushmore1\"\n  # \"Rushmore\"       \"Royal1\"         \"Royal2\"\n  # \"Zissou1\"        \"Darjeeling1\"    \"Darjeeling2\"   \n  # \"Chevalier1\"     \"FantasticFox1\"  \"Moonrise1\"     \n  # \"Moonrise2\"      \"Moonrise3\"      \"Cavalcanti1\"   \n  # \"GrandBudapest1\" \"GrandBudapest2\" \"IsleofDogs1\"   \n  # \"IsleofDogs2\"   \n  # Keep an eye on \"n\".\n  \n  labs(title = \"N-colour continuous gradients\", \n       subtitle = \"Palettes from wesanderson\",\n       caption = \"Palette: GrandBudapest1\") # Change this caption based on palette choice"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#continuous-n-colour-palettes-from-rcolorbrewer",
    "href": "content/labs/r-labs/graphics/colors.html#continuous-n-colour-palettes-from-rcolorbrewer",
    "title": "Lab 05: Colors with Penguins",
    "section": "Continuous n-Colour palettes from RColorBrewer\n",
    "text": "Continuous n-Colour palettes from RColorBrewer\n\nRecall Palette types\n\n\nseq for continuous data mapped to colour\n\nqual for categorical data mapped to colour ( discrete)\n\ndiv continuous data mapped to colour, that has pos and neg extremes from a middle value\n\n\nbrewer.pal.info\n\n\n  \n\n\n\n\n# scale_color_distiller() and scale_fill_distiller() \n# are used to apply the ColorBrewer colour scales \n# to continuous data.\n\np2 +\n  scale_colour_distiller(\n    palette = \"YlGnBu\") + # Play with this palette\n  \n  labs(title = \"RColorBrewer Palette\")"
  },
  {
    "objectID": "content/labs/r-labs/graphics/colors.html#continuous-colour-scales-using-paletteer-palettes",
    "href": "content/labs/r-labs/graphics/colors.html#continuous-colour-scales-using-paletteer-palettes",
    "title": "Lab 05: Colors with Penguins",
    "section": "Continuous Colour scales using paletteer palettes",
    "text": "Continuous Colour scales using paletteer palettes\nThis palette seems to have everything accessible in a simple way! NOTE: In order to access some palettes in paletteer, you may be asked to install other packages. E.g. harrypotter or scico. These need not be brought into your session using library() but are accessed directly by paletteer which is very convenient!!\n\n# What continuous palettes are there in paletteer?\npaletteer::palettes_c_names\n\n\n  \n\n\n\nOK, one of the Games of Thrones Palettes, and Harry Potter!\n\np2 +\n  # scale_colour_paletteer_c(\"gameofthrones::jon_snow\") +\n  # Temporarily not available in paletteer?\n  \n  scale_colour_paletteer_c(`\"harrypotter::dracomalfoy\"`) +\n  labs(title = \"Using Paletteer\",\n       subtitle = \"Continuous Palette-Hary Potter: Draco Malfoy\",\n       caption = \"Oh you awful Srishti people...\")\n\nError in `gen_fun()`:\n! The package \"harrypotter\" is required.\n\n  # Harry Potter Gryffindor Palette.\n  # Will ask for `harrypotter` package to be installed. Say yes!\n  p2 +\n  scale_colour_paletteer_c(\"harrypotter::gryffindor\") +\n  labs(title = \"Using Paletteer\",\n       subtitle = \"Continuous Palette-Harry Potter: Gryffindor\")\n\nError in `gen_fun()`:\n! The package \"harrypotter\" is required."
  },
  {
    "objectID": "content/labs/r-labs/tidy/dplyr.html",
    "href": "content/labs/r-labs/tidy/dplyr.html",
    "title": "Introduction to the dplyr package",
    "section": "",
    "text": "One of the dominant paradigms of working with data in R is to render it into “tidy” form. A huge benefit of the tidy way of working is that it influences your thinking with data and helps plan out your operations, in going from purpose to actual code in a swift and intuitive manner. This tidy form allows for a huge variety of data manipulation, summarizing, and plotting tasks, that can be performed using the packages of the tidyverse, and other packages that leverage the power of the tidyverse."
  },
  {
    "objectID": "content/labs/r-labs/tidy/dplyr.html#introduction",
    "href": "content/labs/r-labs/tidy/dplyr.html#introduction",
    "title": "Introduction to the dplyr package",
    "section": "",
    "text": "One of the dominant paradigms of working with data in R is to render it into “tidy” form. A huge benefit of the tidy way of working is that it influences your thinking with data and helps plan out your operations, in going from purpose to actual code in a swift and intuitive manner. This tidy form allows for a huge variety of data manipulation, summarizing, and plotting tasks, that can be performed using the packages of the tidyverse, and other packages that leverage the power of the tidyverse."
  },
  {
    "objectID": "content/labs/r-labs/tidy/dplyr.html#setting-up-the-packages",
    "href": "content/labs/r-labs/tidy/dplyr.html#setting-up-the-packages",
    "title": "Introduction to the dplyr package",
    "section": "Setting up the Packages",
    "text": "Setting up the Packages\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "content/labs/r-labs/tidy/dplyr.html#tidy-data",
    "href": "content/labs/r-labs/tidy/dplyr.html#tidy-data",
    "title": "Introduction to the dplyr package",
    "section": "Tidy Data",
    "text": "Tidy Data\n\ndata(starwars)\ndim(starwars)\n\n[1] 87 14\n\nstarwars\n\n\n  \n\n\n\n“Tidy Data” is an important way of thinking about what data typically look like in R. Let’s fetch a figure from the web to show the (preferred) structure of data in R.\n\n\nTidy Data\n\nThe three features described in the figure above define the nature of tidy data:\n\nVariables in Columns\n\nObservations in Rows and\n\nMeasurements in Cells.\n\nData are imagined to be resulting from an experiment. Each variable represents a parameter/aspect in the experiment. Each row represents an additional datum of measurement. A cell is a single measurement on a single parameter(column) in a single observation(row).\nWhen working with data you must:\n\nFigure out what you want to do. (Purpose)\nDescribe those tasks in the form of a computer program. (Plain English to R Code)\nExecute the program.\n\nThe dplyr package makes these steps fast and easy:\n\nBy constraining your options, it helps you think about your data manipulation challenges.\nIt provides simple “verbs”, functions that correspond to the most common data manipulation tasks, to help you translate your thoughts into code.\nIt uses efficient backends, so you spend less time waiting for the computer.\n\n\nNe’er you mind about backends ;-) See Shakespeare’s Hamlet.\n\nThis document introduces you to dplyr’s basic set of tools, and shows you how to apply them to data frames. dplyr also supports databases via the dbplyr package, once you’ve installed, read vignette(\"dbplyr\") to learn more."
  },
  {
    "objectID": "content/labs/r-labs/tidy/dplyr.html#data-starwars",
    "href": "content/labs/r-labs/tidy/dplyr.html#data-starwars",
    "title": "Introduction to the dplyr package",
    "section": "Data: starwars",
    "text": "Data: starwars\nTo explore the basic data manipulation verbs of dplyr, we’ll use the dataset starwars. This dataset contains 87 characters and comes from the Star Wars API, and is documented in ?starwars\n\nThis means: type ?starwars in the Console. Try.\n\nNote that starwars is a tibble, a modern re-imagining of the data frame. It’s particularly useful for large datasets because it only prints the first few rows. You can learn more about tibbles at https://tibble.tidyverse.org; in particular you can convert data frames to tibbles with as_tibble().\n\nCheck your Environment Tab to inspect starwars in a separate tab."
  },
  {
    "objectID": "content/labs/r-labs/tidy/dplyr.html#single-table-verbs",
    "href": "content/labs/r-labs/tidy/dplyr.html#single-table-verbs",
    "title": "Introduction to the dplyr package",
    "section": "Single table verbs",
    "text": "Single table verbs\ndplyr aims to provide a function for each basic verb of data manipulation. These verbs can be organised into three categories based on the component of the dataset that they work with:\n\nRows:\n\n\nfilter() chooses rows based on column values.\n\nslice() chooses rows based on location.\n\narrange() changes the order of the rows.\n\n\nColumns:\n\n\nselect() changes whether or not a column is included.\n\nrename() changes the name of columns.\n\nmutate() changes the values of columns and creates new columns.\n\nrelocate() changes the order of the columns.\n\n\nGroups of rows:\n\n\nsummarise() collapses a group into a single row.\n\n\n\n\nThink of the parallels from Microsoft Excel.\n\nThe pipe\nAll of the dplyr functions take a data frame (or tibble) as the first argument. Rather than forcing the user to either save intermediate objects or nest functions, dplyr provides the %&gt;% operator from magrittr. x %&gt;% f(y) turns into f(x, y) so the result from one step is then “piped” into the next step. You can use the pipe to rewrite multiple operations that you can read left-to-right, top-to-bottom (reading the pipe operator as “then”).\nFilter rows with filter()\n\nfilter() allows you to select a subset of rows in a data frame. Like all single verbs, the first argument is the tibble (or data frame). The second and subsequent arguments refer to variables within that data frame, selecting rows where the expression is TRUE.\nFor example, we can select all character with light skin color and brown eyes with:\n\nNote the double equal to sign (==) below! Equivalent to MS Excel Data -&gt; Filter\n\n\nstarwars %&gt;% filter(skin_color == \"light\", eye_color == \"brown\")\n\n\n  \n\n\n\nArrange rows with arrange()\n\narrange() works similarly to filter() except that instead of filtering or selecting rows, it reorders them. It takes a data frame, and a set of column names (or more complicated expressions) to order by. If you provide more than one column name, each additional column will be used to break ties in the values of preceding columns:\n\nstarwars %&gt;% arrange(height, mass)\n\n\n  \n\n\n\nUse desc() to order a column in descending order:\n\nstarwars %&gt;% arrange(desc(height))\n\n\n  \n\n\n\nChoose rows using their position with slice()\n\nslice() lets you index rows by their (integer) locations. It allows you to select, remove, and duplicate rows.\n\nThis is an important step in Prediction, Modelling and Machine Learning.\n\nWe can get characters from row numbers 5 through 10.\n\nstarwars %&gt;% slice(5:10)\n\n\n  \n\n\n\nIt is accompanied by a number of helpers for common use cases:\n\n\nslice_head() and slice_tail() select the first or last rows.\n\n\nstarwars %&gt;% slice_head(n = 3)\n\n\n  \n\n\n\n\n\nslice_sample() randomly selects rows. Use the option prop to choose a certain proportion of the cases.\n\n\nstarwars %&gt;% slice_sample(n = 5)\n\n\n  \n\n\nstarwars %&gt;% slice_sample(prop = 0.1)\n\n\n  \n\n\n\nUse replace = TRUE to perform a bootstrap sample. If needed, you can weight the sample with the weight argument.\n\nBootstrap samples are a special statistical sampling method. Counterintuitive perhaps, since you sample with replacement. Should remind you of your high school Permutation and Combination class, with all those urn models and so on. If you remember.\n\n\n\nslice_min() and slice_max() select rows with highest or lowest values of a variable. Note that we first must choose only the values which are not NA.\n\n\nstarwars %&gt;%\n  filter(!is.na(height)) %&gt;%\n  slice_min(height, n = 3)\n\n\n  \n\n\n\nSelect columns with select()\n\nOften you work with large datasets with many columns but only a few are actually of interest to you. select() allows you to rapidly zoom in on a useful subset using operations that usually only work on numeric variable positions:\n\n# Select columns by name\nstarwars %&gt;% select(hair_color, skin_color, eye_color)\n\n\n  \n\n\n# Select all columns between hair_color and eye_color (inclusive)\nstarwars %&gt;% select(hair_color:eye_color)\n\n\n  \n\n\n# Select all columns except those from hair_color to eye_color (inclusive)\nstarwars %&gt;% select(!(hair_color:eye_color))\n\n\n  \n\n\n# Select all columns ending with color\nstarwars %&gt;% select(ends_with(\"color\"))\n\n\n  \n\n\n\nThere are a number of helper functions you can use within select(), like starts_with(), ends_with(), matches() and contains(). These let you quickly match larger blocks of variables that meet some criterion. See ?select for more details.\nYou can rename variables with select() by using named arguments:\n\nstarwars %&gt;% select(home_world = homeworld)\n\n\n  \n\n\n\nBut because select() drops all the variables not explicitly mentioned, it’s not that useful. Instead, use rename():\n\nstarwars %&gt;% rename(home_world = homeworld)\n\n\n  \n\n\n\nAdd new columns with mutate()\n\nBesides selecting sets of existing columns, it’s often useful to add new columns that are functions of existing columns. This is the job of mutate():\n\nstarwars %&gt;% mutate(height_m = height / 100)\n\n\n  \n\n\n\nWe can’t see the height in meters we just calculated, but we can fix that using a select command.\n\nstarwars %&gt;%\n  mutate(height_m = height / 100) %&gt;%\n  select(height_m, height, everything())\n\n\n  \n\n\n\ndplyr::mutate() is similar to the base transform(), but allows you to refer to columns that you’ve just created:\n\nstarwars %&gt;%\n  mutate(\n    height_m = height / 100,\n    BMI = mass / (height_m^2)\n  ) %&gt;%\n  select(BMI, everything())\n\n\n  \n\n\n\nIf you only want to keep the new variables, use transmute():\n\nstarwars %&gt;%\n  transmute(\n    height_m = height / 100,\n    BMI = mass / (height_m^2)\n  )\n\n\n  \n\n\n\nChange column order with relocate()\n\nUse a similar syntax as select() to move blocks of columns at once\n\nstarwars %&gt;% relocate(sex:homeworld, .before = height)\n\n\n  \n\n\n\nSummarise values with summarise()\n\nThe last verb is summarise(). It collapses a data frame to a single row.\n\nstarwars %&gt;% summarise(mean_height = mean(height, na.rm = TRUE))\n\n\n  \n\n\n\nIt’s not that useful until we learn the group_by() verb below.\nCommonalities\nYou may have noticed that the syntax and function of all these verbs are very similar:\n\nThe first argument is a data frame.\nThe subsequent arguments describe what to do with the data frame. You can refer to columns in the data frame directly without using $.\nThe result is a new data frame\n\nTogether these properties make it easy to chain together multiple simple steps to achieve a complex result.\nThese five functions provide the basis of a language of data manipulation. At the most basic level, you can only alter a tidy data frame in five useful ways: you can reorder the rows (arrange()), pick observations and variables of interest (filter() and select()), add new variables that are functions of existing variables (mutate()), or collapse many values to a summary (summarise())."
  },
  {
    "objectID": "content/labs/r-labs/tidy/dplyr.html#combining-functions-with",
    "href": "content/labs/r-labs/tidy/dplyr.html#combining-functions-with",
    "title": "Introduction to the dplyr package",
    "section": "Combining functions with %>%\n",
    "text": "Combining functions with %&gt;%\n\nThe dplyr API is functional in the sense that function calls don’t have side-effects. You must always save their results. This doesn’t lead to particularly elegant code, especially if you want to do many operations at once. You either have to do it step-by-step:\n\na1 &lt;- group_by(starwars, species, sex)\na2 &lt;- select(a1, height, mass)\na3 &lt;- summarise(a2,\n  height = mean(height, na.rm = TRUE),\n  mass = mean(mass, na.rm = TRUE)\n)\n\nOr if you don’t want to name the intermediate results, you need to wrap the function calls inside each other:\n\nsummarise(\n  select(\n    group_by(starwars, species, sex),\n    height, mass\n  ),\n  height = mean(height, na.rm = TRUE),\n  mass = mean(mass, na.rm = TRUE)\n)\n\n\n  \n\n\n\nThis is difficult to read because the order of the operations is from inside to out. Thus, the arguments are a long way away from the function. To get around this problem, dplyr provides the %&gt;% operator from magrittr. x %&gt;% f(y) turns into f(x, y) so you can use it to rewrite multiple operations that you can read left-to-right, top-to-bottom (reading the pipe operator as “then”):\n\nstarwars %&gt;%\n  group_by(species, sex) %&gt;%\n  summarise(\n    mean_height = mean(height, na.rm = TRUE),\n    mean_mass = mean(mass, na.rm = TRUE)\n  )"
  },
  {
    "objectID": "content/labs/r-labs/tidy/dplyr.html#patterns-of-operations",
    "href": "content/labs/r-labs/tidy/dplyr.html#patterns-of-operations",
    "title": "Introduction to the dplyr package",
    "section": "Patterns of operations",
    "text": "Patterns of operations\nThe dplyr verbs can be classified by the type of operations they accomplish (we sometimes speak of their semantics, i.e., their meaning). It’s helpful to have a good grasp of the difference between select and mutate operations.\nSelecting operations\nOne of the appealing features of dplyr is that you can refer to columns from the tibble as if they were regular variables. However, the syntactic uniformity of referring to bare column names hides semantical differences across the verbs. A column symbol supplied to select() does not have the same meaning as the same symbol supplied to mutate().\nSelecting operations expect column names and positions. Hence, when you call select() with bare variable names, they actually represent their own positions in the tibble. The following calls are completely equivalent from dplyr’s point of view:\n\n# `name` represents the integer 1\nselect(starwars, name)\n\n\n  \n\n\nselect(starwars, 1)\n\n\n  \n\n\n\nBy the same token, this means that you cannot refer to variables from the surrounding context if they have the same name as one of the columns. In the following example, height still represents 2, not 5:\n\nheight &lt;- 5\nselect(starwars, height)\n\n\n  \n\n\n\nOne useful subtlety is that this only applies to bare names and to selecting calls like c(height, mass) or height:mass. In all other cases, the columns of the data frame are not put in scope. This allows you to refer to contextual variables in selection helpers:\n\nname &lt;- \"color\"\nselect(starwars, ends_with(name))\n\n\n  \n\n\n\nThese semantics are usually intuitive. But note the subtle difference:\n\nname &lt;- 5\nselect(starwars, name, identity(name))\n\n\n  \n\n\n\nIn the first argument, name represents its own position 1. In the second argument, name is evaluated in the surrounding context and represents the fifth column.\nMutating operations\nMutate semantics are quite different from selection semantics. Whereas select() expects column names or positions, mutate() expects column vectors. We will set up a smaller tibble to use for our examples.\n\ndf &lt;- starwars %&gt;% select(name, height, mass)\n\nWhen we use select(), the bare column names stand for their own positions in the tibble. For mutate() on the other hand, column symbols represent the actual column vectors stored in the tibble. Consider what happens if we give a string or a number to mutate():\n\nmutate(df, \"height\", 2)\n\n\n  \n\n\n\nmutate() gets length-1 vectors that it interprets as new columns in the data frame. These vectors are recycled so they match the number of rows. That’s why it doesn’t make sense to supply expressions like \"height\" + 10 to mutate(). This amounts to adding 10 to a string! The correct expression is:\n\nmutate(df, height + 10)\n\n\n  \n\n\n\nIn the same way, you can unquote values from the context if these values represent a valid column. They must be either length 1 (they then get recycled) or have the same length as the number of rows. In the following example we create a new vector that we add to the data frame:\n\nvar &lt;- seq(1, nrow(df))\nmutate(df, new = var)\n\n\n  \n\n\n\nA case in point is group_by(). While you might think it has select semantics, it actually has mutate semantics. This is quite handy as it allows to group by a modified column:\n\ngroup_by(starwars, sex)\n\n\n  \n\n\ngroup_by(starwars, sex = as.factor(sex))\n\n\n  \n\n\ngroup_by(starwars, height_binned = cut(height, 3))\n\n\n  \n\n\n\nThis is why you can’t supply a column name to group_by(). This amounts to creating a new column containing the string recycled to the number of rows:\n\ngroup_by(df, \"month\")"
  },
  {
    "objectID": "content/labs/r-labs/tidy/dplyr.html#two-table-verbs",
    "href": "content/labs/r-labs/tidy/dplyr.html#two-table-verbs",
    "title": "Introduction to the dplyr package",
    "section": "Two table verbs",
    "text": "Two table verbs\nSometimes our data is spread across more than one table. Often these tables are linked by some common, or common-looking, variable columns. dplyr allows us to work with such data that is spread over more than one table. More information is available here: Two Table Verbs in dplyr\nThe operations/verbs used to manipulate two-table verbs are:\n\nMutating joins, which add new variables to one table from matching rows in another.\n\ninner_join()\n\n\n\n\n\n\n\n\n\n\n\n\nleft_join()\n\n\n\n\n\n\n\n\n\n\nright_join()\n\n\n\n\n\n\n\n\n\n\nfull_join()\n\n\n\n\n\n\n\n\n\n\nFiltering joins, which filter observations from one table based on whether or not they match an observation in the other table.\n\n\nsemi_join(x, y) keeps all observations in x that have a match in y.\n\n\n\n\n\n\n\n\n\n\n\n\n\nanti_join(x, y) drops all observations in x that have a match in\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSet operations, which combine the observations in the data sets as if they were set elements.\n\nunion()\n\n\n\n\n\n\n\n\n\n\n\n\nunion_all(),\n\n\n\n\n\n\n\n\n\n\nintersect(),\n\n\n\n\n\n\n\n\n\n\nsetdiff()\n\n\n\n\n\n\n\n\n\n\nTidyr Operations:\npivot_longer()\npivot_wider()"
  },
  {
    "objectID": "content/labs/r-labs/dashboard/flexdashboard.html",
    "href": "content/labs/r-labs/dashboard/flexdashboard.html",
    "title": "ggplotly: various examples",
    "section": "",
    "text": "# This example modifies code from Hadley Wickham (https://gist.github.com/hadley/233134)\n# It also uses data from Nathan Yau's flowingdata site (http://flowingdata.com/)\nunemp &lt;- read.csv(\"http://datasets.flowingdata.com/unemployment09.csv\")\nnames(unemp) &lt;- c(\"id\", \"state_fips\", \"county_fips\", \"name\", \"year\", \n                  \"?\", \"?\", \"?\", \"rate\")\nunemp$county &lt;- tolower(gsub(\" County, [A-Z]{2}\", \"\", unemp$name))\nunemp$state &lt;- gsub(\"^.*([A-Z]{2}).*$\", \"\\\\1\", unemp$name)\ncounty_df &lt;- map_data(\"county\")\nnames(county_df) &lt;- c(\"long\", \"lat\", \"group\", \"order\", \"state_name\", \"county\")\ncounty_df$state &lt;- state.abb[match(county_df$state_name, tolower(state.name))]\ncounty_df$state_name &lt;- NULL\nstate_df &lt;- map_data(\"state\")\nchoropleth &lt;- merge(county_df, unemp, by = c(\"state\", \"county\"))\nchoropleth &lt;- choropleth[order(choropleth$order), ]\nchoropleth$rate_d &lt;- cut(choropleth$rate, breaks = c(seq(0, 10, by = 2), 35))\n\n# provide a custom tooltip to plotly with the county name and actual rate\nchoropleth$text &lt;- with(choropleth, paste0(\"County: \", name, \"Rate: \", rate))\np &lt;- ggplot(choropleth, aes(long, lat, group = group)) +\n  geom_polygon(aes(fill = rate_d, text = text), \n               colour = alpha(\"white\", 1/2), size = 0.2) + \n  geom_polygon(data = state_df, colour = \"white\", fill = NA) +\n  scale_fill_brewer(palette = \"PuRd\") + theme_void()\n# just show the text aesthetic in the tooltip\nggplotly(p, tooltip = \"text\")\n\n\n\n\n\n\n\ncrimes &lt;- data.frame(state = tolower(rownames(USArrests)), USArrests)\ncrimesm &lt;- tidyr::gather(crimes, variable, value, -state)\nstates_map &lt;- map_data(\"state\")\ng &lt;- ggplot(crimesm, aes(map_id = state)) +\n  geom_map(aes(fill = value), map = states_map) +\n  expand_limits(x = states_map$long, y = states_map$lat) +\n  facet_wrap( ~ variable) + theme_void()\nggplotly(g)"
  },
  {
    "objectID": "content/labs/r-labs/dashboard/flexdashboard.html#row",
    "href": "content/labs/r-labs/dashboard/flexdashboard.html#row",
    "title": "ggplotly: various examples",
    "section": "",
    "text": "# This example modifies code from Hadley Wickham (https://gist.github.com/hadley/233134)\n# It also uses data from Nathan Yau's flowingdata site (http://flowingdata.com/)\nunemp &lt;- read.csv(\"http://datasets.flowingdata.com/unemployment09.csv\")\nnames(unemp) &lt;- c(\"id\", \"state_fips\", \"county_fips\", \"name\", \"year\", \n                  \"?\", \"?\", \"?\", \"rate\")\nunemp$county &lt;- tolower(gsub(\" County, [A-Z]{2}\", \"\", unemp$name))\nunemp$state &lt;- gsub(\"^.*([A-Z]{2}).*$\", \"\\\\1\", unemp$name)\ncounty_df &lt;- map_data(\"county\")\nnames(county_df) &lt;- c(\"long\", \"lat\", \"group\", \"order\", \"state_name\", \"county\")\ncounty_df$state &lt;- state.abb[match(county_df$state_name, tolower(state.name))]\ncounty_df$state_name &lt;- NULL\nstate_df &lt;- map_data(\"state\")\nchoropleth &lt;- merge(county_df, unemp, by = c(\"state\", \"county\"))\nchoropleth &lt;- choropleth[order(choropleth$order), ]\nchoropleth$rate_d &lt;- cut(choropleth$rate, breaks = c(seq(0, 10, by = 2), 35))\n\n# provide a custom tooltip to plotly with the county name and actual rate\nchoropleth$text &lt;- with(choropleth, paste0(\"County: \", name, \"Rate: \", rate))\np &lt;- ggplot(choropleth, aes(long, lat, group = group)) +\n  geom_polygon(aes(fill = rate_d, text = text), \n               colour = alpha(\"white\", 1/2), size = 0.2) + \n  geom_polygon(data = state_df, colour = \"white\", fill = NA) +\n  scale_fill_brewer(palette = \"PuRd\") + theme_void()\n# just show the text aesthetic in the tooltip\nggplotly(p, tooltip = \"text\")\n\n\n\n\n\n\n\ncrimes &lt;- data.frame(state = tolower(rownames(USArrests)), USArrests)\ncrimesm &lt;- tidyr::gather(crimes, variable, value, -state)\nstates_map &lt;- map_data(\"state\")\ng &lt;- ggplot(crimesm, aes(map_id = state)) +\n  geom_map(aes(fill = value), map = states_map) +\n  expand_limits(x = states_map$long, y = states_map$lat) +\n  facet_wrap( ~ variable) + theme_void()\nggplotly(g)"
  },
  {
    "objectID": "content/labs/r-labs/dashboard/flexdashboard.html#row-1",
    "href": "content/labs/r-labs/dashboard/flexdashboard.html#row-1",
    "title": "ggplotly: various examples",
    "section": "Row",
    "text": "Row\nFaithful Eruptions\n\nm &lt;- ggplot(faithful, aes(x = eruptions, y = waiting)) +\n  stat_density_2d() + xlim(0.5, 6) + ylim(40, 110)\nggplotly(m)\n\n\n\n\n\nFaithful Eruptions (polygon)\n\nm &lt;- ggplot(faithful, aes(x = eruptions, y = waiting)) +\n  stat_density_2d(aes(fill = ..level..), geom = \"polygon\") + \n  xlim(0.5, 6) + ylim(40, 110)\nggplotly(m)\n\n\n\n\n\nFaithful Eruptions (hex)\n\nm &lt;- ggplot(faithful, aes(x = eruptions, y = waiting)) + geom_hex() \nggplotly(m)\n\nError in loadNamespace(x): there is no package called 'hexbin'"
  },
  {
    "objectID": "content/labs/r-labs/installation/installation.html",
    "href": "content/labs/r-labs/installation/installation.html",
    "title": "Lab 01 - Installation",
    "section": "",
    "text": "At the end of this Lab, we will:\n\nhave installed R and RStudio on our machines\nunderstood how to add additional R-packages for specific features and graphic capability\nrun code within RStudio and interpret the results\nhave learnt to look for help within R and RStudio"
  },
  {
    "objectID": "content/labs/r-labs/installation/installation.html#Check-In-R",
    "href": "content/labs/r-labs/installation/installation.html#Check-In-R",
    "title": "Lab 01 - Installation",
    "section": "Check in",
    "text": "Check in\nLaunch R by clicking this logo. You should see one console with a command line interpreter. Try typing 2 + 2 and check !\nClose R."
  },
  {
    "objectID": "content/labs/r-labs/installation/installation.html#Check-In-RStudio",
    "href": "content/labs/r-labs/installation/installation.html#Check-In-RStudio",
    "title": "Lab 01 - Installation",
    "section": "Check in",
    "text": "Check in\nLaunch RStudio. You should get a window similar to the screenshot you see here, but yours will be empty. Look at the bottom left pane: this is the same console window you saw when you opened R in step @Check-In-R\n\nPlace your cursor where you see &gt; and type x &lt;- 2 + 2 again hit enter or return, then type x, and hit enter/return again.\nIf [1] 4 prints to the screen, you have successfully installed R and RStudio, and you can move onto installing packages."
  },
  {
    "objectID": "content/labs/r-labs/installation/installation.html#save-and-share",
    "href": "content/labs/r-labs/installation/installation.html#save-and-share",
    "title": "Lab 01 - Installation",
    "section": "Save and share",
    "text": "Save and share\nSave your work so you can share your favorite plot with us. You will not like the looks of your plot if you mouse over to Export and save it. Instead, use ggplot2’s command for saving a plot with sensible defaults:\n\nhelp(ggsave)\n\n\nggsave(\"file_name_here.pdf\", plot) # please make the filename unique!\n\nThat’s it! You have created and saved your first chart in R!!!"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html",
    "href": "content/labs/r-labs/maps/gram-maps.html",
    "title": "The Grammar of Maps",
    "section": "",
    "text": "This RMarkdown document is part of my Workshop Course in R. The intent is to build Skill in coding in R, and also appreciate R as a way to metaphorically visualize information of various kinds, using predominantly geometric figures and structures.\nAll RMarkdown/Quarto files combine code, text, web-images, and figures developed using code. Everything is text; code chunks are enclosed in fences (```)"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#introduction",
    "href": "content/labs/r-labs/maps/gram-maps.html#introduction",
    "title": "The Grammar of Maps",
    "section": "",
    "text": "This RMarkdown document is part of my Workshop Course in R. The intent is to build Skill in coding in R, and also appreciate R as a way to metaphorically visualize information of various kinds, using predominantly geometric figures and structures.\nAll RMarkdown/Quarto files combine code, text, web-images, and figures developed using code. Everything is text; code chunks are enclosed in fences (```)"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#goals",
    "href": "content/labs/r-labs/maps/gram-maps.html#goals",
    "title": "The Grammar of Maps",
    "section": "Goals",
    "text": "Goals\nAt the end of this Lab session, we should:\n- know the types and structures of spatial data and be able to work with them\n- understand the basics of modern spatial packages in R\n- be able to specify and download spatial data from the web, using R from sources such as naturalearth and Open Streep Map\n- plot static and interactive maps using ggplot, tmap and leaflet packages\n- add symbols and markers for places and regions of our own interest in these maps.\n- plot maps on a globe using the threejs package"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#pedagogical-note",
    "href": "content/labs/r-labs/maps/gram-maps.html#pedagogical-note",
    "title": "The Grammar of Maps",
    "section": "Pedagogical Note",
    "text": "Pedagogical Note\nThe method followed will be based on PRIMM:\n\n\nPREDICT Inspect the code and guess at what the code might do, write predictions\n\n\nRUN the code provided and check what happens\n\nINFER what the parameters of the code do and write comments to explain. What bells and whistles can you see?\n\nMODIFY the parameters code provided to understand the options available. Write comments to show what you have aimed for and achieved.\n\nMAKE : take an idea/concept of your own, and graph it.\n\nAll jargon words will be capitalized and in bold font."
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#set-up",
    "href": "content/labs/r-labs/maps/gram-maps.html#set-up",
    "title": "The Grammar of Maps",
    "section": "Set Up",
    "text": "Set Up\nThe setup code chunk below brings into our coding session R packages that provide specific computational abilities and also datasets which we can use.\nTo reiterate: Packages and datasets are not the same thing !! Packages are (small) collections of programs. Datasets are just….information."
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#setup-the-packages",
    "href": "content/labs/r-labs/maps/gram-maps.html#setup-the-packages",
    "title": "The Grammar of Maps",
    "section": "Setup the Packages",
    "text": "Setup the Packages\n\nRun this in your Console first: devtools::install_github(\"ropensci/rnaturalearthhires\")\nInstall all packages that are flagged by RStudio when you open this RMarkdown file\n\n\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\n\n# Run this in your console first\n# devtools::install_github(\"ropensci/rnaturalearthhires\")\nlibrary(rnaturalearthhires)\n\n# Plotting Maps\nlibrary(tidyverse) # Maps using ggplot + geom_sf\nlibrary(tmap) # Thematic Maps, static and interactive\nlibrary(osmdata) # Fetch map data from osmdata.org\nlibrary(leaflet) # interactive Maps\nlibrary(threejs) # Globe maps in R. Part of the htmlwidgets family of packages\n\n# For Spatial Data Frame Processing\nlibrary(sf)"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#introduction-to-maps-in-r",
    "href": "content/labs/r-labs/maps/gram-maps.html#introduction-to-maps-in-r",
    "title": "The Grammar of Maps",
    "section": "Introduction to Maps in R",
    "text": "Introduction to Maps in R\nWe will take small steps in making maps using just two of the several map making packages in R.\nThe steps we will use are:\n\nSearch for an area of interest\nLearn how to access spatial/map data using osmdata\n\nPlot and dress up our map using ggplot and tmap\n\nCreate interactive maps with leaflet using a variety of map data providers. (Note: tmap can also do interactive maps which we will explore also.)\n\nBas. Onwards and Map-wards!!"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#step1---specifying-an-area-of-interest",
    "href": "content/labs/r-labs/maps/gram-maps.html#step1---specifying-an-area-of-interest",
    "title": "The Grammar of Maps",
    "section": "Step1 - Specifying an area of interest",
    "text": "Step1 - Specifying an area of interest\nIn R, we need to specify a “BOUNDING BOX” first, to declare our area of interest. God made me a BengaluR-kaR…I think..Let’s see if we can declare an area of interest. Then we can order on Swiggy and…never mind.\nWe can declare a BOUNDING BOX in several ways.\n\nUsing a longitude latitude info from Bounding Box Tool which gives bounding boxes in many different formats.\n\n\nLocate the place of interest using the search box.\nclick on the “box with arrow” tool on the upper left. This will create a rectangular shape.\n\nMove/resize this box and then copy the bounding box from the menu at the bottom. Ensure you copy in CSV format.\n\n\n# https://boundingbox.klokantech.com\n# CSV: 77.574028,12.917262,77.595073,12.939895\nbbox_1 &lt;- matrix(\n  c(77.574028, 12.917262, 77.595073, 12.939895),\n  byrow = FALSE,\n  nrow = 2,\n  ncol = 2,\n  dimnames = list(c('x', 'y'), c('min', 'max'))\n)\nbbox_1\n\n       min      max\nx 77.57403 77.59507\ny 12.91726 12.93989\n\n\n\nUsing a place name to look up a BOUNDING BOX with osmdata::getbb. This may not always work if the place name is know well known.\n\n\n# Using getbb command from the osmdata package\nbbox_2 &lt;- osmdata::getbb(\"Jayanagar, Bangalore, India\")\nbbox_2\n\n       min      max\nx 77.56242 77.60242\ny 12.90927 12.94927\n\n\nLet us examine both the calculated BOUNDING BOXes:\n\nbbox_1\n\n       min      max\nx 77.57403 77.59507\ny 12.91726 12.93989\n\nbbox_2\n\n       min      max\nx 77.56242 77.60242\ny 12.90927 12.94927\n\n\nBoth look similar in size; bbox_2 is slightly bigger.\nWe will use the bbox_2 from the above, to ensure we have a decent collection of features. If the download becomes too hefty, we can fall back on the smaller bbox!"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#step2---get-map-data",
    "href": "content/labs/r-labs/maps/gram-maps.html#step2---get-map-data",
    "title": "The Grammar of Maps",
    "section": "Step2 - Get Map data",
    "text": "Step2 - Get Map data\n\nOpenStreetMap (OSM) provides maps of the world mostly created by volunteers. They are completely free to browse and use, with attribution to © OpenStreetMap contributors and adherence to the ODbL license required, and are used by many public and private organisations. OSM data can be downloaded in vector format and used for our own purposes. In this tutorial, we will obtain data from OSM using a query. A query is a request for data from a database. Simple queries can be performed more easily using the osmdata library for R, which automatically constructs the query and imports the data in a convenient format.\n\nOpen Street Map features have attributes in key-value pairs. We can use them to download the specific data we need. These features can easily be explored in the web browser, by using the ‘Query features’ button on OpenStreetMap (OSM):\n\n\nOSM Features\n\nHead off to OSM Street Map to try this out and to get an intuitive understanding of what OSM key-value pairs are, for different types of map features. Look for places of interest to you (features) and see what key-value pairs attach to those features.\nNOTE: key-value pairs are also referred to as tags.\nUseful key-value pairs / tags include:\n\n\n\n\n\n\nKEY\nVALUEs\n\n\n\nbuilding\nyes (all), house residential, apartments\n\n\nhighway\nresidential, service, track, unclassified, footway, path\n\n\namenity\nparking, parking_space, bench; place_of_worship; restaurant, cafe, fast_food; school, waste_basket, fuel, bank, toilets…\n\n\nshop\nconvenience, supermarket, clothes, hairdresser, car-repair…\n\n\nname\nactual name of the place e.g. Main_Street, McDonald’s, Pizza Hut, Subway\n\n\n\nwaterway\n\n\n\nnatural\n\n\n\nboundary\n\n\n\n\nFor more information see: OSM Tags for a nice visual description of popular key-value pairs that we can use. See what the highway tag looks like tag:highway\nThe osmdata commands available_features and available_tags can help also us get the associated *key-value** pairs to retrieve data from OSM.\nosmdata::available_features() %&gt;% as_tibble()\navailable_tags(feature = \"highway\")\navailable_tags(\"amenity\")\navailable_tags(\"natural\")\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nWe can use these key-value pairs to download different types of map data. Within our bbox for Jayanagar, Bangalore, we want to download diverse kinds of FEATURE data. Remember that a FEATURE is any object that can be “seen” on a map. This is done using the OPQ query in the osmdata package. The main parameters for this command are:\n\nbbox\n\nKEY / VALUE pairs (“TAGS”) to specify the kind of feature you need\n\nThe query returns a list data structure, with all geometries and features within the bounding box, and we can use any or all of them. Now we know the map features we are interested in. We also know what key-value pairs will be used to get this info from OSM.\n\n\n\n\n\n\nData Downloads from OSM\n\n\n\nDo not run these commands too many times. Re-run this ONLY if you have changed your BOUNDING BOX. We will get our map data from OSM and then save it avoid repeated downloads. So, please copy/paste and run the following commands in your console. The chunk below is set to eval:false so it will not run when you render!\n\n\n\n# Eval is set to false here\n# This code is for reference\n# Run these commands ONCE in your Console\n# Or run this chunk manually one time\n\n# Get all restaurants, atms, colleges within my bbox\nlocations &lt;- \n  osmdata::opq(bbox = bbox_2) %&gt;% \n  osmdata::add_osm_feature(key = \"amenity\", \n                           value = c(\"restaurant\", \"atm\", \"college\")) %&gt;% \n  osmdata_sf() %&gt;%  # Convert to Simple Features format\n  purrr::pluck(\"osm_points\") # Pull out the data frame of interest\n\n# Get all buildings within my bbox\ndat_buildings &lt;-\n  osmdata::opq(bbox = bbox_2) %&gt;% \n  osmdata::add_osm_feature(key = \"building\") %&gt;% \n  osmdata_sf() %&gt;% \n  purrr::pluck(\"osm_polygons\") \n\n# Get all residential roads within my bbox\ndat_roads &lt;- \n  osmdata::opq(bbox = bbox_2) %&gt;% \n  osmdata::add_osm_feature(key = \"highway\", \n                           value = c(\"residential\")) %&gt;% \n  osmdata_sf() %&gt;% \n  purrr::pluck(\"osm_lines\") \n\n# Get all parks / natural /greenery areas and spots within my bbox\ndat_natural &lt;-   \n  osmdata::opq(bbox = bbox_2) %&gt;% \n  osmdata::add_osm_feature(key = \"natural\",\n                           value = c(\"tree\", \"water\", \"wood\")) %&gt;% \n  osmdata_sf()\ndat_natural\n\ndat_trees &lt;- \n  dat_natural %&gt;% \n  purrr::pluck(\"osm_points\") \n\ndat_greenery &lt;- \n  dat_natural %&gt;% pluck(\"osm_polygons\")\n\nLet us save this data, so we don’t need to download all this again! We will store the downloaded data as .gpkg files on our local hard drives to use when we run this file again later. We will name our stored files as buildings, roads, and greenery, and trees, each with the .gpkg file extension, e.g. trees.gpkg.\nCheck your local project folder for these files after executing these commands.\n\n# Eval is set to false here\n# This code is for reference\n# Run these commands ONCE in your Console\n# Or manually run this chunk once\n\nst_write(dat_roads, dsn = \"roads.gpkg\", \n         append = FALSE, quiet = FALSE)\n\nst_write(dat_buildings, \n         dsn = \"buildings.gpkg\", \n         append = FALSE, \n         quiet = FALSE)\n\nst_write(dat_greenery, dsn = \"greenery.gpkg\", \n         append = FALSE,quiet = FALSE)\n\nst_write(dat_trees, dsn = \"trees.gpkg\", \n         append = FALSE,quiet = FALSE)\n\n\n\n\n\n\n\nWork from here when you resume!\n\n\n\nAlways work from here to avoid repeated downloads from OSM. Start from the top ONLY if you intend to map new locations and need to modify your Bounding Box.\n\n\nLet us now read back the saved Data:\n\nbuildings &lt;- st_read(\"./buildings.gpkg\")\n\nReading layer `buildings' from data source \n  `/Users/arvindv/Documents/RWork/MyWebsites/my-quarto-website/content/labs/r-labs/maps/buildings.gpkg' \n  using driver `GPKG'\nSimple feature collection with 35007 features and 105 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 77.56221 ymin: 12.90906 xmax: 77.60373 ymax: 12.94971\nGeodetic CRS:  WGS 84\n\ngreenery &lt;- st_read(\"./greenery.gpkg\")\n\nReading layer `greenery' from data source \n  `/Users/arvindv/Documents/RWork/MyWebsites/my-quarto-website/content/labs/r-labs/maps/greenery.gpkg' \n  using driver `GPKG'\nSimple feature collection with 14 features and 10 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 77.57552 ymin: 12.91545 xmax: 77.60233 ymax: 12.94947\nGeodetic CRS:  WGS 84\n\ntrees &lt;- st_read(\"./trees.gpkg\")\n\nReading layer `trees' from data source \n  `/Users/arvindv/Documents/RWork/MyWebsites/my-quarto-website/content/labs/r-labs/maps/trees.gpkg' \n  using driver `GPKG'\nSimple feature collection with 797 features and 9 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 77.56566 ymin: 12.91545 xmax: 77.60233 ymax: 12.94947\nGeodetic CRS:  WGS 84\n\nroads &lt;- st_read(\"./roads.gpkg\")\n\nReading layer `roads' from data source \n  `/Users/arvindv/Documents/RWork/MyWebsites/my-quarto-website/content/labs/r-labs/maps/roads.gpkg' \n  using driver `GPKG'\nSimple feature collection with 2662 features and 36 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 77.55748 ymin: 12.90635 xmax: 77.60603 ymax: 12.95636\nGeodetic CRS:  WGS 84\n\n\nHow many rows? ( Rows -&gt; Features ) What kind of geom column in each data set?\n\n# How many buildings?\nnrow(buildings)\n\n[1] 35007\n\nbuildings$geom\n\nGeometry set for 35007 features \nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 77.56221 ymin: 12.90906 xmax: 77.60373 ymax: 12.94971\nGeodetic CRS:  WGS 84\nFirst 5 geometries:\n\nclass(buildings$geom)\n\n[1] \"sfc_POLYGON\" \"sfc\"        \n\n\nSo the buildings dataset has 35007 buildings and their geometry is naturally a POLYGON type of geometry column.\nDo this check for all the other spatial data, in the code chunk below. What kind of geom column does each dataset have?"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#my-first-map-in-r",
    "href": "content/labs/r-labs/maps/gram-maps.html#my-first-map-in-r",
    "title": "The Grammar of Maps",
    "section": "My first Map in R",
    "text": "My first Map in R\nThere are two ways of plotting maps that we will learn:\nggplot and geom_sf()\nFirst we will plot with ggplot and geom_sf() : recall that our data is stored in 5 files: buildings, parks, roads, trees, and greenery.\n\nggplot() +\n  geom_sf(data = buildings, fill = \"gold\", color = \"grey\", linewidth = 0.025) +    # POLYGONS\n  geom_sf(data = roads, color = '#ff9999', linewidth = 0.5) +        # LINES\n  geom_sf(data = greenery, col = \"darkseagreen\") +  # POLYGONS\n  geom_sf(data = trees, col = \"darkgreen\")  +       # POINTS\n  \n  # Set plot limits to exactly the bbox_2\n  coord_sf(xlim = c(bbox_2[1,1], bbox_2[1,2]),\n           ylim = c(bbox_2[2,1], bbox_2[2,2]),\n           expand = FALSE) + \n  theme_minimal()\n\n\n\n\n\n\n\nNote how geom_sf is capable of handling any geometry in the sfc column !!\n\ngeom_sf() is an unusual geom because it will draw different geometric objects depending on what simple features are present in the data: you can get points, lines, or polygons.\n\nSo there, we have our first map!"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#map-using-tmap-package",
    "href": "content/labs/r-labs/maps/gram-maps.html#map-using-tmap-package",
    "title": "The Grammar of Maps",
    "section": "Map using tmap package",
    "text": "Map using tmap package\nWe can also create a map using a package called tmap. Here we also have the option of making the map interactive. tmap plots are made with code in “groups”: each group starts with a tm_shape() command.\n\n# Group-1\ntm_shape(buildings) +\n  tm_fill(col = \"burlywood\") +\n\n#Group-2\ntm_shape(roads) +\n  tm_lines(col = \"grey20\") +\n\n#Group-3  \ntm_shape(greenery) +\n  tm_polygons(col = \"limegreen\") +\n  \n\n#Group-4\ntm_shape(trees) +\n  tm_dots(col = \"darkgreen\")\n\n\n\n\n\n\n\nHow do we make this map interactive? One more line of code !! Add this line in your console and then run the above chunk again\ntmap_mode(\"view\")"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#using-data-from-tmap",
    "href": "content/labs/r-labs/maps/gram-maps.html#using-data-from-tmap",
    "title": "The Grammar of Maps",
    "section": "Using data from tmap\n",
    "text": "Using data from tmap\n\nLike many other packages ( e.g. ggplot ) tmap also has a few built-in spatial datasets: World and metro, rivers, land and a few others. Check help on these. Let’s plot a first map using datasets built into tmap.\n\ndata(\"World\")\nhead(World, n = 3)\n\n\n  \n\n\n\nWe have several 14 attribute variables in World. Attribute variables such as gdp_cap_est, HPI are numeric. Others such as income_grp appear to be factors. iso_a3 is the standard three letter name for the country. name is of course, the name for each country!\n\ndata(\"metro\")\nhead(metro, n = 3)\n\n\n  \n\n\n\nHere too we have attribute variables for the metros, and they seem predominantly numeric. Again iso_a3 is the three letter name for the city.\n\ntmap_mode(\"plot\") # Making this a static plot\n\n# Group 1\ntm_shape(World) + # dataset = World. \n    tm_polygons(\"HPI\") + # Colour polygons by HPI numeric variable\n\n  # Note the \"+\" sign continuation\n  \n# Group 2\ntm_shape(metro) + # dataset = metro\n  tm_bubbles(size = \"pop2030\", \n             col = \"red\") \n\n\n\n\n\n\n# Plot cities as bubbles\n# Size proportional to numeric variable `pop2030`\n\n\ntmap_mode(\"view\") # Change to Interactive\n\n\n# Let's use WaterColor Map this time!!\ntm_tiles(\"Stamen.Watercolor\") + # Watercolor map only with interactive\ntm_shape(World) +\n    tm_polygons(\"HPI\") + # Color by Happiness Index\n  \n  \ntm_shape(metro) + \n  tm_bubbles(size = \"pop2030\", # Size City Markers by Population in 2020\n             col = \"red\")"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#using-data-from-rnaturalearth",
    "href": "content/labs/r-labs/maps/gram-maps.html#using-data-from-rnaturalearth",
    "title": "The Grammar of Maps",
    "section": "Using data from rnaturalearth\n",
    "text": "Using data from rnaturalearth\n\nThe rnaturalearth package allows us to download shapes of countries. We can use it to get borders and also internal state/district boundaries.\n\nindia &lt;- \n  ne_states(country =  \"india\", \n            returnclass = \"sf\") # gives a ready sf dataframe !\n\nindia_neighbours &lt;- \n  ne_states(country = (c(\"sri lanka\", \"pakistan\",\n                         \"afghanistan\", \"nepal\",\"bangladesh\", \"bhutan\")\n                       ),\n            returnclass = \"sf\")\n\nLet’s look at the attribute variable columns to colour our graph and to shape our symbols:\n\nnames(india)\n\n  [1] \"featurecla\" \"scalerank\"  \"adm1_code\"  \"diss_me\"    \"iso_3166_2\"\n  [6] \"wikipedia\"  \"iso_a2\"     \"adm0_sr\"    \"name\"       \"name_alt\"  \n [11] \"name_local\" \"type\"       \"type_en\"    \"code_local\" \"code_hasc\" \n [16] \"note\"       \"hasc_maybe\" \"region\"     \"region_cod\" \"provnum_ne\"\n [21] \"gadm_level\" \"check_me\"   \"datarank\"   \"abbrev\"     \"postal\"    \n [26] \"area_sqkm\"  \"sameascity\" \"labelrank\"  \"name_len\"   \"mapcolor9\" \n [31] \"mapcolor13\" \"fips\"       \"fips_alt\"   \"woe_id\"     \"woe_label\" \n [36] \"woe_name\"   \"latitude\"   \"longitude\"  \"sov_a3\"     \"adm0_a3\"   \n [41] \"adm0_label\" \"admin\"      \"geonunit\"   \"gu_a3\"      \"gn_id\"     \n [46] \"gn_name\"    \"gns_id\"     \"gns_name\"   \"gn_level\"   \"gn_region\" \n [51] \"gn_a1_code\" \"region_sub\" \"sub_code\"   \"gns_level\"  \"gns_lang\"  \n [56] \"gns_adm1\"   \"gns_region\" \"min_label\"  \"max_label\"  \"min_zoom\"  \n [61] \"wikidataid\" \"name_ar\"    \"name_bn\"    \"name_de\"    \"name_en\"   \n [66] \"name_es\"    \"name_fr\"    \"name_el\"    \"name_hi\"    \"name_hu\"   \n [71] \"name_id\"    \"name_it\"    \"name_ja\"    \"name_ko\"    \"name_nl\"   \n [76] \"name_pl\"    \"name_pt\"    \"name_ru\"    \"name_sv\"    \"name_tr\"   \n [81] \"name_vi\"    \"name_zh\"    \"ne_id\"      \"name_he\"    \"name_uk\"   \n [86] \"name_ur\"    \"name_fa\"    \"name_zht\"   \"FCLASS_ISO\" \"FCLASS_US\" \n [91] \"FCLASS_FR\"  \"FCLASS_RU\"  \"FCLASS_ES\"  \"FCLASS_CN\"  \"FCLASS_TW\" \n [96] \"FCLASS_IN\"  \"FCLASS_NP\"  \"FCLASS_PK\"  \"FCLASS_DE\"  \"FCLASS_GB\" \n[101] \"FCLASS_BR\"  \"FCLASS_IL\"  \"FCLASS_PS\"  \"FCLASS_SA\"  \"FCLASS_EG\" \n[106] \"FCLASS_MA\"  \"FCLASS_PT\"  \"FCLASS_AR\"  \"FCLASS_JP\"  \"FCLASS_KO\" \n[111] \"FCLASS_VN\"  \"FCLASS_TR\"  \"FCLASS_ID\"  \"FCLASS_PL\"  \"FCLASS_GR\" \n[116] \"FCLASS_IT\"  \"FCLASS_NL\"  \"FCLASS_SE\"  \"FCLASS_BD\"  \"FCLASS_UA\" \n[121] \"FCLASS_TLC\" \"geometry\"  \n\nnames(india_neighbours)\n\n  [1] \"featurecla\" \"scalerank\"  \"adm1_code\"  \"diss_me\"    \"iso_3166_2\"\n  [6] \"wikipedia\"  \"iso_a2\"     \"adm0_sr\"    \"name\"       \"name_alt\"  \n [11] \"name_local\" \"type\"       \"type_en\"    \"code_local\" \"code_hasc\" \n [16] \"note\"       \"hasc_maybe\" \"region\"     \"region_cod\" \"provnum_ne\"\n [21] \"gadm_level\" \"check_me\"   \"datarank\"   \"abbrev\"     \"postal\"    \n [26] \"area_sqkm\"  \"sameascity\" \"labelrank\"  \"name_len\"   \"mapcolor9\" \n [31] \"mapcolor13\" \"fips\"       \"fips_alt\"   \"woe_id\"     \"woe_label\" \n [36] \"woe_name\"   \"latitude\"   \"longitude\"  \"sov_a3\"     \"adm0_a3\"   \n [41] \"adm0_label\" \"admin\"      \"geonunit\"   \"gu_a3\"      \"gn_id\"     \n [46] \"gn_name\"    \"gns_id\"     \"gns_name\"   \"gn_level\"   \"gn_region\" \n [51] \"gn_a1_code\" \"region_sub\" \"sub_code\"   \"gns_level\"  \"gns_lang\"  \n [56] \"gns_adm1\"   \"gns_region\" \"min_label\"  \"max_label\"  \"min_zoom\"  \n [61] \"wikidataid\" \"name_ar\"    \"name_bn\"    \"name_de\"    \"name_en\"   \n [66] \"name_es\"    \"name_fr\"    \"name_el\"    \"name_hi\"    \"name_hu\"   \n [71] \"name_id\"    \"name_it\"    \"name_ja\"    \"name_ko\"    \"name_nl\"   \n [76] \"name_pl\"    \"name_pt\"    \"name_ru\"    \"name_sv\"    \"name_tr\"   \n [81] \"name_vi\"    \"name_zh\"    \"ne_id\"      \"name_he\"    \"name_uk\"   \n [86] \"name_ur\"    \"name_fa\"    \"name_zht\"   \"FCLASS_ISO\" \"FCLASS_US\" \n [91] \"FCLASS_FR\"  \"FCLASS_RU\"  \"FCLASS_ES\"  \"FCLASS_CN\"  \"FCLASS_TW\" \n [96] \"FCLASS_IN\"  \"FCLASS_NP\"  \"FCLASS_PK\"  \"FCLASS_DE\"  \"FCLASS_GB\" \n[101] \"FCLASS_BR\"  \"FCLASS_IL\"  \"FCLASS_PS\"  \"FCLASS_SA\"  \"FCLASS_EG\" \n[106] \"FCLASS_MA\"  \"FCLASS_PT\"  \"FCLASS_AR\"  \"FCLASS_JP\"  \"FCLASS_KO\" \n[111] \"FCLASS_VN\"  \"FCLASS_TR\"  \"FCLASS_ID\"  \"FCLASS_PL\"  \"FCLASS_GR\" \n[116] \"FCLASS_IT\"  \"FCLASS_NL\"  \"FCLASS_SE\"  \"FCLASS_BD\"  \"FCLASS_UA\" \n[121] \"FCLASS_TLC\" \"geometry\"  \n\n# Look only at attributes\nindia %&gt;% st_drop_geometry() %&gt;% head()\n\n\n  \n\n\nindia_neighbours %&gt;% st_drop_geometry() %&gt;% head()\n\n\n  \n\n\n\nIn the india data frame:\n- Column iso_a2 contains the country name.\n- Column name contains the name of the state\nIn the india_neighbours data frame:\n- Column gu_a3 contains the country abbreviation\n- Column name contains the name of the state\n- Column iso_3166_2 contains the abbreviation of the state within each neighbouring country.\n\ntmap_mode(\"view\")\n\n# Plot India\n  tm_shape(india) +\n  tm_polygons(\"name\", # Colour by States in India\n              legend.show = FALSE) +\n  \n# Plot Neighbours\n  tm_shape(india_neighbours) +\n  tm_fill(col = \"gu_a3\") +  # Colour by Country Name\n  \n# Plot the cities in India alone\n  tm_shape(metro %&gt;% dplyr::filter(iso_a3 == \"IND\")) +\n    \n  tm_dots(size = \"pop2020\",legend.size.show = FALSE) +\n    # size by population in 2020\n    \n  tm_layout(legend.show = FALSE) +\n  tm_credits(\"Geographical Boundaries are not accurate\",\n             size = 0.5,\n             position = \"right\") +\n  tm_compass(position = c(\"right\", \"top\")) +\n  tm_scale_bar(position = \"left\") +\n  tmap_style(style = \"classic\") \n\n\n\n\n#Try other map styles\n#cobalt #gray #white #watercolor #beaver #classic #watercolor #albatross #bw #col_blind"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#your-turn-2",
    "href": "content/labs/r-labs/maps/gram-maps.html#your-turn-2",
    "title": "The Grammar of Maps",
    "section": "Your Turn 2",
    "text": "Your Turn 2\nCan you try to download a map area of your home town and plot it as we have above?"
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#adding-my-favourite-restaurants-to-the-map",
    "href": "content/labs/r-labs/maps/gram-maps.html#adding-my-favourite-restaurants-to-the-map",
    "title": "The Grammar of Maps",
    "section": "Adding my favourite Restaurants to the map",
    "text": "Adding my favourite Restaurants to the map\nIs it time to order on Swiggy…\nLet us adding interesting places to our map: say based on your favourite restaurants etc. We need restaurant data: lat/long + name + maybe type of restaurant. This can be manually created ( like all of OSMdata ) or if it is already there we can download using key-value pairs in our OSM data query.\nRestaurants can be downloaded using key= \"amenity\", value = \"restaurant\" or \"cafe\" etc. There are also other tags to explore!Searching for McDonalds for instance…( key = \"name\", value = \"McDonalds\"). Since we want JUST their location, and not the restaurant BUILDINGs, we extract osm_points.\n\n# Again, run these commands in your Console\ndat_R &lt;-\n  osmdata::opq(bbox = bbox_2) %&gt;% \n  osmdata::add_osm_feature(key = \"amenity\", \n                           value = c(\"restaurant\")) %&gt;% \n  osmdata_sf() %&gt;% \n  purrr::pluck(\"osm_points\") \n\n# Save the data for future use\nwrite_sf(dat_R, dsn = \"restaurants.gpkg\",append = FALSE, quiet = FALSE)\n\nNow reading the saved Restaurant Data\n\nrestaurants &lt;- st_read(\"./restaurants.gpkg\")\n\nReading layer `restaurants' from data source \n  `/Users/arvindv/Documents/RWork/MyWebsites/my-quarto-website/content/labs/r-labs/maps/restaurants.gpkg' \n  using driver `GPKG'\nSimple feature collection with 225 features and 44 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 77.56373 ymin: 12.91003 xmax: 77.60104 ymax: 12.94917\nGeodetic CRS:  WGS 84\n\n\nHow many restaurants have we got?\n\nrestaurants %&gt;% nrow()\n\n[1] 225\n\n\nSo the restaurants dataset has 225 restaurants and their geometry is naturally a POINT type of geometry column.\nThese are the names of columns in the Restaurant Data: Note the cuisine column.\n\nglimpse(restaurants)\n\nRows: 225\nColumns: 45\n$ osm_id             &lt;chr&gt; \"595408703\", \"595409635\", \"595409636\", \"595409790\",…\n$ name               &lt;chr&gt; \"Ganesh Darshan\", \"Upahara Darshini\", \"Nagarjuna Ch…\n$ addr.city          &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ addr.country       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ addr.district      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ addr.floor         &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, \"0\", NA, NA, NA…\n$ addr.full          &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ addr.housename     &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ addr.housenumber   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, \"19/2\", NA, NA, NA, NA,…\n$ addr.postcode      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ addr.street        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, \"South End Main Road\", …\n$ addr.suburb        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ alt_name           &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, \"Upahara Darshini\", NA,…\n$ amenity            &lt;chr&gt; \"restaurant\", \"restaurant\", \"restaurant\", \"restaura…\n$ brand              &lt;chr&gt; NA, NA, NA, NA, \"Pizza Hut\", NA, NA, NA, NA, NA, NA…\n$ brand.wikidata     &lt;chr&gt; NA, NA, NA, NA, \"Q191615\", NA, NA, NA, NA, NA, NA, …\n$ building           &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ capacity           &lt;chr&gt; NA, \"150\", NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ check_date         &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, \"2023-11-29\", N…\n$ cuisine            &lt;chr&gt; NA, NA, \"indian\", \"italian\", \"pizza\", NA, \"indian\",…\n$ delivery           &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ description        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ diet.vegetarian    &lt;chr&gt; NA, \"only\", NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ email              &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ internet_access    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ level              &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, \"0\", NA, NA, NA…\n$ name.en            &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ name.kn            &lt;chr&gt; \"ಗಣೇಶ ದರ್ಶಿನಿ\", \"ಉಪಹಾರ ದರ್ಶಿನಿ\", \"ನಾಗಾರ್ಜುನ ಚಿಮಿನಿ\", \"ಲಾ ಕಾಸಾ…\n$ note               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ opening_hours      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ operator           &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ outdoor_seating    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ payment.mastercard &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ payment.visa       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ phone              &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ smoking            &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ source             &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ survey.date        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ takeaway           &lt;chr&gt; NA, \"yes\", NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ toilets.wheelchair &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"no…\n$ website            &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ wheelchair         &lt;chr&gt; NA, NA, NA, NA, NA, \"no\", NA, NA, NA, NA, NA, \"no\",…\n$ wikidata           &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ wikipedia          &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ geom               &lt;POINT [°]&gt; POINT (77.58403 12.93092), POINT (77.58442 12…\n\n\nSo let us plot the restaurants as POINTs using the restaurants data we have downloaded. The cuisine attribute looks interesting; let us colour the POINT based on the cuisine offered at that restaurant.\nSo Let’s look therefore at the cuisine column!\n\n# ( I want pizza...)\nrestaurants$cuisine %&gt;% unique()\n\n [1] NA                                             \n [2] \"indian\"                                       \n [3] \"italian\"                                      \n [4] \"pizza\"                                        \n [5] \"regional\"                                     \n [6] \"ice_cream\"                                    \n [7] \"chinese\"                                      \n [8] \"american\"                                     \n [9] \"South_Indian\"                                 \n[10] \"Multi-cuisne\"                                 \n[11] \"South_India\"                                  \n[12] \"chicken;regional\"                             \n[13] \"arab\"                                         \n[14] \"indian;seafood;fine_dining\"                   \n[15] \"fast_food\"                                    \n[16] \"kebab;grill\"                                  \n[17] \"chicken\"                                      \n[18] \"chinese;sandwich;tea;indian;coffee_shop\"      \n[19] \"indian,_japanese\"                             \n[20] \"indian;regional\"                              \n[21] \"asian\"                                        \n[22] \"indian;seafood\"                               \n[23] \"south_indian\"                                 \n[24] \"indian;chinese;tibetan;regional;kebab;chicken\"\n\n\nBig mess…many NAs, some double entries, separated by commas and semicolons….\n\n\nThe cuisine attribute:\n\n\nNote: The cuisine variable has more than one entry for a given restaurant. We use tidyr::separate() to make multiple columns out of the cuisine column and retain the first one only. Since the entries are badly entered using both “;” and “,” we need to do this twice ;-() Bad Data entry!!\n\n\nLet’s get one cuisine entry per restaurant, and drop off the ones that do not mention a cuisine at all:\n\nrestaurants &lt;- restaurants %&gt;% \n  drop_na(cuisine) %&gt;% # Knock off nondescript restaurants\n  \n  # Some have more than one classification ;-()\n  # Separated by semicolon or comma, so....\n  separate_wider_delim(cols = cuisine, \n                       names = c(\"cuisine\", NA, NA), \n                       delim = \";\", \n                       too_few = \"align_start\",\n                       too_many = \"drop\") %&gt;% \n  separate_wider_delim(cols = cuisine, \n                       names = c(\"cuisine\", NA, NA), \n                       delim = \",\",\n                       too_few = \"align_start\",\n                       too_many = \"drop\")\n\n# Finally good food?\nrestaurants$cuisine\n\n  [1] \"indian\"       \"italian\"      \"pizza\"        \"indian\"       \"regional\"    \n  [6] \"indian\"       \"pizza\"        \"regional\"     \"ice_cream\"    \"ice_cream\"   \n [11] \"indian\"       \"chinese\"      \"chinese\"      \"indian\"       \"italian\"     \n [16] \"regional\"     \"indian\"       \"indian\"       \"italian\"      \"regional\"    \n [21] \"indian\"       \"indian\"       \"american\"     \"indian\"       \"indian\"      \n [26] \"indian\"       \"ice_cream\"    \"pizza\"        \"South_Indian\" \"regional\"    \n [31] \"regional\"     \"regional\"     \"Multi-cuisne\" \"South_India\"  \"indian\"      \n [36] \"indian\"       \"chicken\"      \"arab\"         \"indian\"       \"regional\"    \n [41] \"regional\"     \"regional\"     \"regional\"     \"regional\"     \"indian\"      \n [46] \"indian\"       \"indian\"       \"indian\"       \"regional\"     \"regional\"    \n [51] \"italian\"      \"regional\"     \"regional\"     \"regional\"     \"regional\"    \n [56] \"regional\"     \"regional\"     \"regional\"     \"regional\"     \"regional\"    \n [61] \"regional\"     \"regional\"     \"fast_food\"    \"indian\"       \"regional\"    \n [66] \"italian\"      \"regional\"     \"regional\"     \"regional\"     \"regional\"    \n [71] \"regional\"     \"regional\"     \"italian\"      \"fast_food\"    \"regional\"    \n [76] \"fast_food\"    \"regional\"     \"regional\"     \"regional\"     \"regional\"    \n [81] \"regional\"     \"regional\"     \"regional\"     \"regional\"     \"regional\"    \n [86] \"regional\"     \"regional\"     \"regional\"     \"regional\"     \"regional\"    \n [91] \"regional\"     \"regional\"     \"regional\"     \"regional\"     \"regional\"    \n [96] \"regional\"     \"regional\"     \"kebab\"        \"chicken\"      \"chinese\"     \n[101] \"indian\"       \"italian\"      \"indian\"       \"indian\"       \"indian\"      \n[106] \"asian\"        \"regional\"     \"indian\"       \"regional\"     \"indian\"      \n[111] \"south_indian\" \"indian\"       \"indian\"      \n\n\nLooks clean! Each entry is only ONE and not multiple any more. Now let’s plot the Restaurants as POINTs:\n\n# http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf\n# \nggplot() +\n  geom_sf(data = buildings, colour = \"burlywood1\") +\n  geom_sf(data = roads, colour = \"gray80\") +\n  geom_sf(\n    data = restaurants %&gt;% drop_na(cuisine),\n    aes(fill = cuisine, geometry = geom),\n    colour = \"black\",\n    shape = 21,\n    size = 3\n  ) +  \n  # Set plot limits to exactly the bbox_2\n  coord_sf(xlim = c(bbox_2[1,1], bbox_2[1,2]),\n           ylim = c(bbox_2[2,1], bbox_2[2,2]),\n           expand = FALSE) + \n  theme_minimal() + \n  theme(legend.position = \"right\") +\n  labs(title = \"Restaurants in South Central Bangalore\",\n       caption = \"Based on osmdata\")\n\n\n\n\n\n\n\nWe could have done a (much!) better job, by combining cuisines into simpler and fewer categories, ( South_India and South_Indian ), but that is for another day!!\nBy now we know that we can use geom_sf() multiple number of times with different datasets to create layered maps in R."
  },
  {
    "objectID": "content/labs/r-labs/maps/gram-maps.html#globejs-usage",
    "href": "content/labs/r-labs/maps/gram-maps.html#globejs-usage",
    "title": "The Grammar of Maps",
    "section": "\nglobejs usage",
    "text": "globejs usage\nThe globejs command from the package threejs allows one to plot points, arcs and images on a globe in 3D. The globe can be rotated and and zoomed. Great Circles and historical routes are a good idea for this perhaps.\nRefer to this page for more ideas http://bwlewis.github.io/rthreejs/globejs.html\nWe will generate some random locations and plot them on the 3D globe.\n\n# Random Lats and Longs\nlat &lt;- rpois(10, 60) + rnorm(10, 80)\nlong &lt;- rpois(10, 60) + rnorm(10, 10)\n\n# Random \"Spike\" heights for each location. Population? Tourists? GDP?\nvalue &lt;- rpois(10, lambda = 80)\n \nglobejs(lat = lat, long = long)\n\n\n\n\n\nAs seen, “spikes” are created at the random lat-lon locations. We can control the height/width/colour of the spikes, as well as the initial view of the globe itself: zoom, location and so on\n\nglobejs(\n  lat = lat,\n  long = long,\n  \n  # random heights of the Spikes (!!) at lat-long combo\n  value = value,\n  color = \"red\",\n  # Zoom factor, default is 35\n  fov = 50\n)\n\n\n\n\n\n\nglobejs(\n  lat = lat,\n  long = long,\n  value = value,\n  color = \"red\",\n  pointsize = 4, # width of the columns\n  # Zoom position\n  fov = 35,\n  # initial position of the globe\n  rotationlat = 0.6, #  in RADIANS !!! Good Heavens!!\n  rotationlong = 0.2 #  in RADIANS !!! Good Heavens!!\n)\n\n\n\n\n\n\nglobejs(\n  lat = lat,\n  long = long,\n  value = value,\n  color = \"red\",\n  pointsize = 4,\n  fov = 35,\n  rotationlat = 0.6,\n  rotationlong = 0.2,\n  lightcolor = \"#aaeeff\",\n  emissive = \"#0000ee\",\n  bodycolor = \"#ffffff\",\n  bg = \"grey\"\n)"
  },
  {
    "objectID": "content/labs/doe/index.html",
    "href": "content/labs/doe/index.html",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "",
    "text": "This project is a quick analysis of the Design of Experiments class carried out in the Order and Chaos course, FSP-2021-2022, at SMI MAHE, Bangalore.\nThe methodology followed was that in A.J. Lawrance’s paper 1 describing a Statistics module based on the method of Design of Experiments. The inquiry relates to Short Term Memory (STM) among students."
  },
  {
    "objectID": "content/labs/doe/index.html#introduction",
    "href": "content/labs/doe/index.html#introduction",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "",
    "text": "This project is a quick analysis of the Design of Experiments class carried out in the Order and Chaos course, FSP-2021-2022, at SMI MAHE, Bangalore.\nThe methodology followed was that in A.J. Lawrance’s paper 1 describing a Statistics module based on the method of Design of Experiments. The inquiry relates to Short Term Memory (STM) among students."
  },
  {
    "objectID": "content/labs/doe/index.html#structure",
    "href": "content/labs/doe/index.html#structure",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "Structure",
    "text": "Structure\nThe total number of students were 17. Eight Pairs of students were created randomly to create eight different Test tools for Short Term Memory testing.\nThe binary ( two - level ) variables/parameters that were used in the tests, were, following Lawrance:\n\nWL: Word List Length ( 7 and 15 words )\n\nSL: Syllables in the Words ( 2 and 5 syllables )\n\nST: Study Time allowed for the Respondents ( 15 and 30 seconds )\n\nOther parameters considered were a) Language b) Structure/Depiction of the Word Lists ( e.g. word clouds, matrices, columns…), c) Whether the words would be shown or read aloud, and d) whether the respondents had to speak out, or write down, the recollected words. These parameters were discussed and abandoned as too complex to mechanize, though they could have had an impact on the STM scores.\nHence a total of 8 Tests were created by 8 pairs of students, and each team tested the remaining 15 students ( Due to COVID restrictions, this testing was carried out entirely online on MS Teams, using individual breakout rooms for the Test Teams. )\nThe data were entered into a Google Sheet and the STM scores were converted to percentages so as to be comparable across WL.\nThe data was then “flattened” for each of the binary parameters; this was logical to do since for each parameter, the other two parameters were balanced out by the Test structure. For instance, for WL = 5, the SL and ST parameters used all the four combinations ( SL = 5, 15 ) and (ST = 15, 30 ). Hence the “common sense” analysis could proceed for each of the parameters individually. Joint effects were not considered for this preliminary class."
  },
  {
    "objectID": "content/labs/doe/index.html#data",
    "href": "content/labs/doe/index.html#data",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "Data",
    "text": "Data\n\nShow the Codestm &lt;- readxl::read_xlsx(\"RandomTesters.xlsx\",\n                         sheet = \"Data\",\n                         range = \"C31:H91\") %&gt;% \n  janitor::clean_names()\nstm\n\n\n  \n\n\n\nThe data has scores that have been combined into single columns for each setting for each of the parameters. For example, the column syllable_2 contains STM scores for all tests that used SL = 2-syllables in their tests. The Word Length WL and Study Time ST go through all their combinations in this column. The other columns are constructed similarly.\nBasic Plots\nWe will use Box Plots and Density Plots to compare the STM score distributions for each Parameter. To do this we need to pivot_longer the adjacent columns ( e.g. syllable_2 and syllable_5) and use these names as categorical variables:\nSyllable Parameter SL\n\nShow the Codestm_syllable &lt;- stm %&gt;%\n  select(contains(\"syllable\")) %&gt;%\n  pivot_longer(\n    data = .,\n    cols = starts_with(\"syllable\"),\n    names_to = \"syllable\",\n    values_to = \"syl_scores\"\n  )\nstm_syllable\n\n\n  \n\n\nShow the Codep1 &lt;- stm_syllable %&gt;%\n  ggplot(.) + geom_boxplot(aes(\n    y = syl_scores,\n    x = syllable,\n    colour = syllable,\n    fill = syllable\n  ),\n  alpha = 0.3) +\n  labs(title = \"STM scores by Syllables in Test Word Lists\")\n\np2 &lt;- stm_syllable %&gt;%\n  ggplot(.) + geom_density(aes(x = syl_scores, colour = syllable, fill = syllable), alpha = 0.3) +\n  labs(title = \"STM scores by Syllables in Test Word Lists\")\n\npatchwork::wrap_plots(p1 + p2, guides = \"collect\")\n\n\n\n\n\n\n\nStudy Time Parameter ST\n\nShow the Codestm_studytime &lt;- stm %&gt;%\n  select(contains(\"study\")) %&gt;%\n  pivot_longer(\n    data = .,\n    cols = starts_with(\"study\"),\n    names_to = \"study\",\n    values_to = \"study_scores\"\n  )\nstm_studytime\n\n\n  \n\n\nShow the Codep1 &lt;- stm_studytime %&gt;%\n  ggplot(.) + geom_boxplot(aes(\n    y = study_scores,\n    x = study,\n    colour = study,\n    fill = study\n  ),\n  alpha = 0.3,\n  ) +\n  labs(title = \"STM scores by Study Time in Test Word Lists\")\np2 &lt;- stm_studytime %&gt;%\n  ggplot(.) + geom_density(aes(x = study_scores, colour = study, fill = study), alpha = 0.3) +\n  labs(title = \"STM scores by Study Time in Test Word Lists\")\n\npatchwork::wrap_plots(p1 + p2, guides = \"collect\")\n\n\n\n\n\n\n\nWord List Length Parameter WL\n\nShow the Codestm_words &lt;- stm %&gt;% \n  select(contains(\"list\")) %&gt;% \n  pivot_longer(data = ., cols = starts_with(\"list\"), \n               names_to = \"list\", values_to = \"list_scores\")\nstm_words\n\n\n  \n\n\nShow the Codep1 &lt;- stm_words %&gt;%\n  ggplot(.) + geom_boxplot(aes(y = list_scores, x = list, colour = list, fill = list), alpha = 0.3) +\n  labs(title = \"STM scores by Word Count in Test Word Lists\")\n\np2 &lt;- stm_words %&gt;%\n  ggplot(.) + geom_density(aes(x = list_scores, colour = list, fill = list), alpha = 0.3) +\n  labs(title = \"STM scores by Study Time in Test Word Lists\")\n\npatchwork::wrap_plots(p1 + p2,guides = \"collect\")"
  },
  {
    "objectID": "content/labs/doe/index.html#preliminary-observations",
    "href": "content/labs/doe/index.html#preliminary-observations",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "Preliminary Observations",
    "text": "Preliminary Observations\nClearly, based on visual inspection of the Plots, the Word Count seems to have a large effect on STM Test Scores, with fewer words ( 7 ) being easier to recall. Study Time ( 15 and 30 seconds ) also seems to have a more modest positive effect on STM scores, while Syllable Count ( 2 or 5 syllables ) seems to have a modest negative effect on STM scores."
  },
  {
    "objectID": "content/labs/doe/index.html#analysis",
    "href": "content/labs/doe/index.html#analysis",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "Analysis",
    "text": "Analysis\nWe wish to establish the significance of the effect size due to each of the Parameters. Already from the Density Plots, we can see that none of the scores are normally distributed. A quick Shapiro-Wilkes Test for each of them confirms that the scores are not normally distributed.\nHence we go for a Permutation Test to check for significance of effect.\nOn the other hand, as remarked in Ernst2, the non-parametric permutation test can be both exact and also intuitively easier for students to grasp, as I can testify from direct observation in this class. There is no need to discuss sampling distributions and means, t-tests and the like. Permutations are easily executed in R, using packages such as mosaic3.\n\nShow the Codeshapiro.test(stm$syllable_2)\n\n\n    Shapiro-Wilk normality test\n\ndata:  stm$syllable_2\nW = 0.95508, p-value = 0.02716\n\nShow the Codeshapiro.test(stm$syllable_5)\n\n\n    Shapiro-Wilk normality test\n\ndata:  stm$syllable_5\nW = 0.95321, p-value = 0.02211\n\nShow the Codeshapiro.test(stm$study_time_15)\n\n\n    Shapiro-Wilk normality test\n\ndata:  stm$study_time_15\nW = 0.9068, p-value = 0.0002348\n\nShow the Codeshapiro.test(stm$study_time_30)\n\n\n    Shapiro-Wilk normality test\n\ndata:  stm$study_time_30\nW = 0.95539, p-value = 0.0281\n\nShow the Codeshapiro.test(stm$list_length_7)\n\n\n    Shapiro-Wilk normality test\n\ndata:  stm$list_length_7\nW = 0.90542, p-value = 0.0002085\n\nShow the Codeshapiro.test(stm$list_length_15)\n\n\n    Shapiro-Wilk normality test\n\ndata:  stm$list_length_15\nW = 0.92806, p-value = 0.001645"
  },
  {
    "objectID": "content/labs/doe/index.html#permutation-tests",
    "href": "content/labs/doe/index.html#permutation-tests",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "Permutation Tests",
    "text": "Permutation Tests\nWe proceed with a Permutation Test for each of the Parameters. We start with the Syllable Parameter SL. We shuffle the labels ( SL- = 2 and SL+ = 5) between the scores and determine the null distribution. This is then compared with the difference in mean scores between the unpermuted sets. We continue similarly for the other two parameters.\n\nShow the Code#Syllable Parameter SL\n\nobs_syl_mean &lt;- mean(stm$syllable_2) - mean(stm$syllable_5)\nobs_syl_mean\n\n[1] 0.0153731\n\nShow the Codenull_dist_syllable &lt;-\n  do(10000) * diff(mean(\n    stm_syllable$syl_scores ~ shuffle(stm_syllable$syllable),\n    data = stm_syllable\n  ))\nhead(null_dist_syllable)\n\n\n  \n\n\nShow the Codep1 &lt;-\n  null_dist_syllable %&gt;% \n  ggplot(., aes(x = syllable_5)) + \n  geom_histogram(aes(fill = syllable_5 &gt;= obs_syl_mean)) +\n  labs(x = \"Distribution of Diff in Means under null hypothesis for Syllables\")\n\n\n# Study Time Parameter ST\nobs_study_mean &lt;- mean(stm$study_time_30) - mean(stm$study_time_15)\nobs_study_mean\n\n[1] 0.08526183\n\nShow the Codenull_dist_studytime &lt;-\n  do(10000) * diff(mean(\n    stm_studytime$study_scores ~ shuffle(stm_studytime$study),\n    data = stm_studytime\n  ))\nhead(null_dist_studytime)\n\n\n  \n\n\nShow the Codep2 &lt;- null_dist_studytime %&gt;%\n  ggplot(., aes(x = study_time_30)) + \n  geom_histogram(aes(fill = study_time_30 &gt;= obs_study_mean)) +\n  labs(x = \"Distribution of Diff in Means under null hypothesis for Study Time\")\n\n# Word List Length Parameter WL\nobs_word_mean &lt;- mean(stm$list_length_7) - mean(stm$list_length_15)\nobs_word_mean\n\n[1] 0.2887539\n\nShow the Codenull_dist_words &lt;-\n  do(10000) * diff(mean(stm_words$list_scores ~ shuffle(stm_words$list), data = stm_words))\nhead(null_dist_words)\n\n\n  \n\n\nShow the Codep3 &lt;-\n  null_dist_words %&gt;% \n  ggplot(., aes(x = list_length_7)) + \n  geom_histogram(aes(fill = list_length_7 &gt;= obs_word_mean)) +\n  labs(x = \"Distribution of Diff in Means under null hypothesis for Words\")\n\n\n\nShow the Code# patchwork::wrap_plots(p1 + p2 + p3, nrow= 3, guides = \"auto\")\np1\n\n\n\n\n\n\nShow the Codep2\n\n\n\n\n\n\nShow the Codep3"
  },
  {
    "objectID": "content/labs/doe/index.html#conclusions",
    "href": "content/labs/doe/index.html#conclusions",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "Conclusions",
    "text": "Conclusions\nFrom the above null distribution plots obtained using Permutation tests, it is clear that both Study Time ( ST ) and List Word Length ( WL) have significant effects on the Short Term Memory Scores. The probability that the observed value is obtained or exceeded by any permutation of scores is very low in both cases.\nOn the other hand, Syllable Count (SL) does not seem to affect the STM scores significantly."
  },
  {
    "objectID": "content/labs/doe/index.html#references",
    "href": "content/labs/doe/index.html#references",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "content/labs/doe/index.html#footnotes",
    "href": "content/labs/doe/index.html#footnotes",
    "title": "Using Permutation Tests in Undergraduate Stats Class",
    "section": "Footnotes",
    "text": "Footnotes\n\nLawrance, A. J. 1996. “A Design of Experiments Workshop as an Introduction to Statistics.” American Statistician 50 (2): 156–58. doi:10.1080/00031305.1996.10474364.↩︎\nErnst, Michael D. 2004. “Permutation Methods: A Basis for Exact Inference.” Statistical Science 19 (4): 676–85. doi:10.1214/088342304000000396.↩︎\nPruim R, Kaplan DT, Horton NJ (2017). “The mosaic Package: Helping Students to ‘Think with Data’ Using R.” The R Journal, 9(1), 77–102. https://journal.r-project.org/archive/2017/RJ-2017-024/index.html.↩︎"
  },
  {
    "objectID": "content/labs/r-labs/maps/PlayingwithLeaflet.html",
    "href": "content/labs/r-labs/maps/PlayingwithLeaflet.html",
    "title": "Playing with Leaflet",
    "section": "",
    "text": "This Tutorial works through the ideas at Leaflet\n\nLeaflet is a JavaScript library for creating dynamic maps that support panning and zooming along with various annotations like markers, polygons, and popups.\n\nIn this tutorial we will work only with vector data. In a second part, we will work with raster data in leaflet.\n\nlibrary(tidyverse)\nlibrary(leaflet)\n\n# Data\nlibrary(osmdata) # Import OSM Vector Data into R\n\n# devtools::install_github(\"datadotworld/data.world-r\", build_vignettes = TRUE)\nlibrary(data.world)"
  },
  {
    "objectID": "content/labs/r-labs/maps/PlayingwithLeaflet.html#add-shapes-to-a-map",
    "href": "content/labs/r-labs/maps/PlayingwithLeaflet.html#add-shapes-to-a-map",
    "title": "Playing with Leaflet",
    "section": "Add Shapes to a Map",
    "text": "Add Shapes to a Map\nleaflet offers several commands to add points, markers, icons, lines, polylines and polygons to a map. Let us examine a few of these.\nAdd Markers with popups\n\nm %&gt;% addMarkers(lng = 77.580643, lat = 12.972442, \n                 popup = \"The birthplace of Rvind\")\n\n\n\n\n# Click on the Marker for the popup to appear\n\nThis uses the default pin shape as the Marker.\nAdding Popups to a Map\nPopups are small boxes containing arbitrary HTML, that point to a specific point on the map. Use the addPopups() function to add standalone popup to the map.\n\nm %&gt;%\n  addPopups(\n    lng = 77.580643,\n    lat = 12.972442,\n    popup = paste(\n      \"The birthplace of Rvind\",\n      \"&lt;br&gt;\",\n      \"Website: https://the-foundation-series.netlify.app\",\n      \"&lt;br&gt;\"\n    ),\n    \n    # Ensuring we cannot close the popup, else we will not be able to find where it is, since there is no Marker\n    options = popupOptions(closeButton = FALSE)\n  )\n\n\n\n\n\nPopups are usually added to icons, Markers and other shapes can show up when these are clicked.\nAdding Labels to a Map\nLabels are messages attached to all shapes, using the argument label wherever it is available.\nLabels are static, and Popups are usually visible on mouse click. Hence a Marker can have both a label and a popup. For example, the function addPopup() offers only a popup argument, whereas the function addMarkers() offers both a popup and a label argument.\nIt is also possible to create labels standalone using addLabelOnlyMarkers() where we can show only text and no Markers.\n\nm %&gt;%\n  addMarkers(\n    lng = 77.580643,\n    lat = 12.972442,\n    \n    # Here is the Label defn.\n    label = \"The birthplace of Rvind\",\n    labelOptions = labelOptions(noHide = TRUE, # Label always visible\n                                textOnly = F, \n                                textsize = 20),\n    \n    # And here is the popup defn.\n    popup = \"This is the Popup Text\"\n  )\n\n\n\n\n\nAdding Circles and CircleMarkers on a Map\nWe can add shapes on to a map to depict areas or locations of interest. NOTE: the radius argument works differently in addCircles() and addCircleMarkers().\n\n# Some Cities in the US and their location\nmd_cities &lt;- tibble(\n  name = c(\"Baltimore\",\"Frederick\",\"Rockville\",\"Gaithersburg\",\"Bowie\",\"Hagerstown\",\"Annapolis\",\"College Park\",\"Salisbury\",\"Laurel\"),\n  pop = c(619493,66169,62334,61045,55232,39890,38880,30587,30484,25346),\n  lat = c(39.2920592,39.4143921,39.0840,39.1434,39.0068,39.6418,38.9784,38.9897,38.3607,39.0993), \n  lng = c(-76.6077852,-77.4204875,-77.1528,-77.2014,-76.7791,-77.7200,-76.4922,-76.9378,-75.5994,-76.8483)\n)\n\n\nmd_cities %&gt;%\n  leaflet() %&gt;%\n  addTiles() %&gt;%\n  \n  # CircleMarkers, in blue\n  # radius scales the Marker. Units are in Pixels!!\n  # Here, radius is made proportional to `pop` number\n  addCircleMarkers(radius = ~ pop/1000, # Pixels!!\n                   color = \"blue\",\n                   stroke = FALSE, # no border for the Markers\n                   opacity = 0.8) %&gt;% \n  \n  \n  # Circles, in red\n  addCircles(\n    radius = 5000, # Meters !!!\n    stroke = TRUE,\n    color = \"yellow\", # Stroke Colour\n    weight = 3, # Stroke Weight\n    fill = TRUE,\n    fillColor = \"red\",\n\n  )\n\n\n\n\n\nThe shapes need not be of fixed size or colour; their attributes can be made to correspond to other attribute variables in the geospatial data, as we did with radius in the addCircleMarkers() function above.\nAdding Rectangles to a Map\n\n## Adding Rectangles\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  setView(lng = 77.580643, lat = 12.972442, zoom = 6) %&gt;% \n  addRectangles(lat1 = 10.3858, lng1 = 75.0595, \n                lat2 = 12.8890, lng2 = 77.9625)\n\n\n\n\n\nAdd Polygons to a Map\n\n## Adding Polygons\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  setView(lng = 77.580643, lat = 12.972442, zoom = 6) %&gt;% \n  \n  # arbitrary vector data for lat and lng\n   addPolygons(lng = c(73.5, 75.9, 76.1, 77.23, 79.8),\n               lat =c(10.12, 11.04, 11.87, 12.04, 10.7))\n\n\n\n\n\nAdd PolyLines to a Map\nThis can be useful say for manually marking a route on a map, with waypoints.\n\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  setView(lng = 77.580643, lat = 12.972442, zoom = 6) %&gt;% \n  \n  # arbitrary vector data for lat and lng\n  # If start and end points are the same, it looks like Polygon\n  # Without the fill\n   addPolylines(lng = c(73.5, 75.9, 76.1, 77.23, 79.8),\n               lat =c(10.12, 11.04, 11.87, 12.04, 10.7)) %&gt;% \n  \n  # Add Waypoint Icons\n  addMarkers(lng = c(73.5, 75.9, 76.1, 77.23, 79.8),\n               lat =c(10.12, 11.04, 11.87, 12.04, 10.7))\n\n\n\n\n\nAs seen, we have created Markers, Labels, Polygons, and PolyLines using fixed.i.e. literal text and numbers. In the following we will also see how external geospatial data columns can be used instead of these literals.\nNOTE: The mapedit package https://r-spatial.org//r/2017/01/30/mapedit_intro.html can also be used to interactively add shapes onto a map and save as an geo-spatial object."
  },
  {
    "objectID": "content/labs/r-labs/maps/PlayingwithLeaflet.html#point-data-sources-for-leaflet",
    "href": "content/labs/r-labs/maps/PlayingwithLeaflet.html#point-data-sources-for-leaflet",
    "title": "Playing with Leaflet",
    "section": "Point Data Sources for leaflet\n",
    "text": "Point Data Sources for leaflet\n\nPoint data for markers can come from a variety of sources:\n\n\nSpatialPoints or SpatialPointsDataFrame objects (from the sp package)\n\n\nPOINT, sfc_POINT, and sf objects (from the sf package); only X and Y dimensions will be considered\n\nTwo-column numeric matrices (first column is longitude, second is latitude)\n\n\nData frame/tibble with latitude and longitude columns. You can explicitly tell the marker function which columns contain the coordinate data (e.g. addMarkers(lng = ~Longitude, lat = ~Latitude)), or let the function look for columns named lat/latitude and lon/lng/long/longitude (case insensitive).\n\nSimply provide numeric vectors as lng and lat arguments, which we have covered already in the preceding sections.\n\nNote that MULTIPOINT objects from sf are not supported at this time.\nWe will not consider the use of sp related data structures for plotting POINTs in leaflet since sp is being phased out in favour of the more modern package sf.\nPoints using simple Data Frames\nLet us read in the data set from data.world that gives us POINT locations of all airports in India in a data frame / tibble. The dataset is available at https://query.data.world/s/ahtyvnm2ybylf65syp4rsb5tulxe6a. You can either download it, save a copy, and read it in as usual, or use the URL itself to read it in from the web. In the latter case, you will need the package data.world and also need to register your credentials for that page with RStudio. The (simple!) instructions are available here at data.world.\n\n#library(devtools)\n# devtools::install_github(\"datadotworld/data.world-r\", build_vignettes = TRUE)\n\nlibrary(data.world)\n\nindia_airports &lt;-\n  read_csv(\"https://query.data.world/s/ahtyvnm2ybylf65syp4rsb5tulxe6a\") %&gt;% \n  slice(-1) %&gt;% # Drop the first row which contains labels\n  dplyr::mutate(\n    id = as.integer(id),\n    latitude_deg = as.numeric(latitude_deg),\n    longitude_deg = as.numeric(longitude_deg),\n    elevation_ft = as.integer(elevation_ft)\n  ) %&gt;% \n  rename(\"lon\" = longitude_deg, \"lat\" = latitude_deg) %&gt;% \n  # Remove four locations which seem to be in the African Atlantic\n  filter(!id %in% c(330834, 330867, 325010, 331083))\n\nindia_airports %&gt;% head()\n\n\n  \n\n\n\nLet us plot this in leaflet, using an ESRI National Geographic style map instead of the OSM Base Map. We will also place small circle markers for each airport.\n\nleaflet(data = india_airports) %&gt;% \n  setView(lat = 18, lng = 77, zoom = 4) %&gt;% \n  \n  # Add NatGeo style base map\n  addProviderTiles(providers$Esri.NatGeoWorldMap) %&gt;% # ESRI Basemap\n  \n  # Add Markers for each airport\n  addCircleMarkers(lng = ~lon, lat = ~lat,\n                   # Optional, variables stated for clarity\n                   # leaflet can automatically detect lon-lat columns\n                   # if they are appropriately named in the data\n                   # longitude/lon/lng\n                   # latitude/lat\n                   radius = 2, # Pixels\n                   color = \"red\",\n                   opacity = 1)\n\n\n\n\n\nWe can also change the icon for each airport. Let us try one of theseveral icon families that we can use with leaflet : glyphicons, ionicons, and fontawesome icons.\n\n# Define popup message for each airport\n# Based on data in india_airports\npopup &lt;- paste(\n  \"&lt;strong&gt;\",\n  india_airports$name,\n  \"&lt;/strong&gt;&lt;br&gt;\",\n  india_airports$iata_code,\n  \"&lt;br&gt;\",\n  india_airports$municipality,\n  \"&lt;br&gt;\",\n  \"Elevation(feet)\",\n  india_airports$elevation_ft,\n  \"&lt;br&gt;\",\n  india_airports$wikipedia_link,\n  \"&lt;br&gt;\"\n)\n\niata_icon &lt;- makeIcon(\n  \"iata-logo-transp.png\", # Downloaded from www.iata.org\n  iconWidth = 24,\n  iconHeight = 24,\n  iconAnchorX = 0,\n  iconAnchorY = 0\n)\n\n# Create the Leaflet map\nleaflet(data = india_airports) %&gt;%\n  setView(lat = 18, lng = 77, zoom = 4) %&gt;%\n  addProviderTiles(providers$Esri.NatGeoWorldMap) %&gt;%\n  addMarkers(\n    icon = iata_icon,\n    popup = popup\n  )\n\n\n\n\n\nThere are other icons we can use to mark the POINTs. leaflet allows the use of ionicons, glyphicons, and FontAwesomeIcons\nIt is possible to create a list of icons, so that different Markers can have different icons. Let us try to map the MNCs in the ITPL area of Bangalore: we use the ideas in Using Leaflet Markers @JLA-Data.net\n\n# Make a dataframe of addresses of Companies we wan to plot in ITPL\ncompanies_itpl &lt;-\n  data.frame(\n    ticker = c(\n      \"MBRDI\",\n      \"DTICI\",\n      \"IBM\",\n      \"Exxon\",\n      \"Mindtree\",\n      \"FIS Global\",\n      \"Sasken\",\n      \"LTI\"),\n    lat = c(\n      12.986178620989264,\n      12.984160906190121,\n      12.983659088566357,\n      12.985112265986636,\n      12.983794997606187,\n      12.980658616215155,\n      12.982080447350246,\n      12.981338168875348),\n    lon = c(\n      77.7270652183105,\n      77.72808445774321,\n      77.73103488768001,\n      77.72935046040699,\n      77.7227844126931,\n      77.72685064158782,\n      77.72545589289041,\n      77.72287024338216)\n  ) %&gt;% sf::st_as_sf(coords = c(\"lon\", \"lat\"), crs = 4326)\n \n# Vanilla leaflet map\nleaflet(companies_itpl) %&gt;% \n  addTiles() %&gt;% \n  addMarkers()\n\n\n\n\n\nLet us make a list of logos of the Companies and use them as markers!\n\n# a named list of rescaled icons with links to images\nfavicons &lt;- iconList(\n  \"MBRDI\" = makeIcon(\n    iconUrl = \"https://www.mercedes-benz.com/etc/designs/brandhub/frontend/static-assets/header/logo.svg\", \n    iconWidth = 25,\n    iconHeight = 25\n  ),\n  \"DTICI\" = makeIcon(\n    iconUrl = \"https://media-exp1.licdn.com/dms/image/C4D0BAQGzOep26lC03w/company-logo_200_200/0/1638298367374?e=2147483647&v=beta&t=mPyF4gvNhNFvd-tedbqNzJofq4q9qcw6A9z9jQeLAwc\",\n    iconWidth = 45,\n    iconHeight = 45\n  ),\n  \"IBM\" = makeIcon(\n    iconUrl = \"https://www.ibm.com/favicon.ico\",\n    iconWidth = 25,\n    iconHeight = 25\n  ),\n  \"Exxon\" = makeIcon(\n    iconUrl = \"https://corporate.exxonmobil.com/-/media/Global/Icons/logos/ExxonMobilLogoColor2x.png\",\n    iconWidth = 45,\n    iconHeight = 25\n  ),\n  \"Mindtree\" = makeIcon(\n    iconUrl = \"https://www.mindtree.com/themes/custom/mindtree_theme/mindtree-lnt-logo-png.png\",\n    iconWidth = 75,\n    iconHeight = 25\n  ),\n  \"FIS Global\" = makeIcon(\n    iconUrl = \"https://1000logos.net/wp-content/uploads/2021/09/FIS-Logo-768x432.png\",\n    iconWidth = 25,\n    iconHeight = 25\n  ),\n  \"Sasken\" = makeIcon(\n    iconUrl = \"https://www.sasken.com/sites/all/themes/sasken_website/logo.png\",\n    iconWidth = 35,\n    iconHeight = 35,\n  ),\n  \"LTI\" = makeIcon(\n    iconUrl = \"https://www.lntinfotech.com/wp-content/uploads/2021/09/LTI-logo.svg\",\n    iconWidth = 25,\n    iconHeight = 25\n  )\n)\n\n\n# Create the Leaflet map\n\nleaflet(companies_itpl) %&gt;% \n  addMarkers(icon = ~ favicons[ticker], # lookup based on ticker\n             label = ~ companies_itpl$ticker,\n             labelOptions = labelOptions(noHide = F,offset = c(15,-25))) %&gt;%\n  addProviderTiles(\"CartoDB.Positron\")\n\n\n\n\n\nPoints using sf objects\nWe will use data from an sf data object. This differs from the earlier situation where we had a simple data frame with lon and lat columns. In sf, the lon and lat info is embedded in the geometry column of the sf data frame.\nThe tmap package has a data set of all World metro cities, titled metro. We will plot these on the map and also scale the markers in proportion to one of the feature attributes, pop2030. The popup will be the name of the metro city. We will also use the CartoDB.Positron base map.\nNote that the metro data set has a POINT geometry, as needed!\n\ndata(metro, package = \"tmap\")\nmetro\n\n\n  \n\n\nleaflet(data = metro) %&gt;% \n  setView(lat = 18, lng = 77, zoom = 4) %&gt;% \n  \n  # Add CartoDB.Positron\n  addProviderTiles(providers$CartoDB.Positron) %&gt;% # CartoDB Basemap\n  \n  # Add Markers for each airport\n  addCircleMarkers(radius = ~ sqrt(pop2030)/350,\n                   color = \"red\",\n                   popup = paste(\"Name: \", metro$name, \"&lt;br&gt;\",\n                                  \"Population 2030: \", metro$pop2030))\n\n\n\n\n\nWe can also try downloading an sf data frame with POINT geometry from say OSM data https://www.openstreetmap.org/#map=16/12.9766/77.5888. Let us get hold of restaurants data in Malleswaram, Bangalore from OSM data:\n\nbbox&lt;- osmdata::getbb(\"Malleswaram, Bengaluru\")\nbbox\n\n       min      max\nx 77.55033 77.59033\ny 12.98274 13.02274\n\nlocations &lt;- \n  osmdata::opq(bbox = bbox) %&gt;% \n  osmdata::add_osm_feature(key = \"amenity\", value = \"restaurant\") %&gt;% \n  osmdata_sf() %&gt;% \n  purrr::pluck(\"osm_points\") %&gt;% \n  dplyr::select(name, cuisine, geometry) %&gt;% \n  dplyr::filter(cuisine == \"indian\")\n\nlocations %&gt;% head()\n\n\n  \n\n\n# Fontawesome icons seem to work in `leaflet` only up to FontAwesome V4.7.0.\n# The Fontawesome V4.7.0 Cheatsheet is here: &lt;https://fontawesome.com/v4/cheatsheet/&gt;\n\n\nleaflet(data = locations, \n        options = leafletOptions(minZoom = 12)) %&gt;% \n  \n  addProviderTiles(providers$CartoDB.Voyager) %&gt;% \n  \n  # Regular `leaflet` code\n  addAwesomeMarkers(icon = awesomeIcons(icon = \"fa-coffee\", \n                                        library = \"fa\",\n                                        markerColor = \"blue\",\n                                        iconColor = \"black\",\n                                        iconRotate = TRUE),\n                     popup = paste(\"Name: \", locations$name,\"&lt;br&gt;\",\n                           \"Food: \", locations$cuisine)) \n\n\n\n\n\nFontawesome Workaround\nFor more later versions of Fontawesome, here below is a workaround from https://github.com/rstudio/leaflet/issues/691. Despite this some fontawesome icons simply do not seem to show up. ;-() ( Update Dec 2023: Seems OK now…)\n\nlibrary(fontawesome)\ncoffee &lt;- makeAwesomeIcon(\n  text = fa(\"mug-hot\"), # mug-hot was introduced in fa version 5\n  iconColor = \"black\",\n  markerColor = \"blue\",\n  library = \"fa\"\n)\n\n\nleaflet(data = locations) %&gt;% \n  addProviderTiles(providers$CartoDB.Voyager) %&gt;% \n  \n  # Workaround code\n\n  addAwesomeMarkers(icon = coffee,\n             popup = paste(\"Name: \", locations$name,\"&lt;br&gt;\",\n                           \"Food: \", locations$cuisine, \"&lt;br&gt;\"))\n\n\n\n\n\nNote that leaflet automatically detects the lon/lat columns from within the POINT geometry column of the sf data frame.\nPoints using Two-Column Matrices\nWe can now quickly try providing lon and lat info in a two column matrix.This can be useful to plot a bunch of points recorded on a mobile phone app.\n\nmysore5 &lt;- matrix(c(runif(5, 76.652985-0.01, 76.652985+0.01),\n                 runif(5, 12.311827-0.01, 12.311827+0.01)),\n                 nrow = 5)\nmysore5\n\n         [,1]     [,2]\n[1,] 76.65634 12.30689\n[2,] 76.65082 12.30254\n[3,] 76.64549 12.31451\n[4,] 76.66062 12.31150\n[5,] 76.65134 12.32025\n\nleaflet(data = mysore5) %&gt;% \n  addProviderTiles(providers$OpenStreetMap) %&gt;% \n  \n# Pick an icon from &lt;https://www.w3schools.com/bootstrap/bootstrap_ref_comp_glyphs.asp&gt;\n  addAwesomeMarkers(icon = awesomeIcons(\n  icon = 'music',\n  iconColor = 'black',\n  library = 'glyphicon'),\n  popup = \"Carnatic Music !!\")"
  },
  {
    "objectID": "content/labs/r-labs/maps/PlayingwithLeaflet.html#polygons-lines-and-polylines-data-sources-for-leaflet",
    "href": "content/labs/r-labs/maps/PlayingwithLeaflet.html#polygons-lines-and-polylines-data-sources-for-leaflet",
    "title": "Playing with Leaflet",
    "section": "Polygons, Lines, and Polylines Data Sources for leaflet\n",
    "text": "Polygons, Lines, and Polylines Data Sources for leaflet\n\nWe have seen how to get POINT data into leaflet.\nLine and polygon data can come from a variety of sources:\n\n\nSpatialPolygons, SpatialPolygonsDataFrame, Polygons, and Polygon objects (from the sp package)\n\n\nSpatialLines, SpatialLinesDataFrame, Lines, and Line objects (from the sp package)\n\n\nMULTIPOLYGON, POLYGON, MULTILINESTRING, and LINESTRING objects (from the sf package)\n\n\nmap objects (from the maps package’s map() function); use map(fill = TRUE) for polygons, FALSE for polylines\n\nTwo-column numeric matrix; the first column is longitude and the second is latitude. Polygons are separated by rows of (NA, NA). It is not possible to represent multi-polygons nor polygons with holes using this method; use SpatialPolygons instead.\n\nWe will concentrate on using sf data into leaflet. We may explore maps() objects at a later date.\nPolygons/MultiPolygons and LineString/MultiLineString using sf data frames\nLet us download College buildings, parks, and the cycling lanes in Amsterdam, Netherlands, and plot these in leaflet.\n\nlibrary(osmdata)\n#Option 1\n# Gives too large a bbox\nbbox &lt;- osmdata::getbb(\"Amsterdam, Netherlands\")\n#bbox\n\n# Setting bbox manually is better\namsterdam_coords &lt;- matrix(c(4.85,4.95,52.325,52.375), \n                           byrow = TRUE, \n                           nrow = 2, ncol = 2, \n                           dimnames = list(c('x','y'),c('min','max')))\namsterdam_coords\n\n     min    max\nx  4.850  4.950\ny 52.325 52.375\n\ncolleges &lt;- amsterdam_coords %&gt;% \n  osmdata::opq() %&gt;% \n  osmdata::add_osm_feature(key = \"amenity\",\n                           value = \"college\") %&gt;% \n  osmdata_sf() %&gt;% \n  purrr::pluck(\"osm_polygons\")\n\nparks &lt;- amsterdam_coords %&gt;% \n  osmdata::opq() %&gt;% \n  osmdata::add_osm_feature(key = \"landuse\", value = \"grass\") %&gt;% \n  osmdata_sf() %&gt;% \n  purrr::pluck(\"osm_polygons\")\n\nroads &lt;- amsterdam_coords %&gt;% \n  osmdata::opq() %&gt;% \n  osmdata::add_osm_feature(key = \"highway\",\n                           value = \"primary\") %&gt;% \n  osmdata_sf() %&gt;% \n  purrr::pluck(\"osm_lines\")\n\ncyclelanes &lt;- amsterdam_coords %&gt;% \n  osmdata::opq() %&gt;% \n  osmdata::add_osm_feature(key = \"cycleway\")  %&gt;% \n  osmdata_sf() %&gt;% \n  purrr::pluck(\"osm_lines\")\n\nWe have 13 colleges, 2808 parks, 280 roads, and 228 cyclelanes in our data.\n\nleaflet() %&gt;% \n  addTiles() %&gt;% \n  addPolygons(data = colleges, color = \"yellow\", \n              popup = ~ colleges$name) %&gt;% \n  addPolygons(data = parks, color = \"green\", popup = parks$name) %&gt;% \n  addPolylines(data = roads, color = \"red\") %&gt;% \n  addPolylines(data = cyclelanes, color = \"purple\")"
  },
  {
    "objectID": "content/labs/r-labs/maps/PlayingwithLeaflet.html#chapter-3-using-raster-data-in-leaflet",
    "href": "content/labs/r-labs/maps/PlayingwithLeaflet.html#chapter-3-using-raster-data-in-leaflet",
    "title": "Playing with Leaflet",
    "section": "Chapter 3: Using Raster Data in leaflet\n",
    "text": "Chapter 3: Using Raster Data in leaflet\n\nSo far all the geospatial data we have plotted in leaflet has been vector data. We will now explore how to plot raster data using leaflet. Raster data are used to depict continuous variables across space, such as vegitation, salinity, forest cover etc. Satellite imagery is frequently available as raster data.\nImporting Raster Data [Work in Progress!]\nRaster data can be imported into R in many ways:\n\nusing the maptiles package\n\nusing the OpenStreetMap package\n\n\nlibrary(terra)\nlibrary(maptiles)\n#library(OpenStreetMap) # causes RStudio to crash..."
  },
  {
    "objectID": "content/labs/r-labs/maps/PlayingwithLeaflet.html#bells-and-whistles-in-leaflet-layers-groups-legends-and-graticules",
    "href": "content/labs/r-labs/maps/PlayingwithLeaflet.html#bells-and-whistles-in-leaflet-layers-groups-legends-and-graticules",
    "title": "Playing with Leaflet",
    "section": "Bells and Whistles in leaflet: layers, groups, legends, and graticules",
    "text": "Bells and Whistles in leaflet: layers, groups, legends, and graticules\nAdding Legends[Work in Progress!]\n\n## Generate some random lat lon data around Bangalore\ndf &lt;- data.frame(lat = runif(20, min = 11.97, max = 13.07),\n                 lng = runif(20, min = 77.48, max = 77.68),\n                 col = sample(c(\"red\", \"blue\", \"green\"), 20, \n                              replace = TRUE),\n                 stringsAsFactors = FALSE)\n\ndf %&gt;%\n  leaflet() %&gt;%\n  addTiles() %&gt;%\n  addCircleMarkers(color = df$col) %&gt;%\n  addLegend(values = df$col, labels = LETTERS[1:3], colors = c(\"blue\", \"red\", \"green\"))"
  },
  {
    "objectID": "content/labs/r-labs/maps/PlayingwithLeaflet.html#using-web-map-services-wms-work-in-progress",
    "href": "content/labs/r-labs/maps/PlayingwithLeaflet.html#using-web-map-services-wms-work-in-progress",
    "title": "Playing with Leaflet",
    "section": "Using Web Map Services (WMS) [Work in Progress!]",
    "text": "Using Web Map Services (WMS) [Work in Progress!]\nTo be included."
  },
  {
    "objectID": "content/labs/r-labs/time/index.html",
    "href": "content/labs/r-labs/time/index.html",
    "title": "Lab 01 - Introduce Yourself",
    "section": "",
    "text": "At the end of this Lab, we will:\n\nhave installed R and RStudio on our machines\nunderstood how to add additional R-packages for specific features and graphic capability\nrun code within RStudio and interpret the results\nhave learnt to look for help within R and RStudio"
  },
  {
    "objectID": "content/labs/r-labs/time/index.html#Check-In-R",
    "href": "content/labs/r-labs/time/index.html#Check-In-R",
    "title": "Lab 01 - Introduce Yourself",
    "section": "Check in",
    "text": "Check in\nLaunch R by clicking this logo. You should see one console with a command line interpreter. Try typing 2 + 2 and check !\nClose R."
  },
  {
    "objectID": "content/labs/r-labs/time/index.html#Check-In-RStudio",
    "href": "content/labs/r-labs/time/index.html#Check-In-RStudio",
    "title": "Lab 01 - Introduce Yourself",
    "section": "Check in",
    "text": "Check in\nLaunch RStudio. You should get a window similar to the screenshot you see here, but yours will be empty. Look at the bottom left pane: this is the same console window you saw when you opened R in step @Check-In-R\n\nPlace your cursor where you see &gt; and type x &lt;- 2 + 2 again hit enter or return, then type x, and hit enter/return again.\nIf [1] 4 prints to the screen, you have successfully installed R and RStudio, and you can move onto installing packages."
  },
  {
    "objectID": "content/labs/r-labs/time/index.html#save-and-share",
    "href": "content/labs/r-labs/time/index.html#save-and-share",
    "title": "Lab 01 - Introduce Yourself",
    "section": "Save and share",
    "text": "Save and share\nSave your work so you can share your favorite plot with us. You will not like the looks of your plot if you mouse over to Export and save it. Instead, use ggplot2’s command for saving a plot with sensible defaults:\n\nhelp(ggsave)\n\n\nggsave(\"file_name_here.pdf\", plot) # please make the filename unique!\n\nUpload this exported plot to Teams -&gt; Assignments.\nBefore you do that, check that you can submit stuff/assignments on the MLS by uploading a cat picture 🐈 first, especially if you are an ailurophile like me. (Acceptable Breeds: Scottish Folds or Maine Coons)"
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html",
    "href": "content/labs/r-labs/networks/index.html",
    "title": "The Grammar of Networks",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE, warning = FALSE)\n\n########################################\n# For General Data Manipulation\nlibrary(tidyverse)\n\n########################################\n# Network Analysis Library (Handle data and Viz)\nlibrary(igraph)\nlibrary(netrankr)\n\n########################################\n# For Network \"Manipulation\"\nlibrary(tidygraph)\n\n# For Network Visualization\nlibrary(ggraph)\nlibrary(graphlayouts)\nlibrary(visNetwork)\n\n# For \"Network\" Datasets\nlibrary(igraphdata)"
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#setting-up-r-packages",
    "href": "content/labs/r-labs/networks/index.html#setting-up-r-packages",
    "title": "The Grammar of Networks",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE, warning = FALSE)\n\n########################################\n# For General Data Manipulation\nlibrary(tidyverse)\n\n########################################\n# Network Analysis Library (Handle data and Viz)\nlibrary(igraph)\nlibrary(netrankr)\n\n########################################\n# For Network \"Manipulation\"\nlibrary(tidygraph)\n\n# For Network Visualization\nlibrary(ggraph)\nlibrary(graphlayouts)\nlibrary(visNetwork)\n\n# For \"Network\" Datasets\nlibrary(igraphdata)"
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#introduction",
    "href": "content/labs/r-labs/networks/index.html#introduction",
    "title": "The Grammar of Networks",
    "section": "\n Introduction",
    "text": "Introduction\nThis Quarto document is part of my workshop course on R . The material is based on A Layered Grammar of Graphics by Hadley Wickham. The intent of this Course is to build Skill in coding in R, and also appreciate R as a way to metaphorically visualize information of various kinds, using predominantly geometric figures and structures.\nAll Quarto document files combine code, text, web-images, and figures developed using code. Everything is text; code chunks are enclosed in fences (```)"
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#goals",
    "href": "content/labs/r-labs/networks/index.html#goals",
    "title": "The Grammar of Networks",
    "section": "Goals",
    "text": "Goals\nAt the end of this Lab session, we should:\n\nknow the types and structures of network data and be able to work with them\nunderstand the basics of modern network packages in R\nbe able to create network visualizations using tidygraph, ggraph( static visualizations ) and visNetwork (interactive visualizations)\nsee directions for how the network metaphor applies in a variety of domains (e.g. biology/ecology, ideas/influence, technology, transportation, to name a few)"
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#pedagogical-note",
    "href": "content/labs/r-labs/networks/index.html#pedagogical-note",
    "title": "The Grammar of Networks",
    "section": "Pedagogical Note",
    "text": "Pedagogical Note\nThe method followed will be based on PRIMM:\n\n\nPREDICT Inspect the code and guess at what the code might do, write predictions\n\n\nRUN the code provided and check what happens\n\nINFER what the parameters of the code do and write comments to explain. What bells and whistles can you see?\n\nMODIFY the parameters code provided to understand the options available. Write comments to show what you have aimed for and achieved.\n\nMAKE : take an idea/concept of your own, and graph it."
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#graph-metaphors",
    "href": "content/labs/r-labs/networks/index.html#graph-metaphors",
    "title": "The Grammar of Networks",
    "section": "Graph Metaphors",
    "text": "Graph Metaphors\nNetwork graphs are characterized by two key terms: nodes and edges\n\n\nNodes : Entities\n\nMetaphors: Individual People? Things? Ideas? Places? to be connected in the network.\nSynonyms: vertices. Nodes have IDs.\n\n\n\nEdges: Connections\n\nMetaphors: Interactions? Relationships? Influence? Letters sent and received? Dependence? between the entities.\nSynonyms: links, ties.\n\n\n\nIn R, we create network representations using node and edge information. One way in which these could be organized are:\n\n\nNode list: a data frame with a single column listing the node IDs found in the edge list. You can also add attribute columns to the data frame such as the names of the nodes or grouping variables. ( Type? Class? Family? Country? Subject? Race? )\n\n\nNode Table\n\n\n\n\n\n\n\nID\nNode Name\nAttribute? Qualities?Categories? Family? Country?Planet?\n\n\n1\nNed\nNursery School Teacher\n\n\n2\nJaguar Paw\nMain Character, Apocalypto\n\n\n3\nJohn Snow\nEpidemiologist\n\n\n\n\n\nEdge list: data frame containing two columns: source node and destination node of an edge. Source and Destination have node IDs.\n\nWeighted network graph: An edge list can also contain additional columns describing attributes of the edges such as a magnitude aspect for an edge. If the edges have a magnitude attribute the graph is considered weighted.\n\n\nEdges Table\n\nFrom\nTo\nRelationship\nWeightage\n\n\n\n1\n3\nFinancial Dealings\n6\n\n\n2\n1\nHistory Lessons\n2\n\n\n2\n3\nVaccination\n15\n\n\n\n\n\nLayout: A geometric arrangement of nodes and edges.\n\nMetaphors: Location? Spacing? Distance? Coordinates? Colour? Shape? Size? Provides visual insight due to the arrangement.\n\n\n\nLayout Algorithms : Method to arranges nodes and edges with the aim of optimizing some metric .\n\nMetaphors: Nodes are masses and edges are springs. The Layout algorithm minimizes the stretching and compressing of all springs.(BTW, are the Spring Constants K the same for all springs?…)\n\n\nDirected and undirected network graph: If the distinction between source and target is meaningful, the network is directed. If the distinction is not meaningful, the network is undirected. Directed edges represent an ordering of nodes, like a relationship extending from one node to another, where switching the direction would change the structure of the network. Undirected edges are simply links between nodes where order does not matter.\n\nExamples:\n - The World Wide Web is an example of a directed network because\n hyperlinks connect one Web page to another, but not necessarily \n the other way around.\n\n - Co-authorship networks represent examples of un-directed networks,\nwhere nodes are authors and they are connected by an edge if they\nhave written a publication together\n\n - When people send e-mail to each other, the distinction between the\nsender (source) and the recipient (target) is clearly meaningful,\ntherefore the network is directed.\n\n\nConnected and Disconnected graphs: If there is some path from any node to any other node, the Networks is said to be Connected. Else, Disconnected."
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#predictruninfer--1",
    "href": "content/labs/r-labs/networks/index.html#predictruninfer--1",
    "title": "The Grammar of Networks",
    "section": "Predict/Run/Infer -1",
    "text": "Predict/Run/Infer -1\nUsing tidygraph and ggraph\n\ntidygraph and ggraph are modern R packages for network data. Graph Data setup and manipulation is done in tidygraph and graph visualization with ggraph.\n\n\ntidygraph Data -&gt; “Network Object” in R.\n\nggraph Network Object -&gt; Plots using a chosen layout/algo.\n\nBoth leverage the power of igraph, which is the Big Daddy of all network packages. We will be using the Grey’s Anatomy dataset in our first foray into networks.\nStep1. Read the data\nDownload these two datasets into your current project-&gt; data folder.\n Grey’s Anatomy Nodes \n Grey’s Anatomy Nodes \n\ngrey_nodes &lt;- read_csv(\"../../../materials/data/networks/grey_nodes.csv\")\ngrey_edges &lt;- read_csv(\"../../../materials/data/networks/grey_edges.csv\")\n\ngrey_nodes\n\n\n  \n\n\ngrey_edges\n\n\n  \n\n\n\n\n\nQuestions and Inferences #1:\n\n\nLook at the console output thumbnail. What does for example name = col_character mean? What attributes (i.e. extra information) are seen for Nodes and Edges? Understand the data in both nodes and edges as shown in the second and third thumbnails. Write some comments and inferences here.\n\n\nStep 2.Create a network object using tidygraph:\nKey function:\n\n\ntbl_graph(): (aka “tibble graph”). Key arguments: nodes, edges and directed. Note this is a very versatile command and can take many input forms, such as data structures that result from other packages. Type ?tbl_graph in the Console and see the Usage section.\n\n\nga &lt;- tbl_graph(nodes = grey_nodes, \n                edges = grey_edges, \n                directed = FALSE)\nga\n\n# A tbl_graph: 54 nodes and 57 edges\n#\n# An undirected simple graph with 4 components\n#\n# Node Data: 54 × 7 (active)\n   name               sex   race  birthyear position  season sign    \n   &lt;chr&gt;              &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   \n 1 Addison Montgomery F     White      1967 Attending      1 Libra   \n 2 Adele Webber       F     Black      1949 Non-Staff      2 Leo     \n 3 Teddy Altman       F     White      1969 Attending      6 Pisces  \n 4 Amelia Shepherd    F     White      1981 Attending      7 Libra   \n 5 Arizona Robbins    F     White      1976 Attending      5 Leo     \n 6 Rebecca Pope       F     White      1975 Non-Staff      3 Gemini  \n 7 Jackson Avery      M     Black      1981 Resident       6 Leo     \n 8 Miranda Bailey     F     Black      1969 Attending      1 Virgo   \n 9 Ben Warren         M     Black      1972 Other          6 Aquarius\n10 Henry Burton       M     White      1972 Non-Staff      7 Cancer  \n# ℹ 44 more rows\n#\n# Edge Data: 57 × 4\n   from    to weight type    \n  &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt;   \n1     5    47      2 friends \n2    21    47      4 benefits\n3     5    46      1 friends \n# ℹ 54 more rows\n\n\n\n\nQuestions and Inferences #2:\n\n\nQuestions and Inferences: What information does the graph object contain? What attributes do the nodes have? What about the edges?\n\n\nStep 3. Plot using ggraph\n\n3a. Quick Plot: autograph() This is to check quickly is the data is imported properly and to decide upon going on to a more elaborate plotting.\n\nautograph(ga)\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #3:\n\n\nQuestions and Inferences: Describe this graph, in simple words here. Try to use some of the new domain words we have just acquired: nodes/edges, connected/disconnected, directed/undirected.\n\n\n3b. More elaborate plot\nKey functions:\n\n\nggraph(layout = \"......\"): Create classic node-edge diagrams; i.e. Sets up the graph. Rather like ggplot for networks!\n\nTwo kinds of geom: one set for nodes, and another for edges\n\ngeom_node_point(aes(.....)): Draws node as “points”. Alternatives are circle / arc_bar / tile / voronoi. Remember the geoms that we have seen before in Grammar of Graphics!\ngeom_edge_link(aes(.....)): Draws edges as “links”. Alternatives are arc / bend / elbow / hive / loop / parallel / diagonal / point / span /tile.\ngeom_node_text(aes(label = ......), repel = TRUE): Adds text labels (non-overlapping). Alternatives are label /...\nlabs(title = \"....\", subtitle = \"....\", caption = \"....\"): Change main titles, axis labels and legend titles. We know this from our work with ggplot.\n\n\n# Write Comments next to each line \n# About what that line does for the overall graph\n\nggraph(graph = ga,  layout = \"kk\") +\n  #\n  \n  geom_edge_link(width = 2, color = \"pink\") +\n  #\n  \n  geom_node_point(\n    shape = 21,\n    size = 8,\n    fill = \"blue\",\n    color = \"green\",\n    stroke = 2\n  ) +\n  #\n  \n  labs(title = \"Whoo Hoo! My first silly Grey's Anatomy graph in R!\",\n       subtitle = \"Why did I ever get in this course...\",\n       caption = \"Bro, they are doing **cool** things in the other\n       classes...\") \n\n\n\n\n\n\n\n\n\nQuestions and Inferences #3:\n\n\nQuestions and Inferences: What parameters have been changed here, compared to the earlier graph? Where do you see these changes in the code above?\n\n\nLet us Play with this graph and see if we can make some small changes. Colour? Fill? Width? Size? Stroke? Labs? Of course!\n\n# Change the parameters in each of the commands here to new ones\n# Use fixed values for colours or sizes...etc. \n\nggraph(graph = ga,  layout = \"kk\") + \n  geom_edge_link(width = 2) + \n  geom_node_point(shape = 21, size = 8, \n                  fill = \"blue\", \n                  color = \"green\", \n                  stroke = 2) +\n  labs(title = \"Whoo Hoo! My next silly Grey's Anatomy graph in R!\",\n       subtitle = \"Why did I ever get in this course...\",\n       caption = \"Bro, they are doing cool things in the other \n       classes...\") \n\n\n\n\n\n\n\n\n\nQuestions and Inferences #4:\n\n\nQuestions and Inferences: What did the shape parameter achieve? What are the possibilities with shape? How about including alpha?\n\n\n3c. Aesthetic Mapping from Node and Edge attribute columns\nUp to now, we have assigned specific numbers to geometric aesthetics such as shape and size. Now we are ready ( maybe ?) change the meaning and significance of the entire graph and each element within it, and use aesthetics / metaphoric mappings to achieve new meanings or insights. Let us try using aes() inside each geom to map a variable to a geometric aspect.\nDon’t try to use more than 2 aesthetic mappings simultaneously!!\nThe node elements we can tweak are:\n\nTypes of Nodes: geom_node_****()\n\nNode Parameters: inside geom_node_****(aes(...............))\n-aes(alpha  = node-variable) : opacity; a value between 0 and 1\n-aes(shape  = node-variable) : node shape\n-aes(colour = node-variable) : node colour\n-aes(fill   = node-variable) : fill colour for node\n-aes(size   = node-variable) : size of node\n\nThe edge elements we can tweak are:\n\nType of Edges” geom_edge_****()\n\nEdge Parameters: inside geom_edge_****(aes(...............))\n-aes(colour = edge-variable) : colour of the edge\n-aes(width  = edge-variable) : width of the edge\n-aes(label  = some_variable) : labels for the edge\n\nType ?geom_node_point and ?geom-edge_link in your Console for more information.\n\nggraph(graph = ga, layout = \"fr\") +\n  geom_edge_link0(aes(width = weight)) + # change variable here\n  \n  geom_node_point(aes(color = race), size = 6) + # change variable here\n\n\n  labs(title = \"Whoo Hoo! Yet another Grey's Anatomy graph in R!\") \n\n\n\n\n\n\n\n\n\nQuestions and Inferences #5:\n\n\nQuestions and Inferences: Describe some of the changes here. What types of edges worked? Which variables were you able to use for nodes and edges and how? What did not work with either of the two?"
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#predictreuseinfer-2",
    "href": "content/labs/r-labs/networks/index.html#predictreuseinfer-2",
    "title": "The Grammar of Networks",
    "section": "Predict/Reuse/Infer-2",
    "text": "Predict/Reuse/Infer-2\n\n# Arc diagram\nggraph(ga, layout = \"linear\") +\n  geom_edge_arc(aes(width = weight), alpha = 0.8) +\n  scale_edge_width(range = c(0.2, 2)) +\n  geom_node_point(size = 2, colour = \"red\") +\n  labs(edge_width = \"Weight\") \n\n\n\n\n\n\n\n\n\nQuestions and Inferences #6:\n\n\nQuestions and Inferences: How does this graph look “metaphorically” different? Do you see a difference in the relationships between people here? Why?\n\n\n\n# Coord diagram, circular\nggraph(ga, layout = \"linear\", circular = TRUE) + \n  geom_edge_arc(aes(width = weight), alpha = 0.8) + \n  scale_edge_width(range = c(0.2, 2)) +\n  geom_node_point(size = 4,colour = \"red\") + \n  geom_node_text(aes(label = name),repel = TRUE, size = 3,\n                 max.overlaps = 20) +\n  labs(edge_width = \"Weight\") +\n  theme_graph() \n\n\n\n\n\n\n\n\n\nQuestions and Inferences #7:\n\n\nQuestions and Inferences: How does this graph look “metaphorically” different? Do you see a difference in the relationships between people here? Why?"
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#hierarchical-layouts",
    "href": "content/labs/r-labs/networks/index.html#hierarchical-layouts",
    "title": "The Grammar of Networks",
    "section": "Hierarchical layouts",
    "text": "Hierarchical layouts\nThese provide for some alternative metaphorical views of networks. Note that not all layouts are possible for all datasets!!\n\nset_graph_style()\n\n# This dataset contains the graph that describes the class \n# hierarchy for the Flare visualization library.\n# Type ?flare in your Console\nhead(flare$vertices)\n\n\n  \n\n\nhead(flare$edges)\n\n\n  \n\n\n# flare class hierarchy\ngraph = tbl_graph(edges = flare$edges, nodes = flare$vertices)\n\n# dendrogram\nggraph(graph, layout = \"dendrogram\") + \n  geom_edge_diagonal() + \n  labs(title = \"Dendrogram\")\n\n\n\n\n\n\n# circular dendrogram\nggraph(graph, layout = \"dendrogram\", circular = TRUE) + \n  geom_edge_diagonal() + \n  geom_node_point(aes(filter = leaf)) + \n  coord_fixed()+ \n  labs(title = \"Circular Dendrogram\")\n\n\n\n\n\n\n# rectangular tree map\nggraph(graph, layout = \"treemap\", weight = size) + \n  geom_node_tile(aes(fill = depth), size = 0.25) + \n  labs(title = \"Rectangular Tree Map\")\n\n\n\n\n\n\n# circular tree map\nggraph(graph, layout = \"circlepack\", weight = size) + \n  geom_node_circle(aes(fill = depth), size = 0.25, n = 50) + \n  coord_fixed() + \n  labs(title = \"Circular Tree Map\")\n\n\n\n\n\n\n# icicle\nggraph(graph, layout = \"partition\") + \n  geom_node_tile(aes(y = -y, fill = depth))\n\n\n\n\n\n\n# sunburst (circular icicle)\nggraph(graph, layout = \"partition\", circular = TRUE) +\n  geom_node_arc_bar(aes(fill = depth)) +\n  coord_fixed() + \n  labs(title = \"Circular Icicle\")\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #8:\n\n\nQuestions and Inferences: How do graphs look “metaphorically” different? Do they reveal different aspects of the group? How?"
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#faceting",
    "href": "content/labs/r-labs/networks/index.html#faceting",
    "title": "The Grammar of Networks",
    "section": "Faceting",
    "text": "Faceting\nFaceting allows to create sub-plots according to the values of a qualitative attribute on nodes or edges.\n\n# setting theme_graph \nset_graph_style()\n\n\n# facet edges by type\nggraph(ga,layout = \"linear\", circular = TRUE) + \n  geom_edge_link(aes(color = type)) + \n  geom_node_point() +\n  facet_edges(~ type) +\n  theme(aspect.ratio = 1)\n\n\n\n\n\n\n# facet nodes by sex\nggraph(ga,layout = \"linear\", circular = TRUE) + \n  geom_edge_link() + \n  geom_node_point() +\n  facet_nodes(~race) +\n  theme(aspect.ratio = 1)\n\n\n\n\n\n\n# facet both nodes and edges\nggraph(ga,layout = \"linear\", circular = TRUE) + \n  geom_edge_link(aes(color = type)) + \n  geom_node_point() +\n  facet_graph(type ~ race) + \n  th_foreground(border = TRUE) +\n  theme(aspect.ratio = 1, legend.position = \"top\")\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #9:\n\n\nQuestions and Inferences: Does splitting up the main graph into subnetworks give you more insight? Describe some of these."
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#network-analysis-with-tidygraph",
    "href": "content/labs/r-labs/networks/index.html#network-analysis-with-tidygraph",
    "title": "The Grammar of Networks",
    "section": "Network analysis with tidygraph",
    "text": "Network analysis with tidygraph\nThe data frame graph representation can be easily augmented with metrics or statistics computed on the graph. Remember how we computed counts with the penguin dataset in Grammar of Graphics.\nBefore computing a metric on nodes or edges use the activate() function to activate either node or edge data frames. Use dplyr verbs (filter, arrange, mutate) to achieve your computation in the proper way.\nNetwork Centrality\nCentrality is a an “ill-defined” metric of node and edge importance in a network. It is therefore calculated in many ways. Type ?centrality in your Console.\n\n\nStandards\n\nLet’s add a few columns to the nodes and edges based on network centrality measures:\n\nga %&gt;% \n  activate(nodes) %&gt;% \n  \n  # Node with  the most connections?\n  mutate(degree = centrality_degree(mode = c(\"in\"))) %&gt;% \n  filter(degree &gt; 0) %&gt;% \n  \n  activate(edges) %&gt;% \n  # \"Busiest\" edge?\n  mutate(betweenness = centrality_edge_betweenness())\n\n# A tbl_graph: 54 nodes and 57 edges\n#\n# An undirected simple graph with 4 components\n#\n# Edge Data: 57 × 5 (active)\n    from    to weight type         betweenness\n   &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt;\n 1     5    47      2 friends             20.3\n 2    21    47      4 benefits            44.7\n 3     5    46      1 friends             39  \n 4     5    41      1 friends             66.3\n 5    18    41      6 friends             39  \n 6    21    41     12 benefits            91.5\n 7    37    41      5 professional       164. \n 8    31    41      2 professional        98.8\n 9    20    31      3 professional        47.2\n10    17    31      4 friends            102. \n# ℹ 47 more rows\n#\n# Node Data: 54 × 8\n  name               sex   race  birthyear position  season sign   degree\n  &lt;chr&gt;              &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1 Addison Montgomery F     White      1967 Attending      1 Libra       3\n2 Adele Webber       F     Black      1949 Non-Staff      2 Leo         1\n3 Teddy Altman       F     White      1969 Attending      6 Pisces      4\n# ℹ 51 more rows\n\n\nPackages tidygraph and ggraph can be pipe-lined to perform analysis and visualization tasks in one go.\n\n# setting theme_graph \nset_graph_style()\n\nga %&gt;% \n  activate(nodes) %&gt;% \n  \n  # Who has the most connections?\n  mutate(degree = centrality_degree()) %&gt;% \n  \n  activate(edges) %&gt;% \n  # Who is the go-through person?\n  mutate(betweenness = centrality_edge_betweenness()) %&gt;%\n  \n  # Now to continue with plotting\n  ggraph(layout = \"nicely\") +\n  geom_edge_link(aes(alpha = betweenness)) +\n  geom_node_point(aes(size = degree, colour = degree)) + \n  \n  # discrete colour legend\n  scale_color_gradient(guide = \"legend\")\n\n\n\n\n\n\n# or even less typing\n  ggraph(ga,layout = \"nicely\") +\n  geom_edge_link(aes(alpha = centrality_edge_betweenness())) +\n  geom_node_point(aes(colour = centrality_degree(), \n                      size = centrality_degree())) + \n  scale_color_gradient(guide = \"legend\",\n                       low = \"green\",\n                       high = \"red\") \n\n\n\n\n\n\n\n\n\nQuestions and Inferences #10:\n\n\nQuestions and Inferences: How do the Centrality Measures show up in the graph? Would you “agree” with the way we have done it? Try to modify the aesthetics by copy-pasting this chunk below and see how you can make an alternative representation.\n\n\nAnalysis and visualizing Network Communities\nWho is close to whom? Which are the groups you can see?\n\n# setting theme_graph \nset_graph_style()\n\n\n# visualize communities of nodes\nga %&gt;% \n  activate(nodes) %&gt;%\n  mutate(community = as.factor(group_louvain())) %&gt;% \n  ggraph(layout = \"graphopt\") + \n  geom_edge_link() + \n  geom_node_point(aes(color = community), size = 5) \n\n\n\n\n\n\n\n\n\nQuestions and Inferences #11:\n\n\nQuestions and Inferences: Is the Community depiction clear? How would you do it, with which aesthetic? Copy Paste this chunk below and try.\n\n\nInteractive Graphs with visNetwork\n\nExploring the VisNetwork package. Make graphs wiggle and shake using tidy commands! The package implements interactivity using the physical metaphor of weights and springs we discussed earlier.\nThe visNetwork() function uses a nodes list and edges list to create an interactive graph. The nodes list must include an “id” column, and the edge list must have “from” and “to” columns. The function also plots the labels for the nodes, using the names of the cities from the “label” column in the node list.\n\nlibrary(visNetwork)\n\n# Prepare the data for plotting by visNetwork\ngrey_nodes\n\n\n  \n\n\ngrey_edges\n\n\n  \n\n\n# Relabel greys anatomy nodes and edges for VisNetwork\ngrey_nodes_vis &lt;- grey_nodes %&gt;% \n  rowid_to_column(var = \"id\") %&gt;% \n  rename(\"label\" = name) %&gt;% \n  mutate(sex = case_when(sex == \"F\" ~ \"Female\",\n                         sex == \"M\" ~ \"Male\")) %&gt;% \n  replace_na(., list(sex = \"Transgender?\")) %&gt;% \n  rename(\"group\" = sex)\ngrey_nodes_vis\n\n\n  \n\n\ngrey_edges_vis &lt;- grey_edges %&gt;% \n  select(from, to) %&gt;% \n  left_join(., grey_nodes_vis, \n            by = c(\"from\" = \"label\")) %&gt;% \n  left_join(., grey_nodes_vis, \n            by = c(\"to\" = \"label\")) %&gt;%\n  select(\"from\"= id.x, \"to\" = id.y)\ngrey_edges_vis\n\n\n  \n\n\n\nUsing fontawesome icons\n\ngrey_nodes_vis %&gt;%\n\n  visNetwork(nodes = ., edges = grey_edges_vis) %&gt;% \n  visNodes(font = list(size = 40)) %&gt;% \n  \n  # Colour and icons for each of the gender-groups\n  visGroups(groupname = \"Female\", shape = \"icon\", \n            icon = list(code = \"f182\", size = 75, color = \"tomato\"),\n            shadow = list(enabled = TRUE)) %&gt;% \n  \n  visGroups(groupname = \"Male\", shape = \"icon\", \n            icon = list(code = \"f183\", size = 75, color = \"slateblue\"), \n            shadow = list(enabled = TRUE)) %&gt;% \n  \n  visGroups(groupname = \"Transgender?\", shape = \"icon\", \n            icon = list(code = \"f22c\", size = 75, color = \"fuchsia\"), \n            shadow = list(enabled = TRUE)) %&gt;% \n  \n  #visLegend() %&gt;%\n  #Add the fontawesome icons!!\n  addFontAwesome() %&gt;% \n  \n  # Add Interaction Controls\n  visInteraction(navigationButtons = TRUE,\n                 hover = TRUE,\n                 selectConnectedEdges = TRUE,\n                 hoverConnectedEdges = TRUE,\n                 zoomView = TRUE)\n\n\n\n\n\nThere is another family of icons available in visNetwork, called ionicons. Let’s see how they look:\n\ngrey_nodes_vis %&gt;%\n\n  visNetwork(nodes = ., edges = grey_edges_vis,) %&gt;%\n  visLayout(randomSeed = 12345) %&gt;%\n  visNodes(font = list(size = 50)) %&gt;%\n  visEdges(color = \"green\") %&gt;%\n  visGroups(\n    groupname = \"Female\",\n    shape = \"icon\",\n    icon = list(\n      face = 'Ionicons',\n      code = \"f25d\",\n      color = \"fuchsia\",\n      size = 125\n    )\n  ) %&gt;%\n  \n  visGroups(\n    groupname = \"Male\",\n    shape = \"icon\",\n    icon = list(\n      face = 'Ionicons',\n      code = \"f202\",\n      color = \"green\",\n      size = 125\n    )\n  ) %&gt;%\n  \n  visGroups(\n    groupname = \"Transgender?\",\n    shape = \"icon\",\n    icon = list(\n      face = 'Ionicons',\n      code = \"f233\",\n      color = \"dodgerblue\",\n      size = 125\n    )\n  ) %&gt;%\n  visLegend() %&gt;%\n  addIonicons() %&gt;%\n  visInteraction(\n    navigationButtons = TRUE,\n    hover = TRUE,\n    selectConnectedEdges = TRUE,\n    hoverConnectedEdges = TRUE,\n    zoomView = TRUE\n  )\n\n\n\n\n\nSome idea of interactivity and controls with visNetwork:\n Star Wars Nodes \n Star Wars Edges \n\n# let's look again at the data\nstarwars_nodes &lt;- read_csv(\"../../../materials/data/networks/star-wars-network-nodes.csv\")\nstarwars_edges &lt;- read_csv(\"../../../materials/data/networks/star-wars-network-edges.csv\")\n\n\n# We need to rename starwars nodes dataframe and edge dataframe columns for visNetwork\nstarwars_nodes_vis &lt;- \n  starwars_nodes %&gt;% \n  rename(\"label\" = name)\n\n# Convert from and to columns to **node ids**\nstarwars_edges_vis &lt;- \n  starwars_edges %&gt;% \n  \n  # Matching Source &lt;- Source Node id (\"id.x\")\n  left_join(., starwars_nodes_vis, by = c(\"source\" = \"label\")) %&gt;% \n  \n  # Matching Target &lt;- Target Node id (\"id.y\")\n  left_join(., starwars_nodes_vis, by = c(\"target\" = \"label\")) %&gt;% \n  \n# Select \"id.x\" and \"id.y\" ONLY\n# Rename them as \"from\" and \"to\"\n# keep \"weight\" column for aesthetics of edges\n  select(\"from\" = id.x, \"to\" = id.y, \"value\" = weight)\n\n# Check everything once\nstarwars_nodes_vis\n\n\n  \n\n\nstarwars_edges_vis\n\n\n  \n\n\n\nOk, let’s make things move and shake!!\n\nvisNetwork(nodes = starwars_nodes_vis,\n           edges = starwars_edges_vis) %&gt;% \n  visNodes(font = list(size = 30), shape = \"icon\", \n           icon = list(code = \"f1e3\", size = 75)) %&gt;% \n  addFontAwesome() %&gt;% \n  visEdges(color = \"red\")\n\n\n\n\n\n\nvisNetwork(nodes = starwars_nodes_vis,\n           edges = starwars_edges_vis) %&gt;% \n  visNodes(font = list(size = 30)) %&gt;% \n  visEdges(color = \"red\")"
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#your-assignments",
    "href": "content/labs/r-labs/networks/index.html#your-assignments",
    "title": "The Grammar of Networks",
    "section": "Your Assignments:",
    "text": "Your Assignments:\nMake-1 : With a ready made dataset\nStep 0. Fire up a New Project! Always!\nStep 1. Fire up a new Quarto document. Fill in the YAML header.\nStep 2. Take any one of the “Make1-Datasets” datasets described below.\nStep 3. Document contents:\n\nIntroduce / Inspect in R your data and describe\nIntroduce your Purpose\nCreate graph objects\nTry different layouts\nWrite comments in the code\nWrite narrative in text with sections, bold ,italic etc.\n\nStep 4. Knit before you submit. Submit only your renderable .qmd file.\nMake1 - Datasets:\n\nAirline Data:\n\n Airlines Nodes \n Airlines Edges \n\nStart with this bit of code in your second chunk, after set up\n\n\n\nairline_nodes &lt;-\n  read_csv(\"./mydatafolder/AIRLINES-NODES.csv\") %&gt;% \n  mutate(Id = Id + 1)\n\nairline_edges &lt;-\n  read_csv(\"./mydatafolder/AIRLINES-EDGES.csv\") %&gt;%\n  mutate(Source = Source + 1, Target = Target + 1)\n\n\nThe Famous Zachary Karate Club dataset\n\n\n\nStart with pulling this data into your Rmarkdown:\ndata(“karate”,package= “igraphdata”) karate\n\nTry ?karate in the console\nNote that this is not a set of nodes, nor edges, but already a graph-object!\nSo no need to create a graph object using tbl_graph.\nYou will need to just go ahead and plot using ggraph.\n\n\nGame of Thrones:\n\n GoT Networks \n\nStart with pulling this data into your Rmarkdown:\n\n\nGoT &lt;- read_rds(\"../../../materials/data/networks/GoT.RDS\")\n\n\nNote that this is a list of 7 graphs from Game of Thrones.\nSelect one using GoT[[index]] where index = 1…7 and then plot directly.\nTry to access the nodes and edges and modify them using any attribute data\n\n\nAny other graph dataset from igraphdata (type ?igraphdata in console)\n\n\nAsk me for help if you need any\nMake-2: Literary Network with TV Show / Book / Story / Play\nThis is in groups. Groups of 4. To be announced\nYou need to create a Network Graph for your favourite Book, play, TV serial or Show. (E.g. Friends, BBT, or LB or HIMYM, B99, TGP, JTV…or Hamlet, Little Women , Pride and Prejudice, or LoTR)\nStep 1. Go to: Literary Networks for instructions.\nStep 2. Make your data using the instructions.\n\nIn the nodes excel, use id and names as your columns. Any other details in other columns to the right.\nIn your edges excel, use from and to are your first columns. Entries in these columns can be names or ids but be consistent and don’t mix.\n\nStep 3. Decide on 3 answers that you to seek and plan to make graphs for.\nStep 4. Create graph objects. Say 3 visualizations.\nStep 5. Write comments/answers in the code and narrative text. Add pictures from the web using Markdown syntax.\nStep 6. Write Reflection ( ok, a short one!) inside your Quarto document. Make sure it renders !!\nStep 7. Group Submission: Submit the render-able .qmd file AND the data. Quarto Markdown with joint authorship. Each person submits on their Assignments. All get the same grade on this one.\nAsk me for clarifications on what to do after you have read the Instructions in your group."
  },
  {
    "objectID": "content/labs/r-labs/networks/index.html#read-more",
    "href": "content/labs/r-labs/networks/index.html#read-more",
    "title": "The Grammar of Networks",
    "section": "Read more",
    "text": "Read more\n\nThomas Lin Pedersen - 1 giraffe, 2 giraffe,GO!\nIgraph: Network Analysis and Visualization. https://CRAN.R-project.org/package=igraph.\nPedersen, Thomas Lin. 2017a. Ggraph: An Implementation of Grammar of Graphics for Graphs and Networks. https://CRAN.R-project.org/package=ggraph.\n———. 2017b. Tidygraph: A Tidy Api for Graph Manipulation. https://CRAN.R-project.org/package=tidygraph.\nTyner, Sam, François Briatte, and Heike Hofmann. 2017. “Network Visualization with ggplot2.” The R Journal 9 (1): 27–59. https://journal.r-project.org/archive/2017/RJ-2017-023/index.html.\nNetwork Datasets https://icon.colorado.edu/#!/networks\nYunran Chen, Introduction to Network Analysis Using R"
  },
  {
    "objectID": "content/labs/r-labs/pronouns/pronouns.html",
    "href": "content/labs/r-labs/pronouns/pronouns.html",
    "title": "Lab-02: Pronouns and Data",
    "section": "",
    "text": "Understand different kinds of data variables\nAppreciate how they can be identified based on the Interrogative Pronouns they answer to\nUnderstand how each kind of variable lends itself to a specific geometric aspect in the data visualization.\nUnderstand how ask Questions of Data to develop Visualizations"
  },
  {
    "objectID": "content/labs/r-labs/pronouns/pronouns.html#set-up",
    "href": "content/labs/r-labs/pronouns/pronouns.html#set-up",
    "title": "Lab-02: Pronouns and Data",
    "section": "Set Up",
    "text": "Set Up\nThe setup code chunk below brings into our coding session R packages that provide specific computational abilities and also datasets which we can use.\nTo reiterate: Packages and datasets are not the same thing !! Packages are (small) collections of programs. Datasets are just….information."
  },
  {
    "objectID": "content/labs/r-labs/pronouns/pronouns.html#the-penguins-dataset",
    "href": "content/labs/r-labs/pronouns/pronouns.html#the-penguins-dataset",
    "title": "Lab-02: Pronouns and Data",
    "section": "The penguins dataset",
    "text": "The penguins dataset\n\nnames(penguins) # Column, i.e. Variable names\n\n[1] \"species\"           \"island\"            \"bill_length_mm\"   \n[4] \"bill_depth_mm\"     \"flipper_length_mm\" \"body_mass_g\"      \n[7] \"sex\"               \"year\"             \n\nhead(penguins) # first six rows\n\n\n  \n\n\ntail(penguins) # Last six rows\n\n\n  \n\n\ndim(penguins) # Size of dataset\n\n[1] 344   8\n\n# Check for missing data\nany(is.na(penguins) == TRUE)\n\n[1] TRUE\n\n\n\n\n\n\n\n\nInspect the Data\n\n\n\n\nWhat are the variable names()?\nWhat would be the Question you might have asked to obtain each of the variables?\nWhat further questions/meta questions would you ask to “process” that variable? ( Hint: Add another word after any of the Interrogative Pronouns, e.g. How…MANY?)\nWhere might the answers take your story?\n\n\n\n\n\n\n\n\n\nYour Turn #1\n\n\n\nState a few questions after discussion with your friend and state possible variables, or what you could DO with the variables, as an answer.\nE.g. Q. How many penguins? A. We need to count…rows?"
  },
  {
    "objectID": "content/labs/r-labs/pronouns/pronouns.html#pronouns-and-variables",
    "href": "content/labs/r-labs/pronouns/pronouns.html#pronouns-and-variables",
    "title": "Lab-02: Pronouns and Data",
    "section": "Pronouns and Variables",
    "text": "Pronouns and Variables\nIn the Table below, we have a rough mapping of interrogative pronouns to the kinds of variables in the data:\n\n\n\n\n\n\nNo\nPronoun\nAnswer\nVariable/Scale\nExample\nWhat Operations?\n\n\n\n1\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities, with Scale and a Zero Value.Differences and Ratios /Products are meaningful.\nQuantitative/Ratio\nLength,Height,Temperature in Kelvin,Activity,Dose Amount,Reaction Rate,Flow Rate,Concentration,Pulse,Survival Rate\nCorrelation\n\n\n2\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities with Scale. Differences are meaningful, but not products or ratios\nQuantitative/Interval\npH,SAT score(200-800),Credit score(300-850),SAT score(200-800),Year of Starting College\nMean,Standard Deviation\n\n\n3\nHow, What Kind, What Sort\nA Manner / Method, Type or Attribute from a list, with list items in some \" order\" ( e.g. good, better, improved, best..)\nQualitative/Ordinal\nSocioeconomic status (Low income, Middle income, High income),Education level (HighSchool, BS, MS, PhD),Satisfaction rating(Very much Dislike, Dislike, Neutral, Like, Very Much Like)\nMedian,Percentile\n\n\n4\nWhat, Who, Where, Whom, Which\nName, Place, Animal, Thing\nQualitative/Nominal\nName\nCount no. of cases,Mode\n\n\n\n\n\n\n\nAs you go from Qualitative to Quantitative data types in the table, I hope you can detect a movement from fuzzy groups/categories to more and more crystallized numbers. Each variable/scale can be subjected to the operations of the previous group. In the words of S.S. Stevens (https://psychology.okstate.edu/faculty/jgrice/psyc3214/Stevens_FourScales_1946.pdf)\n\nthe basic operations needed to create each type of scale is cumulative: to an operation listed opposite a particular scale must be added all those operations preceding it.\n\nDo think about this as you work with data.\n\nDo take a look at these references:\n\nhttps://stats.idre.ucla.edu/other/mult-pkg/whatstat/what-is-the-difference-between-categorical-ordinal-and-interval-variables/\nhttps://www.freecodecamp.org/news/types-of-data-in-statistics-nominal-ordinal-interval-and-ratio-data-types-explained-with-examples/"
  },
  {
    "objectID": "content/labs/r-labs/pronouns/pronouns.html#the-mpg-dataset",
    "href": "content/labs/r-labs/pronouns/pronouns.html#the-mpg-dataset",
    "title": "Lab-02: Pronouns and Data",
    "section": "The mpg dataset",
    "text": "The mpg dataset\n\nnames(mpg) # Column, i.e. Variable names\n\n [1] \"manufacturer\" \"model\"        \"displ\"        \"year\"         \"cyl\"         \n [6] \"trans\"        \"drv\"          \"cty\"          \"hwy\"          \"fl\"          \n[11] \"class\"       \n\nhead(mpg) # first six rows\n\n\n  \n\n\ntail(mpg) # Last six rows\n\n\n  \n\n\ndim(mpg) # Size of dataset\n\n[1] 234  11\n\n# Check for missing data\nany(is.na(mpg) == TRUE)\n\n[1] FALSE\n\n\nYOUR TURN-2\nLook carefully at the variables here. How would you interpret say the cyl variable? Is it a number and therefore Quantitative, or could it be something else?"
  },
  {
    "objectID": "content/labs/r-labs/pronouns/pronouns.html#single-qualitativecategorical-nominal-variable",
    "href": "content/labs/r-labs/pronouns/pronouns.html#single-qualitativecategorical-nominal-variable",
    "title": "Lab-02: Pronouns and Data",
    "section": "Single Qualitative/Categorical/ Nominal Variable",
    "text": "Single Qualitative/Categorical/ Nominal Variable\n\nQuestions: Which? What Kind? How? How many of each Kind?\n\n\nIsland ( Which island ? )\nSpecies ( Which Species? )\n\n\nCalculations: No of levels / Counts for each level\n\n\n\n\ncount / tally of no. of penguins on each island or in each species\n\nsort and order by island or species\n\n\nCharts: Bar Chart / Pie Chart / Tree Map\n\n\n\ngeom_bar / geom_bar + coord_polar() / Find out!!\n\n\npenguins %&gt;% count(species)\n\n\n  \n\n\n\n\nggplot(penguins) + geom_bar(aes(x = island))\n\n\n\n\n\n\nggplot(penguins) + geom_bar(aes(x = sex))\n\n\n\n\n\n\n\nYOUR TURN-3"
  },
  {
    "objectID": "content/labs/r-labs/pronouns/pronouns.html#single-quantitative-variable",
    "href": "content/labs/r-labs/pronouns/pronouns.html#single-quantitative-variable",
    "title": "Lab-02: Pronouns and Data",
    "section": "Single Quantitative Variable",
    "text": "Single Quantitative Variable\n\nQuestions: How many? How few? How often? How much?\nCalculations: max / min / mean / mode / (units)\n\n\n\nmax(), min(), range(), mean(), mode(), summary()\n\n\n\nCharts: Bar Chart / Histogram / Density\n\n\ngeom_histogram() / geom_density()\n\n\n\n\n\nmax(penguins$bill_length_mm)\n\n[1] 59.6\n\nrange(penguins$bill_length_mm, na.rm =TRUE) \n\n[1] 32.1 59.6\n\nsummary(penguins$flipper_length_mm)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    172     190     197     201     213     231 \n\n\n\nggplot(penguins) + geom_density(aes(bill_length_mm))\n\n\n\n\n\n\nggplot(penguins) + geom_histogram(aes(x = bill_length_mm))\n\n\n\n\n\n\n\nYOUR TURN-4\nAre all the above Quantitative variables ratio variables? Justify."
  },
  {
    "objectID": "content/labs/r-labs/pronouns/pronouns.html#two-variables-quantitative-vs-quantitative",
    "href": "content/labs/r-labs/pronouns/pronouns.html#two-variables-quantitative-vs-quantitative",
    "title": "Lab-02: Pronouns and Data",
    "section": "Two Variables: Quantitative vs Quantitative",
    "text": "Two Variables: Quantitative vs Quantitative\nWe can easily extend our intuition about one quantitative variable, to a pair of them. What Questions can we ask?\n\nQuestions: How many of this vs How many of that? Does this depend upon that? How are they related? (Remember \\(y = mx + c\\) and friends?)\nCalculations: Correlation / Covariance / T-test / Chi-Square Test for Two Means etc. We won’t go into this here !\nCharts: Scatter Plot / Line Plot / Regression i.e. best fit lines\n\n\ncor(penguins$bill_length_mm, penguins$bill_depth_mm)\n\n[1] -0.2286256\n\n\n\nggplot(penguins) +\n  geom_point(aes(x = flipper_length_mm,\n                 y = body_mass_g))\n\n\n\n\n\n\nggplot(penguins) +\n  geom_point(aes(x = flipper_length_mm, \n                 y = bill_length_mm))\n\n\n\n\n\n\n\nYOUR TURN-5"
  },
  {
    "objectID": "content/labs/r-labs/pronouns/pronouns.html#two-variables-categorical-vs-categorical",
    "href": "content/labs/r-labs/pronouns/pronouns.html#two-variables-categorical-vs-categorical",
    "title": "Lab-02: Pronouns and Data",
    "section": "Two Variables: Categorical vs Categorical",
    "text": "Two Variables: Categorical vs Categorical\nWhat sort of question could we ask that involves two categorical variables?\n\nQuestions: How Many of this Kind( ~x) are How Many of that Kind( ~y ) ?\n\nCalculations: Counts and Tallies sliced by Category\n\n\ncounts , tally\n\n\n\n\nCharts: Stacked Bar Charts / Grouped Bar Charts / Segmented Bar Chart / Mosaic Chart\n\ngeom_bar()\nUse the second Categorical variables to modify fill, color.\nAlso try to vary the parameter position of the bars.\n\n\n\n\nggplot(penguins) + geom_bar(aes(x = island, \n                                fill = species),\n                            position = \"stack\")\n\n\n\n\n\n\n\nStoryline: तीन पेनगीन। और तुम भी तीन(Oh never mind!)\nYOUR TURN-6"
  },
  {
    "objectID": "content/labs/r-labs/pronouns/pronouns.html#two-variables-quantitative-vs-qualitative",
    "href": "content/labs/r-labs/pronouns/pronouns.html#two-variables-quantitative-vs-qualitative",
    "title": "Lab-02: Pronouns and Data",
    "section": "Two Variables: Quantitative vs Qualitative",
    "text": "Two Variables: Quantitative vs Qualitative\nFinally, what if we want to look at Quant variables and Qual variables together? What questions could we ask?\n\nQuestions: How much of this is Which Kind of that? How many vs Which? How many vs How?\nCalculations: Counts, Means, Ranges etc., grouped by Categorical variable.\n\n\nggplot(penguins) + \n    geom_density(aes(x = body_mass_g, \n                 color = island, \n                 fill = island), \n                 alpha = 0.3)\n\n\n\n\n\n\n\n\nCharts: Bar Chart using group / density plots by group / violin plots by group / box plots by group\n\n\n\ngeom_bar / geom_density / geom_violin / geom_boxplot using Categorical Variable for grouping\n\n\nggplot(penguins) + \n    geom_density(aes(x = body_mass_g, \n                 color = island, \n                 fill = island), \n                 alpha = 0.3)\n\n\n\n\n\n\nggplot(penguins) + \n  geom_histogram(aes(x = flipper_length_mm,\n                 fill = sex))\n\n\n\n\n\n\n\nYOUR TURN-7\nTime to Play\n\nCreate a fresh RMarkdown and similarly analyse two datasets of the following data sets\n\n\nAny dataset in your R installation. Type data() in your console to see what is available.\ndiamonds . This dataset is part of the tidyverse package so just type diamonds in your code and there it is.\ngapminder !! Yes!!You will need to install the gapminder package to access this dataset\nmosaicData package datasets. Install mosaicData\ndata.world: Find Datasets of your choice: https://docs.data.world/en/64499-64516-Quickstarts-and-tutorials.html\nkaggle: https://www.kaggle.com/datasets"
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html",
    "href": "content/labs/r-labs/graphics/index.html",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "",
    "text": "This RMarkdown document is part of my course on R for Artists and Designers. The material is based on A Layered Grammar of Graphics by Hadley Wickham. The intent of this Course is to build Skill in coding in R, and also appreciate R as a way to metaphorically visualize information of various kinds, using predominantly geometric figures and structures.\nAll RMarkdown files combine code, text, web-images, and figures developed using code. Everything is text; code chunks are enclosed in fences (```)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#introduction",
    "href": "content/labs/r-labs/graphics/index.html#introduction",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "",
    "text": "This RMarkdown document is part of my course on R for Artists and Designers. The material is based on A Layered Grammar of Graphics by Hadley Wickham. The intent of this Course is to build Skill in coding in R, and also appreciate R as a way to metaphorically visualize information of various kinds, using predominantly geometric figures and structures.\nAll RMarkdown files combine code, text, web-images, and figures developed using code. Everything is text; code chunks are enclosed in fences (```)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#goals",
    "href": "content/labs/r-labs/graphics/index.html#goals",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "Goals",
    "text": "Goals\nAt the end of this Lab session, we should: - know the types and structures of tidy data and be able to work with them - be able to create data visualizations using ggplot - Understand aesthetics and scales in `ggplot"
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#pedagogical-note",
    "href": "content/labs/r-labs/graphics/index.html#pedagogical-note",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "Pedagogical Note",
    "text": "Pedagogical Note\nThe method followed will be based on PRIMM:\n\n\nPREDICT Inspect the code and guess at what the code might do, write predictions\n\n\nRUN the code provided and check what happens\n\nINFER what the parameters of the code do and write comments to explain. What bells and whistles can you see?\n\nMODIFY the parameters code provided to understand the options available. Write comments to show what you have aimed for and achieved.\n\nMAKE : take an idea/concept of your own, and graph it."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#set-up",
    "href": "content/labs/r-labs/graphics/index.html#set-up",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "Set Up",
    "text": "Set Up\nThe setup code chunk below brings into our coding session R packages that provide specific computational abilities and also datasets which we can use.\nTo reiterate: Packages and datasets are not the same thing !! Packages are (small) collections of programs. Datasets are just….information.\n\nlibrary(tidyverse) \nlibrary(palmerpenguins)\nlibrary(ggformula)\nlibrary(ggstance)\n# A collection of historical datasets\nlibrary(HistData)\nlibrary(sf)\nlibrary(sfheaders)"
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#a-teaser-from-john-snow",
    "href": "content/labs/r-labs/graphics/index.html#a-teaser-from-john-snow",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "A Teaser from John Snow",
    "text": "A Teaser from John Snow"
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#review-of-tidy-data",
    "href": "content/labs/r-labs/graphics/index.html#review-of-tidy-data",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "Review of Tidy Data",
    "text": "Review of Tidy Data\n“Tidy Data” is an important way of thinking about what data typically look like in R. Let’s fetch a figure from the web to show the (preferred) structure of data in R. (The syntax to bring in a web-figure is ![caption](url))\n The three features described in the figure above define the nature of tidy data:\n\n\nVariables in Columns\n\n\nObservations in Rows and\n\n\nMeasurements in Cells.\n\nData are imagined to be resulting from an experiment. Each variable represents a parameter/aspect in the experiment. Each row represents an additional datum of measurement. A cell is a single measurement on a single parameter(column) in a single observation(row)."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#kinds-of-variables",
    "href": "content/labs/r-labs/graphics/index.html#kinds-of-variables",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "Kinds of Variables",
    "text": "Kinds of Variables\nKinds of Variable are defined by the kind of questions they answer to:\n\nWhat/Who/Where? -&gt; Some kind of Name. Categorical variable\nWhat Kind? How? -&gt; Some kind of “Type”. Factor variable\nHow Many? How large? -&gt; Some kind of Quantity. Numerical variable. Most Figures in R are computed with variables, and therefore, with columns."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#interrogations-and-graphs",
    "href": "content/labs/r-labs/graphics/index.html#interrogations-and-graphs",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "Interrogations and Graphs",
    "text": "Interrogations and Graphs\nCreating graphs from data is an act of asking questions and viewing answers in a geometric way. Let us write some simple English descriptions of measures and visuals and see what commands they use in R."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#components-of-the-layered-grammar-of-graphics",
    "href": "content/labs/r-labs/graphics/index.html#components-of-the-layered-grammar-of-graphics",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "Components of the layered grammar of graphics",
    "text": "Components of the layered grammar of graphics\nLayers are used to create the objects on a plot. They are defined by five basic parts:\n\nData (What dataset/spreadsheet am I using?)\nMapping (What does each column do in my graph?)\nStatistical transformation (stat) (Do I have count something first?)\nGeometric object (geom) (What shape, colour, size…do I want?)\nPosition adjustment (position) (Where do I want it on the graph?)\n\nData\nWe will use “real world” data. Let’s use the penguins dataset in the palmerpenguins package. Run ?penguins in the console to get more information about this dataset.\nHead\n\nhead(penguins)\n\n\n  \n\n\n\nTail\n\ntail(penguins)\n\n\n  \n\n\n\nDim\n\ndim(penguins)\n\n[1] 344   8\n\n\nSo we know what our data looks like. We pass this data to ggplot use to plot as follows: in R this creates an empty graph sheet!! Because we have not (yet) declared the geometric shapes we want to use to plot our information.\n\nggplot(data = penguins) # Creates an empty graphsheet, ready for plotting!!\n\n\n\n\n\n\n\nMapping\nNow that we have told R what data to use, we need to state what variables to plot and how.\nAesthetic Mapping defines how the variables are applied to the plot, i.e. we take a variable from the data and “metaphorize” it into a geometric feature. We can map variables metaphorically to a variety of geometric things: coordinate, length, height, size, shape, colour, alpha(how dark?)….\nThe syntax uses: aes(some_geometric_thing = some_variable)\nRemember variable = column.\nSo if we were graphing information from penguins, we might map a penguin’s flipper_length_mm column to the \\(x\\) position, and the body_mass_g column to the \\(y\\) position.\nMapping Example-1\nWe can try another example of aesthetic mapping with the same dataset:\nPlot-1a\n\nggplot(data = penguins)\n\n\n\n\n\n\n\nPlot-1b\n\nggplot(penguins) + \n\n# Plot geom = histogram. So we need a quantity on the x\n  geom_histogram(\n    aes(x = body_mass_g))\n\n\n\n\n\n\n\nPlot-1c\n\nggplot(penguins) +\n\n# Plot geom = histogram. So we need a quantity on the x\n  geom_histogram(\n    aes(x = body_mass_g,\n        fill = island)    # color aesthetic = another variable\n    )\n\n\n\n\n\n\n\nMapping Example-2\nWe can try another example of aesthetic mapping with the same dataset:\nPlot-2a\n\nggplot(data = penguins)\n\n\n\n\n\n\n\nPlot-2b\n\nggplot(penguins) + \n\n# Plot geom = histogram. So we need a quantity on the x\n  geom_histogram(\n    aes(x = body_mass_g)) \n\n\n\n\n\n\n\nPlot-2c\n\nggplot(penguins) +\n\n# Plot geom = histogram. So we need a quantity on the x\n  geom_histogram(\n    aes(x = body_mass_g,\n        fill = species)   #&lt;&lt; # color aesthetic = another variable\n    )"
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#geometric-objects",
    "href": "content/labs/r-labs/graphics/index.html#geometric-objects",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "Geometric objects",
    "text": "Geometric objects\nGeometric objects (geoms) control the type of plot you create. Geoms are classified by their dimensionality:\n\n0 dimensions - point, text\n1 dimension - path, line\n2 dimensions - polygon, interval\n\nEach geom can only display certain aesthetics or visual attributes of the geom. For example, a point geom has position, color, shape, and size aesthetics.\nWe can also stack up geoms on top of one another to add layers to the graph.\nPlot1\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_line()\n\n\n\n\n\n\n\nPlot2\n\nggplot(data = penguins) + \n  geom_line(aes(x = bill_length_mm, \n                y = body_mass_g))\n\n\n\n\n\n\n\nPlot3\n\nggplot(data = penguins) + \n  geom_point(aes(x = bill_length_mm, \n                 y = body_mass_g,\n                 color = island, \n                 shape = species),\n             size = 3) + \n  \n  ggtitle(\"A point geom with position and color and shape aesthetics\")\n\n\n\n\n\n\n\n\nggplot(data = penguins, \n       aes(x = species)) + # x position =&gt; ?\n  # No need to type \"mapping\"...\n  geom_bar() + # Where does the height come from?\n  ggtitle(\"A bar geom with position and height aesthetics\")\n\n\n\n\n\n\n\n\nggplot(data = penguins, aes(x = island)) +\n  geom_bar() +\n  ggtitle(\"A bar geom with position and height aesthetics\")\n\n\n\n\n\n\n\n\nPosition determines the starting location (origin) of each bar\nHeight determines how tall to draw the bar. Here the height is based on the number of observations in the dataset for each possible species."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#position-adjustment",
    "href": "content/labs/r-labs/graphics/index.html#position-adjustment",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "Position adjustment",
    "text": "Position adjustment\nSometimes with dense data we need to adjust the position of elements on the plot, otherwise data points might obscure one another. Bar plots frequently stack or dodge the bars to avoid overlap:\nSometimes scatterplots with few unique \\(x\\) and \\(y\\) values are jittered (random noise is added) to reduce overplotting.\n\nggplot(data = penguins, \n       mapping = aes(x = species, \n                     y = body_mass_g)) +\n  geom_point() +\n  ggtitle(\"A point geom with obscured data points\")\n\n\n\n\n\n\nggplot(data = penguins, \n       mapping = aes(x = species, \n                     y = body_mass_g)) +\n  geom_jitter() +\n  ggtitle(\"A point geom with jittered data points\")"
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#statistical-transformation",
    "href": "content/labs/r-labs/graphics/index.html#statistical-transformation",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "Statistical transformation",
    "text": "Statistical transformation\nA statistical transformation (stat) pre-transforms the data, before plotting. For instance, in a bar graph you might summarize the data by counting the total number of observations within a set of categories, and then plotting the count.\nCount\n\ncount(x = penguins, island)\n\n\n  \n\n\n\nCount and Bar Graph\n\nmydat &lt;- count(penguins,island)\n\nggplot(data = mydat) +\n    geom_col(aes(x = island, y = n))\n\n\n\n\n\n\n\nTidy Count and Bar Graph\n\n\n\n\n\n\n\n\nCount inside the Plot\n\npenguins %&gt;%           # Our pipe Operator\n  \n  ggplot(.) +          # \".\" becomes the penguins dataset\n  \n  geom_bar(aes(x = island)) # Note: geom_BAR !! y = count, and is computed internally!!\n\n\n\n\n\n\n\nSometimes you don’t need to make a statistical transformation. For example, in a scatterplot you use the raw values for the \\(x\\) and \\(y\\) variables to map onto the graph. In these situations, the statistical transformation is an identity transformation - the stat simply passes in the original dataset and exports the exact same dataset."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#scale",
    "href": "content/labs/r-labs/graphics/index.html#scale",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "Scale",
    "text": "Scale\nA scale controls how data is mapped to aesthetic attributes, so we need one scale for every aesthetic property employed in a layer. For example, this graph defines a scale for color:\n\nggplot(data = penguins, \n       mapping = aes(x = bill_depth_mm, \n                     y = bill_length_mm, \n                     color = species)) +\n  geom_point() \n\n\n\n\n\n\n\nThe scale can be changed to use a different color palette:\n\nggplot(data = penguins, \n       mapping = aes(x = bill_length_mm, \n                     y = body_mass_g, \n                     color = species)) +\n  geom_point() +\n  scale_color_brewer(palette = \"Dark2\", direction = -1)\n\n\n\n\n\n\n\nNow we are using a different palette, but the scale is still consistent: all Adelie penguins utilize the same color, whereas Chinstrap use a new color but each Adelie still uses the same, consistent color."
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#coordinate-system",
    "href": "content/labs/r-labs/graphics/index.html#coordinate-system",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "Coordinate system",
    "text": "Coordinate system\nA coordinate system (coord) maps the position of objects onto the plane of the plot, and controls how the axes and grid lines are drawn. Plots typically use two coordinates (\\(x, y\\)), but could use any number of coordinates. Most plots are drawn using the Cartesian coordinate system:\n\nx1 &lt;- c(1, 10)\ny1 &lt;- c(1, 5)\np &lt;- qplot(x = x1, y = y1, \n           geom = \"point\", # Quick Plot. Deprecated, don't use\n           xlab = NULL, ylab = NULL) +\n  theme_bw()\np +\n  ggtitle(label = \"Cartesian coordinate system\")\n\n\n\n\n\n\nggplot(penguins, aes(flipper_length_mm, body_mass_g)) +\n  geom_point() + \n  coord_polar()\n\n\n\n\n\n\n\nThis system requires a fixed and equal spacing between values on the axes. That is, the graph draws the same distance between 1 and 2 as it does between 5 and 6. The graph could be drawn using a semi-log coordinate system which logarithmically compresses the distance on an axis:\n\np +\n  coord_trans(y = \"log10\") +\n  ggtitle(label = \"Semi-log coordinate system\")\n\n\n\n\n\n\n\nOr could even be drawn using polar coordinates:\n\np +\n  coord_polar() +\n  ggtitle(label = \"Polar coordinate system\")"
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#faceting",
    "href": "content/labs/r-labs/graphics/index.html#faceting",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "Faceting",
    "text": "Faceting\nFaceting can be used to split the data up into subsets of the entire dataset. This is a powerful tool when investigating whether patterns are the same or different across conditions, and allows the subsets to be visualized on the same plot (known as conditioned or trellis plots). The faceting specification describes which variables should be used to split up the data, and how they should be arranged.\n\nggplot(data = penguins, \n       mapping = aes(x = bill_length_mm, \n                     y = body_mass_g)) +\n  geom_point() +\n  facet_wrap(~ island)\n\n\n\n\n\n\n\n\nggplot(data = penguins, \n       mapping = aes(x = bill_length_mm, \n                     y = body_mass_g, \n                     color = sex)) +\n  geom_point() +\n  facet_grid(species ~ island, scales = \"free_y\")\n\n\n\n\n\n\n# Ria's explanation: This code did not work because....\n#"
  },
  {
    "objectID": "content/labs/r-labs/graphics/index.html#defaults",
    "href": "content/labs/r-labs/graphics/index.html#defaults",
    "title": "Lab 06 - The Grammar of Graphics",
    "section": "Defaults",
    "text": "Defaults\nRather than explicitly declaring each component of a layered graphic (which will use more code and introduces opportunities for errors), we can establish intelligent defaults for specific geoms and scales. For instance, whenever we want to use a bar geom, we can default to using a stat that counts the number of observations in each group of our variable in the \\(x\\) position.\nConsider the following scenario: you wish to generate a scatterplot visualizing the relationship between penguins’ bill_length and their body_mass. With no defaults, the code to generate this graph is:\n\nggplot() +\n  layer(\n    data = penguins, \n    mapping = aes(x = bill_length_mm, \n                  y = body_mass_g),\n    geom = \"point\", \n    stat = \"identity\", \n    position = \"identity\"\n  ) +\n  scale_x_continuous() +\n  scale_y_continuous() +\n  coord_cartesian()\n\n\n\n\n\n\n\nThe above code:\n\nCreates a new plot object (ggplot)\n\nAdds a layer (layer)\n\nSpecifies the data (penguins)\nMaps engine bill length to the \\(x\\) position and body mass to the \\(y\\) position (mapping)\nUses the point geometric transformation (geom = \"point\")\nImplements an identity transformation and position (stat = \"identity\" and position = \"identity\")\n\n\nEstablishes two continuous position scales (scale_x_continuous and scale_y_continuous)\nDeclares a cartesian coordinate system (coord_cartesian)\n\nHow can we simplify this using intelligent defaults?\n\nWe only need to specify one geom and stat, since each geom has a default stat.\nCartesian coordinate systems are most commonly used, so it should be the default.\n\nDefault scales can be added based on the aesthetic and type of variables.\n\nContinuous values are transformed with a linear scaling.\nDiscrete values are mapped to integers.\nScales for aesthetics such as color, fill, and size can also be intelligently defaulted.\n\n\n\nUsing these defaults, we can rewrite the above code as:\n\nggplot() +\n  geom_point(data = penguins, \n             mapping = aes(x = bill_length_mm, \n                           y = body_mass_g))\n\n\n\n\n\n\n\nThis generates the exact same plot, but uses fewer lines of code. Because multiple layers can use the same components (data, mapping, etc.), we can also specify that information in the ggplot() function rather than in the layer() function:\n\nggplot(data = penguins, \n       mapping = aes(x = bill_length_mm, \n                     y = body_mass_g)) +\n  geom_point()\n\n\n\n\n\n\n\nAnd as we will learn, function arguments in R use specific ordering, so we can omit the explicit call to data and mapping:\n\nggplot(penguins, aes(bill_length_mm, body_mass_g)) +\n  geom_point()"
  },
  {
    "objectID": "content/posts/09-using-lordicons/index.html",
    "href": "content/posts/09-using-lordicons/index.html",
    "title": "Using Lordicons, Fontawesome Icons,Academicons, and Iconify Icons",
    "section": "",
    "text": "This is just a compilation of the example files from the Quarto website, so that I can have ready-made code to copy and paste."
  },
  {
    "objectID": "content/posts/09-using-lordicons/index.html#introduction",
    "href": "content/posts/09-using-lordicons/index.html#introduction",
    "title": "Using Lordicons, Fontawesome Icons,Academicons, and Iconify Icons",
    "section": "",
    "text": "This is just a compilation of the example files from the Quarto website, so that I can have ready-made code to copy and paste."
  },
  {
    "objectID": "content/posts/09-using-lordicons/index.html#installation",
    "href": "content/posts/09-using-lordicons/index.html#installation",
    "title": "Using Lordicons, Fontawesome Icons,Academicons, and Iconify Icons",
    "section": "Installation",
    "text": "Installation\nType these in your Terminal:\n\nIconify: quarto install extension mcanouil/quarto-iconify\nFontAwesome: quarto install extension quarto-ext/fontawesome\nLordicons: quarto install extension jmgirard/lordicon\nAcademicons: quarto install extension schochastics/academicons\n\nThese extensions allows you to use a variety of icons in your Quarto HTML documents."
  },
  {
    "objectID": "content/posts/09-using-lordicons/index.html#using-lordicon-shortcodes",
    "href": "content/posts/09-using-lordicons/index.html#using-lordicon-shortcodes",
    "title": "Using Lordicons, Fontawesome Icons,Academicons, and Iconify Icons",
    "section": "Using Lordicon Shortcodes",
    "text": "Using Lordicon Shortcodes\nThe {{&lt; li &gt;}} shortcode renders an icon (specified by its code) after downloading it the lordicon CDN. The {{&lt; lif &gt;}} shortcode renders an icon (specified by its filepath) from a local .json file. Both shortcodes support the same arguments for customization, described below.\n\n\n\n\n\n\n\n\nPseudocode\nExample Code\nRendered\n\n\n\n\n{{&lt; li code &gt;}}\n{{&lt; li wlpxtupd &gt;}}\n\n\n\n{{&lt; lif file &gt;}}\n{{&lt; lif church.json &gt;}}\n\n\n\n\n\nTriggers\ntrigger controls the icon’s animation type. When using the loop or loop-on-hover triggers, you can also set an optional delay (in ms) between loops.\n\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n\n{{&lt; li wxnxiano &gt;}}\n\n\n\n{{&lt; li wxnxiano trigger=click &gt;}}\n\n\n\n{{&lt; li wxnxiano trigger=hover &gt;}}\n\n\n\n{{&lt; li wxnxiano trigger=loop &gt;}}\n\n\n\n{{&lt; li wxnxiano trigger=loop delay=1000 &gt;}}\n\n\n\n{{&lt; li wxnxiano trigger=loop-on-hover &gt;}}\n\n\n\n{{&lt; li wxnxiano trigger=loop-on-hover delay=1000 &gt;}}\n\n\n\n{{&lt; li wxnxiano trigger=morph &gt;}}\n\n\n\n{{&lt; li wxnxiano trigger=boomerang &gt;}}\n\n\n\n\n\n\nSpeed\nspeed controls how quickly the icon’s animation plays.\n\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n\n{{&lt; li lupuorrc trigger=loop speed=0.5 &gt;}}\n\n\n\n{{&lt; li lupuorrc trigger=loop speed=1.0 &gt;}}\n\n\n\n{{&lt; li lupuorrc trigger=loop speed=2.0 &gt;}}\n\n\n\n\n\n\nColors\ncolors controls the icon’s coloring. Outline icons typically have just a primary and secondary color, but flat and lineal icons can have many more. Each color should be given in rank:color format (where ranks are primary, secondary, tertiary, etc.) and multiple colors should be separated by commas. Colors can be given in HTML color names or hexcodes.\n\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n\n{{&lt; li lupuorrc &gt;}}\n\n\n\n{{&lt; li lupuorrc colors=primary:gold &gt;}}\n\n\n\n{{&lt; li lupuorrc colors=primary:gray,secondary:orange &gt;}}\n\n\n\n{{&lt; li lupuorrc colors=primary:#4030e8,secondary:#ee66aa &gt;}}\n\n\n\n\n\n\nStroke\nstroke controls how thick the lines in an icon are.\n\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n\n{{&lt; li lupuorrc stroke=50 &gt;}}\n\n\n\n{{&lt; li lupuorrc stroke=100 &gt;}}\n\n\n\n{{&lt; li lupuorrc stroke=150 &gt;}}\n\n\n\n\n\n\nScale\nscale controls how large or zoomed in the icon is.\n\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n\n{{&lt; li lupuorrc scale=25 &gt;}}\n\n\n\n{{&lt; li lupuorrc scale=50 &gt;}}\n\n\n\n{{&lt; li lupuorrc scale=100 &gt;}}\n\n\n\n\n\n\nAxis X\nx controls the horizontal position of the center of the icon.\n\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n\n{{&lt; li lupuorrc x=25 &gt;}}\n\n\n\n{{&lt; li lupuorrc x=50 &gt;}}\n\n\n\n{{&lt; li lupuorrc x=100 &gt;}}\n\n\n\n\n\n\nAxis Y\ny controls the vertical position of the center of the icon.\n\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n\n{{&lt; li lupuorrc y=25 &gt;}}\n\n\n\n{{&lt; li lupuorrc y=50 &gt;}}\n\n\n\n{{&lt; li lupuorrc y=100 &gt;}}"
  },
  {
    "objectID": "content/posts/09-using-lordicons/index.html#using-academicons-shortcodes",
    "href": "content/posts/09-using-lordicons/index.html#using-academicons-shortcodes",
    "title": "Using Lordicons, Fontawesome Icons,Academicons, and Iconify Icons",
    "section": "Using Academicons Shortcodes",
    "text": "Using Academicons Shortcodes\nThis extension allows you to use academicons in your Quarto HTML documents. It provides an {{&lt; ai &gt;}} shortcode:\n\nMandatory &lt;icon-name&gt;:\n{{&lt; ai &lt;icon-name&gt; &gt;}}\nOptional &lt;size=...&gt;:\n{{&lt; ai &lt;icon-name&gt; &lt;size=...&gt; &gt;}}\n\nFor example:\n\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n\n{{&lt; ai arxiv &gt;}}\n\n\n\n{{&lt; ai google-scholar &gt;}}\n\n\n\n{{&lt; ai open-access &gt;}}\n\n\n\n{{&lt; ai open-access size=5x &gt;}}"
  },
  {
    "objectID": "content/posts/09-using-lordicons/index.html#using-fontawesome-icons",
    "href": "content/posts/09-using-lordicons/index.html#using-fontawesome-icons",
    "title": "Using Lordicons, Fontawesome Icons,Academicons, and Iconify Icons",
    "section": "Using Fontawesome Icons",
    "text": "Using Fontawesome Icons\nThis extension allows you to use font-awesome icons in your Quarto HTML and PDF documents. It provides an {{&lt; fa &gt;}} shortcode:\n\nMandatory &lt;icon-name&gt;:\n{{&lt; fa &lt;icon-name&gt; &gt;}}\nOptional &lt;group&gt;, &lt;size=...&gt;, and &lt;title=...&gt;:\n{{&lt; fa &lt;group&gt; &lt;icon-name&gt; &lt;size=...&gt; &lt;title=...&gt; &gt;}}\n\nFor example:\n\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n\n{{&lt; fa thumbs-up &gt;}}\n\n\n\n{{&lt; fa folder &gt;}}\n\n\n\n{{&lt; fa chess-pawn &gt;}}\n\n\n\n{{&lt; fa brands bluetooth &gt;}}\n\n\n\n{{&lt; fa brands twitter size=2xl &gt;}} (HTML only)\n\n\n\n{{&lt; fa brands github size=5x &gt;}} (HTML only)\n\n\n\n{{&lt; fa battery-half size=Huge &gt;}}\n\n\n\n{{&lt; fa envelope title=\"An envelope\" &gt;}}"
  },
  {
    "objectID": "content/posts/09-using-lordicons/index.html#using-iconify-shortcodes",
    "href": "content/posts/09-using-lordicons/index.html#using-iconify-shortcodes",
    "title": "Using Lordicons, Fontawesome Icons,Academicons, and Iconify Icons",
    "section": "Using Iconify Shortcodes",
    "text": "Using Iconify Shortcodes\nThis extension allows you to use Iconify icons in your Quarto HTML documents. It provides an {{&lt; iconify &gt;}} shortcode:\n\nMandatory &lt;icon-name&gt;:\n{{&lt; iconify &lt;icon-name&gt; &gt;}}\nOptional &lt;set&gt; (default is fluent-emoji) &lt;size=...&gt;, &lt;width=...&gt;, &lt;height=...&gt;, &lt;flip=...&gt;, and &lt;rotate=...&gt;:\n{{&lt; iconify &lt;set&gt; &lt;icon-name&gt; &lt;size=...&gt; &lt;width=...&gt; &lt;height=...&gt; &lt;flip=...&gt; &lt;rotate=...&gt; &gt;}}\nIf &lt;size=...&gt; is defined, &lt;width=...&gt; and &lt;height=...&gt; are not used.\nSee https://docs.iconify.design/iconify-icon/ for more details.\n\nFor example:\n\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n\n{{&lt; iconify exploding-head &gt;}}\n\n\n\n{{&lt; iconify exploding-head size=2xl &gt;}}\n\n\n\n{{&lt; iconify exploding-head size=5x rotate=180deg &gt;}}\n\n\n\n{{&lt; iconify exploding-head size=Huge &gt;}}\n\n\n\n{{&lt; iconify fluent-emoji-high-contrast 1st-place-medal &gt;}}\n\n\n\n{{&lt; iconify twemoji 1st-place-medal &gt;}}\n\n\n\n{{&lt; iconify line-md loading-alt-loop &gt;}}\n\n\n\n{{&lt; iconify fa6-brands apple width=50px height=10px rotate=90deg flip=vertical &gt;}}"
  },
  {
    "objectID": "content/posts/15-js/Using_sketch.html",
    "href": "content/posts/15-js/Using_sketch.html",
    "title": "Using sketch",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE)\nknitr::knit_engines$set(sketch = sketch::eng_sketch)\nlibrary(tidyverse)\nlibrary(sketch)\n\n#devtools::install_github(\"seankross/p5\")\nlibrary(p5)"
  },
  {
    "objectID": "content/posts/15-js/Using_sketch.html#introduction",
    "href": "content/posts/15-js/Using_sketch.html#introduction",
    "title": "Using sketch",
    "section": "Introduction",
    "text": "Introduction\nTrying to replicate this: https://kcf-jackson.github.io/sketch-website/docs/\n\nprint(\"'sketch' has its own knitr engine from version 1.0.5!\")\n\n\n\n\nsketch::insert_sketch(\n  file = \"./Using_sketch/main.R\", id = \"sketch_1\",\n  width = 500, height = 400\n)\n\n\n\n\n\nsketch::insert_sketch(\n  file = \"./Using_sketch/dots.R\", id = \"sketch_2\", deparsers = default_2_deparsers(),\n    width = 800, height = 600\n  )\n\n\n\n\n\nsketch::insert_sketch(\n  file = \"./Using_sketch/animated_dots.R\", id = \"sketch_2\", \n  deparsers = default_2_deparsers(),\n    width =800, height = 600\n  )\n\n\n\n\n\np5::p5() |&gt;\n  createCanvas(800, 600) |&gt;\n  background(\"#F4F8FC\") |&gt;\n  fill(\"yellow\") |&gt;\n  ellipse(~mouseX, ~mouseY, 30, 30)\n\n\n\n\n\n\nstripes &lt;- tibble(\n  x = rep(0, 7),\n  y = cumsum(c(0, rep(30, 6))),\n  w = rep(300, 7),\n  h = rep(15, 7)\n)\nstripes\n\n\n  \n\n\nstripes %&gt;%\n  p5() %&gt;%\n  createCanvas(300, 200) %&gt;%\n  fill(\"#FF0000\") %&gt;%\n  noStroke() %&gt;%\n  rect()\n\n\n\n\n\n\n#! load_script(src = \"https://cdnjs.cloudflare.com/ajax/libs/p5.js/0.9.0/p5.js\")\nsetup &lt;- function() {\n    createCanvas(400, 300)\n}\n\ndraw &lt;- function() {\n    background(0, 0, 33)    # RGB colors\n\n    for (i in 1:3) {\n        dia &lt;- sin(frameCount * 0.025) * 30 * i\n        fill(255, 70 * i, 0)       # RGB colors\n        circle(100 * i, 150, dia)   # (x, y, diameter)    \n    }\n}"
  },
  {
    "objectID": "content/posts/10-using-nutshell/nutshell.html",
    "href": "content/posts/10-using-nutshell/nutshell.html",
    "title": "Nutshell: Expandable Explanations",
    "section": "",
    "text": "Nutshell is a tool to make “expandable explanations”, like this! This lets your readers learn only the details they need, just-in-time, always-in-context.\nUnlike links, Nutshell lets you include only the snippet you need, not the whole page. Plus, instead of being lost in a jungle of new tabs, your reader stays on one page, keeping their flow of reading. Even if you interrupt a sentence, Nutshell recaps the sentence afterwards, so your reader never loses context.\nYou can find more information on the nutshell webpage and here is a live demo !"
  },
  {
    "objectID": "content/posts/10-using-nutshell/nutshell.html#what-is-nutshell",
    "href": "content/posts/10-using-nutshell/nutshell.html#what-is-nutshell",
    "title": "Nutshell: Expandable Explanations",
    "section": "",
    "text": "Nutshell is a tool to make “expandable explanations”, like this! This lets your readers learn only the details they need, just-in-time, always-in-context.\nUnlike links, Nutshell lets you include only the snippet you need, not the whole page. Plus, instead of being lost in a jungle of new tabs, your reader stays on one page, keeping their flow of reading. Even if you interrupt a sentence, Nutshell recaps the sentence afterwards, so your reader never loses context.\nYou can find more information on the nutshell webpage and here is a live demo !"
  },
  {
    "objectID": "content/posts/10-using-nutshell/nutshell.html#test",
    "href": "content/posts/10-using-nutshell/nutshell.html#test",
    "title": "Nutshell: Expandable Explanations",
    "section": "Test",
    "text": "Test\nThis is a senseless paragraph"
  },
  {
    "objectID": "content/posts/10-using-nutshell/nutshell.html#testing-links",
    "href": "content/posts/10-using-nutshell/nutshell.html#testing-links",
    "title": "Nutshell: Expandable Explanations",
    "section": "Testing Links",
    "text": "Testing Links\n\n:link to senseless paragraph\n:link to wikipedia article\n:link to invisible sections"
  },
  {
    "objectID": "content/posts/10-using-nutshell/nutshell.html#x-invisible",
    "href": "content/posts/10-using-nutshell/nutshell.html#x-invisible",
    "title": "Nutshell: Expandable Explanations",
    "section": ":x invisible",
    "text": ":x invisible\nUse ## :x header to include an invisible section that can be linked to via nutshell"
  },
  {
    "objectID": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.html",
    "href": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.html",
    "title": "A TRIZ Analysis of Lawrence of Arabia",
    "section": "",
    "text": "The movie Lawrence of Arabia is justly famous for its terrific story, great cast of characters and actors, and some truly legendary scenes. Here I take one of the iconic scenes from the movie, the attack on the port town of Aqaba and interpret the entire event and the build up to it from a TRIZ viewpoint.\nI will give a short description of the movie plot first.\nI then discuss the source of the problem, how it occurred to Lawrence, and how he went about solving it in this dramatic fashion. I give an analysis of the Problem from a (classical) TRIZ perspective, including the formulation of the Contradiction, Identification of Causes, the statement of the Ideal Final Result, and finally using the TRIZ Contradiction Matrix to find Inventive Principles that inform Lawrence’s solution. I also dwell in passing upon aspects of how this story can be interpreted as a manifestation of Mihaly Csikszentmihalyi’s Creativity Systems Model.\nThis piece stems from my teaching a course on Creative Thinking and Problem Solving based on TRIZ, titled Play and Invent, over the past 8 years or more at the Srishti Manipal Institute of Art, Design, and Technology, Bangalore, India. (https://srishtimanipalinstitute.in)."
  },
  {
    "objectID": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.html#introduction",
    "href": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.html#introduction",
    "title": "A TRIZ Analysis of Lawrence of Arabia",
    "section": "",
    "text": "The movie Lawrence of Arabia is justly famous for its terrific story, great cast of characters and actors, and some truly legendary scenes. Here I take one of the iconic scenes from the movie, the attack on the port town of Aqaba and interpret the entire event and the build up to it from a TRIZ viewpoint.\nI will give a short description of the movie plot first.\nI then discuss the source of the problem, how it occurred to Lawrence, and how he went about solving it in this dramatic fashion. I give an analysis of the Problem from a (classical) TRIZ perspective, including the formulation of the Contradiction, Identification of Causes, the statement of the Ideal Final Result, and finally using the TRIZ Contradiction Matrix to find Inventive Principles that inform Lawrence’s solution. I also dwell in passing upon aspects of how this story can be interpreted as a manifestation of Mihaly Csikszentmihalyi’s Creativity Systems Model.\nThis piece stems from my teaching a course on Creative Thinking and Problem Solving based on TRIZ, titled Play and Invent, over the past 8 years or more at the Srishti Manipal Institute of Art, Design, and Technology, Bangalore, India. (https://srishtimanipalinstitute.in)."
  },
  {
    "objectID": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.html#lawrence-of-arabia-a-summary",
    "href": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.html#lawrence-of-arabia-a-summary",
    "title": "A TRIZ Analysis of Lawrence of Arabia",
    "section": "Lawrence of Arabia: a Summary",
    "text": "Lawrence of Arabia: a Summary\nThe movie is the story of T.E. Lawrence, the English officer who successfully united and led the diverse, often warring, Arab tribes during World War I in order to fight the Turks. The stellar cast includes Peter O’Toole as Lawrence, Omar Sharif as Ali, Alec Guinness as Prince Feisal, Anthony Quinn as Auda Abu Tayi, Claude Raines as Dryden, and Anthony Quayle as Col. Brighton. The director was David Lean. The editing of the film by Anne Coates is also much admired. ( https://womenfilmeditors.princeton.edu/tag/lawrence-of-arabia/ )\nLawrence is a complex, talented, and yet simple man, who is extremely well read (Greek philosophy and the Koran, for example) and is also an expert in Arab affairs and has considerable skill at map-making. Due to his being interpreted as insolent and insubordinate , he is given a lowly job at the HQ in Cairo. Dryden manages to convince the General that Lawrence should be allowed to go into Arabia and to find out what kind of long-term plans Prince Feisal is making for Arabia.\nHere is the map of the events that are unfolding in the movie at this time.1\n\n\n\n\n\nLawrence encounters Ali in dramatic fashion at the Masturah Well, on the way to meet Feisal, and his Arab guide is shot by Ali, a direct experience for Lawrence of inter-tribe rivalry in Arabia. (Ali is a Harith, and Tafas the guide was a Hashemi). Lawrence peremptorily rejects an offer of help from Ali, and finds his way alone to Wadi Safra, where Feisal is camped. He is met by Col. Brighton as he nears the camp. Both enter camp just in time to witness another bombing raid by Turkish airplanes.\nLater in the meeting with Feisal, Brighton tries to convince Feisal to retreat to Yenbo (Yanbu) and be out of range for the Turks, and where the British Army would supply them, train them to fight against the Turks. Feisal reluctantly accepts this plan, though he would rather the British navy take the port city of Aqaba and supply his army from there. Brighton simply scoffs at that idea, because the Turkish have 12 inch guns at Aqaba and the British have other things to do.\nLawrence has already intrigued Feisal by completing a verse from the Koran as it was being read by Selim, the cleric. At the end of the meeting, Feisal confronts Lawrence alone, as to his intentions in Arabia and finds out, to his astonishment, that Lawrence has his own interpretation of what his tasks and loyalties were, and these did not necessarily coincide with those of Brighton. In fact, Lawrence is not in favour of the Arab Army’s retreat to Yenbo, as it would become one small part of the British Army. As a parting remark, Feisal says to Lawrence that the Arabs need what no man can provide, a miracle.\nHere is the video of that terrific tent meeting scene."
  },
  {
    "objectID": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.html#lawrences-problem",
    "href": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.html#lawrences-problem",
    "title": "A TRIZ Analysis of Lawrence of Arabia",
    "section": "Lawrence’s Problem",
    "text": "Lawrence’s Problem\nLawrence does not sleep that night. Provoked by Feisal’s parting remark, he sits up all night on a sand dune close to the camp, thinking about how Aqaba could be taken, since he wants the Arabs to continue fighting from where they were, and even advance if possible with British help. His detailed understanding of the Arabian geography, his knowledge of the Aqaba port and its fortifications, all come to the fore here. Aqaba is a port at the head end of a narrow gulf to the east of the Sinai Peninsula.\nIn the early morning, seemingly in a eureka moment, he decides that attacking Aqaba from the landward side would be a good solution, since the guns there could not be turned around.\nHere is Lawrence trying to convince Ali about this plan:\n\n\n\n\nLawrence does not inform Brighton of his plans, nor even Feisal. It is Ali who informs Feisal of this enterprise. Clearly, Lawrence does not consider Brighton as a member of his Field (as defined by Csikszentmihalyi in his Creativity Systems Model), but Feisal is a Field Member to Ali.\nApropos, the act of sitting up all night can be seen as the Incubation and Elaboration stages of the 5 Stages of Creativity from Csikszentmihalyi (Preparation, Incubation, Insight, Elaboration, Execution)."
  },
  {
    "objectID": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.html#a-triz-analysis-of-the-plan-to-take-aqaba",
    "href": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.html#a-triz-analysis-of-the-plan-to-take-aqaba",
    "title": "A TRIZ Analysis of Lawrence of Arabia",
    "section": "A TRIZ Analysis of the Plan to Take Aqaba",
    "text": "A TRIZ Analysis of the Plan to Take Aqaba\nFor a TRIZ workflow, we proceed as follows:\nFirst, using the method described in Open Source TRIZ, (https://www.youtube.com/watch?v=cah0OhCH55k), we identify knobs or parameters within the situation and see how turning these could lead to identifying a Cause for a Problem in the form of a Contradiction.\nHere below is a quick Ishikawa Diagram for this purpose:\n\n\n\n\n\n\n\n\nTurning the knobs/parameters in the Ishikawa Diagram, it seems that if the British allies attack Aqaba, they may win, BUT they may lose a few warships. If the Arabs want to attack, they are too small in number and have no warships, and hence their chances of success are very slim. Herein lies the Contradiction, which we can now specify as an Administrative Contradiction(AC) in plain English:\n\n\nAC: The Arabs need the British to supply them via Aqaba port. Aqaba has huge guns and they will sink the British ships in that narrow gulf if they try a naval attack. So the Arabs need to take Aqaba without losing British ships.\n\n\nNext, based on this Contradiction and the inspection of the Ishikawa Diagram above, we are now ready to define the Ideal Final Result:\n\n\nIFR: The Arabs need to attack and take Aqaba port, and the big guns there should have no effect.\n\n\nNote how the tone of this IFR is like a “eat my cake and have it too”. Very typical for IFRs, this impossible-sounding tone!\nLet us take the AC and convert it into a Technical Contradiction(TC). We will look at the 48 TRIZ Parameters in the TRIZ Contradiction Matrix (PDF) and see which Parameter we want to improve, while not worsening another. Here is what we can attempt, stating the Contradiction both ways2:\n\n- TC 1: Increase Duration of Action of a Moving Object (12) and not worsen Stability of Objects Composition (21)- TC 2: Increase Stability of Objects Composition (21) and not worsen Duration of Action of a Moving Object (12)\n\nHere we choose these Parameters based on our IFR that the guns at Aqaba should not affect the Arab attack at all. The Parameters chosen from the TRIZ Matrix can be thought of as metaphors for the knobs that lie within our AC. Going from the AC to the TC is an act of making metaphors. We could easily have chosen the Parameter Power(18) or Illumination Intensity(23) to “metaphorize” the effectiveness of the attack, if our imaginations run in that direction. There is here a considerable flexibility and possibility for imaginative interpretations of the AC, but using the language of TRIZ.\nWe could even stretch to making a Physical Contradiction(PC)3 happen:\n\n\nPC: The Ships must be near the guns but not be near enough to be shot at. (They must be near and not near at the same time)"
  },
  {
    "objectID": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.html#solving-the-technical-contradiction",
    "href": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.html#solving-the-technical-contradiction",
    "title": "A TRIZ Analysis of Lawrence of Arabia",
    "section": "Solving the Technical Contradiction",
    "text": "Solving the Technical Contradiction\nLet us take the both the TC-s into the Contradiction Matrix and arrive at the list of TRIZ Inventive Principles. Here is the Matrix solution for TC-1 in the figure below:\n\n\n\n\n\n\n\n\nThe Inventive Principles are:(TC1)\n\nIP 13 (The Other Way Around)\n\nIP 35 (Parameter Change)\n\nIP 24 (Intermediary)\n\nIP 40 (Composite Materials)\n\nand (TC2)\n\nIP 10 (Preliminary Action)\n\nIP 5 (Merging)\n\nIP 35 (Parameter Change)\n\nIP 13 (The Other Way Around)\n\nHow are we to apply these Inventive Principles? Here again is an imaginative exercise as we map these Generalized Solutions back into the Problem at hand:\n\nIP 13: The Other Way Around. Change the Direction? Of what? Two aspects have “direction” as attributes: the attack, and the guns, which Lawrence can’t control. So how does he use this? Not attack by sea? Wait…ATTACK BY LAND!! Change the DIRECTION of Attack! So attack from the other side, the land side!! (We could retrospectively add this parameter to the Ishikawa Diagram too). Will this work? Yes, the guns can’t turn around!!\nIP 35: Parameter Change. But “ships” on land?? Note, the desert is an ocean into which no oar is dipped. Sand and Water are both Resources in the problem, as we have duly noted in the Ishikawa Diagram. So a different kind of ocean and therefore a different kind of ship? At a stretch, we can say the warships of the British Navy are being substituted with the use of ….Camels!! And, metaphorically speaking, it is still an attack using ships….The Ships of the Desert!! So water becomes sand, and the warships become camels using Parameter Change !\nIP 10: Prior Action. How? Lawrence and Ali are far from Aqaba and cannot do anything “in advance”. What could this be?\nIP 5: Merging. However, on the way to Aqaba, Lawrence and Ali must recruit the Howeitat tribe “in advance” of their attack !! As Lawrence tells Ali, If 50 men came out of the Nefud Desert, they might be 50 men other men would join. This is in accordance with what IP 10 is suggesting, to get other tribes to join in, in advance of the attack.\nIP 40: Composite Materials. What object within the situation can we reconstitute with smaller pieces of different types? The British Army…so an army made up of pieces? Yes! The Tribes need to unite into one composite army.\nAnd, instead of large warships, the Arabs switch to a composite force with camels…\n\nSo IP 13 works nicely now, along with IP 35 and IP 40, to give us a camel-borne attack from the landward side. IP 10 also teams up with IP 40 and IP 5 to give the idea of tribe unification.\nAnd so Lawrence and Ali, with the help of Auda Abu Tayi, attack Aqaba port from the landward side by crossing the Nefud desert on camels, and take it! And we have justified their decision using TRIZ !!\nHere is the final solution in action !!\n\n\n\n\n I hope that was as much fun to read as it was for me to write it up !!"
  },
  {
    "objectID": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.html#points-to-ponder",
    "href": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.html#points-to-ponder",
    "title": "A TRIZ Analysis of Lawrence of Arabia",
    "section": "Points to Ponder",
    "text": "Points to Ponder\n\nDo we each of us need a Dryden to vouch for us and help us get access to the Field?\nDoes TRIZ work in both mundane and industrial contexts? (Yes of course!)\nCan we just take the 40 Inventive Principles directly and throw them at every Problem, without necessarily going through the process of creating Contradictions and IFR? Hipple’s book has a remark in this direction."
  },
  {
    "objectID": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.html#references",
    "href": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.html#references",
    "title": "A TRIZ Analysis of Lawrence of Arabia",
    "section": "References",
    "text": "References\n\nLawrence of Arabia at the Internet Movie Data Base https://www.imdb.com/title/tt0056172/\n\nMihaly Csikszentmihalyi, Creativity, Flow, and the Psychology of Discovery and Invention. Harper Perennial; Reprint edition (August 6, 2013)\nJack Hipple, The Ideal Result and How to Achieve It. Springer; 2012th edition (June 26, 2012)\nValery Souchkov, Defining Contradictions. http://www.xtriz.com/Training/TRIZ_DefineContradiction_Tutorial.pdf"
  },
  {
    "objectID": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.html#footnotes",
    "href": "content/projects/2022-12-20-TRIZ-Lawrence-of-Arabia/index.html#footnotes",
    "title": "A TRIZ Analysis of Lawrence of Arabia",
    "section": "Footnotes",
    "text": "Footnotes\n\nI was not able to ascertain who is the author of this map. I would be happy to write to obtain permission and use it with acknowledgement.↩︎\nThe Contradiction Matrix is not quite symmetric, so stating the Contradiction both ways allows us to access a slightly larger set of Inventive Principles from two cells of the Matrix.↩︎\nArriving at Physical Contradictions is not always easy! If we can, then there are a very crisp set of TRIZ Separation Principles that we can apply to solve the Problem.↩︎"
  },
  {
    "objectID": "content/projects/2023-11-15-TRIZ-Intro/index.html#introduction",
    "href": "content/projects/2023-11-15-TRIZ-Intro/index.html#introduction",
    "title": "TRIZ – An Inventive Problem-Solving Method",
    "section": "\n Introduction",
    "text": "Introduction\n\n\n\n\n\n\n\n\nTRIZ is a system of Inventive Problem Solving created by Genrikh Altshuller. Altshuller, was born in Russia in 1926, made his first invention at age 14 (9th Grade), and was later educated as a mechanical engineer. At the time he started working on TRIZ, in 1946, he was employed in the patent department of the Soviet navy, assisting inventors in filing their patents, in Baku, Azerbaijan. While there he became intrigued by the question of how an invention happens:\n\nIs it a matter of luck? The result of a mental “light bulb” turning on, as in the comics? Or can inventions be seen as the result of systematic patterns of inventive thinking?\n\nAltshuller adopted an empirical approach to answering this question. He studied thousands of patents, looking for commonalities, repetitive patterns, and principles of inventive thought. As he found these, he codified and documented them. His results, when eventually published, attracted many enthusiasts who continued and expanded the work over the years, reviewing what is now estimated to be more than three million patents worldwide. TRIZ is actively used in Companies such as Boeing, Bridgestone, Eastman Kodak, Ford Motor Company, Harley-Davidson Motor Company, Hewlett-Packard, Illinois Tool Works, Inficon, Ingersoll Rand, Kimberly-Clark, L.G. Electronics, Lucent Technologies, Michelin, National Semiconductor, NASA, Philips, Rolls-Royce, Samsung, Siemens, Western Digital, and Xerox, among others.\nAltshuller found that the most inventive of patents did two things:\n\nThey stated PROBLEMS as CONTRADICTIONs (using just 48 unique phrases)\nThese CONTRADICTIONs were resolved across a wide variety of patents using an astonishingly few INVENTIVE PRINCIPLEs. (only 40 in number!)"
  },
  {
    "objectID": "content/projects/2023-11-15-TRIZ-Intro/index.html#there-is-world-of-problems",
    "href": "content/projects/2023-11-15-TRIZ-Intro/index.html#there-is-world-of-problems",
    "title": "TRIZ – An Inventive Problem-Solving Method",
    "section": "\n There is World of Problems!",
    "text": "There is World of Problems!\nLet us take our first step into the world of TRIZ. What did you think of immediately when you saw the first picture on this page? You surely saw the Contradiction: it is graffiti but claiming not to be! In TRIZ, the fundamental way of looking at an Inventive Design Problem is to discover and propose Contradictions. These are rendered in as simple and stark a language as possible…the starker the better!\nWhat sort of Contradictions do we see in these familiar objects below ? What is good and what is not so good? Could that be the source of a problem to solve?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContemplate and note down for each Object: Does it embody a CONTRADICTION?\nYes, each one does, in its own way: The pizza box lid collapses on the pizza when it is hot and spoils the topping. The wrench must be turned by pushing against its narrower edge; we would find it easier and less painful if we could apply force on the broader surface of the handle. And the chain? Everyone know it needs to be stiff and strong to be able to pull the wheel, and yet flexible enough to go around the sprocket…and get horribly entangled, leading to greasy fingers!\nIn this way we look at OBJECTS, PROCESSES, METHODS, PRODUCTS, and indeed CIRCUMSTANCES to find CONTRADICTIONs!\nHere are some more examples in the figure below:\n\n\n\n\n\n\nJuice\n\n\n\n\n\n\n\n\n\n\nSnails\n\n\n\n\n\nWhat could be our Contradictions here?\n\nFresh Juice is good for health, but ads from juice companies wish to portray it as harmful, and they may not be able to sell!\nThe ground is hot for a snail, but above ground it is visible to predators."
  },
  {
    "objectID": "content/projects/2023-11-15-TRIZ-Intro/index.html#appreciate-the-situation",
    "href": "content/projects/2023-11-15-TRIZ-Intro/index.html#appreciate-the-situation",
    "title": "TRIZ – An Inventive Problem-Solving Method",
    "section": "\n Appreciate the Situation",
    "text": "Appreciate the Situation\nWe do not always contemplate only objects; indeed, as inventors, we want to be able to make objects or systems. What we more commonly contemplate is a situation. So how does one assess a situation? We might use what is called an Ishikawa Fishbone Diagram. This is shown below.\n\n\n\n\nIshikawa Fishbone Diagram\n\n\n\nThere are many versions of this diagram depending upon the DOMAIN it is applied in. This diagram is very helpful to us in assessing resources and processes, and watching how their interplay in a situation could lead to…a Contradiction."
  },
  {
    "objectID": "content/projects/2023-11-15-TRIZ-Intro/index.html#determining-the-administrative-contradiction",
    "href": "content/projects/2023-11-15-TRIZ-Intro/index.html#determining-the-administrative-contradiction",
    "title": "TRIZ – An Inventive Problem-Solving Method",
    "section": "\n Determining the Administrative Contradiction",
    "text": "Determining the Administrative Contradiction\nIn the Ishikawa diagram, each of the items listed is considered a TRIZ KNOB, which is either in our control or not. By turning these KNOBs in either direction we can change a specific PARAMETER that the KNOB affects. This change may improve the situation for us but we may find that something else typically gets worse. This is the source of our CONTRADICTION for Situations. When this happens, we can usually state a CONTRADICTION in simple English.\nFor example: We want to boil milk, but we get bored watching it boil.\nHere Time and the Milk would both have been listed in your Ishikawa as Resources. You can further document your analysis of the Object or Situation using the following questionnaire 5W+H format:\n\nWhat does the problem seem to be?\nWho has the problem?\nWhen does the problem occur? All the time? Under certain circumstances?\nWhere does the problem occur?\nWhy does the problem occur? (“Ask why 5 times” – W. Edwards Deming)\nHow does the problem occur?\n\nThis will help in “aiming” the solutions that TRIZ offers, in the right way.\nSTEP1: In TRIZ, this way of expressing a Problem as a simple CONTRADICTION is referred to as stating an ADMINISTRATIVE CONTRADICTION (AC). You should be able to state an Administrative Contradiction in the following (loose!) sentence structure — Items in &lt; &gt; come from the Ishikawa and your 5W + H questions).\nWhen we, as &lt; WHO / MANPOWER &gt;, attempt to perform &lt; HOW / METHOD &gt; during &lt;WHEN&gt; on &lt; WHERE / MACHINERY / KNOB&gt;, we improve &lt;EFFECT&gt;, but lose out on &lt;negative EFFECT / KNOB&gt;"
  },
  {
    "objectID": "content/projects/2023-11-15-TRIZ-Intro/index.html#stating-a-technical-contradiction-tc",
    "href": "content/projects/2023-11-15-TRIZ-Intro/index.html#stating-a-technical-contradiction-tc",
    "title": "TRIZ – An Inventive Problem-Solving Method",
    "section": "\n Stating a Technical Contradiction (TC)",
    "text": "Stating a Technical Contradiction (TC)\nNow that we know how problems can be stated as simple Administrative Contradictions (AC), we need to take the next step and make what TRIZ calls Technical Contradictions (TC). Altshuller found that problems across domains could be expressed in a “TRIZ Language,” a set of metaphoric phrases that are an integral part of (classical) TRIZ. We will call these the 48 TRIZ Parameters. Some examples of TRIZ Parameters: Weight of a stationary Object, Loss of Substance, and Temperature.\nEvery problem could be described as a contradiction using some pair of these 48 parameters. These metaphoric phrases are simple enough and provide rich troves for imaginative problem solving. Expressing our specific problem in this way allowed us to see the similarity it has with problems in other domains and helps us to leverage solutions from there.\nSTEP 2: Take the AC and state it in terms of these 48 TRIZ Parameters, for example: Improve Loss of Substance and not worsen Weight of a Stationary Object ; Improve Loss of Information and not worsen Power"
  },
  {
    "objectID": "content/projects/2023-11-15-TRIZ-Intro/index.html#using-the-triz-contradiction-matrix",
    "href": "content/projects/2023-11-15-TRIZ-Intro/index.html#using-the-triz-contradiction-matrix",
    "title": "TRIZ – An Inventive Problem-Solving Method",
    "section": "\n Using the TRIZ Contradiction Matrix",
    "text": "Using the TRIZ Contradiction Matrix\nArmed with our TCs, we plug them into the TRIZ Contradiction Matrix. The TRIZ Matrix is a 48 X 48 structure, with every possible TRIZ Parameter being paired with every other TRIZ Parameter.\nSTEP3: One chooses one TRIZ Parameter from the TC as the ROW and the other as the COLUMN in the TRIZ Matrix. At their intersection lies a single cell which contains one or more TRIZ Inventive Principles. These Inventive Principles have been derived as solutions from hundreds and thousands of patents.\nSTEP4: These Inventive Principles must now be applied into our Problem to solve it. This is the TRIZ Contradiction Matrix Workflow."
  },
  {
    "objectID": "content/projects/2023-11-15-TRIZ-Intro/index.html#a-complete-example",
    "href": "content/projects/2023-11-15-TRIZ-Intro/index.html#a-complete-example",
    "title": "TRIZ – An Inventive Problem-Solving Method",
    "section": "\n A Complete Example",
    "text": "A Complete Example\nLet us consider the example of the pizza box that we saw at the start of this article. (We will not trouble to make the Ishikawa for this simple problem) Here is our AC:\nAC: The pizza needs to be hot but the steam it gives off must not make the lid collapse and ruin the pizza\nWe can convert this into a TC by choosing several pairs of TRIZ Parameters:\nTC: Improve 21(Temperature) while not worsening 22(Stability)!\nLooking up the TRIZ Contradiction Matrix(Row#21 Col#22), we get the TRIZ Inventive Principles: 24: Intermediary. 35: Parameter Change, 32: Colour Change, 3: Local Quality.\nLooking at IP 24 (Intermediary) we need to think of something between the pizza and the lid, and IP 3 ( Local Quality) suggests that it should we quite small, or “local” compared to the size of the pizza! What could that be? This!\n\n\n\n\nPizza Saver or Pizza Table"
  },
  {
    "objectID": "content/projects/2023-11-15-TRIZ-Intro/index.html#conclusion",
    "href": "content/projects/2023-11-15-TRIZ-Intro/index.html#conclusion",
    "title": "TRIZ – An Inventive Problem-Solving Method",
    "section": "\n Conclusion",
    "text": "Conclusion\nThe TRIZ process allows us to leverage solutions that have been obtained from a vast number of patents. The TRIZ language allows us to access these solutions by expressing our specific problem in terms of the TRIZ Parameters, and leads us to the relevant TRIZ Inventive Principles which can solve our problem!\nWorth mastering!!"
  },
  {
    "objectID": "content/projects/2023-11-15-TRIZ-Intro/index.html#references",
    "href": "content/projects/2023-11-15-TRIZ-Intro/index.html#references",
    "title": "TRIZ – An Inventive Problem-Solving Method",
    "section": "\n References",
    "text": "References\n\nOpen Source TRIZ. &lt;TRIZ PowerTools - Free downloads ebooks pdfs teaching materials (opensourcetriz.com)&gt;\nJack Hipple’s Webpage.https://innovation-triz.com/personnel/JHipple.html\nValeri Souchkov’s Webpage.http://www.xtriz.com/ValeriSouchkov.htm"
  },
  {
    "objectID": "content/projects/Rmd_Project/index.html",
    "href": "content/projects/Rmd_Project/index.html",
    "title": "Rmd Project for Quarto Website",
    "section": "",
    "text": "This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00"
  },
  {
    "objectID": "content/projects/Rmd_Project/index.html#r-markdown",
    "href": "content/projects/Rmd_Project/index.html#r-markdown",
    "title": "Rmd Project for Quarto Website",
    "section": "",
    "text": "This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00"
  },
  {
    "objectID": "content/projects/Rmd_Project/index.html#including-plots",
    "href": "content/projects/Rmd_Project/index.html#including-plots",
    "title": "Rmd Project for Quarto Website",
    "section": "Including Plots",
    "text": "Including Plots\nYou can also embed plots, for example:\n\n\n\n\n\n\n\n\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot."
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "",
    "text": "This is an attempt to rework , using tidygraph and ggraph, much of Network Visualization with R Polnet 2018 Workshop Tutorial, Washington, DC by Prof. Katherine (Katya) Ognyanova.\nThe aim is to get a working acquaintance with both these packages and also to appreciate some of the concepts in Networks. My code is by no means intended to be elegant; it merely works and there are surely many improvements that people may think of!\nI have attempted to write code for the Sections 2:5. I have retained Prof. Ognyanova’s text in all places."
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#introduction",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#introduction",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "",
    "text": "This is an attempt to rework , using tidygraph and ggraph, much of Network Visualization with R Polnet 2018 Workshop Tutorial, Washington, DC by Prof. Katherine (Katya) Ognyanova.\nThe aim is to get a working acquaintance with both these packages and also to appreciate some of the concepts in Networks. My code is by no means intended to be elegant; it merely works and there are surely many improvements that people may think of!\nI have attempted to write code for the Sections 2:5. I have retained Prof. Ognyanova’s text in all places."
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#contents",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#contents",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "CONTENTS",
    "text": "CONTENTS\n\nWorking with colors in R plots\nReading in the network data\nNetwork plots in ‘igraph’\nPlotting two-mode networks\nPlotting multiplex networks\nQuick example using ‘network’\nSimple plot animations in R\nInteractive JavaScript networks\nInteractive and dynamic networks with ndtv-d3\nPlotting networks on a geographic map"
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#dataset-1-edgelist--",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#dataset-1-edgelist--",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "——-~~ DATASET 1: edgelist ~~——-",
    "text": "——-~~ DATASET 1: edgelist ~~——-\n\n# Read in the data:\nnodes &lt;- read.csv(\"./Dataset1-Media-Example-NODES.csv\", header = T, as.is = T)\nlinks &lt;- read.csv(\"./Dataset1-Media-Example-EDGES.csv\", header = T, as.is = T)\n\n\n# Examine the data:\nhead(nodes)\n\n\n  \n\n\nhead(links)\n\n\n  \n\n\n\nConverting the data to an igraph object:\nThe graph_from_data_frame() function takes two data frames: ‘d’ and ‘vertices’. - ‘d’ describes the edges of the network - it should start with two columns containing the source and target node IDs for each network tie. - ‘vertices’ should start with a column of node IDs. It can be omitted. - Any additional columns in either data frame are interpreted as attributes.\nNOTE: ID columns need not be numbers or integers!!\n\nnet &lt;- graph_from_data_frame(d = links, vertices = nodes, directed = T)\n\n# Examine the resulting object:\nclass(net)\n\n[1] \"igraph\"\n\nnet\n\nIGRAPH 57b939e DNW- 17 49 -- \n+ attr: name (v/c), media (v/c), media.type (v/n), type.label (v/c),\n| audience.size (v/n), type (e/c), weight (e/n)\n+ edges from 57b939e (vertex names):\n [1] s01-&gt;s02 s01-&gt;s03 s01-&gt;s04 s01-&gt;s15 s02-&gt;s01 s02-&gt;s03 s02-&gt;s09 s02-&gt;s10\n [9] s03-&gt;s01 s03-&gt;s04 s03-&gt;s05 s03-&gt;s08 s03-&gt;s10 s03-&gt;s11 s03-&gt;s12 s04-&gt;s03\n[17] s04-&gt;s06 s04-&gt;s11 s04-&gt;s12 s04-&gt;s17 s05-&gt;s01 s05-&gt;s02 s05-&gt;s09 s05-&gt;s15\n[25] s06-&gt;s06 s06-&gt;s16 s06-&gt;s17 s07-&gt;s03 s07-&gt;s08 s07-&gt;s10 s07-&gt;s14 s08-&gt;s03\n[33] s08-&gt;s07 s08-&gt;s09 s09-&gt;s10 s10-&gt;s03 s12-&gt;s06 s12-&gt;s13 s12-&gt;s14 s13-&gt;s12\n[41] s13-&gt;s17 s14-&gt;s11 s14-&gt;s13 s15-&gt;s01 s15-&gt;s04 s15-&gt;s06 s16-&gt;s06 s16-&gt;s17\n[49] s17-&gt;s04\n\n\nThe description of an igraph object starts with four letters:\n- D or U, for a directed or undirected graph - N for a named graph (where nodes have a name attribute) - W for a weighted graph (where edges have a weight attribute) -B for a bipartite (two-mode) graph (where nodes have a type attribute) The two numbers that follow (17 49) refer to the number of nodes and edges in the graph. The description also lists node & edge attributes.\nWe can access the nodes, edges, and their attributes:\n\nE(net)\n\n+ 49/49 edges from 57b939e (vertex names):\n [1] s01-&gt;s02 s01-&gt;s03 s01-&gt;s04 s01-&gt;s15 s02-&gt;s01 s02-&gt;s03 s02-&gt;s09 s02-&gt;s10\n [9] s03-&gt;s01 s03-&gt;s04 s03-&gt;s05 s03-&gt;s08 s03-&gt;s10 s03-&gt;s11 s03-&gt;s12 s04-&gt;s03\n[17] s04-&gt;s06 s04-&gt;s11 s04-&gt;s12 s04-&gt;s17 s05-&gt;s01 s05-&gt;s02 s05-&gt;s09 s05-&gt;s15\n[25] s06-&gt;s06 s06-&gt;s16 s06-&gt;s17 s07-&gt;s03 s07-&gt;s08 s07-&gt;s10 s07-&gt;s14 s08-&gt;s03\n[33] s08-&gt;s07 s08-&gt;s09 s09-&gt;s10 s10-&gt;s03 s12-&gt;s06 s12-&gt;s13 s12-&gt;s14 s13-&gt;s12\n[41] s13-&gt;s17 s14-&gt;s11 s14-&gt;s13 s15-&gt;s01 s15-&gt;s04 s15-&gt;s06 s16-&gt;s06 s16-&gt;s17\n[49] s17-&gt;s04\n\nV(net)\n\n+ 17/17 vertices, named, from 57b939e:\n [1] s01 s02 s03 s04 s05 s06 s07 s08 s09 s10 s11 s12 s13 s14 s15 s16 s17\n\nE(net)$type\n\n [1] \"hyperlink\" \"hyperlink\" \"hyperlink\" \"mention\"   \"hyperlink\" \"hyperlink\"\n [7] \"hyperlink\" \"hyperlink\" \"hyperlink\" \"hyperlink\" \"hyperlink\" \"hyperlink\"\n[13] \"mention\"   \"hyperlink\" \"hyperlink\" \"hyperlink\" \"mention\"   \"mention\"  \n[19] \"hyperlink\" \"mention\"   \"mention\"   \"hyperlink\" \"hyperlink\" \"mention\"  \n[25] \"hyperlink\" \"hyperlink\" \"mention\"   \"mention\"   \"mention\"   \"hyperlink\"\n[31] \"mention\"   \"hyperlink\" \"mention\"   \"mention\"   \"mention\"   \"hyperlink\"\n[37] \"mention\"   \"hyperlink\" \"mention\"   \"hyperlink\" \"mention\"   \"mention\"  \n[43] \"mention\"   \"hyperlink\" \"hyperlink\" \"hyperlink\" \"hyperlink\" \"mention\"  \n[49] \"hyperlink\"\n\nV(net)$media\n\n [1] \"NY Times\"            \"Washington Post\"     \"Wall Street Journal\"\n [4] \"USA Today\"           \"LA Times\"            \"New York Post\"      \n [7] \"CNN\"                 \"MSNBC\"               \"FOX News\"           \n[10] \"ABC\"                 \"BBC\"                 \"Yahoo News\"         \n[13] \"Google News\"         \"Reuters.com\"         \"NYTimes.com\"        \n[16] \"WashingtonPost.com\"  \"AOL.com\"            \n\n\n\n# Using tidygraph\ntbl_graph(nodes, links, directed = TRUE) %&gt;% \n  activate(edges) %&gt;% \n  select(type)\n\n# A tbl_graph: 17 nodes and 49 edges\n#\n# A directed multigraph with 1 component\n#\n# Edge Data: 49 × 3 (active)\n    from    to type     \n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;    \n 1     1     2 hyperlink\n 2     1     3 hyperlink\n 3     1     4 hyperlink\n 4     1    15 mention  \n 5     2     1 hyperlink\n 6     2     3 hyperlink\n 7     2     9 hyperlink\n 8     2    10 hyperlink\n 9     3     1 hyperlink\n10     3     4 hyperlink\n# ℹ 39 more rows\n#\n# Node Data: 17 × 5\n  id    media               media.type type.label audience.size\n  &lt;chr&gt; &lt;chr&gt;                    &lt;int&gt; &lt;chr&gt;              &lt;int&gt;\n1 s01   NY Times                     1 Newspaper             20\n2 s02   Washington Post              1 Newspaper             25\n3 s03   Wall Street Journal          1 Newspaper             30\n# ℹ 14 more rows\n\ntbl_graph(nodes, links, directed = TRUE) %&gt;% \n  activate(nodes) %&gt;% \n  select(media)\n\n# A tbl_graph: 17 nodes and 49 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 17 × 1 (active)\n   media              \n   &lt;chr&gt;              \n 1 NY Times           \n 2 Washington Post    \n 3 Wall Street Journal\n 4 USA Today          \n 5 LA Times           \n 6 New York Post      \n 7 CNN                \n 8 MSNBC              \n 9 FOX News           \n10 ABC                \n11 BBC                \n12 Yahoo News         \n13 Google News        \n14 Reuters.com        \n15 NYTimes.com        \n16 WashingtonPost.com \n17 AOL.com            \n#\n# Edge Data: 49 × 4\n   from    to type      weight\n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;int&gt;\n1     1     2 hyperlink     22\n2     1     3 hyperlink     22\n3     1     4 hyperlink     21\n# ℹ 46 more rows\n\n\nOr find specific nodes and edges by attribute:(that returns objects of type vertex sequence / edge sequence)\n\nV(net)[media == \"BBC\"]\n\n+ 1/17 vertex, named, from 57b939e:\n[1] s11\n\nE(net)[type == \"mention\"]\n\n+ 20/49 edges from 57b939e (vertex names):\n [1] s01-&gt;s15 s03-&gt;s10 s04-&gt;s06 s04-&gt;s11 s04-&gt;s17 s05-&gt;s01 s05-&gt;s15 s06-&gt;s17\n [9] s07-&gt;s03 s07-&gt;s08 s07-&gt;s14 s08-&gt;s07 s08-&gt;s09 s09-&gt;s10 s12-&gt;s06 s12-&gt;s14\n[17] s13-&gt;s17 s14-&gt;s11 s14-&gt;s13 s16-&gt;s17\n\n\n\n#Using tidygraph\ntbl_graph(nodes, links, directed = TRUE) %&gt;% \n  activate(nodes) %&gt;% \n  filter(media == \"BBC\")\n\n# A tbl_graph: 1 nodes and 0 edges\n#\n# A rooted tree\n#\n# Node Data: 1 × 5 (active)\n  id    media media.type type.label audience.size\n  &lt;chr&gt; &lt;chr&gt;      &lt;int&gt; &lt;chr&gt;              &lt;int&gt;\n1 s11   BBC            2 TV                    34\n#\n# Edge Data: 0 × 4\n# ℹ 4 variables: from &lt;int&gt;, to &lt;int&gt;, type &lt;chr&gt;, weight &lt;int&gt;\n\ntbl_graph(nodes, links, directed = TRUE) %&gt;% \n  activate(edges) %&gt;% \n  filter(type == \"mention\")\n\n# A tbl_graph: 17 nodes and 20 edges\n#\n# A directed simple graph with 3 components\n#\n# Edge Data: 20 × 4 (active)\n    from    to type    weight\n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;int&gt;\n 1     1    15 mention     20\n 2     3    10 mention      2\n 3     4     6 mention      1\n 4     4    11 mention     22\n 5     4    17 mention      2\n 6     5     1 mention      1\n 7     5    15 mention     21\n 8     6    17 mention     21\n 9     7     3 mention      1\n10     7     8 mention     22\n11     7    14 mention      4\n12     8     7 mention     21\n13     8     9 mention     23\n14     9    10 mention     21\n15    12     6 mention      2\n16    12    14 mention     22\n17    13    17 mention      1\n18    14    11 mention      1\n19    14    13 mention     21\n20    16    17 mention     21\n#\n# Node Data: 17 × 5\n  id    media               media.type type.label audience.size\n  &lt;chr&gt; &lt;chr&gt;                    &lt;int&gt; &lt;chr&gt;              &lt;int&gt;\n1 s01   NY Times                     1 Newspaper             20\n2 s02   Washington Post              1 Newspaper             25\n3 s03   Wall Street Journal          1 Newspaper             30\n# ℹ 14 more rows\n\n\nIf you need them, you can extract an edge list or a matrix back from the igraph networks.\n\nas_edgelist(net, names = T)\n\n      [,1]  [,2] \n [1,] \"s01\" \"s02\"\n [2,] \"s01\" \"s03\"\n [3,] \"s01\" \"s04\"\n [4,] \"s01\" \"s15\"\n [5,] \"s02\" \"s01\"\n [6,] \"s02\" \"s03\"\n [7,] \"s02\" \"s09\"\n [8,] \"s02\" \"s10\"\n [9,] \"s03\" \"s01\"\n[10,] \"s03\" \"s04\"\n[11,] \"s03\" \"s05\"\n[12,] \"s03\" \"s08\"\n[13,] \"s03\" \"s10\"\n[14,] \"s03\" \"s11\"\n[15,] \"s03\" \"s12\"\n[16,] \"s04\" \"s03\"\n[17,] \"s04\" \"s06\"\n[18,] \"s04\" \"s11\"\n[19,] \"s04\" \"s12\"\n[20,] \"s04\" \"s17\"\n[21,] \"s05\" \"s01\"\n[22,] \"s05\" \"s02\"\n[23,] \"s05\" \"s09\"\n[24,] \"s05\" \"s15\"\n[25,] \"s06\" \"s06\"\n[26,] \"s06\" \"s16\"\n[27,] \"s06\" \"s17\"\n[28,] \"s07\" \"s03\"\n[29,] \"s07\" \"s08\"\n[30,] \"s07\" \"s10\"\n[31,] \"s07\" \"s14\"\n[32,] \"s08\" \"s03\"\n[33,] \"s08\" \"s07\"\n[34,] \"s08\" \"s09\"\n[35,] \"s09\" \"s10\"\n[36,] \"s10\" \"s03\"\n[37,] \"s12\" \"s06\"\n[38,] \"s12\" \"s13\"\n[39,] \"s12\" \"s14\"\n[40,] \"s13\" \"s12\"\n[41,] \"s13\" \"s17\"\n[42,] \"s14\" \"s11\"\n[43,] \"s14\" \"s13\"\n[44,] \"s15\" \"s01\"\n[45,] \"s15\" \"s04\"\n[46,] \"s15\" \"s06\"\n[47,] \"s16\" \"s06\"\n[48,] \"s16\" \"s17\"\n[49,] \"s17\" \"s04\"\n\nas_adjacency_matrix(net, attr = \"weight\")\n\n17 x 17 sparse Matrix of class \"dgCMatrix\"\n                                                     \ns01  . 22 22 21 .  .  .  .  .  .  .  .  .  . 20  .  .\ns02 23  . 21  . .  .  .  .  1  5  .  .  .  .  .  .  .\ns03 21  .  . 22 1  .  .  4  .  2  1  1  .  .  .  .  .\ns04  .  . 23  . .  1  .  .  .  . 22  3  .  .  .  .  2\ns05  1 21  .  . .  .  .  .  2  .  .  .  .  . 21  .  .\ns06  .  .  .  . .  1  .  .  .  .  .  .  .  .  . 21 21\ns07  .  .  1  . .  .  . 22  . 21  .  .  .  4  .  .  .\ns08  .  .  2  . .  . 21  . 23  .  .  .  .  .  .  .  .\ns09  .  .  .  . .  .  .  .  . 21  .  .  .  .  .  .  .\ns10  .  .  2  . .  .  .  .  .  .  .  .  .  .  .  .  .\ns11  .  .  .  . .  .  .  .  .  .  .  .  .  .  .  .  .\ns12  .  .  .  . .  2  .  .  .  .  .  . 22 22  .  .  .\ns13  .  .  .  . .  .  .  .  .  .  . 21  .  .  .  .  1\ns14  .  .  .  . .  .  .  .  .  .  1  . 21  .  .  .  .\ns15 22  .  .  1 .  4  .  .  .  .  .  .  .  .  .  .  .\ns16  .  .  .  . . 23  .  .  .  .  .  .  .  .  .  . 21\ns17  .  .  .  4 .  .  .  .  .  .  .  .  .  .  .  .  .\n\n# Using tidygraph\n# No direct command seems available ...\n\n\n# Or data frames describing nodes and edges:\nigraph::as_data_frame(x = net, what = \"edges\")\n\n\n  \n\n\nigraph::as_data_frame(x = net, what = \"vertices\")\n\n\n  \n\n\n#Using tidygraph\ntbl_graph(nodes, links, directed = TRUE) %&gt;% \n  activate(nodes) %&gt;% \n  as_tibble()\n\n\n  \n\n\ntbl_graph(nodes, links, directed = TRUE)%&gt;% \n  activate(edges) %&gt;% \n  as_tibble()\n\n\n  \n\n\n\n\n# You can also access the network matrix directly:\nnet[1,]\n\ns01 s02 s03 s04 s05 s06 s07 s08 s09 s10 s11 s12 s13 s14 s15 s16 s17 \n  0  22  22  21   0   0   0   0   0   0   0   0   0   0  20   0   0 \n\nnet[5,7]\n\n[1] 0\n\n# Using tidygraph\n# Does not seem possible, even with `as.matrix()`.\n# Returns tibbles only as in the code chunk above\n\n\n# First attempt to plot the graph:\nplot(net) # not pretty!\n\n\n\n\n\n\n# Removing loops from the graph:\nnet &lt;-\n  igraph::simplify(net, remove.multiple = F, remove.loops = T)\n\n# Let's and reduce the arrow size and remove the labels:\nplot(net, edge.arrow.size = .4, vertex.label = NA)\n\n\n\n\n\n\n\n\n# Using tidygraph\ntbl_graph(nodes, links, directed = TRUE) %&gt;%\n  ggraph(., layout = \"graphopt\") +\n  geom_edge_link(\n    color = \"grey\",\n    end_cap = circle(0.2, \"cm\"),\n    start_cap = circle(0.2, \"cm\"),\n    # clears an area near the node\n    \n    arrow = arrow(\n      type = \"closed\",\n      ends = \"last\",\n      length = unit(3, \"mm\")\n    )\n  ) +\n  geom_node_point(size = 8, shape = 21, fill = \"orange\") +\n  geom_node_text(aes(label = id), size = 3)\n\n\n\n\n\n\n# Removing loops from the graph:\n# From the docs:\n# convert() is a shorthand for performing both `morph` and `crystallise` along with extracting a single tbl_graph (defaults to the first). For morphs w(h)ere you know they only create a single graph, and you want to keep it, this is an easy way.\n#\ntbl_graph(nodes, links, directed = TRUE) %&gt;%\n\n  convert(to_simple) %&gt;%\n  ggraph(., layout = \"graphopt\") +\n  geom_edge_link(\n    color = \"grey\",\n    end_cap = circle(0.2, \"cm\"),\n    start_cap = circle(0.2, \"cm\"),\n \n    arrow = arrow(\n      type = \"closed\",\n      ends = \"last\",\n      length = unit(3, \"mm\")\n    )\n  ) +\n  geom_node_point(size = 6, shape = 21, fill = \"orange\")"
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#dataset-2-matrix",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#dataset-2-matrix",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "——-~~ DATASET 2: matrix ——–",
    "text": "——-~~ DATASET 2: matrix ——–\n\n# Read in the data:\nnodes2 &lt;- read.csv(\"./Dataset2-Media-User-Example-NODES.csv\", header = T, as.is = T)\nlinks2 &lt;- read.csv(\"./Dataset2-Media-User-Example-EDGES.csv\", header = T, row.names = 1)\n\n# Examine the data:\nhead(nodes2)\n\n\n  \n\n\nhead(links2)\n\n\n  \n\n\n# links2 is a matrix for a two-mode network:\nlinks2 &lt;- as.matrix(links2)\ndim(links2)\n\n[1] 10 20\n\ndim(nodes2)\n\n[1] 30  5\n\n\nNote: What is a two-mode network? A network that as a node$type variable and can be a bipartite or a k-partite network as a result.\n\n# Create an igraph network object from the two-mode matrix:\nnet2 &lt;- igraph::graph_from_incidence_matrix(links2)\n\n# To transform a one-mode network matrix into an igraph object,\n# we would use graph_from_adjacency_matrix()\n\n# A built-in vertex attribute 'type' shows which mode vertices belong to.\ntable(V(net2)$type)\n\n\nFALSE  TRUE \n   10    20 \n\n# Basic igraph plot\nplot(net2,vertex.label = NA)\n\n\n\n\n\n\n\n\n# using tidygraph\n# For all objects that are not node and edge data_frames\n# tidygraph uses `as_tbl_graph()`\n# \ngraph &lt;- as_tbl_graph(links2)\ngraph %&gt;% activate(nodes) %&gt;% as_tibble()\n\n\n  \n\n\ngraph %&gt;% activate(edges) %&gt;% as_tibble()\n\n\n  \n\n\ngraph %&gt;% \n  ggraph(., layout = \"graphopt\") + \n  geom_edge_link(color = \"grey\") + \n  geom_node_point(fill = \"orange\", \n                  shape = 21, size = 6, \n                  color = \"black\")\n\n\n\n\n\n\n\n\n# Examine the resulting object:\nclass(net2)\n\n[1] \"igraph\"\n\nnet2\n\nIGRAPH 01225d0 UN-B 30 31 -- \n+ attr: type (v/l), name (v/c)\n+ edges from 01225d0 (vertex names):\n [1] s01--U01 s01--U02 s01--U03 s02--U04 s02--U05 s02--U20 s03--U06 s03--U07\n [9] s03--U08 s03--U09 s04--U09 s04--U10 s04--U11 s05--U11 s05--U12 s05--U13\n[17] s06--U13 s06--U14 s06--U17 s07--U14 s07--U15 s07--U16 s08--U16 s08--U17\n[25] s08--U18 s08--U19 s09--U06 s09--U19 s09--U20 s10--U01 s10--U11\n\n\nNote: The remaining attributes for the nodes ( in data frame nodes2) are not (yet) a part of the graph, either with igraph or with tidygraph."
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#plot-parameters-in-igraph",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#plot-parameters-in-igraph",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "——~~ Plot parameters in igraph ——–",
    "text": "——~~ Plot parameters in igraph ——–\nCheck out the node options (starting with ‘vertex.’) and the edge options (starting with ‘edge.’).Type ?igraph.plotting in your console\n\n?igraph.plotting\n\nWe can set the node & edge options in two ways - one is to specify them in the plot() function, as we are doing below.\n\nPlot with curved edges (edge.curved = .1) and reduce arrow size:\n\n\nplot(net, edge.arrow.size = .4, edge.curved = .1)\n\n\n\n\n\n\n# Using tidygraph\ngraph &lt;- tbl_graph(nodes, links, directed = TRUE)\ngraph\n\n# A tbl_graph: 17 nodes and 49 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 17 × 5 (active)\n   id    media               media.type type.label audience.size\n   &lt;chr&gt; &lt;chr&gt;                    &lt;int&gt; &lt;chr&gt;              &lt;int&gt;\n 1 s01   NY Times                     1 Newspaper             20\n 2 s02   Washington Post              1 Newspaper             25\n 3 s03   Wall Street Journal          1 Newspaper             30\n 4 s04   USA Today                    1 Newspaper             32\n 5 s05   LA Times                     1 Newspaper             20\n 6 s06   New York Post                1 Newspaper             50\n 7 s07   CNN                          2 TV                    56\n 8 s08   MSNBC                        2 TV                    34\n 9 s09   FOX News                     2 TV                    60\n10 s10   ABC                          2 TV                    23\n11 s11   BBC                          2 TV                    34\n12 s12   Yahoo News                   3 Online                33\n13 s13   Google News                  3 Online                23\n14 s14   Reuters.com                  3 Online                12\n15 s15   NYTimes.com                  3 Online                24\n16 s16   WashingtonPost.com           3 Online                28\n17 s17   AOL.com                      3 Online                33\n#\n# Edge Data: 49 × 4\n   from    to type      weight\n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;int&gt;\n1     1     2 hyperlink     22\n2     1     3 hyperlink     22\n3     1     4 hyperlink     21\n# ℹ 46 more rows\n\ngraph %&gt;% ggraph(., layout = \"graphopt\") +\n  geom_edge_arc(\n    color = \"grey\",\n    strength = 0.1,\n    end_cap = circle(.2, \"cm\"),\n\n    arrow = arrow(\n      type = \"closed\",\n      ends = \"both\",\n      length = unit(3, \"mm\")\n    )\n  ) +\n  geom_node_point(\n    fill = \"orange\",\n    shape = 21,\n    size = 8,\n    color = \"black\"\n  ) +\n  geom_node_text(aes(label = id), size  = 3)\n\n\n\n\n\n\n\n\nSet node color to orange and the border color to hex 555555\nReplace the vertex label with the node names stored in “media”\n\n\nplot(\n  net,\n  edge.arrow.size = .2,\n  edge.curved = 0,\n  vertex.color = \"orange\",\n  vertex.frame.color = \"#555555\",\n  vertex.label = V(net)$media,\n  vertex.label.color = \"black\",\n  vertex.label.cex = .7\n)\n\n\n\n\n\n\n# Using tidygraph\n#graph &lt;- tbl_graph(nodes, links, directed = TRUE)\n#graph\ngraph %&gt;%\n  ggraph(., layout = \"gem\") +\n  geom_edge_link(\n    color = \"grey\",\n    end_cap = circle(.3, \"cm\"),\n    \n    arrow = arrow(\n      type = \"closed\",\n      ends = \"both\",\n      length = unit(1, \"mm\")\n    )\n  ) +\n  geom_node_point(\n    fill = \"orange\",\n    shape = 21,\n    size = 6,\n    color = \"#555555\"\n  ) +\n  geom_node_text(aes(label = media))\n\n\n\n\n\n\n\nThe second way to set attributes is to add them to the igraph object.\n\nGenerate colors based on media type:\n\n\ncolrs &lt;- c(\"gray50\", \"tomato\", \"gold\")\nV(net)$color &lt;- colrs[V(net)$media.type]\nplot(net)\n\n\n\n\n\n\n\n\nCompute node degrees (#links) and use that to set node size:\n\n\ndeg &lt;- igraph::degree(net, mode = \"all\")\nV(net)$size &lt;- deg*3\n# Alternatively, we can set node size based on audience size:\nV(net)$size &lt;- V(net)$audience.size*0.7\nV(net)$size\n\n [1] 14.0 17.5 21.0 22.4 14.0 35.0 39.2 23.8 42.0 16.1 23.8 23.1 16.1  8.4 16.8\n[16] 19.6 23.1\n\n# The labels are currently node IDs.\n# Setting them to NA will render no labels:\nV(net)$label.color &lt;- \"black\"\nV(net)$label &lt;- NA\n\n# Set edge width based on weight:\nE(net)$width &lt;- E(net)$weight/6\n\n#change arrow size and edge color:\nE(net)$arrow.size &lt;- .2\nE(net)$edge.color &lt;- \"gray80\"\n\n# We can even set the network layout:\ngraph_attr(net, \"layout\") &lt;- layout_with_lgl\nplot(net)\n\n\n\n\n\n\n\n\n# Using tidygraph\n# graph &lt;- tbl_graph(nodes, links, directed = TRUE)\n# graph\ngraph %&gt;%\n  activate(nodes) %&gt;%\n  mutate(size = centrality_degree()) %&gt;%\n  ggraph(., layout = \"lgl\") +\n  geom_edge_link(\n    aes(width = weight),\n    color = \"grey80\",\n    end_cap = circle(.2, \"cm\"),\n    arrow = arrow(\n      type = \"closed\",\n      ends = \"last\",\n      length = unit(1, \"mm\")\n    )\n  ) +\n  geom_node_point(aes(fill = type.label, size = size),\n                  shape = 21,\n                  color = \"black\") +\n  scale_fill_manual(name = \"Media Type\",\n                    values = c(\"grey50\", \"gold\", \"tomato\")) +\n  scale_edge_width(range = c(0.2, 1.5), guide = \"none\") +\n  scale_size_continuous(\"Degree\", range =  c(2, 16)) +\n  \n  guides(fill = guide_legend(title = \"Media Type\",\n                             override.aes = list(pch = 21, size = 4)))\n\n\n\n\n\n\n\nWe can also override the attributes explicitly in the plot:\n\nplot(net, edge.color = \"orange\", vertex.color = \"gray50\")\n\n\n\n\n\n\n\nWe can also add a legend explaining the meaning of the colors we used:\n\nplot(net)\nlegend(x = -2.1, y = -1.1, \n       c(\"Newspaper\",\"Television\", \"Online News\"), \n       pch = 21,col = \"#777777\", \n       pt.bg = colrs, pt.cex = 2.5, bty = \"n\", ncol = 1)\n\n\n\n\n\n\n# legends are automatic with the tidygraph + ggraph flow\n\nSometimes, especially with semantic networks, we may be interested in plotting only the labels of the nodes:\n\nplot(net, vertex.shape = \"none\", vertex.label = V(net)$media,\n     vertex.label.font = 2, vertex.label.color = \"gray40\",\n     vertex.label.cex = .7, edge.color = \"gray85\")\n\n\n\n\n\n\n#using tidygraph\n\nggraph(net, layout = \"gem\") +\n  geom_edge_link(color = \"grey80\", width = 2,\n                 end_cap = circle(0.5,\"cm\"), \n                 start_cap = circle(0.5, \"cm\")) +\n    geom_node_text(aes(label = media))\n\n\n\n\n\n\n\nLet’s color the edges of the graph based on their source node color. We’ll get the starting node for each edge with ends().\nNote: Edge attribute is being set by start node.\n\nedge.start &lt;- ends(net, es = E(net), names = F)[,1]\nedge.col &lt;- V(net)$color[edge.start] # How simple this is !!!\n# The three colors are recycled \n# \nplot(net, edge.color = edge.col, edge.curved = .4)\n\n\n\n\n\n\n\nNOTE: The source node colour has been set using the media.type, which is a node attribute. Node attributes are not typically accessible to edges. So we need to build a combo data frame using dplyr, so that edges can use this node attribute. ( There may be other ways…)\n\n# Using tidygraph\n# Make a \"combo\" data frame of nodes *and* edges with left_join()\n# Join by `from` so that type.label is based on from = edge.start\n\nlinks %&gt;%\n  left_join(., nodes, by = c(\"from\" = \"id\")) %&gt;%\n  tbl_graph(edges = ., nodes = nodes) %&gt;%\n  \n  mutate(size = centrality_degree()) %&gt;%\n  \n  ggraph(., layout = \"lgl\") +\n  geom_edge_arc(aes(color = type.label,\n                    width = weight),\n                strength = 0.3)  +\n  geom_node_point(aes(fill = type.label,\n                      # type.label is now available as edge attribute\n                      size = size),\n                  shape = 21,\n                  color = \"black\") +\n  scale_fill_manual(\n    name = \"Media Type\",\n    values = c(\"grey50\", \"gold\", \"tomato\"),\n    guide = \"legend\"\n  ) +\n  scale_edge_color_manual(name = \"Source Type\",\n                          values = c(\"grey80\", \"gold\", \"tomato\")) +\n  scale_edge_width(range = c(0.2, 1.5), guide = \"none\") +\n  scale_size_continuous(\"Degree\", range =  c(2, 16)) +\n  # not \"limits\"!\n  guides(fill = guide_legend(override.aes = list(pch = 21,\n                                                 size = 4)))"
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#network-layouts-in-igraph",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#network-layouts-in-igraph",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "——-~~ Network Layouts in ‘igraph’ ——–",
    "text": "——-~~ Network Layouts in ‘igraph’ ——–\nNetwork layouts are algorithms that return coordinates for each node in a network.\nLet’s generate a slightly larger 100-node graph using a preferential attachment model (Barabasi-Albert).\n\nnet.bg &lt;- sample_pa(n =  100, power =  1.2)\nV(net.bg)$size &lt;- 8\nV(net.bg)$frame.color &lt;- \"white\"\nV(net.bg)$color &lt;- \"orange\"\nV(net.bg)$label &lt;- \"\"\nE(net.bg)$arrow.mode &lt;- 0\nplot(net.bg)\n\n\n\n\n\n\n# Using tidygraph\ngraph &lt;- play_barabasi_albert(n = 100, power = 1.2)\ngraph %&gt;%\n  ggraph(., layout = \"graphopt\") +\n  geom_edge_link(color = \"grey\") +\n  geom_node_point(color = \"orange\", size = 4) +\n  theme_graph()\n\n\n\n\n\n\n\nNow let’s plot this network using the layouts available in igraph. You can set the layout in the plot function:\n\nplot(net.bg, layout = layout_randomly)\n\n\n\n\n\n\n\nOr calculate the vertex coordinates in advance:\n\nl &lt;- layout_in_circle(net.bg)\nplot(net.bg, layout = l)\n\n\n\n\n\n\n# Using tidygraph\n# graph &lt;- play_barabasi_albert(n = 100, power = 1.2)\ngraph %&gt;% ggraph(., layout = \"circle\") +\n  geom_edge_link(color = \"grey\") +\n  geom_node_point(color = \"orange\", size = 2) +\n  theme_graph() +\n  theme(aspect.ratio = 1)\n\n\n\n\n\n\n\nl is simply a matrix of x,y coordinates (N x 2) for the N nodes in the graph. You can generate your own:\n\nl &lt;- cbind(1:vcount(net.bg), c(1, vcount(net.bg):2))\nl\n\n       [,1] [,2]\n  [1,]    1    1\n  [2,]    2  100\n  [3,]    3   99\n  [4,]    4   98\n  [5,]    5   97\n  [6,]    6   96\n  [7,]    7   95\n  [8,]    8   94\n  [9,]    9   93\n [10,]   10   92\n [11,]   11   91\n [12,]   12   90\n [13,]   13   89\n [14,]   14   88\n [15,]   15   87\n [16,]   16   86\n [17,]   17   85\n [18,]   18   84\n [19,]   19   83\n [20,]   20   82\n [21,]   21   81\n [22,]   22   80\n [23,]   23   79\n [24,]   24   78\n [25,]   25   77\n [26,]   26   76\n [27,]   27   75\n [28,]   28   74\n [29,]   29   73\n [30,]   30   72\n [31,]   31   71\n [32,]   32   70\n [33,]   33   69\n [34,]   34   68\n [35,]   35   67\n [36,]   36   66\n [37,]   37   65\n [38,]   38   64\n [39,]   39   63\n [40,]   40   62\n [41,]   41   61\n [42,]   42   60\n [43,]   43   59\n [44,]   44   58\n [45,]   45   57\n [46,]   46   56\n [47,]   47   55\n [48,]   48   54\n [49,]   49   53\n [50,]   50   52\n [51,]   51   51\n [52,]   52   50\n [53,]   53   49\n [54,]   54   48\n [55,]   55   47\n [56,]   56   46\n [57,]   57   45\n [58,]   58   44\n [59,]   59   43\n [60,]   60   42\n [61,]   61   41\n [62,]   62   40\n [63,]   63   39\n [64,]   64   38\n [65,]   65   37\n [66,]   66   36\n [67,]   67   35\n [68,]   68   34\n [69,]   69   33\n [70,]   70   32\n [71,]   71   31\n [72,]   72   30\n [73,]   73   29\n [74,]   74   28\n [75,]   75   27\n [76,]   76   26\n [77,]   77   25\n [78,]   78   24\n [79,]   79   23\n [80,]   80   22\n [81,]   81   21\n [82,]   82   20\n [83,]   83   19\n [84,]   84   18\n [85,]   85   17\n [86,]   86   16\n [87,]   87   15\n [88,]   88   14\n [89,]   89   13\n [90,]   90   12\n [91,]   91   11\n [92,]   92   10\n [93,]   93    9\n [94,]   94    8\n [95,]   95    7\n [96,]   96    6\n [97,]   97    5\n [98,]   98    4\n [99,]   99    3\n[100,]  100    2\n\nplot(net.bg, layout = l)\n\n\n\n\n\n\n# Using tidygraph\n# graph &lt;- play_barabasi_albert(n = 100, power = 1.2)\ngraph %&gt;% ggraph(., layout = l) +\n  geom_edge_link(color = \"grey\") +\n  geom_node_point(color = \"orange\", size = 2)+\n  theme_graph()\n\n\n\n\n\n\n\nThis layout is just an example and not very helpful - thankfully igraph has a number of built-in layouts, including:\n\nRandomly placed vertices\n\n\nl &lt;- layout_randomly(net.bg)\nplot(net.bg, layout = l)\n\n\n\n\n\n\n# Using tidygraph\n# graph &lt;- play_barabasi_albert(n = 100, power = 1.2)\ngraph %&gt;% ggraph(., layout = layout_randomly(.)) + \n  geom_edge_link0(colour = \"grey\") + \n  geom_node_point(colour = \"orange\", size = 4)\n\n\n\n\n\n\n\n\nCircle layout\n\n\nl &lt;- layout_in_circle(net.bg)\nplot(net.bg, layout = l)\n\n\n\n\n\n\n# Using tidygraph\n# graph &lt;- play_barabasi_albert(n = 100, power = 1.2)\ngraph %&gt;% ggraph(., layout = layout_in_circle(.)) + \n  geom_edge_link0(colour = \"grey\") + \n  geom_node_point(colour = \"orange\") +\n  theme(aspect.ratio = 1)\n\n\n\n\n\n\n\n\n3D sphere layout\n\n\nl &lt;- layout_on_sphere(net.bg)\nplot(net.bg, layout = l)\n\n\n\n\n\n\n# Using tidygraph\n# graph &lt;- play_barabasi_albert(n = 100, power = 1.2)\ngraph %&gt;% ggraph(., layout = layout_on_sphere(.)) + \n  geom_edge_link0(colour = \"grey\") + \n  geom_node_point(colour = \"orange\")\n\n\n\n\n\n\n\n\nThe Fruchterman-Reingold force-directed algorithm: Nice but slow, most often used in graphs smaller than ~1000 vertices.\n\n\nl &lt;- layout_with_fr(net.bg)\nplot(net.bg, layout = l)\n\n\n\n\n\n\n# Using tidygraph\n# graph &lt;- play_barabasi_albert(n = 100, power = 1.2)\ngraph %&gt;% ggraph(., layout = layout_with_fr(.)) + \n  geom_edge_link0(colour = \"grey\") + \n  geom_node_point(colour = \"orange\")\n\n\n\n\n\n\n\nYou will also notice that the F-R layout is not deterministic - different runs will result in slightly different configurations. Saving the layout in l allows us to get the exact same result multiple times.\n\npar(mfrow = c(2,2), mar = c(1,1,1,1))\nplot(net.bg, layout = layout_with_fr)\nplot(net.bg, layout = layout_with_fr)\nplot(net.bg, layout = l)\nplot(net.bg, layout = l)\n\n\n\n\n\n\n\nBy default, the coordinates of the plots are rescaled to the [-1,1] interval for both x and y. You can change that with the parameter rescale = FALSE and rescale your plot manually by multiplying the coordinates by a scalar. You can use norm_coords to normalize the plot with the boundaries you want. This way you can create more compact or spread out layout versions.\n\n#Get the layout coordinates:\nl &lt;- layout_with_fr(net.bg)\n# Normalize them so that they are in the -1, 1 interval:\nl &lt;- norm_coords(l, ymin = -1, ymax = 1, xmin = -1, xmax = 1)\n\npar(mfrow = c(2,2), mar = c(0,0,0,0))\nplot(net.bg, rescale = F, layout = l*0.4)\nplot(net.bg, rescale = F, layout = l*0.8)\nplot(net.bg, rescale = F, layout = l*1.2)\nplot(net.bg, rescale = F, layout = l*1.6)\n\n\n\n\n\n\n# Using tidygraph\n# Can't do this with tidygraph ( multiplying layout * scalar ), it seems\n\nAnother popular force-directed algorithm that produces nice results for connected graphs is Kamada Kawai. Like Fruchterman Reingold, it attempts to minimize the energy in a spring system.\n\nl &lt;- layout_with_kk(net.bg)\nplot(net.bg, layout = l)\n\n\n\n\n\n\n# Using tidygraph\n# graph &lt;- play_barabasi_albert(n = 100, power = 1.2)\ngraph %&gt;% ggraph(., layout = layout_with_kk(.)) + \n  geom_edge_link0(colour = \"grey\") + \n  geom_node_point(colour = \"orange\", size = 4)\n\n\n\n\n\n\n\nThe MDS (multidimensional scaling) algorithm tries to place nodes based on some measure of similarity or distance between them. More similar/less distant nodes are placed closer to each other. By default, the measure used is based on the shortest paths between nodes in the network. That can be changed with the dist parameter.\n\nplot(net.bg, layout = layout_with_mds)\n\n\n\n\n\n\n# Using tidygraph\n# graph &lt;- play_barabasi_albert(n = 100, power = 1.2)\ngraph %&gt;% ggraph(., layout = layout_with_mds(.)) + \n  geom_edge_link0(colour = \"grey\") + \n  geom_node_point(colour = \"orange\", size = 4)\n\n\n\n\n\n\n\nThe LGL algorithm is for large connected graphs. Here you can specify a root- the node that will be placed in the middle of the layout.\n\nplot(net.bg, layout = layout_with_lgl)\n\n\n\n\n\n\n# Using tidygraph\n# graph &lt;- play_barabasi_albert(n = 100, power = 1.2)\ngraph %&gt;% ggraph(., layout = layout_with_lgl(.)) + \n  geom_edge_link0(colour = \"grey\") + \n  geom_node_point(colour = \"orange\", size = 4)\n\n\n\n\n\n\n\nBy default, igraph uses a layout called layout_nicely which selects an appropriate layout algorithm based on the properties of the graph. Check out all available layouts in igraph:Type ?igraph::layout_ in your console\n\nlayouts &lt;- grep(\"^layout_\", ls(\"package:igraph\"), value = TRUE)[-1]\n\n# Remove layouts that do not apply to our graph.\nlayouts &lt;- layouts[!grepl(\"bipartite|merge|norm|sugiyama|tree\", layouts)]\n\npar(mfrow = c(3,3), mar = c(1,1,1,1))\n\nfor (layout in layouts) {\n  print(layout)\n  l &lt;- do.call(layout, list(net))\n  plot(net, edge.arrow.mode = 0, layout = l, main = layout) }\n\n[1] \"layout_as_star\"\n\n\n[1] \"layout_components\"\n\n\n[1] \"layout_in_circle\"\n\n\n[1] \"layout_nicely\"\n\n\n[1] \"layout_on_grid\"\n\n\n[1] \"layout_on_sphere\"\n\n\n[1] \"layout_randomly\"\n\n\n[1] \"layout_with_dh\"\n\n\n[1] \"layout_with_drl\"\n\n\n\n\n\n\n\n\n[1] \"layout_with_fr\"\n\n\n[1] \"layout_with_gem\"\n\n\n[1] \"layout_with_graphopt\"\n\n\n[1] \"layout_with_kk\"\n\n\n[1] \"layout_with_lgl\"\n\n\n[1] \"layout_with_mds\""
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#highlighting-specific-nodes-or-links",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#highlighting-specific-nodes-or-links",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "——-~~ Highlighting specific nodes or links ——–",
    "text": "——-~~ Highlighting specific nodes or links ——–\nSometimes we want to focus the visualization on a particular node or a group of nodes. Let’s represent distance from the NYT:\n\n\ndistances() calculates shortest path from vertices in ‘v’ to ones in ‘to’.\n\n\ndist.from.NYT &lt;- distances(net, \n                           v = V(net)[media == \"NY Times\"], \n                           to = V(net), \n                           weights = NA)\n\n#Set colors to plot the distances:\noranges &lt;- colorRampPalette(c(\"dark red\", \"gold\"))\ncol &lt;- oranges(max(dist.from.NYT)+1)\ncol &lt;- col[dist.from.NYT+1]\n\n# Let's have same coordinates for Nodes in both graph renderings\n# Then we can verify that the distance calculations are the same for both renderings\ncoords &lt;- igraph::layout_nicely(net)\nplot(net, vertex.label = dist.from.NYT,\n     vertex.color = col, vertex.label.color = \"black\",\n     layout = coords)\n\n\n\n\n\n\n\n\n# Using tidygraph\ngraph &lt;- tbl_graph(nodes, links, directed = TRUE)\ngraph\n\n# A tbl_graph: 17 nodes and 49 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 17 × 5 (active)\n   id    media               media.type type.label audience.size\n   &lt;chr&gt; &lt;chr&gt;                    &lt;int&gt; &lt;chr&gt;              &lt;int&gt;\n 1 s01   NY Times                     1 Newspaper             20\n 2 s02   Washington Post              1 Newspaper             25\n 3 s03   Wall Street Journal          1 Newspaper             30\n 4 s04   USA Today                    1 Newspaper             32\n 5 s05   LA Times                     1 Newspaper             20\n 6 s06   New York Post                1 Newspaper             50\n 7 s07   CNN                          2 TV                    56\n 8 s08   MSNBC                        2 TV                    34\n 9 s09   FOX News                     2 TV                    60\n10 s10   ABC                          2 TV                    23\n11 s11   BBC                          2 TV                    34\n12 s12   Yahoo News                   3 Online                33\n13 s13   Google News                  3 Online                23\n14 s14   Reuters.com                  3 Online                12\n15 s15   NYTimes.com                  3 Online                24\n16 s16   WashingtonPost.com           3 Online                28\n17 s17   AOL.com                      3 Online                33\n#\n# Edge Data: 49 × 4\n   from    to type      weight\n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;int&gt;\n1     1     2 hyperlink     22\n2     1     3 hyperlink     22\n3     1     4 hyperlink     21\n# ℹ 46 more rows\n\n# Set up NY Times as root node first\n# V(net)[media == \"NY Times\"] cannot be used since it returns an `igraph.vs` ( i.e. a list ) object. \n# We need an integer node id.\nroot_nyt &lt;- graph %&gt;%\n  activate(nodes) %&gt;%\n  as_tibble() %&gt;%\n  rowid_to_column(var = \"node_id\") %&gt;%\n  filter(media == \"NY Times\") %&gt;%\n  select(node_id) %&gt;% as_vector()\nroot_nyt\n\nnode_id \n      1 \n\ngraph %&gt;%\n  activate(nodes) %&gt;%\n  mutate(size = centrality_degree()) %&gt;%\n  \n  # new stuff:\n  # breadth first search for all distances from the root node\n  mutate(order = bfs_dist(root = root_nyt)) %&gt;%\n  \n  ggraph(., layout = coords) + # same layout\n  geom_edge_link(\n    aes(width = weight),\n    color = \"grey80\",\n    end_cap = circle(.2, \"cm\"),\n    arrow = arrow(\n      type = \"closed\",\n      ends = \"last\",\n      length = unit(1, \"mm\")\n    )\n  ) +\n  geom_node_point(aes(fill = order,\n                      size = size),\n                  shape = 21,\n                  color = \"black\") +\n  \n  geom_node_text(aes(label = order)) +\n  \n  scale_fill_gradient(\n    name = \"Distance from NY Times\",\n    low = \"dark red\",\n    high = \"gold\",\n    guide = \"legend\"\n  ) +\n  scale_edge_width(range = c(0.2, 1.5), guide = \"none\") +\n  scale_size_continuous(\"Degree\", range =  c(2, 16)) +\n  guides(fill = guide_legend(override.aes = list(pch = 21,\n                                                 size = 4)))\n\n\n\n\n\n\n\nOr, a bit more readable:\n\nplot(net, vertex.color = col, \n     vertex.label = dist.from.NYT, edge.arrow.size = .6,\n     vertex.label.color = \"white\", \n     vertex.size = V(net)$size*1.6, \n     edge.width = 2,\n     layout = norm_coords(layout_with_lgl(net))*1.4, rescale = F)"
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#path-highlighting",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#path-highlighting",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "Path Highlighting",
    "text": "Path Highlighting\nWe can also highlight paths between the nodes in the network.\n\nSay here between MSNBC and the New York Post\n\n\nnews.path &lt;- shortest_paths(net,\n                            from  =  V(net)[media == \"MSNBC\"],\n                            to   =  V(net)[media == \"New York Post\"],\n                            output  =  \"both\")  #both path nodes and edges\nnews.path.distance &lt;- distances(net,\n                                V(net)[media == \"MSNBC\"],\n                                V(net)[media == \"New York Post\"] )\nnews.path\n\n$vpath\n$vpath[[1]]\n+ 4/17 vertices, named, from ea8819e:\n[1] s08 s03 s12 s06\n\n\n$epath\n$epath[[1]]\n+ 3/48 edges from ea8819e (vertex names):\n[1] s08-&gt;s03 s03-&gt;s12 s12-&gt;s06\n\n\n$predecessors\nNULL\n\n$inbound_edges\nNULL\n\nnews.path.distance\n\n    s06\ns08   5\n\n#Generate edge color variable to plot the path:\necol &lt;- rep(\"gray80\", ecount(net))\necol[unlist(news.path$epath)] &lt;- \"orange\"\n\n#Generate edge width variable to plot the path:\new &lt;- rep(2, ecount(net))\new[unlist(news.path$epath)] &lt;- 4\n\n#Generate node color variable to plot the path:\nvcol &lt;- rep(\"gray40\", vcount(net))\nvcol[unlist(news.path$vpath)] &lt;- \"gold\"\n\nplot(net, vertex.color = vcol, \n     edge.color = ecol,\n     edge.width = ew, \n     edge.arrow.mode = 0,\n     ## added lines\n     vertex.label = V(net)$media,\n     vertex.label.font = 2, \n     vertex.label.color = \"gray40\",\n     vertex.label.cex = .7,\n     layout = coords * 1.5)\n\n\n\n\n\n\n\n\n# Using tidygraph\n# We need to use:\n# to_shortest_path(graph, from, to, mode = \"out\", weights = NULL)\n# Let's set up `to` and `from` nodes\n#\n# V(net)[media == \"NY Times\"] cannot be used since it returns an `igraph.vs` ( i.e. a list ) object.\n# We need integer node ids for `from` and `to` in `to_shortest_path`\n\nmsnbc &lt;- graph %&gt;%\n  activate(nodes) %&gt;%\n  as_tibble() %&gt;%\n  rowid_to_column(var = \"node_id\") %&gt;%\n  filter(media == \"MSNBC\") %&gt;%\n  select(node_id) %&gt;% as_vector()\nmsnbc\n\nnode_id \n      8 \n\nnypost &lt;- graph %&gt;%\n  activate(nodes) %&gt;%\n  as_tibble() %&gt;%\n  rowid_to_column(var = \"node_id\") %&gt;%\n  filter(media == \"New York Post\") %&gt;%\n  select(node_id) %&gt;% as_vector()\nnypost\n\nnode_id \n      6 \n\n# Let's create a fresh graph object using morph\n# However we want to merge it back with the original `graph`\n# to get an overlay plot\n#\n# # Can do this to obtain a separate graph\n# convert(to_shortest_path,from = msnbc,to = nypost)\n# However we want to merge it back with the original `graph`\n# to get an overlay plot\nmsnbc_nyp &lt;-\n  graph %&gt;%\n  # first mark all nodes and edges as *not* on the shortest path\n  activate(nodes) %&gt;%\n  mutate(shortest_path_node = FALSE) %&gt;%\n  activate(edges) %&gt;%\n  mutate(shortest_path_edge = FALSE) %&gt;%\n  \n  # Find shortest path between MSNBC and NY Post\n  morph(to_shortest_path, from = msnbc, to = nypost) %&gt;%\n  \n  # Now to mark the shortest_path nodes as TRUE\n  activate(nodes) %&gt;%\n  mutate(shortest_path_node = TRUE) %&gt;%\n  \n  # Now to mark the shortest_path edges as TRUE\n  activate(edges) %&gt;%\n  mutate(shortest_path_edge = TRUE) %&gt;%\n  #\n  # Merge back into main graph; Still saving it as a `msnbc_nyp`\n  unmorph()\nmsnbc_nyp\n\n# A tbl_graph: 17 nodes and 49 edges\n#\n# A directed multigraph with 1 component\n#\n# Edge Data: 49 × 5 (active)\n    from    to type      weight shortest_path_edge\n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;int&gt; &lt;lgl&gt;             \n 1     1     2 hyperlink     22 FALSE             \n 2     1     3 hyperlink     22 FALSE             \n 3     1     4 hyperlink     21 FALSE             \n 4     1    15 mention       20 FALSE             \n 5     2     1 hyperlink     23 FALSE             \n 6     2     3 hyperlink     21 FALSE             \n 7     2     9 hyperlink      1 FALSE             \n 8     2    10 hyperlink      5 FALSE             \n 9     3     1 hyperlink     21 FALSE             \n10     3     4 hyperlink     22 TRUE              \n# ℹ 39 more rows\n#\n# Node Data: 17 × 6\n  id    media             media.type type.label audience.size shortest_path_node\n  &lt;chr&gt; &lt;chr&gt;                  &lt;int&gt; &lt;chr&gt;              &lt;int&gt; &lt;lgl&gt;             \n1 s01   NY Times                   1 Newspaper             20 FALSE             \n2 s02   Washington Post            1 Newspaper             25 FALSE             \n3 s03   Wall Street Jour…          1 Newspaper             30 TRUE              \n# ℹ 14 more rows\n\nmsnbc_nyp %&gt;%\n  activate(nodes) %&gt;%\n  mutate(size = centrality_degree()) %&gt;%\n  ggraph(layout = coords) +\n  #geom_edge_link0(colour = \"grey\") +\n  geom_edge_link0(aes(colour = shortest_path_edge,\n                      width = shortest_path_edge)) +\n  \n  geom_node_point(aes(size = size,\n                      fill = shortest_path_node), shape = 21) +\n  geom_node_text(aes(label = media)) +\n  \n  scale_size_continuous(\"Degree\", range =  c(2, 16)) +\n  scale_fill_manual(\"Shortest Path\",\n                    values = c(\"grey\", \"gold\")) +\n  \n  scale_edge_width_manual(values = c(1, 4)) +\n  \n  scale_edge_colour_manual(values = c(\"grey\", \"orange\")) +\n  guides(\n    fill = guide_legend(override.aes = list(pch = 21,\n                                            size = 6)),\n    edge_colour = \"none\",\n    edge_width = \"none\"\n  )\n\n\n\n\n\n\n\n\nHighlight the edges going into or out of a vertex, for instance the WSJ. For a single node, use incident(), for multiple nodes use incident_edges()\n\n\n\ninc.edges &lt;-\n  incident(net, V(net)[media == \"Wall Street Journal\"], mode = \"all\")\n\n#Set colors to plot the selected edges.\necol &lt;- rep(\"gray80\", ecount(net))\necol[inc.edges] &lt;- \"orange\"\nvcol &lt;- rep(\"grey40\", vcount(net))\nvcol[V(net)$media == \"Wall Street Journal\"] &lt;- \"gold\"\nplot(\n  net,\n  vertex.color = vcol,\n  edge.color = ecol,\n  edge.width = 2,\n  layout = coords\n)\n\n\n\n\n\n\n\n\n# Using tidygraph\nwsj &lt;- graph %&gt;% \n  activate(nodes) %&gt;% \n  as_tibble() %&gt;% \n  rowid_to_column(var = \"node_id\") %&gt;% \n  filter(media == \"Wall Street Journal\") %&gt;% \n  select(node_id) %&gt;% as_vector()\n\ngraph %&gt;% \n  activate(nodes) %&gt;% \n  mutate(wsj_adjacent = node_is_adjacent(to = wsj, mode = \"all\", \n                                         include_to = TRUE),\n         size = centrality_degree()) %&gt;% \n  mutate(WSJ = if_else(media == \"Wall Street Journal\", TRUE, FALSE)) %&gt;% \n  activate(edges) %&gt;% \n  mutate(wsj_links = edge_is_incident(wsj)) %&gt;% \n  \n  ggraph(., layout = coords) +\n  geom_edge_link0(aes(colour = wsj_links), width = 2) + \n  \n  geom_node_point(aes(fill = WSJ, \n                      size = size),shape = 21) +\n  \n  geom_node_text(aes(label = media), repel = TRUE) + \n  \n  scale_fill_manual(\"WSJ Neighbours\", \n                      values = c(\"grey\", \"gold\"), \n                      guide = guide_legend(override.aes = \n                                             list(pch = 21, \n                                                  size = 5))) + \n  scale_edge_colour_manual(\"WSJ Links\", \n                      values = c(\"grey\", \"orange\")) + \n  scale_size(\"Degree\", range = c( 2, 16)) +\n  ggtitle(label = \"Highlighting WSJ Neighbours and Links\") +\n  guides(shape = \"none\", fill = \"none\" #, colour = \"none\"\n  )"
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#highlight-neighbours",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#highlight-neighbours",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "Highlight Neighbours",
    "text": "Highlight Neighbours\nOr we can highlight the immediate neighbors of a vertex, say WSJ. The neighbors function finds all nodes one step out from the focal actor. To find the neighbors for multiple nodes, use adjacent_vertices(). To find node neighborhoods going more than one step out, use function ego() with parameter order set to the number of steps out to go from the focal node(s).\n\nneigh.nodes &lt;- neighbors(net, V(net)[media == \"Wall Street Journal\"], mode = \"out\")\n\n# Set colors to plot the neighbors:\nvcol[neigh.nodes] &lt;- \"#ff9d00\"\nplot(net, vertex.color = vcol)\n\n\n\n\n\n\n\n\n# Using tidygraph\nwsj &lt;- graph %&gt;% \n  activate(nodes) %&gt;% \n  as_tibble() %&gt;% \n  rowid_to_column(var = \"node_id\") %&gt;% \n  filter(media == \"Wall Street Journal\") %&gt;% \n  select(node_id) %&gt;% as_vector()\n\ngraph %&gt;% \n  activate(nodes) %&gt;% \n  mutate(wsj_adjacent = node_is_adjacent(to = wsj, mode = \"all\", \n  # remove WSJ from the list!\n  # highlight only the neighbours\n  \n                                         include_to = FALSE),\n         size = centrality_degree()) %&gt;% \n  mutate(WSJ = if_else(media == \"Wall Street Journal\", TRUE, FALSE)) %&gt;% \n  activate(edges) %&gt;% \n  mutate(wsj_links = edge_is_incident(wsj)) %&gt;% \n  \n  ggraph(., layout = coords) +\n  geom_edge_link0(aes(colour = wsj_links), width = 2) + \n  \n  geom_node_point(aes(fill = wsj_adjacent, \n                      size = size),shape = 21) +\n  \n  geom_node_text(aes(label = media), repel = TRUE) + \n  \n  scale_fill_manual(\"WSJ Neighbours\", \n                      values = c(\"grey\", \"gold\"), \n                      guide = guide_legend(override.aes = \n                                             list(pch = 21, \n                                                  size = 5))) + \n  scale_edge_colour_manual(\"WSJ Links\", \n                      values = c(\"grey\", \"orange\")) + \n  scale_size(\"Degree\", range = c( 2, 16)) +\n  ggtitle(label = \"Highlighting WSJ Neighbours and Links\") +\n  guides(shape = \"none\", fill = \"none\" #, colour = \"none\"\n  )\n\n\n\n\n\n\n\nAnother way to draw attention to a group of nodes: (This is generally not recommended since, depending on layout, nodes that are not ‘marked’ can accidentally get placed on top of the mark)\n\nplot(net, mark.groups = c(1,4,5,8), mark.col = \"#C5E5E7\", mark.border = NA)\n\n\n\n\n\n\n# Mark multiple groups:\nplot(net, mark.groups = list(c(1,4,5,8), c(15:17)),\n          mark.col = c(\"#C5E5E7\",\"#ECD89A\"), mark.border = NA)"
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#interactive-plotting-with-tkplot",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#interactive-plotting-with-tkplot",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "——-~~ Interactive plotting with ‘tkplot’ ——–",
    "text": "——-~~ Interactive plotting with ‘tkplot’ ——–\nR and igraph offer interactive plotting capabilities (mostly helpful for small networks)\n\ntkid &lt;- tkplot(net) #tkid is the id of the tkplot\n\nl &lt;- tkplot.getcoords(tkid) # grab the coordinates from tkplot\nplot(net, layout = l)"
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#other-ways-to-represent-a-network",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#other-ways-to-represent-a-network",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "——-~~ Other ways to represent a network ——–",
    "text": "——-~~ Other ways to represent a network ——–\nOne reminder that there are other ways to represent a network:\n\nHeatmap of the network matrix:\n\n\nnetm &lt;- as_adjacency_matrix(net, attr = \"weight\", sparse = F)\ncolnames(netm) &lt;- V(net)$media\nrownames(netm) &lt;- V(net)$media\n\npalf &lt;- colorRampPalette(c(\"gold\", \"dark orange\"))\n\n# The Rowv & Colv parameters turn dendrograms on and off\nheatmap(netm[,17:1], Rowv  =  NA, Colv  =  NA, col  =  palf(20),\n        scale = \"none\", margins = c(10,10) )\n\n\n\n\n\n\n\n\nDegree distribution\n\n\ndeg.dist &lt;- degree_distribution(net, cumulative = T, mode = \"all\")\n# degree is available in `sna` too\nplot(x = 0:max(igraph::degree(net)), y = 1-deg.dist, pch = 19, cex = 1.4, col = \"orange\", xlab = \"Degree\", ylab = \"Cumulative Frequency\")\n\n\n\n\n\n\n# Using Tidygraph\n# https://stackoverflow.com/questions/18356860/cumulative-histogram-with-ggplot2\ngraph %&gt;% \n  activate(nodes) %&gt;% \n  mutate(degree = centrality_degree(mode = \"all\")) %&gt;% \n  as_tibble() %&gt;% \n  ggplot(aes(x = degree, y = stat(count))) +\n  # geom_histogram(aes(y = cumsum(..count..)), binwidth = 1) + \n  stat_bin(aes(y = cumsum(after_stat(count))),\n                binwidth = 1,# Ta-Da !!\n                geom =\"point\",color =\"orange\", size = 5)"
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#plotting-two-mode-networks",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#plotting-two-mode-networks",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "4. Plotting two-mode networks",
    "text": "4. Plotting two-mode networks\n\nhead(nodes2)\n\n\n  \n\n\nhead(links2)\n\n    U01 U02 U03 U04 U05 U06 U07 U08 U09 U10 U11 U12 U13 U14 U15 U16 U17 U18 U19\ns01   1   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\ns02   0   0   0   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\ns03   0   0   0   0   0   1   1   1   1   0   0   0   0   0   0   0   0   0   0\ns04   0   0   0   0   0   0   0   0   1   1   1   0   0   0   0   0   0   0   0\ns05   0   0   0   0   0   0   0   0   0   0   1   1   1   0   0   0   0   0   0\ns06   0   0   0   0   0   0   0   0   0   0   0   0   1   1   0   0   1   0   0\n    U20\ns01   0\ns02   1\ns03   0\ns04   0\ns05   0\ns06   0\n\nnet2\n\nIGRAPH 01225d0 UN-B 30 31 -- \n+ attr: type (v/l), name (v/c)\n+ edges from 01225d0 (vertex names):\n [1] s01--U01 s01--U02 s01--U03 s02--U04 s02--U05 s02--U20 s03--U06 s03--U07\n [9] s03--U08 s03--U09 s04--U09 s04--U10 s04--U11 s05--U11 s05--U12 s05--U13\n[17] s06--U13 s06--U14 s06--U17 s07--U14 s07--U15 s07--U16 s08--U16 s08--U17\n[25] s08--U18 s08--U19 s09--U06 s09--U19 s09--U20 s10--U01 s10--U11\n\nplot(net2)\n\n\n\n\n\n\n\nThis time we will make nodes look different based on their type. Media outlets are blue squares, audience nodes are orange circles:\n\nV(net2)$color &lt;- c(\"steel blue\", \"orange\")[V(net2)$type+1]\nV(net2)$shape &lt;- c(\"square\", \"circle\")[V(net2)$type+1]\n\n# Media outlets will have name labels, audience members will not:\nV(net2)$label &lt;- \"\"\nV(net2)$label[V(net2)$type == F] &lt;- nodes2$media[V(net2)$type == F]\nV(net2)$label.cex = .6\nV(net2)$label.font = 2\n\nplot(net2, vertex.label.color = \"white\", vertex.size = (2-V(net2)$type)*8)\n\n\n\n\n\n\n\n\n# Using tidygraph\nas_tbl_graph(x = links2, directed = TRUE) %&gt;%\n  activate(nodes) %&gt;%\n  left_join(nodes2, by = c(\"name\" = \"id\")) %&gt;%\n  ggraph(layout = \"nicely\") +\n  geom_edge_link0() +\n  geom_node_point(aes(shape = type, fill = type, size = type)) +\n  geom_node_text(aes(label = if_else(type, \"\", media)), colour = \"white\", size = 3) +\n  \n  scale_shape_manual(\n    \"Type\",\n    values = c(22, 21),\n    labels = c(\"Media\", \"Persons\"),\n    guide = guide_legend(override.aes = list(size = 6))\n  ) +\n  \n  scale_fill_manual(\n    \"Type\",\n    values = c(\"dodgerblue\", \"orange\"),\n    labels = c(\"Media\", \"Persons\")\n  ) +\n  \n  scale_size_manual(values = c(10, 4), guide = \"none\") \n\n\n\n\n\n\n\nigraph has a built-in bipartite layout, though it’s not the most helpful:\n\nplot(net2, vertex.label = NA, vertex.size = 7, layout = layout_as_bipartite)\n\n\n\n\n\n\n# using tidygraph\nas_tbl_graph(x = links2, directed = TRUE) %&gt;%\n  activate(nodes) %&gt;%\n  left_join(nodes2, by = c(\"name\" = \"id\")) %&gt;%\n  ggraph(., layout = \"igraph\", algorithm = \"bipartite\") +\n  geom_edge_link0() +\n  geom_node_point(aes(shape = type, fill = type, size = type)) +\n  geom_node_text(aes(label = if_else(type, \"\", media)), colour = \"white\", size = 3) +\n  \n  scale_shape_manual(\n    \"Type\",\n    values = c(22, 21),\n    labels = c(\"Media\", \"Persons\"),\n    guide = guide_legend(override.aes = list(size = 6))\n  ) +\n  \n  scale_fill_manual(\n    \"Type\",\n    values = c(\"dodgerblue\", \"orange\"),\n    labels = c(\"Media\", \"Persons\")\n  ) +\n  \n  scale_size_manual(values = c(10, 4), guide = \"none\") \n\n\n\n\n\n\n\n\nUsing text as nodes:\n\n\npar(mar = c(0,0,0,0))\nplot(net2, vertex.shape = \"none\", vertex.label = nodes2$media,\n     vertex.label.color = V(net2)$color, vertex.label.font = 2,\n     vertex.label.cex = .95, edge.color = \"gray70\",  edge.width = 2)\n\n\n\n\n\n\n# Using tidygraph\nas_tbl_graph(x = links2, directed = TRUE) %&gt;%\n  activate(nodes) %&gt;%\n  left_join(nodes2, by = c(\"name\" = \"id\")) %&gt;%\n  ggraph(layout = \"nicely\") +\n  geom_edge_link(end_cap = circle(.4,\"cm\"), \n                 start_cap = circle(0.4, \"cm\")) +\n  # geom_node&gt;point(aes(shape = type, fill = type, size = type)) +\n  geom_node_text(aes(label= media, colour = type), size = 4) +\n  \n  scale_shape_manual(\n    \"Type\",\n    values = c(22, 21),\n    labels = c(\"Media\", \"Persons\"),\n    guide = guide_legend(override.aes = list(size = 4))\n  ) +\n  \n  scale_fill_manual(\n    \"Type\",\n    values = c(\"dodgerblue\", \"orange\"),\n    labels = c(\"Media\", \"Persons\")\n  ) +\n  \n  scale_size_manual(values = c(10, 4), guide = \"none\") \n\n\n\n\n\n\n\n\nUsing images as nodes You will need the ‘png’ package to do this:\n\n\n# install.packages(\"png\")\nlibrary(\"png\")\n\nimg.1 &lt;- readPNG(\"./images/news.png\")\nimg.2 &lt;- readPNG(\"./images/user.png\")\n\nV(net2)$raster &lt;- list(img.1, img.2)[V(net2)$type+1]\n\npar(mar = c(3,3,3,3))\n\nplot(net2, vertex.shape = \"raster\", vertex.label = NA,\n     vertex.size = 16, vertex.size2 = 16, edge.width = 2)\n\n\n# By the way, you can also add any image you want to any plot. For example, many #network graphs could be improved by a photo of a puppy carrying a basket full of kittens.\nimg.3 &lt;- readPNG(\"./images/puppy.png\")\nrasterImage(img.3,  xleft = -1.7, xright = 0, ybottom = -1.2, ytop = 0)\n\n\n\n\n\n\n# The numbers after the image are coordinates for the plot.\n# The limits of your plotting area are given in par()$usr\n\n\n# Using ~~tidygraph~~ visNetwork\n# See this cheatsheet:\n# system.file(\"fontAwesome/Font_Awesome_Cheatsheet.pdf\", package = \"visNetwork\")\nlibrary(visNetwork)\n\nas_tbl_graph(x = links2, directed = TRUE) %&gt;%\n  activate(nodes) %&gt;%\n  left_join(nodes2, by = c(\"name\" = \"id\")) %&gt;% \n  \n  # visNetwork needs a \"group\" variable for grouping...\n  mutate(group = as.character(type)) %&gt;% \n  visIgraph(.) %&gt;% \n  visGroups(groupname = \"FALSE\",shape = \"icon\", \n            icon = list(code = \"f26c\", size = 75, color = \"orange\")) %&gt;% \n  visGroups(groupname = \"TRUE\",shape = \"icon\", \n            icon = list(code = \"f007\", size = 75)) %&gt;% \n  addFontAwesome()\n\n\n\n\n\nWe can also generate and plot bipartite projections for the two-mode network : (co-memberships are easy to calculate by multiplying the network matrix by its transposed matrix, or using igraph’s bipartite.projection function)\n\nnet2.bp &lt;- bipartite.projection(net2)\n\n#We can calculate the projections manually as well:\nas_incidence_matrix(net2)  %*% t(as_incidence_matrix(net2))\n\n    s01 s02 s03 s04 s05 s06 s07 s08 s09 s10\ns01   3   0   0   0   0   0   0   0   0   1\ns02   0   3   0   0   0   0   0   0   1   0\ns03   0   0   4   1   0   0   0   0   1   0\ns04   0   0   1   3   1   0   0   0   0   1\ns05   0   0   0   1   3   1   0   0   0   1\ns06   0   0   0   0   1   3   1   1   0   0\ns07   0   0   0   0   0   1   3   1   0   0\ns08   0   0   0   0   0   1   1   4   1   0\ns09   0   1   1   0   0   0   0   1   3   0\ns10   1   0   0   1   1   0   0   0   0   2\n\nt(as_incidence_matrix(net2)) %*%   as_incidence_matrix(net2)\n\n    U01 U02 U03 U04 U05 U06 U07 U08 U09 U10 U11 U12 U13 U14 U15 U16 U17 U18 U19\nU01   2   1   1   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0\nU02   1   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\nU03   1   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\nU04   0   0   0   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\nU05   0   0   0   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\nU06   0   0   0   0   0   2   1   1   1   0   0   0   0   0   0   0   0   0   1\nU07   0   0   0   0   0   1   1   1   1   0   0   0   0   0   0   0   0   0   0\nU08   0   0   0   0   0   1   1   1   1   0   0   0   0   0   0   0   0   0   0\nU09   0   0   0   0   0   1   1   1   2   1   1   0   0   0   0   0   0   0   0\nU10   0   0   0   0   0   0   0   0   1   1   1   0   0   0   0   0   0   0   0\nU11   1   0   0   0   0   0   0   0   1   1   3   1   1   0   0   0   0   0   0\nU12   0   0   0   0   0   0   0   0   0   0   1   1   1   0   0   0   0   0   0\nU13   0   0   0   0   0   0   0   0   0   0   1   1   2   1   0   0   1   0   0\nU14   0   0   0   0   0   0   0   0   0   0   0   0   1   2   1   1   1   0   0\nU15   0   0   0   0   0   0   0   0   0   0   0   0   0   1   1   1   0   0   0\nU16   0   0   0   0   0   0   0   0   0   0   0   0   0   1   1   2   1   1   1\nU17   0   0   0   0   0   0   0   0   0   0   0   0   1   1   0   1   2   1   1\nU18   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   1   1   1\nU19   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   1   1   1   2\nU20   0   0   0   1   1   1   0   0   0   0   0   0   0   0   0   0   0   0   1\n    U20\nU01   0\nU02   0\nU03   0\nU04   1\nU05   1\nU06   1\nU07   0\nU08   0\nU09   0\nU10   0\nU11   0\nU12   0\nU13   0\nU14   0\nU15   0\nU16   0\nU17   0\nU18   0\nU19   1\nU20   2\n\npar(mfrow = c(1, 2))\n\nplot(\n  net2.bp$proj1,\n  vertex.label.color = \"black\",\n  vertex.label.dist = 2,\n  vertex.label = nodes2$media[!is.na(nodes2$media.type)]\n)\n\nplot(\n  net2.bp$proj2,\n  vertex.label.color = \"black\",\n  vertex.label.dist = 2,\n  vertex.label = nodes2$media[is.na(nodes2$media.type)]\n)\n\n\n\n\n\n\n\n\n# Using tidygraph\n# Calculate projections and add attributes/labels\nproj1 &lt;-\n  as_incidence_matrix(net2)  %*% t(as_incidence_matrix(net2)) %&gt;%\n  as_tbl_graph() %&gt;%\n  activate(nodes) %&gt;%\n  left_join(., nodes2, by = c(\"name\" = \"id\"))\nproj2 &lt;-\n  t(as_incidence_matrix(net2)) %*%   as_incidence_matrix(net2) %&gt;% as_tbl_graph() %&gt;%\n  activate(nodes) %&gt;%\n  left_join(., nodes2, by = c(\"name\" = \"id\"))\n\n\np1 &lt;- proj1 %&gt;%\n  ggraph(layout = \"graphopt\") +\n  geom_edge_link0() +\n  geom_node_point(size = 6, colour = \"orange\") +\n  geom_node_text(aes(label = media), repel = TRUE)\n\np2 &lt;- proj2 %&gt;%\n  ggraph(layout = \"graphopt\") +\n  geom_edge_link0() +\n  geom_node_point(\n    aes(colour = media.type),\n    size = 6,\n    shape  = 15,\n    colour = \"dodgerblue\"\n  ) +\n  geom_node_text(aes(label = media), repel = TRUE)\n\np1 + p2"
  },
  {
    "objectID": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#plotting-multiplex-networks",
    "href": "content/projects/2021-06-16-a-tidygraph-version-of-a-popular-network-science-tutorial/index.html#plotting-multiplex-networks",
    "title": "A Tidygraph version of a Popular Network Science Tutorial",
    "section": "5. Plotting multiplex networks",
    "text": "5. Plotting multiplex networks\nIn some cases, the networks we want to plot are multigraphs: they can have multiple edges connecting the same two nodes. A related concept, multiplex networks, contain multiple types of ties – e.g. friendship, romantic, and work relationships between individuals.\nIn our example network, we also have two tie types: hyperlinks and mentions. One thing we can do is plot each type of tie separately:\n\nE(net)$width &lt;- 2\nplot(\n  net,\n  edge.color = c(\"dark red\", \"slategrey\")[(E(net)$type == \"hyperlink\") +\n                                            1],\n  vertex.color = \"gray40\",\n  layout = layout_in_circle,\n  edge.curved = .3\n)\n\n\n\n\n\n\n# Another way to delete edges using the minus operator:\nnet.m &lt;- net - E(net)[E(net)$type == \"hyperlink\"]\nnet.h &lt;- net - E(net)[E(net)$type == \"mention\"]\n\n#Plot the two links separately:\npar(mfrow = c(1, 2))\n\nplot(net.h,\n     vertex.color = \"orange\",\n     layout = layout_with_fr,\n     main = \"Tie: Hyperlink\")\nplot(net.m,\n     vertex.color = \"lightsteelblue2\",\n     layout = layout_with_fr,\n     main = \"Tie: Mention\")\n\n\n\n\n\n\n\n\nMake sure the nodes stay in the same place in both plots:\n\n\npar(mfrow = c(1, 2), mar = c(1, 1, 4, 1))\n\nl &lt;- layout_with_fr(net)\nplot(net.h,\n     vertex.color = \"orange\",\n     layout = l,\n     main = \"Tie: Hyperlink\")\nplot(net.m,\n     vertex.color = \"lightsteelblue2\",\n     layout = l,\n     main = \"Tie: Mention\")\n\n\n\n\n\n\n\n\n#Using tidygraph\n\nlayout &lt;- layout_in_circle(net)\np1 &lt;- tbl_graph(nodes, links, directed = TRUE) %&gt;% \n  activate(nodes) %&gt;% \n  mutate(size = centrality_degree()) %&gt;% \n  activate(edges) %&gt;% \n  filter(type == \"hyperlink\") %&gt;% \n  \n  # reusing the earlier computed layout\n  ggraph(layout = layout) +\n  geom_edge_arc(strength = 0.05) +\n  geom_node_point(aes(size = size), shape = 21, \n                  fill = \"orange\") +\n  scale_size(range = c(2, 12)) + \n  labs(title = \"Tie: Hyperlink\") + \n  theme(aspect.ratio = 1,,\n        legend.position = \"bottom\")\n\np2 &lt;- tbl_graph(nodes, links, directed = TRUE) %&gt;% \n  activate(nodes) %&gt;% \n  mutate(size = centrality_degree()) %&gt;% \n  activate(edges) %&gt;% \n  filter(type == \"mention\") %&gt;% \n   # reusing the earlier computed layout\n  ggraph(layout = layout) +\n  geom_edge_arc(strength = 0.05) +\n  geom_node_point(aes(size = size), shape = 21, \n                  fill = \"lightsteelblue2\") +\n  scale_size(range = c(2, 12)) + \n  labs(title = \"Tie: Mention\") + \n  theme(aspect.ratio = 1, legend.position = \"bottom\")\n\nwrap_plots(p1, p2,guides = \"collect\") & \n  # note this \"pipe\" for patchwork!\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nIn our example network, we don’t have node dyads connected by multiple types of connections (we never have both a ‘hyperlink’ and a ‘mention’ tie between the same two news outlets) – however that could happen.\nNote: See the edges between s03 and s10…these are in opposite directions. So no dyads.\n\nlayout &lt;- layout_in_circle(net)\ntbl_graph(nodes, links, directed = TRUE) %&gt;%  \n  activate(nodes) %&gt;% \n  mutate(size = centrality_degree()) %&gt;% \n\n  # reusing the earlier computed layout\n  ggraph(layout = layout) +\n  geom_edge_arc(strength = 0.05, aes(colour = type)) +\n  geom_node_point(aes(size = size), shape = 21, \n                  fill = \"orange\") +\n  geom_node_text(aes(label = id), repel = TRUE) +\n  scale_size(range = c(2, 12)) + \n  labs(title = \"Tie: Hyperlink\") + \n  theme(aspect.ratio = 1,,\n        legend.position = \"bottom\")\n\n\n\n\n\n\n\nOne challenge in visualizing multiplex networks is that multiple edges between the same two nodes may get plotted on top of each other in a way that makes them impossible to distinguish. For example, let us generate a simple multiplex network with two nodes and three ties between them:\n\nmultigtr &lt;- graph(edges = c(1, 2, 1, 2, 1, 2), n = 2)\n\nl &lt;- layout_with_kk(multigtr)\n\n# Let's just plot the graph:\nplot(\n  multigtr,\n  vertex.color = \"lightsteelblue\",\n  vertex.frame.color = \"white\",\n  vertex.size = 40,\n  vertex.shape = \"circle\",\n  vertex.label = NA,\n  edge.color = c(\"gold\", \"tomato\", \"yellowgreen\"),\n  edge.width = 10,\n  edge.arrow.size = 5,\n  edge.curved = 0.1,\n  layout = l\n)\n\n\n\n\n\n\n# Using tidygraph\nmultigtr %&gt;% \n  as_tbl_graph() %&gt;% \n  activate(edges) %&gt;% \n  mutate(edge_col = c(\"gold\", \"tomato\", \"yellowgreen\")) %&gt;% \nggraph(., layout = l) +\n  geom_edge_arc(strength = 0.1, aes(colour = edge_col)) + \n  geom_node_point(size = 4, colour = \"lightsteelblue\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nBecause all edges in the graph have the same curvature, they are drawn over each other so that we only see the last one. What we can do is assign each edge a different curvature. One useful function in ‘igraph’ called curve_multiple() can help us here. For a graph G, curve.multiple(G) will generate a curvature for each edge that maximizes visibility.\n\nplot(\n  multigtr,\n  vertex.color = \"lightsteelblue\",\n  vertex.frame.color = \"white\",\n  vertex.size = 40,\n  vertex.shape = \"circle\",\n  vertex.label = NA,\n  edge.color = c(\"gold\", \"tomato\", \"yellowgreen\"),\n  edge.width = 10,\n  edge.arrow.size = 5,\n  edge.curved = curve_multiple(multigtr),\n  layout = l\n)\n\n\n\n\n\n\n\n\nmultigtr %&gt;% \n  as_tbl_graph() %&gt;% \n  activate(edges) %&gt;% \n  mutate(edge_col = c(\"gold\", \"tomato\", \"yellowgreen\")) %&gt;% \nggraph(., layout = l) +\n  geom_edge_fan(strength = 0.1, aes(colour = edge_col),width = 2) + \n  geom_node_point(size = 4, colour = \"lightsteelblue\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nAnd that is the end of this reoworked tutorial! Hope you enjoyed it and found it useful!!"
  },
  {
    "objectID": "content/projects/Resources-For-Order-and-Chaos/index.html",
    "href": "content/projects/Resources-For-Order-and-Chaos/index.html",
    "title": "Resources for Order and Chaos",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#what-makes-human-experience",
    "href": "content/slides/r-slides/nature/new.html#what-makes-human-experience",
    "title": "The Nature of Data",
    "section": "What makes Human Experience?",
    "text": "What makes Human Experience?\n\nHow would we begin to describe this experience?\n\n\n\nWhere / When?\nWho?\nHow?\nHow Big? How small? How frequent? How sudden?\n\n\n\n\n\nAnd….How Surprising ! How Shocking! How sad…How Wonderful !!!\nSo: Our Questions, and our Surprise lead us to creating Human Experiences."
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#does-this-surprise-you",
    "href": "content/slides/r-slides/nature/new.html#does-this-surprise-you",
    "title": "The Nature of Data",
    "section": "Does this Surprise you?",
    "text": "Does this Surprise you?\n\n\nNeeds to be celebrated. Spotted in a men’s washroom @BLRAirport - a diaper change station\nChildcare is not just a woman’s responsibility.\npic/twitter.com/Za4CG9jZfR\n-Sukhada(@appadappajapa), June 27, 2022"
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#the-element-of-surprise",
    "href": "content/slides/r-slides/nature/new.html#the-element-of-surprise",
    "title": "The Nature of Data",
    "section": "The Element of Surprise?",
    "text": "The Element of Surprise?\n\n\n\n\n\nJane Austen knew a lot about human information processing as these snippets from Pride and Prejudice (published in 1813 – over 200 years ago)1 show :\n\nShe was a woman of mean understanding, little information , and uncertain temper.\nCatherine and Lydia had information for them of a different sort.\nWhen this information was given, and they had all taken their seats, Mr. Collins was at leisure to look around him and admire,…\nYou could not have met with a person more capable of giving you certain information on that head than myself, for I have been connected with his family in a particular manner from my infancy.\nThis information made Elizabeth smile, as she thought of poor Miss Bingley.\nThis information, however, startled Mrs. Bennet …\n\n\n\n\nhttps://www.cs.bham.ac.uk/research/projects/cogaff/misc/austen-info.html"
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#claude-shannon-and-information",
    "href": "content/slides/r-slides/nature/new.html#claude-shannon-and-information",
    "title": "The Nature of Data",
    "section": "Claude Shannon and Information",
    "text": "Claude Shannon and Information\n\nhttps://plus.maths.org/content/information-surprise"
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#human-experience-is.data",
    "href": "content/slides/r-slides/nature/new.html#human-experience-is.data",
    "title": "The Nature of Data",
    "section": "Human Experience is….Data??",
    "text": "Human Experience is….Data??"
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#experiments-and-hypotheses-a-kitchen-experiment",
    "href": "content/slides/r-slides/nature/new.html#experiments-and-hypotheses-a-kitchen-experiment",
    "title": "The Nature of Data",
    "section": "Experiments and Hypotheses: A Kitchen Experiment",
    "text": "Experiments and Hypotheses: A Kitchen Experiment\n\n\nInputs are: Ingredients, Recipes, Processes\nOutputs are: Taste, Texture, Colour, Quantity!!"
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#what-is-the-result-of-an-experiment",
    "href": "content/slides/r-slides/nature/new.html#what-is-the-result-of-an-experiment",
    "title": "The Nature of Data",
    "section": "What is the Result of an Experiment?",
    "text": "What is the Result of an Experiment?\n\n\n\nAll experiments give us data about phenomena\nOutputs: We obtain data about the things that happen\nInputs: What makes things happen?\nProcess: How?\nFactors: When?\nEffect Size: How much “output” is caused by how much “input”?\n\n\nAll Experiments stem from\n- Human Curiosity,\n- a Hypothesis, and  - a Desire to Find out and Talk about Something"
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#a-famous-lady-and-her-famous-experiment",
    "href": "content/slides/r-slides/nature/new.html#a-famous-lady-and-her-famous-experiment",
    "title": "The Nature of Data",
    "section": "A Famous Lady and her Famous Experiment",
    "text": "A Famous Lady and her Famous Experiment\n\n\n\n\n\n\n\n\n\n\nIn 1853, Turkey declared war on Russia. After the Russian Navy destroyed a Turkish squadron in the Black Sea, Great Britain and France joined with Turkey. In September of the following year, the British landed on the Crimean Peninsula and set out, with the French and Turks, to take the Russian naval base at Sevastopol.\nWhat followed was a tragicomedy of errors – failure of supply, failed communications, international rivalries. Conditions in the armies were terrible, and disease ate through their ranks. They finally did take Sevastopol a year later, after a ghastly assault. It was ugly business all around. Well over half a million soldiers lost their lives during the Crimean War."
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#florence-nightingales-data",
    "href": "content/slides/r-slides/nature/new.html#florence-nightingales-data",
    "title": "The Nature of Data",
    "section": "Florence Nightingale’s Data",
    "text": "Florence Nightingale’s Data\n\n\n   Month Year Disease.rate Wounds.rate Other.rate\n1    Apr 1854          1.4         0.0        7.0\n2    May 1854          6.2         0.0        4.6\n3    Jun 1854          4.7         0.0        2.5\n4    Jul 1854        150.0         0.0        9.6\n5    Aug 1854        328.5         0.4       11.9\n6    Sep 1854        312.2        32.1       27.7\n7    Oct 1854        197.0        51.7       50.1\n8    Nov 1854        340.6       115.8       42.8\n9    Dec 1854        631.5        41.7       48.0\n10   Jan 1855       1022.8        30.7      120.0"
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#how-does-data-look-like-then",
    "href": "content/slides/r-slides/nature/new.html#how-does-data-look-like-then",
    "title": "The Nature of Data",
    "section": "How Does Data look Like, then?",
    "text": "How Does Data look Like, then?\n\n\n\n\nTypes of Variables - Using Interrogative Pronouns\n\nNominal: What? Who? Where? (Factors, Dimensions)\nOrdinal: Which Types? What Sizes? How Big? (Factors, Dimensions)\nInterval: How Often? (Numbers, Facts)\nRatio: How many? How much? How heavy? (Numbers, Facts)"
  },
  {
    "objectID": "content/slides/r-slides/nature/new.html#types-of-variables-in-nightingale-data",
    "href": "content/slides/r-slides/nature/new.html#types-of-variables-in-nightingale-data",
    "title": "The Nature of Data",
    "section": "Types of Variables in Nightingale Data",
    "text": "Types of Variables in Nightingale Data\n\n\nUsing Interrogative Pronouns\n\nNominal: None\nOrdinal: (Factors, Dimensions)\n\nHOW? War, Disease, Other\n\nInterval: (Numbers, Facts)\n\nWHEN? Year, Month\n\nRatio: (Numbers, Facts)\n\nHOW MANY? Rate of Deaths (War, Disease, Other)\n\n\n\n\n\n\n\n\n\nhttps://arvindvenkatadri.com"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#image-right",
    "href": "content/slides/r-slides/networks/index.html#image-right",
    "title": "Introduction to Networks in R",
    "section": ".image-right",
    "text": ".image-right\n\n\nWe can use the .image-right and .image-left classes to insert images in the background\nThese images will be placed behind most other content"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#image-left",
    "href": "content/slides/r-slides/networks/index.html#image-left",
    "title": "Introduction to Networks in R",
    "section": ".image-left",
    "text": ".image-left\n\n\n\n\nIt is therefore recommened that you use multiple columns to only have text on the background area"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#section",
    "href": "content/slides/r-slides/networks/index.html#section",
    "title": "Introduction to Networks in R",
    "section": "",
    "text": "background images"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#setting-background-colors",
    "href": "content/slides/r-slides/networks/index.html#setting-background-colors",
    "title": "Introduction to Networks in R",
    "section": "Setting background colors",
    "text": "Setting background colors\nyou can set your background as you normally would"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#video-slide-title",
    "href": "content/slides/r-slides/networks/index.html#video-slide-title",
    "title": "Introduction to Networks in R",
    "section": "Video Slide Title",
    "text": "Video Slide Title\nThis slides’s background video will play in a loop with audio muted."
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#slide-title",
    "href": "content/slides/r-slides/networks/index.html#slide-title",
    "title": "Introduction to Networks in R",
    "section": "Slide Title",
    "text": "Slide Title"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#further-modifying-theme",
    "href": "content/slides/r-slides/networks/index.html#further-modifying-theme",
    "title": "Introduction to Networks in R",
    "section": "Further Modifying theme",
    "text": "Further Modifying theme\nIf you want to modify theme, you can specify the .scss my modifying the yaml to look like this\nformat: \n  letterbox-revealjs:\n    theme: [default, style.scss]"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#modifying-letterbox-background",
    "href": "content/slides/r-slides/networks/index.html#modifying-letterbox-background",
    "title": "Introduction to Networks in R",
    "section": "Modifying letterbox background",
    "text": "Modifying letterbox background\nThe background colors can be with with, where #444444 represents the new background color and #222222 represents the color of the shadow\n.quarto-light {\n  background-color: #444444;\n}\n\n.quarto-dark {\n  background-color: #444444;\n}\n\n.slides {\n  box-shadow: #222222 0px 0px 30px 0px;\n}"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#quarto",
    "href": "content/slides/r-slides/networks/index.html#quarto",
    "title": "Introduction to Networks in R",
    "section": "Quarto",
    "text": "Quarto\nQuarto enables you to weave together content and executable code into a finished presentation. To learn more about Quarto presentations see https://quarto.org/docs/presentations/."
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#bullets",
    "href": "content/slides/r-slides/networks/index.html#bullets",
    "title": "Introduction to Networks in R",
    "section": "Bullets",
    "text": "Bullets\nWhen you click the Render button a document will be generated that includes:\n\nContent authored with markdown\nOutput from executable code"
  },
  {
    "objectID": "content/slides/r-slides/networks/index.html#code",
    "href": "content/slides/r-slides/networks/index.html#code",
    "title": "Introduction to Networks in R",
    "section": "Code",
    "text": "Code\nWhen you click the Render button a presentation will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "content/slides/r-slides/spatial/PlayingwithLeaflet.knit.html",
    "href": "content/slides/r-slides/spatial/PlayingwithLeaflet.knit.html",
    "title": "Playing with Leaflet",
    "section": "",
    "text": "This Tutorial works through the ideas at Leaflet\n\nLeaflet is a JavaScript library for creating dynamic maps that support panning and zooming along with various annotations like markers, polygons, and popups.\n\nIn this tutorial we will work only with vector data. In a second part, we will work with raster data in leaflet.\n\n```{r setup}\nlibrary(tidyverse)\nlibrary(leaflet)\nlibrary(maps)\nlibrary(sf)\n\n# Data\nlibrary(osmdata) # Import OSM Vector Data into R\nlibrary(osmplotr) # Creating maps with OSM data in R\n# library(OpenStreetMap) # Raster Data\n```"
  },
  {
    "objectID": "content/slides/r-slides/spatial/PlayingwithLeaflet.knit.html#add-shapes-to-a-map",
    "href": "content/slides/r-slides/spatial/PlayingwithLeaflet.knit.html#add-shapes-to-a-map",
    "title": "Playing with Leaflet",
    "section": "Add Shapes to a Map",
    "text": "Add Shapes to a Map\nleaflet offers several commands to add points, markers, icons, lines, polylines and polygons to a map. Let us examine a few of these.\n\nAdd Markers with popups\n\n```{r adding_markers}\nm %&gt;% addMarkers(lng = 77.580643, lat = 12.972442, \n                 popup = \"The birthplace of Rvind\")\n\n# Click on the Marker for the popup to appear\n```\n\n\n\n\n\nThis uses the default pin shape as the Marker.\n\n\nAdding Popups to a Map\nPopups are small boxes containing arbitrary HTML, that point to a specific point on the map. Use the addPopups() function to add standalone popup to the map.\n\n```{r popups}\nm %&gt;%\n  addPopups(\n    lng = 77.580643,\n    lat = 12.972442,\n    popup = paste(\n      \"The birthplace of Rvind\",\n      \"&lt;br&gt;\",\n      \"Website: https://the-foundation-series.netlify.app\",\n      \"&lt;br&gt;\"\n    ),\n    \n    # Ensuring we cannot close the popup, else we will not be able to find where it is, since there is no Marker\n    options = popupOptions(closeButton = FALSE)\n  )\n```\n\n\n\n\n\nPopups are usually added to icons, Markers and other shapes can show up when these are clicked.\n\n\nAdding Labels to a Map\nLabels are messages attached to all shapes, using the argument label wherever it is available.\nLabels are static, and Popups are usually visible on mouse click. Hence a Marker can have both a label and a popup. For example, the function addPopup() offers only a popup argument, whereas the function addMarkers() offers both a popup and a label argument.\nIt is also possible to create labels standalone using addLabelOnlyMarkers() where we can show only text and no Markers.\n\n```{r labels}\nm %&gt;%\n  addMarkers(\n    lng = 77.580643,\n    lat = 12.972442,\n    \n    # Here is the Label defn.\n    label = \"The birthplace of Rvind\",\n    labelOptions = labelOptions(noHide = TRUE, # Label always visible\n                                textOnly = F, \n                                textsize = 20),\n    \n    # And here is the popup defn.\n    popup = \"This is the Popup Text\"\n  )\n```\n\n\n\n\n\n\n\nAdding Circles and CircleMarkers on a Map\nWe can add shapes on to a map to depict areas or locations of interest. NOTE: the radius argument works differently in addCircles() and addCircleMarkers().\n\n```{r drawing_circles_on_a_map}\n#| message: false\n# Some Cities in the US and their location\nmd_cities &lt;- tibble(\n  name = c(\"Baltimore\",\"Frederick\",\"Rockville\",\"Gaithersburg\",\"Bowie\",\"Hagerstown\",\"Annapolis\",\"College Park\",\"Salisbury\",\"Laurel\"),\n  pop = c(619493,66169,62334,61045,55232,39890,38880,30587,30484,25346),\n  lat = c(39.2920592,39.4143921,39.0840,39.1434,39.0068,39.6418,38.9784,38.9897,38.3607,39.0993), \n  lng = c(-76.6077852,-77.4204875,-77.1528,-77.2014,-76.7791,-77.7200,-76.4922,-76.9378,-75.5994,-76.8483)\n)\n\n\nmd_cities %&gt;%\n  leaflet() %&gt;%\n  addTiles() %&gt;%\n  \n  # CircleMarkers, in blue\n  # radius scales the Marker. Units are in Pixels!!\n  # Here, radius is made proportional to `pop` number\n  addCircleMarkers(radius = ~ pop/1000, # Pixels!!\n                   color = \"blue\",\n                   stroke = FALSE, # no border for the Markers\n                   opacity = 0.8) %&gt;% \n  \n  \n  # Circles, in red\n  addCircles(\n    radius = 5000, # Meters !!!\n    stroke = TRUE,\n    color = \"yellow\", # Stroke Colour\n    weight = 3, # Stroke Weight\n    fill = TRUE,\n    fillColor = \"red\",\n\n  )\n```\n\n\n\n\n\nThe shapes need not be of fixed size or colour; their attributes can be made to correspond to other attribute variables in the geospatial data, as we did with radius in the addCircleMarkers() function above.\n\n\nAdding Rectangles to a Map\n\n```{r}\n## Adding Rectangles\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  setView(lng = 77.580643, lat = 12.972442, zoom = 6) %&gt;% \n  addRectangles(lat1 = 10.3858, lng1 = 75.0595, \n                lat2 = 12.8890, lng2 = 77.9625)\n```\n\n\n\n\n\n\n\nAdd Polygons to a Map\n\n```{r}\n## Adding Polygons\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  setView(lng = 77.580643, lat = 12.972442, zoom = 6) %&gt;% \n  \n  # arbitrary vector data for lat and lng\n   addPolygons(lng = c(73.5, 75.9, 76.1, 77.23, 79.8),\n               lat =c(10.12, 11.04, 11.87, 12.04, 10.7))\n```\n\n\n\n\n\n\n\nAdd PolyLines to a Map\nThis can be useful say for manually marking a route on a map, with waypoints.\n\n```{r}\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  setView(lng = 77.580643, lat = 12.972442, zoom = 6) %&gt;% \n  \n  # arbitrary vector data for lat and lng\n  # If start and end points are the same, it looks like Polygon\n  # Without the fill\n   addPolylines(lng = c(73.5, 75.9, 76.1, 77.23, 79.8),\n               lat =c(10.12, 11.04, 11.87, 12.04, 10.7)) %&gt;% \n  \n  # Add Waypoint Icons\n  addMarkers(lng = c(73.5, 75.9, 76.1, 77.23, 79.8),\n               lat =c(10.12, 11.04, 11.87, 12.04, 10.7))\n```\n\n\n\n\n\nAs seen, we have created Markers, Labels, Polygons, and PolyLines using fixed.i.e. literal text and numbers. In the following we will also see how external geospatial data columns can be used instead of these literals.\nNOTE: The mapedit package https://r-spatial.org//r/2017/01/30/mapedit_intro.html can also be used to interactively add shapes onto a map and save as an geo-spatial object."
  },
  {
    "objectID": "content/slides/r-slides/spatial/PlayingwithLeaflet.knit.html#point-data-sources-for-leaflet",
    "href": "content/slides/r-slides/spatial/PlayingwithLeaflet.knit.html#point-data-sources-for-leaflet",
    "title": "Playing with Leaflet",
    "section": "Point Data Sources for leaflet",
    "text": "Point Data Sources for leaflet\nPoint data for markers can come from a variety of sources:\n\nSpatialPoints or SpatialPointsDataFrame objects (from the sp package)\n\nPOINT, sfc_POINT, and sf objects (from the sf package); only X and Y dimensions will be considered\n\nTwo-column numeric matrices (first column is longitude, second is latitude)\n\nData frame/tibble with latitude and longitude columns. You can explicitly tell the marker function which columns contain the coordinate data (e.g. addMarkers(lng = ~Longitude, lat = ~Latitude)), or let the function look for columns named lat/latitude and lon/lng/long/longitude (case insensitive).\n\nSimply provide numeric vectors as lng and lat arguments, which we have covered already in the preceding sections.\n\nNote that MULTIPOINT objects from sf are not supported at this time.\nWe will not consider the use of sp related data structures for plotting POINTs in leaflet since sp is being phased out in favour of the more modern package sf.\n\nPoints using simple Data Frames\nLet us read in the data set from data.world that gives us POINT locations of all airports in India in a data frame / tibble. The dataset is available at https://query.data.world/s/ahtyvnm2ybylf65syp4rsb5tulxe6a. You can either download it, save a copy, and read it in as usual, or use the URL itself to read it in from the web. In the latter case, you will need the package data.world and also need to register your credentials for that page with RStudio. The (simple!) instructions are available here at data.world.\n\n```{r data.world_leaflet_example}\n#library(devtools)\n#devtools::install_github(\"datadotworld/data.world-r\", build_vignettes = TRUE)\n\nlibrary(data.world)\n\nindia_airports &lt;-\n  read_csv(\"https://query.data.world/s/ahtyvnm2ybylf65syp4rsb5tulxe6a\") %&gt;% \n  slice(-1) %&gt;% # Drop the first row which contains labels\n  dplyr::mutate(\n    id = as.integer(id),\n    latitude_deg = as.numeric(latitude_deg),\n    longitude_deg = as.numeric(longitude_deg),\n    elevation_ft = as.integer(elevation_ft)\n  ) %&gt;% \n  rename(\"lon\" = longitude_deg, \"lat\" = latitude_deg) %&gt;% \n  # Remove four locations which seem to be in the African Atlantic\n  filter(!id %in% c(330834, 330867, 325010, 331083))\n\nindia_airports %&gt;% head()\n```\n\n\n  \n\n\n\nLet us plot this in leaflet, using an ESRI National Geographic style map instead of the OSM Base Map. We will also place small circle markers for each airport.\n\n```{r}\nleaflet(data = india_airports) %&gt;% \n  setView(lat = 18, lng = 77, zoom = 4) %&gt;% \n  \n  # Add NatGeo style base map\n  addProviderTiles(providers$Esri.NatGeoWorldMap) %&gt;% # ESRI Basemap\n  \n  # Add Markers for each airport\n  addCircleMarkers(lng = ~lon, lat = ~lat,\n                   # Optional, variables stated for clarity\n                   # leaflet can automatically detect lon-lat columns\n                   # if they are appropriately named in the data\n                   # longitude/lon/lng\n                   # latitude/lat\n                   radius = 2, # Pixels\n                   color = \"red\",\n                   opacity = 1)\n```\n\n\n\n\n\nWe can also change the icon for each airport. Let us try one of theseveral icon families that we can use with leaflet : glyphicons, ionicons, and fontawesome icons.\n\n```{r airports_with_popups}\n# Define popup message for each airport\n# Based on data in india_airports\npopup &lt;- paste(\n  \"&lt;strong&gt;\",\n  india_airports$name,\n  \"&lt;/strong&gt;&lt;br&gt;\",\n  india_airports$iata_code,\n  \"&lt;br&gt;\",\n  india_airports$municipality,\n  \"&lt;br&gt;\",\n  \"Elevation(feet)\",\n  india_airports$elevation_ft,\n  \"&lt;br&gt;\",\n  india_airports$wikipedia_link,\n  \"&lt;br&gt;\"\n)\n\niata_icon &lt;- makeIcon(\n  \"iata-logo.png\", # Downloaded from www.iata.org\n  iconWidth = 24,\n  iconHeight = 24,\n  iconAnchorX = 0,\n  iconAnchorY = 0\n)\n\n# Create the Leaflet map\nleaflet(data = india_airports) %&gt;%\n  setView(lat = 18, lng = 77, zoom = 4) %&gt;%\n  addProviderTiles(providers$Esri.NatGeoWorldMap) %&gt;%\n  addMarkers(\n    icon = iata_icon,\n    popup = popup\n  )\n```\n\n\n\n\n\nThere are other icons we can use to mark the POINTs. leaflet allows the use of ionicons, glyphicons, and FontAwesomeIcons\nIt is possible to create a list of icons, so that different Markers can have different icons. Let us try to map the MNCs in the ITPL area of Bangalore: we use the ideas in Using Leaflet Markers @JLA-Data.net\n\n```{r itpl}\n# Make a dataframe of addresses of Companies we wan to plot in ITPL\ncompanies_itpl &lt;-\n  data.frame(\n    ticker = c(\n      \"MBRDI\",\n      \"DTICI\",\n      \"IBM\",\n      \"Exxon\",\n      \"Mindtree\",\n      \"FIS Global\",\n      \"Sasken\",\n      \"LTI\"),\n    lat = c(\n      12.986178620989264,\n      12.984160906190121,\n      12.983659088566357,\n      12.985112265986636,\n      12.983794997606187,\n      12.980658616215155,\n      12.982080447350246,\n      12.981338168875348),\n    lon = c(\n      77.7270652183105,\n      77.72808445774321,\n      77.73103488768001,\n      77.72935046040699,\n      77.7227844126931,\n      77.72685064158782,\n      77.72545589289041,\n      77.72287024338216)\n  ) %&gt;% sf::st_as_sf(coords = c(\"lon\", \"lat\"), crs = 4326)\n \n# Vanilla leaflet map\nleaflet(companies_itpl) %&gt;% \n  addTiles() %&gt;% \n  addMarkers()\n```\n\n\n\n\n\nLet us make a list of logos of the Companies and use them as markers!\n\n```{r}\n# a named list of rescaled icons with links to images\nfavicons &lt;- iconList(\n  \"MBRDI\" = makeIcon(\n    iconUrl = \"https://www.mercedes-benz.com/etc/designs/brandhub/frontend/static-assets/header/logo.svg\", \n    iconWidth = 25,\n    iconHeight = 25\n  ),\n  \"DTICI\" = makeIcon(\n    iconUrl = \"https://media-exp1.licdn.com/dms/image/C4D0BAQGzOep26lC03w/company-logo_200_200/0/1638298367374?e=2147483647&v=beta&t=mPyF4gvNhNFvd-tedbqNzJofq4q9qcw6A9z9jQeLAwc\",\n    iconWidth = 45,\n    iconHeight = 45\n  ),\n  \"IBM\" = makeIcon(\n    iconUrl = \"https://www.ibm.com/favicon.ico\",\n    iconWidth = 25,\n    iconHeight = 25\n  ),\n  \"Exxon\" = makeIcon(\n    iconUrl = \"https://corporate.exxonmobil.com/-/media/Global/Icons/logos/ExxonMobilLogoColor2x.png\",\n    iconWidth = 45,\n    iconHeight = 25\n  ),\n  \"Mindtree\" = makeIcon(\n    iconUrl = \"https://www.mindtree.com/themes/custom/mindtree_theme/mindtree-lnt-logo-png.png\",\n    iconWidth = 75,\n    iconHeight = 25\n  ),\n  \"FIS Global\" = makeIcon(\n    iconUrl = \"https://1000logos.net/wp-content/uploads/2021/09/FIS-Logo-768x432.png\",\n    iconWidth = 25,\n    iconHeight = 25\n  ),\n  \"Sasken\" = makeIcon(\n    iconUrl = \"https://www.sasken.com/sites/all/themes/sasken_website/logo.png\",\n    iconWidth = 35,\n    iconHeight = 35,\n  ),\n  \"LTI\" = makeIcon(\n    iconUrl = \"https://www.lntinfotech.com/wp-content/uploads/2021/09/LTI-logo.svg\",\n    iconWidth = 25,\n    iconHeight = 25\n  )\n)\n\n\n# Create the Leaflet map\n\nleaflet(companies_itpl) %&gt;% \n  addMarkers(icon = ~ favicons[ticker], # lookup based on ticker\n             label = ~ companies_itpl$ticker,\n             labelOptions = labelOptions(noHide = F,offset = c(15,-25))) %&gt;%\n  addProviderTiles(\"CartoDB.Positron\")\n```\n\n\n\n\n\n\n\nPoints using sf objects\nWe will use data from an sf data object. This differs from the earlier situation where we had a simple data frame with lon and lat columns. In sf, the lon and lat info is embedded in the geometry column of the sf data frame.\nThe tmap package has a data set of all World metro cities, titled metro. We will plot these on the map and also scale the markers in proportion to one of the feature attributes, pop2030. The popup will be the name of the metro city. We will also use the CartoDB.Positron base map.\nNote that the metro data set has a POINT geometry, as needed!\n\n```{r,message=FALSE}\ndata(metro, package = \"tmap\")\nmetro\n\nleaflet(data = metro) %&gt;% \n  setView(lat = 18, lng = 77, zoom = 4) %&gt;% \n  \n  # Add CartoDB.Positron\n  addProviderTiles(providers$CartoDB.Positron) %&gt;% # CartoDB Basemap\n  \n  # Add Markers for each airport\n  addCircleMarkers(radius = ~ sqrt(pop2030)/350,\n                   color = \"red\",\n                   popup = paste(\"Name: \", metro$name, \"&lt;br&gt;\",\n                                  \"Population 2030: \", metro$pop2030))\n```\n\n\n  \n\n\n\n\n\n\n\nWe can also try downloading an sf data frame with POINT geometry from say OSM data&lt;https://osm. Let us get hold of restaurants data in Malleswaram, Bangalore from OSM data:\n\n```{r}\nbbox&lt;- osmdata::getbb(\"Malleswaram, Bengaluru\")\nbbox\n\nlocations &lt;- osmplotr::extract_osm_objects(\n  bbox = bbox,\n  key = \"amenity\",\n  value = \"restaurant\",\n  return_type = \"point\") \n\nlocations &lt;- locations %&gt;% \n  dplyr::filter(cuisine == \"indian\")\nlocations %&gt;% head()\n\n# Fontawesome icons seem to work in `leaflet` only up to FontAwesome V4.7.0.\n# The Fontawesome V4.7.0 Cheatsheet is here: &lt;https://fontawesome.com/v4/cheatsheet/&gt;\n\n\nleaflet(data = locations, options = leafletOptions(minZoom = 12)) %&gt;% \n  \n  addProviderTiles(providers$CartoDB.Voyager) %&gt;% \n  \n  # Regular `leaflet` code\n  addAwesomeMarkers(icon = awesomeIcons(icon = \"fa-coffee\", \n                                        library = \"fa\",\n                                        markerColor = \"blue\",\n                                        iconColor = \"black\",\n                                        iconRotate = TRUE),\n                     popup = paste(\"Name: \", locations$name,\"&lt;br&gt;\",\n                           \"Food: \", locations$cuisine)) \n```\n\n       min      max\nx 77.55033 77.59033\ny 12.98274 13.02274\n\n\n\n  \n\n\n\n\n\n\n\nFontawesome Workaround\nFor more later versions of Fontawesome, here below is a workaround from https://github.com/rstudio/leaflet/issues/691. Despite this some fontawesome icons simply do not seem to show up. ;-()\n\n```{r fontawesome_workaround}\nlibrary(fontawesome)\ncoffee &lt;- makeAwesomeIcon(\n  text = fa(\"mug-hot\"), # mug-hot was introduced in fa version 5\n  iconColor = \"black\",\n  markerColor = \"blue\",\n  library = \"fa\"\n)\n\n\nleaflet(data = locations) %&gt;% \n  addProviderTiles(providers$CartoDB.Voyager) %&gt;% \n  \n  # Workaround code\n\n  addAwesomeMarkers(icon = coffee,\n             popup = paste(\"Name: \", locations$name,\"&lt;br&gt;\",\n                           \"Food: \", locations$cuisine, \"&lt;br&gt;\"))\n```\n\n\n\n\n\nNote that leaflet automatically detects the lon/lat columns from within the POINT geometry column of the sf data frame.\n\n\nPoints using Two-Column Matrices\nWe can now quickly try providing lon and lat info in a two column matrix.This can be useful to plot a bunch of points recorded on a mobile phone app.\n\n```{r matrix_point_data}\nmysore5 &lt;- matrix(c(runif(5, 76.652985-0.01, 76.652985+0.01),\n                 runif(5, 12.311827-0.01, 12.311827+0.01)),\n                 nrow = 5)\nmysore5\n\nleaflet(data = mysore5) %&gt;% \n  addProviderTiles(providers$OpenStreetMap) %&gt;% \n  \n# Pick an icon from &lt;https://www.w3schools.com/bootstrap/bootstrap_ref_comp_glyphs.asp&gt;\n  addAwesomeMarkers(icon = awesomeIcons(\n  icon = 'music',\n  iconColor = 'black',\n  library = 'glyphicon'),\n  popup = \"Carnatic Music !!\")\n```\n\n         [,1]     [,2]\n[1,] 76.64400 12.31891\n[2,] 76.65291 12.31729\n[3,] 76.65451 12.31965\n[4,] 76.64680 12.31829\n[5,] 76.65230 12.32089"
  },
  {
    "objectID": "content/slides/r-slides/spatial/PlayingwithLeaflet.knit.html#polygons-lines-and-polylines-data-sources-for-leaflet",
    "href": "content/slides/r-slides/spatial/PlayingwithLeaflet.knit.html#polygons-lines-and-polylines-data-sources-for-leaflet",
    "title": "Playing with Leaflet",
    "section": "Polygons, Lines, and Polylines Data Sources for leaflet",
    "text": "Polygons, Lines, and Polylines Data Sources for leaflet\nWe have seen how to get POINT data into leaflet.\nLine and polygon data can come from a variety of sources:\n\nSpatialPolygons, SpatialPolygonsDataFrame, Polygons, and Polygon objects (from the sp package)\n\nSpatialLines, SpatialLinesDataFrame, Lines, and Line objects (from the sp package)\n\nMULTIPOLYGON, POLYGON, MULTILINESTRING, and LINESTRING objects (from the sf package)\n\nmap objects (from the maps package’s map() function); use map(fill = TRUE) for polygons, FALSE for polylines\n\nTwo-column numeric matrix; the first column is longitude and the second is latitude. Polygons are separated by rows of (NA, NA). It is not possible to represent multi-polygons nor polygons with holes using this method; use SpatialPolygons instead.\n\nWe will concentrate on using sf data into leaflet. We may explore maps() objects at a later date.\n\nPolygons/MultiPolygons and LineString/MultiLineString using sf data frames\nLet us download College buildings, parks, and the cycling lanes in Amsterdam, Netherlands, and plot these in leaflet.\n\n```{r, cache=TRUE}\nbbox &lt;- osmdata::getbb(\"Amsterdam, Netherlands\")\nbbox\n# Run the lines below ONE TIME in your CONSOLE!\n# \n# colleges &lt;- osmplotr::extract_osm_objects(bbox = bbox,\n#                                            key = \"amenity\",\n#                                            value = \"college\",\n#                                            return_type = \"polygon\" )\n# parks &lt;- osmplotr::extract_osm_objects(bbox = bbox,\n#                                            key = \"park\",\n#                                            return_type = \"polygon\" )\n# roads &lt;- osmplotr::extract_osm_objects(bbox = bbox,\n#                                        key = \"highway\",\n#                                        value = \"primary\",\n#                                        return_type = \"line\")\n# cyclelanes &lt;-\n#   osmplotr::extract_osm_objects(bbox,\n#                                 key = \"cycleway\",\n#                                 value =  \"lane\",\n#                                 return_type = \"line\")\n# st_write(colleges, \n#          dsn = \"colleges.gpkg\", \n#          append = FALSE, \n#          quiet = FALSE)\n# st_write(parks, \n#          dsn = \"parks.gpkg\", \n#          append = FALSE, \n#          quiet = FALSE)\n# st_write(roads, \n#          dsn = \"roads.gpkg\", \n#          append = FALSE, \n#          quiet = FALSE)\n# st_write(cyclelanes, \n#          dsn = \"cyclelanes.gpkg\", \n#          append = FALSE, \n#          quiet = FALSE)\n```\n\n        min       max\nx  4.728756  5.079162\ny 52.278174 52.431064\n\n\n\n```{r}\ncolleges &lt;- st_read(\"./colleges.gpkg\")\nparks &lt;- st_read(\"./parks.gpkg\")\ncyclelanes &lt;- st_read(\"./cyclelanes.gpkg\")\nroads &lt;- st_read(\"./roads.gpkg\")\n```\n\nReading layer `colleges' from data source \n  `C:\\Users\\Arvind\\Documents\\R work\\MyWebsites\\my-quarto-website\\content\\slides\\r-slides\\spatial\\colleges.gpkg' \n  using driver `GPKG'\nSimple feature collection with 21 features and 38 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 4.827297 ymin: 52.27956 xmax: 4.971438 ymax: 52.3907\nGeodetic CRS:  WGS 84\nReading layer `parks' from data source \n  `C:\\Users\\Arvind\\Documents\\R work\\MyWebsites\\my-quarto-website\\content\\slides\\r-slides\\spatial\\parks.gpkg' \n  using driver `GPKG'\nSimple feature collection with 384 features and 52 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 4.74452 ymin: 52.27908 xmax: 5.070239 ymax: 52.43409\nGeodetic CRS:  WGS 84\nReading layer `cyclelanes' from data source \n  `C:\\Users\\Arvind\\Documents\\R work\\MyWebsites\\my-quarto-website\\content\\slides\\r-slides\\spatial\\cyclelanes.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1031 features and 163 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 4.717813 ymin: 52.27064 xmax: 5.061863 ymax: 52.43169\nGeodetic CRS:  WGS 84\nReading layer `roads' from data source \n  `C:\\Users\\Arvind\\Documents\\R work\\MyWebsites\\my-quarto-website\\content\\slides\\r-slides\\spatial\\roads.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1845 features and 136 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 4.72487 ymin: 52.27799 xmax: 5.082486 ymax: 52.43236\nGeodetic CRS:  WGS 84\n\n\nWe have 21 colleges in our data and 384 parks in our data.\n\n```{r}\nleaflet() %&gt;% \n  addTiles() %&gt;% \n  addPolygons(data = colleges, color= \"yellow\",\n              popup = ~colleges$name) %&gt;% \n  addPolygons(data = parks, color = \"green\", popup = parks$name) %&gt;% \n  addPolylines(data = roads, color = \"red\") %&gt;% \n  addPolylines(data = cyclelanes, color = \"purple\")\n```"
  },
  {
    "objectID": "content/slides/r-slides/spatial/PlayingwithLeaflet.knit.html#chapter-3-using-raster-data-in-leaflet",
    "href": "content/slides/r-slides/spatial/PlayingwithLeaflet.knit.html#chapter-3-using-raster-data-in-leaflet",
    "title": "Playing with Leaflet",
    "section": "Chapter 3: Using Raster Data in leaflet",
    "text": "Chapter 3: Using Raster Data in leaflet\nSo far all the geospatial data we have plotted in leaflet has been vector data. We will now explore how to plot raster data using leaflet. Raster data are used to depict continuous variables across space, such as vegitation, salinity, forest cover etc. Satellite imagery is frequently available as raster data.\n\nImporting Raster Data [Work in Progress!]\nRaster data can be imported into R in many ways:\n\nusing the maptiles package\n\nusing the OpenStreetMap package\n\n\n```{r raster_data_in_leaflet}\nlibrary(terra)\nlibrary(maptiles)\n#library(OpenStreetMap) # causes RStudio to crash...\n```"
  },
  {
    "objectID": "content/slides/r-slides/spatial/PlayingwithLeaflet.knit.html#adding-legendswork-in-progress",
    "href": "content/slides/r-slides/spatial/PlayingwithLeaflet.knit.html#adding-legendswork-in-progress",
    "title": "Playing with Leaflet",
    "section": "Adding Legends[Work in Progress!]",
    "text": "Adding Legends[Work in Progress!]\n\n```{r}\n## Generate some random lat lon data around Bangalore\ndf &lt;- data.frame(lat = runif(20, min = 11.97, max = 13.07),\n                 lng = runif(20, min = 77.48, max = 77.68),\n                 col = sample(c(\"red\", \"blue\", \"green\"), 20, \n                              replace = TRUE),\n                 stringsAsFactors = FALSE)\n\ndf %&gt;%\n  leaflet() %&gt;%\n  addTiles() %&gt;%\n  addCircleMarkers(color = df$col) %&gt;%\n  addLegend(values = df$col, labels = LETTERS[1:3], colors = c(\"blue\", \"red\", \"green\"))\n```"
  },
  {
    "objectID": "content/slides/projects-slides/portfolio/LICENSE.html",
    "href": "content/slides/projects-slides/portfolio/LICENSE.html",
    "title": "MIT License",
    "section": "",
    "text": "MIT License\nCopyright (c) 2022 quarto-letterbox authors\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/courses/ML4Artists/listing.html",
    "href": "content/courses/ML4Artists/listing.html",
    "title": "Machine Learning for Artists and Managers",
    "section": "",
    "text": "🐉 Intro to Orange\n\n\n\n\n\nUsing A Visual drag and drop tool called Orange\n\n\n\n\n\nOct 17, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\n\n\n\n\n\n\nML - Regression\n\n\n\n\n\nUsing Linear Regression to Predict Numerical Data\n\n\n\n\n\nAug 16, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\n\n\n\n\n\n\nML - Classification\n\n\n\n\n\nWe will look at the basic models for Classification of Data\n\n\n\n\n\nJul 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nML - Clustering\n\n\n\n\n\nWe will look at the basic models for Clustering of Data.\n\n\n\n\n\nJul 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n🕔 Modelling Time Series\n\n\n\n\n\nWe will look at the basic models for Time Series\n\n\n\n\n\nNov 19, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\nNo matching items\n\n Back to top",
    "crumbs": [
      "Teaching",
      "ML for Artists and Managers"
    ]
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/50-ModelTimeSeries/index.html",
    "href": "content/courses/ML4Artists/Modules/50-ModelTimeSeries/index.html",
    "title": "🕔 Modelling Time Series",
    "section": "",
    "text": "# knitr::opts_chunk$set(echo = TRUE,message = FALSE, warning = FALSE)\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(mosaic)\n\nlibrary(tsibble)\nlibrary(feasts)\nlibrary(fable)\nlibrary(sweep) # Tidy forecast Model objects\nlibrary(timetk)\nlibrary(forecast)\nlibrary(prophet)\n\nlibrary(fontawesome)",
    "crumbs": [
      "Teaching",
      "ML for Artists and Managers",
      "🕔 Modelling Time Series"
    ]
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/50-ModelTimeSeries/index.html#setup-the-packages",
    "href": "content/courses/ML4Artists/Modules/50-ModelTimeSeries/index.html#setup-the-packages",
    "title": "🕔 Modelling Time Series",
    "section": "",
    "text": "# knitr::opts_chunk$set(echo = TRUE,message = FALSE, warning = FALSE)\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(mosaic)\n\nlibrary(tsibble)\nlibrary(feasts)\nlibrary(fable)\nlibrary(sweep) # Tidy forecast Model objects\nlibrary(timetk)\nlibrary(forecast)\nlibrary(prophet)\n\nlibrary(fontawesome)",
    "crumbs": [
      "Teaching",
      "ML for Artists and Managers",
      "🕔 Modelling Time Series"
    ]
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/50-ModelTimeSeries/index.html#introduction",
    "href": "content/courses/ML4Artists/Modules/50-ModelTimeSeries/index.html#introduction",
    "title": "🕔 Modelling Time Series",
    "section": "\n Introduction",
    "text": "Introduction\nIn this module we will look at modelling of time series. We will start with the simplest of exponential models and go all the way through ARIMA and forecasting with Prophet.\nFirst, some terminology!",
    "crumbs": [
      "Teaching",
      "ML for Artists and Managers",
      "🕔 Modelling Time Series"
    ]
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/50-ModelTimeSeries/index.html#additive-and-multiplicative-time-series-models",
    "href": "content/courses/ML4Artists/Modules/50-ModelTimeSeries/index.html#additive-and-multiplicative-time-series-models",
    "title": "🕔 Modelling Time Series",
    "section": "Additive and Multiplicative Time Series Models",
    "text": "Additive and Multiplicative Time Series Models\nAdditive Time Series can be represented as:\n\\[\nY_t = S_t + T_t + ϵ_t\n\\]\nMultiplicative Time Series can be described as:\n\\[\nY_t = S_t × T_t × ϵ_t\n\\]\nLet us consider a Multiplicative Time Series, pertaining to sales of souvenirs at beaches in Australia: The time series looks like this:\n\n\n\n\n\n\nNote that along with the trend, the amplitude of both seasonal and noise components are also increasing in a multiplicative here !! A multiplicative time series can be converted to additive by taking a log of the time series.",
    "crumbs": [
      "Teaching",
      "ML for Artists and Managers",
      "🕔 Modelling Time Series"
    ]
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/50-ModelTimeSeries/index.html#stationarity",
    "href": "content/courses/ML4Artists/Modules/50-ModelTimeSeries/index.html#stationarity",
    "title": "🕔 Modelling Time Series",
    "section": "Stationarity",
    "text": "Stationarity\nA time series is said to be stationary if it holds the following conditions true:\n\nThe mean value of time-series is constant over time, which implies,the trend component is nullified/constant.\nThe variance does not increase over time.\nSeasonality effect is minimal.\n\nThis means it is devoid of trend or seasonal patterns, which makes it looks like a random white noise irrespective of the observed time interval.( i.e. self-similar and fractal)",
    "crumbs": [
      "Teaching",
      "ML for Artists and Managers",
      "🕔 Modelling Time Series"
    ]
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/50-ModelTimeSeries/index.html#a-bit-of-forecasting",
    "href": "content/courses/ML4Artists/Modules/50-ModelTimeSeries/index.html#a-bit-of-forecasting",
    "title": "🕔 Modelling Time Series",
    "section": "A Bit of Forecasting?",
    "text": "A Bit of Forecasting?\nWe are always interested in the future. We will do this in three ways:\n\nuse Simple Exponential Smoothing\nuse a package called forecast to fit an ARIMA (Autoregressive Moving Average Integrated Model) model to the data and make predictions for weekly sales;\nAnd do the same using a package called prophet.\n\nForecasting using Exponential Smoothing\nFor example, the file contains total annual rainfall in inches for London, from 1813-1912 (original data from Hipel and McLeod, 1994).\n\nrain &lt;- scan(\"https://robjhyndman.com/tsdldata/hurst/precip1.dat\", skip = 2)\nrainseries &lt;- ts(rain, start = c(1813))\nplot(rainseries)\n\n\n\n\n\n\n\nThere is a nearly constant value of about 25 around which there are random fluctuations and it seems to be an additive model. How can we make forecasts with this time series?\nA deliberate detour:\nLet’s see some quick notation to aid understanding: Much of smoothing is based on the high school concept of a straight line, \\(y = m*x + c\\).\nIn the following, we choose to describe the models with:\n\n\n\\(y\\) : the actual values in the time series\n\n\\(\\hat y\\) : our predictions from whichever model we create\n\n\\(l\\) : a level or mean as forecast;\n\n\\(b\\) : a trend variable; akin to the slope in the straight line equation;\n\n\\(s\\) : seasonal component of the time series. Note that this is a set of values that stretch over one cycle of the time series.\n\nIn Exponential Smoothing and Forecasting, we make three models of increasing complexity:\n\nSimple Exponential Model: Here we deal only with the mean or level aspect of the (decomposed) time series and make predictions with that.\nHolt Model: Here we use the level and the trend from the decomposed time series for predictions\nHolt-Winters Model: Here we use the level, the trend, and the seasonal component from the decomposed time series for predictions.\n\n\n[&lt;start&gt;st]-&gt;[&lt;input&gt;input]\n[&lt;input&gt; input]-&gt;[&lt;package&gt; Time  Series|Decomposition]\n[&lt;package&gt; Time  Series|Decomposition]-&gt;[&lt;component&gt; Mean/Level]\n[&lt;package&gt; Time  Series|Decomposition]-&gt;[&lt;component&gt; Slope/Trend]\n[&lt;package&gt; Time  Series|Decomposition]-&gt;[&lt;component&gt; Seasonal]\n\n//Simple Exponential Smoothing\n[&lt;component&gt; Mean/Level]-&gt;[Delay A1]\n[Delay A1]-&gt;[Delay A2]\n[Delay A2]-&gt;[Delay A3]\n[Delay A3]...-&gt;...[Delay AN]\n[Delay A1]-&gt;[&lt;state&gt; A1]\n[Delay A2]-&gt;[&lt;state&gt; A2]\n[Delay A3]-&gt;[&lt;state&gt; A3]\n[Delay AN]-&gt;[&lt;state&gt; AN]\n[&lt;state&gt; AN]---([&lt;note&gt; $$alpha(1-alpha)^i$$]\n\n[&lt;state&gt; A1]-&gt;[&lt;state&gt; Add1]\n[&lt;state&gt; A2]-&gt;[&lt;state&gt; Add1]\n[&lt;state&gt; A3]-&gt;[&lt;state&gt; Add1]\n[&lt;state&gt; AN]-&gt;[&lt;state&gt; Add1]\n[&lt;state&gt; Add1]-&gt;[&lt;end&gt; Output]\n\n//Holt \n[&lt;component&gt; Slope/Trend]-&gt;[Delay B1]\n[Delay B1]-&gt;[Delay B2]\n[Delay B2]-&gt;[Delay B3]\n[Delay B3]...-&gt;...[Delay BN]\n[Delay B1]-&gt;[&lt;state&gt; B1]\n[Delay B2]-&gt;[&lt;state&gt; B2]\n[Delay B3]-&gt;[&lt;state&gt; B3]\n[Delay BN]-&gt;[&lt;state&gt; BN]\n[&lt;state&gt; BN]---([&lt;note&gt; $$beta(1-beta)^i$$]\n[&lt;state&gt; B1]-&gt;[&lt;state&gt; Add2]\n[&lt;state&gt; B2]-&gt;[&lt;state&gt; Add2]\n[&lt;state&gt; B3]-&gt;[&lt;state&gt; Add2]\n[&lt;state&gt; BN]-&gt;[&lt;state&gt; Add2]\n[&lt;state&gt; Add2]-&gt;[&lt;end&gt; Output]\n\n// Holt Winters\n[&lt;component&gt; Seasonal]-&gt;[Delay C1]\n[Delay C1]-&gt;[Delay C2]\n[Delay C2]-&gt;[Delay C3]\n[Delay C3]...-&gt;...[Delay CN]\n[Delay C1]-&gt;[&lt;state&gt; C1]\n[Delay C2]-&gt;[&lt;state&gt; C2]\n[Delay C3]-&gt;[&lt;state&gt; C3]\n[Delay CN]-&gt;[&lt;state&gt; CN]\n[&lt;state&gt; CN]---([&lt;note&gt; $$gamma(1-gamma)^i$$]\n[&lt;state&gt; C1]-&gt;[&lt;state&gt; Add3]\n[&lt;state&gt; C2]-&gt;[&lt;state&gt; Add3]\n[&lt;state&gt; C3]-&gt;[&lt;state&gt; Add3]\n[&lt;state&gt; CN]-&gt;[&lt;state&gt; Add3]\n[&lt;state&gt; Add3]-&gt;[&lt;end&gt; Output]\n\n// Final Output\n[&lt;end&gt; Output]-&gt;[&lt;receiver&gt; Forecast]\n\n\n\n\n\nSimple Smoothing is smoothing based forecasting using just the level ( i.e. mean) of the Time Series to make forecasts.\nDouble exponential smoothing, or Holt Smoothing Model, is just exponential smoothing applied to both level and trend.\nThe idea behind triple exponential smoothing, or the Holt-Winters Smoothing Model, is to apply exponential smoothing to the seasonal components in addition to level and trend.\nWhat does “Exponential” mean?\nAll three models use memory: at each time instant in the Time Series, a set of past values, along with the present sample is used to make a prediction of the relevant parameter ( level / slope / seasonal). These are then added together to make the forecast.\nThe memory in each case controlled by a parameter: alpha for the estimate of the level beta for the slope estimate, and gamma for the seasonal component estimate at the current time point. All these parameters are between 0 and 1. The model takes a weighted average of past values of each parameter. The weights are derived in the form of \\(\\alpha(1-\\alpha)^i\\), where \\(i\\) defines how old the sample is compared to the present one, thus forming a set of weights that decrease exponentially with delay. Values of \\(\\alpha, \\beta. \\gamma\\) that are close to 0 mean that significant weightage is placed on observations in the past.(Memory is “stronger”). To express this in mathematical notation we now need three equations: one for level, one for the trend and one to combine the level and trend to get the expected \\(\\hat y\\).\nTo make forecasts using simple exponential smoothing in R, we can use the HoltWinters() function in R, or the forecast::ets() function from forecasts. This latter function is more powerful.\n\nargs(HoltWinters)\n\nfunction (x, alpha = NULL, beta = NULL, gamma = NULL, seasonal = c(\"additive\", \n    \"multiplicative\"), start.periods = 2, l.start = NULL, b.start = NULL, \n    s.start = NULL, optim.start = c(alpha = 0.3, beta = 0.1, \n        gamma = 0.1), optim.control = list()) \nNULL\n\nargs(forecast::ets)\n\nfunction (y, model = \"ZZZ\", damped = NULL, alpha = NULL, beta = NULL, \n    gamma = NULL, phi = NULL, additive.only = FALSE, lambda = NULL, \n    biasadj = FALSE, lower = c(rep(1e-04, 3), 0.8), upper = c(rep(0.9999, \n        3), 0.98), opt.crit = c(\"lik\", \"amse\", \"mse\", \"sigma\", \n        \"mae\"), nmse = 3, bounds = c(\"both\", \"usual\", \"admissible\"), \n    ic = c(\"aicc\", \"aic\", \"bic\"), restrict = TRUE, allow.multiplicative.trend = FALSE, \n    use.initial.values = FALSE, na.action = c(\"na.contiguous\", \n        \"na.interp\", \"na.fail\"), ...) \nNULL\n\n\nTo use HoltWinters() for simple exponential smoothing, we need to set the parameters beta=FALSE and gamma=FALSE in the HoltWinters() function (the beta and gamma parameters are used for double exponential smoothing, or triple exponential smoothing.\nTo use forecast::ets, we set the model argument to “ANN”, “AAN”, and “AAA” respectively for each of the three smoothing models.\nNote: The HoltWinters() function returns a list variable, that contains several named elements.\n\nrainseriesforecasts &lt;- forecast::ets(rainseries, model = \"ANN\")\n# class(rainseriesforecasts)\n# str(rainseriesforecasts)\nplot(rainseriesforecasts)\n\n\n\n\n\n\nplot(forecast(rainseriesforecasts, 10))\n\n\n\n\n\n\n\nARIMA\nWe can also use past trends and seasonality in the data to make predictions about the future using the forecast package. Here we use an auto ARIMA model to guess at the trend in the time series. Then we use that model to forecast a few periods into the future.\nMathematically an ARIMA model can be shown as follows:\n\n\n\n\n\n\nWe will use the familiar Walmart Sales dataset, and try to predict weekly sales for one of the Departments.\n\ndata(\"walmart_sales_weekly\")\nwalmart_wide &lt;- walmart_sales_weekly %&gt;% \n  pivot_wider(., id_cols = c(Date), \n              names_from = Dept, \n              values_from = Weekly_Sales,\n              names_prefix = \"Sales_\")\n\n## forecast::auto.arima needs a SINGLE time series, so we pick one, Dept95\nsales_95_ts &lt;- walmart_wide %&gt;% \n  select(Sales_95) %&gt;% \n  ts(start = c(2010,1), end = c(2012,52),frequency = 52)\nsales_95_ts\n\nTime Series:\nStart = c(2010, 1) \nEnd = c(2012, 52) \nFrequency = 52 \n  [1] 106690.06 111390.36 107952.07 103652.58 112807.75 112048.41 117716.13\n  [8] 113117.35 111466.37 116770.82 126341.84 110204.77 107648.14 125592.28\n [15] 120247.90 120036.99 121902.19 133056.97 131995.00 134118.05 120172.47\n [22] 124821.44 126241.20 121386.73 116256.35 108781.57 131128.96 131288.83\n [29] 124601.48 117929.58 124220.10 125027.49 124372.90 114702.69 113009.41\n [36] 120764.22 123510.99 110052.15 105793.40 110332.92 110209.31 107544.02\n [43] 106015.41 100834.31 111384.36 116521.67 121695.13  93676.95 107317.32\n [50] 109955.90 103724.16  99043.34 114270.08 117548.75 112165.80 107742.95\n [57] 116225.68 120621.32 123405.41 122280.13 112905.09 126746.25 126834.30\n [64] 118632.26 111764.31 120882.84 124953.94 112581.20 119815.67 135260.49\n [71] 136364.46 135197.63 121814.84 128054.88 133213.04 127906.50 121483.11\n [78] 117284.94 138538.47 138567.10 133260.84 122721.92 130446.34 133762.77\n [85] 133939.40 116165.28 115663.78 132805.42 125954.30 116931.34 108018.21\n [92] 114793.92 115047.16 113966.34 112688.97 102798.99 119053.80 120721.07\n [99] 125041.39  93358.91 116427.93 118685.12 113021.23 102202.04 115507.25\n[106] 125038.09 119807.63 110870.94 118406.27 125840.82 132318.50 117030.73\n[113] 127706.00 137958.76 129438.22 123172.79 118589.44 130920.36 131341.85\n[120] 129031.19 127603.00 130573.37 139857.10 140806.36 124594.40 131935.56\n[127] 148798.05 129724.74 126861.49 121030.79 134832.22 137408.20 136264.68\n[134] 118845.34 124741.33 140657.40 128542.73 119121.35 115326.47 127009.22\n[141] 124559.93 123346.24 117375.38 106690.06 111390.36 107952.07 103652.58\n[148] 112807.75 112048.41 117716.13 113117.35 111466.37 116770.82 126341.84\n[155] 110204.77 107648.14\n\narima_dept_95 &lt;- forecast::auto.arima(y = sales_95_ts)\narima_dept_95\n\nSeries: sales_95_ts \nARIMA(0,1,1)(0,1,0)[52] \n\nCoefficients:\n          ma1\n      -0.8842\ns.e.   0.0530\n\nsigma^2 = 29974424:  log likelihood = -1033.02\nAIC=2070.03   AICc=2070.15   BIC=2075.3\n\nplot(arima_dept_95)\n\n\n\n\n\n\n# Use the model to forecast 12 weeks into the future\nsales95_forecast &lt;- forecast(arima_dept_95, h = 12)\n\n# Plot the forecast. Again, we can use autoplot.\nautoplot(sales95_forecast) +\n  theme_minimal()\n\n\n\n\n\n\n\nWe’re fairly limited in what we can actually tweak when using autoplot(), so instead we can convert the forecast object to a data frame and use ggplot() like normal:\n\n# Get data out of this weird sales95_forecast object\nsales95_forecast\n\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n2013.000       116571.1 109554.8 123587.5 105840.6 127301.7\n2013.019       126102.0 119038.7 133165.2 115299.7 136904.3\n2013.038       120871.5 113761.7 127981.4 109998.0 131745.1\n2013.058       111934.8 104778.7 119091.0 100990.5 122879.2\n2013.077       119470.2 112268.0 126672.3 108455.5 130484.9\n2013.096       126904.7 119656.9 134152.5 115820.1 137989.3\n2013.115       133382.4 126089.2 140675.6 122228.3 144536.5\n2013.135       118094.6 110756.3 125433.0 106871.6 129317.7\n2013.154       128769.9 121386.7 136153.1 117478.2 140061.6\n2013.173       139022.7 131594.8 146450.5 127662.8 150382.5\n2013.192       130502.1 123030.0 137974.3 119074.5 141929.8\n2013.212       124236.7 116720.5 131752.9 112741.7 135731.7\n\nsales95_forecast_tidy &lt;- sweep::sw_sweep(sales95_forecast, \n                                         fitted = TRUE, \n                                         timetk_idx = TRUE)\n\nsales95_forecast_tidy\n\n\n  \n\n\n# For whatever reason, the date column here is a special type of variable called\n# \"yearmon\", which ggplot doesn't know how to deal with (like, we can't zoom in\n# on the plot with coord_cartesian). We use zoo::as.Date() to convert the\n# yearmon variable into a regular date\nsales95_forecast_tidy_real_date &lt;- \n  sales95_forecast_tidy %&gt;% \n  mutate(actual_date = zoo::as.Date(index, frac = 1))\nsales95_forecast_tidy_real_date\n\n\n  \n\n\n# Plot this puppy!\nggplot(sales95_forecast_tidy, aes(x = index, y = value, color = key)) +\n  geom_ribbon(aes(ymin = lo.95, ymax = hi.95), \n              fill = \"#3182bd\", color = NA) +\n  geom_ribbon(aes(ymin = lo.80, ymax = hi.80, fill = key), \n              fill = \"#deebf7\", color = NA, alpha = 0.8) +\n  geom_line(size = 1) + \n  geom_point(size = 0.5) +\n  labs(x = NULL, y = \"sales95\") +\n  scale_y_continuous(labels = scales::comma) +\n  # Zoom in on 2012-2016\n  #coord_cartesian(xlim = ymd(c(\"2004-07-01\", \"2007-07-31\"))) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\nplot_time_series(.data = sales95_forecast_tidy,.date_var = index,.value = value,.color_var = key,.smooth = FALSE)\n\n\n\n\n\nA Bit of Forecasting?\nWe are always interested in the future. We will do this in three ways:\n\nuse Simple Exponential Smoothing\nuse a package called forecast to fit an ARIMA (Auto-regressive Moving Average Integrated Model) model to the data and make predictions for weekly sales;\nAnd do the same using a package called ’prophet`.",
    "crumbs": [
      "Teaching",
      "ML for Artists and Managers",
      "🕔 Modelling Time Series"
    ]
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/50-ModelTimeSeries/index.html#conclusion",
    "href": "content/courses/ML4Artists/Modules/50-ModelTimeSeries/index.html#conclusion",
    "title": "🕔 Modelling Time Series",
    "section": "Conclusion",
    "text": "Conclusion",
    "crumbs": [
      "Teaching",
      "ML for Artists and Managers",
      "🕔 Modelling Time Series"
    ]
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/50-ModelTimeSeries/index.html#references",
    "href": "content/courses/ML4Artists/Modules/50-ModelTimeSeries/index.html#references",
    "title": "🕔 Modelling Time Series",
    "section": "References",
    "text": "References\n1, Shampoo Dataset Brownlee: https://raw.githubusercontent.com/jbrownlee/Datasets/master/shampoo.csv",
    "crumbs": [
      "Teaching",
      "ML for Artists and Managers",
      "🕔 Modelling Time Series"
    ]
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/30-Classification/index.html",
    "href": "content/courses/ML4Artists/Modules/30-Classification/index.html",
    "title": "ML - Classification",
    "section": "",
    "text": "Have you played a Childhood Game called 20 Questions? Someone has a “target” entity in mind ( a person or a thing or a literary character) and the others need to discover that entity by asking 20 questions.\n\nHow does one create questions in the game?\n\nCategories?\nNumbers? How?\nComparisons?\n\n\nWhat sort of answers can you expect for each question?",
    "crumbs": [
      "Teaching",
      "ML for Artists and Managers",
      "ML - Classification"
    ]
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/30-Classification/index.html#a-childhood-game",
    "href": "content/courses/ML4Artists/Modules/30-Classification/index.html#a-childhood-game",
    "title": "ML - Classification",
    "section": "",
    "text": "Have you played a Childhood Game called 20 Questions? Someone has a “target” entity in mind ( a person or a thing or a literary character) and the others need to discover that entity by asking 20 questions.\n\nHow does one create questions in the game?\n\nCategories?\nNumbers? How?\nComparisons?\n\n\nWhat sort of answers can you expect for each question?",
    "crumbs": [
      "Teaching",
      "ML for Artists and Managers",
      "ML - Classification"
    ]
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/30-Classification/index.html#twenty-questions-game-as-a-play-with-data",
    "href": "content/courses/ML4Artists/Modules/30-Classification/index.html#twenty-questions-game-as-a-play-with-data",
    "title": "ML - Classification",
    "section": "Twenty Questions Game as a Play with Data…",
    "text": "Twenty Questions Game as a Play with Data…\nAssuming we think of a 20Q Target as say, celebrity singer like Taylor Swift, or a cartoon character like Thomas the Tank Engine, what would an underlying “data structure” look like? We would ask Questions for instance in the following order to find the target of Taylor Swift:\n\nHuman?(Yes)\nLiving?(Yes)\nMale?(No)\nCelebrity?(Yes)\nMusic?(Yes)\nUSA?(Yes)\n\nOh…Taylor Swift!!!\nLet us try to construct the “datasets” underlying this game!\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nOccupation\nSex\nLiving\nNationality\ngenre\npet\n\n\nTaylor Swift\nSinger\nF\nTRUE\nUSA\ncountry/rock\nScottish Fold Cats\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nType\nLiving\nhuman\nNationality\ncolour\nmaterial\n\n\nThomas, the Tank Engine\nCartoon Character\nFALSE\nFALSE\nUK\nblue\nmetal\n\n\n\n\nIt should be fairly clear that the Questions we ask are based on the COLUMNs in the respective 1-row datasets! The TARGET Column in both cases is the name column.",
    "crumbs": [
      "Teaching",
      "ML for Artists and Managers",
      "ML - Classification"
    ]
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/30-Classification/index.html#what-is-a-decision-tree",
    "href": "content/courses/ML4Artists/Modules/30-Classification/index.html#what-is-a-decision-tree",
    "title": "ML - Classification",
    "section": "What is a Decision Tree?",
    "text": "What is a Decision Tree?\nCan you imagine how the 20 Questions Game can be shown as a tree?\n\n\n\n\n\n\nEach Question we ask, based on one of the Feature columns, begets a Yes/NO answer and we turn the left or right accordingly. When we arrive at the leaf, we should be in a position to guess the answer !",
    "crumbs": [
      "Teaching",
      "ML for Artists and Managers",
      "ML - Classification"
    ]
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/30-Classification/index.html#twenty-times-20-questions",
    "href": "content/courses/ML4Artists/Modules/30-Classification/index.html#twenty-times-20-questions",
    "title": "ML - Classification",
    "section": "Twenty times 20 Questions !!",
    "text": "Twenty times 20 Questions !!\nWhat if the dataset we had contained many rows, instead of just one row? How would we play the 20Q Game in this situation? Here is a sample of the famous penguins dataset:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\nAdelie\nTorgersen\n36.2\n17.2\n187\n3150\nfemale\n2009\n\n\nGentoo\nBiscoe\nNA\nNA\nNA\nNA\nNA\n2009\n\n\nGentoo\nBiscoe\n45.7\n13.9\n214\n4400\nfemale\n2008\n\n\nGentoo\nBiscoe\n43.2\n14.5\n208\n4450\nfemale\n2008\n\n\nAdelie\nDream\n41.5\n18.5\n201\n4000\nmale\n2009\n\n\nAdelie\nDream\n38.8\n20.0\n190\n3950\nmale\n2007\n\n\nGentoo\nBiscoe\n45.5\n14.5\n212\n4750\nfemale\n2009\n\n\nAdelie\nBiscoe\n38.2\n18.1\n185\n3950\nmale\n2007\n\n\nAdelie\nTorgersen\n39.0\n17.1\n191\n3050\nfemale\n2009\n\n\nAdelie\nTorgersen\n41.1\n17.6\n182\n3200\nfemale\n2007\n\n\nAdelie\nDream\n39.2\n18.6\n190\n4250\nmale\n2009\n\n\nAdelie\nTorgersen\n42.8\n18.5\n195\n4250\nmale\n2008\n\n\n\n\n\nAs before, we would need to look at the dataset as containing a TARGET column which we want to predict using several other FEATURE columns. Let us choose species.\nWhen we look at the FEATURE columns, We would need to formulate questions based on entire columns at a time. For instance:\n\n\n“Is the bill_length_mm* greater than 45mm?” considers the entire bill_length_mm* FEATURE column\nIs the sex female? considers the entire sex column\n\nIf the specific FEATURE column is a Numerical (N) variable, the question would use some “thresholding” as shown in the question above, to convert the Numerical Variable into a Categorical variable.\nIf a specific FEATURE column is a Categorical (C) variable, the question would be like a filter operation in Excel.\nEither way, we end up answering with a smaller and smaller subset of rows in the dataset, to which the questions are answered with a Yes. It is as if we played many 20 Questions games in parallel, since there are so many simultaneous “answers”!\nOnce we exhaust all the FEATURE columns, then what remains is a subset (i.e. rows) of the original dataset and we read off the TARGET column, which should now contain a set of identical entries, e.g. “Adelie”. Thus we can extend a single-target 20Q game to a multiple-target one using a larger dataset. ( Note how the multiple targets are all the same: “Adelie”, or “Gentoo”, or “Chinstrap”)\nThis forms the basic intuition for a Machine Learning Algorithm called a Decision Tree.\nDecision Tree in Orange\nLet us visualize this Decision Tree in Orange. Look at the now famous penguins dataset, available here:\nhttps://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\nWe see that there are three species of penguins, that live on three islands. The measurements for each penguin are flipper_length_mm, bill_length_mm, bill_depth_mm, and body_mass_g.\n\nTask 1: Create a few data visualizations for the variables, and pairs of variables from this dataset.\nTask 2: Can you inspect the visualizations and imagine how each of this dataset can be used in a 20 Questions Game, to create a Decision Tree for this dataset as shown below?\n\n\n\nPenguins Decision Tree!\n\nWhat did we learn?\n\nThe 20Q Game can be viewed as a “Decision Tree” of Questions and Answers,\nEach fork in the game is a Question.\nDepending upon whether the current answer is yes or no, we turn in one direction or the other.\nEach of our questions is based on the information available in one or other of the columns!!\nWe arrive at a final “answer” or “target” after a particular sequence of yes/no answers. This is the one of the leaf nodes in the Tree.\nThe island and the species columns are categories and are especially suited to being the targets for a 20 Questions Game.\nWe can therefore use an entire column of data as our 20Questions target, rather than just one entity, person.\n\nThis is how we will use this Game as a Model for our first ML algorithm, classification using Decision Trees.",
    "crumbs": [
      "Teaching",
      "ML for Artists and Managers",
      "ML - Classification"
    ]
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/30-Classification/index.html#how-do-we-make-predictions-using-our-decision-tree",
    "href": "content/courses/ML4Artists/Modules/30-Classification/index.html#how-do-we-make-predictions-using-our-decision-tree",
    "title": "ML - Classification",
    "section": "How do we Make Predictions using our Decision Tree",
    "text": "How do we Make Predictions using our Decision Tree\nOur aim is to make predictions. Predictions of what? When we are given new unseen data in the same format, we should be able to predict TARGET variable using the same FEATURE columns.\nNOTE: This that is usually a class/category (We CAN also predict a numerical value with a Decision Tree; but we will deal with that later.)\nIn order to make predictions with completely unseen data, we need to first check if the algorithm is working well with known data. The way to do this is to use a large portion of data to design the tree, and then use the tree to predict some aspect of the remaining, but similar, data. Let us split the penguins dataset into two pieces: a training set to design our tree, and a test set to check how it is working.\nDownload this penguin tree file and open it in Orange.\nHow good are the Predictions? What is the Classification Error Rate?",
    "crumbs": [
      "Teaching",
      "ML for Artists and Managers",
      "ML - Classification"
    ]
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/30-Classification/index.html#how-many-trees-do-we-need-enter-the-random-forest",
    "href": "content/courses/ML4Artists/Modules/30-Classification/index.html#how-many-trees-do-we-need-enter-the-random-forest",
    "title": "ML - Classification",
    "section": "How Many Trees do we Need? Enter the Random Forest!",
    "text": "How Many Trees do we Need? Enter the Random Forest!\nCheck all your individual Decision Trees: do they ask the same Questions? Do they fork in the same way? Yes, they all seem to use the same set of parameters to reach the target. So they are capable of being “biased” and make the same mistakes. So we ask: Does it help to use more than one tree, if all the questions/forks in the Trees are similar?\nNo…we need different Trees to be able to ask different questions, based on different variables or features in the data. That will make the Trees as different as possible and so…unbiased. This is what we also saw when we played 20Q: offbeat questions opened up some avenues for predicting the answer/target.\nA forest of such trees is called the Wild Wood a Random Forest !",
    "crumbs": [
      "Teaching",
      "ML for Artists and Managers",
      "ML - Classification"
    ]
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/30-Classification/index.html#an-introduction-to-random-forests",
    "href": "content/courses/ML4Artists/Modules/30-Classification/index.html#an-introduction-to-random-forests",
    "title": "ML - Classification",
    "section": "An Introduction to Random Forests",
    "text": "An Introduction to Random Forests\nIn the Random Forest method, we do as follows:\n\nSplit the dataset into training and test subsets (70::30 proportion is very common). Keep aside the testing dataset for final testing.\nDecide on a number of trees, say 100-500 in the forest.\nTake the training dataset and repeatedly sample some of the rows in it. Rows can be repeated too; this is called bootstrap sampling.\nGive this sampled training set to each tree. Each tree develops a question from this dataset, in a random fashion, using a randomly chosen variable. E.g. with penguins, if our target is species, then some trees will will use island, some others will use body_mass_g and some others may use bill_length_mm.\nEach tree will “grow its questions” in a unique way !! Since the questions are possibly based on a different variable at each time, the trees will grow in very different ways.\nStop when the required accuracy has been achieved (the sets contain observations/rows from only one species predominantly)\nWith the test set let each tree vote on which species it has decided upon. Take the majority vote.\n\nPhew!!\nLet’s get a visual sense of how all this works:\nhttps://waternova.github.io/random-forest-viz/",
    "crumbs": [
      "Teaching",
      "ML for Artists and Managers",
      "ML - Classification"
    ]
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/30-Classification/index.html#random-forest-classification-for-heart-patients",
    "href": "content/courses/ML4Artists/Modules/30-Classification/index.html#random-forest-classification-for-heart-patients",
    "title": "ML - Classification",
    "section": "Random Forest Classification for Heart Patients",
    "text": "Random Forest Classification for Heart Patients\nDo you want to develop an ML model for heart patients? We have a dataset of heart patients at the University of California, Arvind Irvine ML Dataset Repository\nHeart Patient Data. Import into Orange !!\nWhat are the variables?\n\n(age): age in years\n(sex): 1 = male; 0 = female\n(cp): chest-pain type( 4 types, 1/2/3/4)\n(trestbps): resting blood pressure (in mm Hg on admission to the hospital)\n(chol) : serum cholesterol in mg/dl\n(fbs): (fasting blood sugar &gt; 120 mg/dl) (1 = true; 0 = false)\n(restecg): resting electrocardiograph results (0 = normal; 1= ST-T wave abnormality; 3 = LV hypertrophy)\n(thalach): maximum heart rate achieved\n(exang): exercise induced angina (1 = yes; 0 = no) (remember Puneet Rajkumar)\n(oldpeak): ST depression induced by exercise relative to rest\n(slope): the slope of the peak exercise ST segment\n\nValue 1: upsloping\nValue 2: flat\nValue 3: downsloping\n\n\n(ca): number of major vessels (0-3) colored by fluoroscopy\n(thal): 3 = normal; 6 = fixed defect; 7 = reversible defect\n(num) : the target attribute, diagnosis of heart disease (angiographic disease status)\n\nValue 0: &lt; 50% diameter narrowing\nValue 1: &gt; 50% diameter narrowing\n(in any major vessel: attributes 59 through 68 are vessels)\n\n\n\nWe will create a Random Forest Model for this dataset, and compare with the Decision Tree for the same dataset.",
    "crumbs": [
      "Teaching",
      "ML for Artists and Managers",
      "ML - Classification"
    ]
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/30-Classification/index.html#how-good-is-my-random-forest",
    "href": "content/courses/ML4Artists/Modules/30-Classification/index.html#how-good-is-my-random-forest",
    "title": "ML - Classification",
    "section": "How good is my Random Forest?",
    "text": "How good is my Random Forest?\n\nClassification Error\n\nGini Impurity\n\nCross Entropy",
    "crumbs": [
      "Teaching",
      "ML for Artists and Managers",
      "ML - Classification"
    ]
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/30-Classification/index.html#references",
    "href": "content/courses/ML4Artists/Modules/30-Classification/index.html#references",
    "title": "ML - Classification",
    "section": "References",
    "text": "References\n\nhttps://towardsdatascience.com/data-science-made-easy-data-modeling-and-prediction-using-orange-f451f17061fa\nThe beauty of Random Forests: https://orangedatamining.com/blog/2016/12/22/the-beauty-of-random-forest/\nPythagorean Trees for Random Forests: https://orangedatamining.com/blog/2016/07/29/pythagorean-trees-and-forests/\ndata.tree sample applications, Christoph Glur, 2020-07-31. https://cran.r-project.org/web/packages/data.tree/vignettes/applications.html\nhttps://ryjohnson09.netlify.app/post/caret-and-tidymodels/",
    "crumbs": [
      "Teaching",
      "ML for Artists and Managers",
      "ML - Classification"
    ]
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/40-Clustering/index.html",
    "href": "content/courses/ML4Artists/Modules/40-Clustering/index.html",
    "title": "ML - Clustering",
    "section": "",
    "text": "Quoting from http://baoqiang.org/?p=579\n\n\nThese two are arguably the two commonly used cluster methods. One of the reasons is that they are easy to use and also somehow straightforward. So how do they work?\nk-Nearest-Neighbour: Provide N n-dimension entries with known associated classes for each entry, the number of classes is k, that is, \\[\n\\{\\vec{x_i}, y_i\\} ,\\ \\vec{x_i} \\in\\ {\\Re^{n}}\\ , y_i\\ = \\{c_1,...c_k\\},\ni = 1...N\n\\]\nFor a new entry \\(\\vec{v_j}\\), to which class should it belong? We need use a distance measure to get the k closest entries of the new entry , the final decision is simple majority vote based the closest k neighbors. The distance metric could be euclidean or other similar ones.\n\n\n\n\nK-means: Given N n-dimension entries and classify them in k classes. At first, we randomly choose k entries and assign them to k clusters. They are the seed classes. Then we calculate the distance between each entry and each class. Each entry will be assigned into one class in terms of the its distance to each class, i.e., assign the entry to its closest class. After the assignment is complete, we then calculate the centroid of each class based on their new members. After the centroid calculation, we go back to the distance calculation and therefore new round classification. We stop the iteration when there is convergence,i.e,, no new centroid and classification.\nThe two methods are all semi-supervised learning algorithms because they do need we provide the number of clusters prior the clustering.",
    "crumbs": [
      "Teaching",
      "ML for Artists and Managers",
      "ML - Clustering"
    ]
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/40-Clustering/index.html#introduction",
    "href": "content/courses/ML4Artists/Modules/40-Clustering/index.html#introduction",
    "title": "ML - Clustering",
    "section": "",
    "text": "Quoting from http://baoqiang.org/?p=579\n\n\nThese two are arguably the two commonly used cluster methods. One of the reasons is that they are easy to use and also somehow straightforward. So how do they work?\nk-Nearest-Neighbour: Provide N n-dimension entries with known associated classes for each entry, the number of classes is k, that is, \\[\n\\{\\vec{x_i}, y_i\\} ,\\ \\vec{x_i} \\in\\ {\\Re^{n}}\\ , y_i\\ = \\{c_1,...c_k\\},\ni = 1...N\n\\]\nFor a new entry \\(\\vec{v_j}\\), to which class should it belong? We need use a distance measure to get the k closest entries of the new entry , the final decision is simple majority vote based the closest k neighbors. The distance metric could be euclidean or other similar ones.\n\n\n\n\nK-means: Given N n-dimension entries and classify them in k classes. At first, we randomly choose k entries and assign them to k clusters. They are the seed classes. Then we calculate the distance between each entry and each class. Each entry will be assigned into one class in terms of the its distance to each class, i.e., assign the entry to its closest class. After the assignment is complete, we then calculate the centroid of each class based on their new members. After the centroid calculation, we go back to the distance calculation and therefore new round classification. We stop the iteration when there is convergence,i.e,, no new centroid and classification.\nThe two methods are all semi-supervised learning algorithms because they do need we provide the number of clusters prior the clustering.",
    "crumbs": [
      "Teaching",
      "ML for Artists and Managers",
      "ML - Clustering"
    ]
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/40-Clustering/index.html#workflow-using-orange",
    "href": "content/courses/ML4Artists/Modules/40-Clustering/index.html#workflow-using-orange",
    "title": "ML - Clustering",
    "section": "Workflow using Orange",
    "text": "Workflow using Orange",
    "crumbs": [
      "Teaching",
      "ML for Artists and Managers",
      "ML - Clustering"
    ]
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/40-Clustering/index.html#workflow-using-radiant",
    "href": "content/courses/ML4Artists/Modules/40-Clustering/index.html#workflow-using-radiant",
    "title": "ML - Clustering",
    "section": "Workflow using Radiant",
    "text": "Workflow using Radiant",
    "crumbs": [
      "Teaching",
      "ML for Artists and Managers",
      "ML - Clustering"
    ]
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/40-Clustering/index.html#workflow-using-r",
    "href": "content/courses/ML4Artists/Modules/40-Clustering/index.html#workflow-using-r",
    "title": "ML - Clustering",
    "section": "Workflow using R",
    "text": "Workflow using R",
    "crumbs": [
      "Teaching",
      "ML for Artists and Managers",
      "ML - Clustering"
    ]
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/40-Clustering/index.html#conclusion",
    "href": "content/courses/ML4Artists/Modules/40-Clustering/index.html#conclusion",
    "title": "ML - Clustering",
    "section": "Conclusion",
    "text": "Conclusion",
    "crumbs": [
      "Teaching",
      "ML for Artists and Managers",
      "ML - Clustering"
    ]
  },
  {
    "objectID": "content/courses/ML4Artists/Modules/40-Clustering/index.html#references",
    "href": "content/courses/ML4Artists/Modules/40-Clustering/index.html#references",
    "title": "ML - Clustering",
    "section": "References",
    "text": "References\n\nK-means Cluster Analysis. UC Business Analytics R Programming Guide https://uc-r.github.io/kmeans_clustering#optimal\nThean C Lim. Clustering: k-means, k-means ++ and gganimate. https://theanlim.rbind.io/post/clustering-k-means-k-means-and-gganimate/\nhttps://www.datacamp.com/tutorial/hierarchical-clustering-R\nhttps://www.datacamp.com/tutorial/k-means-clustering-r\nMichele Coscia. 2019. Who will Cluster the Cluster Makers? https://www.michelecoscia.com/?p=1709 Accessed 12 Jan 2024.",
    "crumbs": [
      "Teaching",
      "ML for Artists and Managers",
      "ML - Clustering"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/01-NatureofData/index.html",
    "href": "content/courses/NoCode/Modules/01-NatureofData/index.html",
    "title": "🕶 Science, Human Experience, Experiments, and Data",
    "section": "",
    "text": "We will need to form a basic understanding of basic scientific enterprise. Let us look at the slides.",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🕶 Science, Human Experience, Experiments, and Data"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/01-NatureofData/index.html#sec-where-data",
    "href": "content/courses/NoCode/Modules/01-NatureofData/index.html#sec-where-data",
    "title": "🕶 Science, Human Experience, Experiments, and Data",
    "section": "",
    "text": "We will need to form a basic understanding of basic scientific enterprise. Let us look at the slides.",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🕶 Science, Human Experience, Experiments, and Data"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/01-NatureofData/index.html#why-visualize",
    "href": "content/courses/NoCode/Modules/01-NatureofData/index.html#why-visualize",
    "title": "🕶 Science, Human Experience, Experiments, and Data",
    "section": "\n Why Visualize?",
    "text": "Why Visualize?\n\nWe can digest information more easily when it is pictorial\nOur Working Memories are both short-term and limited in capacity. So a picture abstracts the details and presents us with an overall summary, an insight, or a story that is both easy to recall and easy on retention.\nData Viz includes shapes that carry strong cultural memories; and impressions for us. These cultural memories help us to use data viz in a universal way to appeal to a wide variety of audiences. (Do humans have a gene for geometry?1);\nIt helps sift facts and mere statements: for example:\n\n\n\n\n\n\nFigure 1: Rape Capital\n\n\n\n\n\n\n\nFigure 2: Data Reveals Crime",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🕶 Science, Human Experience, Experiments, and Data"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/01-NatureofData/index.html#why-analyze",
    "href": "content/courses/NoCode/Modules/01-NatureofData/index.html#why-analyze",
    "title": "🕶 Science, Human Experience, Experiments, and Data",
    "section": "\n Why Analyze?",
    "text": "Why Analyze?\nFor us to draw good inferences and make actionable plans based on data, it has to satisfy specific statistical tests. These tests tell us how significant the data are and whether the data are telling us a story that can be justified, or it is just a random coincidence. Depending upon our Research Question, different statistical tests will be applicable.",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🕶 Science, Human Experience, Experiments, and Data"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/01-NatureofData/index.html#what-are-data-types",
    "href": "content/courses/NoCode/Modules/01-NatureofData/index.html#what-are-data-types",
    "title": "🕶 Science, Human Experience, Experiments, and Data",
    "section": "\n What are Data Types?",
    "text": "What are Data Types?\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Data\n\n\n\nEach variable is a column; a column contains one kind of data. Each observation or case is a row.",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🕶 Science, Human Experience, Experiments, and Data"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/01-NatureofData/index.html#sec-data-types",
    "href": "content/courses/NoCode/Modules/01-NatureofData/index.html#sec-data-types",
    "title": "🕶 Science, Human Experience, Experiments, and Data",
    "section": "\n How do we Spot Data Variable Types?",
    "text": "How do we Spot Data Variable Types?\nBy asking questions! Shown below is a table of different kinds of questions you could use to query a dataset. The variable or variables that “answer” the question would be in the category indicated by the question.\n\n\n\n\n\n\nNo\nPronoun\nAnswer\nVariable/Scale\nExample\nWhat Operations?\n\n\n\n1\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities, with Scale and a Zero Value.Differences and Ratios /Products are meaningful.\nQuantitative/Ratio\nLength,Height,Temperature in Kelvin,Activity,Dose Amount,Reaction Rate,Flow Rate,Concentration,Pulse,Survival Rate\nCorrelation\n\n\n2\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities with Scale. Differences are meaningful, but not products or ratios\nQuantitative/Interval\npH,SAT score(200-800),Credit score(300-850),SAT score(200-800),Year of Starting College\nMean,Standard Deviation\n\n\n3\nHow, What Kind, What Sort\nA Manner / Method, Type or Attribute from a list, with list items in some \" order\" ( e.g. good, better, improved, best..)\nQualitative/Ordinal\nSocioeconomic status (Low income, Middle income, High income),Education level (HighSchool, BS, MS, PhD),Satisfaction rating(Very much Dislike, Dislike, Neutral, Like, Very Much Like)\nMedian,Percentile\n\n\n4\nWhat, Who, Where, Whom, Which\nName, Place, Animal, Thing\nQualitative/Nominal\nName\nCount no. of cases,Mode\n\n\n\n\n\n\n\nAs you go from Qualitative to Quantitative data types in the table, I hope you can detect a movement from fuzzy groups/categories to more and more crystallized numbers.\n\n\n\n\n\nType of Variables\n\nEach variable/scale can be subjected to the operations of the previous group. In the words of S.S. Stevens\n\nthe basic operations needed to create each type of scale is cumulative: to an operation listed opposite a particular scale must be added all those operations preceding it.",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🕶 Science, Human Experience, Experiments, and Data"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/01-NatureofData/index.html#some-examples-of-data-variables",
    "href": "content/courses/NoCode/Modules/01-NatureofData/index.html#some-examples-of-data-variables",
    "title": "🕶 Science, Human Experience, Experiments, and Data",
    "section": "Some Examples of Data Variables",
    "text": "Some Examples of Data Variables\n\n\n\n\n\n\nExample 1: AllCountries dataset\n\n\n\n\n\n\n  \n\n\n\nQuestions\nQ1. How many people in Andorra have internet access?\nA1. This leads to the Internet variable, which is a Quantitative variable, a proportion.2 The answer is \\(70.5\\%\\). Q2. What would be the units for LifeExpectancy? A2. Decimal Years.\n\n\n\n\n\n\n\n\n\nExample 2:StudentSurveys\n\n\n\n\n\n\n  \n\n\n\nQuestions\nQ.1. What kind of students are these?\nA.1. The variables Gender, and Year both answer to this Question. And they are both Qualitative/Categorical variables, of course.\nQ.2. What is their status in their respective families?\nA.2. Hmm…they are either first-born, or second-born, or third…etc. While this is recorded as a number, it is still a Qualitative variable3! Think! Can you do math operations with BirthOrder? Like mean or median?\nQ.3.How big are the families?\nA.3. Clearly, the variable that answers is Siblings and since the question is synonymous with “how many”, this is a Quantitative variable.",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🕶 Science, Human Experience, Experiments, and Data"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/01-NatureofData/index.html#questions-1",
    "href": "content/courses/NoCode/Modules/01-NatureofData/index.html#questions-1",
    "title": "🕶 Science, Human Experience, Experiments, and Data",
    "section": "Questions",
    "text": "Questions\nQ.1. What kind of students are these?\nA.1. The variables Gender, and Year both answer to this Question. And they are both Qualitative/Categorical variables, of course.\nQ.2. What is their status in their respective families?\nA.2. Hmm…they are either first-born, or second-born, or third…etc. While this is recorded as a number, it is still a Qualitative variable3! Think! Can you do math operations with BirthOrder? Like mean or median?\nQ.3.How big are the families?\nA.3. Clearly, the variable that answers is Siblings and since the question is synonymous with “how many”, this is a Quantitative variable.",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🕶 Science, Human Experience, Experiments, and Data"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/01-NatureofData/index.html#what-is-a-data-visualization",
    "href": "content/courses/NoCode/Modules/01-NatureofData/index.html#what-is-a-data-visualization",
    "title": "🕶 Science, Human Experience, Experiments, and Data",
    "section": "\n What is a Data Visualization?",
    "text": "What is a Data Visualization?\n\n Data Viz = Data + Geometry\n\n Shapes\nData Visualization is the act of “mapping” a geometric aspect/aesthetic to a variable in data. The aesthetic then varies in accordance with the data variable, creating (part of) a chart.\nWhat might be the geometric aesthetics available to us? An aesthetic is a geometric property, such as x-coordinate, y-coordinate, length/breadth/height,radius,shape,size, linewidth, linetype, and even colour…\n\n\n\n\n\nCommon Geometric Aesthetics in Charts\n\n\n Mapping\nWhat does this “mapping” mean? That the geometric aesthetics are used to represent qualitative or quantitative variables from your data, by varying in accordance to the data variable.\nFor instance, length or height of a bar can be made proportional to theage or income of a person. Colour of points can be mapped to gender, with a unique colour for each gender. Position along an axis x can vary in accordance with a height variable and position along the y axis can vary with a bodyWeight variable.\n\n\n\n\n\n\n\n\nA chart may use more than one aesthetic: position, shape, colour, height and angle,pattern or texture to name several. Usually, each aesthetic is mapped to just one variable to ensure there is no cognitive error. There is of course a choice and you should be able to map any kind of variable to any geometric aspect/aesthetic that may be available.\n\n\n\n\n\n\nA Natural Mapping\n\n\n\nNote that here is also a “natural” mapping between aesthetic and [kind of variable] Section 5, Quantitative or Qualitative. For instance, shape is rarely mapped to a Quantitative variable; we understand this because the nature of variation between the Quantitative variable and the shape aesthetic is not similar (i.e. not continuous). Bad choices may lead to bad, or worse, misleading charts!\n\n\nIn the above chart, it is pretty clear what kind of variable is plotted on the x-axis and the y-axis. What about colour? Could this be considered a z-axis in the chart? There are also other aspects that you can choose (not explicitly shown here) such as the plot theme(colours, fonts, backgrounds etc), which may not be mapped to data, but are nonetheless choices to be made. We will get acquainted with this aspect as we build charts.",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🕶 Science, Human Experience, Experiments, and Data"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/01-NatureofData/index.html#sec-data-viz",
    "href": "content/courses/NoCode/Modules/01-NatureofData/index.html#sec-data-viz",
    "title": "🕶 Science, Human Experience, Experiments, and Data",
    "section": "\n Basic Types of Charts",
    "text": "Basic Types of Charts\nWe can think of simple visualizations as combinations of aesthetics, mapped to combinations of variables. Some examples:\n\nGeometries , Combinations, and Graphs\n\n\n\n\n\n\n\nVariable #1\nVariable #2\nChart Names\nChart Shape\n\n\n\nQuant\nNone\nHistogram and Density\n\n\n\n\n\nQual\nNone\nBar Chart\n\n\n\nQuant\nQuant\nScatter Plot, Line Chart, Bubble Plot, Area Chart\n\n\n\n\n\nQuant\nQual\nPie Chart, Donut Chart, Column Chart, Box-Whisker Plot, Radar Chart, Bump Chart, Tree Diagram\n\n\n\n\n\nQual\nQual\nStacked Bar Chart, Mosaic Chart, Sankey, Chord Diagram, Network Diagram",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🕶 Science, Human Experience, Experiments, and Data"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/01-NatureofData/index.html#conclusion",
    "href": "content/courses/NoCode/Modules/01-NatureofData/index.html#conclusion",
    "title": "🕶 Science, Human Experience, Experiments, and Data",
    "section": "\n Conclusion",
    "text": "Conclusion\nSo there we have it:\n\nQuestions lead to Types of Variables (Quant and Qual)\n\nFurther Questions lead to relationships between them, which we describe using Data Visualizations\n\nData Visualizations are Data mapped onto Geometry \nMultiple Variable-to-Geometry Mappings = A Complete Data Visualization\n\n\nYou might think of all these Questions, Answers, Mapping as being equivalent to metaphors as a language in itself. So creating a chart is like stacking up a set of metaphors.",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🕶 Science, Human Experience, Experiments, and Data"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/01-NatureofData/index.html#references",
    "href": "content/courses/NoCode/Modules/01-NatureofData/index.html#references",
    "title": "🕶 Science, Human Experience, Experiments, and Data",
    "section": "\n References",
    "text": "References\n\nRandomized Trials:\n\n\n\n \nMartyn Shuttleworth, Lyndsay T Wilson (Jun 26, 2009). What is the Scientific Method? Retrieved Mar 12, 2024 from Explorable.com: https://explorable.com/what-is-the-scientific-method\nhttps://safetyculture.com/topics/design-of-experiments/\nOpen Intro Stats: Types of Variables\nLock, Lock, Lock, Lock, and Lock. Statistics: Unlocking the Power of Data, Third Edition, Wiley, 2021. https://www.wiley.com/en-br/Statistics:+Unlocking+the+Power+of+Data,+3rd+Edition-p-9781119674160)\nClaus Wilke. Fundamentals of Data Visualization. https://clauswilke.com/dataviz/\nTim C. Hesterberg (2015). What Teachers Should Know About the Bootstrap: Resampling in the Undergraduate Statistics Curriculum, The American Statistician, 69:4, 371-386, DOI:10.1080/00031305.2015.1089789. PDF here\n\n\n\n\nFigure 1: Rape Capital\nFigure 2: Data Reveals Crime\nType of Variables\nCommon Geometric Aesthetics in Charts",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🕶 Science, Human Experience, Experiments, and Data"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/01-NatureofData/index.html#footnotes",
    "href": "content/courses/NoCode/Modules/01-NatureofData/index.html#footnotes",
    "title": "🕶 Science, Human Experience, Experiments, and Data",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://www.xcode.in/genes-and-personality/how-genes-influence-your-math-ability/↩︎\nHow might this data have been obtained? By asking people in a survey and getting Yes/No answers!↩︎\nQualitative variables are called Factor variables in R, and are stored, internally, as numeric variables together with their levels. The actual values of the numeric variable are 1, 2, and so on.↩︎",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🕶 Science, Human Experience, Experiments, and Data"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/35-MosaicCharts/index.html",
    "href": "content/courses/NoCode/Modules/35-MosaicCharts/index.html",
    "title": "\n Mosaic Charts",
    "section": "",
    "text": "Variable #1\nVariable #2\nChart Names\nChart Shape\n\n\nQual\nQual\nMosaic Charts",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "<iconify-icon icon=\"emojione-monotone:passenger-ship\"></iconify-icon> Mosaic Charts"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/35-MosaicCharts/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/NoCode/Modules/35-MosaicCharts/index.html#what-graphs-will-we-see-today",
    "title": "\n Mosaic Charts",
    "section": "",
    "text": "Variable #1\nVariable #2\nChart Names\nChart Shape\n\n\nQual\nQual\nMosaic Charts",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "<iconify-icon icon=\"emojione-monotone:passenger-ship\"></iconify-icon> Mosaic Charts"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/35-MosaicCharts/index.html#what-kind-of-data-variables-will-we-choose",
    "href": "content/courses/NoCode/Modules/35-MosaicCharts/index.html#what-kind-of-data-variables-will-we-choose",
    "title": "\n Mosaic Charts",
    "section": "\n What kind of Data Variables will we choose?",
    "text": "What kind of Data Variables will we choose?\n\n\n\n\n\n\nNo\nPronoun\nAnswer\nVariable/Scale\nExample\nWhat Operations?\n\n\n3\nHow, What Kind, What Sort\nA Manner / Method, Type or Attribute from a list, with list items in some \" order\" ( e.g. good, better, improved, best..)\nQualitative/Ordinal\nSocioeconomic status (Low income, Middle income, High income),Education level (HighSchool, BS, MS, PhD),Satisfaction rating(Very much Dislike, Dislike, Neutral, Like, Very Much Like)\nMedian,Percentile",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "<iconify-icon icon=\"emojione-monotone:passenger-ship\"></iconify-icon> Mosaic Charts"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/35-MosaicCharts/index.html#inspiration",
    "href": "content/courses/NoCode/Modules/35-MosaicCharts/index.html#inspiration",
    "title": "\n Mosaic Charts",
    "section": "\n Inspiration",
    "text": "Inspiration\n\n\n\n\n\nFigure 1: Covid Deaths https://datatopics.worldbank.org/sdgatlas/goal-3-good-health-and-well-being?lang=en\n\n\nAccording to World Bank, six countries (India, Russia, Indonesia, United States, Brazil, and Mexico) accounted for over 60 percent of the total additional deaths in the first two years of the pandemic.",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "<iconify-icon icon=\"emojione-monotone:passenger-ship\"></iconify-icon> Mosaic Charts"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/35-MosaicCharts/index.html#how-do-these-charts-work",
    "href": "content/courses/NoCode/Modules/35-MosaicCharts/index.html#how-do-these-charts-work",
    "title": "\n Mosaic Charts",
    "section": "\n How do these Chart(s) Work?",
    "text": "How do these Chart(s) Work?\nWe saw with Bar Charts that when we deal with single Qual variables, we perform counts for each level of the variable. What is there are two Quals? Or even more?\nThe answer is to take them pair-wise and make all combinations of levels for both and calculate counts for these. This is called a Contingency Table.\n\n\n\n\n\n\nWhat is a Contingency Table?\n\n\n\nFrom Wolfram Alpha:\n\nA contingency table, sometimes called a two-way frequency table, is a tabular mechanism with at least two rows and two columns used in statistics to present categorical data in terms of frequency counts.\nMore precisely, an \\(r \\times c\\) contingency table shows the observed frequency of two variables the observed frequencies of which are arranged into \\(r\\) rows and \\(c\\) columns. The intersection of a row and a column of a contingency table is called a cell.\n\n\n\nThe Contingency Table is then plotted in a chart called the Mosaic Chart.",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "<iconify-icon icon=\"emojione-monotone:passenger-ship\"></iconify-icon> Mosaic Charts"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/35-MosaicCharts/index.html#dataset-general-social-survey-2002",
    "href": "content/courses/NoCode/Modules/35-MosaicCharts/index.html#dataset-general-social-survey-2002",
    "title": "\n Mosaic Charts",
    "section": "\n Dataset: General Social Survey 2002",
    "text": "Dataset: General Social Survey 2002\nLet us first construct a Contingency Table from this dataset, and then plot the mosaic chart for it.\n\n\n GSS data\n\n\n\n Examine the Data\n\n\n\n\n\nFigure 2: GSS2002 Data Table\n\n\n\n\n\n\n\n\n\n\n\n(a) GSS2002 Data Summary #1\n\n\n\n\n\n\n\n\n\n(b) GSS2002 Data Summary #2\n\n\n\n\n\n\nFigure 3\n\n\n\n Data Dictionary\n\n\n\n\n\n\nQuantitative Data\n\n\n\n\n\nID is the only Quant data variable!\n\n\n\n\n\n\n\n\n\nQualitative Data\n\n\n\n“ID” “Region” “Gender” “Race”\n“Education” “Marital” “Religion” “Happy”\n“Income” “PolParty” “Politics” “Marijuana”\n“DeathPenalty” “OwnGun” “GunLaw” “SpendMilitary” “SpendEduc” “SpendEnv” “SpendSci” “Pres00”\n“Postlife”\nare all Qual variables! Let us choose just two Qual variables from this dataset, DeathPenalty and Education.\n\n\nDeathPenalty: (chr) Opinion as to whether they favour or oppose the death penalty\n\nEducation: (chr) Education among respondents, 5 levels (Left HS, HS, Jr Col, Bachelors, Graduate).\n\n\n\nA Contingency table with these two Qual variables looks like Figure 4:\n\n\n\n\n\n\n\nLeft HS\nHS\nJr Col\nBachelors\nGraduate\nSum\n\n\n\nFavor\n117\n511\n71\n135\n64\n898\n\n\nOppose\n72\n200\n16\n71\n50\n409\n\n\nSum\n189\n711\n87\n206\n114\n1307\n\n\n\n\n\n\nFigure 4: Contingency Table for General Social Survey 2002\n\n\n\nNow then, how does one plot a set of data that looks like this, a matrix? No column is a single variable, nor is each row a single observation, which is what we understand with the idea of tidy data.\nThe answer is provided in the very shape of the data: we plot this as a set of tiles, where \\[ \\pmb{area~of~tile \\sim count} \\] In this way we recursively partition off a (usually) square area into vertical and horizontal pieces whose area is proportional to the count at a specific combination of levels of the two Qual variables.\n\n Research Questions\n\n\n\n\n\n\nQuestion\n\n\n\nQ1. Are Education and DeathPenalty associated?\nLet us plot the mosaic chart in two steps: we now choose Qual variables Education and DeathPenalty, in that order to plot the mosaic chart. Here are the two steps in the recursion:\n\n\n\n\n\n\n\n\n\n(a) GSS Mosaic Chart Step #1\n\n\n\n\nThis first split shows the various levels of Education and their counts as widths. This splitting corresponds to the bottom ROW of the Figure 4. HS is clearly the larget subgroup in Education .\n\n\n\n\n\n\n\n\n\n(b) GSS Mosaic Chart Step #2\n\n\n\n\n\n\nFigure 5\n\n\nIn this second step, the columns from step #1 are sliced horizontally into tiles, in proportion to the number of people in each Education category/level who support/do not support DeathPenalty. This is done in proportion to all the entries in each COLUMN.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that the order in which we choose the variables matters, since the mosaic plot is fundamentally asymmetric. More on this in a bit.\n\n\n\n\n\n\n\n\nColouring by Pearson Residuals\n\n\n\nMosaic Charts generated by Orange can be coloured based on “Pearson Residuals”. What this means is that the mosaic plot calculates what might be the “expected counts” (see below) in the Contingency Table and calculates the differences (i.e. “residuals” ) between Observed/Actual and Expected values. If the errors are negative (Obs &lt; Exp) then the tile is coloured red. And blue if the error is positive (Obs &gt; Exp).\nIn Figure 5 (b) we see that there is a small positive and a small negative residual at two locations in the mosaic chart. By and large the chart is white, showing very little association between Education and DeathPenalty. However, we should verify this using a statistical “chi-square” \\(X^2\\) test.\nMore on “expected counts” and the “chi-square” \\(X^2\\) test below.",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "<iconify-icon icon=\"emojione-monotone:passenger-ship\"></iconify-icon> Mosaic Charts"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/35-MosaicCharts/index.html#plotting-mosaic-charts",
    "href": "content/courses/NoCode/Modules/35-MosaicCharts/index.html#plotting-mosaic-charts",
    "title": "\n Mosaic Charts",
    "section": "\n Plotting Mosaic Charts",
    "text": "Plotting Mosaic Charts\n\n\nUsing Orange\nUsing RAWgraphs\nUsing DataWrapper\n\n\n\nThe description of the Orange widget for mosaic charts is here.\nLet us take a very sadly famous data set (no, not iris again 🙀), but titanic and examine it in Orange.\n Download the Orange Mosaic Chart Workflow \n\n\nNot a mosaic plot, but a Matrix Plot.\n\nDownload this RAWGraphs workflow file and import there and see.\n Download RAWgraphs Mosaic Chart Workflow \n\n\nDoes not seem to have a mosaic diagram capability.",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "<iconify-icon icon=\"emojione-monotone:passenger-ship\"></iconify-icon> Mosaic Charts"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/35-MosaicCharts/index.html#dataset-titanic",
    "href": "content/courses/NoCode/Modules/35-MosaicCharts/index.html#dataset-titanic",
    "title": "\n Mosaic Charts",
    "section": "\n Dataset: Titanic",
    "text": "Dataset: Titanic\nOk, let us see if we can rescue Jack. Here is the titanic data.\n\n Examine the Data\n\n\n\n\n\n\n\n\n\n(a) Titanic Data Table\n\n\n\n\n\n\n\n\n\n(b) Titanic Data Table\n\n\n\n\n\n\nFigure 6\n\n\nThere were 2201 passengers, as per this dataset.\n\n Data Dictionary\n\n\n\n\n\n\nQuantitative Data\n\n\n\nNone.\n\n\n\n\n\n\n\n\nQualitative Data\n\n\n\n\n\nsurvived: (chr) yes or no\n\nstatus: (chr) Class of Travel, else “crew”\n\nage: (chr) Adult, Child\n\nsex: (chr) Male / Female.\n\n\n\n\n Research Questions\n\n\n\n\n\n\nQ.1. What is the dependence of survived upon sex?\n\n\n\n\n\n\n\n\nFigure 7: Titanic Mosaic Chart\n\n\nNote the huge imbalance in survived with sex: men have clearly perished in larger numbers than women. Which is why the colouring by the Pearson Residuals show large positive residuals for men who died, and large negative residuals for women who died.\nSo sadly Jack is far more likely to have died than Rose.\n\n\n\n\n\n\n\n\nQ.2. How does survived depend upon status?\n\n\n\n\n\n\n\n\nFigure 8: Titanic Mosaic Chart\n\n\nCrew has seen deaths in large numbers, as seen by the large negative residual for crew-survivals. First Class passengers have had speedy access to the boats and have survived in larger proportions than say second or third class. There is a large positive residual for first-class survivals.\nRose travelled first class and Jack was third class. So again the odds are stacked against him.\n\n\n\n What is the Story Here?\nIn Figure 8, we have plotted sex vs status, and coloured by whether the (subset of) people survived or not. (Red is YES, Blue is NO!). As can be seen the areas are very dissimilar across both variables. More deaths occurred among the crew than among the passengers; and more first class passengers have survived than third class passengers. And of course, more men died than women.\nSo we can state that:\n\n\nStatus and Survived are not un-associated\n\nSex and Survived are not un-associated\nDoes ticking the Compare with Total box in Orange help to arrive at this inference? How so? Or does it confuse?\n\nIt remains to figure out just how serious this association is. For that we need the statistical “chi-square” \\(X^2\\) test.\n\n\n\n\n\n\nActual and “Expected” Counts\n\n\n\nThe mosaic chart is a visualization of the obtained counts, based on which the tiles are constructed.\nIt is also possible to compute a per-cell expected count, if the categorical variables are assumed independent, that is, not correlated. This is the NULL Hypothesis. The test for whether they are independent or not, as any inferential test, is based on comparing the observed counts with these expected counts under the null hypothesis. So, what might the expected frequency of a cell be in cross-tabulation table for cell \\(i,j\\) given no relationship between the variables of interest?\nRepresent the sum of row \\(i\\) with \\(n_{+i}\\), the sum of column \\(j\\) with \\(n_{j+}\\), and the grand total of all the observations with \\(n\\). And independence of variables means that their joint probability is the product of their probabilities. Therefore, the Expected Cell Frequency/Count is given by:\n\\[\n\\begin{array}{lcl} ~Expected~Count~ e_{i,j}  &=& \\frac{rowSum ~\\times~colSum}{n}\\\\\n&=& \\frac{(n_{+i})(n_{j+})}{n}\\\\\n\\end{array}\n\\]\nThe comparison of what occurred to what is expected is based on their difference, scaled by the square root of the expected, the Pearson Residual:\n\\[\n\\begin{array}{lcl} r_{i,j} &=& \\frac{(Actual - Expected)}{\\sqrt{\\displaystyle Expected}}\\\\\n&=& \\frac{(o_{i,j}- e_{i,j})}{\\sqrt{\\displaystyle e_{i,j}}}\n\\end{array}\n\\]\nThe sum of all the squared Pearson residuals is the chi-square statistic, χ2, upon which the inferential analysis follows.\n\n\n\n\n\n\n\n\n χ2-Test for the Cat-egorically Curious\n\n\n\nFor the intrepid and insatiably curious, there is an intuitive explanation, and some hand-calculations and walk-through of the Contingency table and the χ2-test here.",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "<iconify-icon icon=\"emojione-monotone:passenger-ship\"></iconify-icon> Mosaic Charts"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/35-MosaicCharts/index.html#dataset-who-does-the-housework",
    "href": "content/courses/NoCode/Modules/35-MosaicCharts/index.html#dataset-who-does-the-housework",
    "title": "\n Mosaic Charts",
    "section": "\n Dataset: Who Does the Housework?",
    "text": "Dataset: Who Does the Housework?\nLet us take this dataset on household tasks, and who does them. Download this dataset and import in into your Mosaic Chart workflow.\n Download the Household Tasks Dataset \n\n Examine the Data\n\n\n\n\n\nFigure 9: Household Tasks Distribution Raw Data\n\n\n52 observations.\n\n Data Dictionary\n\n\n\n\n\n\nQuantitative Data\n\n\n\n\n\nFreq: (int) No of times a task was carried out by specific people\n\n\n\n\n\n\n\n\n\nQualitative Data\n\n\n\n\n\nWho: (chr) Who carried out the task?\n\nTask: (chr) Task? Which task? Can’t you see I’m tired?\n\n\n\nLet us plot the mosaic chart:\n\n\n\n\n\n\n\nFigure 10: Household Tasks Distribution Raw Data\n\n\nThis data looks fine all right, but this mosaic plot looks bewildering, dumbfounding, and is of course wrong. The reason for this is that the basic HouseTasks.csv data is pre-aggregated: we have a neat column of counts already in the Freq data. And why is this a problem? Orange expects data to be purely categorical for the Mosaic Chart and does it own counting internally. It is not able to sensibly use this Freq column. Orange simply counts categories here, which are of course utterly symmetric and unique and of no use.\n\n\n\n\n\n\nStat Figures and Stats\n\n\n\nMost, if not all, statistical graphs do some internal computation. For instance the bar chart performs counts vs Qual variables; a Histogram both bins the Quant variable, and counts for entries in each bin. This is a good thing, people, but it does mean that the data needs to be in specific format before using it for plots.\n\n\nSo now what? We need to (wait for it):\n\n\nuncount the data 🙀 🙀 🙀\nHow? Take each combination of Quals Who and Task\n\nRepeat ( i.e copy-paste) that combo line as many times as the value in Freq\n\n(optionally) Deleting the Freq column, or at least not using it further\n\nAll this is (to the best of my ability) not possible in any of these trifling tools that we are using here, and can be done in a jiffy in R or Python. Didn’t I tell you coding was far far far far simpler? Peasants.\nSo following this ashtavakra procedure of jumping to another tool and coming back here, good things can be somehow made to happen, and so here is the “un-aggregated” data for you:\n\n\n Household data Cleaned up for Peasants\n\n\nImport this into Orange.\n\n Research Questions\n\n\n\n\n\n\nQ.1 Is there association between Who carries out the task, and the Task itself?\n\n\n\n\n\n\n\n\nFigure 11: Household Tasks Mosaic\n\n\n\n\n\n What is the Story Here?\nThe Mosaic plot in Figure 11 is seriously coloured, showing that there are Pearson Residuals/Errors in both directions (positive and negative). The χ2-value is large (not visible here, check in Orange) and the p-value is zero. This indicates that it is very very unlikely that this data happened by chance, assuming the two Qual variables are un-related. Hence, we are likely to conclude that our assumption that they are un-related can be rejected. (Note this complex wording here. We don’t say they are related.)\nWhy is this unsurprising? Men don’t do housework, it would seem.\nIn general, if you want to spot association, look for serious amounts of colour in the mosaic chart.",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "<iconify-icon icon=\"emojione-monotone:passenger-ship\"></iconify-icon> Mosaic Charts"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/35-MosaicCharts/index.html#your-turn",
    "href": "content/courses/NoCode/Modules/35-MosaicCharts/index.html#your-turn",
    "title": "\n Mosaic Charts",
    "section": "\n Your Turn",
    "text": "Your Turn\n\n\nClothing and Intelligence Rating of Children!! Are well-dressed actually smarter? Is that the exact reverse with SMI faculty? Or …?\n\n Download the Gilby Study dataset \n\n\nPre-marital Sex and Divorce.\n\n Download the pre- and extra-marital sex and divorce dataset",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "<iconify-icon icon=\"emojione-monotone:passenger-ship\"></iconify-icon> Mosaic Charts"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/35-MosaicCharts/index.html#wait-but-why",
    "href": "content/courses/NoCode/Modules/35-MosaicCharts/index.html#wait-but-why",
    "title": "\n Mosaic Charts",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nWe can detect correlation between Quant variables using the scatter plots and regression lines\n\nAnd we can detect association between Qual variables using mosaics and sieves (which we did not see here, but is possible in Orange)\nYour project primary research data may be pure Qualitative too, as with a Questionnaire / Survey instrument.\nOne such Qual variable therein will be your target variable\n\nYou will need to justify whether the target variable is dependent upon the other Quals, and then to decide what to do about that.\n\n\n\n\n\n\n\nSurvey Data and Likert Plots\n\n\n\nOften times, the primary research questionnaire is in the form of Questions whose answer is on a Likert Scale data, where several respondents rate a product, or a service, on a scale of Very much like, somewhat like, neutral, Dislike and Very much dislike, for example. The data are again categorical; but a Contingency Table / Mosaic Chart would be quite complex to behold and understand. A Likert Plot is what is constructed at such times. Here is a sample Likert Plot for a fictitious app called “QuickEZ”:\n\nYeah, this is possible in R and Python. But not in these barbarian tools that we are using. There are some websites that offer free apps for these plots too.\nFor more tutorial information, head off to Visualizing Survey Data (in R).",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "<iconify-icon icon=\"emojione-monotone:passenger-ship\"></iconify-icon> Mosaic Charts"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/35-MosaicCharts/index.html#references",
    "href": "content/courses/NoCode/Modules/35-MosaicCharts/index.html#references",
    "title": "\n Mosaic Charts",
    "section": "References",
    "text": "References\n\nMichael friendly. A Brief History of the Mosaic Display. https://www.datavis.ca/papers/moshist.pdf\nDavid Meyer, Achim Zeileis, Kurt Hornik. Visualizing Contingency Tables. Some very clear and simple pictures at https://statmath.wu.ac.at/projects/vcd/\nNice Chi-square interactive story at https://statisticalstories.xyz/chi-square\nA different graph on Housework Inequality,but the same story! https://datatopics.worldbank.org/sdgatlas/goal-5-gender-equality?lang=en#c4\n\n\n\n\nFigure 1: Covid Deaths https://datatopics.worldbank.org/sdgatlas/goal-3-good-health-and-well-being?lang=en\nFigure 2: GSS2002 Data Table\nFigure 3 (a): GSS2002 Data Summary #1\nFigure 3 (b): GSS2002 Data Summary #2\nFigure 5 (a): GSS Mosaic Chart Step #1\nFigure 5 (b): GSS Mosaic Chart Step #2\nFigure 6 (a): Titanic Data Table\nFigure 6 (b): Titanic Data Table\nFigure 7: Titanic Mosaic Chart\nFigure 8: Titanic Mosaic Chart\nFigure 9: Household Tasks Distribution Raw Data\nFigure 10: Household Tasks Distribution Raw Data\nFigure 11: Household Tasks Mosaic",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "<iconify-icon icon=\"emojione-monotone:passenger-ship\"></iconify-icon> Mosaic Charts"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/32-LinePlots/index.html",
    "href": "content/courses/NoCode/Modules/32-LinePlots/index.html",
    "title": "🏃 Line Plots",
    "section": "",
    "text": "Variable #1\nVariable #2\nChart Names\nChart Shape\n\n\nQuant\nQuant\nLine Plot",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🏃 Line Plots"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/32-LinePlots/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/NoCode/Modules/32-LinePlots/index.html#what-graphs-will-we-see-today",
    "title": "🏃 Line Plots",
    "section": "",
    "text": "Variable #1\nVariable #2\nChart Names\nChart Shape\n\n\nQuant\nQuant\nLine Plot",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🏃 Line Plots"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/32-LinePlots/index.html#what-kind-of-data-variables-will-we-choose",
    "href": "content/courses/NoCode/Modules/32-LinePlots/index.html#what-kind-of-data-variables-will-we-choose",
    "title": "🏃 Line Plots",
    "section": "\n What kind of Data Variables will we choose?",
    "text": "What kind of Data Variables will we choose?\n\n\n\n\n\n\nNo\nPronoun\nAnswer\nVariable/Scale\nExample\nWhat Operations?\n\n\n1\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities, with Scale and a Zero Value.Differences and Ratios /Products are meaningful.\nQuantitative/Ratio\nLength,Height,Temperature in Kelvin,Activity,Dose Amount,Reaction Rate,Flow Rate,Concentration,Pulse,Survival Rate\nCorrelation",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🏃 Line Plots"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/32-LinePlots/index.html#inspiration",
    "href": "content/courses/NoCode/Modules/32-LinePlots/index.html#inspiration",
    "title": "🏃 Line Plots",
    "section": "\n Inspiration",
    "text": "Inspiration\nEk Ledecky bheegi-bhaagi si, is it?\n\n\n\n\n\nFigure 1: LinePlot Inspiration https://fivethirtyeight.com/features/katie-ledecky-is-the-present-and-the-future-of-swimming/\n\n\nYeh Ledecky hai, ya jal-pari?\n\nIn Figure 1, the black line is the average of the 50 best times at each distance since 2000. The top 200 times for each distance since 2000 are also plotted, with light orange lines each representing one swimmer.\nHer races and her career essentially follow the same pattern — the more she swims, the more she separates from the field.\n\nHer 1500 metres record timing is better than the best time for 800m!!😱",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🏃 Line Plots"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/32-LinePlots/index.html#how-do-these-charts-work",
    "href": "content/courses/NoCode/Modules/32-LinePlots/index.html#how-do-these-charts-work",
    "title": "🏃 Line Plots",
    "section": "\n How do these Chart(s) Work?",
    "text": "How do these Chart(s) Work?\nLine Plots take two separate Quant variables as inputs. Each of the variables is mapped to a position, or coordinate: one for the X-axis, and the other for the Y-axis. Each pair of observations from the two Quant variables ( which would be in one row!) give us a point. All this much is identical with the Scatter Plot.\nAnd here, the points are connected together and sometimes thrown away altogether, leaving just the line.\nLooking at the lines, we get a very function-al sense of the variation: is it upward or downward? Is it linear or nonlinear? Is it periodic or seasonal…all these questions can be answered with Line Charts.",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🏃 Line Plots"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/32-LinePlots/index.html#plotting-a-scatter-plot",
    "href": "content/courses/NoCode/Modules/32-LinePlots/index.html#plotting-a-scatter-plot",
    "title": "🏃 Line Plots",
    "section": "\n Plotting a Scatter Plot",
    "text": "Plotting a Scatter Plot\n\n\nUsing Orange\nUsing RAWgraphs\nUsing DataWrapper\n\n\n\nAlthough this widget exists, , it does not provide quite the kind of Line Chart is considered standard in the data viz industry.\nNonetheless, let us at least look at this data in Orange, before plotting it elsewhere. Yes, peasants.\n Download the RIAA Music Revenue data \nImport this into Orange and see…(sigh)\n\n\n\n\n\nFigure 2: RIAA Revenues Data Table\n\n\nFigure 2 states that there are 432 data points, with 7 variables in the dataset; some missing data.\nFor now, the variables we need are :\n\n\n\n\n\n\nQuantitative Data\n\n\n\n\n\nYear: (int) Year in which RIAA revenue was logged\n\nValue (For Charting): (int) Revenue in million USD We can ignore the rest for now, unless we plan to work more with this data, and need to know more. The other numerical data showing billions of USD are not easily decipherable, an example of data that is not documented well…\n\n\n\n\n\n\n\n\n\nQualitative Data\n\n\n\n\n\nCategory: (chr) Form of the Music released ( CD etc..)\n\n\n\n\n\n\n Download the RAWGraphs Line Chart Tutorial File \nUpload this into https://app.rawgraphs.io/ and play! Here is something we can create there:\n\n\n\nhttps://academy.datawrapper.de/article/23-how-to-create-a-line-chart\nHere be dragons: DataWrapper wants the data in wide format: each Format of music needs to have its figures in a separate column! 🤦. And this is not a data transformation that we can achieve within DataWrapper.\nWe are probably better off plotting a regular scatter plot. Here too there seem to be limitations because we are not able to colour the series based on type of music Format.",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🏃 Line Plots"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/32-LinePlots/index.html#what-is-the-story-here",
    "href": "content/courses/NoCode/Modules/32-LinePlots/index.html#what-is-the-story-here",
    "title": "🏃 Line Plots",
    "section": "What is the Story here?",
    "text": "What is the Story here?\n\nOver the years different music formats have had their place in the sun\nAll physical forms are on the wane; streaming music is the current mode of music consumption.\n\n\n\n\n\n\n\nLong Form and Wide Form Data\n\n\n\nSeveral tools such as DataWrapper (and others, yes, I admit, even with code, as we will see) need data transformed to a specific shape. We should now look at this idea of shape in data. Consider the data tables below:\n\n\n\n\n\n\n\n\nProduct\nPower\nCost\nHarmony\nStyle\nSize\nManufacturability\nDurability\nUniversality\n\n\n\nG1\n0.5858003\n0.2773750\n0.7244059\n0.0731445\n0.1000535\n0.4551024\n0.9622046\n0.9966129\n\n\nG2\n0.0089458\n0.8135742\n0.9060922\n0.7546750\n0.9540688\n0.9710557\n0.7617024\n0.5062709\n\n\nG3\n0.2937396\n0.2604278\n0.9490402\n0.2860006\n0.4156071\n0.5839880\n0.7145085\n0.4899432\n\n\n\n\n\n\n\nProduct\nParameter\nRating\n\n\n\nG1\nPower\n0.5858003\n\n\nG1\nCost\n0.2773750\n\n\nG1\nHarmony\n0.7244059\n\n\nG1\nStyle\n0.0731445\n\n\nG1\nSize\n0.1000535\n\n\nG1\nManufacturability\n0.4551024\n\n\nG1\nDurability\n0.9622046\n\n\nG1\nUniversality\n0.9966129\n\n\nG2\nPower\n0.0089458\n\n\nG2\nCost\n0.8135742\n\n\nG2\nHarmony\n0.9060922\n\n\nG2\nStyle\n0.7546750\n\n\nG2\nSize\n0.9540688\n\n\nG2\nManufacturability\n0.9710557\n\n\nG2\nDurability\n0.7617024\n\n\nG2\nUniversality\n0.5062709\n\n\nG3\nPower\n0.2937396\n\n\nG3\nCost\n0.2604278\n\n\nG3\nHarmony\n0.9490402\n\n\nG3\nStyle\n0.2860006\n\n\nG3\nSize\n0.4156071\n\n\nG3\nManufacturability\n0.5839880\n\n\nG3\nDurability\n0.7145085\n\n\nG3\nUniversality\n0.4899432\n\n\n\n\n\n\nWhat we have done is:\n- convert all the variable names into a stacked column Parameter\n- Put all the numbers into another column Rating\n- Repeated the Product column values as many times as needed to cover all Parameters (8 times).\nSee the gif below to get an idea of how this transformation can be worked reversibly. (Yeah, never mind the code there.)\n\n\n\nSo how can we actually do this? Turns out there are some nice people at U. San Diego who have built an R-oriented app that can do this pretty much click-and-point, though it is nowhere as much fun as Orange. Head off here:\nhttps://vnijs.shinyapps.io/radiant We upload our original data, pivot it, and download the pivotted data. Peasants.\nNow the pivotted wide-form data should work in DataWrapper.",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🏃 Line Plots"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/32-LinePlots/index.html#dataset-cancer",
    "href": "content/courses/NoCode/Modules/32-LinePlots/index.html#dataset-cancer",
    "title": "🏃 Line Plots",
    "section": "\n Dataset: Cancer",
    "text": "Dataset: Cancer\nWe can use the same Workflow as before.\n\n Examine the Data\n\n\n\n\n\nFigure 3: Cancer Data Table\n\n\nFrom Figure 3, we see that there is one Qual column Diagnosis, and all the remaining 31 columns seem to be some Quant measurements of a total of 569 tumours. (Not all columns are visible)\n\n\n\n\n\nFigure 4: Cancer Data Summary Table #1\n\n\nFigure 4 gives is histograms and statistics of all the 32 columns. Most histograms seem roughly symmetric, but a detailed look must be taken.\n\n\n\n\n\nFigure 5: Cancer Data Summary Table #2\n\n\nIn Figure 5, we see that there is some imbalance between the counts for the one Qual variable, Diagnosis.\n\n Data Dictionary\n\n\n\n\n\n\nQuantitative Data\n\n\n\n\n\n“Id”\n\n\n\n“Radius (mean)”\n“Texture (mean)”\n\n\n“Perimeter (mean)”\n“Area (mean)”\n\n\n“Smoothness (mean)”\n“Compactness (mean)”\n\n\n“Concavity (mean)”\n“Concave points (mean)”\n\n\n“Symmetry (mean)”\n“Fractal dimension (mean)”\n\n\n“Radius (se)”\n“Texture (se)”\n\n\n“Perimeter (se)”\n“Area (se)”\n\n\n“Smoothness (se)”\n“Compactness (se)”\n\n\n“Concavity (se)”\n“Concave points (se)”\n\n\n“Symmetry (se)”\n“Fractal dimension (se)”\n\n\n“Radius (worst)”\n“Texture (worst)”\n\n\n“Perimeter (worst)”\n“Area (worst)”\n\n\n“Smoothness (worst)”\n“Compactness (worst)”\n\n\n“Concavity (worst)”\n“Concave points (worst)”\n\n\n“Symmetry (worst)”\n“Fractal dimension (worst)”\n\n\n\n\n\nMany of the Quant variables seem to be mean measurements, with the mean presumably taken over several “sites” within the same tumour.\nAlong with the mean, there are also measurements of se or standard error which is, roughly speaking, a measure of the standard deviation of the multiple measurements made. So for instance, Area(mean) and Area(se) are pairs of measurements created using multiple “sites” or “cross-sections” on one tumour.\nSome other variables are labelled as worst, which may be either the max or min of such a set of “multi-site” tumour measurements.\n\n\n\n\n\n\n\nMay the (data) Source be with you\n\n\n\nIt is important to note that these are (educated?) guesses; one is best off connecting with the person/agency that provided the data for a precise understanding of variables. This will prevent nonsensical plots/models and inferences from showing up in your work.\n\n\n\n\n\n\n\n\nQualitative Data\n\n\n\n\n\nDiagnosis: (text) (B)enign, or (M)alignant\n\n\n\n\n Research Questions\n\n\n\n\n\n\nQuestion\n\n\n\nQ1. Are the mean and se observations correlated, for a particular variable?\n\n\n\n\n\n\n\n\n\n(a) Cancer Scatter Plot #1\n\n\n\n\n\n\n\n\n\n(b) Cancer Scatter Plot #2\n\n\n\n\n\n\n\n\n\n\n\n(c) Cancer Scatter Plot #3\n\n\n\n\n\n\n\n\n\n(d) Cancer Scatter Plot #4\n\n\n\n\n\n\nFigure 6\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ2. Are the mean and worst observations correlated, for a particular variable?\n\n\n\n\n\n\n\n\n\n(a) Cancer Scatter Plot #5\n\n\n\n\n\n\n\n\n\n(b) Cancer Scatter Plot #6\n\n\n\n\n\n\n\n\n\n\n\n(c) Cancer Scatter Plot #7\n\n\n\n\n\n\n\n\n\n(d) Cancer Scatter Plot #8\n\n\n\n\n\n\nFigure 7\n\n\n\n\n\n What is the Story Here?\nFrom Figure 7 (a), we see that the area(mean) and area(se) are somewhat correlated; moreover the correlation is slightly higher for the malignant tumours ( red dots, appropriately…). This trend shows up also for radius in Figure 7 (b), and for fractaldimension in Figure 7 (d). However, for smoothness, we see much lower correlation {#fig-cancer-smoothness-mean-se}.\nFor the mean vs worst scatter plots, we see decent correlations all around, with each of the graphs showing clouds tilted upward to the right.\n\n\n\n\n\n\nSimpson’s Paradox\n\n\n\nTry to remove colours and then plot a regression line. This usually gives a more clear idea of the correlation, without running into problems such as the Simpson’s Paradox:\n\n\n\n\n\nFigure 8: Simpson’s Paradox",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🏃 Line Plots"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/32-LinePlots/index.html#your-turn",
    "href": "content/courses/NoCode/Modules/32-LinePlots/index.html#your-turn",
    "title": "🏃 Line Plots",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nTry to play this online Correlation Game\nTry this dataset on School Expenditure and Grades.\n\n\nGas Prices and Consumption, described here. Note the log-transformed Quant data…why do you reckon this was done in the data set itself?\n\n\nHorror Movies ( oh, you awful people..)",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🏃 Line Plots"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/32-LinePlots/index.html#wait-but-why",
    "href": "content/courses/NoCode/Modules/32-LinePlots/index.html#wait-but-why",
    "title": "🏃 Line Plots",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nScatter Plots, when they show “linear” clouds, tell us that there is some relationship between two Quant variables we have just plotted\nIf so, then if one is the target variable you are trying to design for, then the other independent, or controllable, variable is something you might want to design with.\nAlways, always, plot and test your data! Both numerical summaries and graphical summaries are necessary! See below!!\n\n\n\n\n\n\n\nAnd How about these datasets?\n\n\n\n\n\n\n\n\ndataset\nmean_x\nmean_y\nstd_dev_x\nstd_dev_y\ncorr_x_y\n\n\n\naway\n54.26610\n47.83472\n16.76983\n26.93974\n-0.0641284\n\n\nbullseye\n54.26873\n47.83082\n16.76924\n26.93573\n-0.0685864\n\n\ncircle\n54.26732\n47.83772\n16.76001\n26.93004\n-0.0683434\n\n\ndino\n54.26327\n47.83225\n16.76514\n26.93540\n-0.0644719\n\n\ndots\n54.26030\n47.83983\n16.76774\n26.93019\n-0.0603414\n\n\nh_lines\n54.26144\n47.83025\n16.76590\n26.93988\n-0.0617148\n\n\nhigh_lines\n54.26881\n47.83545\n16.76670\n26.94000\n-0.0685042\n\n\nslant_down\n54.26785\n47.83590\n16.76676\n26.93610\n-0.0689797\n\n\nslant_up\n54.26588\n47.83150\n16.76885\n26.93861\n-0.0686092\n\n\nstar\n54.26734\n47.83955\n16.76896\n26.93027\n-0.0629611\n\n\nv_lines\n54.26993\n47.83699\n16.76996\n26.93768\n-0.0694456\n\n\nwide_lines\n54.26692\n47.83160\n16.77000\n26.93790\n-0.0665752\n\n\nx_shape\n54.26015\n47.83972\n16.76996\n26.93000\n-0.0655833\n\n\n\n\n\n\n\n\n\n\n\n\nYes, you did want to plot these in Orange, didn’t you? Here is the data then!!\n\n\n DataSaurus Dirty Dozen\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\nCan selling more ice-cream make people drown?\nUse your head about pairs of variables. Do not fall into this trap)",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🏃 Line Plots"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/32-LinePlots/index.html#references",
    "href": "content/courses/NoCode/Modules/32-LinePlots/index.html#references",
    "title": "🏃 Line Plots",
    "section": "\n References",
    "text": "References\n\nRohrer JM. Thinking Clearly About Correlations and Causation: Graphical Causal Models for Observational Data. Advances in Methods and Practices in Psychological Science. 2018;1(1):27-42. https://doi.org/10.1177/2515245917745629 PDF\nCase Study on Horror Movies https://notawfulandboring.blogspot.com/2024/04/using-pulse-rates-to-determine-scariest.html\nThe Datasaurus Package: https://cran.r-project.org/web/packages/datasauRus/vignettes/Datasaurus.html\nA superb web-scrolly on Sustainable Development Goals (SDGs)s! Go and see!!https://datatopics.worldbank.org/sdgatlas/goal-1-no-poverty?lang=en\nHunter, W. G. (1981). Six Statistical Tales. The Statistician, 30(2), 107. doi:10.2307/2987563. https://sci-hub.ru/10.2307/2987563\n\n\n\n\nFigure 1: LinePlot Inspiration https://fivethirtyeight.com/features/katie-ledecky-is-the-present-and-the-future-of-swimming/\nFigure 2: RIAA Revenues Data Table\nFigure 3: Cancer Data Table\nFigure 4: Cancer Data Summary Table #1\nFigure 5: Cancer Data Summary Table #2\nFigure 6 (a): Cancer Scatter Plot #1\nFigure 6 (b): Cancer Scatter Plot #2\nFigure 6 (c): Cancer Scatter Plot #3\nFigure 6 (d): Cancer Scatter Plot #4\nFigure 7 (a): Cancer Scatter Plot #5\nFigure 7 (b): Cancer Scatter Plot #6\nFigure 7 (c): Cancer Scatter Plot #7\nFigure 7 (d): Cancer Scatter Plot #8\nFigure 8: Simpson’s Paradox",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🏃 Line Plots"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/22-BarCharts/index.html",
    "href": "content/courses/NoCode/Modules/22-BarCharts/index.html",
    "title": "👨🏻‍👩🏻‍👦🏻‍👧🏻Bar Charts",
    "section": "",
    "text": "Variable #1\nVariable #2\nChart Names\nChart Shape\n\n\nQuant\nNone\nBar Chart",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "👨🏻‍👩🏻‍👦🏻‍👧🏻Bar Charts"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/22-BarCharts/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/NoCode/Modules/22-BarCharts/index.html#what-graphs-will-we-see-today",
    "title": "👨🏻‍👩🏻‍👦🏻‍👧🏻Bar Charts",
    "section": "",
    "text": "Variable #1\nVariable #2\nChart Names\nChart Shape\n\n\nQuant\nNone\nBar Chart",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "👨🏻‍👩🏻‍👦🏻‍👧🏻Bar Charts"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/22-BarCharts/index.html#what-kind-of-data-variables-will-we-choose",
    "href": "content/courses/NoCode/Modules/22-BarCharts/index.html#what-kind-of-data-variables-will-we-choose",
    "title": "👨🏻‍👩🏻‍👦🏻‍👧🏻Bar Charts",
    "section": "\n What kind of Data Variables will we choose?",
    "text": "What kind of Data Variables will we choose?\n\n\n\n\n\n\nNo\nPronoun\nAnswer\nVariable/Scale\nExample\nWhat Operations?\n\n\n3\nHow, What Kind, What Sort\nA Manner / Method, Type or Attribute from a list, with list items in some \" order\" ( e.g. good, better, improved, best..)\nQualitative/Ordinal\nSocioeconomic status (Low income, Middle income, High income),Education level (HighSchool, BS, MS, PhD),Satisfaction rating(Very much Dislike, Dislike, Neutral, Like, Very Much Like)\nMedian,Percentile",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "👨🏻‍👩🏻‍👦🏻‍👧🏻Bar Charts"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/22-BarCharts/index.html#inspiration",
    "href": "content/courses/NoCode/Modules/22-BarCharts/index.html#inspiration",
    "title": "👨🏻‍👩🏻‍👦🏻‍👧🏻Bar Charts",
    "section": "\n Inspiration",
    "text": "Inspiration\n\n\n\n\n\nFigure 1\n\n\nWhich would be India’s city? What would be the reduction in percentage? And what’s with these Germans anyhow?",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "👨🏻‍👩🏻‍👦🏻‍👧🏻Bar Charts"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/22-BarCharts/index.html#how-do-these-charts-work",
    "href": "content/courses/NoCode/Modules/22-BarCharts/index.html#how-do-these-charts-work",
    "title": "👨🏻‍👩🏻‍👦🏻‍👧🏻Bar Charts",
    "section": "\n How do these Chart(s) Work?",
    "text": "How do these Chart(s) Work?\nBar are used to show “counts” and “tallies” with respect to Qual variables. For instance, in a survey, how many people vs Gender? In a Target Audience survey on Weekly Consumption, how many low, medium, or high expenditure people?\nEach Qual variable potentially has many levels as we saw in the Nature of Data. For instance, in the above example on Weekly Expenditure, low, medium and high were levels for the Qual variable Expenditure. Bar charts perform internal counts for each level of the Qual variable under consideration. The Bar Plot is then a set of disjoint bars representing these counts; see the icon above, and then that for histograms!! The X-axis is the set of levels in the Qual variable, and the Y-axis represents the counts for each level.\n\n\n\n\n\n\nBar Charts and Column Charts\n\n\n\nAnd Column charts just plot numbers over categories. No internal counting. As you can see in the Figure 1 above.\nThough in many places, these two names are used interchangeably! But be aware of what the tool may be doing!",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "👨🏻‍👩🏻‍👦🏻‍👧🏻Bar Charts"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/22-BarCharts/index.html#plotting-a-bar-chart",
    "href": "content/courses/NoCode/Modules/22-BarCharts/index.html#plotting-a-bar-chart",
    "title": "👨🏻‍👩🏻‍👦🏻‍👧🏻Bar Charts",
    "section": "\n Plotting a Bar Chart",
    "text": "Plotting a Bar Chart\n\n\nUsing Orange\nUsing RAWgraphs\nUsing DataWrapper\n\n\n\nThe Bar Plot widget in Orange is described here. https://orangedatamining.com/widget-catalog/visualize/barplot/\n Download the Banned Books data \nAnd download the Bar Chart workflow file for this data:\n Download the Bar Chart Workflow \n\n\n\n\n\nhttps://academy.datawrapper.de/category/74-bar-charts",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "👨🏻‍👩🏻‍👦🏻‍👧🏻Bar Charts"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/22-BarCharts/index.html#dataset-banned-books-in-the-usa",
    "href": "content/courses/NoCode/Modules/22-BarCharts/index.html#dataset-banned-books-in-the-usa",
    "title": "👨🏻‍👩🏻‍👦🏻‍👧🏻Bar Charts",
    "section": "\n Dataset: Banned Books in the USA",
    "text": "Dataset: Banned Books in the USA\nHere is a dataset from Jeremy Singer-Vine’s blog, Data Is Plural. This is a list of all books banned in schools across the US.\nDownload this data to your machine and use it in Orange.\n Download the Banned Books data \n\n Examine the Data\n\n\n\n\n\nFigure 2: Banned Books Data Table\n\n\n\n\n\n\n\nFigure 3: Banned Books Data Summary\n\n\nFigure 2 states that we have 1586 rows, 7 columns. So 1586 banned books are on this list! 🙀 🙀 🙀\nThe Figure 3 already has a thumbnail-like bar chart. We will still make a “proper” one with the appropriate widget.\n\n\n\n\n\n\nWarning\n\n\n\nIn the workflow below, note how it is still the Distributions widget that gives the Bar Chart. This is unfortunate, since we have been at pains to state how a Bar Chart and the Histogram deal with different types of variables (Qual and Quant respectively). Just one of those things we need to get used to!!\n\n\n\n Data Dictionary\n\n\n\n\n\n\nQuantitative Data\n\n\n\n\n\nDate of Challenge: Date the book was (selected to be?) banned\n\n\n\n\n\n\n\n\n\nQualitative Data\n\n\n\n\n\nAuthor: (text) Meta Data. Can be treated as Qual\n\nTitle: (text) Meta Data. Can be treated as Qual\n\nState: (text) Qual factor\n\nDistrict: (text) Qual factor\n\nType of Ban: (text) Qual factor\n\nOrigin of Challenge: (text) Who requested the Ban?\n\nHow many levels in each?? Find out in Orange!!\n\n\n\n Research Questions\n\n\n\n\n\n\nNote\n\n\n\nQ1. Which is the US state that bans the most? Which state is least involved in banning books? What can you say of the “geography of book banning” based on your understanding of the US of A? 😆\n\n\n\n\n\nFigure 4: Banned Books Count by State\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQ2. Create Bar charts of the count of banned books by Reason for Banning!!\n\n\n\n What is the Story Here?\n\n\nFigure 4 says that Texas is the worst at book banning!\nTexas, Florida, Oklahoma, Kansas, Indiana,..are next in line\nIs there a “Bible Belt” story here?\n\n\n\n\n\n\nFigure 5: Bible Belt\n\n\n\nAnd what, Californians are too busy making money to care about book-banning!!! The state does not even show up in the chart! 😄",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "👨🏻‍👩🏻‍👦🏻‍👧🏻Bar Charts"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/22-BarCharts/index.html#your-turn",
    "href": "content/courses/NoCode/Modules/22-BarCharts/index.html#your-turn",
    "title": "👨🏻‍👩🏻‍👦🏻‍👧🏻Bar Charts",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nAiRbnb Price Data on the French Riviera:\n\n\n\n AiRbnb data\n\n\n\nApartment price vs ground living area:\n\n\n\n Apartment Data\n\n\n\nFertility: This rather large and interesting Fertility related dataset from https://vincentarelbundock.github.io/Rdatasets/csv/AER/Fertility.csv\n\n\n Download the Fertility Data",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "👨🏻‍👩🏻‍👦🏻‍👧🏻Bar Charts"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/22-BarCharts/index.html#wait-but-why",
    "href": "content/courses/NoCode/Modules/22-BarCharts/index.html#wait-but-why",
    "title": "👨🏻‍👩🏻‍👦🏻‍👧🏻Bar Charts",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nAlways count your chickens count your data before you model or infer!\nCounts first give you an absolute sense of how much data you have.\nCounts by different Qual variables give you a sense of the combinations you have in your data: \\((Male/Female) * (Income-Status) * (Old/Young) * (Urban/Rural)\\) (Say 2 * 3 * 2 * 2 = 24 combinations of data)\nCounts then give an idea whether your data is lop-sided: do you have too many observations of one category(level) and too few of another category(level) in a given Qual variable?\nBalance is important in order to draw decent inferences\nAnd for ML algorithms, to train them properly.\n\n\n\n\nFigure 1: \nFigure 2: Banned Books Data Table\nFigure 3: Banned Books Data Summary\nFigure 4: Banned Books Count by State\nFigure 5: Bible Belt",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "👨🏻‍👩🏻‍👦🏻‍👧🏻Bar Charts"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/05-IntroductiontoOrange/index.html",
    "href": "content/courses/NoCode/Modules/05-IntroductiontoOrange/index.html",
    "title": "🟠 Introduction to Orange",
    "section": "",
    "text": "Orange is a visual drag-and-drop tool for\n\nData visualization\n\nStatistical Tests\nMachine Learning\n\nData mining\n\nand much more. Here is what it looks like:\n\nAll operations are done using a visual menu-driven interface. We drag and drop widgets that can have inputs and outputs. Widgets perform operations on data sent to them by other widgets. The visuals can be exported to PNG/SVG/PDF and the entire workflow can be exported into a Report Form and edited for presentation and sharing.\nOrange also has add-ons that provide widgets for specific tasks such as Machine Learning, Time Series Analysis and so on.",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🟠 Introduction to Orange"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/05-IntroductiontoOrange/index.html#introduction-to-orange",
    "href": "content/courses/NoCode/Modules/05-IntroductiontoOrange/index.html#introduction-to-orange",
    "title": "🟠 Introduction to Orange",
    "section": "",
    "text": "Orange is a visual drag-and-drop tool for\n\nData visualization\n\nStatistical Tests\nMachine Learning\n\nData mining\n\nand much more. Here is what it looks like:\n\nAll operations are done using a visual menu-driven interface. We drag and drop widgets that can have inputs and outputs. Widgets perform operations on data sent to them by other widgets. The visuals can be exported to PNG/SVG/PDF and the entire workflow can be exported into a Report Form and edited for presentation and sharing.\nOrange also has add-ons that provide widgets for specific tasks such as Machine Learning, Time Series Analysis and so on.",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🟠 Introduction to Orange"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/05-IntroductiontoOrange/index.html#installing-orange",
    "href": "content/courses/NoCode/Modules/05-IntroductiontoOrange/index.html#installing-orange",
    "title": "🟠 Introduction to Orange",
    "section": "Installing Orange",
    "text": "Installing Orange\nYou can download and install Orange from here:\nhttps://orangedatamining.com/download/",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🟠 Introduction to Orange"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/05-IntroductiontoOrange/index.html#basic-usage-of-orange",
    "href": "content/courses/NoCode/Modules/05-IntroductiontoOrange/index.html#basic-usage-of-orange",
    "title": "🟠 Introduction to Orange",
    "section": "Basic Usage of Orange",
    "text": "Basic Usage of Orange",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🟠 Introduction to Orange"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/05-IntroductiontoOrange/index.html#orange-workflows",
    "href": "content/courses/NoCode/Modules/05-IntroductiontoOrange/index.html#orange-workflows",
    "title": "🟠 Introduction to Orange",
    "section": "Orange Workflows",
    "text": "Orange Workflows",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🟠 Introduction to Orange"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/05-IntroductiontoOrange/index.html#widgets-and-channels",
    "href": "content/courses/NoCode/Modules/05-IntroductiontoOrange/index.html#widgets-and-channels",
    "title": "🟠 Introduction to Orange",
    "section": "Widgets and Channels",
    "text": "Widgets and Channels",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🟠 Introduction to Orange"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/05-IntroductiontoOrange/index.html#loading-data-into-orange",
    "href": "content/courses/NoCode/Modules/05-IntroductiontoOrange/index.html#loading-data-into-orange",
    "title": "🟠 Introduction to Orange",
    "section": "Loading data into Orange",
    "text": "Loading data into Orange\n\n\n\nWe are good to get started with Orange!!",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🟠 Introduction to Orange"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/05-IntroductiontoOrange/index.html#simple-visuals-using-orange",
    "href": "content/courses/NoCode/Modules/05-IntroductiontoOrange/index.html#simple-visuals-using-orange",
    "title": "🟠 Introduction to Orange",
    "section": "Simple Visuals using Orange",
    "text": "Simple Visuals using Orange\nLet us create some simple visualizations using Orange.\n\nUse the File Widget to import the iris dataset into your session\nUse the Data Table Widget to look at the data, and note its variable names\nUse the Visualization Widgets ( Scatter Plot, Bar Plot, and Distributions) to look at the properties of the variables, and examine relationships between them.",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🟠 Introduction to Orange"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/05-IntroductiontoOrange/index.html#reference",
    "href": "content/courses/NoCode/Modules/05-IntroductiontoOrange/index.html#reference",
    "title": "🟠 Introduction to Orange",
    "section": "Reference",
    "text": "Reference\n\nIntroduction to Data Mining-Working notes for the hands-on course with Orange Data Mining.\nOrange Data Mining Widget Catalog: Look here for help and guidance! https://orangedatamining.com/widget-catalog/\nhttps://orangedatamining.com/blog/visualizations-101/",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "🟠 Introduction to Orange"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/40-SankeyCharts/index.html",
    "href": "content/courses/NoCode/Modules/40-SankeyCharts/index.html",
    "title": "\n Sankey, Alluvial, and Parallet-Set Plots",
    "section": "",
    "text": "Variable #1\nVariable #2\nChart Names\nChart Shape\n\n\nQuant\nNone\nSankey Plot\n\n\n\n\n\n\n\n\n\n\n\nNo\nPronoun\nAnswer\nVariable/Scale\nExample\nWhat Operations?\n\n\n\n2\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities with Scale. Differences are meaningful, but not products or ratios\nQuantitative/Interval\npH,SAT score(200-800),Credit score(300-850),SAT score(200-800),Year of Starting College\nMean,Standard Deviation\n\n\n3\nHow, What Kind, What Sort\nA Manner / Method, Type or Attribute from a list, with list items in some \" order\" ( e.g. good, better, improved, best..)\nQualitative/Ordinal\nSocioeconomic status (Low income, Middle income, High income),Education level (HighSchool, BS, MS, PhD),Satisfaction rating(Very much Dislike, Dislike, Neutral, Like, Very Much Like)\nMedian,Percentile",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "<iconify-icon icon=\"maki:watermill\"></iconify-icon> Sankey, Alluvial, and Parallet-Set Plots"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/40-SankeyCharts/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/NoCode/Modules/40-SankeyCharts/index.html#what-graphs-will-we-see-today",
    "title": "\n Sankey, Alluvial, and Parallet-Set Plots",
    "section": "",
    "text": "Variable #1\nVariable #2\nChart Names\nChart Shape\n\n\nQuant\nNone\nSankey Plot\n\n\n\n\n\n\n\n\n\n\n\nNo\nPronoun\nAnswer\nVariable/Scale\nExample\nWhat Operations?\n\n\n\n2\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities with Scale. Differences are meaningful, but not products or ratios\nQuantitative/Interval\npH,SAT score(200-800),Credit score(300-850),SAT score(200-800),Year of Starting College\nMean,Standard Deviation\n\n\n3\nHow, What Kind, What Sort\nA Manner / Method, Type or Attribute from a list, with list items in some \" order\" ( e.g. good, better, improved, best..)\nQualitative/Ordinal\nSocioeconomic status (Low income, Middle income, High income),Education level (HighSchool, BS, MS, PhD),Satisfaction rating(Very much Dislike, Dislike, Neutral, Like, Very Much Like)\nMedian,Percentile",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "<iconify-icon icon=\"maki:watermill\"></iconify-icon> Sankey, Alluvial, and Parallet-Set Plots"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/40-SankeyCharts/index.html#how-do-these-charts-work",
    "href": "content/courses/NoCode/Modules/40-SankeyCharts/index.html#how-do-these-charts-work",
    "title": "\n Sankey, Alluvial, and Parallet-Set Plots",
    "section": "\n How do these Chart(s) Work?",
    "text": "How do these Chart(s) Work?\nSometimes Qual data can itself vary over, or depend upon, over a bunch of independent Qual data categories. For instance we can contemplate enrollment at a University, and show how students move from course to course in a University. Or how customers drift from one category of products or brands to another….or the movement of cricket players from one IPL Team to another !!\nAs can be surmised, the independent categories can be interpreted both as time ( e.g semesters / cycles / years) and space (teams / courses / departments). And we can chart another Quant or Qual variable that moves across levels of the first chosen Qual variable.\n\n\n\n\n\nThe Qualitative variables being connected are mapped to stages/axes\n\nEach level within a Qual variable is mapped to nodes / strata / lodes;\nAnd the connections between the strata of the axes are called flows / links / alluvia.\n\n\nSuch diagrams are best used when you want to show a many-to-many mapping between two domains or multiple paths through a set of stages E.g Students going through multiple courses during a semester of study.\nHere is an example of a Sankey Diagram for the Titanic dataset:\n\n\n\n\n\n\n\nFigure 1: Titanic Sankey Plot\n\n\n\n\nIt is seen from Figure 1 that the x-axis has Qual variables stages shown as “pillars” and these are split into nodes based on the levels for each Qual variable stage respectively. Flows with variable thickness connect one node at one stage to another node and another stage.\n\n\n\n\n\n\nSankey, Parallel sets, and Alluvial Charts\n\n\n\nHere is what Thomas Lin Pedersen says:\nA parallel sets diagram is a type of visualisation showing the interaction between multiple categorical variables.\nIf the variables have an intrinsic order the representation can be thought of as a Sankey Diagram.\nIf each variable is a point in time it will resemble an Alluvial diagram.",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "<iconify-icon icon=\"maki:watermill\"></iconify-icon> Sankey, Alluvial, and Parallet-Set Plots"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/40-SankeyCharts/index.html#creating-sankey-plots",
    "href": "content/courses/NoCode/Modules/40-SankeyCharts/index.html#creating-sankey-plots",
    "title": "\n Sankey, Alluvial, and Parallet-Set Plots",
    "section": "\n Creating Sankey Plots",
    "text": "Creating Sankey Plots\n\n\nUsing Orange\nUsing RAWgraphs\nUsing DataWrapper\n\n\n\nIt does not seem possible to create an Alluvial Diagram in Orange. 😢\n\n\n\n Download the RAWgraphs Alluvial Tutorial File \nDownload this file and upload to https://app.rawgraphs.io/.\nLet us however examine this data in Orange:\n Download the Hate Crimes dataset \n\n\n\n\n\nFigure 2: Hate Crimes in NY 2020\n\n\nFrom the Figure 2, We see that the data is all Qualitative, except for Age. The Precinct, while apparently an integer, is really a Qual variable! Why?\nAnd here is the Sankey Diagram:\n\n\n\n\n\nFigure 3: Hate Crime Sankey Diagram\n\n\nIn the Figure 3, we have three Qual variables along the x-axis: Gender, Race and Bias-Motivation. The chart counts the crime episodes at each stage/node and portrays them as flows with varying thickness leading to the next stage/node.\nWhat trends do you detect from this diagram?\n\n\nIt does not seem possible to create an Alluvial Diagram in DataWrapper.",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "<iconify-icon icon=\"maki:watermill\"></iconify-icon> Sankey, Alluvial, and Parallet-Set Plots"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/40-SankeyCharts/index.html#dataset-course-allocations",
    "href": "content/courses/NoCode/Modules/40-SankeyCharts/index.html#dataset-course-allocations",
    "title": "\n Sankey, Alluvial, and Parallet-Set Plots",
    "section": "\n Dataset: Course Allocations",
    "text": "Dataset: Course Allocations\nLet us try one more dataset:\n Download the FSP Course Allocation data \n\n Examine the Data\nLet us still import into Orange and see the data anyway!\n\n\n\n\n\nFigure 4: Course Allocation Data\n\n\nFrom Figure 4, we see that we have 300 students and their course allocations over one Foundation year at SMI. (The data is anonymized but accurate; no staff or students were harmed in the collection of this data)!\n\n Data Dictionary\n\n\n\n\n\n\nQuantitative Data\n\n\n\n\nNone!!\n\n\n\n\n\n\n\n\n\nQualitative Data\n\n\n\n\nMajor(chr): Student Major\nAll other columns: Courses they were allocated to during the course of the year.\nMM = Mark Making; DM = Digital Making; FM = Form Making;\nDTT = Digital Thinking Tools; VTT = Visual Thinking Tools;\nO&C = Order and Chaos; P&I = Play and Invent; S&P = Space and Place; C&P = Communities and Practices; F&S = Form and Structure; B&C = Body and Context; L&L = Layers and Lenses; E&C = Everything’s Connected; P&P = Patterns and Paradigms(Oh all right)\n\n\n\n\n Research Questions\nLet’s try a few questions and see if they are answerable with Sankey/Alluvial Plots\n\n\n\n\n\n\nQuestion\n\n\n\nQ1. Do all DMA/Film/CAP students take one B&C course?\n\n\n\n\n\n\n\n\n\nFigure 5\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ2. Do all IADP, HCD, and PSD students take one P&I course?\n\n\n\n What is the Story Here?\nWrite in!!",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "<iconify-icon icon=\"maki:watermill\"></iconify-icon> Sankey, Alluvial, and Parallet-Set Plots"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/40-SankeyCharts/index.html#your-turn",
    "href": "content/courses/NoCode/Modules/40-SankeyCharts/index.html#your-turn",
    "title": "\n Sankey, Alluvial, and Parallet-Set Plots",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nWithin the ggalluvial package from R, are two datasets, majors and vaccinations. Plot alluvial charts for both of these, and write their stories.\n\n\n\n Majors data\n\n\n\n\n\n Vaccinations data\n\n\n\n\nGo to the American Life Panel Website where you will find many public datasets. Try to take one and make charts from it that we have learned in this Module.\nTry this from Vincent Arel-Bundock’s website: Cybersecurity breaches reported to the US Department of Health and Human Services. The dataset is downloadable here CSV. NOTE: data may require some cleaning beforehand in Excel!",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "<iconify-icon icon=\"maki:watermill\"></iconify-icon> Sankey, Alluvial, and Parallet-Set Plots"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/40-SankeyCharts/index.html#wait-but-why",
    "href": "content/courses/NoCode/Modules/40-SankeyCharts/index.html#wait-but-why",
    "title": "\n Sankey, Alluvial, and Parallet-Set Plots",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nMosaic Charts gave us a view of counts of data across level-combinations of Qual variables.\nSankey/Alluvial Charts give us a sense of flow: how did different observations flow from one Qual level to another?\nThis is very valuable if these Qual variables and their levels have a natural sequence. E.g. Choices made in purchases, Attitudes over time and situation, Affiliations and Friendships over time etc.\nThe sequence may even be conceptualized as a consequence, provided you have adequate insight into the situations involved.\nYou get a sense of the sub-populations in each combo of Qual variables and can decide what to do about both plenitude and rarity!",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "<iconify-icon icon=\"maki:watermill\"></iconify-icon> Sankey, Alluvial, and Parallet-Set Plots"
    ]
  },
  {
    "objectID": "content/courses/NoCode/Modules/40-SankeyCharts/index.html#references",
    "href": "content/courses/NoCode/Modules/40-SankeyCharts/index.html#references",
    "title": "\n Sankey, Alluvial, and Parallet-Set Plots",
    "section": "\n References",
    "text": "References\n\nA good pictorial introduction to different parts of a Sankey Chart. https://github.com/davidsjoberg/ggsankey\nMinard’s famous Alluvial Plot of Napoleon’s Invasion of Russia. https://www.andrewheiss.com/blog/2017/08/10/exploring-minards-1812-plot-with-ggplot2/?utm_content=buffer70e4b&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer\nMinard Revisited. https://www.masswerk.at/nowgobang/2020/minard-revisited\n100+ years of the Titanic data. https://www.datavis.ca/papers/titanic/\n\n\n\n\nFigure 1: Titanic Sankey Plot\nFigure 2: Hate Crimes in NY 2020\nFigure 3: Hate Crime Sankey Diagram\nFigure 4: Course Allocation Data",
    "crumbs": [
      "Teaching",
      "Data Vis with No Code",
      "<iconify-icon icon=\"maki:watermill\"></iconify-icon> Sankey, Alluvial, and Parallet-Set Plots"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/200-TRIZ-Advanced-Stuff/index.html#references",
    "href": "content/courses/TRIZ4ProbSolving/Modules/200-TRIZ-Advanced-Stuff/index.html#references",
    "title": "TRIZ - Substance Field Analysis, and ARIZ",
    "section": "References",
    "text": "References\n\nhttps://the-trizjournal.com/altshullers-greatest-discovery-beyond/\nProject TETRIS: Chapter 3: Short Review of ARIZ: Algorithm for Innovative Problem Solving and a Case Study (PDF)\nProject TETRIS: Chapter 4: Substance-Field ( “Su-Field” ) Analysis and Standard Solutions basic notions and rules (PDF)",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "TRIZ - Substance Field Analysis, and ARIZ"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/80-A-Contradictory-Language/index.html#metaphors-for-our-contradictions",
    "href": "content/courses/TRIZ4ProbSolving/Modules/80-A-Contradictory-Language/index.html#metaphors-for-our-contradictions",
    "title": "TRIZ - A Contradictory Language",
    "section": "Metaphors for our Contradictions",
    "text": "Metaphors for our Contradictions\nRecall that you considered several familiar Objects in the earlier module on Problems and Contradictions. And also considered a few sample Situations to find Administrative Contradictions in them.\nWe used the method of:\n\nFinding Knobs in Objects and Situations using the Ishikawa Diagram\nTurning the Knobs both ways\nFinding what improves or worsens in each case\nDeciding upon an Administrative Contradiction\n\n\nNow that we know how problems can be stated as simple Administrative Contradictions (AC), we need to take the next step and make what TRIZ calls Technical Contradictions (TC) and Physical Contradictions (PC). To do this, we will use a set of metaphoric phrases that are an integral part of (classical) TRIZ. These metaphoric phrases are simple enough and provide rich troves for imaginative problem solving.\nBefore we study this TRIZ Contradiction Language however, we need to obtain a quick understanding of its history.",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "TRIZ - A Contradictory Language"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/80-A-Contradictory-Language/index.html#the-history-of-triz",
    "href": "content/courses/TRIZ4ProbSolving/Modules/80-A-Contradictory-Language/index.html#the-history-of-triz",
    "title": "TRIZ - A Contradictory Language",
    "section": "The History of TRIZ",
    "text": "The History of TRIZ\n\n\n\n\n\n\n\n\nThe creator of TRIZ, Genrich Altshuller, was born in Russia in 1926, made his first invention at age 14 (9th Grade), and was later educated as a mechanical engineer. At the time he started working on TRIZ, in 1946, he was employed in the patent department of the Soviet navy, assisting inventors in filing their patents, in Baku, Azerbaijan.\nWhile there he became intrigued by the question of how an invention happens:\n\nIs it a matter of luck? The result of a mental “light bulb” turning on, as in the comics? Or can inventions be seen as the result of systematic patterns of inventive thinking?\n\nAltshuller adopted an empirical approach to answering this question. He studied thousands of patents, looking for commonalities, repetitive patterns, and principles of inventive thought. As he found them he codified and documented them, as shown below. His results, when eventually published, attracted many enthusiasts who continued and expanded the work over the years, reviewing what is now estimated to be more than two million patents worldwide.\n\n\n\n\n\n\n\n\nLevel 1: Apparent Solution. This level requires no real invention; it consists of minor adaptations of existing concepts. Simple improvement of a technical system. They require knowledge available within an industry relevant to that system.\nLevel 2: Improvement. This level makes small improvements to existing approaches. Inventions include the resolution of a technical contradiction(TC). They require knowledge from different areas within an industry relevant to the system.\nLevel 3: Invention Inside the Paradigm. This level uses methods from other fields and improves previous approaches. This is an invention containing a resolution of a physical contradiction(PC). It requires knowledge from other industries.\nLevels #2 & #3 solve contradictions and therefore are innovative by definition.\nLevel 4. Invention Outside the Paradigm. This level involves a new design that is based on modifications of existing principles but in a manner not previously used. This level involves development a new technology. It is developed by using breakthrough solutions that requires knowledge from different fields of science. This fourth level also improves upon a technical system, but without solving an existing technical problem. Instead, it improves the function by replacing the original technology with a new technology. For example a mechanical system is replaced with a chemical system to perform the function.\nLevel 5: Discovery. This is the highest invention level. It consists of completely new concepts using new principles, and involves the discovery of new phenomena. The new phenomenon is discovered that allows pushing the existing technology to a higher level.\nAltshuller concluded from his research that a large number of patents (77%) belong only to Levels #1 and #2. The practical utilization of TRIZ methodology can help inventors elevate their innovative solutions to Levels #3 and #4. As a result of this work, hundreds of technical papers and many books on TRIZ have been published, including 14 books by Altshuller himself.\nTRIZ is actively used in Companies such as Boeing, Bridgestone, Eastman Kodak, Ford Motor Company, Harley-Davidson Motor Company, Hewlett-Packard, Illinois Tool Works, Inficon, Ingersoll Rand, Kimberly-Clark, L.G. Electronics, Lucent Technologies, Michelin, National Semiconductor, NASA, Philips, Rolls-Royce, Samsung, Siemens, Western Digital, and Xerox among others.",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "TRIZ - A Contradictory Language"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/80-A-Contradictory-Language/index.html#speaking-triz",
    "href": "content/courses/TRIZ4ProbSolving/Modules/80-A-Contradictory-Language/index.html#speaking-triz",
    "title": "TRIZ - A Contradictory Language",
    "section": "Speaking TRIZ",
    "text": "Speaking TRIZ\nAltshuller made two more discoveries (at least!) based on his study of Soviet Patents:\n\nThat when the the problems solved in the patents were expressed as Contradictions, there were only a few standard phrases using which these Contradictions could be constructed. These were an astonishingly small 48 in number!! Every problem could be described as a contradiction using some pair of these 48 parameters. We will call these the 48 TRIZ Parameters1.\nWhen the solutions in the Patents were examined, they could all be expressed simply as one of 40 abstract nouns or phrases. Just…40! We will call these the 40 TRIZ Inventive Principles\n\nSome examples:\n\nTRIZ Parameters: Weight of an stationary Object, Loss of Substance, and Temperature.\n\nTRIZ Inventive Principles: Asymmetry, Parameter Change and The Other Way Around.\n\nSo the task is:\n\nTake your Administrative Contradiction (AC)\n\nConvert “each side” (i.e terms) of the AC into a TRIZ Technical Contradiction by using a pair of the 48 TRIZ Parameters.\n\nMake sure that you are able to justify to yourself the metaphorical connection between the terms of AC and the chosen pair of TRIZ Parameters. For example, “Loss of substance” could mean “loss of money”!! Write this down, for both sides of your AC.\n\nWe may be able to also create a Physical Contradiction at this time, when we are examining the Ishikawa knobs.",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "TRIZ - A Contradictory Language"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/80-A-Contradictory-Language/index.html#some-contradiction-examples",
    "href": "content/courses/TRIZ4ProbSolving/Modules/80-A-Contradictory-Language/index.html#some-contradiction-examples",
    "title": "TRIZ - A Contradictory Language",
    "section": "Some Contradiction Examples",
    "text": "Some Contradiction Examples\nTC Examples:\n\nIncreasing the power of an engine (positive improvement) requires an increase in the size of the engine (negative effect). So, an inventor considers increasing the power partially in order to reduce the negative effect (compromise solution).\n\nTo increase the speed of an airplane, a new and more powerful engine is installed. This increases the weight of the airplane so the wings can no longer support it during takeoff. Increasing the wing size produces more drag, slowing the airplane down.\n\nPC Examples:\n\nLanding gear must be present on an airplane in order to land and takeoff. It should not be present during flight because of an increase in air drag. The physical contradiction is that the landing gear must be both present and absent. This contradiction is resolved by separating the requirements in time — make the landing gear retractable.\nFor high water diving, water must be “hard” to support the diver and “soft” so as not to injure the diver. The physical contradiction: The water must be hard and soft at the same time. This contradiction is resolved by separating the requirements in space: Saturate the water with air bubbles — the pool contains both air and water.\n… an inherent contradiction in takeout pizza products. The customer wants the pizza to be hot, but hot pizza gives off steam. As a result, the cardboard box lid absorbs the steam, softens, and collapses down on the pizza. When the customer lifts the lid of the pizza box after arriving home, some of the pizza, which has stuck to the lid, attaches to the raised lid and the customer is not pleased because a significant amount of the cheese that he paid for is not edible. We want the pizza to be hot for one reason (an enjoyable pizza) and we want it to be cold for another (preventing the lid from becoming soggy and collapsing due to steam and water absorption). The little plastic tripod that is in the center of many takeout pizzas was the subject of the talk I heard. Such a simple invention! It resolves the contradiction of the pizza being hot and cold at the same time. (Jack Hipple, The Ideal Result)",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "TRIZ - A Contradictory Language"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/80-A-Contradictory-Language/index.html#thinking-problem-solving-and-language",
    "href": "content/courses/TRIZ4ProbSolving/Modules/80-A-Contradictory-Language/index.html#thinking-problem-solving-and-language",
    "title": "TRIZ - A Contradictory Language",
    "section": "Thinking, Problem Solving, and Language",
    "text": "Thinking, Problem Solving, and Language\nHere is Albert Einstein speaking on The Language of Science. The transcript is here.PDF.\nOne quote from this short reading seems very apposite here:\n\n“The super-national character of scientific concepts and scientific language is due to the fact that they have been set up by the best brains of all countries and all times. In solitude, and yet in cooperative effort as regards the final effect, they created the spiritual tools for the technical revolutions which have transformed the life of mankind in the last centuries. Their system of concepts has served as a guide in the bewildering chaos of perceptions so that we learned to grasp general truths from particular observations.”",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "TRIZ - A Contradictory Language"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/80-A-Contradictory-Language/index.html#references",
    "href": "content/courses/TRIZ4ProbSolving/Modules/80-A-Contradictory-Language/index.html#references",
    "title": "TRIZ - A Contradictory Language",
    "section": "References",
    "text": "References\n\nValeri Souchkov, Differentiating Among the Five Levels of Solutions,Web\nValeri Souchkov, “How to Define a Contradiction”, Web and PDF\nOpen Source TRIZ: TRIZ Power Tools: Formulating Contradictions: Webpage",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "TRIZ - A Contradictory Language"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/80-A-Contradictory-Language/index.html#additional-readings",
    "href": "content/courses/TRIZ4ProbSolving/Modules/80-A-Contradictory-Language/index.html#additional-readings",
    "title": "TRIZ - A Contradictory Language",
    "section": "Additional Readings",
    "text": "Additional Readings\n\nhttps://www.bbc.com/worklife/article/20201109-why-the-paradox-mindset-is-the-key-to-success\nhttps://hbr.org/2008/06/the-contradictions-that-drive-toyotas-success",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "TRIZ - A Contradictory Language"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/80-A-Contradictory-Language/index.html#footnotes",
    "href": "content/courses/TRIZ4ProbSolving/Modules/80-A-Contradictory-Language/index.html#footnotes",
    "title": "TRIZ - A Contradictory Language",
    "section": "Footnotes",
    "text": "Footnotes\n\nClassical TRIZ started off with 39 Inventive Parameters. In recent years, the parameters have been increased to 48, after a study of an ever-increasing body of patent literature.↩︎",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "TRIZ - A Contradictory Language"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/index.html#introduction",
    "href": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/index.html#introduction",
    "title": "I Think, Therefore I am",
    "section": "Introduction",
    "text": "Introduction\nOften times we swiftly come to conclusions or decisions, without being aware that\na) we are making a big mistake based on the information that is available,\nb) we are looking for our fondest beliefs to be true, or worse\nc) we are being manipulated !\nHence, one of the important points of view we need to retain is to be aware of our Cognitive Biases. Being aware of this may be the first step to check ourselves and therefore arrive a more nuanced “appreciation of the situation”, that would then allow us to formulate our Innovative Problems better.\nLet us now engage in some simple activities that show off! to us, our Cognitive Biases!",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "I Think, Fast and Slow"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/index.html#some-simple-cognitive-bias-games",
    "href": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/index.html#some-simple-cognitive-bias-games",
    "title": "I Think, Therefore I am",
    "section": "Some Simple Cognitive Bias Games",
    "text": "Some Simple Cognitive Bias Games",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "I Think, Fast and Slow"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/index.html#bias-1-cognitive-miserliness",
    "href": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/index.html#bias-1-cognitive-miserliness",
    "title": "I Think, Therefore I am",
    "section": "Bias #1: Cognitive Miserliness",
    "text": "Bias #1: Cognitive Miserliness\nGame\nLet us start with a set of relatively easy games. Here PDF are some questions for you. Please write or call out the answers as you see them.\nInterpretation\nReferences for Further Reading",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "I Think, Fast and Slow"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/index.html#bias-2-halo-effect",
    "href": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/index.html#bias-2-halo-effect",
    "title": "I Think, Therefore I am",
    "section": "Bias #2 Halo Effect",
    "text": "Bias #2 Halo Effect\nGame\nLook at the set of words given here PDF. We have a set of words arranged in a series; each word-set describing a person. Please look at these and then write a brief description of that person.\nLet us now compare what different people wrote about the persons described the respective word-sets.\nInterpretation\nWhat we saw was that the word-set are the same but in reverse order. The SEQUENCE in which we encounter them colours our opinion of the person whom they describe. So first impressions do seem to be last impressions! This is a Cognitive Bias the Halo Effect at work. We formulate decisions based on first impressions. What can we do to over come this bias?\nReferences for Further Reading",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "I Think, Fast and Slow"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/index.html#bias-3-priming",
    "href": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/index.html#bias-3-priming",
    "title": "I Think, Therefore I am",
    "section": "Bias #3 Priming",
    "text": "Bias #3 Priming\nGame\nYou will be given a small questionnaire PDF, with a single multiple-choice question. Please enter your answer at the appropriate location.\nIntepretation\nThe Marvels of Priming\nAs is common in science, the first big breakthrough in our understanding of the mechanism of association was an improvement in a method of measurement. Until a few decades ago, the only way to study associations was to ask many people questions such as, “What is the first word that comes to your mind when you hear the word DAY?” The researchers tallied the frequency of responses, such as “night,” “sunny,” or “long.” In the 1980s, psychologists discovered that exposure to a word causes immediate and measurable changes in the ease with which many related words can be evoked. If you have recently seen or heard the word EAT, you are temporarily more likely to complete the word fragment SO_P as SOUP than as SOAP. The opposite would happen, of course, if you had just seen WASH. We call this a priming effect and say that the idea of EAT primes the idea of SOUP, and that WASH primes SOAP.\nPriming effects take many forms. If the idea of EAT is currently on your mind (whether or not you are conscious of it), you will be quicker than usual to recognize the word SOUP when it is spoken in a whisper or presented in a blurry font. And of course you are primed not only for the idea of soup but also for a multitude of food-related ideas, including fork, hungry, fat, diet, and cookie. If for your most recent meal you sat at a wobbly restaurant table, you will be primed for wobbly as well. Furthermore, the primed ideas have some ability to prime other ideas, although more weakly. Like ripples on a pond, activation spreads through a small part of the vast network of associated ideas. The mapping of these ripples is now one of the most exciting pursuits in psychological research.\nThe Ideomotor Effect Another major advance in our understanding of memory was the discovery that priming is not restricted to concepts and words. You cannot know this from conscious experience, of course, but you must accept the alien idea that your actions and your emotions can be primed by events of which you are not even aware.\nIn an experiment that became an instant classic, the psychologist John Bargh and his collaborators asked students at New York University — most aged eighteen to twentytwo — to assemble four-word sentences from a set of five words (for example, “finds he it yellow instantly”). For one group of students, half the scrambled sentences contained words associated with the elderly, such as Florida, forgetful, bald, gray, or wrinkle. When they had completed that task, the young participants were sent out to do another experiment in an office down the hall. That short walk was what the experiment was about. The researchers unobtrusively measured the time it took people to get from one end of the corridor to the other. As Bargh had predicted, the young people who had fashioned a sentence from words with an elderly theme walked down the hallway significantly more slowly than the others.\nReferences for Further Reading",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "I Think, Fast and Slow"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/index.html#bias-4",
    "href": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/index.html#bias-4",
    "title": "I Think, Therefore I am",
    "section": "Bias #4",
    "text": "Bias #4\nGame\nIntepretation\nReferences for Further Reading",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "I Think, Fast and Slow"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/index.html#conclusions",
    "href": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/index.html#conclusions",
    "title": "I Think, Therefore I am",
    "section": "Conclusions",
    "text": "Conclusions\nSo what should we do to try to overcome these ( and other ) Cognitive Biases?\n\nBe aware: Awareness is the first step towards overcoming errors in judgement. Keeping in mind the potentially harmful consequences of first impressions is helpful when meeting new people.\n\n\nSlow down: The second step is to deliberately slow down your judgement and any subsequent decisions. For example, never make a recruitment choice straight after the interview.\n\n\nBe systematic: Finally, try to engage your analytical reasoning skills by taking a systematic approach. This sounds trickier than it is. In the context of interviewing, you could prepare a list of essential criteria and force yourself to consider each one carefully before making a choice.\n\nThis helps us to not arrive at flawed ideas to pursue but ones that we are reasonably sure we can justify to….our Field, as conceptualized by Mihaly Csikszentmihalyi.",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "I Think, Fast and Slow"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/index.html#references",
    "href": "content/courses/TRIZ4ProbSolving/Modules/37-What-Am-I-Thinking-Of/index.html#references",
    "title": "I Think, Therefore I am",
    "section": "References",
    "text": "References\n\nKahneman, Daniel. “Thinking Fast and Slow”\nBenson, Nigel C.; Ginsburg, Joannah; Grand, Voula; Lazyan, Merrin & Weeks, Marcus; Collin, Catherine, “The Psychology Handbook: Big Ideas Simply Explained”\nPashler, Harold. “Encyclopedia of the Mind”\nHolyoak, Keith J.& Morrison, Robert G., “The Cambridge Handbook of Thinking and Reasoning”\nQuality Enhancement Program (QEP cafe) https://sites.google.com/site/qepcafe/#training-modules",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "I Think, Fast and Slow"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/60-Problems-and-Contradictions/index.html#introduction",
    "href": "content/courses/TRIZ4ProbSolving/Modules/60-Problems-and-Contradictions/index.html#introduction",
    "title": "TRIZ - Problems and Contradictions",
    "section": "Introduction",
    "text": "Introduction\nLet us take our first step into the world of TRIZ. What did you think of immediately when you saw the first picture on this page?\nIn TRIZ, the fundamental way of looking at an Inventive Design Problem is to discover and propose Contradictions. These are rendered in as simple and stark a language as possible…the starker the better!\nOnce we have our Contradiction (and there can be more than one in a given Design Situation !) we can use TRIZ Principles to solve them WITHOUT COMPROMISE.\nWhat sort of Contradictions do we see in these familiar objects? What is good and what is not so good? Could that be the source of a problem to solve?",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "TRIZ - Problems and Contradictions"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/60-Problems-and-Contradictions/index.html#some-everyday-objects-for-us-to-contemplate",
    "href": "content/courses/TRIZ4ProbSolving/Modules/60-Problems-and-Contradictions/index.html#some-everyday-objects-for-us-to-contemplate",
    "title": "TRIZ - Problems and Contradictions",
    "section": "Some Everyday Objects for us to Contemplate",
    "text": "Some Everyday Objects for us to Contemplate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContemplate and note down for each Object:\n\nWhat is the Object meant to do? What is its Main Purpose?\nWhat are the other Accompanying Objects that it works with?\nWhat is One Aspect, or Parameter, or Knob for each of the Objects that you have listed, that makes the Object useful?\n\n\n\n\n\n\n\n\n\n\nWhat are the current Settings/Values for each Knob?\nChange the Setting of Each Knob to its natural opposite extreme. What Happens? Is it a good thing? When?\nYou will see that in many cases, each Knob creates a Certain Outcome at one Setting and another Outcome at the Opposite Setting.\nAre both Outcomes desirable? Do you want “eat your cake and have it too?”\nThis could be the source of your PROBLEM / CONTRADICTION !!\n\nIn TRIZ, this way of expressing a Problem as a simple Contradiction is referred to as stating an ADMINISTRATIVE CONTRADICTION (AC). (Oh those Russians…).\nLater we will use our experience with making metaphors to convert the AC into a more TRIZ-like Contradiction, using TRIZ language.\nLet us now consider some examples first and then get some practice at setting up simple AC:",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "TRIZ - Problems and Contradictions"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/60-Problems-and-Contradictions/index.html#some-example-contradictions",
    "href": "content/courses/TRIZ4ProbSolving/Modules/60-Problems-and-Contradictions/index.html#some-example-contradictions",
    "title": "TRIZ - Problems and Contradictions",
    "section": "Some Example Contradictions…",
    "text": "Some Example Contradictions…\nWe see them everywhere….if one is observant!! Let’s see a few:\nC1: Contradiction-1\nHow is this for a Contradiction? A shelf in a supermarket carries this placard for a shelf of FRESH JUICE:\n\n\n\n\n\n\n\n\n\n\nC2: Contradiction-2\nHere is another: \n\nC3: Contradiction-3\nSnails want to stay safe, and there is safety on the ground, but the ground is too hot. On the Ground, a Snail can be by itself, above the ground, they become visible to predators.\n\n\n\n\nSo…they need to group together.\nC4: Contradiction-4\nMost people’s healthcare is tied to their job….Therefore, every corona-time layoff creates yet another person without health insurance in this country. (The Corona Pandemic needs healthcare and therefore health Insurance.)\n\nThis outbreak is highlighting, with extreme clarity, every major contradiction of this society and its decaying social order. https://t.co/EtSUY3pPMi\n— Revolutionary Left Radio (@RevLeftRadio)",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "TRIZ - Problems and Contradictions"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/60-Problems-and-Contradictions/index.html#contradictory-situations",
    "href": "content/courses/TRIZ4ProbSolving/Modules/60-Problems-and-Contradictions/index.html#contradictory-situations",
    "title": "TRIZ - Problems and Contradictions",
    "section": "Contradictory Situations",
    "text": "Contradictory Situations\nWe don’t contemplate only objects at all times; indeed, as designers/artists/creators, we want to be able to make objects. What we more commonly contemplate is a situation.\nHow does one figure Parameters/Aspects/Knobs on situations?\nWe use what is called an Ishikawa Fishbone Diagram. There are many versions of this diagram depending upon the DOMAIN it is applied in; it should be considered more as a process for thinking. You should search for other forms of this diagram and quickly learn to apply them.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nDeciding which are the Knobs that matter, and how they effect the outcome is not always this simple. It would typically involve research using structured experimentation to determine the effects of turning the knobs and deciding which ones matter. This experimentation lies within a DOMAIN called [Design of Experiments].",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "TRIZ - Problems and Contradictions"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/60-Problems-and-Contradictions/index.html#documenting-the-administrative-contradiction",
    "href": "content/courses/TRIZ4ProbSolving/Modules/60-Problems-and-Contradictions/index.html#documenting-the-administrative-contradiction",
    "title": "TRIZ - Problems and Contradictions",
    "section": "Documenting the Administrative Contradiction",
    "text": "Documenting the Administrative Contradiction\nYou can document your analysis of the Situation using the following questionnaire 5W+H format:\n\nWhat does the problem seem to be?\n\nWho has the problem?\n\n(Think Knobs!!)\n\n\nWhen does the problem occur? All the time? Under certain circumstances?\nWhere does the problem occur?\n\nWhy does the problem occur?\n\n“Ask why 5 times” – W. Edwards Deming\n\n\nHow does the problem occur?\n\n\nNOTE: This process should remind you of our exercise on the Guilford and Wallach-Kogan Divergent Thinking Game, except that this is of course more structured method, whereas that was simple brainstorming.\n\nWith this method you should be able to state an Administrative Contradiction in the following (loose!) sentence structure — Items in &lt; &gt; come from the Ishikawa and your 5W + H questions):\nWhen we, as &lt; WHO / MANPOWER &gt;, attempt to perform &lt; HOW / METHOD &gt; during &lt;WHEN&gt; on &lt; WHERE / MACHINERY / KNOB&gt;, we improve &lt;EFFECT&gt;, but lose out on &lt;negative EFFECT&gt;.\nLet’s see this method in action. The Domain of work may not be ours, but the context should be clear enough!\n\n  Let us now apply these ideas to examine the following challenges below.",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "TRIZ - Problems and Contradictions"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/60-Problems-and-Contradictions/index.html#contradiction-challenges",
    "href": "content/courses/TRIZ4ProbSolving/Modules/60-Problems-and-Contradictions/index.html#contradiction-challenges",
    "title": "TRIZ - Problems and Contradictions",
    "section": "Contradiction Challenges",
    "text": "Contradiction Challenges\n\nChallenge #1: Web and PDF\nChallenge #2: Web and PDF\nChallenge #3: Web and PDF\nChallenge #4: Web and PDF",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "TRIZ - Problems and Contradictions"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/60-Problems-and-Contradictions/index.html#additional-readings",
    "href": "content/courses/TRIZ4ProbSolving/Modules/60-Problems-and-Contradictions/index.html#additional-readings",
    "title": "TRIZ - Problems and Contradictions",
    "section": "Additional Readings",
    "text": "Additional Readings\nDo glance through these articles and try to form a “contradiction mentality” about things around you:\n\nEllen Domb (1997). How to Help TRIZ Beginners Succeed. https://www.metodolog.ru/triz-journal/archives/1997/04/a/index.html\nhttps://www.bbc.com/worklife/article/20201109-why-the-paradox-mindset-is-the-key-to-success\nhttps://hbr.org/2008/06/the-contradictions-that-drive-toyotas-success\nOpen Source TRIZ: Contradiction Challenges! https://www.opensourcetriz.com/images/1_OpenSourceTRIZ_Pictures/1.1_Teaching_Materials/01_Contradictions_Exercises_Actual.pdf\nhttps://taproot.com/whats-wrong-with-5-whys-complete-article/&lt;/u",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "TRIZ - Problems and Contradictions"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/30-MBTI-Big-5/index.html#introduction",
    "href": "content/courses/TRIZ4ProbSolving/Modules/30-MBTI-Big-5/index.html#introduction",
    "title": "Birds of Different Feathers",
    "section": "Introduction",
    "text": "Introduction\nSocrates said, “Gnothi seauton”(Γνῶθι σαυτόν), know thyself. Let us do that, as we create some novel situations for ourselves in a Game.\n\nIcebreaker Game!\nLook at the slide full of numbers on the screen. I would like you to:\n\nSilently work through the slide until you have found all the numbers\n\nRaise your hand when you have done so.\n\n\n\nA Personality Game\nI have divided you up into 4 groups. Each group will be given a specific task. You will need to:\n\nDiscuss how to complete the Task\n\nComplete the Task and document your work\n\nAlso document the main aspects of Discussion in your group. Who said/ suggested what; what were the points of agreement; what were the contentious points etc.\n\nPresent all of this : 7 + 5 minutes / team",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "Birds of Different Feathers"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/30-MBTI-Big-5/index.html#references",
    "href": "content/courses/TRIZ4ProbSolving/Modules/30-MBTI-Big-5/index.html#references",
    "title": "Birds of Different Feathers",
    "section": "References",
    "text": "References\n\nHere is the MBTI Test site: https://www.16personalities.com/free-personality-test\nIce-breaker Activity Presentation (Go to Webpage)\nMBTI Game: PDF",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "Birds of Different Feathers"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/30-MBTI-Big-5/index.html#why-does-this-matter-to-us",
    "href": "content/courses/TRIZ4ProbSolving/Modules/30-MBTI-Big-5/index.html#why-does-this-matter-to-us",
    "title": "Birds of Different Feathers",
    "section": "Why does this Matter to Us",
    "text": "Why does this Matter to Us\nFrom the two exercises, we see that:\n- reality really matters upon the viewer: and that there are many possible points of view.\n- We all have our “default modes” for behaviour\n- As creators we need to be able straddle, if possible, the spectrum of the 4 preference pairs, in order to comprehend reality better.",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "Birds of Different Feathers"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/10-Flow-and-Play/index.html#introduction",
    "href": "content/courses/TRIZ4ProbSolving/Modules/10-Flow-and-Play/index.html#introduction",
    "title": "I am Water",
    "section": "Introduction",
    "text": "Introduction\nThere is some stuff lying on the floor of the classroom.\n\nLego\nWhere’s Waldo? Books \nMagic Eye Books \nDominos \nMagnets + Links \nParquetry Blocks \nImaginarium Train Set + Thomas the Tank Engine \n\nPlay!! Yes, you.",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "I am Water"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/10-Flow-and-Play/index.html#discussion-1",
    "href": "content/courses/TRIZ4ProbSolving/Modules/10-Flow-and-Play/index.html#discussion-1",
    "title": "I am Water",
    "section": "Discussion #1",
    "text": "Discussion #1\n\nDid you get bored?\nDid you lose sense of time at any point?\nDid anything become too difficult at any time?\nWhat did you do at such times?",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "I am Water"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/10-Flow-and-Play/index.html#what-is-flow",
    "href": "content/courses/TRIZ4ProbSolving/Modules/10-Flow-and-Play/index.html#what-is-flow",
    "title": "I am Water",
    "section": "What is Flow?",
    "text": "What is Flow?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe will understand Flow from:\n\nThe book Flow and the Psychology of Happiness, pp; 72-76; by Mihaly Csikszentmihalyi. First, let us learn to say his name!!\n\n Your browser does not support the audio tag;  for browser support, please see:  https://www.w3schools.com/tags/tag_audio.asp \nHere is the book.\n\nThis video: \n\n\n\n\n\nThis TED Talk : Flow, the Secret to Happiness\n\n\n\n\n 3. This is Water: David Foster Wallace’s famous Talk: \n  4. Let us quickly read and discuss this extract from the Play and Playground Encyclopedia https://www.pgpedia.com/m/man-play-and-games",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "I am Water"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/10-Flow-and-Play/index.html#discussion-2",
    "href": "content/courses/TRIZ4ProbSolving/Modules/10-Flow-and-Play/index.html#discussion-2",
    "title": "I am Water",
    "section": "Discussion #2",
    "text": "Discussion #2\n\nIs the idea of Flow elusive?\nDo only some people get it, and then only at some times?\nWas David Foster Wallace’s talk somewhat self-deluding?\nThen what is this Empathy stuff we talk about when we speak of Design Thinking?",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "I am Water"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/10-Flow-and-Play/index.html#pop-quix-want-still-more-flow",
    "href": "content/courses/TRIZ4ProbSolving/Modules/10-Flow-and-Play/index.html#pop-quix-want-still-more-flow",
    "title": "I am Water",
    "section": "Pop Quix: Want still more Flow?",
    "text": "Pop Quix: Want still more Flow?\nNo, that is not a bad spelling….why not?\nGo find me in this picture !!",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "I am Water"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/10-Flow-and-Play/index.html#references",
    "href": "content/courses/TRIZ4ProbSolving/Modules/10-Flow-and-Play/index.html#references",
    "title": "I am Water",
    "section": "References",
    "text": "References\n\nScott Eberle, “The Aspects of Play” https://www.journalofplay.org/sites/www.journalofplay.org/files/pdf-articles/6-2-article-elements-of-play.pdf\n“Platform Creativity: Domain, Field, and Person”. https://medium.com/call4/domain-8a22b6b486f4\n8 Ways To Create Flow According to Mihaly Csikszentmihalyi. https://positivepsychology.com/mihaly-csikszentmihalyi-father-of-flow/\nIan Bogost, “Play at Anything”.",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "I am Water"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/40-Parallel-Thinking/index.html#parallel-thinking",
    "href": "content/courses/TRIZ4ProbSolving/Modules/40-Parallel-Thinking/index.html#parallel-thinking",
    "title": "The Art of Parallel Thinking",
    "section": "Parallel Thinking",
    "text": "Parallel Thinking\nEdward de Bono invented the concept and phrase, “Parallel Thinking” in his book with the same title. Let us understand what this is:\n\n\n\n  Download PDF File\n   \n    Unable to display PDF file. Download instead.",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "The Art of Parallel Thinking"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/40-Parallel-Thinking/index.html#propositions",
    "href": "content/courses/TRIZ4ProbSolving/Modules/40-Parallel-Thinking/index.html#propositions",
    "title": "The Art of Parallel Thinking",
    "section": "Propositions",
    "text": "Propositions\nNow that we have an idea of about Parallel Thinking, let us get into it! Let us consider a couple of ideas for schemes that have been “around” for some time:\n\nMilitary Conscription for 2 years\nChildren Manage the House for one week\nYoung People must be assigned one Elderly Person to live with\nAll Cars must be removed from the City Centre\n\nCan we apply Parallel Thinking to these and come up with a vote, or even a way to achieve these, if we can agree with the Propositions?",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "The Art of Parallel Thinking"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/40-Parallel-Thinking/index.html#some-of-the-de-bono-thinking-tools",
    "href": "content/courses/TRIZ4ProbSolving/Modules/40-Parallel-Thinking/index.html#some-of-the-de-bono-thinking-tools",
    "title": "The Art of Parallel Thinking",
    "section": "Some of the de Bono Thinking Tools:",
    "text": "Some of the de Bono Thinking Tools:\n\n6 Thinking Hats Quick Reference (PDF Link)\n6H Technique - PMI: Plus Minus Interesting : https://www.debono.com/de-bono-thinking-lessons-1/1.-PMI-lesson-plan\nAlso see:https://sites.google.com/site/qepcafe/modules/expand/plus-minus-interesting-pmi\n6H Technique - APC: Alternatives, Possibilities, Choices: https://www.debono.com/de-bono-thinking-lessons-1/6.-APC-lesson-plan\n6H Technique - CAF: Consider All Factors: https://www.debono.com/de-bono-thinking-lessons-1/2.-CAF-lesson-plan\n6H Technique - OPV: Other Peoples’ Voices: https://www.debono.com/de-bono-thinking-lessons-1/7.-OPV-lesson-plan\n6H Technique - CnS: Consequence and Sequel : https://www.debono.com/de-bono-thinking-lessons-1/3.-C%26S-lesson-plan",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "The Art of Parallel Thinking"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/40-Parallel-Thinking/index.html#references",
    "href": "content/courses/TRIZ4ProbSolving/Modules/40-Parallel-Thinking/index.html#references",
    "title": "The Art of Parallel Thinking",
    "section": "References",
    "text": "References\n\nReading in Parallel Thinking (Web Link)\nEdward de Bono, Six Thinking Hats (Goodreads)\nQuality Enhancement Program (QEP cafe) https://sites.google.com/site/qepcafe/#training-modules ( Link appears to be dead ;-() )\nReverse Brainstorming https://sites.google.com/site/qepcafe/modules/expand/reverse-brainstorming ( Link appears to be dead ;-() )\nhttps://www.mindtools.com/a32qxsh/reverse-brainstorming",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "The Art of Parallel Thinking"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/70-TRIZ-Resources/index.html#introduction",
    "href": "content/courses/TRIZ4ProbSolving/Modules/70-TRIZ-Resources/index.html#introduction",
    "title": "TRIZ - The Unreasonable Effectiveness of Available Resources",
    "section": "Introduction",
    "text": "Introduction\nIn the previous Module on Problems and Contradictions we understood how to identify an Administrative Contradiction in everyday situations. This is the first and most important step in the TRIZ Problem Solving Method.\nAnother important idea in TRIZ is that of Available Resources. Let us appreciate this idea with the help of two games.",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "TRIZ - The Unreasonable Effectiveness of Available Resources"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/70-TRIZ-Resources/index.html#available-resources-game-1",
    "href": "content/courses/TRIZ4ProbSolving/Modules/70-TRIZ-Resources/index.html#available-resources-game-1",
    "title": "TRIZ - The Unreasonable Effectiveness of Available Resources",
    "section": "Available Resources: Game #1",
    "text": "Available Resources: Game #1\nYou have to Stick the lighted candle to the Wall in such a way that the melting wax does not drop on to the floor. Your resources are:\n\nCandle\nMatchbox\nThumbtack",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "TRIZ - The Unreasonable Effectiveness of Available Resources"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/70-TRIZ-Resources/index.html#available-resources-game-2",
    "href": "content/courses/TRIZ4ProbSolving/Modules/70-TRIZ-Resources/index.html#available-resources-game-2",
    "title": "TRIZ - The Unreasonable Effectiveness of Available Resources",
    "section": "Available Resources: Game #2",
    "text": "Available Resources: Game #2\nLook at the graph below: does it remind you of something you know very well?\n\n\n\n\n\n\n\n\nWhat does this graph represent?\nLet us pretend we are part of this graph and see where our Problem Formulating Skills take us!",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "TRIZ - The Unreasonable Effectiveness of Available Resources"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/70-TRIZ-Resources/index.html#discussion",
    "href": "content/courses/TRIZ4ProbSolving/Modules/70-TRIZ-Resources/index.html#discussion",
    "title": "TRIZ - The Unreasonable Effectiveness of Available Resources",
    "section": "Discussion",
    "text": "Discussion\n\nProblems and Contradictions\nAll Available Resources\n\nAssumptions and Functional Fixedness\n\n\n\nA comparable switch of attention occurs in an old joke about a worker in a high security factory, in which the employees were carefully watched when they left at the end of their work day. On a particular day, this worker was stopped at the factory gate as he walked out with a wheelbarrow full of styrofoam packing peanuts. He explained that he had salvaged these from the trash, and was planning to use them in shipping gifts to his grandchildren. Searching through this packing material, the guards found nothing, and so they let the man go home. The following week the same thing happened, and the worker was again stopped. But he offered the very same story, and when the guards searched through the packing peanuts and found nothing, he was allowed to leave. But this continued, week after week, until the guards could no longer believe that one person would want or could make use of so much packing material. Finally, the man was held for interrogation, at which time he admitted that he had absolutely no use for packing peanuts - and that, all these weeks, he had been stealing wheelbarrows.\n\n\nHearing this joke, I am reminded of the phrase “part and parcel”, which is a rough equivalent of “figure and ground”, the Gestalt Principles. Throughout most of it, the packing peanuts occupy center stage as figure (part), while the wheelbarrows (which function merely as containers) are completely ignored as innocuous ground (parcel). At the end of the joke, there is an unexpected twist, a switch of emphasis, a recentering, when we learn that the parcel is really the part.\n\nThis should also remind us of the Guilford Alternative Uses Exercise that we did, where we forced ourselves to leave the “regular use” of an object behind and think of it as serving quite another function.",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "TRIZ - The Unreasonable Effectiveness of Available Resources"
    ]
  },
  {
    "objectID": "content/courses/TRIZ4ProbSolving/Modules/70-TRIZ-Resources/index.html#references",
    "href": "content/courses/TRIZ4ProbSolving/Modules/70-TRIZ-Resources/index.html#references",
    "title": "TRIZ - The Unreasonable Effectiveness of Available Resources",
    "section": "References",
    "text": "References\n\nhttps://www.wikiwand.com/en/Candle_problem\nResources Game PDF\nhttps://thedecisionlab.com/biases/functional-fixedness\nhttps://www.interaction-design.org/literature/article/the-laws-of-figure-ground-praegnanz-closure-and-common-fate-gestalt-principles-3\nStan Kaplan, An Introduction to TRIZ (PDF) This is a simple and short introduction to all aspects of Classical TRIZ.\nJack Hipple, “The Ideal Result: What it is and how to achieve it”, Springer, 2012.",
    "crumbs": [
      "Teaching",
      "TRIZ for Problem Solving",
      "TRIZ - The Unreasonable Effectiveness of Available Resources"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/140-WorkingWithTime/index.html",
    "href": "content/courses/MathModelsDesign/Modules/140-WorkingWithTime/index.html",
    "title": "Working with Time",
    "section": "",
    "text": "Topics that may be covered here: - Seasons and Trends - Forecasting - Queues and Queuing Theory ( R package “simmer”)",
    "crumbs": [
      "Teaching",
      "Math Models in Design",
      "Working with Time"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/140-WorkingWithTime/index.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/140-WorkingWithTime/index.html#introduction",
    "title": "Working with Time",
    "section": "",
    "text": "Topics that may be covered here: - Seasons and Trends - Forecasting - Queues and Queuing Theory ( R package “simmer”)",
    "crumbs": [
      "Teaching",
      "Math Models in Design",
      "Working with Time"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-WorkingWithSound/index.html",
    "href": "content/courses/MathModelsDesign/Modules/100-WorkingWithSound/index.html",
    "title": "Working with Sound",
    "section": "",
    "text": "Topics that may be covered here:\n- Fourier Transforms\n- Sound Analysis and Synthesis using FFT\n- The idea of Convolution!",
    "crumbs": [
      "Teaching",
      "Math Models in Design",
      "Working with Sound"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-WorkingWithSound/index.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/100-WorkingWithSound/index.html#introduction",
    "title": "Working with Sound",
    "section": "",
    "text": "Topics that may be covered here:\n- Fourier Transforms\n- Sound Analysis and Synthesis using FFT\n- The idea of Convolution!",
    "crumbs": [
      "Teaching",
      "Math Models in Design",
      "Working with Sound"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/100-WorkingWithSound/index.html#resources",
    "href": "content/courses/MathModelsDesign/Modules/100-WorkingWithSound/index.html#resources",
    "title": "Working with Sound",
    "section": "Resources",
    "text": "Resources\n\nhttps://www.electronicbeats.net/the-feed/excel-drum-machine/\nStrudel REPL https://strudel.cc\nIntroducing Jukebox, a neural net that generates music, including rudimentary singing, as raw audio in a variety of genres and artist styles. We’re releasing a tool for everyone to explore the generated samples, as well as the model and code: https://t.co/EUq7hNZv62 pic.twitter.com/sh5yHz7qrc\n\n— OpenAI (@OpenAI) April 30, 2020\nvia Twitter https://twitter.com/OpenAI\nApril 30, 2020 at 09:26PM",
    "crumbs": [
      "Teaching",
      "Math Models in Design",
      "Working with Sound"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/90-WorkingWithShape/index.html",
    "href": "content/courses/MathModelsDesign/Modules/90-WorkingWithShape/index.html",
    "title": "Working with Shape",
    "section": "",
    "text": "Topics that may be covered here:\n- Iterated Function Systems and Fractals\n- Symmetries in Space - L Systems, Mandalas and Kolams\n- Logistic Functions - Projections &gt; &gt; NEW VIDEO! This equation will change how you see the &gt; world…https://t.co/xoDKE5KbBy\nIt’s about the Logistic Map, bifurcation diagrams, the Mandelbrot set, animal populations, dripping faucets, neuron firing rates and more. pic.twitter.com/U8rlkWBbkv\nWorking with Shadows. https://www.wikiwand.com/en/Map_projection\nWorking with Fourier Series and Epicyles http://www.jezzamon.com/fourier/index.html and https://alex.miller.im/posts/fourier-series-spinning-circles-visualization/\nhttps://twitter.com/i/status/962449509782495232 https://codegolf.stackexchange.com/questions/36374/redraw-an-image-with-just-one-closed-curve",
    "crumbs": [
      "Teaching",
      "Math Models in Design",
      "Working with Shape"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/90-WorkingWithShape/index.html#introduction",
    "href": "content/courses/MathModelsDesign/Modules/90-WorkingWithShape/index.html#introduction",
    "title": "Working with Shape",
    "section": "",
    "text": "Topics that may be covered here:\n- Iterated Function Systems and Fractals\n- Symmetries in Space - L Systems, Mandalas and Kolams\n- Logistic Functions - Projections &gt; &gt; NEW VIDEO! This equation will change how you see the &gt; world…https://t.co/xoDKE5KbBy\nIt’s about the Logistic Map, bifurcation diagrams, the Mandelbrot set, animal populations, dripping faucets, neuron firing rates and more. pic.twitter.com/U8rlkWBbkv\nWorking with Shadows. https://www.wikiwand.com/en/Map_projection\nWorking with Fourier Series and Epicyles http://www.jezzamon.com/fourier/index.html and https://alex.miller.im/posts/fourier-series-spinning-circles-visualization/\nhttps://twitter.com/i/status/962449509782495232 https://codegolf.stackexchange.com/questions/36374/redraw-an-image-with-just-one-closed-curve",
    "crumbs": [
      "Teaching",
      "Math Models in Design",
      "Working with Shape"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/90-WorkingWithShape/index.html#generative-art",
    "href": "content/courses/MathModelsDesign/Modules/90-WorkingWithShape/index.html#generative-art",
    "title": "Working with Shape",
    "section": "Generative Art",
    "text": "Generative Art\n\nArt from Code: Danielle Navarro. https://art-from-code.netlify.app\nhttps://generative.substack.com/p/generative-art-and-r\nhttps://github.com/cutterkom/generativeart\nhttps://www.bigbookofr.com/art.html#thinking-outside-the-grid---a-bare-bones-intro-to-rtistry-concepts-in-r-using-ggplot\nhttps://clauswilke.com/art/\nhttps://generatecoll.medium.com/how-i-used-excel-to-create-abstract-album-artwork-fee740d4414f\nRandom Digital Beauty. https://anaselk.com/p/generative-r/\nhttps://nannou.cc\nhttps://openframeworks.cc/\nhttps://libcinder.org",
    "crumbs": [
      "Teaching",
      "Math Models in Design",
      "Working with Shape"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/90-WorkingWithShape/index.html#kolams-in-r-and-other-tools",
    "href": "content/courses/MathModelsDesign/Modules/90-WorkingWithShape/index.html#kolams-in-r-and-other-tools",
    "title": "Working with Shape",
    "section": "Kolams, in R and other tools",
    "text": "Kolams, in R and other tools\n\nReddy, Anu, and Alex McLean. 2024. “Drawing Kolam Patterns in Stitches and Code.” Algorithmic Pattern, March. https://doi.org/10.21428/108765d1.53f112a6.\nEzine for Kolam: ( Bad Link) https://files.cargocollective.com/c989887/Kolam_zine--1-.png\nhttps://www.ted.com/talks/ron_eglash_the_fractals_at_the_heart_of_african_designs\nhttps://www.atlasobscura.com/articles/indian-rice-art-kolam\nhttps://www.sciencedirect.com/science/article/abs/pii/0146664X74900112\nhttps://www.sciencedirect.com/science/article/pii/S0166218X09000845#b26\nhttp://interdigitation.embodimentlabs.org/\nhttps://www.jeremykun.com/2014/09/29/hybrid-images/ Using Fourier Series to morph images !!!",
    "crumbs": [
      "Teaching",
      "Math Models in Design",
      "Working with Shape"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/90-WorkingWithShape/index.html#book-references",
    "href": "content/courses/MathModelsDesign/Modules/90-WorkingWithShape/index.html#book-references",
    "title": "Working with Shape",
    "section": "Book References",
    "text": "Book References\n\nMarcia Ascher, “Ethnomathematics”\nClaudia Zaslavsky, “Africa Counts”\nRon Eglash, “Africa Fractals”\nPaulus Gerdes, “Adventures in the World of Matrices”",
    "crumbs": [
      "Teaching",
      "Math Models in Design",
      "Working with Shape"
    ]
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/180-WorkingWithNeuralNets/index.html",
    "href": "content/courses/MathModelsDesign/Modules/180-WorkingWithNeuralNets/index.html",
    "title": "Working With Thoughts",
    "section": "",
    "text": "We will play several short games followed by discussions. These games may bring to light some of our Cognitive Biases and see how they affect us, and especially as we try to function as Artists/Designers/Creators.\nThere will be short readings that follow after each game.\nBut first, let us see how frail/fragile/fallible… we all are:\n\n\n\nRight! On to our first little fallibility!\n\n\n\n\nTest: PPT\n\nShort Reading: PDF\n\nTool: PDF\n\nReading: Here is a short reading on Exaggerated Emotional Coherence, also known as the Halo Effect** Download PDF **\n\nYou have to Stick the lighted candle to the Wall in such a way that the melting wax does not drop on to the floor.\n\nLook at the graph below: does it remind you of something you know very well?\n\n\n\n\n\n\n\n\nWhat does this graph represent?\nLet us pretend we are part of this graph and see where our Problem Formulating Skills take us!"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/180-WorkingWithNeuralNets/index.html#can-you-see-straight",
    "href": "content/courses/MathModelsDesign/Modules/180-WorkingWithNeuralNets/index.html#can-you-see-straight",
    "title": "Working With Thoughts",
    "section": "",
    "text": "We will play several short games followed by discussions. These games may bring to light some of our Cognitive Biases and see how they affect us, and especially as we try to function as Artists/Designers/Creators.\nThere will be short readings that follow after each game.\nBut first, let us see how frail/fragile/fallible… we all are:\n\n\n\nRight! On to our first little fallibility!\n\n\n\n\nTest: PPT\n\nShort Reading: PDF\n\nTool: PDF\n\nReading: Here is a short reading on Exaggerated Emotional Coherence, also known as the Halo Effect** Download PDF **\n\nYou have to Stick the lighted candle to the Wall in such a way that the melting wax does not drop on to the floor.\n\nLook at the graph below: does it remind you of something you know very well?\n\n\n\n\n\n\n\n\nWhat does this graph represent?\nLet us pretend we are part of this graph and see where our Problem Formulating Skills take us!"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/180-WorkingWithNeuralNets/index.html#discussion",
    "href": "content/courses/MathModelsDesign/Modules/180-WorkingWithNeuralNets/index.html#discussion",
    "title": "Working With Thoughts",
    "section": "Discussion",
    "text": "Discussion\n\nProblems and Contradictions\nAll Available Resources\n\nAssumptions and Functional Fixedness\n\n\n\nA comparable switch of attention occurs in an old joke about a worker in a high security factory, in which the employees were carefully watched when they left at the end of their work day. On a particular day, this worker was stopped at the factory gate as he walked out with a wheelbarrow full of styrofoam packing peanuts. He explained that he had salvaged these from the trash, and was planning to use them in shipping gifts to his grandchildren. Searching through this packing material, the guards found nothing, and so they let the man go home. The following week the same thing happened, and the worker was again stopped. But he offered the very same story, and when the guards searched through the packing peanuts and found nothing, he was allowed to leave. But this continued, week after week, until the guards could no longer believe that one person would want or could make use of so much packing material. Finally, the man was held for interrogation, at which time he admitted that he had absolutely no use for packing peanuts - and that, all these weeks, he had been stealing wheelbarrows.\n\n\nHearing this joke, I am reminded of the phrase “part and parcel”, which is a rough equivalent of “figure and ground”, the Gestalt Principles. Throughout most of it, the packing peanuts occupy center stage as figure (part), while the wheelbarrows (which function merely as containers) are completely ignored as innocuous ground (parcel). At the end of the joke, there is an unexpected twist, a switch of emphasis, a recentering, when we learn that the parcel is really the part.\n\nThis should also remind us of the Guilford Alternative Uses Exercise that we did, where we forced ourselves to leave the “regular use” of an object behind and think of it as serving quite another function.\nBias on TV\nLet’s find some of these ideas in our favourite Episode of one Season of your favourite show and tell everybody with a poster!"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/180-WorkingWithNeuralNets/index.html#bayesian-estimation",
    "href": "content/courses/MathModelsDesign/Modules/180-WorkingWithNeuralNets/index.html#bayesian-estimation",
    "title": "Working With Thoughts",
    "section": "Bayesian Estimation",
    "text": "Bayesian Estimation\nTaxicab Accident problem\nDisease Problem\nBaseball score prediction in R ( David Robinson)"
  },
  {
    "objectID": "content/courses/MathModelsDesign/Modules/180-WorkingWithNeuralNets/index.html#references",
    "href": "content/courses/MathModelsDesign/Modules/180-WorkingWithNeuralNets/index.html#references",
    "title": "Working With Thoughts",
    "section": "References",
    "text": "References\n\nThe Halo Effect, https://explorable.com/halo-effect\nNisbett, R. E., & Wilson, T. D. (1977). The halo effect: Evidence for unconscious alteration of judgments. Journal of Personality and Social Psychology, 35(4), 250–256. https://doi.org/10.1037/0022-3514.35.4.250 Download PDF\nBayesian Thinking Tutorial https://arbital.com/p/bayes_frequency_diagram/?l=55z&pathId=86923\nhttp://ndl.ethernet.edu.et/bitstream/123456789/37455/1/Max_Marchi.pdf"
  },
  {
    "objectID": "content/courses/R4Artists/listing.html",
    "href": "content/courses/R4Artists/listing.html",
    "title": "R for Artists and Managers",
    "section": "",
    "text": "🕶 Lab-1: Science, Human Experience, Experiments, and Data\n\n\nWhy do we visualize data\n\n\n\n\n\nNov 1, 2021\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nLab-2: Down the R-abbit Hole…\n\n\nWelcome ! Introduce Yourself to R, RStudio, and to all of Us!\n\n\n\n\n\nJul 9, 2021\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nLab-3: Drink Me!\n\n\nWorking with Quarto\n\n\n\n\n\nMar 10, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nLab-4: I say what I mean and I mean what I say\n\n\nGetting started with Data in R\n\n\n\n\n\nJul 12, 2022\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nLab-5: Twas brillig, and the slithy toves…\n\n\nTidy Data at the wabe MoMA\n\n\n\n\n\nNov 22, 2022\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nLab-6: These Roses have been Painted !!\n\n\nThe Grammar of Graphics in R\n\n\n\n\n\nAug 21, 2022\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nLab-7: The Lobster Quadrille\n\n\nFonts and other Wizardy in ggplot\n\n\n\n\n\nJul 21, 2021\n\n\n34 min\n\n\n\n\n\n\n\n\n\n\n\n\nLab-8: Did you ever see such a thing as a drawing of a muchness?\n\n\nWorking with htmlwidgets in R!\n\n\n\n\n\nJul 10, 2022\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nLab-9: If you please sir…which way to the Secret Garden?\n\n\nThe Grammar of Maps\n\n\n\n\n\nJul 10, 2022\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nLab-10: An Invitation from the Queen…to play Croquet\n\n\nThe Grammar of Networks\n\n\n\n\n\nJun 6, 2022\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nLab-11: The Queen of Hearts, She Made some Tarts\n\n\nThe Grammar of Diagrams\n\n\n\n\n\nInvalid Date\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\nLab-12: Time is a Him!!\n\n\nTime Series in R\n\n\n\n\n\nFeb 14, 2022\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nIteration: Learning to purrr\n\n\nPerforming Iterations in R\n\n\n\n\n\nJun 14, 2023\n\n\n17 min\n\n\n\n\n\n\nNo matching items\n\n Back to top",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html",
    "title": "The Grammar of Diagrams",
    "section": "",
    "text": "There are many presentation and drawing tools out there. And these allow the user full control over the diagram so generally result in prettier diagrams that can convey more information to the audience at that point in time.\nBut that point in time passes, and pretty pictures can quickly become out-of-date and, ironically, misinforming if they don’t match the reality of the system they are describing. This is especially so if one team is drawing the pretty pictures, and another team is writing the software/implementing the system.\nHaving diagrams as code that can live beside the system design/code, that the stakeholders are equally comfortable editing and viewing,reduces the gap i.e. “Where system diagrams meet system reality”.\nWe will “explore” two packages to do this: DiagrammeR and nomnoml. Each of these follows a specific grammar so that sets of “sentences” will morph into very different kinds of diagrams."
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#sequence-diagram",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#sequence-diagram",
    "title": "The Grammar of Diagrams",
    "section": "Sequence Diagram",
    "text": "Sequence Diagram\nLook at the code below: What do you think it represents?\n\nCodeDiagrammeR(\"\nsequenceDiagram\nArvind -&gt;&gt; Anamika: Why are you late today?\nAnamika -&gt;&gt; Anamika: Ulp...\nAnamika -&gt;&gt; Arvind: I am sorry... &lt;br&gt; may I come in please?\n\nArvind -&gt;&gt; Komal: And you? What kept you?\nKomal -&gt;&gt; Anamika: (Quietly) He's having a bad day, dude...\nAnamika -&gt;&gt; Komal: (Whisper) Boomer...\n\")\n\n\n\n\n\nThis is a simple Sequence Diagram! Shows a strictly imaginary exchange between a pair of students and an unknown Faculty Member.\nLet us now see how we can embellish this kind of diagram. Can we have a Garden of Forking Paths?\n\nCodeDiagrammeR(\"\n  graph LR\n    A--&gt;B\n    A--&gt;C\n    C--&gt;E\n    B--&gt;D\n    C--&gt;D\n    D--&gt;F\n    E--&gt;F\n\")\n\n\n\n\n\n\nCodeDiagrammeR(\"\n        sequenceDiagram\n        \n        alt Anamika is always punctual\n        Arvind -&gt;&gt; Anamika: Why haven't you put up your Daily Reflection?\n        Anamika -&gt;&gt; Anamika: Ulp...\n        Note right of Anamika : I have had it today..\n        Anamika -&gt;&gt; Arvind: I am sorry... \n        Arvind -&gt;&gt; Anamika: Ok write it today\n        \n        else Anamika is usually tardy\n        Arvind -&gt;&gt; Anamika: Why haven't you put up your Daily Reflection?\n        Anamika -&gt;&gt; Anamika: Ulp...\n        Anamika -&gt;&gt; Arvind: I am sorry... \n        Arvind -&gt;&gt; Anamika: This is not acceptable and will reflect in your grade\n        end\n        \n        Arvind -&gt;&gt; Komal: And you? What kept you?\n        Komal -&gt;&gt; Anamika: (Quietly) He's having a bad day, dude...\n        Anamika -&gt;&gt; Komal: (Whisper) Boomer...\n        Note over Anamika,Komal: Giggle...\n\")\n\n\n\n\n\nFrom here: https://cyberhelp.sesync.org/blog/visualization-with-diagrammeR.html\n\nCodegrViz(\"digraph{\n\n      graph[rankdir = LR]\n  \n      node[shape = rectangle, style = filled]\n  \n      node[fillcolor = Coral, margin = 0.2]\n      A[label = 'Figure 1: Map']\n      B[label = 'Figure 2: Metrics']\n  \n      node[fillcolor = Cyan, margin = 0.2]\n      C[label = 'Figures.Rmd']\n  \n      node[fillcolor = Violet, margin = 0.2]\n      D[label = 'Analysis_1.R']\n      E[label = 'Analysis_2.R']\n  \n      subgraph cluster_0 {\n        graph[shape = rectangle]\n        style = rounded\n        bgcolor = Gold\n    \n        label = 'Data Source 1'\n        node[shape = rectangle, fillcolor = LemonChiffon, margin = 0.25]\n        F[label = 'my_dataframe_1.csv']\n        G[label = 'my_dataframe_2.csv']\n      }\n  \n      subgraph cluster_1 {\n         graph[shape = rectangle]\n         style = rounded\n         bgcolor = Gold\n    \n         label = 'Data Source 2'\n         node[shape = rectangle, fillcolor = LemonChiffon, margin = 0.25]\n         H[label = 'my_dataframe_3.csv']\n         I[label = 'my_dataframe_4.csv']\n      }\n  \n      edge[color = black, arrowhead = vee, arrowsize = 1.25]\n      C -&gt; {A B}\n      D -&gt; C\n      E -&gt; C\n      F -&gt; D\n      G -&gt; D\n      H -&gt; E\n      I -&gt; E\n      \n      }\")\n\n\n\n\n\n\nCodemermaid(\"\n        graph BT\n        A((Salinity))\n        A--&gt;B(Barnacles)\n        B-.-&gt;|-0.10|B1{Mussels}\n        A-- 0.30 --&gt;B1\n\n        C[Air Temp]\n        C--&gt;B\n        C-.-&gt;E(Macroalgae)\n        E--&gt;B1\n        C== 0.89 ==&gt;B1\n\n        style A fill:#FFF, stroke:#333, stroke-width:4px\n        style B fill:#9AA, stroke:#9AA, stroke-width:2px\n        style B1 fill:#879, stroke:#333, stroke-width:1px\n        style C fill:#ADF, stroke:#333, stroke-width:2px\n        style E fill:#9C2, stroke:#9C2, stroke-width:2px\n\n        \")\n\n\n\n\n\n\nCodeDiagrammeR(\"\nsequenceDiagram\n  Arvind -&gt;&gt;ticket seller: ask ticket\n  ticket seller-&gt;&gt;database: seats\n  alt tickets available\n    database-&gt;&gt;ticket seller: ok\n    ticket seller-&gt;&gt;customer: confirm\n    Arvind -&gt;&gt;ticket seller: ok\n    ticket seller-&gt;&gt;database: book a seat\n    ticket seller-&gt;&gt;printer: print ticket\n  else sold out\n    database-&gt;&gt;ticket seller: none left\n    ticket seller-&gt;&gt;customer: sorry\n  end\n\")\n\n\n\n\n\n\nCodeDiagrammeR(\n\"graph TB;\nA(Rounded)--&gt;B[Squared];\nB--&gt;C{A Decision};\nC--&gt;D[Square One];\nC--&gt;E[Square Two];\n\n%% Now styling these blocks\nstyle A fill:#E5E25F;  \nstyle B fill:#87AB51; \nstyle C fill:#3C8937;\nstyle D fill:#23772C;  \nstyle E fill:#B6E6E6;\n\"\n)\n\n\n\n\n\n\nCode  grViz(\"\ndigraph boxes_and_circles {\n\n  # a 'graph' statement\n  graph [overlap = true, fontsize = 10,forcelabels = true]\n\n  # several 'node' statements\n  node [shape = box,fontname = Helvetica, color = red, style = filled]\n  A[label = 'This is \\\\n an internal \\\\n label', xlabel = 'This is \\\\nan external \\\\nlabel']; B; C; D; E; F\n\n  node [shape = circle, fixedsize = true, color = palegreen, width = 0.9] // sets as circles\n  1; 2; 3; 4; 5; 6; 7; 8\n\n  # several 'edge' statements\n  A-&gt;{1,2,3,4} B-&gt;2 B-&gt;3 B-&gt;4 C-&gt;A\n  1-&gt;D E-&gt;A 2-&gt;4 1-&gt;5 1-&gt;F\n  E-&gt;6 4-&gt;6 5-&gt;7 6-&gt;7 3-&gt;8 3-&gt;1\n}\n\")"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#sequence-diagram-2",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#sequence-diagram-2",
    "title": "The Grammar of Diagrams",
    "section": "Sequence Diagram-2",
    "text": "Sequence Diagram-2"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#sequence-diagram-3",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#sequence-diagram-3",
    "title": "The Grammar of Diagrams",
    "section": "Sequence Diagram 3",
    "text": "Sequence Diagram 3"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#mindmap",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#mindmap",
    "title": "The Grammar of Diagrams",
    "section": "Mindmap",
    "text": "Mindmap"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#gantt-chart",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#gantt-chart",
    "title": "The Grammar of Diagrams",
    "section": "Gantt Chart",
    "text": "Gantt Chart"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#flow-chart",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#flow-chart",
    "title": "The Grammar of Diagrams",
    "section": "Flow chart",
    "text": "Flow chart"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#some-definitions-on-the-grammar-of-shapes-in-nomnoml",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#some-definitions-on-the-grammar-of-shapes-in-nomnoml",
    "title": "The Grammar of Diagrams",
    "section": "Some definitions on the “grammar of shapes” in nomnoml\n",
    "text": "Some definitions on the “grammar of shapes” in nomnoml\n\n\nAssociation Types: Connectors between blocks( i.e. Classifiers)\nClassifier Types: Kinds of blocks.\nDirective Types: Directives change the nature of the diagram rendered, by affective parameters like colour, direction and margins. ( Ha! VC people!!)\n\nCSS colours https://www.w3schools.com/cssref/css_colors.asp Only these colours are permitted, so use either the names or these specific colour hash codes. Any general hash code will not render.\n\nCode//association-1\n[a] - [b] \n\n//association-2\n[b] -&gt; [c] \n\n//association_3\n[c] &lt;-&gt; [a]\n\n//dependency-1\n[a] &lt;--&gt;[d]\n\n//dependency-2\n#.ell: visual=ellipse fill=#fbfb09 bold\n#.arvind: visual=rhomb fill=#ff2234 bold\n[&lt;ell&gt;e]--&gt;[a]\n//generalization-1\n[c]-:&gt;[&lt;arvind&gt;k]\n\n//implementation --:&gt;\n[k]--:&gt;[d]\n\n\n\n\n\n\nCode//composition +-\n[a]+-[b]\n//composition +-&gt;\n[b]-+[c]\n//aggregation o-\n[c]o-&gt;[d]\n//aggregation o-&gt;\n[d]o-&gt;[a]\n//note --\n[d]--[everything happens;here]\n//hidden -/-\n[d]-/-[f]\n////////////////////////\n//weightless edge _&gt;\n//[k]_&gt;[d] //not working\n//weightless dashed__\n//[d]__[j] //not working\n\n\n\n\n\nClassifier Types\nThese are different kinds of blocks.\n\nCode[class]-&gt;[&lt;abstract&gt; abstract]\n[&lt;abstract&gt; abstract]-:&gt;[&lt;instance&gt; instance]\n[&lt;instance&gt; instance]-:&gt;[&lt;note&gt; note]\n[&lt;note&gt; note]--&gt;[&lt;reference&gt; reference]\n\n\n\n\n\n\nCode[&lt;package&gt; package|components]--&gt;[&lt;frame&gt; frame|]\n[&lt;database&gt; database]--&gt;[&lt;start&gt; start]\n[&lt;end&gt; end]-o&gt;[&lt;state&gt; state]\n\n\n\n\n\n\nCode[&lt;choice&gt; choice]---&gt;[&lt;sync&gt; sync]\n[&lt;input&gt; input]-&gt;[&lt;sender&gt; sender]\n[&lt;receiver&gt; receiver]o-[&lt;transceiver&gt; transceiver]\n\n\n\n\n\n\nCode#direction:down\n#background:lightgrey\n#fill: fuchsia; green; purple\n#fillArrows: false\n#font: Courier\n[class]-&gt;[&lt;abstract&gt; abstract]\n[&lt;abstract&gt; abstract]-:&gt;[&lt;instance&gt; instance]\n[&lt;instance&gt; instance]-:&gt;[&lt;note&gt; note]\n[&lt;note&gt; note]--&gt;[&lt;reference&gt; reference]\n\n\n\n\n\n\nCode#font: CenturySchoolbook\n#fill: lightyellow\n#stroke: green\n\n[&lt;actor&gt; actor]---[&lt;usecase&gt; usecase]\n[&lt;usecase&gt; usecase]&lt;--&gt;[&lt;label&gt; label]\n[&lt;usecase&gt; usecase]-/-[&lt;hidden&gt; hidden]\n\n\n\n\n\n\nCode[&lt;table&gt; table| a | 5 || b | 7]\n\n\n\n\n\n\nCode[&lt;table&gt; table| c | 9 ]"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#directives",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#directives",
    "title": "The Grammar of Diagrams",
    "section": "Directives",
    "text": "Directives\nDirectives change the nature of the diagram rendered, by affective parameters like colour, direction and margins."
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#custom-classifier-styles",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#custom-classifier-styles",
    "title": "The Grammar of Diagrams",
    "section": "Custom classifier styles",
    "text": "Custom classifier styles\nA directive that starts with “.” define a classifier’s style. The style is written as a space separated list of modifiers and key/value pairs.\n\nCode#.box: fill=#8f8 dashed\n#.blob: visual=ellipse title=bold\n#.arvind: visual=rhomb title=bold dashed fill=CornFlowerBlue\n[&lt;box&gt; GreenBox]\n[&lt;blob&gt; Blobby]\n[&lt;arvind&gt; Someone]"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#nomnoml-keyvalue-pairs",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#nomnoml-keyvalue-pairs",
    "title": "The Grammar of Diagrams",
    "section": "\nnomnoml Key/value pairs",
    "text": "nomnoml Key/value pairs\n\nfill=(any css color)\nstroke=(any css color)\nalign=center align=left\ndirection=right direction=down\nvisual=actor\nvisual=class\nvisual=database\nvisual=ellipse\nvisual=end\nvisual=frame\nvisual=hidden\nvisual=input\nvisual=none\nvisual=note\nvisual=package\nvisual=receiver\nvisual=rhomb\nvisual=roundrect\nvisual=sender\nvisual=start\nvisual=sync\nvisual=table\nvisual=transceiver"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#text-modifiers",
    "href": "content/courses/R4Artists/Modules/110-GoD/08-Grammar-of-Diagrams.html#text-modifiers",
    "title": "The Grammar of Diagrams",
    "section": "Text modifiers",
    "text": "Text modifiers\nbold center italic left underline\n\nCode# .box: fill=#8f8 dashed\n# .blob: visual=rhomb title=bold fill=#8f8 dashed\n\n[A]-[B]\n[B]--[&lt;usecase&gt;C]\n[C]-[&lt;box&gt; D]\n[B]--[&lt;blob&gt; Jabba;TheHut]\n\n\n\n\n\n\nCode[a] -&gt;[b]\n[b] -:&gt; [c]\n[c]o-&gt;[d]\n[d]-/-[e]\n\n\n\n\n\n\nCode#fill: lightgreen; lightblue; lightyellow; grey; white\n\n[&lt;table&gt; table | c | 9 ]\n\n[R | [&lt;table&gt; Packages |\n         Base R |\n         [ &lt;table&gt; tidyverse| ggplot | tidyr | readr |\n             [&lt;table&gt; dplyr|\n                 magrittr | Others]]]]\n\n\n\n\n\n\nCode#fill: lightgreen; lightblue; lightyellow; pink; white\n\n[RStudio | [R | [&lt;table&gt; Packages |\n                   Base R | [ tidyverse |\n                               ggplot | tidyr | readr |\n                               [dplyr]--[magrittr]\n                               [dplyr]--[Others]\n                             | tibble\n                             ]\n                 | lubridate | DiagrammeR | Lattice]]]\n\n\n\n\n\n\nCode[Linux]+-[Ubuntu]\n[Linux]+-[Mint]\n[Ubuntu]--[Mint]\n[Linux]+-[Rosa Linux]\n[Linux]+-[Mx Linux]\n[Debian]-+[Linux]\n\n\n[Fedora]-+[Linux]\n[Puppy Linux]-+[Linux]\n[Personal Pups]-+[Puppy Linux]"
  },
  {
    "objectID": "content/courses/R4Artists/Modules/40-working-in-r/index.html",
    "href": "content/courses/R4Artists/Modules/40-working-in-r/index.html",
    "title": "Lab-4: I say what I mean and I mean what I say",
    "section": "",
    "text": "R Tutorial\n Slides",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-4: I say what I mean and I mean what I say"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/40-working-in-r/index.html#slides-and-tutorials",
    "href": "content/courses/R4Artists/Modules/40-working-in-r/index.html#slides-and-tutorials",
    "title": "Lab-4: I say what I mean and I mean what I say",
    "section": "",
    "text": "R Tutorial\n Slides",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-4: I say what I mean and I mean what I say"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/40-working-in-r/index.html#introduction",
    "href": "content/courses/R4Artists/Modules/40-working-in-r/index.html#introduction",
    "title": "Lab-4: I say what I mean and I mean what I say",
    "section": " Introduction",
    "text": "Introduction\nWe will get acquainted with data and its representations in R! We will also form a view of how English Grammar ( pronouns, verbs, adjectives, figures of speech..) get metaphorized into the R World!!",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-4: I say what I mean and I mean what I say"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/40-working-in-r/index.html#readings",
    "href": "content/courses/R4Artists/Modules/40-working-in-r/index.html#readings",
    "title": "Lab-4: I say what I mean and I mean what I say",
    "section": "Readings",
    "text": "Readings\n\nR for Data Science, Workflow: Basics Chapter: http://r4ds.had.co.nz/workflow-basics.html\nModern Dive, Getting Started Chapter: http://moderndive.com/2-getting-started.html\nR & RStudio Basics: https://bookdown.org/chesterismay/rbasics/3-rstudiobasics.html\nRStudio IDE Cheatsheet: https://github.com/rstudio/cheatsheets/blob/master/rstudio-ide.pdf",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-4: I say what I mean and I mean what I say"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/index.html#introduction",
    "href": "content/courses/R4Artists/Modules/70-wizardy/index.html#introduction",
    "title": "Lab-7: The Lobster Quadrille",
    "section": "Introduction",
    "text": "Introduction\nWe will add icing and froth to our vanilla ggplots: fonts, annotations, highlights and even pictures!!",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-7: The Lobster Quadrille"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/index.html#goals",
    "href": "content/courses/R4Artists/Modules/70-wizardy/index.html#goals",
    "title": "Lab-7: The Lobster Quadrille",
    "section": "Goals",
    "text": "Goals\n\nAppreciate that a publication-worth graphic takes a lot of work!!\nAdding annotations, pictures and references to graphs is necessary for good understanding\nJudicious use of colour and scales can enhance comprehension.",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-7: The Lobster Quadrille"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/index.html#pedagogical-note",
    "href": "content/courses/R4Artists/Modules/70-wizardy/index.html#pedagogical-note",
    "title": "Lab-7: The Lobster Quadrille",
    "section": "Pedagogical Note",
    "text": "Pedagogical Note\nThe method followed will be based on PRIMM:\n\n\nPREDICT Inspect the code and guess at what the code might do, write predictions\n\n\nRUN the code provided and check what happens\n\nINFER what the parameters of the code do and write comments to explain. What bells and whistles can you see?\n\nMODIFY the parameters code provided to understand the options available. Write comments to show what you have aimed for and achieved.\n\nMAKE : take an idea/concept of your own, and graph it.\n\nLet’s load up a few packages that we need to start:\n\n## packages\nlibrary(tidyverse)   ## data science package collection (incl. the ggplot2 package)\nlibrary(systemfonts) ## use custom fonts (need to be installed on your OS)  \nlibrary(paletteer)   ## scico  and many other colour palettes palettes(http://www.fabiocrameri.ch/colourmaps.php) in R \nlibrary(ggtext)      ## add improved text rendering to ggplot2\nlibrary(ggforce)     ## add missing functionality to ggplot2\nlibrary(concaveman)  ## Needed by ggforce for plot annotation hulls\nlibrary(ggdist)      ## add uncertainty visualizations to ggplot2\nlibrary(ggformula)   ## Formula interface to ggplot\nlibrary(magick)      ## load images into R\nlibrary(patchwork)   ## combine outputs from ggplot2\nlibrary(palmerpenguins)\n\nlibrary(showtext)   ## add google fonts to plots\n\nknitr::opts_chunk$set(\n  error = TRUE,\n  comment = NA,\n  warning = FALSE,\n  errors = FALSE,\n  message = FALSE,\n  tidy = FALSE,\n  cache = FALSE,\n  echo = TRUE,\n  warning = FALSE,\n# from the vignette for the showtext package\n  fig.showtext = TRUE,\n  fig.retina = 1,\n  fig.path = \"figs/\"\n  # fig.height = 3.09,\n  # fig.width = 5\n)\n\nUsing Google Fonts\nWe will want to add a few new fonts to our graphs. The best way (currently) is to use the showtext package (which we loaded above) to bring into our work fonts from Google. To view and select the fonts you might want to work with, spend some time looking over:\n\nGoogle Webfonts Helper App\nGoogle Fonts\n\n\nlibrary(sysfonts)\nlibrary(showtext)\n\nsysfonts::font_add_google(\"Roboto\", \"roboto\")\nfont_add_google(\"Noto Sans\", \"noto\")\nfont_add_google(\"Open Sans\", \"open\")\nfont_add_google(\"Anton\", \"anton\")\nfont_add_google(\"Tangerine\", \"tangerine\")\n\n # set the google fonts as default\nshowtext::showtext_auto(enable = TRUE)\n\nWe will work with a familiar dataset, so that we can concentrate on the chart aesthetics, without having to spend time getting used to the data: the penguins dataset again, from the palmerpenguins package.\n\n\n\n\n\n\nggformula and ggplot worlds do intersect!\n\n\n\nIt seems we can mix `ggformula` code with `ggtext` code, using the `+` sign!! What joy !!! Need to find out if this works for other `ggplot` extensions as well !!!",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-7: The Lobster Quadrille"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/index.html#data",
    "href": "content/courses/R4Artists/Modules/70-wizardy/index.html#data",
    "title": "Lab-7: The Lobster Quadrille",
    "section": "Data",
    "text": "Data\nAlways start your work with a table of the data:\n\npenguins &lt;- penguins %&gt;% drop_na() \n# remove data containing missing data\npenguins",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-7: The Lobster Quadrille"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/index.html#basic-plot",
    "href": "content/courses/R4Artists/Modules/70-wizardy/index.html#basic-plot",
    "title": "Lab-7: The Lobster Quadrille",
    "section": "Basic Plot",
    "text": "Basic Plot\nA basic scatter plot, which we will progressively dress up.\n\n\nUsing ggformula\nUsing ggplot\n\n\n\n\n## simple plot: data + mappings + geometry\n## no colour or fill yet\ngf &lt;-  gf_point(bill_depth_mm ~ bill_length_mm, \n                 data = penguins, \n                 alpha = 0.6, size = 3.5)\ngf\n\n\n\n\n\n\n\n\n\n\n## simple plot: data + mappings + geometry\n## no colour or fill yet\ngg &lt;- ggplot(penguins, aes(x = bill_length_mm,\n                           y = bill_depth_mm)) +\n  geom_point(alpha = .6, size = 3.5)\ngg",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-7: The Lobster Quadrille"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/index.html#customized-plot",
    "href": "content/courses/R4Artists/Modules/70-wizardy/index.html#customized-plot",
    "title": "Lab-7: The Lobster Quadrille",
    "section": "Customized Plot",
    "text": "Customized Plot\nLet us set some ggplot theme aspects now!! Here is a handy picture showing (most of) the theme-able aspects of a ggplot plot.\n\n\nRosana Ferrero (@RosanaFerrero) on Twitter Sept 11, 2022\n\nFor more info, type ?theme in your console.\n\n## change global theme settings (for all following plots)\nmy_theme &lt;- theme_set(theme_classic(base_size = 12, \n                        base_family = \"roboto\"\n                        )) +  \n\n## modify plot elements globally (for all following plots)\ntheme_update(\n  axis.ticks = element_line(color = \"grey92\"),\n  axis.ticks.length = unit(.5, \"lines\"),\n  panel.grid.minor = element_blank(),\n  legend.title = element_text(size = 12),\n  legend.text = element_text(color = \"grey30\"),\n  plot.title = element_text(size = 18, face = \"bold\"),\n  plot.subtitle = element_text(size = 12, color = \"grey30\"),\n  plot.caption = element_text(size = 9, margin = margin(t = 15))\n)\n\nSince we know what the basic plot looks like, let’s add titles, labels and colours. We will also set limits and scales.\n\n\nUsing ggformula\nUsing ggplot\n\n\n\n\ntheme_set(my_theme)\n\ngf1 &lt;- penguins %&gt;% \n  gf_point(bill_depth_mm ~ bill_length_mm, \n           \n           # colour by continuous variable\n           color =  ~ body_mass_g, \n           alpha = .6, size = 3.5) %&gt;% \n\n  \n  ## custom axes scaling\n  gf_refine(\n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)),\n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5), \n                     limits = c(12.5, 22.5)), \n  \n  ## custom colors from the scico package\n  ## using the paletteer super package\n  paletteer:: scale_color_paletteer_c(`\"scico::bamako\"`, \n                                      direction = -1),\n  \n  ## custom labels\n  labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (Pygoscelis)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) PLoS ONE',\n    x = 'Bill Length (mm)', \n    y = 'Bill Depth (mm)',\n## See this!\n    color = 'Body mass (g)'\n  ))\n\ngf1\n\n\n\n\n\n\n\nNote this neat way of naming a scale and the legend in the labs command above!\n\n\n\ntheme_set(my_theme)\n\n\ngg1 &lt;- penguins %&gt;% \n  ggplot(aes(y = bill_depth_mm, x = bill_length_mm), \n         alpha = .6) +\n  geom_point(aes(colour = body_mass_g), size = 3.5) + \n\n  \n  ## custom axes scaling\n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) + \n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5), \n                     limits = c(12.5, 22.5)) + \n  \n  ## custom colors from the scico package\n  paletteer:: scale_color_paletteer_c(`\"scico::bamako\"`, \n                                      direction = -1) + \n  \n  ## custom labels\n  labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (Pygoscelis)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) PLoS ONE',\n    x = 'Bill Length (mm)', \n    y = 'Bill Depth (mm)',\n    color = 'Body mass (g)' \n  )\ngg1",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-7: The Lobster Quadrille"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/index.html#using-ggtext",
    "href": "content/courses/R4Artists/Modules/70-wizardy/index.html#using-ggtext",
    "title": "Lab-7: The Lobster Quadrille",
    "section": "Using {ggtext}",
    "text": "Using {ggtext}\nFrom Claus Wilke’s website → www.wilkelab.org/ggtext\n\nThe ggtext package provides simple Markdown and HTML rendering for ggplot2. Under the hood, the package uses the gridtext package for the actual rendering, and consequently it is limited to the feature set provided by gridtext.\nSupport is provided for Markdown both in theme elements (plot titles, subtitles, captions, axis labels, legends, etc.) and in geoms (similar to geom_text()). In both cases, there are two alternatives, one for creating simple text labels and one for creating text boxes with word wrapping.\n\n\n\n\n\n\n\nWorking with ggtext\n\n\n\nNOTE: on some machines, the ggtext package may not work as expected. In this case, please do as follows, using your Console:\n\nremove gridtext: remove.packages(gridtext).\nInstall development version of gridtext: install.packages(remotes) remotes::install_github(\"wilkelab/gridtext\")\n\n\n\n\nUsing element_markdown()\nWe can use our familiar markdown syntax right inside the titles and captions of the plot. element_markdown() is a theme-ing command made available by the ggtext package.\nelement_markdown() → formatted text elements, e.g. titles, caption, axis text, striptext.\n\n\nUsing ggformula\nUsing ggplot\n\n\n\n\ntheme_set(my_theme)\n\ngf2 &lt;- penguins %&gt;% gf_point(bill_depth_mm ~ bill_length_mm, \n                            color = ~ body_mass_g, \n                            alpha = 0.6, size = 3.5) %&gt;% \n gf_refine(\n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)),\n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5), \n                     limits = c(12.5, 22.5)), \n  \n  ## custom colors from the scico package\n  paletteer:: scale_color_paletteer_c(\"scico::bamako\", \n                                      direction = -1),\n  \n  ## custom labels using element_markdown()\n   labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) *PLoS ONE*',\n    x = '**Bill Length** (mm)', \n    y = '**Bill Depth** (mm)',\n    color = 'Body mass (g)')\n  ) %&gt;% \n  \n  # New code from here\n  # Enables markdown titles, captions and labels\n  gf_theme(theme(\n    plot.title = ggtext::element_markdown(),\n    plot.caption = ggtext::element_markdown(),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown()\n  ))\n\n gf2\n\n\n\n\n\n\n\n\n\n\ntheme_set(my_theme)\n\ngg2 &lt;- ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5) + \n  \n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5), \n                     limits = c(12.5, 22.5)) +\n  paletteer:: scale_color_paletteer_c(`\"scico::bamako\"`, \n                                       direction = -1) +\n   \n  ## New code starts here: Two Step Procedure with ggtext\n  ## 1. Markdown formatting of labels and title, using asterisks\n  labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) *PLoS ONE*',\n    x = '**Bill Length** (mm)', \n    y = '**Bill Depth** (mm)',\n    color = 'Body mass (g)'\n  ) +\n   \n  ## 2. Add theme related commands from ggtext\n  ## render respective text elements\n  theme(\n    plot.title = ggtext::element_markdown(),\n    plot.caption = ggtext::element_markdown(),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown()\n  )\ngg2\n\n\n\n\n\n\n\n\n\n\n\nelement_markdown() in combination with HTML\nThis allows us to change fonts in titles, labels, and captions.\n\n\nUsing ggformula\nUsing ggplot\n\n\n\ntheme_set(my_theme)\n\n## use HTML syntax to change text color\n\ngf2 %&gt;% \n  \n  # html in labels\n  gf_labs(title = 'Bill Dimensions of Brush-Tailed Penguins\n          &lt;i style = \"color:#28A87D;\"&gt;Pygoscelis &lt;/i&gt;'\n            ) \n## use HTML syntax to change font and text size\ngf2 %&gt;% \n  gf_labs(title = 'Bill Dimensions of Brush-Tailed Penguins &lt;b style=\"font-size:32pt;font-family:tangerine;\"&gt;Pygoscelis&lt;/b&gt;') \n\n\n\n\n\n\n\n\n\n\n\n\ntheme_set(my_theme)\n\n## use HTML syntax to change text color\ngg2 +\n  labs(title = 'Bill Dimensions of Brush-Tailed Penguins &lt;i style=\"color:#28A87D;\"&gt;Pygoscelis&lt;/i&gt;') +\n  theme(plot.margin = margin(t = 15))\n## use HTML syntax to change font and text size\ngg2 +\n  labs(title = 'Bill Dimensions of Brush-Tailed Penguins &lt;b style=\"font-size:32pt;font-family:tangerine;\"&gt;Pygoscelis&lt;/b&gt;')\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdding images to ggplot\nSave an image from the web in the same folder as your RMarkdown. Use html tags to include it, say as part of your plot title, as shown below.\n\n\nUsing ggformula\nUsing ggplot\n\n\n\n\ntheme_set(my_theme)\n\n## use HTML syntax to add images to text elements\ngf2 %&gt;% \n  gf_labs(title = 'Bill Dimensions of Brush-Tailed Penguins &nbsp;&nbsp;&nbsp; &lt; img src=\"images/culmen_depth.png\"‚ width=\"480\"/&gt;') \n\n\n\n\n\n\n\n\n\n\ntheme_set(my_theme)\n\n## use HTML syntax to add images to text elements\ngg2 + \n  labs(title = 'Bill Dimensions of Brush-Tailed Penguins &nbsp;&nbsp;&nbsp; &lt;img src=\"images/culmen_depth.png\"‚ width=\"480\"/&gt;')\n\n\n\n\n\n\n\n\n\n\nAnnotations with geom_richtext() and geom_textbox()\n\nFurther ggplot annotations can be achieved using geom_richtext() and geom_textbox(). geom_richtext() also allows formatted text labels with 360° rotation. One needs to pass a tibble to geom_richtext() giving the location, colour, rotation etc of the label annotation.\n\n\nDesign the labels\nUsing ggformula\nUsing ggplot\n\n\n\n\n# Create a label tibble\n# Three rich text labels, \n# so three sets of locations x and y, and angle of rotation\n\nlabels &lt;- tibble(\n      x = c(34, 56, 54), \n      y = c(20, 18.5, 14.5),\n      angle = c(12, 20, 335),\n      species = c(\"Adelie\", \"Chinstrap\", \"Gentoo\"),\n      \n      lab = c(\"&lt;b style='font-family:anton;font-size:24pt;'&gt;Adélie&lt;/b&gt;&lt;br&gt;&lt;i style='color:darkgrey;'&gt;P. adéliae&lt;/i&gt;\", \n              \"&lt;b style='font-family:anton;font-size:24pt;'&gt;Chinstrap&lt;/b&gt;&lt;br&gt;&lt;i style='color:darkgrey;'&gt;P. antarctica&lt;/i&gt;\", \n              \"&lt;b style='font-family:anton;font-size:24pt;'&gt;Gentoo&lt;/b&gt;&lt;br&gt;&lt;i style='color:darkgrey;'&gt;P. papua&lt;/i&gt;\"))\nlabels\n\n\n  \n\n\n\n\n\n\ntheme_set(my_theme)\n\ngf_rich &lt;- penguins %&gt;% \n  gf_point(bill_depth_mm ~ bill_length_mm, \n                      color = ~ species, \n                      alpha = 0.6, size = 3.5, data = penguins) +\n  \n  \n    ## add text annotations for each species\n    ggtext::geom_richtext(\n    data = labels,\n    # Now pass the data variables as aesthetics\n    aes(x, y, label = lab, color = species, angle = angle), \n    size = 4, fill = NA, label.color = NA,\n    lineheight = .3) +\n    # show.legend = FALSE else we get some unusual legends!\n    # fill = NA makes the labels' fill transparent\n  \n  \n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5), \n                     limits = c(12.5, 22.5)) +\n  scale_colour_paletteer_d(palette = `\"rcartocolor::Bold\"`, \n                           guide = \"none\") +\n  \n  labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) *PLoS ONE*',\n    x = '**Bill Length** (mm)', \n    y = '**Bill Depth** (mm)',\n    color = 'Body mass (g)'\n  ) +\n  \n  # Use theme and element_markdown() to format axes and titles as usual\n  theme(\n    plot.title = ggtext::element_markdown(),\n    plot.caption = ggtext::element_markdown(),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown(),\n    plot.margin = margin(25, 6, 15, 6)\n  )\n\n\ngf_rich\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote the plus sign usage here!!We are combining the ggformula and ggplot syntax, and it works!\n\n\n\n\n\ntheme_set(my_theme)\n\n\ngg_rich &lt;- ggplot(penguins, aes(x = bill_length_mm, \n                                y = bill_depth_mm)) +\n  geom_point(aes(color = species), alpha = .6, size = 3.5) + \n  \n  ## add text annotations for each species\n  ggtext::geom_richtext(\n    data = labels,\n    # Now pass the data variables as aesthetics\n    aes(x, y, label = lab, color = species, angle = angle), \n    size = 4, fill = NA, label.color = NA,\n    lineheight = .3) +\n  \n  \n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5), \n                     limits = c(12.5, 22.5)) +\n  scale_colour_paletteer_d(`\"rcartocolor::Bold\"`, guide = \"none\")+\n  \n  labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) *PLoS ONE*',\n    x = '**Bill Length** (mm)', \n    y = '**Bill Depth** (mm)',\n    color = 'Body mass (g)'\n  ) +\n  \n  # Use theme and element_markdown() to format axes and titles as usual\n  theme(\n    plot.title = ggtext::element_markdown(),\n    plot.caption = ggtext::element_markdown(),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown(),\n    plot.margin = margin(25, 6, 15, 6)\n  )\n  \ngg_rich\n\n\n\n\n\n\n\n\n\n\nFormatted Text boxes on plots\nelement_textbox() and element_textbox_simple() → formatted text boxes with word wrapping.\n\n\nUsing ggformula\nUsing ggplot\n\n\n\n\ntheme_set(my_theme)\n\ngf_box &lt;- penguins %&gt;% \n  gf_point(bill_depth_mm ~ bill_length_mm, \n                      color = ~ species, \n                      alpha = 0.6, size = 3.5, data = penguins) +\n  \n  \n    ## add text annotations for each species\n    ggtext::geom_richtext(\n    data = labels,\n    # Now pass the data variables as aesthetics\n    aes(x, y, label = lab, color = species, angle = angle), \n    size = 4, fill = NA, label.color = NA,\n    lineheight = .3) +\n    # show.legend = FALSE else we get some unusual legends!\n    # fill = NA makes the labels' fill transparent\n  \n  \n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5), \n                     limits = c(12.5, 22.5)) +\n  scale_colour_paletteer_d(palette = `\"rcartocolor::Bold\"`, \n                           guide = \"none\") +\n  \n  \n# Now for the Plot Titles and Labels, as before  \n  labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) *PLoS ONE*',\n    x = '**Bill Length** (mm)',\n    y = '**Bill Depth** (mm)',\n    color = 'Body mass (g)'\n  ) +\n  \n# Add the ggtext theme related commands\n  theme(\n    ## turn title into filled textbox\n    plot.title = ggtext::element_textbox_simple(\n      color = \"white\",\n      fill = \"#28A78D\",\n      size = 28,\n      padding = margin(8, 4, 8, 4),\n      margin = margin(b = 5),\n      lineheight = .9\n    ),\n    plot.subtitle = ggtext::element_textbox_simple(\n      size = 10,\n      padding = margin(5.5, 5.5, 5.5, 5.5),\n      margin = margin(0, 0, 5.5, 0),\n      fill = \"orange1\"\n    ),\n    \n    ## add round outline to caption\n    plot.caption = ggtext::element_textbox_simple(\n      width = NULL,\n      linetype = 1,\n      fill = \"grey\",\n      padding = margin(4, 8, 4, 8),\n      margin = margin(t = 15),\n      r = grid::unit(8, \"pt\")\n    ),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown(),\n    plot.margin = margin(25, 6, 15, 6)\n  )\n\ngf_box\n\n\n\n\n\n\n\n\n\n\ntheme_set(my_theme)\n\ngg_box &lt;- ggplot(penguins, \n                 aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(aes(color = species), \n             alpha = .6, size = 3.5) +\n  \n     ## add text annotations for each species\n  ggtext::geom_richtext(data = labels,\n    # Now pass the data variables as aesthetics\n    aes(x, y, label = lab, color = species, angle = angle), \n    size = 4, fill = NA, label.color = NA,\n    lineheight = .3) +\n    # show.legend = FALSE else we get some unusual legends!\n    # fill = NA makes the labels' fill transparent\n  \n  \n  scale_x_continuous(breaks = 3:6 * 10, limits = c(30, 60)) +\n  scale_y_continuous(breaks = seq(12.5, 22.5, by = 2.5), \n                     limits = c(12.5, 22.5)) +\n  scale_colour_paletteer_d(palette = `\"rcartocolor::Bold\"`, \n                           guide = \"none\") +\n  \n  \n# Now for the Plot Titles and Labels, as before  \n  labs(\n    title = 'Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)',\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = 'Data: Gorman, Williams & Fraser (2014) *PLoS ONE*',\n    x = '**Bill Length** (mm)',\n    y = '**Bill Depth** (mm)',\n    color = 'Body mass (g)'\n  ) +\n  \n# Add the ggtext theme related commands\n  theme(\n    ## turn title into filled textbox\n    plot.title = ggtext::element_textbox_simple(\n      color = \"white\",\n      fill = \"#28A78D\",\n      size = 28,\n      padding = margin(8, 4, 8, 4),\n      margin = margin(b = 5),\n      lineheight = .9\n    ),\n    plot.subtitle = ggtext::element_textbox_simple(\n      size = 10,\n      padding = margin(5.5, 5.5, 5.5, 5.5),\n      margin = margin(0, 0, 5.5, 0),\n      fill = \"orange1\"\n    ),\n    \n    ## add round outline to caption\n    plot.caption = ggtext::element_textbox_simple(\n      width = NULL,\n      linetype = 1,\n      fill = \"grey\",\n      padding = margin(4, 8, 4, 8),\n      margin = margin(t = 15),\n      r = grid::unit(8, \"pt\")\n    ),\n    axis.title.x = ggtext::element_markdown(),\n    axis.title.y = ggtext::element_markdown(),\n    plot.margin = margin(25, 6, 15, 6)\n  )\n\ngg_box\n\n\n\n\n\n\n\n\n\n\nUsing geom_texbox() for formatted text boxes with word wrapping\n\n\nUsing ggformula\nUsing ggplot\n\n\n\n\ntheme_set(my_theme)\n\ntext_box &lt;- tibble(x = 34, y = 13.7, label = \"&lt;span style='font-size:12pt;font-family:anton;'&gt;Lorem Ipsum Dolor Sit Amet&lt;/span&gt;&lt;br&gt;&lt;br&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\")\n\n\ngf_box +\n  ## add textbox with long paragraphs\n  ggtext::geom_textbox(\n    data = text_box,\n    aes(x, y, \n        label = label),\n    size = 2.2, family = \"sans\",\n    fill = \"cornsilk\", \n    \n    colour = \"black\",\n# This is ESSENTIAL !!!\n# It appears that the original colour aesthetic mapping in `gf_box` and a possible colour aesthetic with `geom_textbox` have a clash, *only* with ggformula. No such issues below with the ggplot.\n# So declaring a colour here is essential\n\n    box.color = \"cornsilk3\",\n    #box.padding = c(2,2,2,2),\n    width = unit(11, \"lines\")\n  ) +\n  coord_cartesian(clip = \"off\") # ensure no clipping of labels near the edge\n\n\n\n\n\n\n\n\n\n\ntheme_set(my_theme)\n\ngg_box +\n  ## add textbox with long paragraphs\n  ggtext::geom_textbox(\n    data = text_box,\n    aes(x, y, label = label),\n    size = 2.2, family = \"sans\",\n    fill = \"cornsilk\", box.color = \"cornsilk3\",\n    width = unit(11, \"lines\")\n  ) +\n  coord_cartesian(clip = \"off\") # ensure no clipping of labels near the edge\n\n\n\n\n\n\n\n\n\n\nUsing {ggforce}\n\nFrom Thomas Lin Pedersen’s website → www.ggforce.data-imaginist.com\n\nggforce is a package aimed at providing missing functionality to ggplot2 through the extension system introduced with ggplot2 v2.0.0. Broadly speaking ggplot2 has been aimed primarily at explorative data visualization in order to investigate the data at hand, and less at providing utilities for composing custom plots a la D3.js. ggforce is mainly an attempt to address these “shortcomings” (design choices might be a better description). The goal is to provide a repository of geoms, stats, etc. that are as well documented and implemented as the official ones found in ggplot2.\n\nWe will start with the basic plot, with the ggtext related work done up to now:\n\n## use ggtext rendering for the following plots\ntheme_set(my_theme)\ntheme_update(\n  plot.title = ggtext::element_markdown(),\n  plot.caption = ggtext::element_markdown(),\n  axis.title.x = ggtext::element_markdown(),\n  axis.title.y = ggtext::element_markdown()\n)\n\n\n\nUsing ggformula and ggforce\nUsing ggplot and ggforce\n\n\n\n\ntheme_set(my_theme)\ntheme_update(\n  plot.title = ggtext::element_markdown(),\n  plot.caption = ggtext::element_markdown(),\n  axis.title.x = ggtext::element_markdown(),\n  axis.title.y = ggtext::element_markdown()\n)\n  \n## plot that we will annotate with ggforce afterwards\ngf3 &lt;- penguins %&gt;% \n  gf_point(bill_depth_mm ~ bill_length_mm,\n           color = ~ body_mass_g,\n           alpha = .6, \n           size = 3.5) + \n  \n  coord_cartesian(xlim = c(25, 65), ylim = c(10, 25)) +\n  \n  # Add Colour scales\n  scale_color_paletteer_c(`\"grDevices::Lajolla\"`, \n                          direction = -1) +\n  \n  # Add labels\n  labs(\n    title = \"Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)\",\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE*\",\n    x = \"**Bill Length** (mm)\",\n    y = \"**Bill Depth** (mm)\",\n    color = \"Body mass (g)\",\n    fill = \"Species\"\n  )\n\n\n\n## ellipsoids for all groups\ngf3 +\n  ggforce::geom_mark_ellipse(\n    aes(fill = species, \n        label = species), \n    \n    color = \"black\", \n    # This is good to include\n    # Else ellipses get coloured too\n    \n    alpha = .15, \n    show.legend = FALSE\n  ) \n\n\n\n\n\n\n\n\n\n\ntheme_set(my_theme)\ntheme_update(\n  plot.title = ggtext::element_markdown(),\n  plot.caption = ggtext::element_markdown(),\n  axis.title.x = ggtext::element_markdown(),\n  axis.title.y = ggtext::element_markdown()\n)\n\n## plot that we will annotate with ggforce afterwards\ngg3 &lt;- ggplot(penguins, \n              aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(aes(color = body_mass_g), \n             alpha = .6, \n             size = 3.5) + \n\n  coord_cartesian(xlim = c(25, 65), ylim = c(10, 25)) +\n  \n  # Add Colour scales\n  scale_color_paletteer_c(`\"grDevices::Lajolla\"`, \n                          direction = -1) +\n\n  \n  # Add labels\n  labs(\n    title = \"Bill Dimensions of Brush-Tailed Penguins (*Pygoscelis*)\",\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE*\",\n    x = \"**Bill Length** (mm)\",\n    y = \"**Bill Depth** (mm)\",\n    color = \"Body mass (g)\",\n    fill = \"Species\"\n  )\n\n\n## ellipsoids for all groups\ngg3 +\n  ggforce::geom_mark_ellipse(\n    aes(fill = species, \n        label = species), \n    alpha = .15, \n    show.legend = FALSE\n  ) \n\n\n\n\n\n\n\n\ntheme_set(my_theme)\ntheme_update(\n  plot.title = ggtext::element_markdown(),\n  plot.caption = ggtext::element_markdown(),\n  axis.title.x = ggtext::element_markdown(),\n  axis.title.y = ggtext::element_markdown()\n)\n\n## ellipsoids for specific subset\ngg3 +\n  ggforce::geom_mark_ellipse(\n    aes(fill = species, label = species, \n        filter = species == 'Gentoo'), \n    alpha = 0, show.legend = FALSE\n  ) +\n geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5)\n\n\n\n\n\n\n\n\ntheme_set(my_theme)\ntheme_update(\n  plot.title = ggtext::element_markdown(),\n  plot.caption = ggtext::element_markdown(),\n  axis.title.x = ggtext::element_markdown(),\n  axis.title.y = ggtext::element_markdown()\n)\n\n## circles\ngg3 +\n  ggforce::geom_mark_circle(\n    aes(fill = species, label = species, \n        filter = species == 'Gentoo'), \n    alpha = 0, show.legend = FALSE\n  ) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5)\n\n\n\n\n\n\n\n\ntheme_set(my_theme)\ntheme_update(\n  plot.title = ggtext::element_markdown(),\n  plot.caption = ggtext::element_markdown(),\n  axis.title.x = ggtext::element_markdown(),\n  axis.title.y = ggtext::element_markdown()\n)\n\n## rectangles\ngg3 +\n  ggforce::geom_mark_rect(\n    aes(fill = species, label = species, \n        filter = species == 'Gentoo'), \n    alpha = 0, show.legend = FALSE\n  ) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5) \n\n\n\n\n\n\n\n\ntheme_set(my_theme)\ntheme_update(\n  plot.title = ggtext::element_markdown(),\n  plot.caption = ggtext::element_markdown(),\n  axis.title.x = ggtext::element_markdown(),\n  axis.title.y = ggtext::element_markdown()\n)\n\n## hull\ngg3 +\n  ggforce::geom_mark_hull(\n    aes(fill = species, label = species, \n        filter = species == 'Gentoo'), \n    alpha = 0, show.legend = FALSE\n  ) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5)\n\n\n\n\n\n\n\n\n\n\nggplot tricks\n\ntheme_set(my_theme)\ntheme_update(\n  plot.title = ggtext::element_markdown(),\n  plot.caption = ggtext::element_markdown(),\n  axis.title.x = ggtext::element_markdown(),\n  axis.title.y = ggtext::element_markdown(),\n  legend.title = ggtext::element_markdown()\n)\n\ngg0 &lt;-\n  ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  ggforce::geom_mark_ellipse(aes(fill = species,\n                                 label = species),\n                             alpha = .15,\n                             show.legend = FALSE) +\n  geom_point(aes(color = body_mass_g), alpha = .6, size = 3.5) +\n  \n  scale_x_continuous(breaks = seq(25, 65, by = 5), \n                     limits = c(25, 65)) +\n  scale_y_continuous(breaks = seq(12, 24, by = 2), \n                     limits = c(12, 24)) +\n  \n  scico::scale_color_scico(palette = \"bamako\", direction = -1) +\n  labs(\n    title = \"**Bill Dimensions of Brush-Tailed Penguins** (*Pygoscelis*)\",\n    subtitle = 'A scatter plot of bill depth versus bill length.',\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE*\",\n    x = \"**Bill Length** (mm)\",\n    y = \"**Bill Depth** (mm)\",\n    color = \"**Body mass** (g)\"\n  )\ngg0\n\n\n\n\n\n\n\nLeft-Aligned Title\n\ntheme_set(my_theme)\ntheme_update(\n  plot.title = ggtext::element_markdown(),\n  plot.caption = ggtext::element_markdown(),\n  axis.title.x = ggtext::element_markdown(),\n  axis.title.y = ggtext::element_markdown(),\n  legend.title = ggtext::element_markdown()\n)\n\n(gg1 &lt;- gg0 + theme(plot.title.position = \"plot\"))\n\n\n\n\n\n\n\nRight-Aligned Caption\n\ntheme_set(my_theme)\ntheme_update(\n  plot.title = ggtext::element_markdown(),\n  plot.caption = ggtext::element_markdown(),\n  axis.title.x = ggtext::element_markdown(),\n  axis.title.y = ggtext::element_markdown(),\n  legend.title = ggtext::element_markdown()\n)\n\ngg1b &lt;- gg1 +  theme(plot.caption.position = \"plot\")\ngg1b\n\n\n\n\n\n\n\nLegend Design\ntheme_set(my_theme)\ntheme_update(\n  plot.title = ggtext::element_markdown(),\n  plot.caption = ggtext::element_markdown(),\n  axis.title.x = ggtext::element_markdown(),\n  axis.title.y = ggtext::element_markdown(),\n  legend.title = ggtext::element_markdown()\n)\n\ngg1b + theme(legend.position = \"top\")\n#ggsave(\"06a_legend_position.pdf\", width = 9, height = 8, device = cairo_pdf)\n\ngg1b + \n  theme(legend.position = \"top\") +\n  guides(color = guide_colorbar(title.position = \"top\", \n                                title.hjust = .5, \n                                barwidth = unit(20, \"lines\"), \n                                barheight = unit(.5, \"lines\")))\n\n\n\n\n\n\nError in `setup_elements()`:\n! Can't merge the `legend.title` theme element.\nCaused by error in `merge_element()`:\n! Only elements of the same class can be merged.\n\n\n\n\n\n\n\n\nAdd Images\n\ntheme_set(my_theme)\ntheme_update(\n  plot.title = ggtext::element_markdown(),\n  plot.caption = ggtext::element_markdown(),\n  axis.title.x = ggtext::element_markdown(),\n  axis.title.y = ggtext::element_markdown(),\n  legend.title = ggtext::element_markdown()\n)\n\n## read PNG file from web\npng &lt;- magick::image_read(\"images/culmen_depth.png\")\n\n## turn image into `rasterGrob`\nimg &lt;- grid::rasterGrob(png, interpolate = TRUE)\n\ngg5 &lt;- \n  gg1 + \n  annotation_custom(img, ymin = 18, ymax = 28, \n                    xmin = 58, xmax = 65) +\n  labs(caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE* &bull; Illustration: Allison Horst\")   +\n  coord_cartesian(clip = \"off\") # ensure no clipping of labels near the edge\ngg5\n\n\n\n\n\n\n\nUsing {patchwork}\n\n\nThe goal of patchwork is to make it ridiculously simple to combine separate ggplots into the same graphic. As such it tries to solve the same problem as gridExtra::grid.arrange() and cowplot::plot_grid but using an API that incites exploration and iteration, and scales to arbitrarily complex layouts.\n\n→ https://patchwork.data-imaginist.com/\nLet us make two plots and combine them into a single patchwork plot.\n\ntheme_set(my_theme)\ntheme_update(\n  plot.title = ggtext::element_markdown(),\n  plot.caption = ggtext::element_markdown(),\n  axis.title.x = ggtext::element_markdown(),\n  axis.title.y = ggtext::element_markdown(),\n  legend.title = ggtext::element_markdown()\n)\n\n## calculate bill ratio\npenguins_stats &lt;- penguins %&gt;%\n  mutate(bill_ratio = bill_length_mm / bill_depth_mm) %&gt;%\n  filter(!is.na(bill_ratio))\n\n## create a second chart\ngg6 &lt;-\n  ggplot(penguins_stats,\n         aes(\n           y = bill_ratio,\n           x = species,\n           fill = species,\n           color = species\n         )) + geom_violin() +\n  labs(\n    y = \"Bill ratio\",\n    x = \"Species\",\n    subtitle = \"\",\n    caption = \"Data: Gorman, Williams & Fraser (2014) *PLoS ONE* &bull; Illustration: Allison Horst\"\n  ) +\n  theme(\n    panel.grid.major.x = element_line(linewidth = .35),\n    panel.grid.major.y = element_blank(),\n    axis.text.y = element_text(size = 13),\n    axis.ticks.length = unit(0, \"lines\"),\n    plot.title.position = 'plot',\n    plot.subtitle = element_text(margin = margin(t = 5, b = 10)),\n    plot.margin = margin(10, 25, 10, 25)\n  )\n\nNow to combine both plots into one using simple operators:\n\nFor the special case of putting plots besides each other or on top of each other patchwork provides 2 shortcut operators. | will place plots next to each other while / will place them on top of each other.\n\nFirst we stack up the graphs side by side:\n\ntheme_set(my_theme)\ntheme_update(\n  plot.title = ggtext::element_markdown(),\n  plot.caption = ggtext::element_markdown(),\n  axis.title.x = ggtext::element_markdown(),\n  axis.title.y = ggtext::element_markdown(),\n  legend.title = ggtext::element_markdown()\n)\n\n## combine both plots\ngg5 | (gg6 + labs(title = \"Bill Ratios of Brush-Tailed Penguins\",\n                  subtitle = \"Violin Plots of Bill Ration versus species\"))\n\n\n\n\n\n\n\nWe can place them in one column:\n\ntheme_set(my_theme)\ntheme_update(\n  plot.title = ggtext::element_markdown(),\n  plot.caption = ggtext::element_markdown(),\n  axis.title.x = ggtext::element_markdown(),\n  axis.title.y = ggtext::element_markdown(),\n  legend.title = ggtext::element_markdown()\n)\n\ngg5 / (gg6 + labs(title = \"Bill Ratios of Brush-Tailed Penguins\",\n                  subtitle = \"Violin Plots of Bill Ration versus species\")) +\n  plot_layout(heights = c(0.4, 0.4))",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-7: The Lobster Quadrille"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/70-wizardy/index.html#references",
    "href": "content/courses/R4Artists/Modules/70-wizardy/index.html#references",
    "title": "Lab-7: The Lobster Quadrille",
    "section": "References",
    "text": "References\n\n\n1. Thomas Lin Pedersen, https://www.data-imaginist.com/. The creator of ggforce, and patchwork packages.\n2. Claus Wilke, cowplot – Streamlined plot theme and plot annotations for ggplot2, https://wilkelab.org/cowplot/index.html\n3. Claus Wilke, Spruce up your ggplot2 visualizations with formatted text, https://clauswilke.com/talk/rstudio_conf_2020/. Slides, Code, and Video !\n4. Robert Kabacoff, ggplot theme cheatsheet, https://rkabacoff.github.io/datavis/modifyingthemes.pdf\n5. Zuguang Gu, Circular Visualization in R, https://jokergoo.github.io/circlize_book/book/\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggdist\n3.3.2\n\n@ggdist2024a; @ggdist2024b\n\n\n\nggforce\n0.4.2\n@ggforce\n\n\nggtext\n0.1.2\n@ggtext\n\n\ngrid\n4.4.0\n@grid\n\n\nmagick\n2.8.3\n@magick\n\n\npaletteer\n1.6.0\n@paletteer\n\n\npatchwork\n1.2.0\n@patchwork\n\n\nscico\n1.5.0\n@scico\n\n\nshowtext\n0.9.7\n@showtext\n\n\nsysfonts\n0.8.9\n@sysfonts\n\n\nsystemfonts\n1.1.0\n@systemfonts",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-7: The Lobster Quadrille"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/200-wrap/index.html#introduction",
    "href": "content/courses/R4Artists/Modules/200-wrap/index.html#introduction",
    "title": "Lab-13: Old Tortoise Taught Us",
    "section": "Introduction",
    "text": "Introduction\nWe will spend a little time wrapping up everything that we have learnt so far, in R.\nWe will take one large dataset that has numerical, spatial and network type data, and see how we can make multiple visual depictions of it. We will use most of the packages that we have encountered so far and try to make a polished info visualization with all the graphs, text, and descriptions put together!",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-13: Old Tortoise Taught Us"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/200-wrap/index.html#references",
    "href": "content/courses/R4Artists/Modules/200-wrap/index.html#references",
    "title": "Lab-13: Old Tortoise Taught Us",
    "section": "References",
    "text": "References\n\nAlex Cookson, Great Data Sets github repo, https://github.com/tacookson/data",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-13: Old Tortoise Taught Us"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/300-website/index.html#introduction",
    "href": "content/courses/R4Artists/Modules/300-website/index.html#introduction",
    "title": "Lab-14: You’re are Nothing but a Pack of Cards!!",
    "section": "Introduction",
    "text": "Introduction\nLet’s make a website in RStudio to show off our data viz portfolio, and to share with friends, parents, prospective employers…\nWe will encounter a new package called blogdown and use workflows with github and a free web hosting service called Netlify to create a website where all our RMarkdowns become individual blog posts, complete with Titles, Sections, Text, Diagrams and Links!\nNOTE: Need to update this to Quarto!!!",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-14: You're are Nothing but a Pack of Cards!!"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/300-website/index.html#references",
    "href": "content/courses/R4Artists/Modules/300-website/index.html#references",
    "title": "Lab-14: You’re are Nothing but a Pack of Cards!!",
    "section": "References",
    "text": "References\n\nAllison Hill\nSharon Macliss\nYihui Xie",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-14: You're are Nothing but a Pack of Cards!!"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/100-GoN/index.html#slides-and-tutorials",
    "href": "content/courses/R4Artists/Modules/100-GoN/index.html#slides-and-tutorials",
    "title": "Lab-10: An Invitation from the Queen…to play Croquet",
    "section": " Slides and Tutorials",
    "text": "Slides and Tutorials",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-10: An Invitation from the Queen...to play Croquet"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/100-GoN/index.html#introduction",
    "href": "content/courses/R4Artists/Modules/100-GoN/index.html#introduction",
    "title": "Lab-10: An Invitation from the Queen…to play Croquet",
    "section": " Introduction",
    "text": "Introduction\nNetwork Diagrams are important in data visualization to bring out relationships between diverse entities. They are used in ecology, biology, transportation, and even history!\nAnd hey, whom did Jon Snow marry?",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-10: An Invitation from the Queen...to play Croquet"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/100-GoN/index.html#references",
    "href": "content/courses/R4Artists/Modules/100-GoN/index.html#references",
    "title": "Lab-10: An Invitation from the Queen…to play Croquet",
    "section": " References",
    "text": "References\n\nMichael Gastner, Data Analysis and Visualisation with R, Chapter 23: Networks\nDavid Schoch, Network Visualizations in R using ggraph and graphlayouts\nKonrad M. Lawson, Toilers and Gangsters:Simple Network Visualization with R for Historians",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-10: An Invitation from the Queen...to play Croquet"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/130-purrr/purrr.html",
    "href": "content/courses/R4Artists/Modules/130-purrr/purrr.html",
    "title": "Iteration: Learning to purrr",
    "section": "",
    "text": "knitr::opts_chunk$set(message = FALSE)\nlibrary(tidyverse)\nlibrary(gapminder)\nlibrary(ggformula)\n\n\n\nLearning to purrr",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Iteration: Learning to purrr"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/130-purrr/purrr.html#setting-up-r-packages",
    "href": "content/courses/R4Artists/Modules/130-purrr/purrr.html#setting-up-r-packages",
    "title": "Iteration: Learning to purrr",
    "section": "",
    "text": "knitr::opts_chunk$set(message = FALSE)\nlibrary(tidyverse)\nlibrary(gapminder)\nlibrary(ggformula)\n\n\n\nLearning to purrr",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Iteration: Learning to purrr"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/130-purrr/purrr.html#introduction",
    "href": "content/courses/R4Artists/Modules/130-purrr/purrr.html#introduction",
    "title": "Iteration: Learning to purrr",
    "section": "\n Introduction",
    "text": "Introduction\nOften we want to perform the same operation on several different sets of data. Rather than repeat the operation for each instance of data, it is faster, more intuitive, and less error-prone if we create a data structure that holds all the data, and use the map-* series functions from the purrr package to perform all the repeated operations in one shot.\nThis requires getting used to. We need to understand:\n\nthe data structure\nthe iteration mechanism using map functions\nthe form of the results",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Iteration: Learning to purrr"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/130-purrr/purrr.html#case-study-1-multiple-models-for-life-expectancy-with-gapminder",
    "href": "content/courses/R4Artists/Modules/130-purrr/purrr.html#case-study-1-multiple-models-for-life-expectancy-with-gapminder",
    "title": "Iteration: Learning to purrr",
    "section": "\n Case Study #1: Multiple Models for Life Expectancy with gapminder\n",
    "text": "Case Study #1: Multiple Models for Life Expectancy with gapminder\n\nWe will start with a complete case study and then work backwards to understand the various pieces of code that make it up.\nLet us look at the gapminder dataset:\n\nskimr::skim(gapminder)\n\n\nData summary\n\n\nName\ngapminder\n\n\nNumber of rows\n1704\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\ncountry\n0\n1\nFALSE\n142\nAfg: 12, Alb: 12, Alg: 12, Ang: 12\n\n\ncontinent\n0\n1\nFALSE\n5\nAfr: 624, Asi: 396, Eur: 360, Ame: 300\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nyear\n0\n1\n1979.50\n17.27\n1952.00\n1965.75\n1979.50\n1993.25\n2007.0\n▇▅▅▅▇\n\n\nlifeExp\n0\n1\n59.47\n12.92\n23.60\n48.20\n60.71\n70.85\n82.6\n▁▆▇▇▇\n\n\npop\n0\n1\n29601212.32\n106157896.74\n60011.00\n2793664.00\n7023595.50\n19585221.75\n1318683096.0\n▇▁▁▁▁\n\n\ngdpPercap\n0\n1\n7215.33\n9857.45\n241.17\n1202.06\n3531.85\n9325.46\n113523.1\n▇▁▁▁▁\n\n\n\n\n\nWe have lifeExp, gdpPerCap, and pop as Quant variables over time (year) for each country in the world. Suppose now that we wish to create Linear Regression Models predicting lifeExp using year, for each country. ( We will leave out gdpPercap and pop for now) The straightforward by laborious and naive way would be to use the lm command after filtering the dataset for each country, creating 140+ Linear Models manually! This would be horribly tedious!\nThere is a better way with purrr, and also more recently, with dplyr itself. Let us see both methods, the established purrr method first, and the new dplyr based method thereafter.\n\n {{}} EDA Plots\nWe can first plot lifeExp over year, grouped by country:\nggplot(gapminder,aes(x = year, y = lifeExp, colour = country)) + \n  geom_line(show.legend = FALSE) + \n  theme_classic()\nggplot(gapminder,aes(x = year, y = lifeExp, colour = country)) + \n  geom_line(show.legend = FALSE) + \n  facet_wrap(~ continent) + \n  theme_classic()\n\n\n\n\n\n\n\n\n\n\nBy and large we see positive slopes, but some countries do show non-linear behaviour.\nConstructing a Linear Model\nLet us take \\(1950\\) as a baseline year for all countries. Then we model lifeExp using year1950 across all countries together:\n\ngapminder &lt;- gapminder %&gt;% \n  mutate(year1950 = year - 1950) # baseline year\nmodel &lt;- lm(lifeExp ~ year1950, data = gapminder)\nsummary(model)\n\n\nCall:\nlm(formula = lifeExp ~ year1950, data = gapminder)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-39.949  -9.651   1.697  10.335  22.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 49.86028    0.55792   89.37   &lt;2e-16 ***\nyear1950     0.32590    0.01632   19.96   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.63 on 1702 degrees of freedom\nMultiple R-squared:  0.1898,    Adjusted R-squared:  0.1893 \nF-statistic: 398.6 on 1 and 1702 DF,  p-value: &lt; 2.2e-16\n\n\nmodel %&gt;% broom::tidy() # Parameters of the Model\nmodel %&gt;% broom::glance() # Statistics of the Model\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nSince the slope 0.3259038 is positive, life expectancy has been increasing over the years, across all countries. (But r.squared 0.1897571 is low, so this model does not explain much).\nHow do we do this for each country? We need to use the split-apply-combine method to achieve this. The combination of group_by and summarise is a example of the split &gt; apply &gt; combine method. For example, we could (split) the data by country, calculate the linear model each group (apply), and (combine) the results in a data frame.\nHowever, this first-attempt code for a per-country linear model does not work:\n\n```{r}\n#| eval: false\ngapminder %&gt;% \n  group_by(country) %&gt;% \n  summarise(linmod = lm(lifeExp ~ year1950, data = .))\n```\n\nThis is because the linmod variable is a list variable and cannot be accommodated in a simple column, which is what summarize will try to create. So we need to be able to create “list” columns in a data frame…how do we do that? Before we contemplate that, let us understand the capabilities of the purrr package in R.\nThe purrr package\nThe purrr package contains a new class of functions, that can take vectors/tibbles/lists as input, and perform an identical function over each component of these, and generate vectors/tibbles/lists as output. These are the map_* functions that are part of the purrr package. The * in the map_* function defines what kind of output (vector/tibble/list) the function generates.\nLet us look at a few short examples.\nUsing map_* functions from purrr\n\nThe basic structure of the map_* functions is:\n\n```{r}\n#| eval: false\nmap_typeOfResult(.x = what_to_iterate_with, \n                 .f = function_to_apply)\n\nmap_typeOfResult(.x = what_to_iterate_with, \n                 .f = \\(x) function_to_apply(x, additional_parameters))\n```\n\nTwo examples:\n\n# Example 1: Input: vector, Output: vector\ndiamonds %&gt;% \n  select(where(is.numeric)) %&gt;% \n  \n  # We need dbl-type numbers in output **vector**\nmap_dbl(.x = ., \n        .f = mean)\n\n       carat        depth        table        price            x            y \n   0.7979397   61.7494049   57.4571839 3932.7997219    5.7311572    5.7345260 \n           z \n   3.5387338 \n\n# Example 2: Input: vector, Output: tibble\ndiamonds %&gt;% \n  select(where(is.numeric)) %&gt;% \n  \n  # We need dbl-type numbers in output **vector**\nmap_df(.x = ., \n       .f = mean)\n\n\n  \n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote map_dbl outputs a (numeric) vector, and map_df outputs a tibble.\nIn each of the above examples, each vector in the diamonds dataset was passed to the respective map_* function as the parameter.x.\n\n\nSometimes the function .f may need some additional parameters to be specified, and these may not come from the input .x:\n\n# Example 3, with additional parameters to .f\npalmerpenguins::penguins %&gt;% \n  select(where(is.numeric)) %&gt;% \n  \n  map_dbl(.x = ., \n          .f = \\(x) mean(x, na.rm = TRUE))\n\n   bill_length_mm     bill_depth_mm flipper_length_mm       body_mass_g \n         43.92193          17.15117         200.91520        4201.75439 \n             year \n       2008.02907 \n\n  # penguins has two rows of NA entries which need to be dropped\n  # Hence this additional parameter for the `mean` function\n\n\n# Example 4: if we want a tibble output\npalmerpenguins::penguins %&gt;% \n  select(where(is.numeric)) %&gt;% \n  \n  map_df(.x = ., \n         .f = \\(x) mean(x, na.rm = TRUE))\n\n\n  \n\n\n\nThe .f function can be anything, even a ggformula plot command; in this case the output will not be a vector or a tibble, but a list:\n#library(ggformula)\npalmerpenguins::penguins %&gt;% \n  select(where(is.numeric)) %&gt;% select(-year) %&gt;% drop_na() %&gt;% \n  \n  # `map` gives a list output\n  map(.x = .,\n      .f = \\(x) gf_histogram(~x, bins = 30) %&gt;% \n        gf_theme(theme_classic())\n  )\n\n\n\n$bill_length_mm\n\n\n\n\n\n\n\n\n$bill_depth_mm\n\n\n\n\n\n\n\n\n$flipper_length_mm\n\n\n\n\n\n\n\n\n$body_mass_g\n\n\n\n\n\n\nNote: we need to do just a bit of extra pre-work to get the variable names on the x-axis of the histograms. There is a possibility to store all the plots in a separate column\nOK, so we can get vectors/tibbles/lists as output using vectors as inputs. Why would it be desirable to provide tibble/list as an input to a map_* function?\n\n Using purrr to create multiple models\nNow that we have some handle on purrr’s map functions, we can see how to develop a linear regression model for every country in the gapminder dataset. It should be clear from the command for a linear model:\n\n```{r}\n#| eval: false\nmodel &lt;- lm(target ~ predictor(s), \n            data  = tibble_containing_target_and_predictors_columns)\n```\n\nthat we need to specify three things: target, predictors, and the data tibble for the development of a linear model. To do this for each country in gapminder, here is the process:\n\nGroup the gapminder data by country (and continent)\nCreate a column containing unique per-country data for each country. This column would hence contain a tibble in each cell. This is a list column!\nUse map which would take country and the data columns created above to create an lm object for each country (in another list column)\nUse map again with broom::tidy as the function to give us clean columns for the model per country.\nUse that multi-model tibble to plot graphs for each country.\n\nLet us do this now!\n\ngapminder_models &lt;- gapminder %&gt;% \n  group_by(continent, country) %&gt;% \n  \n  # Create a per-country tibble in a new column called \"data_list\"\n  nest(.key = \"data_list\") \ngapminder_models\n\n\n  \n\n\ngapminder_models &lt;- gapminder_models %&gt;%\n  # We use mutate + map to add a list column containing linear models\n  mutate(model = map(.x = data_list, \n                     \n          # One column .x to iterate over\n          # The .x list column contains data frames\n          # So we access individual columns for target and predictors \n          # within these individual data frames\n                     .f = \\(.x) lm(lifeExp ~ year1950, data = .x)\n          )) %&gt;% \n  \n  # Use mutate + map again to expose the columns of the models\n  # Use broom:: tidy, broom::glance(), and \n  # Use broom::augment for separate columns\n  mutate(model_params = map(.x = model, \n                      .f = \\(.x) broom::tidy(.x, \n                                             conf.int = TRUE, \n                                             conf.lvel = 0.95)),\n         model_metrics = map(.x = model,\n                      .f = \\(.x) broom::glance(.x)),\n         \n         model_augment = map(.x  = model, \n                             .f = \\(.x) broom::augment(.x))\n         ) \ngapminder_models\n\n\n  \n\n\n\nWe can now take this tibble with multiple models and use broom to tidy, to glance at, and to augment the models:\n\nparams &lt;- gapminder_models %&gt;% \n  select(continent, country,model_params, model_metrics) %&gt;% \n  ungroup() %&gt;% \n  # Now unpack the linear model parameters into columns\n  unnest(cols = model_params)\nparams\n\n\n  \n\n\n###\nmetrics &lt;- gapminder_models %&gt;% \n  select(continent, country, model_metrics) %&gt;% \n  ungroup() %&gt;% \n  # Now unpack the linear model parameters into columns\n  unnest(cols = model_metrics)\nmetrics\n\n\n  \n\n\n###\naugments &lt;- gapminder_models %&gt;% \n  select(continent, country, model_augment) %&gt;% \n  ungroup() %&gt;% \n  # Now unpack the linear model parameters into columns\n  unnest(cols = model_augment)\naugments\n\n\n  \n\n\n\n\n Model Visualization\nWe can now plot these models and their uncertainty (i.e Confidence Intervals). We can select a few of the countries and plot:\nparams_filtered &lt;- params %&gt;% \n  filter(country %in% c(\"India\", \"United States\", \"Brazil\", \"China\"), \n         term == \"year1950\") %&gt;% \n  select(country, estimate, conf.low, conf.high,p.value) %&gt;%\n  arrange(estimate)\nparams_filtered\n###\nparams_filtered %&gt;%\n  gf_errorbar(conf.high + conf.low ~ reorder(country, estimate),\n              linewidth = ~ -log10(p.value), width = 0.3,\n              ylab = \"Effect Size\", \n              xlab = \"Country\",\n              title = \"Effect of years on Life Expectancy\",\n              caption = \"Significance = - log10(p.value)\") %&gt;% \n  \n  gf_point(estimate ~ reorder(country,estimate), \n           colour = \"black\", size = 4) %&gt;%\n  \n  gf_theme(theme_classic()) %&gt;% \n  \n  gf_refine(coord_flip(),\n            scale_linewidth_continuous(\"Significance\", \n                                       range = c(0.2, 3))) %&gt;%\n  gf_refine(guides(linewidth = guide_legend(reverse = TRUE)),\n            theme(axis.text.x = element_text(angle = 30, hjust = 1)))\n\n\n\n\n  \n\n\n\n\n\n\n\nBut we can do better: visualize all models at once. What we will do is to plot the r.squared on the x-axis and the model term year1950 on the y-axis. We will need to combine params and metrics to do this:\nparams_combo &lt;- params %&gt;% \n  select(continent, country, term , estimate) %&gt;% \n  filter(term == \"year1950\") %&gt;% \n  left_join(metrics %&gt;% select(continent, country, r.squared))\nparams_combo\n###\nparams_combo %&gt;% \n  gf_point(reorder(country, r.squared) ~ r.squared, \n           color = \"grey70\") %&gt;%\n  gf_point(reorder(country, r.squared) ~ r.squared, \n           data = params_combo %&gt;% filter(continent == \"Africa\"),\n           shape = 21, size = 3,\n    fill = \"salmon\",\n    ylab = \"Country\",\n    title = \"African Countries are Hard to Model\") %&gt;% \n  \n  gf_label(60 ~ 0.25,\n           label = \"African Countries\",\n           fill = \"salmon\",\n           color = \"black\",\n           inherit = FALSE) %&gt;%\n    \n  gf_theme(theme_classic()) %&gt;% \n  gf_refine(theme(axis.text.y = element_text(size = 3, face = \"bold\")))\n###\nparams_combo %&gt;%\n  gf_point(estimate ~ r.squared, color = \"grey70\") %&gt;% \n  gf_point(estimate ~ r.squared,\n           data = params_combo %&gt;% \n             filter(continent == \"Africa\"),\n           shape = 21,size =3,\n    fill = \"salmon\",\n    ylab = \"Slope Estimate for Linear Model\",\n    title = \"African Countries are Hard to Model\",\n    show.legend = FALSE) %&gt;%\n  gf_label(0.3 ~ 0.25,\n           label = \"African Countries\",\n           fill = \"salmon\",\n           color = \"black\",\n           inherit = FALSE) %&gt;%\n  gf_theme(theme_minimal())\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nAs can be seen, there are many models with low values of r.squared and these are sadly all about countries in Africa. The linear model fares badly for these countries, since there are other factors (not just year) that affects lifeExp in these countries.\nWe can look at the model metrics and see for which (African) countries the model fares the worst. We will reverse sort on r.squared and choose the 5 worst models:\n\nmetrics %&gt;% slice_min(order_by = r.squared, n = 5)\n\n\n  \n\n\n\nThere are of course reasons for this: genocide in Rwanda, and hyper-inflation in Zimbabwe, and of course the HIV-AIDS pandemic. These reasons are not captured in the original gapminder data!\nOne last plot! We can plot the model intercept on the x-axis and the slope year term on the y-axis to see where countries were in the beginning (1950) and at what rate they have improved in lifeExp:\n\nparams %&gt;%\n  select(continent, country, term , estimate) %&gt;%\n  pivot_wider(\n    id_cols = c(continent, country),\n    names_from = term,\n    values_from = estimate\n  ) %&gt;%\n  left_join(metrics %&gt;% select(continent, country, r.squared)) %&gt;%\n  \n  gf_point(\n    year1950 ~ `(Intercept)`,\n    color = ~ continent,\n    size = ~ r.squared,\n    xlab = \"Baseline at 1950\",\n    ylab = \"Rate of Improvement\",\n    title = \"Asian Countries Show Improvement in Life Expectancy\",\n    subtitle = \"African Countries still struggling\",\n    caption = \"Data from Gapminder\"\n  ) %&gt;%\n  gf_refine(scale_size(range = c(0.1, 4)),\n            scale_color_manual(\n              values =\n                c(\n                  \"Africa\" = \"salmon\",\n                  \"Asia\" = \"limegreen\",\n                  \"Americas\" = \"grey90\",\n                  \"Europe\" = \"grey90\",\n                  \"Oceania\" = \"grey90\"\n                )\n            )) %&gt;%\n  gf_refine(guides(size = guide_legend(reverse = TRUE))) %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\nMany Asian countries were low in lifeExp in 1950 and have shown good rates of improvement; r.squared is also decent. Sadly African countries had low lifeExp in 1950 and have not shown good rates of improvement.",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Iteration: Learning to purrr"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/130-purrr/purrr.html#recent-developments-in-dplyr",
    "href": "content/courses/R4Artists/Modules/130-purrr/purrr.html#recent-developments-in-dplyr",
    "title": "Iteration: Learning to purrr",
    "section": "\n Recent developments in dplyr\n",
    "text": "Recent developments in dplyr\n\nIn recent times, the familiar dplyr package also has experimental functions that are syntactically easier and offer pretty much purrr-like capability, and without introducing the complexity of the list columns or list output.\nLook the code below and decipher how it works:\n\n# Using group_modify\ngapminder_model_dplyr &lt;- gapminder %&gt;%\n  group_by(continent, country) %&gt;%\n  \n  # Here is the new function in dplyr!\n  # No need to use `mutate`\n  dplyr::group_modify(\n    .data = .,\n    \n    # .f MUST generate a tibble here and *not* a list\n    # Hence broom::tidy is essential!\n    # glance/tidy is part of the group_map's .f variable.\n    # Applies to each model\n\n    .f = ~ lm(lifeExp ~ year, data = .) %&gt;%\n      broom::glance(conf.int = TRUE,  # try `tidy()` and `augment()`\n                    conf.lvel = 0.95)\n  ) %&gt;%\n  \n  # We already have a grouped tibble from `group_modify()`\n  # So just ungroup()\n  ungroup()\n\ngapminder_model_dplyr\n\n\n  \n\n\n\nThere is no nesting and un-nesting; the data is the familiar tibble throughout! This seems like a simple and elegant method.\n\n\n\n\n\n\nUsing dplyr::group_modify\n\n\n\nNote: group_modify is new experimental function in dplyr (June 2023), as are group_map, list_cbind and list_rbind. group_modify requires that the operation in .fgenerates a tibble, not a list, and we can retain the grouping variable easily too. We can remove the groups with ungroup.\ngroup_modify() looks very clear and crisp, in my opinion. And very learner-friendly!",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Iteration: Learning to purrr"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/130-purrr/purrr.html#conclusion",
    "href": "content/courses/R4Artists/Modules/130-purrr/purrr.html#conclusion",
    "title": "Iteration: Learning to purrr",
    "section": "\n Conclusion",
    "text": "Conclusion\nWe have seen how purrr simplifies the application of functions iteratively to large groups of data, in a faster, replicable, and less error-prone manner. The basic idea (see video below) is:\n- Use tidyr::nest to create a grouped data frame with a nested list column\n- Use purrr::map_* to create a model for each of these data frames in the list column. The model will also be a column(usually) containing a list\n- Use broom::tidy to convert the list model-column into a data frame for visualization",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Iteration: Learning to purrr"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/130-purrr/purrr.html#references",
    "href": "content/courses/R4Artists/Modules/130-purrr/purrr.html#references",
    "title": "Iteration: Learning to purrr",
    "section": "\n References",
    "text": "References\n\n\nRebecca Barter, Learn to purrr. https://www.rebeccabarter.com/blog/2019-08-19_purrr\nEmorie Beck, Introduction to purrr. https://emoriebeck.github.io/R-tutorials/purrr/#\nSander Wuyts, purrr Tutorial. https://sanderwuyts.com/en/blog/purrr-tutorial/\nJared Wilber, Using the tidyverse for Machine Learning. https://www.jwilber.me/nest/\nDan Ovando,Data Wrangling and Model Fitting using purrr\nCormac Nolan, Modelling with Nested Data frames. https://github.com/cormac85/modelling_practice/blob/master/nested_data_frames.Rmd",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Iteration: Learning to purrr"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/60-GoG/index.html",
    "href": "content/courses/R4Artists/Modules/60-GoG/index.html",
    "title": "Lab-6: These Roses have been Painted !!",
    "section": "",
    "text": "R Tutorial  \n  Slides \n  Colour in R \nAdvanced Graphics",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-6: These Roses have been Painted !!"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/60-GoG/index.html#slides-and-tutorials",
    "href": "content/courses/R4Artists/Modules/60-GoG/index.html#slides-and-tutorials",
    "title": "Lab-6: These Roses have been Painted !!",
    "section": "",
    "text": "R Tutorial  \n  Slides \n  Colour in R \nAdvanced Graphics",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-6: These Roses have been Painted !!"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/60-GoG/index.html#introduction",
    "href": "content/courses/R4Artists/Modules/60-GoG/index.html#introduction",
    "title": "Lab-6: These Roses have been Painted !!",
    "section": " Introduction",
    "text": "Introduction\nAh…ggplot ! All those wonderful pictures and graphs, that Alice might have relished!\nMetaphors, aesthetics, geometries…and pictures !! ggplot seems to equate ravens to writing desks in its syntax…and out come graphs!!\nAnd colours: Wes Anderson! Tim Burton! The Economist… and many others!!",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-6: These Roses have been Painted !!"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/60-GoG/index.html#references",
    "href": "content/courses/R4Artists/Modules/60-GoG/index.html#references",
    "title": "Lab-6: These Roses have been Painted !!",
    "section": " References",
    "text": "References\n\nGeorge Lakoff and Mark Johnson, Metaphors We Live By, https://www.youtube.com/watch?v=lYcQcwUfo8c\nWickham and Grolemund, R for Data Science, ggplot chapter: https://r4ds.had.co.nz/data-visualisation.html\nCMDLineTips, 10 Tips to Customize Text Color, Font, Size in ggplot2 with element_text(), https://cmdlinetips.com/2021/05/tips-to-customize-text-color-font-size-in-ggplot2-with-element_text/\nCMDLineTips, How to write a simple custom ggplot theme from scratch, https://cmdlinetips.com/2022/05/how-to-write-a-simple-custom-ggplot-theme-from-scratch/\nAsha Hill @ mode.com, 12 Extensions to ggplot2 for More Powerful R Visualizations, https://mode.com/blog/r-ggplot-extension-packages/\nEmil Hvitfeldt, ggplot Trial and Error, https://www.emilhvitfeldt.com/post/2018-06-12-ggplot2-trial-and-error-us-trade-data/",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-6: These Roses have been Painted !!"
    ]
  },
  {
    "objectID": "content/courses/R4Artists/Modules/60-GoG/index.html#fun-stuff",
    "href": "content/courses/R4Artists/Modules/60-GoG/index.html#fun-stuff",
    "title": "Lab-6: These Roses have been Painted !!",
    "section": " Fun Stuff",
    "text": "Fun Stuff\n\nYihan Wu, Mapping ggplot geoms and aesthetic parameters, ( An interactive view of which aesthetic parameters work with which ggplot geom!! ) https://www.yihanwu.ca/post/geoms-and-aesthetic-parameters/\nhttps://www.theartstory.org/artist/kandinsky-wassily/",
    "crumbs": [
      "Teaching",
      "R for Artists and Managers",
      "Lab-6: These Roses have been Painted !!"
    ]
  },
  {
    "objectID": "content/courses/ISTW/listing.html",
    "href": "content/courses/ISTW/listing.html",
    "title": "Literary Jukebox: In Short, the World",
    "section": "",
    "text": "Italy-Dino Buzzati\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\nFrance - Guy de Maupassant\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\nJapan - Hisaye Yamamoto\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\nPeru - Ventura Garcia Calderon\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\nRussia-Maxim Gorky\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\nEgypt-Alifa Rifaat\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\nBrazil-Clarice Lispector\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\nEngland - V S Pritchett\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\nRussia-Ivan Bunin\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\nCzech Republic - Milan Kundera\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\nSweden-Lars Gustaffsson\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\nCanada - John Cheever\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\nIreland - William Trevor\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\nUSA-Raymond Carver\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\nItaly-Primo Levi\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\nIndia - Ruth Prawer Jhabvala\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\nUSA-Carson McCullers\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\nZimbabwe - Petina Gappah\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\nIndia - Bharati Mukherjee\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\nUSA - Lucia Berlin\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\nSpain-Merce Rodoreda\n\n\n1 min\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n Back to top",
    "crumbs": [
      "Teaching",
      "Literary Jukebox: In Short, the World"
    ]
  },
  {
    "objectID": "content/courses/ISTW/Modules/120-Sweden-LarsGustaffson/index.html",
    "href": "content/courses/ISTW/Modules/120-Sweden-LarsGustaffson/index.html",
    "title": "Sweden-Lars Gustaffsson",
    "section": "",
    "text": "From https://larsgustafssonblog.blogspot.com:\n\nLars Gustafsson (born May 17, 1936) is a Swedish, poet, novelist and scholar. He was born in Västerås, completed his secondary education at the Västerås gymnasium and continued to Uppsala University; he received his Licentiate degree in 1960 and was awarded his Ph.D. in Theoretical Philosophy in 1978. He lived in Austin, Texas until 2003, and has recently returned to Sweden. He served as a professor at the University of Texas in Austin, Texas, where he taught Philosophy and Creative Writing, until May 2006, when he retired.\nGustafsson is one of the most prolific Swedish writers since August Strindberg. Since the late 1950s he has produced a voluminous flow of poetry, novels, short stories, critical essays, and editorials. He is also an example of a Swedish writer who has gained international recognition with literary awards such as the Prix International Charles Veillon des Essais in 1983, the Heinrich Steffens Preis in 1986, Una Vita per la Litteratura in 1989, a John Simon Guggenheim Memorial Foundation Fellowship for poetry in 1994, and several others.\n\n\nWe will read Greatness Strikes Where It Pleases.\n\n“Greatness Strikes Where It Pleases” by Swedish writer Lars Gustafsson, was first published in Sweden in 1981. Translated into English in 1986, it appeared in Stories of Happy People (Norton, 1986; in print). It can also be found in You’ve Got to Read This: Contemporary Writers Introduce Stories That Held Them in Awe, edited by Ron Hansen (New York, 1994).\n\n\n\nBeing “Different”\nSensorial Learning\nSiblings and Families\nSchools, Institutions, and Teaching Pedagogies\nNature Deprivation\nEpiphanies"
  },
  {
    "objectID": "content/courses/ISTW/Modules/120-Sweden-LarsGustaffson/index.html#lars-gustafsson",
    "href": "content/courses/ISTW/Modules/120-Sweden-LarsGustaffson/index.html#lars-gustafsson",
    "title": "Sweden-Lars Gustaffsson",
    "section": "",
    "text": "From https://larsgustafssonblog.blogspot.com:\n\nLars Gustafsson (born May 17, 1936) is a Swedish, poet, novelist and scholar. He was born in Västerås, completed his secondary education at the Västerås gymnasium and continued to Uppsala University; he received his Licentiate degree in 1960 and was awarded his Ph.D. in Theoretical Philosophy in 1978. He lived in Austin, Texas until 2003, and has recently returned to Sweden. He served as a professor at the University of Texas in Austin, Texas, where he taught Philosophy and Creative Writing, until May 2006, when he retired.\nGustafsson is one of the most prolific Swedish writers since August Strindberg. Since the late 1950s he has produced a voluminous flow of poetry, novels, short stories, critical essays, and editorials. He is also an example of a Swedish writer who has gained international recognition with literary awards such as the Prix International Charles Veillon des Essais in 1983, the Heinrich Steffens Preis in 1986, Una Vita per la Litteratura in 1989, a John Simon Guggenheim Memorial Foundation Fellowship for poetry in 1994, and several others.\n\n\nWe will read Greatness Strikes Where It Pleases.\n\n“Greatness Strikes Where It Pleases” by Swedish writer Lars Gustafsson, was first published in Sweden in 1981. Translated into English in 1986, it appeared in Stories of Happy People (Norton, 1986; in print). It can also be found in You’ve Got to Read This: Contemporary Writers Introduce Stories That Held Them in Awe, edited by Ron Hansen (New York, 1994).\n\n\n\nBeing “Different”\nSensorial Learning\nSiblings and Families\nSchools, Institutions, and Teaching Pedagogies\nNature Deprivation\nEpiphanies"
  },
  {
    "objectID": "content/courses/ISTW/Modules/120-Sweden-LarsGustaffson/index.html#notes-and-references",
    "href": "content/courses/ISTW/Modules/120-Sweden-LarsGustaffson/index.html#notes-and-references",
    "title": "Sweden-Lars Gustaffsson",
    "section": "Notes and References",
    "text": "Notes and References\n\n\n“Greatness Strikes Where It Pleases.” Short Stories for Students. Retrieved March 18, 2024 from Encyclopedia.com: https://www.encyclopedia.com/education/news-wires-white-papers-and-books/greatness-strikes-where-it-pleases\n\nLouv, Richard. October 15, 2019. What is Nature-Deficit Disorder? https://richardlouv.com/blog/what-is-nature-deficit-disorder\n\nMcleod, Saul.February 1, 2024. Jerome Bruner’s Theory Of Learning And Cognitive Development. https://www.simplypsychology.org/bruner.html. Especially read about Bruner’s idea of the Spiral Curriculum.\n\nSong for the Story !!\nA terrific comment on education itself!!\nArtistes: Super Tramp ( Roger Hodgson )\nAlbum: Breakfast in America\nYear: 1979\nChart Position: #6(US); #1(Australia, Canada, Norway, France, ..)\nFrom https://genius.com/Supertramp-the-logical-song-lyrics\n\n“The Logical Song” was the lead single from Supertramp’s chart-topping album Breakfast In America. It was internationally successful, reaching the top 20 in several countries including a #6 peak in the US and a #7 peak in the UK – their best showing in both countries. In Canada, “The Logical Song” not only topped the Canada Singles Chart, but was the #1 song of the entire year.\n\n\n“The Logical Song,” not unlike others in that period (eg – Pink Floyd’s “Another Brick in the Wall”) is a scathing criticism of British school and education at the time.\n\n\nWhen Paul McCartney was asked his favorite song of 1979, he chose “The Logical Song”. The song also won (Roger) Hodgson an Ivor Novello award from The British Academy of Composers and Songwriters in 1980."
  },
  {
    "objectID": "content/courses/ISTW/Modules/120-Sweden-LarsGustaffson/index.html#writing-prompts",
    "href": "content/courses/ISTW/Modules/120-Sweden-LarsGustaffson/index.html#writing-prompts",
    "title": "Sweden-Lars Gustaffsson",
    "section": "Writing Prompts",
    "text": "Writing Prompts\n\nI never let my Schooling interfere with my Education.(Mark Twain)\nDescribing a Concept metaphorically using words from an unrelated field. Inspiration: Primo Levi’s immortal story, “Carbon”.\n\nOn being the odd child in a group of siblings (no victim-mentality-based ranting, please!)"
  },
  {
    "objectID": "content/courses/ISTW/Modules/200-Zimbabwe-PetinaGappah/index.html",
    "href": "content/courses/ISTW/Modules/200-Zimbabwe-PetinaGappah/index.html",
    "title": "Zimbabwe - Petina Gappah",
    "section": "",
    "text": "Petina Gappah is an Geneva-based international trade lawyer who has written poignant, humane and funny collection of stories about her home country, Zimbabwe. Her first collection of stories, An Elegy for Easterly was a winner of the Guardian First Book Award in 2009. Gappah’s collection of 13 stories, An Elegy for Easterly, tells of the lives of people, rich and poor, caught up in events over which they have little control.\n\n\n\nThe Mupandawana Dancing Champion\nPetina Gappah reads her own story Weblink to audio\n\n\nPoverty and Unemployment\nInflation and Purchasing Power\nIncompetent and Corrupt Government\nSilliness of High-Ranking Officials\nMusic, Dance and pursuits of common people\nHumour as a Defence Mechanism"
  },
  {
    "objectID": "content/courses/ISTW/Modules/200-Zimbabwe-PetinaGappah/index.html#zimbabwe-petina-gappah",
    "href": "content/courses/ISTW/Modules/200-Zimbabwe-PetinaGappah/index.html#zimbabwe-petina-gappah",
    "title": "Zimbabwe - Petina Gappah",
    "section": "",
    "text": "Petina Gappah is an Geneva-based international trade lawyer who has written poignant, humane and funny collection of stories about her home country, Zimbabwe. Her first collection of stories, An Elegy for Easterly was a winner of the Guardian First Book Award in 2009. Gappah’s collection of 13 stories, An Elegy for Easterly, tells of the lives of people, rich and poor, caught up in events over which they have little control.\n\n\n\nThe Mupandawana Dancing Champion\nPetina Gappah reads her own story Weblink to audio\n\n\nPoverty and Unemployment\nInflation and Purchasing Power\nIncompetent and Corrupt Government\nSilliness of High-Ranking Officials\nMusic, Dance and pursuits of common people\nHumour as a Defence Mechanism"
  },
  {
    "objectID": "content/courses/ISTW/Modules/200-Zimbabwe-PetinaGappah/index.html#notes-and-references",
    "href": "content/courses/ISTW/Modules/200-Zimbabwe-PetinaGappah/index.html#notes-and-references",
    "title": "Zimbabwe - Petina Gappah",
    "section": "Notes and References",
    "text": "Notes and References\nAdditional Material\nThe Guardian. 5 Sept 2015. Interview with Petina Gappah. I’ve written a very Zimbabwean story.\nSong for the Story!\nBilly Joel’s Allentown from his 1982 album The Nylon Curtain:\n&gt; About Allentown\n\n“Allentown” is a song by American singer Billy Joel, which was the lead track on Joel’s The Nylon Curtain (1982) album, accompanied by a conceptual music video. Upon its release, and especially in subsequent years, “Allentown” emerged as an anthem of blue-collar America, representing both the aspirations and frustrations of America’s working class in the late 20th century.\n\n\n  Well we’re living here in Allentown\nAnd they’re closing all the factories down\nOut in Bethlehem they’re killing time\nFilling out forms\nStanding in line\nWell our fathers fought the Second World War\nSpent their weekends on the Jersey Shore\nMet our mothers in the USO1\nAsked them to dance\nDanced with them slow\nAnd we’re living here in AllentownBut the restlessness was handed down\nAnd it’s getting very hard to stay\nWell we’re waiting here in Allentown\nFor the Pennsylvania we never found\nFor the promises our teachers gave\nIf we worked hard\nIf we behaved\nSo the graduations hang on the wall\nBut they never really helped us at all\nNo they never taught us what was real\nIron and coal\nAnd chromium steel\nAnd we’re waiting here in Allentown\nBut they’ve taken all the coal from the ground\nAnd the union people crawled away\nEvery child has a pretty good shot\nTo get at least as far as their old man got\nBut something happened on the way to that place\nThey threw an American flag in our face\nWell I’m living here in Allentown\nAnd it’s hard to keep a good man down\nBut I won’t be giving up today\nAnd we’re living here in Allentown"
  },
  {
    "objectID": "content/courses/ISTW/Modules/200-Zimbabwe-PetinaGappah/index.html#writing-prompts",
    "href": "content/courses/ISTW/Modules/200-Zimbabwe-PetinaGappah/index.html#writing-prompts",
    "title": "Zimbabwe - Petina Gappah",
    "section": "Writing Prompts",
    "text": "Writing Prompts\n\nHumour and Laughter as our “last refuge”\nAn encounter with a Government Department\nListening to a Politician’s speech\nParticipating in an HR function at your workspot\nOn a story told to you by an elderly relative, full of mother-tongue words"
  },
  {
    "objectID": "content/courses/ISTW/Modules/200-Zimbabwe-PetinaGappah/index.html#footnotes",
    "href": "content/courses/ISTW/Modules/200-Zimbabwe-PetinaGappah/index.html#footnotes",
    "title": "Zimbabwe - Petina Gappah",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://en.wikipedia.org/wiki/United_Service_Organizations↩︎"
  },
  {
    "objectID": "content/courses/ISTW/Modules/60-Egypt-AlifaRifaat/index.html",
    "href": "content/courses/ISTW/Modules/60-Egypt-AlifaRifaat/index.html",
    "title": "Egypt-Alifa Rifaat",
    "section": "",
    "text": "From the book blurb:\n\n“More convincingly than any other woman writing in Arabic today, Alifa Rifaat lifts the veil on what it means to be a woman living within a traditional Muslim society.” So states the translator’s foreword to this collection of the Egyptian author’s best short stories. Rifaat (1930–1996) did not go to university, spoke only Arabic, and seldom traveled abroad. This virtual immunity from Western influence lends a special authenticity to her direct yet sincere accounts of death, sexual fulfillment, the lives of women in purdah, and the frustrations of everyday life in a male-dominated Islamic environment.\nTranslated from the Arabic by Denys Johnson-Davies https://www.theguardian.com/world/2017/jun/18/denys-johnson-davies-obituary, the collection admits the reader into a hidden private world, regulated by the call of the mosque, but often full of profound anguish and personal isolation. Badriyya’s despairing anger at her deceitful husband, for example, or the haunting melancholy of “At the Time of the Jasmine,” are treated with a sensitivity to the discipline and order of Islam."
  },
  {
    "objectID": "content/courses/ISTW/Modules/60-Egypt-AlifaRifaat/index.html#alifa-rifaat",
    "href": "content/courses/ISTW/Modules/60-Egypt-AlifaRifaat/index.html#alifa-rifaat",
    "title": "Egypt-Alifa Rifaat",
    "section": "",
    "text": "From the book blurb:\n\n“More convincingly than any other woman writing in Arabic today, Alifa Rifaat lifts the veil on what it means to be a woman living within a traditional Muslim society.” So states the translator’s foreword to this collection of the Egyptian author’s best short stories. Rifaat (1930–1996) did not go to university, spoke only Arabic, and seldom traveled abroad. This virtual immunity from Western influence lends a special authenticity to her direct yet sincere accounts of death, sexual fulfillment, the lives of women in purdah, and the frustrations of everyday life in a male-dominated Islamic environment.\nTranslated from the Arabic by Denys Johnson-Davies https://www.theguardian.com/world/2017/jun/18/denys-johnson-davies-obituary, the collection admits the reader into a hidden private world, regulated by the call of the mosque, but often full of profound anguish and personal isolation. Badriyya’s despairing anger at her deceitful husband, for example, or the haunting melancholy of “At the Time of the Jasmine,” are treated with a sensitivity to the discipline and order of Islam."
  },
  {
    "objectID": "content/courses/ISTW/Modules/60-Egypt-AlifaRifaat/index.html#story",
    "href": "content/courses/ISTW/Modules/60-Egypt-AlifaRifaat/index.html#story",
    "title": "Egypt-Alifa Rifaat",
    "section": "Story",
    "text": "Story\nWe will read Rifaat’s story Bahiyya’s Eyes from the Collection Distant View of a Minaret and Other Stories."
  },
  {
    "objectID": "content/courses/ISTW/Modules/60-Egypt-AlifaRifaat/index.html#themes",
    "href": "content/courses/ISTW/Modules/60-Egypt-AlifaRifaat/index.html#themes",
    "title": "Egypt-Alifa Rifaat",
    "section": "Themes",
    "text": "Themes\n\nAge and Aging\nPoverty, Disease, and Healthcare\nWoman’s Place in Society\n“Eyes are the Gateway to the Soul” - Herman Melville\nReligion, Tradition, and Conservative Belief: Is Belief always “Blind”?\nThrough a Child’s Eyes: A Child’s Discoveries about the World\nChildren and Family"
  },
  {
    "objectID": "content/courses/ISTW/Modules/60-Egypt-AlifaRifaat/index.html#additional-material",
    "href": "content/courses/ISTW/Modules/60-Egypt-AlifaRifaat/index.html#additional-material",
    "title": "Egypt-Alifa Rifaat",
    "section": "Additional Material",
    "text": "Additional Material\nNotes and References\n\nبَهِيّة Bahiyyah is an Arabic name for girls that means “brilliant”, “beautiful”, “radiant”.\nAlifa Rifaat on the Dangerous Women Project https://dangerouswomenproject.org/2016/04/30/alifa-rifaat/\nBanipal Magazine of Modern Arabic Literature. http://www.banipal.co.uk\nhttps://www.arabicfiction.org/en\nDenys Johnson-Davies https://www.theguardian.com/world/2017/jun/18/denys-johnson-davies-obituary\nMalti-Douglas, Fedwa. Men, Women, and God(s): Nawal El Saadawi and Arab Feminist Poetics. Berkeley: University of California Press, c1995 1995. http://ark.cdlib.org/ark:/13030/ft8c6009n4/\nYa Baheya, the Woman Behind the Legend. http://www.shira.net/music/ya-baheya-background.htm\nOyun Baheya, the Song. https://youtu.be/sb4ZNdLGeIo\nFemale Genital Mutilation. https://www.who.int/news-room/fact-sheets/detail/female-genital-mutilation and https://data.unicef.org/topic/child-protection/female-genital-mutilation/\nSong for the Story\nSong: Do Naina aur Ek Kahani\nFilm: Masoom (1983)\nArtist: Arati Mukherjee\nMusic Director: R.D Burman\nLyricist: Gulzar\nStarring: Naseeruddin Shah, Shabana Azmi, Master Jugal, Baby Urmila, Saeed Jaffery\nDirector : Shekhar Kapoor\nhttps://youtu.be/qfrdkbQHw48\n Your browser does not support the audio tag;  for browser support, please see:  https://www.w3schools.com/tags/tag_audio.asp \n\n\n\n\n\n\nHindi Lyrics\nEnglish Translation\n\n\n\nDo naina aur ek kahanee(2)\nTwo eyes and one story,\n\n\nthoda sa badal, thoda sa panee aur ek kahanee(2)\nsome clouds, some water, and a story\n\n\n——\n——\n\n\nChhotee see do jheelon me woh, bahatee rahatee hai(2)\nIn two small lakes, they keep floating/flowing\n\n\nKoi sune ya na sune, kahatee rahatee hai\nWhether anyone is listening or not, they keep telling the story,\n\n\nKuchh likh ke aur, kuchh zubanee\nSome Written, and Sometimes by speaking out\n\n\n——\n——\n\n\nThodee sai hain janee hui, thodee see nayee(2)\nA part of the story is known, but a small part of it is new\n\n\nJahaan ruke aansu, waheen puree ho gayee\nWhenever the tears stop, that is where the story stops too,\n\n\nHai toh nayee phir bhi, hain puranee\nIt is new but its old at the same time\n\n\n——\n——-\n\n\nEk khatm ho toh, dusaree raat aa jatee hai(2)\nAs one night passes, it is time for the other night,\n\n\nHothhon pe phir bhulee hui, baat aa jatee hai\nAnd those forgotten words cross my lips\n\n\nDo naino kee hain yeh kahanee\nThat is the story of the two eyes!\n\n\nthoda sa badal, thoda sa panee aur ek kahanee\nsome clouds, some water, and a story\n\n\nDo naina aur ek kahanee…\nTwo eyes and one story….."
  },
  {
    "objectID": "content/courses/ISTW/Modules/60-Egypt-AlifaRifaat/index.html#writing-prompts",
    "href": "content/courses/ISTW/Modules/60-Egypt-AlifaRifaat/index.html#writing-prompts",
    "title": "Egypt-Alifa Rifaat",
    "section": "Writing Prompts",
    "text": "Writing Prompts\n\nContemplate your life, and demise, and write a Will. Dwell on relationships and not merely on (imaginary) opulence/ property.\nYour Friend is Blind.\nA Child discovers a piece of Family History.\nCompare the theme of blindness in this story with that in Raymond Carver’s story, Cathedral"
  },
  {
    "objectID": "content/courses/ISTW/Modules/10-Italy-DinoBuzatti/index.html#notes-and-references",
    "href": "content/courses/ISTW/Modules/10-Italy-DinoBuzatti/index.html#notes-and-references",
    "title": "Italy-Dino Buzzati",
    "section": "Notes and References",
    "text": "Notes and References\nSong for the Story !!\nWaheeda Rehman, the stunning “Falling Girl” in “Guide” ❤️ !! Waah! Dino Buzzati should have watched this!! He would have approved!!\n{{% youtube \"_ji8jZjk56c\" %}}\nLyrics and Translation: https://www.filmyquotes.com/songs/1245\n\n\n\n\n\n\nHindi Lyrics\nEnglish Translation\n\n\n\nKaanton se kheench ke yeh aanchal\nAfter pulling my dress out of thorns\n\n\nTodke bandhan baandhi payal\nAfter breaking the anklets of vows\n\n\nHo koi na roko dil ki udaan ko\nPlease no one stop the flight of the heart\n\n\nDil woh chala aa aa aa\nThere goes my heart flying…\n\n\n—————\n—————\n\n\nAaj phir jeene ki tamanna hai(2)\nToday, I have a desire to live again\n\n\nAaj phir marne ka irada hai(2)\nToday, I have an intention to die again!\n\n\n—————-\n—————-\n\n\nApne hi bas mein nahi main(2)\nI don’t have any control over myself\n\n\nDil hai kahin toh hoon kahin main(2)\nMy heart is somewhere and I’m somewhere else\n\n\nHo jaane kya paake meri zindagi ne\nI don’t know what my life has attained\n\n\nHanskar kaha ha ha ha\nBut it says this laughingly\n\n\n—————–\n——————\n\n\nMain hoon gubaar ya toofan hoon(2)\nAm I intoxicated or am I a storm\n\n\nKoi bataye main kahan hoon(2)\nPlease someone tell me where am I\n\n\nHo dar hai safar mein kahin kho na jaaon main\nI’m scared that I might get lost in the journey\n\n\nRasta naya aa aa aa\nAs these roads are new for me\n\n\n—————-\n——————–\n\n\nKal ke andheron se nikal ke(2)\nAfter coming out from the darkness of yesterday\n\n\nDekha hai aankhen malte malte(2)\nI’ve seen this rubbing my eyes\n\n\nHo phool hi phool zindagi bahaar hai\nThere is spring and flowers everywhere\n\n\nTay kar liya aa aa aa\nHence I’ve decided that…\n\n\n—————–\n———————\n\n\nAaj phir jeene ki tamanna hai(2)\nToday, I have a desire to live again\n\n\nAaj phir marne ka irada hai(2)\nToday, I have an intention to die again!"
  },
  {
    "objectID": "content/courses/ISTW/Modules/10-Italy-DinoBuzatti/index.html#writing-prompts",
    "href": "content/courses/ISTW/Modules/10-Italy-DinoBuzatti/index.html#writing-prompts",
    "title": "Italy-Dino Buzzati",
    "section": "Writing Prompts",
    "text": "Writing Prompts\n\nOn Comparing this Falling Girl with another very Famous One\nOn Looking in the Mirror 25 Years from Now\nA Letter from my 60-year-old self to me at 18-years-old\nOn saying “Anyways” and “My Bad”: Trying to Be Different in the Same Way\nFirst Copy, Then Innovate!"
  },
  {
    "objectID": "content/courses/ISTW/Modules/230-USA-Lucia-Berlin/index.html",
    "href": "content/courses/ISTW/Modules/230-USA-Lucia-Berlin/index.html",
    "title": "USA - Lucia Berlin",
    "section": "",
    "text": "https://www.theguardian.com/books/booksblog/2016/may/18/a-brief-survey-of-the-short-story-lucia-berlin"
  },
  {
    "objectID": "content/courses/ISTW/Modules/230-USA-Lucia-Berlin/index.html#lucia-berlin",
    "href": "content/courses/ISTW/Modules/230-USA-Lucia-Berlin/index.html#lucia-berlin",
    "title": "USA - Lucia Berlin",
    "section": "",
    "text": "https://www.theguardian.com/books/booksblog/2016/may/18/a-brief-survey-of-the-short-story-lucia-berlin"
  },
  {
    "objectID": "content/courses/ISTW/Modules/140-Canada-JohnCheever/index.html",
    "href": "content/courses/ISTW/Modules/140-Canada-JohnCheever/index.html",
    "title": "Canada - John Cheever",
    "section": "",
    "text": "We will read Cheever’s dystopian tech story, The Enormous Radio."
  },
  {
    "objectID": "content/courses/ISTW/Modules/140-Canada-JohnCheever/index.html#john-cheever",
    "href": "content/courses/ISTW/Modules/140-Canada-JohnCheever/index.html#john-cheever",
    "title": "Canada - John Cheever",
    "section": "",
    "text": "We will read Cheever’s dystopian tech story, The Enormous Radio."
  },
  {
    "objectID": "content/courses/ISTW/Modules/140-Canada-JohnCheever/index.html#themes",
    "href": "content/courses/ISTW/Modules/140-Canada-JohnCheever/index.html#themes",
    "title": "Canada - John Cheever",
    "section": "Themes",
    "text": "Themes\n\nCommunications Technology: Boon and Bane\n“Sweet Old Tech”\nUses of Tech, and Used by Tech\n“We Wish to Be SomeWhere We are Not”\n“We Wish to Be SomeWhenTime We are Not”\n“We Wish to Be SomeOne We are Not”\n“We Wish to Be SomeThing We are Not”\nForbidden Knowledge\nLove Thy Neighbour\n\nAdditional Material\nNotes and References\n\n\nSong for the Story !!\nSong: Somebody’s Watching Me\nGroup: Rockwell and Michael Jackson\nRelease Date: Pongal / Makara Sankranthi, January 14, 1984\n\n“Somebody’s Watching Me” is a song recorded by American singer Rockwell (Kennedy Gordy), released by the Motown label in 1984, as the lead single from his debut studio album of the same name. Rockwell’s debut single release, the song features guest vocals by brothers Michael Jackson (in the chorus) and Jermaine Jackson (additional backing vocals).\n\n\n\nAlso listen to The Buggles - Video Killed the Radio Star and Queen - Radio Gaga."
  },
  {
    "objectID": "content/courses/ISTW/Modules/140-Canada-JohnCheever/index.html#writing-prompts",
    "href": "content/courses/ISTW/Modules/140-Canada-JohnCheever/index.html#writing-prompts",
    "title": "Canada - John Cheever",
    "section": "Writing Prompts",
    "text": "Writing Prompts\n\nOn being Average, or being thought to be Average\nAm I using [your favourite SM app] or is it using me?\nOn Human Dreams and Technology. Take inspiration from Robert Lucky New Communications Services –What does Society want?. Look especially on the Section VI. therein on Fulfilling Ancient Dreams.\nOn bugs in internet apps. Take inspiration from Vinton G. Cerf’s famous Internet document in poetry form, RFC968 https://datatracker.ietf.org/doc/html/rfc968."
  },
  {
    "objectID": "content/courses/ISTW/Modules/20-France-GuydeMaupassant/index.html#story",
    "href": "content/courses/ISTW/Modules/20-France-GuydeMaupassant/index.html#story",
    "title": "France - Guy de Maupassant",
    "section": "Story",
    "text": "Story\nWe will read Maupassant’s short short story, Boule de Suif\nA Youtube discussion on this story\n{{% youtube \"zZtoLsWsECA\" %}}"
  },
  {
    "objectID": "content/courses/ISTW/Modules/20-France-GuydeMaupassant/index.html#themes",
    "href": "content/courses/ISTW/Modules/20-France-GuydeMaupassant/index.html#themes",
    "title": "France - Guy de Maupassant",
    "section": "Themes",
    "text": "Themes\n\nWomen and Objectification\nBeing Fat\nPrejudice\nFalse Gratitude?\nScapegoat Mechanism\nNetwork Effects: Reputations, Rumours, and Viral Trends\n“Convenience”"
  },
  {
    "objectID": "content/courses/ISTW/Modules/20-France-GuydeMaupassant/index.html#additional-material",
    "href": "content/courses/ISTW/Modules/20-France-GuydeMaupassant/index.html#additional-material",
    "title": "France - Guy de Maupassant",
    "section": "Additional Material",
    "text": "Additional Material\n\nThe Collected Works of Guy de Maupassant https://www.gutenberg.org/files/21327/21327-h/21327-h.htm\n\n\nNotes and References\n\nRene Girard’s Mimetic Theory and the Scapegoat: https://violenceandreligion.com/mimetic-theory/\nFrear, G. L. (1992). René Girard on Mimesis, Scapegoats, and Ethics. The Annual of the Society of Christian Ethics, 12, 115–133. http://www.jstor.org/stable/23559770 1\nThe Network Effects Bible. https://www.nfx.com/post/network-effects-bible\nThe Beauty of Fat Women: Leonard Nimoy (“Mr Spock”) and his Full Body Project: https://www.theguardian.com/artanddesign/gallery/2015/mar/03/the-full-body-project-by-leonard-nimoy-in-pictures\n\n\nLeonard Nimoy, who died February 27,2015 at the age of 83, was beloved by fans for his distinctive portrayal of Mr. Spock on Star Trek. Those fans may not have known that Nimoy, through his work as a photographer, also championed women who did not conform to Hollywood’s ideal of physical perfection. [see also http://mashable.com/2015/02/26/body-positivity-get-involved] In 2007, Nimoy published The Full Body Project, a collection of photos featuring nude women of many shapes and sizes. Nimoy’s previous book of photographs captured images of nude women as well, though the models’ slim bodies hewed closely to the conventional standards of beauty. The inspiration for The Fully Body Project struck when a full-figured woman approached Nimoy and asked if he might photograph her and her friends.\n\nListen to Leonard Nimoy discuss this project on NPR:\n\n\n\nIt Ain’t Over until the Fat Lady Sings: https://knowyourphrase.com/aint-over-until-the-fat-lady-sings\nIs it all about Hips?: http://bollynatyam.com/books/\n\n\n\n\n\n\n\n\n\nhttps://www.youtube.com/embed/ByAbV-MKDgs?si=PU26bRl2sXOynWpL"
  },
  {
    "objectID": "content/courses/ISTW/Modules/20-France-GuydeMaupassant/index.html#videossongs-for-the-story",
    "href": "content/courses/ISTW/Modules/20-France-GuydeMaupassant/index.html#videossongs-for-the-story",
    "title": "France - Guy de Maupassant",
    "section": "Videos/Songs for the Story",
    "text": "Videos/Songs for the Story\n\nFirst, A Song that is all about Network Effects !!!\n\nSong: Kuchh Toh Log Kahenge\nSinger: Kishore Kumar\nMusic: R. D. Burman\nFilm: Amar Prem (1972)\n Your browser does not support the audio tag;  for browser support, please see:  https://www.w3schools.com/tags/tag_audio.asp \nhttps://www.youtube.com/watch?v=OK6Hux4spNM\n\n\n\n\n\n\nHindi Lyrics\nEnglish Translation\n\n\n\nKuch toh log kahenge\nPeople will say something or the other\n\n\nLogon ka kaam hai kehna\nIt’s the job of people to say something\n\n\nChhodo bekaar ki baaton mein\nForget all these useless things\n\n\nKahin beet na jaaye raina\nOr else our night will end just around them\n\n\nKuch toh log kahenge\nPeople will say something or the other\n\n\n——\n——-\n\n\nKuch reet jagat ki aisi hai(2)\nSome traditions of the world are such that\n\n\nHar ek subah ki shaam huyi(2)\nEvery morning has had an evening (One thing leads to another)\n\n\nTu kaun hai, tera naam hai kya\nWho are you and what’s your name\n\n\nSita bhi yahan badnaam huyi\nEven Sita has been defamed here\n\n\nPhir kyun sansaar ki baaton se\nThen why with these conversations of the world\n\n\nBheeg gaye tere naina\nHave your eyes become bedewed\n\n\nKuch toh log kahenge\nPeople will say something or the other\n\n\n———-\n————\n\n\nHumko joh taane dete hai(2)\nThose who taunt us saying that\n\n\nHum khoye hai in rang-raliyon mein(2)\nWe’re lost in this debauchery\n\n\nHumne unko bhi chup chupke\nSecretly I’ve also seen them\n\n\nAate dekha in galiyon mein\nComing in these streets\n\n\nYeh sach hai jhoothi baat nahi\nThis is the truth and not a lie\n\n\nTum bolo yeh sach hai na\nYou tell me, isn’t this the truth?\n\n\nKuch toh log kahenge\nPeople will say something or the other\n\n\n…\n…\n\n\n\n\n\nA Woman From Berlin\n\nBetween April 20th and June 22nd of 1945 the anonymous author of A Woman in Berlin wrote about life within the falling city as it was sacked by the Russian Army. Fending off the boredom and deprivation of hiding, the author records her experiences, observations and meditations in this stark and vivid diary. Accounts of the bombing, the rapes, the rationing of food and the overwhelming terror of death are rendered in the dispassionate, though determinedly optimistic prose of a woman fighting for survival amidst the horror and inhumanity of war. This diary was first published in America in 1954 in an English translation and in Britain in 1955. A German language edition was published five years later in Geneva and was met with tremendous controversy. In 2003, over forty years later, it was republished in Germany to critical acclaim - and more controversy. This diary has been unavailable since the 1960s and is now newly translated into English. A Woman in Berlin is an astonishing and deeply affecting account.\nIt has also been made into a movie. https://www.theguardian.com/film/2009/nov/26/anonyma-a-woman-in-berlin.\nHere is the trailer:"
  },
  {
    "objectID": "content/courses/ISTW/Modules/20-France-GuydeMaupassant/index.html#writing-prompts",
    "href": "content/courses/ISTW/Modules/20-France-GuydeMaupassant/index.html#writing-prompts",
    "title": "France - Guy de Maupassant",
    "section": "Writing Prompts",
    "text": "Writing Prompts\n\nWhen You Did Nothing to Prevent Something\nActs of Commission vs Acts of Omission: Which Regret is easier?\nMy Big Fat Friend"
  },
  {
    "objectID": "content/courses/ISTW/Modules/20-France-GuydeMaupassant/index.html#footnotes",
    "href": "content/courses/ISTW/Modules/20-France-GuydeMaupassant/index.html#footnotes",
    "title": "France - Guy de Maupassant",
    "section": "Footnotes",
    "text": "Footnotes\n\nYou should be able to log in to JSTOR from campus.↩︎"
  },
  {
    "objectID": "content/courses/ISTW/Modules/40-Peru-VGCalderon/index.html",
    "href": "content/courses/ISTW/Modules/40-Peru-VGCalderon/index.html",
    "title": "Peru - Ventura Garcia Calderon",
    "section": "",
    "text": "Ventura García Calderón (1886–1959) was a Peruvian man of letters and a diplomat who was at the center of the hispanophone community in Paris in the first half of the twentieth century. Known as a proponent of Spanish American literature, García Calderón achieved a global celebrity for his dramatic, colorful, and ironic short stories. These stories, published in both Spanish and French, feature a raw depiction of reality, a strong sense of retributive justice, and a sympathy for the marginalized people that characterize European Naturalism. García Calderón adapted this style to advance his goal of providing European readers with an authentic understanding of Peru and Spanish America, thus replacing the voyeuristic and patronizing notion of the “exotic” inherited from literary romanticism and nineteenth-century travel writers.\n\n\nThe construction of García Calderón’s stories was subversive and destabilized the widespread notion of Peru held by European critics and readers. While international critics during the author’s lifetime unanimously praised García Calderón’s fiction as well as his essays that theorize the transformation and renaissance of Spanish language and literature by americano writers, scholars since the 1960s have largely misunderstood his reformative project.\n\n\nWe will read Ventura Garcia Calderon’s short short story, The Lottery Ticket\n\n\nMarginalized People\nColour\nWomen and Objectification\nMob Justice\nThe Art of Protest!"
  },
  {
    "objectID": "content/courses/ISTW/Modules/40-Peru-VGCalderon/index.html#ventura-garcia-calderon",
    "href": "content/courses/ISTW/Modules/40-Peru-VGCalderon/index.html#ventura-garcia-calderon",
    "title": "Peru - Ventura Garcia Calderon",
    "section": "",
    "text": "Ventura García Calderón (1886–1959) was a Peruvian man of letters and a diplomat who was at the center of the hispanophone community in Paris in the first half of the twentieth century. Known as a proponent of Spanish American literature, García Calderón achieved a global celebrity for his dramatic, colorful, and ironic short stories. These stories, published in both Spanish and French, feature a raw depiction of reality, a strong sense of retributive justice, and a sympathy for the marginalized people that characterize European Naturalism. García Calderón adapted this style to advance his goal of providing European readers with an authentic understanding of Peru and Spanish America, thus replacing the voyeuristic and patronizing notion of the “exotic” inherited from literary romanticism and nineteenth-century travel writers.\n\n\nThe construction of García Calderón’s stories was subversive and destabilized the widespread notion of Peru held by European critics and readers. While international critics during the author’s lifetime unanimously praised García Calderón’s fiction as well as his essays that theorize the transformation and renaissance of Spanish language and literature by americano writers, scholars since the 1960s have largely misunderstood his reformative project.\n\n\nWe will read Ventura Garcia Calderon’s short short story, The Lottery Ticket\n\n\nMarginalized People\nColour\nWomen and Objectification\nMob Justice\nThe Art of Protest!"
  },
  {
    "objectID": "content/courses/ISTW/Modules/40-Peru-VGCalderon/index.html#additional-material",
    "href": "content/courses/ISTW/Modules/40-Peru-VGCalderon/index.html#additional-material",
    "title": "Peru - Ventura Garcia Calderon",
    "section": "Additional Material",
    "text": "Additional Material\n\n\nKursi Nashin: A Certificate of Discrimination from British India\n\n\n\nReview of The Lottery Ticket https://caponomics.blogspot.com/2013/05/short-story-review-lottery-ticket-by.html\nVentural Garcia Calderon at Short Story Magic Tricks: https://shortstorymagictricks.com/2021/07/06/the-lottery-ticket-by-ventura-garcia-calderon/\nGoldberg, N. S. (2014). Rereading Ventura García Calderón. Hispania, 97(2), 220–232. http://www.jstor.org/stable/24368768\n\nNotes and References\n\nRene Girard’s Mimetic Theory and the Scapegoat: https://violenceandreligion.com/mimetic-theory/\nFrear, G. L. (1992). René Girard on Mimesis, Scapegoats, and Ethics. The Annual of the Society of Christian Ethics, 12, 115–133. http://www.jstor.org/stable/23559770\nThe Beauty of Fat Women: Leonard Nimoy (“Mr Spock”) and his Full Body Project: https://www.theguardian.com/artanddesign/gallery/2015/mar/03/the-full-body-project-by-leonard-nimoy-in-pictures\n\n\nLeonard Nimoy, who died February 27,2015 at the age of 83, was beloved by fans for his distinctive portrayal of Mr. Spock on Star Trek. Those fans may not have known that Nimoy, through his work as a photographer, also championed women who did not conform to Hollywood’s ideal of physical perfection. [see also http://mashable.com/2015/02/26/body-positivity-get-involved In 2007, Nimoy published The Full Body Project, a collection of photos featuring nude women of many shapes and sizes. Nimoy’s previous book of photographs captured images of nude women as well, though the models’ slim bodies hewed closely to the conventional standards of beauty. The inspiration for The Fully Body Project struck when a full-figured woman approached Nimoy and asked if he might photograph her and her friends.\n\nListen to Leonard Nimoy discuss this project on NPR:\n\n\n\nIt Ain’t Over until the Fat Lady Sings: https://knowyourphrase.com/aint-over-until-the-fat-lady-sings\nIs it all about Hips? http://bollynatyam.com/books/\n\n\n\n\n\n\n\n\n\nhttps://www.youtube.com/embed/ByAbV-MKDgs?si=PU26bRl2sXOynWpL\nSongs for the Story !!\n\n\nSong for the Black Hero of this story:\nTitle: What About Me?\nBand: Moving Pictures\nBand Location: Sydney, NSW, Australia\nAlbum: Days Of Innocence\nComposed By: Garry Frost, Frances Swan\nRelease Date: January, 1982\nChart Position:\n\nNo.1 (Australia)\nNo.29 (US Billboard Hot 100)\n\n\n{{% youtube \"OzQKECQgjW8\" %}}\n\n\n\nSong for Cielito:\nTitle: Bette Davis Eyes\nArtiste: Kim Carnes\nAlbum: Mistaken Identity\nComposed by: Donna Weiss and Jackie DeShannon\nYear: 1981\nChart Position:\n\nNo.1 (Australia)\nNo.1 (US Billboard Hot 100)\nNo. 10 (UK Singles Charts)\n\n\n\n\n{{% youtube \"2Wdu5FYGTRs\" %}}\nThe “Lottery” Idea in Literature\n\nShirley Jackson, The Lottery. https://www.newyorker.com/magazine/1948/06/26/the-lottery\nMunshi Premchand. Lottery, short story in Hindi. https://en.wikipedia.org/wiki/Lottery_(short_story)\nAnton Chekhov, The Lottery Ticket. https://www.classicshorts.com/stories/lottery.html\nJose Luis Borges. The Lottery of Babylon."
  },
  {
    "objectID": "content/courses/ISTW/Modules/40-Peru-VGCalderon/index.html#writing-prompt",
    "href": "content/courses/ISTW/Modules/40-Peru-VGCalderon/index.html#writing-prompt",
    "title": "Peru - Ventura Garcia Calderon",
    "section": "Writing Prompt",
    "text": "Writing Prompt\n\nHow I went on Strike\nOn an ad for Fairness Cream\nOn a personal encounter with racism/bias\nDid I win the Lottery?\nCompare Cielito with the Prostitute in Guy de Maupassant’s story Boule de Suif."
  },
  {
    "objectID": "content/courses/ISTW/Modules/220-India-BharatiMukherjee/index.html#themes",
    "href": "content/courses/ISTW/Modules/220-India-BharatiMukherjee/index.html#themes",
    "title": "India - Bharati Mukherjee",
    "section": "Themes",
    "text": "Themes\nNotes and References\nSong for the Story\nBilly Currington’s Country hit from 2008, Walk a Little Straighter, Daddy\n{{% youtube \"U1no7Or9BeI\" %}}"
  },
  {
    "objectID": "content/courses/ISTW/Modules/190-USA-CarsonMcCullers/index.html",
    "href": "content/courses/ISTW/Modules/190-USA-CarsonMcCullers/index.html",
    "title": "USA-Carson McCullers",
    "section": "",
    "text": "We will read McCullers’ astonishing story, A Rock. A Tree. A Cloud. PDF\n\n\nWhat we might call weirdness in some people…wrongly!!\nHow to Know someone has Figured it All Out\nHow to Live\nBad Places and Bad Habits and Bad People…etc."
  },
  {
    "objectID": "content/courses/ISTW/Modules/190-USA-CarsonMcCullers/index.html#carson-mccullers",
    "href": "content/courses/ISTW/Modules/190-USA-CarsonMcCullers/index.html#carson-mccullers",
    "title": "USA-Carson McCullers",
    "section": "",
    "text": "We will read McCullers’ astonishing story, A Rock. A Tree. A Cloud. PDF\n\n\nWhat we might call weirdness in some people…wrongly!!\nHow to Know someone has Figured it All Out\nHow to Live\nBad Places and Bad Habits and Bad People…etc."
  },
  {
    "objectID": "content/courses/ISTW/Modules/190-USA-CarsonMcCullers/index.html#notes-and-references",
    "href": "content/courses/ISTW/Modules/190-USA-CarsonMcCullers/index.html#notes-and-references",
    "title": "USA-Carson McCullers",
    "section": "Notes and References",
    "text": "Notes and References\nSongs for the Story !!\n\n\nAnd sadly, Renowned country singer Toby Keith dies at 62 after battle with stomach cancer.\nAnd so, maybe this one…\n\n\nAdditional Readings\n\nThe Arrested Development of Carson McCullers. https://www.newyorker.com/magazine/2024/03/04/carson-mccullers-a-life-mary-v-dearborn-book-review. “A writer lives differently from people who don’t write.” Of course.\n“Pretend it’s a Seed, OK?” https://youtu.be/wKbUVxjh0mg"
  },
  {
    "objectID": "content/courses/ISTW/Modules/190-USA-CarsonMcCullers/index.html#writing-prompts",
    "href": "content/courses/ISTW/Modules/190-USA-CarsonMcCullers/index.html#writing-prompts",
    "title": "USA-Carson McCullers",
    "section": "Writing Prompts",
    "text": "Writing Prompts\n\n“So long as you learn, it doesn’t matter who you learn from, does it?” ( Quote from “To Sir, With Love”)\nOn being scolded for being in Bad Company\nLife Lessons from Graffiti?\nOn seeing Artists and Designers everywhere…(bah!)\nA Strange Transformational Hobby…\nOn being a Gig Worker"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/listing.html",
    "href": "content/courses/Analytics/Predictive/listing.html",
    "title": "Predictive Analytics",
    "section": "",
    "text": "🐉 Intro to Orange\n\n\n\n\n\nUsing A Visual drag and drop tool called Orange\n\n\n\n\n\nOct 17, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\n\n\n\n\n\n\nML - Regression\n\n\n\n\n\n\nLinear Regression\n\n\nTrend Line\n\n\nFrancis Galton\n\n\n\nUsing Linear Regression to Predict Numerical Data\n\n\n\n\n\nAug 16, 2022\n\n\nArvind Venkatadri\n\n\n\n\n\n\n\n\n\n\n\n\nML - Classification\n\n\n\n\n\nWe will look at the basic models for Classification of Data\n\n\n\n\n\nJul 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nML - Clustering\n\n\n\n\n\nWe will look at the basic models for Clustering of Data.\n\n\n\n\n\nJul 19, 2022\n\n\n\n\n\n\nNo matching items\n\n Back to top",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Predictive Modelling"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/20-Regression/index.html",
    "href": "content/courses/Analytics/Predictive/Modules/20-Regression/index.html",
    "title": "ML - Regression",
    "section": "",
    "text": "Interpolation:\n\nbetween TWO colours, both colours inclusive using a straight line between them\nbetween several different colours?\n\nby mixing “equal proportions” of each\nProportions based on “distance” from each colour\nOn a “plane” with these points",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Predictive Modelling",
      "ML - Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/20-Regression/index.html#workflow-in-orange",
    "href": "content/courses/Analytics/Predictive/Modules/20-Regression/index.html#workflow-in-orange",
    "title": "ML - Regression",
    "section": "Workflow in Orange",
    "text": "Workflow in Orange\nLet us “draw inspiration” from the picture above, and see if we can replicate it. We will fire up Orange, paint some data and see if we can fit a linear regression ML model to it.\nHere is the Orange file for you to download. Open this file in Orange.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Predictive Modelling",
      "ML - Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/20-Regression/index.html#workflow-in-radiant",
    "href": "content/courses/Analytics/Predictive/Modules/20-Regression/index.html#workflow-in-radiant",
    "title": "ML - Regression",
    "section": "Workflow in Radiant",
    "text": "Workflow in Radiant",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Predictive Modelling",
      "ML - Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/20-Regression/index.html#workflow-in-r",
    "href": "content/courses/Analytics/Predictive/Modules/20-Regression/index.html#workflow-in-r",
    "title": "ML - Regression",
    "section": "Workflow in R",
    "text": "Workflow in R",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Predictive Modelling",
      "ML - Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/20-Regression/index.html#conclusion",
    "href": "content/courses/Analytics/Predictive/Modules/20-Regression/index.html#conclusion",
    "title": "ML - Regression",
    "section": "Conclusion",
    "text": "Conclusion",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Predictive Modelling",
      "ML - Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/files/Random-Forests.html",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/files/Random-Forests.html",
    "title": "Random Forests",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE)\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(prettydoc)\nlibrary(corrplot)\nlibrary(ggformula)\nlibrary(palmerpenguins) # Allison Horst's `penguins` data.\n##\nlibrary(tidymodels)\nlibrary(dials)\nlibrary(modeldata)\nlibrary(rsample)\nlibrary(recipes)\nlibrary(yardstick)\nlibrary(parsnip)"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/files/Random-Forests.html#penguin-random-forest-model-withrandomforest",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/files/Random-Forests.html#penguin-random-forest-model-withrandomforest",
    "title": "Random Forests",
    "section": "Penguin Random Forest Model withrandomForest\n",
    "text": "Penguin Random Forest Model withrandomForest\n\nUsing the penguins dataset and Random Forest Classification.\n\npenguins\n\n\n  \n\n\nsummary(penguins)\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2                                 \n\npenguins %&gt;% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n344\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nspecies\n0\n1.00\nFALSE\n3\nAde: 152, Gen: 124, Chi: 68\n\n\nisland\n0\n1.00\nFALSE\n3\nBis: 168, Dre: 124, Tor: 52\n\n\nsex\n11\n0.97\nFALSE\n2\nmal: 168, fem: 165\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nbill_length_mm\n2\n0.99\n43.92\n5.46\n32.1\n39.23\n44.45\n48.5\n59.6\n▃▇▇▆▁\n\n\nbill_depth_mm\n2\n0.99\n17.15\n1.97\n13.1\n15.60\n17.30\n18.7\n21.5\n▅▅▇▇▂\n\n\nflipper_length_mm\n2\n0.99\n200.92\n14.06\n172.0\n190.00\n197.00\n213.0\n231.0\n▂▇▃▅▂\n\n\nbody_mass_g\n2\n0.99\n4201.75\n801.95\n2700.0\n3550.00\n4050.00\n4750.0\n6300.0\n▃▇▆▃▂\n\n\nyear\n0\n1.00\n2008.03\n0.82\n2007.0\n2007.00\n2008.00\n2009.0\n2009.0\n▇▁▇▁▇\n\n\n\n\npenguins &lt;- penguins %&gt;% tidyr::drop_na()\n# Spent one hour trying to find `drop-na()` (14 June 2020)\n\n\n# library(corrplot)\ncor &lt;- penguins %&gt;% select(where(is.numeric)) %&gt;% cor() \ncor %&gt;% corrplot(., method = \"ellipse\", order = \"hclust\",tl.cex = 1.0,)\n\n\n\n\n\n\n# try these too:\n# cor %&gt;% corrplot(., method = \"square\", order = \"hclust\",tl.cex = 0.5)\n# cor %&gt;% corrplot(., method = \"color\", order = \"hclust\",tl.cex = 0.5)\n# cor %&gt;% corrplot(., method = \"shade\", order = \"hclust\",tl.cex = 0.5)\n\nNotes: - flipper_length_mm and culmen_depth_mm are negatively correlated at approx (-0.7) - flipper_length_mm and body_mass_g are positively correlated at approx 0.8\nSo we will use steps in the recipe to remove correlated variables.\nPenguin Data Sampling and Recipe\n\n# Data Split\npenguin_split &lt;- initial_split(penguins, prop = 0.6)\npenguin_train &lt;- training(penguin_split)\npenguin_test &lt;- testing(penguin_split)\npenguin_split\n\n&lt;Training/Testing/Total&gt;\n&lt;199/134/333&gt;\n\nhead(penguin_train)\n\n\n  \n\n\n# Recipe\npenguin_recipe &lt;- penguins %&gt;% \n  recipe(species ~ .) %&gt;% \n  step_normalize(all_numeric()) %&gt;% # Scaling and Centering\n  step_corr(all_numeric()) %&gt;%  # Handling correlated variables\n  prep()\n\n# Baking the data\npenguin_train_baked &lt;-  penguin_train %&gt;% \n  bake(object = penguin_recipe, new_data = .)\n\npenguin_test_baked &lt;-  penguin_test %&gt;% \n  bake(object = penguin_recipe, new_data = .)\n\nhead(penguin_train_baked)\n\n\n  \n\n\n\nPenguin Random Forest Model\n\npenguin_model &lt;- \n  rand_forest(trees = 100) %&gt;% \n  set_engine(\"randomForest\") %&gt;% \n  set_mode(\"classification\")\npenguin_model\n\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  trees = 100\n\nComputational engine: randomForest \n\npenguin_fit &lt;- \n  penguin_model %&gt;% \n  fit(species ~ .,penguin_train_baked)\npenguin_fit\n\nparsnip model object\n\n\nCall:\n randomForest(x = maybe_data_frame(x), y = y, ntree = ~100) \n               Type of random forest: classification\n                     Number of trees: 100\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 1.51%\nConfusion matrix:\n          Adelie Chinstrap Gentoo class.error\nAdelie        83         2      0  0.02352941\nChinstrap      1        40      0  0.02439024\nGentoo         0         0     73  0.00000000\n\n# iris_ranger &lt;- \n#   rand_forest(trees = 100) %&gt;% \n#   set_mode(\"classification\") %&gt;% \n#   set_engine(\"ranger\") %&gt;% \n#   fit(Species ~ ., data = iris_training_baked)\n\nMetrics for the Penguin Random Forest Model\n\n# Predictions\npredict(object = penguin_fit, new_data = penguin_test_baked) %&gt;%  \n  dplyr::bind_cols(penguin_test_baked) %&gt;% \n  glimpse()\n\nRows: 134\nColumns: 9\n$ .pred_class       &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; -0.6752636, -1.3335592, -0.9312674, -1.7175649, -1.3…\n$ bill_depth_mm     &lt;dbl&gt; 0.42409105, 1.08424573, 0.32252879, 1.99830605, 0.32…\n$ flipper_length_mm &lt;dbl&gt; -0.42573251, -0.56842897, -1.42460769, -0.21168783, …\n$ body_mass_g       &lt;dbl&gt; -1.188572125, -0.940191505, -0.722858463, 0.23961643…\n$ sex               &lt;fct&gt; female, female, female, male, female, male, male, ma…\n$ year              &lt;dbl&gt; -1.2818130, -1.2818130, -1.2818130, -1.2818130, -1.2…\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n\n# Prediction Accuracy Metrics\npredict(object = penguin_fit, new_data = penguin_test_baked) %&gt;%  \n  dplyr::bind_cols(penguin_test_baked) %&gt;% \n  yardstick::metrics(truth = species, estimate = .pred_class)\n\n\n  \n\n\n# Prediction Probabilities\npenguin_fit_probs &lt;- \n  predict(penguin_fit, penguin_test_baked, type = \"prob\") %&gt;%\n  dplyr::bind_cols(penguin_test_baked)\nglimpse(penguin_fit_probs)\n\nRows: 134\nColumns: 11\n$ .pred_Adelie      &lt;dbl&gt; 0.98, 0.98, 1.00, 0.89, 1.00, 0.74, 0.51, 1.00, 1.00…\n$ .pred_Chinstrap   &lt;dbl&gt; 0.02, 0.02, 0.00, 0.11, 0.00, 0.24, 0.44, 0.00, 0.00…\n$ .pred_Gentoo      &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.02, 0.05, 0.00, 0.00…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; -0.6752636, -1.3335592, -0.9312674, -1.7175649, -1.3…\n$ bill_depth_mm     &lt;dbl&gt; 0.42409105, 1.08424573, 0.32252879, 1.99830605, 0.32…\n$ flipper_length_mm &lt;dbl&gt; -0.42573251, -0.56842897, -1.42460769, -0.21168783, …\n$ body_mass_g       &lt;dbl&gt; -1.188572125, -0.940191505, -0.722858463, 0.23961643…\n$ sex               &lt;fct&gt; female, female, female, male, female, male, male, ma…\n$ year              &lt;dbl&gt; -1.2818130, -1.2818130, -1.2818130, -1.2818130, -1.2…\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n\n# Confusion Matrix\npenguin_fit$fit$confusion %&gt;% tidy()\n\n\n  \n\n\n# Gain Curves\npenguin_fit_probs %&gt;% \n  yardstick::gain_curve(species, .pred_Adelie:.pred_Gentoo) %&gt;%\n  autoplot()\n\n\n\n\n\n\n# ROC Plot\npenguin_fit_probs%&gt;%\n  roc_curve(species, .pred_Adelie:.pred_Gentoo) %&gt;%\n  autoplot()\n\n\n\n\n\n\n\nUsing broom on the penguin model\n\npenguin_split\n\n&lt;Training/Testing/Total&gt;\n&lt;199/134/333&gt;\n\npenguin_split %&gt;% broom::tidy()\n\n\n  \n\n\npenguin_recipe %&gt;% broom::tidy()\n\n\n  \n\n\n# Following do not work for `random forest models` !! ;-()\n#penguin_model %&gt;% tidy()\n#penguin_fit %&gt;% tidy() \npenguin_model %&gt;% str()\n\nList of 7\n $ args                 :List of 3\n  ..$ mtry : language ~NULL\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_EmptyEnv&gt; \n  ..$ trees: language ~100\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_EmptyEnv&gt; \n  ..$ min_n: language ~NULL\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_EmptyEnv&gt; \n $ eng_args             : Named list()\n  ..- attr(*, \"class\")= chr [1:2] \"quosures\" \"list\"\n $ mode                 : chr \"classification\"\n $ user_specified_mode  : logi TRUE\n $ method               : NULL\n $ engine               : chr \"randomForest\"\n $ user_specified_engine: logi TRUE\n - attr(*, \"class\")= chr [1:2] \"rand_forest\" \"model_spec\"\n\npenguin_test_baked"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/files/Random-Forests.html#iris-random-forest-model-with-ranger",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/files/Random-Forests.html#iris-random-forest-model-with-ranger",
    "title": "Random Forests",
    "section": "Iris Random Forest Model with ranger\n",
    "text": "Iris Random Forest Model with ranger\n\nUsing the iris dataset and Random Forest Classification. This part uses rsample to split the data and the recipes to prep the data for model making.\n\n#set.seed(100)\niris_split &lt;- rsample::initial_split(iris, prop = 0.6)\niris_split\n\n&lt;Training/Testing/Total&gt;\n&lt;90/60/150&gt;\n\niris_split %&gt;% training() %&gt;% glimpse()\n\nRows: 90\nColumns: 5\n$ Sepal.Length &lt;dbl&gt; 6.5, 6.4, 4.6, 4.8, 5.0, 5.8, 5.4, 6.5, 5.0, 5.7, 4.5, 5.…\n$ Sepal.Width  &lt;dbl&gt; 3.2, 3.2, 3.1, 3.1, 2.3, 2.7, 3.0, 3.0, 2.0, 2.9, 2.3, 2.…\n$ Petal.Length &lt;dbl&gt; 5.1, 5.3, 1.5, 1.6, 3.3, 5.1, 4.5, 5.8, 3.5, 4.2, 1.3, 5.…\n$ Petal.Width  &lt;dbl&gt; 2.0, 2.3, 0.2, 0.2, 1.0, 1.9, 1.5, 2.2, 1.0, 1.3, 0.3, 1.…\n$ Species      &lt;fct&gt; virginica, virginica, setosa, setosa, versicolor, virgini…\n\niris_split %&gt;% testing() %&gt;% glimpse()\n\nRows: 60\nColumns: 5\n$ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 5.0, 5.0, 4.4, 4.8, 4.3, 5.8, 5.7, 5.1, 5.1, 4.…\n$ Sepal.Width  &lt;dbl&gt; 3.5, 3.0, 3.6, 3.4, 2.9, 3.0, 3.0, 4.0, 3.8, 3.8, 3.7, 3.…\n$ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.4, 1.5, 1.4, 1.4, 1.1, 1.2, 1.7, 1.5, 1.5, 1.…\n$ Petal.Width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.1, 0.1, 0.2, 0.3, 0.3, 0.4, 0.…\n$ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n\nIris Data Pre-Processing: Creating the Recipe\nThe recipes package provides an interface that specializes in data pre-processing. Within the package, the functions that start, or execute, the data transformations are named after cooking actions. That makes the interface more user-friendly. For example:\n\nrecipe() - Starts a new set of transformations to be applied, similar to the ggplot() command. Its main argument is the model’s formula.\nprep() - Executes the transformations on top of the data that is supplied (typically, the training data). Each data transformation is a step() function. ( Recall what we did with the caret package: Centering, Scaling, Removing Correlated variables…)\n\nNote that in order to avoid data leakage (e.g: transferring information from the train set into the test set), data should be “prepped” using the train_tbl only. https://towardsdatascience.com/modelling-with-tidymodels-and-parsnip-bae2c01c131c CRAN: The idea is that the preprocessing operations will all be created using the training set and then these steps will be applied to both the training and test set.\n\n# Pre Processing the Training Data\n\niris_recipe &lt;- \n  training(iris_split) %&gt;% # Note: Using TRAINING data !!\n  recipe(Species ~ .)      # Note: Outcomes ~ Predictors !!\n\n# The data contained in the `data` argument need not be the training set; this data is only used to catalog the names of the variables and their types (e.g. numeric, etc.).\n\nQ: How does the recipe “figure” out which are the outcomes and which are the predictors? A.The recipe command defines Outcomes and Predictors using the formula interface. Not clear how this recipe “figures” out which are the outcomes and which are the predictors, when we have not yet specified them…\nQ. Why is the recipe not agnostic to data set? Is that a meaningful question? A. The use of the training set in the recipe command is just to declare the variables and specify the roles of the data, nothing else. Roles are open-ended and extensible. From https://cran.r-project.org/web/packages/recipes/vignettes/Simple_Example.html :\n\nThis document demonstrates some basic uses of recipes. First, some definitions are required: - variables are the original (raw) data columns in a data frame or tibble. For example, in a traditional formula Y ~ A + B + A:B, the variables are A, B, and Y. - roles define how variables will be used in the model. Examples are: predictor (independent variables), response, and case weight. This is meant to be open-ended and extensible. - terms are columns in a design matrix such as A, B, and A:B. These can be other derived entities that are grouped, such as a set of principal components or a set of columns, that define a basis function for a variable. These are synonymous with features in machine learning. Variables that have predictor roles would automatically be main effect terms.\n\n\n# Apply the transformation steps\niris_recipe &lt;- iris_recipe %&gt;% \n  step_corr(all_predictors()) %&gt;% \n  step_center(all_predictors(), -all_outcomes()) %&gt;% \n  step_scale(all_predictors(), -all_outcomes()) %&gt;% \n  prep()\n\nThis has created the recipe() and prepped it too. We now need to apply it to our datasets:\n\nTake training data and bake() it to prepare it for modelling.\nDo the same for the testing set.\n\n\niris_training_baked &lt;- \n  iris_split %&gt;% \n  training() %&gt;% \n  bake(iris_recipe,.)\niris_training_baked\n\n\n  \n\n\niris_testing_baked &lt;- \n  iris_split %&gt;% \n  testing() %&gt;% \n  bake(iris_recipe,.)\niris_testing_baked \n\n\n  \n\n\n\nIris Model Training using parsnip\n\nDifferent ML packages provide different interfaces (APIs ) to do the same thing (e.g random forests). The tidymodels package provides a consistent interface to invoke a wide variety of packages supporting a wide variety of models.\nThe parsnip package is a successor to caret.\nTo model with parsnip: 1. Pick a model : 2. Set the engine 3. Set the mode (if needed): Classification or Regression\nCheck here for models available in parsnip.\n\nMode: classification and regression in parsnip, each using a variety of models. ( Which Way). This defines the form of the output.\nEngine: The engine is the R package that is invoked by parsnip to execute the model. E.g glm, glmnet,keras.( How ) parsnip provides wrappers for models from these packages.\nModel: is the specific technique used for the modelling task. E.g linear_reg(), logistic_reg(), mars, decision_tree, nearest_neighbour…(What model).\n\nand models have: - hyperparameters: that are numerical or factor variables that tune the model ( Like the alpha beta parameters for Bayesian priors)\nWe can use the random forest model to classify the iris into species. Here Species is the Outcome variable and the rest are predictor variables. The random forest model is provided by the ranger package, to which tidymodels/parsnip provides a simple and consistent interface.\n\nlibrary(ranger)\niris_ranger &lt;- \n  rand_forest(trees = 100) %&gt;% \n  set_mode(\"classification\") %&gt;% \n  set_engine(\"ranger\") %&gt;% \n  fit(Species ~ ., data = iris_training_baked)\n\nranger can generate random forest models for classification, regression, survival( time series, time to event stuff). Extreme Forests are also supported, wherein all points in the dataset are used ( instead of bootstrap samples) along with feature bagging. We can also run the same model using the randomForest package:\n\nlibrary(randomForest,quietly = TRUE)\niris_rf &lt;- \n  rand_forest(trees = 100) %&gt;% \n  set_mode(\"classification\") %&gt;% \n  set_engine(\"randomForest\") %&gt;% \n  fit(Species ~ ., data = iris_training_baked)\n\nIris Predictions\nThe predict() function run against a parsnip model returns a prediction tibble. By default, the prediction variable is called .pred_class.\n\npredict(object = iris_ranger, new_data = iris_testing_baked) %&gt;%  \n  dplyr::bind_cols(iris_testing_baked) %&gt;% \n  glimpse()\n\nRows: 60\nColumns: 5\n$ .pred_class  &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n$ Sepal.Length &lt;dbl&gt; -0.9583431, -1.1940012, -1.0761721, -1.0761721, -1.783146…\n$ Sepal.Width  &lt;dbl&gt; 1.0840679, -0.1264298, 1.3261675, 0.8419684, -0.3685293, …\n$ Petal.Width  &lt;dbl&gt; -1.39841778, -1.39841778, -1.39841778, -1.39841778, -1.39…\n$ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n\nIris Classification Model Validation\nWe use metrics() function from the yardstick package to evaluate how good the model is.\n\npredict(iris_ranger, iris_testing_baked) %&gt;%\n  dplyr::bind_cols(iris_testing_baked) %&gt;% \n  yardstick::metrics(truth = Species, estimate = .pred_class)\n\n\n  \n\n\n\nWe can also check the metrics for randomForest model:\n\npredict(iris_rf, iris_testing_baked) %&gt;%\n  dplyr::bind_cols(iris_testing_baked) %&gt;% \n  yardstick::metrics(truth = Species, estimate = .pred_class)\n\n\n  \n\n\n\nIris Per-Classifier Metrics\nWe can use the parameter type = \"prob\" in the predict() function to obtain a probability score on each prediction. TBD: How is this prob calculated? Possible answer: the Random Forest model outputs its answer by majority voting across n trees. Each of the possible answers( i.e. predictions) for a particular test datum gets a share of the vote, that represents its probability. Hence each dataum in the test vector can show a probability for the “winning” answer. ( Quite possibly we can get the probabilities for all possible outcomes for each test datum)\n\niris_ranger_probs &lt;- \n  predict(iris_ranger, iris_testing_baked, type = \"prob\") %&gt;%\n  dplyr::bind_cols(iris_testing_baked)\nglimpse(iris_ranger_probs)\n\nRows: 60\nColumns: 7\n$ .pred_setosa     &lt;dbl&gt; 0.990833333, 0.726757937, 0.996000000, 0.996000000, 0…\n$ .pred_versicolor &lt;dbl&gt; 0.009166667, 0.264353175, 0.004000000, 0.004000000, 0…\n$ .pred_virginica  &lt;dbl&gt; 0.000000000, 0.008888889, 0.000000000, 0.000000000, 0…\n$ Sepal.Length     &lt;dbl&gt; -0.9583431, -1.1940012, -1.0761721, -1.0761721, -1.78…\n$ Sepal.Width      &lt;dbl&gt; 1.0840679, -0.1264298, 1.3261675, 0.8419684, -0.36852…\n$ Petal.Width      &lt;dbl&gt; -1.39841778, -1.39841778, -1.39841778, -1.39841778, -…\n$ Species          &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos…\n\niris_rf_probs &lt;- \n  predict(iris_rf, iris_testing_baked, type = \"prob\") %&gt;%\n  dplyr::bind_cols(iris_testing_baked)\nglimpse(iris_rf_probs)\n\nRows: 60\nColumns: 7\n$ .pred_setosa     &lt;dbl&gt; 1.00, 0.78, 0.99, 1.00, 0.78, 0.81, 0.82, 0.77, 0.85,…\n$ .pred_versicolor &lt;dbl&gt; 0.00, 0.22, 0.00, 0.00, 0.22, 0.19, 0.18, 0.12, 0.09,…\n$ .pred_virginica  &lt;dbl&gt; 0.00, 0.00, 0.01, 0.00, 0.00, 0.00, 0.00, 0.11, 0.06,…\n$ Sepal.Length     &lt;dbl&gt; -0.9583431, -1.1940012, -1.0761721, -1.0761721, -1.78…\n$ Sepal.Width      &lt;dbl&gt; 1.0840679, -0.1264298, 1.3261675, 0.8419684, -0.36852…\n$ Petal.Width      &lt;dbl&gt; -1.39841778, -1.39841778, -1.39841778, -1.39841778, -…\n$ Species          &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos…\n\n# Tabulating the probabilities\nftable(iris_rf_probs$.pred_versicolor)\n\n  0 0.01 0.03 0.04 0.06 0.07 0.08 0.09 0.12 0.17 0.18 0.19 0.22 0.28 0.37 0.38 0.45 0.65 0.66 0.67 0.68 0.69 0.74 0.76 0.81 0.82 0.84 0.87 0.88 0.9 0.92 0.94 0.95 0.96 0.97 0.98 0.99\n                                                                                                                                                                                      \n 14    3    2    3    1    1    2    1    1    1    2    1    2    1    1    1    1    1    1    1    2    1    1    1    2    1    1    1    1   1    1    1    1    1    1    1    1\n\nftable(iris_rf_probs$.pred_virginica)\n\n  0 0.01 0.02 0.03 0.05 0.06 0.08 0.11 0.12 0.13 0.15 0.18 0.24 0.25 0.3 0.32 0.33 0.34 0.5 0.53 0.63 0.83 0.89 0.92 0.93 0.94 0.95 0.96 0.97 0.98  1\n                                                                                                                                                     \n 16    5    5    1    2    1    1    1    1    1    1    3    1    1   1    2    1    1   1    1    1    1    1    1    1    1    1    3    1    1  1\n\nftable(iris_rf_probs$.pred_setosa)\n\n  0 0.01 0.02 0.03 0.04 0.05 0.12 0.17 0.18 0.72 0.77 0.78 0.81 0.82 0.85 0.97 0.98 0.99  1\n                                                                                           \n 23    5    1    1    3    1    1    1    1    1    1    2    1    2    1    1    4    2  8\n\n\nIris Classifier: Gain and ROC Curves\nWe can plot gain and ROC curves for each of these models\n\niris_ranger_probs %&gt;% \n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %&gt;% \n  glimpse()\n\nRows: 135\nColumns: 5\n$ .level          &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"set…\n$ .n              &lt;dbl&gt; 0, 1, 4, 9, 11, 12, 13, 14, 15, 16, 19, 20, 21, 22, 23…\n$ .n_events       &lt;dbl&gt; 0, 1, 4, 9, 11, 12, 13, 14, 15, 16, 19, 20, 21, 22, 23…\n$ .percent_tested &lt;dbl&gt; 0.000000, 1.666667, 6.666667, 15.000000, 18.333333, 20…\n$ .percent_found  &lt;dbl&gt; 0.000000, 4.347826, 17.391304, 39.130435, 47.826087, 5…\n\niris_ranger_probs %&gt;% \n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %&gt;% \n  autoplot()\n\n\n\n\n\n\niris_ranger_probs %&gt;% \n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %&gt;% \n  glimpse()\n\nRows: 138\nColumns: 4\n$ .level      &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\"…\n$ .threshold  &lt;dbl&gt; -Inf, 0.000000000, 0.001111111, 0.003333333, 0.004111111, …\n$ specificity &lt;dbl&gt; 0.0000000, 0.0000000, 0.1891892, 0.3513514, 0.3783784, 0.4…\n$ sensitivity &lt;dbl&gt; 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0…\n\niris_ranger_probs %&gt;% \n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n\niris_rf_probs %&gt;% \n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %&gt;% \n  glimpse()\n\nRows: 90\nColumns: 5\n$ .level          &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"set…\n$ .n              &lt;dbl&gt; 0, 8, 10, 14, 15, 16, 18, 19, 21, 22, 23, 24, 25, 26, …\n$ .n_events       &lt;dbl&gt; 0, 8, 10, 14, 15, 16, 18, 19, 21, 22, 23, 23, 23, 23, …\n$ .percent_tested &lt;dbl&gt; 0.000000, 13.333333, 16.666667, 23.333333, 25.000000, …\n$ .percent_found  &lt;dbl&gt; 0.000000, 34.782609, 43.478261, 60.869565, 65.217391, …\n\niris_rf_probs %&gt;% \n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %&gt;% \n  autoplot()\n\n\n\n\n\n\niris_rf_probs %&gt;% \n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %&gt;% \n  glimpse()\n\nRows: 93\nColumns: 4\n$ .level      &lt;chr&gt; \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\"…\n$ .threshold  &lt;dbl&gt; -Inf, 0.00, 0.01, 0.02, 0.03, 0.04, 0.05, 0.12, 0.17, 0.18…\n$ specificity &lt;dbl&gt; 0.0000000, 0.0000000, 0.6216216, 0.7567568, 0.7837838, 0.8…\n$ sensitivity &lt;dbl&gt; 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0…\n\niris_rf_probs %&gt;% \n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\nIris Classifier: Metrics\n\npredict(iris_ranger, iris_testing_baked, type = \"prob\") %&gt;% \n  bind_cols(predict(iris_ranger,iris_testing_baked)) %&gt;% \n  bind_cols(select(iris_testing_baked,Species)) %&gt;% \n  glimpse()\n\nRows: 60\nColumns: 5\n$ .pred_setosa     &lt;dbl&gt; 0.990833333, 0.726757937, 0.996000000, 0.996000000, 0…\n$ .pred_versicolor &lt;dbl&gt; 0.009166667, 0.264353175, 0.004000000, 0.004000000, 0…\n$ .pred_virginica  &lt;dbl&gt; 0.000000000, 0.008888889, 0.000000000, 0.000000000, 0…\n$ .pred_class      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos…\n$ Species          &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos…\n\n# predict(iris_ranger, iris_testing_baked, type = \"prob\") %&gt;% \n#   bind_cols(predict(iris_ranger,iris_testing_baked)) %&gt;% \n#   bind_cols(select(iris_testing_baked,Species)) %&gt;% \n#   yardstick::metrics(data = ., truth = Species, estimate = .pred_class, ... = .pred_setosa:.pred_virginica)\n\n\n# And for the `randomForest`method\n\npredict(iris_rf, iris_testing_baked, type = \"prob\") %&gt;% \n  bind_cols(predict(iris_ranger,iris_testing_baked)) %&gt;% \n  bind_cols(select(iris_testing_baked,Species)) %&gt;% \n  glimpse()\n\nRows: 60\nColumns: 5\n$ .pred_setosa     &lt;dbl&gt; 1.00, 0.78, 0.99, 1.00, 0.78, 0.81, 0.82, 0.77, 0.85,…\n$ .pred_versicolor &lt;dbl&gt; 0.00, 0.22, 0.00, 0.00, 0.22, 0.19, 0.18, 0.12, 0.09,…\n$ .pred_virginica  &lt;dbl&gt; 0.00, 0.00, 0.01, 0.00, 0.00, 0.00, 0.00, 0.11, 0.06,…\n$ .pred_class      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos…\n$ Species          &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos…\n\n# predict(iris_rf, iris_testing_baked, type = \"prob\") %&gt;% \n#   bind_cols(predict(iris_ranger,iris_testing_baked)) %&gt;% \n#   bind_cols(select(iris_testing_baked,Species)) %&gt;% \n#   yardstick::metrics(data = ., truth = Species, estimate = .pred_class, ... = .pred_setosa:.pred_virginica)"
  },
  {
    "objectID": "content/courses/Analytics/Predictive/Modules/30-Classification/files/Random-Forests.html#references",
    "href": "content/courses/Analytics/Predictive/Modules/30-Classification/files/Random-Forests.html#references",
    "title": "Random Forests",
    "section": "References",
    "text": "References\n\nMachine Learning Basics - Random Forest at Shirin’s Playground"
  },
  {
    "objectID": "content/courses/Analytics/Tools/listing.html",
    "href": "content/courses/Analytics/Tools/listing.html",
    "title": "Tools and Software",
    "section": "",
    "text": "🐉 Introduction to R and RStudio\n\n\n\n\n\n\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\n🐉 Introduction to Radiant\n\n\n\n\n\n\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\n🐉 Introduction to Orange\n\n\n\n\n\n\n\n\n2 min\n\n\n\n\n\n\nNo matching items\n\n Back to top",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Tools"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/30-Intro-to-Orange/orange.html",
    "href": "content/courses/Analytics/Tools/30-Intro-to-Orange/orange.html",
    "title": "🐉 Introduction to Orange",
    "section": "",
    "text": "Orange is a visual drag-and-drop tool for\n\nData visualization\nMachine Learning\nData mining\n\nand much more. Here is what it looks like:\n\nAll operations are done using a visual menu-driven interface. The visuals can be exported to PNG/SVG/PDF and the entire workflow can be exported into a Report Form and edited for presentation and sharing.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Tools",
      "🐉 Introduction to Orange"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/30-Intro-to-Orange/orange.html#introduction-to-orange",
    "href": "content/courses/Analytics/Tools/30-Intro-to-Orange/orange.html#introduction-to-orange",
    "title": "🐉 Introduction to Orange",
    "section": "",
    "text": "Orange is a visual drag-and-drop tool for\n\nData visualization\nMachine Learning\nData mining\n\nand much more. Here is what it looks like:\n\nAll operations are done using a visual menu-driven interface. The visuals can be exported to PNG/SVG/PDF and the entire workflow can be exported into a Report Form and edited for presentation and sharing.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Tools",
      "🐉 Introduction to Orange"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/30-Intro-to-Orange/orange.html#installing-orange",
    "href": "content/courses/Analytics/Tools/30-Intro-to-Orange/orange.html#installing-orange",
    "title": "🐉 Introduction to Orange",
    "section": "Installing Orange",
    "text": "Installing Orange\nYou can download and install Orange from here:\nhttps://orangedatamining.com/download/",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Tools",
      "🐉 Introduction to Orange"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/30-Intro-to-Orange/orange.html#basic-usage-of-orange",
    "href": "content/courses/Analytics/Tools/30-Intro-to-Orange/orange.html#basic-usage-of-orange",
    "title": "🐉 Introduction to Orange",
    "section": "Basic Usage of Orange",
    "text": "Basic Usage of Orange",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Tools",
      "🐉 Introduction to Orange"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/30-Intro-to-Orange/orange.html#orange-workflows",
    "href": "content/courses/Analytics/Tools/30-Intro-to-Orange/orange.html#orange-workflows",
    "title": "🐉 Introduction to Orange",
    "section": "Orange Workflows",
    "text": "Orange Workflows",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Tools",
      "🐉 Introduction to Orange"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/30-Intro-to-Orange/orange.html#widgets-and-channels",
    "href": "content/courses/Analytics/Tools/30-Intro-to-Orange/orange.html#widgets-and-channels",
    "title": "🐉 Introduction to Orange",
    "section": "Widgets and Channels",
    "text": "Widgets and Channels",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Tools",
      "🐉 Introduction to Orange"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/30-Intro-to-Orange/orange.html#loading-data-into-orange",
    "href": "content/courses/Analytics/Tools/30-Intro-to-Orange/orange.html#loading-data-into-orange",
    "title": "🐉 Introduction to Orange",
    "section": "Loading data into Orange",
    "text": "Loading data into Orange\n\n\n\nWe are good to get started with Orange!!",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Tools",
      "🐉 Introduction to Orange"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/30-Intro-to-Orange/orange.html#simple-visuals-using-orange",
    "href": "content/courses/Analytics/Tools/30-Intro-to-Orange/orange.html#simple-visuals-using-orange",
    "title": "🐉 Introduction to Orange",
    "section": "Simple Visuals using Orange",
    "text": "Simple Visuals using Orange\nLet us create some simple visualizations using Orange.\n\nUse the File Widget to import the iris dataset into your session\nUse the Data Table Widget to look at the data, and note its variable names\nUse the Visualization Widgets ( Scatter Plot, Bar Plot, and Distributions) to look at the properties of the variables, and examine relationships between them.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Tools",
      "🐉 Introduction to Orange"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/30-Intro-to-Orange/orange.html#reference",
    "href": "content/courses/Analytics/Tools/30-Intro-to-Orange/orange.html#reference",
    "title": "🐉 Introduction to Orange",
    "section": "Reference",
    "text": "Reference\n\nIntroduction to Data Mining-Working notes for the hands-on course with Orange Data Mining.(Download file)\nOrange Data Mining Widget Catalog: Look here for help and guidance! https://orangedatamining.com/widget-catalog/",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Tools",
      "🐉 Introduction to Orange"
    ]
  },
  {
    "objectID": "content/courses/Analytics/listing.html",
    "href": "content/courses/Analytics/listing.html",
    "title": "Data Analytics",
    "section": "",
    "text": "This Course takes Business Practitioners on a journey of Business Analytics: using data to derive insights, make predictions, and decide on plans of action that can be communicated and actualized in a Business context.\n\n“Business analytics, or simply analytics, is the use of data, information technology, statistical analysis, quantitative methods, and mathematical or computer-based models to help managers gain improved insight about their business operations and make better, fact-based decisions. Business analytics is”a process of transforming data into actions through analysis and insights in the context of organizational decision making and problem solving.”\n\n-Libertore and Luo, 2010\n\n\n The Course starts with Descriptive Analytics: Datasets from various domains of Business enterprise and activity are introduced. The datasets are motivated from the point of view of the types of information they contain: students will relate the Data Variables (Qualitative and Quantitative) to various types of Data/Information Visualizations.\nStatistical Concepts such as Sampling, Hypothesis Tests, Simulation / Modelling, and Uncertainty will be introduced.\nPredictive Analytics will take us into looking at Data and training standard ML algorithms to make predictions with new Data. Regression, Clustering, and Classification will be covered.\nPrescriptive Analytics will deal with coming to terms with the uncertainty in Predictions, and using tools such as both ML, Linear/non-Linear Programming, and Decision-Making to make Business Decisions, with an assessment of the Risks involved.\nThe Course will culminate in a full Business Analytics Workflow that includes Data Gathering and Cleaning, Descriptive and Predictive Analytics, Prescriptive Analytics and Decision Making, and Communication resulting in a publication-worthy documents.(HTML / PDF/ Word)",
    "crumbs": [
      "Teaching",
      "Data Analytics"
    ]
  },
  {
    "objectID": "content/courses/Analytics/listing.html#abstract",
    "href": "content/courses/Analytics/listing.html#abstract",
    "title": "Data Analytics",
    "section": "",
    "text": "This Course takes Business Practitioners on a journey of Business Analytics: using data to derive insights, make predictions, and decide on plans of action that can be communicated and actualized in a Business context.\n\n“Business analytics, or simply analytics, is the use of data, information technology, statistical analysis, quantitative methods, and mathematical or computer-based models to help managers gain improved insight about their business operations and make better, fact-based decisions. Business analytics is”a process of transforming data into actions through analysis and insights in the context of organizational decision making and problem solving.”\n\n-Libertore and Luo, 2010\n\n\n The Course starts with Descriptive Analytics: Datasets from various domains of Business enterprise and activity are introduced. The datasets are motivated from the point of view of the types of information they contain: students will relate the Data Variables (Qualitative and Quantitative) to various types of Data/Information Visualizations.\nStatistical Concepts such as Sampling, Hypothesis Tests, Simulation / Modelling, and Uncertainty will be introduced.\nPredictive Analytics will take us into looking at Data and training standard ML algorithms to make predictions with new Data. Regression, Clustering, and Classification will be covered.\nPrescriptive Analytics will deal with coming to terms with the uncertainty in Predictions, and using tools such as both ML, Linear/non-Linear Programming, and Decision-Making to make Business Decisions, with an assessment of the Risks involved.\nThe Course will culminate in a full Business Analytics Workflow that includes Data Gathering and Cleaning, Descriptive and Predictive Analytics, Prescriptive Analytics and Decision Making, and Communication resulting in a publication-worthy documents.(HTML / PDF/ Word)",
    "crumbs": [
      "Teaching",
      "Data Analytics"
    ]
  },
  {
    "objectID": "content/courses/Analytics/listing.html#what-you-will-learn",
    "href": "content/courses/Analytics/listing.html#what-you-will-learn",
    "title": "Data Analytics",
    "section": "What you will learn",
    "text": "What you will learn\n\nData Basics: What does data look like and why should we care?\nRapidly and intuitively creating Graphs and Data Visualizations to explore data for insights\nUse Statistical Tests, Procedures, Models, and Simulations and to answer Business Questions\nUsing ML algorithms such Regression, Classification, and Clustering to develop Business Insights\nUse Linear Programming to make Business Decisions\nCreate crisp and readable Reports that can be shared in a Business Context",
    "crumbs": [
      "Teaching",
      "Data Analytics"
    ]
  },
  {
    "objectID": "content/courses/Analytics/listing.html#texts",
    "href": "content/courses/Analytics/listing.html#texts",
    "title": "Data Analytics",
    "section": "Texts",
    "text": "Texts\n\nJames R Evans, Business Analytics: Methods, Models, and Decisions, Pearson Education, 2021.",
    "crumbs": [
      "Teaching",
      "Data Analytics"
    ]
  },
  {
    "objectID": "content/courses/Analytics/listing.html#references",
    "href": "content/courses/Analytics/listing.html#references",
    "title": "Data Analytics",
    "section": "References",
    "text": "References\n\nRobert Kabacoff. Modern Data Visualization with R. https://rkabacoff.github.io/datavis/. Available free Online.\nJack Dougherty and Ilya Ilyankou, Hands-On Data Visualization: Interactive Storytelling from Spreadsheets to Code, https://handsondataviz.org/. Available free Online.\nClaus O. Wilke, Fundamentals of Data Visualization, https://clauswilke.com/dataviz/. Available free Online.\nJonathan Schwabish, Better Data Visualizations: A Guide for Scholars, Researchers, and Wonks, Columbia University Press, 2021.\nAlberto Cairo, The Functional Art:An introduction to information graphics and visualization, New Riders. 2013. ISBN-9780133041361.\nCole Nussbaumer Knaflic, Storytelling With Data: A Data Visualization Guide for Business Professionals, Wiley 2015. ISBN-9781119002253.\nJudd, C.M., McClelland, G.H., & Ryan, C.S. (2017). Data Analysis: A Model Comparison Approach To Regression, ANOVA, and Beyond, Third Edition (3rd ed.). Routledge. https://doi.org/10.4324/9781315744131\nThomas Maydon, The 4 Types of Data Analytics, https://www.kdnuggets.com/2017/07/4-types-data-analytics.html\nDimitris Bertsimas, Robert Freund, Data, Models, and Decisions: the Fundamentals of Management Science, Dynamic Ideas Press, 2004.\nCliff T. Ragsdale, Spreadsheet Modeling & Decision Analysis: A Practical Introduction to Management Science, South Western, Cengage Learning, Mason, OH, 2012.",
    "crumbs": [
      "Teaching",
      "Data Analytics"
    ]
  },
  {
    "objectID": "content/courses/Analytics/listing.html#pedagogical-notes",
    "href": "content/courses/Analytics/listing.html#pedagogical-notes",
    "title": "Data Analytics",
    "section": "Pedagogical Notes",
    "text": "Pedagogical Notes",
    "crumbs": [
      "Teaching",
      "Data Analytics"
    ]
  },
  {
    "objectID": "content/courses/Analytics/listing.html#our-tools",
    "href": "content/courses/Analytics/listing.html#our-tools",
    "title": "Data Analytics",
    "section": "Our Tools",
    "text": "Our Tools\nThis is eventually meant to be a three-in-one course, during which we will gain exposure to the following free and open source tools:\n\nR https://cran.r-project.org/ and RStudio https://posit.co/\nR is a freely available language and environment for statistical computing and graphics which provides a wide variety of statistical and graphical techniques: linear and nonlinear modelling, statistical tests, time series analysis, classification, clustering, etc. RStudio is an integrated development environment (IDE) for R and Python.\nOrange Data Mining https://orangedatamining.com/\nOrange is a FOSS visual point-and-click software for Data Mining and ML, developed at the University of Slovenia, Ljubljana.\n\n\n\n\nRadiant – Business analytics using R and Shiny https://radiant-rstats.github.io/docs/index.html\n\nRadiant is a FOSS platform-independent browser-based interface for business analytics in R, developed at the University of San Diego. The application is based on the Shiny package and can be run using R, or in your browser with no installation required. The tool automatically installs a version of R and adds a Shiny-based GUI that removes the need to write R-code. Radiant can also be installed on top of an existing installation of R and invoked from within RStudio.",
    "crumbs": [
      "Teaching",
      "Data Analytics"
    ]
  },
  {
    "objectID": "content/courses/Analytics/listing.html#modules",
    "href": "content/courses/Analytics/listing.html#modules",
    "title": "Data Analytics",
    "section": "Modules",
    "text": "Modules",
    "crumbs": [
      "Teaching",
      "Data Analytics"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LogReg/logistic.html",
    "href": "content/courses/Analytics/Modelling/Modules/LogReg/logistic.html",
    "title": "Modelling with Logistic Regression",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggformula)\nlibrary(mosaic)\nlibrary(infer)\nlibrary(regressinator) # pedagogic tool for GLMs\nlibrary(GLMsData) # Datasets from Dunn and Smyth\nlibrary(HSAUR3) # Datasets from Everitt and Hothorn\nlibrary(prettyglm) # create beautiful coefficient summaries of generalised linear models.\n# remotes::install_github(\"UCLATALL/JMRData\")\n# library(JMRData)\n\n\n# Let us set a plot theme for Data visualization\nlibrary(thematic)\ntheme_set(theme_classic(base_size = 12, base_family = \"Roboto Condensed\"))\n\ntheme_update(\n  panel.grid.minor = element_blank(),\n  plot.title = element_text(face = \"bold\"),\n  plot.title.position = \"plot\"\n)\nthematic_rmd(bg = \"#FFFFF8\",fg = \"#111111\", accent = \"#DD1144\", font = \"Roboto Condensed\")",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Inferential Modelling",
      "Modelling with Logistic Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LogReg/logistic.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Modelling/Modules/LogReg/logistic.html#setting-up-r-packages",
    "title": "Modelling with Logistic Regression",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggformula)\nlibrary(mosaic)\nlibrary(infer)\nlibrary(regressinator) # pedagogic tool for GLMs\nlibrary(GLMsData) # Datasets from Dunn and Smyth\nlibrary(HSAUR3) # Datasets from Everitt and Hothorn\nlibrary(prettyglm) # create beautiful coefficient summaries of generalised linear models.\n# remotes::install_github(\"UCLATALL/JMRData\")\n# library(JMRData)\n\n\n# Let us set a plot theme for Data visualization\nlibrary(thematic)\ntheme_set(theme_classic(base_size = 12, base_family = \"Roboto Condensed\"))\n\ntheme_update(\n  panel.grid.minor = element_blank(),\n  plot.title = element_text(face = \"bold\"),\n  plot.title.position = \"plot\"\n)\nthematic_rmd(bg = \"#FFFFF8\",fg = \"#111111\", accent = \"#DD1144\", font = \"Roboto Condensed\")",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Inferential Modelling",
      "Modelling with Logistic Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LogReg/logistic.html#introduction",
    "href": "content/courses/Analytics/Modelling/Modules/LogReg/logistic.html#introduction",
    "title": "Modelling with Logistic Regression",
    "section": "\n Introduction",
    "text": "Introduction\nSometimes the dependent variable is an either/or categorization. For example, the variable we want to predict might be won or lost the contest, has an ailment or not, voted or not in the last election, or graduated from college or not. There might even be more than two categories such as voted for Congress, BJP, or Independent; or never smoker, former smoker, or current smoker.\nWe saw with the General Linear Model that it models the mean of a target Quantitative variable as a linear weighted sum of the predictor variables:\n\ny \\sim N(x_i^T * \\beta, ~~\\sigma^2)\n\nThis model is considered to be general because of the dependence on potentially more than one explanatory variable, v.s. the simple linear model:1 y = \\beta_0 + \\beta_1*x_1 + \\epsilon. The general linear model gives us model “shapes” that start from a simple straight line to a p-dimensional hyperplane.\nAlthough a very useful framework, there are some situations where general linear models are not appropriate:\n\nthe range of Y is restricted (e.g. binary, count)\nthe variance of Y depends on the mean (Taylor’s Law)2\n\n\nHow do we use the familiar linear model framework when the target/dependent variable is Categorical?\nLinear Models for Categorical Targets?\nRecall that we spoke of dummy **predictor** variables for our linear models and how we would dummy code them using numerical values, such as 0 and 1, or +1 and -1. Could we try the same way for a target categorical variable?\n\nY_i = \\beta_0 + \\beta_1*Xi + \\epsilon_i\\\\ \\nonumber\nwhere\\\\\n\\begin{align}\nY_i &= 0 ~ if ~ \"No\"\\\\ \\nonumber\n    &= 1 ~ if ~\"Yes\"  \\nonumber\n\\end{align}\n\nSadly this seems to not work for categorical dependent variables using a simple linear model as before. Consider the Credit Card Default data from the package ISLR.\n\n\nRows: 10,000\nColumns: 4\n$ default &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, No, No, No, No…\n$ student &lt;fct&gt; No, Yes, No, No, No, Yes, No, Yes, No, No, Yes, Yes, No, No, N…\n$ balance &lt;dbl&gt; 729.5265, 817.1804, 1073.5492, 529.2506, 785.6559, 919.5885, 8…\n$ income  &lt;dbl&gt; 44361.625, 12106.135, 31767.139, 35704.494, 38463.496, 7491.55…\n\n\nWe see balance and income are quantitative predictors; student is a qualitative predictor, and default is a qualitative target variable. If we naively use a linear model equation as model = lm(default ~ balance, data = Default) and plot it, then…\n\n\n\n\n\n\n\n\n\nFigure 1: Naive Linear Model\n\n\n\n\n\n\n…it is pretty much clear from Figure 1 that something is very odd. (no pun intended! See below!) If the only possible values for default are No = 0 and Yes = 1, how could we interpret predicted value of, say, Y_i = 0.25 or Y_i = 1.55, or perhaps Y_i = -0.22? Anything other than Yes/No is hard to interpret!\n\n\n\n Problems…and Solutions\nWhere do we go from here?\nLet us state what we might desire of our model:\n\n\nModel Equation: Despite this setback, we would still like our model to be as close as possible to the familiar linear model equation.\n\n\nPredictors and Weights: We have quantitative predictors so we still want to use a linear-weighted sum for the RHS (i.e predictor side) of the model equation.\n\nWhat can we try to make this work? Especially for the LHS (i.e the target side)?\n\n\nMaking the LHS continuous: What can we try? In dummy encoding our target variable, we found a range of [0,1], which is the same range for a probability value! Could we try to use probability of the outcome as our target, even though we are interested in binary outcomes? This would still leave us with a range of [0,1] for the target variable, as before.\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\nBinomially distributed target variable\nIf we map our Categorical/Qualitative target variable into a Quantitative probability, we need immediately to look at the LINE assumptions in linear regression.\nIn linear regression, we assume a normally distributed target variable, i.e. the errors around the predicted value are normally distributed. With a categorical target variable with two levels 0 and 1 it would be impossible for the errors e_i = Y_i - \\hat{Y_i} to have a normal distribution, as assumed for the statistical tests to be valid. The errors are bounded by [0,1]! One candidate for the error distribution in this case is the binomial distribution, whose mean and variance are p and np(1-p) respectively.\nNote immediately that the binomial variance moves with the mean!\n\n\nThe LINE assumption of normality is clearly violated. And from Figure 2, extreme probabilities (near 1 or 0) are more stable (i.e., have less error variance) than middle probabilities. So the model has “built-in” heteroscedasticity, which we need to counter with transformations such as the log() function. More on this later.\n\n\n\n\nOdds?: How would one “extend” the range of a target variable from [0,1] to [-\\infty, \\infty] ? One step would be to try the odds of the outcome, instead of trying to predict the outcomes directly (Yes or No), or their probabilities [0,1].\n\n\n\n\n\n\n\nOdds\n\n\n\nOdds of an event with probability p of occurrence is defined as Odds = p/(1-p). As can be seen, the odds are the ratio of two probabilities, that of the event and its complement. In the Default dataset just considered, the odds of default and the odds of non-default can be calculated as:\n\n\n\n  \n\n\n\n\n\\begin{align}\nOddsDefault &=p(noDefault)/(1-p(noDefault))\\\\ \\nonumber\n            &= 0.9667/(1-0.9667)\\\\ \\nonumber\n            &= 29.0303\\\\\n\\end{align}\n\nand OddsNoDefault = 1/29.0303 = 0.03444709.\nNow, odds cover half of real number line, i.e. [0, \\infty] ! Clearly, when the probability p of an event is 0, the odds are 0…and when it nears 1, the odds tend to \\infty. So we have transformed a simple probability that lies between [0,1] to odds lying between [0, \\infty]. That’s one step towards making a linear model possible; we have “removed” one of the limits on our linear model’s prediction range by using Odds as our target variable.\n\n\n\n\nTransformation using log()?: We need one more leap of faith: how do we convert a [0, \\infty] range to a [-\\infty, \\infty]? Can we try a log transformation?\n\n\nlog([0, \\infty]) ~ = ~ [-\\infty, \\infty]\n This extends the range of our Qualitative target to the same as with a Quantitative target!\nThere is an additional benefit if this log() transformation: the Error Distributions with Odds targets. See Figure 3 below. Odds are a necessarily nonlinear function of probability; the slope of Odds ~ probability also depends upon the probability itself, as we saw with the probability curve Figure 2.\n\n\n\n\n\n\n\n\n\n(a) Odds\n\n\n\n\n\n\n\n\n\n(b) Log Odds\n\n\n\n\n\n\nFigure 3: Odds Plot\n\n\nTo understand this issue intuitively, consider what happens to, say, a 5% change in the odds ratio near 1.0 compared to more extreme odds ratios, Figure 3 (a) . If the odds ratio is 1.0, then the probabilities p and 1-p are 0.5, and 0.5. A 20% increase in the odds ratio to 1.20 would correspond to probabilities of 0.545 and 0.455. However, if the original probabilities were 0.9 and 0.1 for an odds ratio 9, then a 20% increase to 10.8 would correspond to probabilities of 0.915 and 0.085, a much smaller change in the probabilities. The log transformation provides a more linear relationship, which is what we desire.\nSo in our model, instead of modeling odds as the dependent variable, we will use log(odds), also known as the logit, defined as:\n\n\\begin{align}\nlog(odds_i) &= log[(p_i)/(1-p_i)]\\\\ \\nonumber\n            &= logit(p_i)\\\\\n\\end{align}\n\n\n\nEstimation of Model Parameters: The last problem to solve is that because we have made so many transformations to get to the logits that we want to model, the logic of minimizing the sum of squared errors(SSE) is no longer appropriate.\n\n\n\n\n\n\n\nNote\n\n\n\nThe probabilities for default are 0 and 1…the log(odds) will map respectively to -\\infty and \\infty. So if we naively try to take residuals, we will find that they are all \\infty !! Hence SSE cannot be computed and we need another way to assess the quality of our model.\n\n\nInstead, we will have to use maximum likelihood estimation(MLE) to estimate the models, and we will use the X^2 (“chi-squared”) statistic instead of t and Fto evaluate the model comparisons. The maximum likelihood method maximizes the probability of obtaining the data at hand against every choice of model parameters \\beta_i.\nThis is our Logistic Regression Model, which uses a Quantitative Predictor variable to predict a Categorical target variable. We write the model as ( for the Default dataset:\n\n\\begin{align}\nlogit(default) & = \\beta_0 + \\beta_1 * balance&\\\\  \\nonumber\nlog(p(default)/(1-p(default))) & = \\beta_0+\\beta_1 * balance&\\\\ \\nonumber\ntherefore\\\\\np(default) & = \\frac{exp(\\beta_0 + \\beta_1 * balance)}{1 + exp(\\beta_0 + \\beta_1 * balance)}\\\\\n\\end{align}\n\nFrom the Eqn.4 above it should be clear that a unit increase in balance should increase the odds of default by \\beta_1 units.\nThe RHS of Eqn.5 is a sigmoid function of the weighted sum of predictors and is limited to the range [0,1]. The parameters \\beta_i need to be estimated using maximum likelihood methods.\n\n\n\n\n\n\n\n\n\n(a) naive linear regression model\n\n\n\n\n\n\n\n\n\n(b) logistic regression model\n\n\n\n\n\n\n\n\n\n(c) log odds gives linear models\n\n\n\n\n\n\nFigure 4: Model Plots",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Inferential Modelling",
      "Modelling with Logistic Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LogReg/logistic.html#logistic-regression-models-as-hypothesis-tests",
    "href": "content/courses/Analytics/Modelling/Modules/LogReg/logistic.html#logistic-regression-models-as-hypothesis-tests",
    "title": "Modelling with Logistic Regression",
    "section": "\n Logistic Regression Models as Hypothesis Tests",
    "text": "Logistic Regression Models as Hypothesis Tests\nTo Be Written Up.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Inferential Modelling",
      "Modelling with Logistic Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LogReg/logistic.html#workflow-read-the-data",
    "href": "content/courses/Analytics/Modelling/Modules/LogReg/logistic.html#workflow-read-the-data",
    "title": "Modelling with Logistic Regression",
    "section": "\n Workflow: Read the Data",
    "text": "Workflow: Read the Data\nLet us now read in the data and check for these assumptions as part of our Workflow.\n\n\n\n\n\n\nResearch Question\n\n\n\nTo Be Written Up.\n\n\n\ndata(\"earinf\", package = \"GLMsData\")\ninspect(earinf)\n\n\ncategorical variables:  \n  name  class levels   n missing                                  distribution\n1 Swim factor      2 287       0 Occas (50.2%), Freq (49.8%)                  \n2  Loc factor      2 287       0 Beach (51.2%), NonBeach (48.8%)              \n3  Age factor      3 287       0 15-19 (48.8%), 20-24 (27.5%) ...             \n4  Sex factor      2 287       0 Male (65.5%), Female (34.5%)                 \n\nquantitative variables:  \n      name   class min Q1 median Q3 max      mean        sd   n missing\n1 NumInfec integer   0  0      0  2  17 1.3867596 2.3385412 287       0\n2    Infec integer   0  0      0  1   1 0.4738676 0.5001888 287       0\n\nglimpse(earinf)\n\nRows: 287\nColumns: 6\n$ Swim     &lt;fct&gt; Occas, Occas, Occas, Occas, Occas, Occas, Occas, Occas, Occas…\n$ Loc      &lt;fct&gt; NonBeach, NonBeach, NonBeach, NonBeach, NonBeach, NonBeach, N…\n$ Age      &lt;fct&gt; 15-19, 15-19, 15-19, 15-19, 15-19, 15-19, 15-19, 15-19, 15-19…\n$ Sex      &lt;fct&gt; Male, Male, Male, Male, Male, Male, Male, Male, Male, Male, M…\n$ NumInfec &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2…\n$ Infec    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n\nskimr::skim(earinf)\n\n\nData summary\n\n\nName\nearinf\n\n\nNumber of rows\n287\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n4\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nSwim\n0\n1\nFALSE\n2\nOcc: 144, Fre: 143\n\n\nLoc\n0\n1\nFALSE\n2\nBea: 147, Non: 140\n\n\nAge\n0\n1\nFALSE\n3\n15-: 140, 20-: 79, 25-: 68\n\n\nSex\n0\n1\nFALSE\n2\nMal: 188, Fem: 99\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nNumInfec\n0\n1\n1.39\n2.34\n0\n0\n0\n2\n17\n▇▁▁▁▁\n\n\nInfec\n0\n1\n0.47\n0.50\n0\n0\n0\n1\n1\n▇▁▁▁▇",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Inferential Modelling",
      "Modelling with Logistic Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LogReg/logistic.html#workflow-eda",
    "href": "content/courses/Analytics/Modelling/Modules/LogReg/logistic.html#workflow-eda",
    "title": "Modelling with Logistic Regression",
    "section": "\n Workflow: EDA",
    "text": "Workflow: EDA\nTo Be Written Up.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Inferential Modelling",
      "Modelling with Logistic Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LogReg/logistic.html#workflow-model-building",
    "href": "content/courses/Analytics/Modelling/Modules/LogReg/logistic.html#workflow-model-building",
    "title": "Modelling with Logistic Regression",
    "section": "\n Workflow: Model Building",
    "text": "Workflow: Model Building\n\n\n Model Code\n Logistic Regression Model Intuitive\n Logistic Regression Models Manually Demonstrated\n Using Other Packages\n\n\n\nTo Be Written Up.\n\n\nTo Be Written Up.\n\n\nTo Be Written Up.\n\n\nTo Be Written Up.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Inferential Modelling",
      "Modelling with Logistic Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LogReg/logistic.html#sec-diagnostics",
    "href": "content/courses/Analytics/Modelling/Modules/LogReg/logistic.html#sec-diagnostics",
    "title": "Modelling with Logistic Regression",
    "section": "\n Workflow: Model Checking and Diagnostics",
    "text": "Workflow: Model Checking and Diagnostics\n\n Checks for Uncertainty\nTo Be Written Up.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Inferential Modelling",
      "Modelling with Logistic Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LogReg/logistic.html#conclusions",
    "href": "content/courses/Analytics/Modelling/Modules/LogReg/logistic.html#conclusions",
    "title": "Modelling with Logistic Regression",
    "section": "\n Conclusions",
    "text": "Conclusions\nSo our Linear Modelling workflow might look like this: we have not seen all stages yet, but that is for another course module or tutorial!\n\n\n\n\n\nflowchart TD\n    A[(A: Data)] --&gt;|mosaic  +  ggformula|B[B:EDA] \n    B --&gt; |corrplot +  corrgram  + ggformula + purrr + cor.test| C(C: Check Relationships)\n    C --&gt; D[D: Decide on Simple/Complex Model]\n    D --&gt; E{E: Is the Model Possible?}\n    E --&gt; |Yes| G[G: Build Model]\n    E --&gt;|Nope| F[F: Transform Variables]\n    E --&gt;|Nope| K[K: Try Multiple Regression &lt;br&gt; and/or Interaction Terms]\n    K --&gt; D\n    F --&gt; D\n    G --&gt; H{H: Check Model Diagnostics}\n    H --&gt; |Problems| D\n    H --&gt; |All   good| I(Interpret Your Model)\n    I --&gt; J(((Apply the Model for Predictions)))",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Inferential Modelling",
      "Modelling with Logistic Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LogReg/logistic.html#sec-references",
    "href": "content/courses/Analytics/Modelling/Modules/LogReg/logistic.html#sec-references",
    "title": "Modelling with Logistic Regression",
    "section": "\n References",
    "text": "References\n\nJudd, Charles M. & McClelland, Gary H. & Ryan, Carey S. Data Analysis: A Model Comparison Approach to Regression, ANOVA, and Beyond. Routledge, Aug 2017. Chapter 14.\nhttps://yury-zablotski.netlify.app/post/how-logistic-regression-works/\nhttps://uc-r.github.io/logistic_regression\nhttps://francisbach.com/self-concordant-analysis-for-logistic-regression/\nhttps://statmath.wu.ac.at/courses/heather_turner/glmCourse_001.pdf\nhttps://jasp-stats.org/2022/06/30/generalized-linear-models-glm-in-jasp/\nP. Bingham, N.Q. Verlander, M.J. Cheal (2004). John Snow, William Farr and the 1849 outbreak of cholera that affected London: a reworking of the data highlights the importance of the water supply. Public Health Volume 118, Issue 6, September 2004, Pages 387-394. Read the PDF.\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggtext\n0.1.2\nWilke and Wiernik (2022)\n\n\nGLMsData\n1.4\nDunn and Smyth (2022)\n\n\nHSAUR3\n1.0.14\nHothorn and Everitt (2023)\n\n\nprettyglm\n1.0.1\nFowler (2023)\n\n\nregressinator\n0.1.3\nReinhart (2024)\n\n\n\n\n\n\nDunn, Peter K., and Gordon K. Smyth. 2022. GLMsData: Generalized Linear Model Data Sets. https://CRAN.R-project.org/package=GLMsData.\n\n\nFowler, Jared. 2023. prettyglm: Pretty Summaries of Generalized Linear Model Coefficients. https://CRAN.R-project.org/package=prettyglm.\n\n\nHothorn, Torsten, and Brian S. Everitt. 2023. HSAUR3: A Handbook of Statistical Analyses Using r (3rd Edition). https://CRAN.R-project.org/package=HSAUR3.\n\n\nReinhart, Alex. 2024. regressinator: Simulate and Diagnose (Generalized) Linear Models. https://CRAN.R-project.org/package=regressinator.\n\n\nWilke, Claus O., and Brenton M. Wiernik. 2022. ggtext: Improved Text Rendering Support for “ggplot2”. https://CRAN.R-project.org/package=ggtext.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Inferential Modelling",
      "Modelling with Logistic Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LogReg/logistic.html#footnotes",
    "href": "content/courses/Analytics/Modelling/Modules/LogReg/logistic.html#footnotes",
    "title": "Modelling with Logistic Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://statmath.wu.ac.at/courses/heather_turner/glmCourse_001.pdf↩︎\nhttps://en.wikipedia.org/wiki/Taylor%27s_law↩︎",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Inferential Modelling",
      "Modelling with Logistic Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/corr-lm-explorations.html",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/corr-lm-explorations.html",
    "title": "Correlation and Regression Explorations",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(broom)\nlibrary(ggformula)\nggplot2::theme_set(new = theme_classic())"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/corr-lm-explorations.html#packages",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/corr-lm-explorations.html#packages",
    "title": "Correlation and Regression Explorations",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(broom)\nlibrary(ggformula)\nggplot2::theme_set(new = theme_classic())"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/corr-lm-explorations.html#intro",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/corr-lm-explorations.html#intro",
    "title": "Correlation and Regression Explorations",
    "section": "Intro",
    "text": "Intro\nI will work through and “unify” at least two things:\n\nHadley Wickham’s chapter on modelling and his analysis of the linear model for the diamonds dataset\nThe diagnostic aspects of Linear Regression as detailed in Crawley’s book"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/corr-lm-explorations.html#explorations-into-diagnostic-plots",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/corr-lm-explorations.html#explorations-into-diagnostic-plots",
    "title": "Correlation and Regression Explorations",
    "section": "Explorations into Diagnostic Plots",
    "text": "Explorations into Diagnostic Plots\nLet us create dependent y* variables with different sorts of errors:\n\nx &lt;- 0:300\nen &lt;- rnorm(301, mean = 0, sd = 5)\neu &lt;- (runif(n = 301) -0.5) * 20\neb &lt;- rnbinom(n = 301,prob = 0.3,size = 2)\neg &lt;- rgamma(n = 301,shape = 1, rate = 1/x)\nyn &lt;- x + 10 + en\nyu &lt;- x + 10 + eu\nyb &lt;- x + 10 + eb\nyg &lt;- x + 10 + eg\ndata &lt;- tibble(x, yn, yu, yb, yg)\ndata\n\n\n  \n\n\n\nNormal Errors\n\nlm_norm_aug &lt;- lm(yn ~ x, data = data) %&gt;% \n  augment()\nlm_norm_aug %&gt;% gf_point(.resid ~ .fitted)\n\n\n\n\n\n\nlm_norm_aug %&gt;% gf_qq(~ .resid) %&gt;% gf_qqline()\n\n\n\n\n\n\n\nUniform Errors\n\nlm_unif_aug &lt;- lm(yu ~ x, data = data) %&gt;% \n  augment()\nlm_unif_aug %&gt;% gf_point(.resid ~ .fitted)\n\n\n\n\n\n\nlm_unif_aug %&gt;% gf_qq(~ .resid, distribution = stats::qnorm) %&gt;% gf_qqline()\n\n\n\n\n\n\n\nNegative Binom Errors\n\nlm_nbinom_aug &lt;- lm(yb ~ x, data = data) %&gt;% \n  augment()\nlm_nbinom_aug %&gt;% gf_point(.resid ~ .fitted)\n\n\n\n\n\n\nlm_nbinom_aug %&gt;% gf_qq(~ .resid, distribution = stats::qnorm) %&gt;% gf_qqline()\n\n\n\n\n\n\n\nGamma Errors\n\nlm_gamm_aug &lt;- lm(yg ~ x, data = data) %&gt;% \n  augment()\nlm_gamm_aug %&gt;% gf_point(.resid ~ .fitted)\n\n\n\n\n\n\nlm_gamm_aug %&gt;% gf_qq(~ .resid, distribution = stats::qnorm) %&gt;% gf_qqline()"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/lin-perm.html",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/lin-perm.html",
    "title": "Permutation Tests for Linear Regression",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE,warning = FALSE,message = FALSE)\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(mosaic)\nlibrary(infer)"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/lin-perm.html#linear-regression-using-permutation-tests",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/lin-perm.html#linear-regression-using-permutation-tests",
    "title": "Permutation Tests for Linear Regression",
    "section": "Linear Regression using Permutation Tests",
    "text": "Linear Regression using Permutation Tests\nWe wish to establish the significance of the effect size due to each of the levels in TempFac. From the normality tests conducted earlier we see that except at one level of TempFac, the times are are not normally distributed. Hence we opt for a Permutation Test to check for significance of effect.\nAs remarked in Ernst[^2], the non-parametric permutation test can be both exact and also intuitively easier for students to grasp. Permutations are easily executed in R, using packages such as mosaic[^3].\nWe proceed with a Permutation Test for TempFac. We shuffle the levels (13, 18, 25) randomly between the Times and repeat the ANOVA test each time and calculate the F-statistic. The Null distribution is the distribution of the F-statistic over the many permutations and the p-value is given by the proportion of times the F-statistic equals or exceeds that observed.\nRead the Data\n\ndata(\"BostonHousing2\", package = \"mlbench\")\nhousing &lt;- BostonHousing2\ninspect(housing)\n\n\ncategorical variables:  \n  name  class levels   n missing                                  distribution\n1 town factor     92 506       0 Cambridge (5.9%) ...                         \n2 chas factor      2 506       0 0 (93.1%), 1 (6.9%)                          \n\nquantitative variables:  \n      name   class       min          Q1     median          Q3       max\n1    tract integer   1.00000 1303.250000 3393.50000 3739.750000 5082.0000\n2      lon numeric -71.28950  -71.093225  -71.05290  -71.019625  -70.8100\n3      lat numeric  42.03000   42.180775   42.21810   42.252250   42.3810\n4     medv numeric   5.00000   17.025000   21.20000   25.000000   50.0000\n5    cmedv numeric   5.00000   17.025000   21.20000   25.000000   50.0000\n6     crim numeric   0.00632    0.082045    0.25651    3.677083   88.9762\n7       zn numeric   0.00000    0.000000    0.00000   12.500000  100.0000\n8    indus numeric   0.46000    5.190000    9.69000   18.100000   27.7400\n9      nox numeric   0.38500    0.449000    0.53800    0.624000    0.8710\n10      rm numeric   3.56100    5.885500    6.20850    6.623500    8.7800\n11     age numeric   2.90000   45.025000   77.50000   94.075000  100.0000\n12     dis numeric   1.12960    2.100175    3.20745    5.188425   12.1265\n13     rad integer   1.00000    4.000000    5.00000   24.000000   24.0000\n14     tax integer 187.00000  279.000000  330.00000  666.000000  711.0000\n15 ptratio numeric  12.60000   17.400000   19.05000   20.200000   22.0000\n16       b numeric   0.32000  375.377500  391.44000  396.225000  396.9000\n17   lstat numeric   1.73000    6.950000   11.36000   16.955000   37.9700\n           mean           sd   n missing\n1  2700.3557312 1.380037e+03 506       0\n2   -71.0563887 7.540535e-02 506       0\n3    42.2164403 6.177718e-02 506       0\n4    22.5328063 9.197104e+00 506       0\n5    22.5288538 9.182176e+00 506       0\n6     3.6135236 8.601545e+00 506       0\n7    11.3636364 2.332245e+01 506       0\n8    11.1367787 6.860353e+00 506       0\n9     0.5546951 1.158777e-01 506       0\n10    6.2846344 7.026171e-01 506       0\n11   68.5749012 2.814886e+01 506       0\n12    3.7950427 2.105710e+00 506       0\n13    9.5494071 8.707259e+00 506       0\n14  408.2371542 1.685371e+02 506       0\n15   18.4555336 2.164946e+00 506       0\n16  356.6740316 9.129486e+01 506       0\n17   12.6530632 7.141062e+00 506       0\n\n\nWe will use mosaic and also try with infer.\n\n\nUsing mosaic\nUsing infer\n\n\n\nmosaic offers an easy and intuitive way of doing a repeated permutation test, using the do() command. We will shuffle the TempFac factor to jumble up the Time observations, 10000 times. Each time we shuffle, we compute the F_statistic and record it. We then plot the 10000 F-statistics and compare that with the real-world observation of F-stat.\nThe Null distribution of the F_statistic under permutation shows it never crosses the real-world observed value, testifying the strength of the effect of TempFac on hatching Time. And the p-value is:\n\n\nWe calculate the observed F-stat with infer, which also has a very direct, if verbose, syntax for doing permutation tests:\nWe see that the observed F-Statistic is of course \\(385.8966\\) as before. Now we use infer to generate a NULL distribution using permutation of the factor TempFac:\nAs seen, the infer based permutation test also shows that the permutationally generated F-statistics are nowhere near that which was observed. The effect of TempFac is very strong."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html",
    "title": "Modelling with Linear Regression",
    "section": "",
    "text": "Multiple Regression - Forward Selection  \n\n Multiple Regression - Backward Selection  \n\n  Permutation Test for Regression",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Inferential Modelling",
      "Modelling with Linear Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#sec-linreg-tutorials",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#sec-linreg-tutorials",
    "title": "Modelling with Linear Regression",
    "section": "",
    "text": "Multiple Regression - Forward Selection  \n\n Multiple Regression - Backward Selection  \n\n  Permutation Test for Regression",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Inferential Modelling",
      "Modelling with Linear Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#setting-up-r-packages",
    "title": "Modelling with Linear Regression",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nknitr::opts_chunk$set(echo = TRUE,warning = FALSE,message = FALSE)\n#options(scipen = 1, digits = 3) #set digits to three decimal places\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(mosaic)\nlibrary(GGally)\nlibrary(corrplot)\nlibrary(corrgram)\nlibrary(ggstatsplot)",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Inferential Modelling",
      "Modelling with Linear Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#introduction",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#introduction",
    "title": "Modelling with Linear Regression",
    "section": "\n Introduction",
    "text": "Introduction\nOne of the most common problems in Prediction Analytics is that of predicting a Quantitative response variable, based on one or more Quantitative predictor variables or features. This is called Linear Regression. We will use the intuitions built up during our study of ANOVA to develop our ideas about Linear Regression.\nSuppose we have data on salaries in a Company, with years of study and previous experience. Would we be able to predict the prospective salary of a new candidate, based on their years of study and experience? Or based on mileage done, could we predict the resale price of a used car? These are typical problems in Linear Regression.\nIn this tutorial, we will use the Boston housing dataset. Our research question is:\n\n\n\n\n\n\nResearch Question\n\n\n\nHow do we predict the price of a house in Boston, based on other parameters Quantitative parameters such as area, location, rooms, and crime-rate in the neighbourhood?",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Inferential Modelling",
      "Modelling with Linear Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#the-linear-regression-model",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#the-linear-regression-model",
    "title": "Modelling with Linear Regression",
    "section": "\n The Linear ( Regression ) Model",
    "text": "The Linear ( Regression ) Model\nThe premise here is that many common statistical tests are special cases of the linear model.\nA linear model estimates the relationship between one continuous or ordinal variable (dependent variable or “response”) and one or more other variables (explanatory variable or “predictors”). It is assumed that the relationship is linear:1\n\\[\ny = \\beta_0 + \\beta_1 *x\n\\]\n\\(\\beta_0\\) is the intercept and \\(\\beta_1\\) is the slope of the linear fit, that predicts the value of y based the value of x. Each prediction leaves a small “residual” error between the actual and predicted values. \\(\\beta_0\\) and \\(\\beta_1\\) are calculated based on minimizing the sum of squares of these residuals, and hence this method is called “ordinary least squares” (OLS) regression.\n\n\nLeast Squares\n\nThe net area of all the shaded squares is minimized in the calculation of \\(\\beta_0\\) and \\(\\beta_1\\). It is also possible that there is more than one explanatory variable: this is multiple regression.\n\\[\ny = \\beta_0 + \\beta_1*x_1 + \\beta_2*x_2 ...+ \\beta_n*x_n\n\\]\nwhere each of the \\(\\beta_i\\) are slopes defining the relationship between y and \\(x_i\\). Together, the RHS of that equation defines an n-dimensional hyperplane. The model is linear in the parameters \\(\\beta_i\\), e.g. these are OK:\n\\[\n\\color{blue}{\n\\begin{cases}\n& y_i = \\pmb\\beta_0 + \\pmb\\beta_1x_1 + \\pmb\\beta_2x_1^2 + \\epsilon_i\\\\\n& y_1 = \\pmb\\beta_0 + \\pmb\\gamma_1\\pmb\\delta_1x_1 + exp(\\pmb\\beta_2)x_2+ \\epsilon_i\\\\\n\\end{cases}\n}\n\\] but not, for example, these:\n\\[\n\\color{red}{\n\\begin{cases}\n& y_i = \\pmb\\beta_0 + \\pmb\\beta_1x_1^{\\beta_2} + \\epsilon_i\\\\\n& y_i = \\pmb\\beta_0 + exp(\\pmb\\beta_1x_1) + \\epsilon_i\\\\\n\\end{cases}\n}\n\\]\nAs per Lindoloev, many statistical tests, going from one-sample t-tests to two-way ANOVA, are special cases of this system. Also see Jeffrey Walker “A linear-model-can-be-fit-to-data-with-continuous-discrete-or-categorical-x-variables”",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Inferential Modelling",
      "Modelling with Linear Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#linear-models-as-hypothesis-tests",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#linear-models-as-hypothesis-tests",
    "title": "Modelling with Linear Regression",
    "section": "\n Linear Models as Hypothesis Tests",
    "text": "Linear Models as Hypothesis Tests\nUsing linear models is based on the idea of Testing of Hypotheses. The Hypothesis Testing method typically defines a NULL Hypothesis where the statements read as “there is no relationship” between the variables at hand, explanatory and responses. The Alternative Hypothesis typically states that there is a relationship between the variables.\nAccordingly, in fitting a linear model, we follow the process as follows: With \\(y = \\beta_0 + \\beta_1 *x\\)\n\nMake the following hypotheses: \\[\nNULL\\ Hypothesis\\ H_0 =&gt; x\\ and\\ y\\ are\\ unrelated.\\ (\\beta_1 = 0)\n\\] \\[\nAlternate\\ Hypothesis\\ H_1 =&gt; x\\ and\\ y\\ are\\ linearly\\ related\\ (\\beta_1 \\ne 0)\n\\]\n\nWe “assume” that \\(H_0\\) is true.\nWe calculate \\(\\beta_1\\).\nWe then find probability p that \\(\\beta_1 = Estimated\\ Value\\) when the NULL Hypothesis is assumed TRUE. This is the p-value. If that probability is p&gt;=0.05, we say we “cannot reject” \\(H_0\\) and there is unlikely to be significant linear relationship.\nHowever, if p&lt;= 0.05 can we reject the NULL hypothesis, and say that there could be a significant linear relationship, because the probability p that \\(\\beta_1 = Estimated\\ Value\\) by mere chance under \\(H_0\\) is very small.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Inferential Modelling",
      "Modelling with Linear Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#sec-assumptions-in-linear-models",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#sec-assumptions-in-linear-models",
    "title": "Modelling with Linear Regression",
    "section": "\n Assumptions in Linear Models",
    "text": "Assumptions in Linear Models\nWe can write the assumptions in Linear Regression Models as an acronym, LINE:\n1. L: \\(\\color{blue}{linear}\\) relationship\n2. I: Errors are independent (across observations)\n3. N: y is \\(\\color{red}{normally}\\) distributed at each “level” of x.\n4. E: equal variance at all levels of x. No heteroscedasticity.\n\n\nOLS Assumptions\n\nHence a very concise way of expressing the Linear Model is:\n\\[\ny \\sim N(x_i^T * \\beta, ~~\\sigma^2)\n\\]\n\n\n\n\n\n\nGeneral Linear Models\n\n\n\nThe target variable \\(y\\) is modelled as a normally distribute variable whose mean depends upon a linear combination of predictor variables \\(x\\), and whose variance is \\(\\sigma^2\\).\n\n\nLet us now read in the data and check for these assumptions as part of our Workflow.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Inferential Modelling",
      "Modelling with Linear Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#workflow-read-the-data",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#workflow-read-the-data",
    "title": "Modelling with Linear Regression",
    "section": "\n Workflow: Read the Data",
    "text": "Workflow: Read the Data\n\ndata(\"BostonHousing2\", package = \"mlbench\")\nhousing &lt;- BostonHousing2\ninspect(housing)\n\n\ncategorical variables:  \n  name  class levels   n missing                                  distribution\n1 town factor     92 506       0 Cambridge (5.9%) ...                         \n2 chas factor      2 506       0 0 (93.1%), 1 (6.9%)                          \n\nquantitative variables:  \n      name   class       min          Q1     median          Q3       max\n1    tract integer   1.00000 1303.250000 3393.50000 3739.750000 5082.0000\n2      lon numeric -71.28950  -71.093225  -71.05290  -71.019625  -70.8100\n3      lat numeric  42.03000   42.180775   42.21810   42.252250   42.3810\n4     medv numeric   5.00000   17.025000   21.20000   25.000000   50.0000\n5    cmedv numeric   5.00000   17.025000   21.20000   25.000000   50.0000\n6     crim numeric   0.00632    0.082045    0.25651    3.677083   88.9762\n7       zn numeric   0.00000    0.000000    0.00000   12.500000  100.0000\n8    indus numeric   0.46000    5.190000    9.69000   18.100000   27.7400\n9      nox numeric   0.38500    0.449000    0.53800    0.624000    0.8710\n10      rm numeric   3.56100    5.885500    6.20850    6.623500    8.7800\n11     age numeric   2.90000   45.025000   77.50000   94.075000  100.0000\n12     dis numeric   1.12960    2.100175    3.20745    5.188425   12.1265\n13     rad integer   1.00000    4.000000    5.00000   24.000000   24.0000\n14     tax integer 187.00000  279.000000  330.00000  666.000000  711.0000\n15 ptratio numeric  12.60000   17.400000   19.05000   20.200000   22.0000\n16       b numeric   0.32000  375.377500  391.44000  396.225000  396.9000\n17   lstat numeric   1.73000    6.950000   11.36000   16.955000   37.9700\n           mean           sd   n missing\n1  2700.3557312 1.380037e+03 506       0\n2   -71.0563887 7.540535e-02 506       0\n3    42.2164403 6.177718e-02 506       0\n4    22.5328063 9.197104e+00 506       0\n5    22.5288538 9.182176e+00 506       0\n6     3.6135236 8.601545e+00 506       0\n7    11.3636364 2.332245e+01 506       0\n8    11.1367787 6.860353e+00 506       0\n9     0.5546951 1.158777e-01 506       0\n10    6.2846344 7.026171e-01 506       0\n11   68.5749012 2.814886e+01 506       0\n12    3.7950427 2.105710e+00 506       0\n13    9.5494071 8.707259e+00 506       0\n14  408.2371542 1.685371e+02 506       0\n15   18.4555336 2.164946e+00 506       0\n16  356.6740316 9.129486e+01 506       0\n17   12.6530632 7.141062e+00 506       0\n\n\nThe original data are 506 observations on 14 variables, medv being the target variable:\n\n\n\n\n\n\n\ncrim\nper capita crime rate by town\n\n\nzn\nproportion of residential land zoned for lots over 25,000 sq.ft\n\n\nindus\nproportion of non-retail business acres per town\n\n\nchas\nCharles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n\n\nnox\nnitric oxides concentration (parts per 10 million)\n\n\nrm\naverage number of rooms per dwelling\n\n\nage\nproportion of owner-occupied units built prior to 1940\n\n\ndis\nweighted distances to five Boston employment centres\n\n\nrad\nindex of accessibility to radial highways\n\n\ntax\nfull-value property-tax rate per USD 10,000\n\n\nptratio\npupil-teacher ratio by town\n\n\nb\n\n\\(1000(B - 0.63)^2\\) where B is the proportion of Blacks by town\n\n\nlstat\npercentage of lower status of the population\n\n\nmedv\nmedian value of owner-occupied homes in USD 1000’s\n\n\n\nThe corrected data set has the following additional columns:\n\n\ncmedv\ncorrected median value of owner-occupied homes in USD 1000’s\n\n\ntown\nname of town\n\n\ntract\ncensus tract\n\n\nlon\nlongitude of census tract\n\n\nlat\nlatitude of census tract\n\n\nOur response variable is cmedv, the corrected median value of owner-occupied homes in USD 1000’s. Their are many Quantitative feature variables that we can use to predict cmedv. And there are two Qualitative features, chas and tax.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Inferential Modelling",
      "Modelling with Linear Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#workflow-eda",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#workflow-eda",
    "title": "Modelling with Linear Regression",
    "section": "\n Workflow: EDA",
    "text": "Workflow: EDA\nIn order to fit the linear model, we need to choose predictor variables that have strong correlations with the target variable. We will first do this with GGally, and then with the tidyverse itself. Both give us a very unique view into the correlations that exist within this dataset.\n\n\nCorrelations with GGally\nCorrelations using cor.test and purrr\n\n\n\nLet us select a few sets of Quantitative and Qualitative features, along with the target variable cmedv and do a pairs-plots with them:\nhousing %&gt;%\n  # Target variable cmedv\n  # Predictors Rooms / Age / Distance to City Centres / Radial Highway Access\n  select(cmedv, rm, age, dis) %&gt;%\n  GGally::ggpairs(title = \"Plot 1\",\n                  lower = list(continuous = wrap(\"smooth\", \n                                                 alpha = 0.2))) \n\nhousing %&gt;%\n  # Target variable cmedv\n  # Predictors: Access to Radial Highways, / Resid. Land Proportion / proportion of non-retail business acres / full-value property-tax rate per USD 10,000\n  select(cmedv, rad, zn, indus, tax) %&gt;%\n  GGally::ggpairs(title = \"Plot 2\", \n                  lower = list(continuous = wrap(\"smooth\", \n                                                 alpha = 0.2))) \n\nhousing %&gt;%\n  # Target variable cmedv\n  # Predictors Crime Rate / Nitrous Oxide / Black Population / Lower Status Population\n  select(cmedv, crim, nox, rad, b, lstat) %&gt;%\n  GGally::ggpairs(title = \"Plot 3\", \n                  lower = list(continuous = wrap(\"smooth\", \n                                                 alpha = 0.2))) \n\n\n\n\n\n\n\n\nFigure 1: Pairs Plot 1\n\n\n\n\n\n\n\n\n\nFigure 2: Pairs Plot 2\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Pairs Plot 3\n\n\n\n\n\nSee the top row of Figure 1. Clearly, rm (avg. number of rooms) is a big determining feature for median price cmedv. This we infer based on the large correlation of rm withcmedv, \\(0.696\\). The variableage (proportion of owner-occupied units built prior to 1940) may also be a significant influence on cmedv, with a correlation of \\(-0.378\\).\nNone of the Quant variables rad, zn, indus, tax have a overly strong correlation with cmedv. See top row of Figure 2.\nThe variable lstat (proportion of lower classes in the neighbourhood) as expected, has a strong (negative) correlation with cmedv; rad(index of accessibility to radial highways), nox(nitrous oxide) and crim(crime rate) also have fairly large correlations with cmedv, as seen from Figure 3.\n\n\n\n\n\n\nCorrelation Scores and Uncertainty\n\n\n\nRecall that cor.test reports a correlation score and the p-value for the same. There is also a confidence interval reported for the correlation score, an interval within which we are 95% sure that the true correlation value is to be found.\nNote that GGally too reports the significance of the correlation scores using stars, *** or **. This indicates the p-value in the scores obtained by GGally; Presumably, there is an internal cor.test that is run for each pair of variables and the p-value and confidence levels are also computed internally.\n\n\nLet us plot (again) scatter plots of Quant Variables that have strong correlation with cmedv:\ngf_point(\n  data = housing,\n  cmedv ~ age,\n  title = \"Price vs Proportion of houses older than 1940\",\n  ylab = \"Median Price\",\n  xlab = \"Proportion of older-than-1940 buildings\")\ngf_point(\n  data = housing,\n  cmedv ~ lstat,\n  title = \"Price vs Proportion of lower classes in the neighbourhood\",\n  ylab = \"Median Price\",\n  xlab = \"proportion of lower classes in the neighbourhood\")\ngf_point(\n  data = housing,\n  cmedv ~ rm,\n  title = \"Price vs Average no. of Rooms\",\n  ylab = \"(cmedv) Median Price\",\n  xlab = \"(rm) Avg. No. of Rooms\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo, rm does have a positive effect on cmedv, and age may have a (mild?) negative effect on cmedv; lstat seems to have a pronouced negative effet on cmedv. We have now managed to get a decent idea which Quant predictor variables might be useful in modelling cmedv: rm, lstat for starters, then perhapsage.\nLet us also check the Qualitative predictor variables: Access to the Charles river (chas) does seem to affect the prices somewhat.\n\nhousing %&gt;%\n  # Target variable cmedv\n  # Predictor Access to Charles River\n  select(cmedv, chas) %&gt;%\n  GGally::ggpairs(title = \"Plot 4\", \n                  lower = list(continuous = wrap(\"smooth\", \n                                                 alpha = 0.2)))\n\n\n\n\n\n\nFigure 4\n\n\n\n\nLook at the Figure 4 bar plot above. While not too many properties can be near the Charles River (for obvious reasons) the box plots do seem to show some dependency of cmedv on chas.\n\n\n\n\n\n\nNote\n\n\n\nQualitative predictors for a Quantitative target can be included in the model using what is called dummy variables, where each level of the Qualitative variable is given a one-hot kind of encoding. See for example https://www.statology.org/dummy-variables-regression/\n\n\n\n\nThis is somewhat advanced material: We will use the purrr package to develop all correlations with respect to our target variable in one shot and also plot these correlation test scores in an error-bar plot. See Tidy Modelling with R. This has the advantage of being able to depict all correlations in one plot. (We will use this approach again here when we trim our linear models down from the maximal one to a workable one of lesser complexity.). Let us do this.\nWe develop a list object containing all correlation test results with respect to cmedv, tidy these up using broom::tidy, and then plot these:\n\nall_corrs &lt;- housing %&gt;% \n  select(where(is.numeric)) %&gt;% \n  # leave off target variable cmedv and IDs\n  # get all the remaining ones\n  select(-cmedv, -medv) %&gt;%  \n\n  purrr::map(.x = ., # All numeric variables selected in the previous step\n             .f = \\(.x) cor.test(.x, housing$cmedv)) %&gt;% # Apply the cor.test with `cmedv`\n  \n  # Tidy up the cor.test outputs into neat columns\n  # Need \".id\" column to keep track of predictor variable name\n  map_dfr(broom::tidy, .id = \"predictor\") \n\nall_corrs\n\n\n  \n\n\nall_corrs %&gt;%\n  gf_hline(\n    yintercept = 0,\n    color = \"grey\",\n    linewidth = 2,\n    title = \"Correlations: Target Variable vs All Predictors\",\n    subtitle = \"Boston Housing Dataset\"\n  ) %&gt;%\n  gf_errorbar(\n    conf.high + conf.low ~ reorder(predictor, estimate),\n    colour = ~ estimate,\n    width = 0.5,\n    linewidth = ~ -log10(p.value),\n    caption = \"Significance = -log10(p.value)\"\n  ) %&gt;%\n  \n  # Plot points(smallest geom) last!\n  gf_point(estimate ~ reorder(predictor, estimate)) %&gt;%\n  gf_labs(x = \"Predictors\", y = \"Correlation with cmedv\") %&gt;%\n  #gf_theme(theme_minimal()) %&gt;% \n  \n  # tilt the x-axis labels for readability\n  gf_theme(theme(axis.text.x = element_text(angle = 45, hjust = 1))) %&gt;%\n  \n  # Colour and linewidth scales + legends\n  gf_refine(\n    scale_colour_distiller(\"Correlation\", type = \"div\", palette = \"RdBu\"),\n    scale_linewidth_continuous(\"Significance\", range = c(0.25, 3), \n                               \n  # guide_legend(reverse = TRUE): Fat Lines mean higher significance\n    )) %&gt;%\n  gf_refine(guides(linewidth = guide_legend(reverse = TRUE)))\n\n\n\n\n\n\n\nWe can clearly see that rm and lstat have strong correlations with cmedv and should make good choices for setting up a minimal linear regression model. (medv is the older errored version of cmedv)\n\n\n\n\n\n\n\n\n\nSimple Regression vs Multiple Regression\n\n\n\nIf there are many predictor variables, we would typically want to use more of them to make our model and predictions. There are three ways2 to include more predictors:\n\n\nBackward Selection: We would typically start with a maximal model3 and progressively simplify the model by knocking off predictors that have the least impact on model accuracy.\n\nForward Selection: Start with no predictors and systematically add them one by one to increase the quality of the model\n\nMixed Selection: Wherein we start with no predictors and add them to gain improvement, or remove them at as their significance changes based on other predictors that have been added.\n\nWe will do the first two in the other tutorials (see Section 1 above); Mixed Selection we will leave for a more advanced course. But for now we will first use just one predictor rm(Avg. no. of Rooms) to model housing prices.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Inferential Modelling",
      "Modelling with Linear Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#workflow-model-building",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#workflow-model-building",
    "title": "Modelling with Linear Regression",
    "section": "\n Workflow: Model Building",
    "text": "Workflow: Model Building\nWe will first execute the lm test with code and evaluate the results. Then we will do an intuitive walk through of the process and finally, hand-calculate entire analysis for clear understanding.\n\n\n Model Code\n Forecasting with the Linear Model\n Linear Model Intuitive\n Linear Models Manually Demonstrated (Apologies to Spinoza)\n Using Other Packages\n\n\n\nR offers a very simple command lm to execute an Linear Model: Note the familiar formula of stating the variables: ( \\(y \\sim x\\); where \\(y\\) = target, \\(x\\) = predictor)\n\nhousing_lm &lt;- lm(cmedv ~ rm, data = housing)\nsummary(housing_lm)\n\n\nCall:\nlm(formula = cmedv ~ rm, data = housing)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-23.336  -2.425   0.093   2.918  39.434 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -34.6592     2.6421  -13.12   &lt;2e-16 ***\nrm            9.0997     0.4178   21.78   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.597 on 504 degrees of freedom\nMultiple R-squared:  0.4848,    Adjusted R-squared:  0.4838 \nF-statistic: 474.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\nThe model for \\(\\widehat{cmedv}\\) , the prediction for cmedvcan be written in the form of \\(y = mx + c\\), as:\n\\[\n\\widehat{cmedv} \\sim -34.65924 + 9.09967* rm\n\\tag{1}\\]\n\n\n\n\n\n\nImportant\n\n\n\n\nThe effect size of rm on predicting cmedv a (slope) value of \\(9.09967\\) which is significant at p-value of \\(&lt;2.2e-16\\); for every one room increase in rm, we have a $ USD 90997$ increase in median price cmedv.\nThe F-statistic for the Linear Model is given by \\(474.3\\), which is very high. (We will use the F-statistic again when we do Multiple Regression.)\nThe R-squared value is 48% which means that rm is able to explain about half of the trend in cmedv; there is substantial variation in cmedv that is left to explain, an indication that we should perhaps use a richer model, with more predictors. We will explore this in the Tutorials. Section 1\n\n\n\n\nWe can plot the scatter plot of these two variables with the model also over-plotted.\n# Tidy Data frame for the model using `broom`\nhousing_lm_tidy &lt;- \n  housing_lm %&gt;% \n  broom::tidy(conf.int= TRUE, \n              conf.level = 0.95)\nhousing_lm_tidy\nhousing_lm_augment &lt;- \n  housing_lm %&gt;% \n  broom::augment(se_fit = TRUE,\n                 interval = \"confidence\")\nhousing_lm_augment\nintercept &lt;- \n  housing_lm_tidy %&gt;%\n  filter(term == \"(Intercept)\") %&gt;%\n  select(estimate) %&gt;%\n  as.numeric()\n\nslope &lt;- \n  housing_lm_tidy %&gt;%\n  filter(term == \"rm\") %&gt;%\n  select(estimate) %&gt;%\n  as.numeric()\n\ngf_point(\n  data = housing,\n  cmedv ~ rm,\n  title = \"Price vs Average no. of Rooms\",\n  ylab = \"Median Price\",\n  xlab = \"Avg. No. of Rooms\",\n  alpha = 0.2\n) %&gt;%\n  \n  # Plot the model equation\n  gf_abline(slope = slope, intercept = intercept, colour = \"lightcoral\",\n            linewidth = 2)  %&gt;%\n  \n  # Plot the model prediction points on the line\n  gf_smooth(method = \"lm\", geom = \"point\", color = \"yellow\", size = 0.5) %&gt;%\n  \n  gf_segment(\n    0 + 29 ~ 7 + 7, # manually calculated\n    linetype = \"dashed\",\n    color = \"dodgerblue\",\n    arrow = arrow(\n      angle = 30,\n      length = unit(0.25, \"inches\"),\n      ends = \"last\",\n      type = \"closed\"\n    ),\n    data = housing_lm_augment\n  ) %&gt;%\n  \n  gf_segment(\n    29 + 29 ~ 2.5 + 7, # manually calculated\n    linetype = \"dashed\",\n    arrow = arrow(\n      angle = 30,\n      length = unit(0.25, \"inches\"),\n      ends = \"first\",\n      type = \"closed\"\n    ),\n    color = \"dodgerblue\",\n    data = housing_lm_augment\n  ) %&gt;%\n  \n  gf_refine(\n    scale_x_continuous(limits = c(2.5, 10),\n                       expand = c(0, 0)),\n    # removes plot panel margins\n    scale_y_continuous(limits = c(0, 55),\n                       expand = c(0, 0))\n  )  \n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n\n\n\nFor any new value of rm, we go up to the vertical blue line and read off the predicted median price by following the horizontal blue line. That is how the model is used (by hand).\n\n\nIn practice, we use the broom package functions (tidy, glance and augment) to obtain a clear view of the model parameters and predictions of cmedv for all existing values of rm. We see estimates for the intercept and slope (rm) for the linear model, along with the standard errors and p.values for these estimated parameters. And we see the fitted values of cmedv for the existing rm; these values will naturally lie on the straight-line depicting the model. We will examine this augment-ed data more in Section 10.\nTo predict cmedv with new values of rm, we use predict. Let us now try to make predictions with some new data:\n\nnew &lt;- tibble(rm = seq(3, 10)) # must be named \"rm\"\nnew %&gt;% mutate(predictions =\n                 stats::predict(\n                   object = housing_lm,\n                   newdata = .,\n                   se.fit = FALSE\n                 ))\n\n\n  \n\n\n\nNote that “negative values” for predicted cmedv would have no meaning!\n\n\nAll that is very well, but what is happening under the hood of the lm command? Consider the cmedv (target) variable and the rm feature/predictor variable. What we do is:\n\nPlot a scatter plot gf_point(cmedv ~ rm, housing)\n\nFind a line that, in some way, gives us some prediction of cmedv for any given rm\n\nCalculate the errors in prediction and use those to find the “best” line.\nUse that “best” line henceforth as a model for prediction.\n\nHow does one fit the “best” line? Consider a choice of “lines” that we can use to fit to the data. Here are 6 lines of varying slopes (and intercepts ) that we can try as candidates for the best fit line:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt should be apparent that while we cannot determine which line may be the best, the worst line seems to be the one in the final plot, which ignores the x-variable rm altogether. This corresponds to the NULL Hypothesis, that there is no relationship between the two variables. Any of the other lines could be a decent candidate, so how do we decide?\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn Fig A, the horizontal blue line is the overall mean of cmedv, denoted as \\(\\mu_{tot}\\). The vertical green lines to the points show the departures of each point from this overall mean, called residuals. The sum of squares of these residuals in Fig A is called the Total Sum of Squares (SST).\n\\[\nSST = \\Sigma (y - \\mu_{tot})^2\n\\tag{2}\\]\nIn Fig B, the vertical red lines are the residuals of each point from the potential line of fit. The sum of the squares of these lines is called the Total Error Sum of Squares (SSE).\n\\[\nSSE = \\Sigma [(y - a - b * rm)^2]\n\\tag{3}\\]\nIt should be apparent that if there is any positive linear relationship between cmedv and rm,then \\(SSE &lt; SST\\).\nHow do we get the optimum slope + intercept? If we plot the \\(SSE\\) as a function of varying slope, we get:\n\n\n\n\n\n\n\n\nWe see that there is a quadratic minimum \\(SSE\\) at the optimum value of slope and at all other slopes, the \\(SSE\\) is higher. We can use this to find the optimum slope, which is what the function lm does.\n\n\nLet us hand-calculate the numbers so we know what the test is doing. Here is the SST: we pretend that there is no relationship between cmedv ans rm and compute a NULL model:\n\n# Calculate overall sum squares SST\n\nSST &lt;- deviance(lm(cmedv ~ 1, data = housing))\nSST\n\n[1] 42577.74\n\n\nAnd here is the SSE:\n\nSSE &lt;- deviance(housing_lm)\nSSE\n\n[1] 21934.39\n\n\nGiven that the model leaves unexplained variations in cmedv to the extent of \\(SSE\\), we can compute the \\(SSR\\), the Regression Sum of Squares, the amount of variation in cmedv that the linear model does explain:\n\nSSR &lt;- SST - SSE\nSSR\n\n[1] 20643.35\n\n\nWe have \\(SST = 42577.74\\), \\(SSE = 21934.39\\) and therefore \\(SSR = 20643.35\\).\nIn order to calculate the F-Statistic, we need to compute the variances, using these sum of squares. We obtain variances by dividing by their Degrees of Freedom:\n\\[\nF_{stat} = \\frac{SSR / df_{SSR}}{SSE / df_{SSE}}\n\\]\nwhere \\(df_{SSR}\\) and \\(df_{SSE}\\) are respectively the degrees of freedom in SSR and SSE.\nLet us calculate these Degrees of Freedom. If we have \\(n=\\) 506 observations of data, then:\n\n\n\\(SST\\) clearly has degree of freedom \\(n-1 = 505\\), since it uses all observations but loses one degree to calculate the global mean.\n\n\\(SSE\\) was computed using the slope and intercept, so it has \\((n-2) = 504\\) as degrees of freedom.\nAnd therefore \\(SSR\\) being their difference has just \\(1\\) degree of freedom.\n\nNow we are ready to compute the F-statistic:\n\nn &lt;- housing %&gt;% count() %&gt;% as.numeric()\ndf_SSR &lt;- 1\ndf_SSE &lt;- n -2\nF_stat &lt;- (SSR/df_SSR) / (SSE/df_SSE)\nF_stat\n\n[1] 474.3349\n\n\nThe F-stat is compared with a critical value of the F-statistic, which is computed using the formula for the f-distribution in R. As with our hypothesis tests, we set the significance level to 0.95, and quote the two relevant degrees of freedom as parameters to qf() which computes the critical F value as a quartile:\n\nF_crit &lt;-  qf(p = 0.95,     # Significance level is 5%\n              df1 = df_SSR, # Numerator degrees of freedom \n              df2 = df_SSE) # Denominator degrees of freedom\nF_crit\n\n[1] 3.859975\n\nF_stat\n\n[1] 474.3349\n\n\nThe F_crit value can also be seen in a plot4:\n\nmosaic::pdist(dist = \"f\",\n              q = F_crit, \n              df1 = df_SSR, df2 = df_SSE)\n\n\n\n\n\n\n\n[1] 0.95\n\n\nAny value of F more than the \\(F_{crit}\\) occurs with smaller probability than 0.05. Our F_stat is much higher than \\(F_{crit}\\), by orders of magnitude! And so we can say with confidence that rm has a significant effect on cmedv.\nThe value of R.squared is also calculated from the previously computed sums of squares:\n\\[\nR.squared = \\frac{SSR}{SST} = \\frac{SSY-SSE}{SST}\n\\tag{4}\\]\n\nr_squared &lt;- (SST - SSE)/SST\nr_squared\n\n[1] 0.484839\n\n# Also computable by\n# mosaic::rsquared(housing_lm)\n\nSo R.squared = 0.484839\nThe value of Slope and Intercept are computed using a maximum likelihood derivation and the knowledge that the means square error is a minimum at the optimum slope: for a linear model \\(y \\sim mx + c\\)\n\\[\nslope = \\frac{\\Sigma[(y - y_{mean})*(x - x_{mean})]}{\\Sigma(x - x_{mean})^2}\n\\]\n\n\n\n\n\n\nTip\n\n\n\nNote that the slope is equal to the ratio of the covariance of x and y to the variance of x.\n\n\nand\n\\[\nIntercept = y_{mean} - slope * x_{mean}\n\\]\n\nslope &lt;- mosaic::cov(cmedv ~ rm, data = housing) / mosaic::var(~ rm, data = housing)\nslope\n\n[1] 9.09967\n\nintercept &lt;- mosaic::mean(~ cmedv, data = housing) - slope * mosaic::mean(~ rm, data = housing)\nintercept\n\n[1] -34.65924\n\n\nSo, there we are! All of this is done for us by one simple formula, lm()!\n\n\nThere is a very neat package called ggstatsplot5 that allows us to plot very comprehensive statistical graphs. Let us quickly do this:\n\nlibrary(ggstatsplot)\nhousing_lm %&gt;%\n  ggstatsplot::ggcoefstats(title = \"Linear Model for Boston Housing\", caption = \"Using ggstatsplot\")\n\n\n\n\n\n\n\nThis chart shows the estimates for the intercept and rm along with their error bars, the t-statistic, degrees of freedom, and the p-value.\nWe can also obtain crisp-looking model tables from the new supernova package 6, which is based on the methods discussed in Judd et al.\nlibrary(supernova)\nsupernova::supernova(housing_lm)\n\n\n\n Analysis of Variance Table (Type III SS)\n Model: cmedv ~ rm\n\n                                SS  df        MS       F   PRE     p\n ----- --------------- | --------- --- --------- ------- ----- -----\n Model (error reduced) | 20643.347   1 20643.347 474.335 .4848 .0000\n Error (from model)    | 21934.392 504    43.521                    \n ----- --------------- | --------- --- --------- ------- ----- -----\n Total (empty model)   | 42577.739 505    84.312                    \n\n\n\nThis table is very neat in that it gives the Sums of Squares for both the NULL (empty) model, and the current model for comparison. The PRE entry is the Proportional Reduction in Error, a measure that is identical with r.squared, which shows how much the model reduces the error compared to the NULL model(48%). The PRE idea is nicely discussed in Judd et al Section 12",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Inferential Modelling",
      "Modelling with Linear Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#sec-diagnostics",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#sec-diagnostics",
    "title": "Modelling with Linear Regression",
    "section": "\n Workflow: Model Checking and Diagnostics",
    "text": "Workflow: Model Checking and Diagnostics\nWe will follow much of the treatment on Linear Model diagnostics, given here on the STHDA website.\n\nA first step of this regression diagnostic is to inspect the significance of the regression beta coefficients, as well as, the R.square that tells us how well the linear regression model fits to the data.\nFor example, the linear regression model makes the assumption that the relationship between the predictors (x) and the outcome variable is linear. This might not be true. The relationship could be polynomial or logarithmic.\nAdditionally, the data might contain some influential observations, such as outliers (or extreme values), that can affect the result of the regression.\nTherefore, the regression model must be closely diagnosed in order to detect potential problems and to check whether the assumptions made by the linear regression model are met or not. To do so, we generally examine the distribution of residuals errors, that can tell us more about our data.\n\n\n Checks for Uncertainty\nLet us first look at the uncertainties in the estimates of slope and intercept. These are most easily read off from the broom::tidy-ed model:\n\n# housing_lm_tidy &lt;-  housing_lm %&gt;% broom::tidy()\nhousing_lm_tidy\n\n\n  \n\n\n\nPlotting this is simple too:\n\nhousing_lm_tidy %&gt;%\n  gf_col(estimate ~ term, fill = ~ term, width = 0.25) %&gt;% \n  gf_hline(yintercept = 0) %&gt;% \n  gf_errorbar(conf.low + conf.high ~ term, \n              width = 0.1, \n              title = \"Model Estimates with Confidence Intervals\") \n\n\n\n\n\n\n\n\n Checks for Constant Variance/Heteroscedasticity\nLinear Modelling makes 4 fundamental assumptions:(“LINE”)\n\n\nLinear relationship between y and x\nObservations are independent.\nResiduals are normally distributed\nVariance of the y variable is equal at all values of x.\n\nWe can check these using checks and graphs: Here we plot the residuals against the independent/feature variable and see if there is a gross variation in their range\nhousing_lm_augment %&gt;% \n  gf_point(.resid ~ .fitted, title = \"Residuals vs Fitted\") %&gt;%\n  gf_smooth(method = \"loess\")\nhousing_lm_augment %&gt;% \n  gf_hline(yintercept = 0, colour = \"grey\", linewidth = 2) %&gt;%\n  gf_point(.resid ~ cmedv, title = \"Residuals vs Target Variable\") \nhousing_lm_augment %&gt;% \n  gf_dhistogram(~ .resid, title = \"Histogram of Residuals\") %&gt;% \n  gf_fitdistr()\nhousing_lm_augment %&gt;% \n  gf_qq(~ .resid, title = \"Q-Q Residuals\") %&gt;% \n  gf_qqline() \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Q-Q plot of residuals also has significant deviations from the normal quartiles. The residuals are not quite “like the night sky”, i.e. random enough. These point to the need for a richer model, with more predictors. The “trend line” of residuals vs predictors show a U-shaped pattern, indicating significant nonlinearity: there is a curved relationship in the graph. The solution can be a nonlinear transformation of the predictor variables, such as \\(\\sqrt(X)\\), \\(log(X)\\), or even \\(X^2\\). For instance, we might try a model for cmedv using \\(rm^2\\) instead of just rm as we have done. This will still be a linear model!\n\n\n\n\n\n\nTip\n\n\n\nBase R has a crisp command to plot these diagnostic graphs. But we will continue to use ggformula.\nplot(housing_lm)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nOne of the ggplot extension packages named lindia also has a crisp command to plot these diagnostic graphs.\n\nlibrary(lindia)\ngg_diagnose(housing_lm, \n            mode = \"base_r\", # plots like those with base-r\n  theme = theme(axis.title = element_text(size = 6, face = \"bold\"),\n                title = element_text(size = 8))\n)\n\n\n\n\n\n\n\n\n\n\n\nThe r-squared for a model lm(cmedv ~ rm^2) shows some improvement:\n\n\n[1] 0.5501221",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Inferential Modelling",
      "Modelling with Linear Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#conclusions",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#conclusions",
    "title": "Modelling with Linear Regression",
    "section": "\n Conclusions",
    "text": "Conclusions\nWe have seen how starting from a basic EDA of the data, we have been able to choose a single Quantitative predictor variable to model a Quantitative target variable, using Linear Regression. As stated earlier, we may have wish to use more than one predictor variables, to build more sophisticated models with improved prediction capability. And there is more than one way of selecting these predictor variables, which we will examine in the Tutorials in Section 1.\nSecondly, sometimes it may be necessary to mathematically transform the variables in the dataset to enable the construction of better models, something that was not needed here.\nWe may also encounter cases where the predictor variables seem to work together; one predictor may influence “how well” another predictor works, something called an interaction effect or a synergy effect. We might then have to modify our formula to include interaction terms that look like \\(predictor1 \\times predictor2\\).\nSo our Linear Modelling workflow might look like this: we have not seen all stages yet, but that is for another course module or tutorial!\n\n\n\n\nOur Linear Regression WorkflowDataEDACheck RelationshipsBuild ModelTransform VariablesTry Multiple Regression and/or interaction effectsCheck Model DiagnosticsCheck R^2Interpret ModelApply ModelSimple orComplex Model DecisionIs the Model Possible?inspectggformulaglimpseskimcorrplotcorrgramggformula + purrrcor.testAll GoodInadequate11 Still Inadequate22Check R^2",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Inferential Modelling",
      "Modelling with Linear Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#sec-references",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#sec-references",
    "title": "Modelling with Linear Regression",
    "section": "\n References",
    "text": "References\n\n\nhttps://mlu-explain.github.io/linear-regression/\n\nThe Boston Housing Dataset, corrected version. StatLib @ CMU, lib.stat.cmu.edu/datasets/boston_corrected.txt\n\n\nhttps://feliperego.github.io/blog/2015/10/23/Interpreting-Model-Output-In-R\n\nAndrew Gelman, Jennifer Hill, Aki Vehtari. Regression and Other Stories, Cambridge University Press, 2023.Available Online\n\nMichael Crawley, The R Book,second edition, 2013. Chapter 11.\n\nGareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, Introduction to Statistical Learning, Springer, 2021. Chapter 3. https://www.statlearning.com/\n\nDavid C Howell, Permutation Tests for Factorial ANOVA Designs\n\nMarti Anderson, Permutation tests for univariate or multivariate analysis of variance and regression\n\n\nhttp://r-statistics.co/Assumptions-of-Linear-Regression.html\n\nJudd, Charles M., Gary H. McClelland, and Carey S. Ryan. 2017. “Introduction to Data Analysis.” In, 1–9. Routledge. https://doi.org/10.4324/9781315744131-1. Also see http://www.dataanalysisbook.com/index.html\n\nPatil, I. (2021). Visualizations with statistical details: The ‘ggstatsplot’ approach. Journal of Open Source Software, 6(61), 3167,https://doi:10.21105/joss.03167\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\ncorrgram\n1.14\nWright (2021)\n\n\ncorrplot\n0.92\nWei and Simko (2021)\n\n\nGGally\n2.2.1\nSchloerke et al. (2024)\n\n\nggstatsplot\n0.12.3\nPatil (2021)\n\n\nISLR\n1.4\nJames et al. (2021)\n\n\njanitor\n2.2.0\nFirke (2023)\n\n\nlindia\n0.10\nLee and Ventura (2023)\n\n\nreghelper\n1.1.2\nHughes and Beiner (2023)\n\n\nsupernova\n3.0.0\nBlake et al. (2024)\n\n\ntidyverse\n2.0.0\nWickham et al. (2019)\n\n\n\n\n\n\nBlake, Adam, Jeff Chrabaszcz, Ji Son, and Jim Stigler. 2024. supernova: Judd, McClelland, & Ryan Formatting for ANOVA Output. https://CRAN.R-project.org/package=supernova.\n\n\nFirke, Sam. 2023. janitor: Simple Tools for Examining and Cleaning Dirty Data. https://CRAN.R-project.org/package=janitor.\n\n\nHughes, Jeffrey, and David Beiner. 2023. reghelper: Helper Functions for Regression Analysis. https://CRAN.R-project.org/package=reghelper.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Rob Tibshirani. 2021. ISLR: Data for an Introduction to Statistical Learning with Applications in r. https://CRAN.R-project.org/package=ISLR.\n\n\nLee, Yeuk Yu, and Samuel Ventura. 2023. lindia: Automated Linear Regression Diagnostic. https://CRAN.R-project.org/package=lindia.\n\n\nPatil, Indrajeet. 2021. “Visualizations with statistical details: The ‘ggstatsplot’ approach.” Journal of Open Source Software 6 (61): 3167. https://doi.org/10.21105/joss.03167.\n\n\nSchloerke, Barret, Di Cook, Joseph Larmarange, Francois Briatte, Moritz Marbach, Edwin Thoen, Amos Elberg, and Jason Crowley. 2024. GGally: Extension to “ggplot2”. https://CRAN.R-project.org/package=GGally.\n\n\nWei, Taiyun, and Viliam Simko. 2021. R Package “corrplot”: Visualization of a Correlation Matrix. https://github.com/taiyun/corrplot.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWright, Kevin. 2021. corrgram: Plot a Correlogram. https://CRAN.R-project.org/package=corrgram.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Inferential Modelling",
      "Modelling with Linear Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#footnotes",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/LinReg.html#footnotes",
    "title": "Modelling with Linear Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\nThe model is linear in the parameters \\(\\beta_i\\), e.g. We can have this: \\[\ny_i \\sim \\beta_1*x_i + \\beta_0\\\\\n\\] or \\[\ny_1 \\sim exp(\\beta_1)*x_i + \\beta_0\n\\] but not: \\[\ny_i \\sim \\beta_1*exp(\\beta_2*x_i) + \\beta_0\\\\\n\\] or \\[\ny_i \\sim \\beta_1 *x^{\\beta_2} + \\beta_0\n\\]↩︎\nGareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, Introduction to Statistical Learning, Springer, 2021. Chapter 3. Linear Regression. Available Online↩︎\nMichael Crawley, The R Book, Third Edition 2023. Chapter 9. Statistical Modelling↩︎\nMichael Crawley, The R Book, Third Edition 2023. Chapter 9. Statistical Modelling↩︎\nhttps://indrajeetpatil.github.io/ggstatsplot/reference/ggcoefstats.html↩︎\nhttps://github.com/UCLATALL/supernova↩︎",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Inferential Modelling",
      "Modelling with Linear Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/20-SimplexMethodIntuitive/index.html",
    "href": "content/courses/Analytics/Prescriptive/Modules/20-SimplexMethodIntuitive/index.html",
    "title": "💭 The Simplex Method - Intuitively",
    "section": "",
    "text": "knitr::opts_chunk$set(\n  echo = FALSE,\n  collapse = TRUE,\n  #cache = TRUE, autodep = TRUE, \n  comment = \"#\",\n  fig.show = \"asis\", \n  warning=FALSE, message=FALSE, fig.align = \"center\",\n  scipen = 1, digits = 2)\n\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(gMOIP)",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Prescriptive Modelling",
      "💭 The Simplex Method - Intuitively"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/20-SimplexMethodIntuitive/index.html#what-is-the-simplex-method",
    "href": "content/courses/Analytics/Prescriptive/Modules/20-SimplexMethodIntuitive/index.html#what-is-the-simplex-method",
    "title": "💭 The Simplex Method - Intuitively",
    "section": "What is the Simplex Method?",
    "text": "What is the Simplex Method?\nTo be written up.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Prescriptive Modelling",
      "💭 The Simplex Method - Intuitively"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/20-SimplexMethodIntuitive/index.html#a-random-walk-with-the-simplex-method",
    "href": "content/courses/Analytics/Prescriptive/Modules/20-SimplexMethodIntuitive/index.html#a-random-walk-with-the-simplex-method",
    "title": "💭 The Simplex Method - Intuitively",
    "section": "A Random Walk with the Simplex Method",
    "text": "A Random Walk with the Simplex Method\nLet us try to form a geometric intuition for the Simplex method.\nWe will define an LP problem, and geometrically traverse the steps the Simplex method might take to solve for the optimum solution.\nLet us define a problem:\n\\[\nMaximise\\ 7.75x_1 + 10x_2\\\\\n\\] \\[\nSubject\\ to\\\\\n  \\begin{cases}\n    C1: -3x_1 + 2x_2 &&lt;= 3\\\\\n    C2: 2x_1 + 4x_2 &&lt;= 27\\\\\n    C3: 9x_1 + 10x_2 &&lt;= 90\\\\\n    x_1, x_2 &gt;= 0\n  \\end{cases}\n\\]\nThe Objective function is: \\(7.75x_1 + 10x_2\\)\nThe Constraints are defined by the three inequalities \\(C1::C3\\). In order to plot these, we convert the inequalities to equalities and plot these as lines. Each line splits the \\(x_1:x_2\\) plane into two half-planes. The inequality part is then taken into account by choosing the appropriate half-plane created by the equation. The intersection of all the half-planes defined by the constraints is the Feasibility Region.\nThe Feasibility region for this LP problem is plotted below:\n\n\n\n\n\n\n\n\nThe corner points of the Feasibility Region are:\n\n\n\n  \n\n\n\nRecall that:\n\nThe optimum in an LP problem is found on the boundary, at one of the vertices\nAt each of these vertices one or more constraints (\\(C1::C_n\\)) is tight, i.e. there is no slack.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Prescriptive Modelling",
      "💭 The Simplex Method - Intuitively"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/20-SimplexMethodIntuitive/index.html#procedure",
    "href": "content/courses/Analytics/Prescriptive/Modules/20-SimplexMethodIntuitive/index.html#procedure",
    "title": "💭 The Simplex Method - Intuitively",
    "section": "Procedure",
    "text": "Procedure\n\nWe start with an arbitrary point on the edge of the Feasibility Region. \\(A = (0,0)\\) is a common choice. At this point, since all variables are \\(0\\), the objective function is also \\(0\\).\nWe (arbitrarily) decide to move along the boundary of the Feasibility Region, to another FSP. We arbitrarily chose the \\(x_1\\) axis, and set/keep \\(x_2 = 0\\). We now wish to find out the \\(x_1\\) coordinate of the next FSP point. This would be at the intersection of the \\(x_1\\) axis and one of the Constraint lines.\nAll the three Constraint Lines would possibly intersect the \\(x_1\\) axis. We need to choose that intercept point that has the smallest, non-negative \\(x_1\\) intercept value. (Why?)\nSo, which Constraint Line intersects the \\(x_1\\) axis at the smallest value? Is it point B, or point F?\nTo find out, we substitute \\(x_2 = 0\\) in each of the Constraint equations, and solve for the \\(x_1\\):\\[\n\\begin{cases}\nC1: -3x_1 + 2 \\times 0 = 3 \\ =&gt; x_1 = \\color{red}{-1}\\\\\nC2: 2x_1 + 4\\times0 = 27 \\ =&gt; x_1 = 13.5\\ Point\\ F\\\\\n   {\\mathbf{ \\color{lightgreen}{C3}: 9x_1 + 10\\times0 = 90 \\ =&gt; x_1 = 10\\  \\color{lightgreen}{Point\\ B}}}\n\\end{cases}\n\\]\nNegative values for any variable are not permitted. So the smallest value of intercept is \\(x_1 = 10\\) for Constraint \\(C3\\). We therefore move to point \\(B(10,0)\\). At this point the objective function has improved to:\n\n\\[\nObjective = 7.75\\times 10 + 10\\times0 = 77.5\\ at\\ Point\\ B\n\\]\n\nWe now start from Point B, and move to the next nearest point. In identical fashion to Step2, we “imagine” that we move along a new axis defined by:\\[\nIntercept = Point\\ B(10,0)\\\\\n\\] \\[\nEquation = Constraint\\ C3: 9x_1 + 10x_2 = 90\\\\\n\\] We express \\(x_1\\) in terms of \\(x_2\\) with \\(C3\\): \\[\n\\hat C3: x_1 = \\frac{90 - 10x_2}{9}\n\\] As in Step 2, we substitute this equation \\(\\hat C3\\) into the other two constraints, \\(C1\\) and \\(C2\\): \\[\n\\begin{cases}\nC1: -3\\times \\frac{90 - 10x_2}{9} + 2x_2 = 3 \\ =&gt; x_2 = 6.18\\ Point\\ K\\\\\n{\\mathbf{ \\color{lightgreen}{C2}: 2\\times \\frac{90 - 10x_2}{9}+ 4x_2 = 27 =&gt; x_2 = 3.93\\ \\color{lightgreen}{Point\\ C}}}\n\\end{cases}\n\\] As before we choose the smaller of the two intercepts, so \\(x_2 = 3.93\\). Calculating for \\(x_1\\), we get point \\(C(5.63, 3.93)\\). At this point the objective function has improved to:\n\n\\[\n7.75\\times 5.63 + 10\\times 3.93 = 82.9\\ at\\ Point\\ C\n\\]\n\nWe now proceed along the constraint line \\(C2\\) towards the next point. In identical fashion to Step 2 and 3, we “imagine” that we move along a new axis defined by: \\[\nIntercept = Point\\ C(5.63,3.93)\n\\] \\[\nEquation = Constraint\\ C2: 2x_1 + 4x_2 = 27 \\\\\n\\] Again, We express \\(x_1\\) in terms of \\(x_2\\) with \\(C2\\) this time: \\[\n\\hat C2: x_1 = \\frac{27 - 4x_2}{2}\n\\] As in Step 2 and, we substitute this equation \\(\\hat C2\\) into the other constraint, the only remaining \\(C1\\): \\[\n{\\mathbf C1: -3\\times \\frac{27 - 4x_2}{2} + 2x_2 = 3 \\ =&gt; x_2 = 5.44\\ Point\\ D\\\\}\n\\] Calculating for \\(x_1\\), we get point \\(C(2.63, 5.44)\\). At this point the objective function has improved decreased to: \\[\n7.75\\times 2.63 + 10\\times 5.44 = 74.8\\ at\\ Point\\ D\n\\] Since this value for the Objective function is smaller than that at the previous point, our search terminates and we decide that Point \\(C(5.63,3.93)\\) is the optimal point.\nSo the final result is: \\[\n   x_1(max) = 5.63\\\\\n\\] \\[\n   x_2(max) = 3.93\\\\\n\\] \\[\n   Maximum\\ Objective\\ Function\\ Value = 82.9\n\\] The final result is plotted below:",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Prescriptive Modelling",
      "💭 The Simplex Method - Intuitively"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/20-SimplexMethodIntuitive/index.html#summary",
    "href": "content/courses/Analytics/Prescriptive/Modules/20-SimplexMethodIntuitive/index.html#summary",
    "title": "💭 The Simplex Method - Intuitively",
    "section": "Summary",
    "text": "Summary\nThe essence of this “intuitive method” can be captured as follows:\n\nStart from a known simple point on the edge of Feasibility Region, e.g. (0,0), since the two coordinate axes frequently form two edges to the Feasibility Region.\n\nMove along one of the axis to find a first adjacent edge point. This adjacent point corresponds to the “tightening” of one or other of the Constraint equations(i.e. slack = 0 for that Constraint)\n\nCalculate the Objective function at that point.\n\nUse this new point as the next starting point and move along the Constraint line from the previous step.\n\nRepeat step 2 and 3, calculating the Objective function each time.\n\nKeep the solution point where the objective function hits a maximum, i.e. when moving to the next point reduces the value of the Objective function.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Prescriptive Modelling",
      "💭 The Simplex Method - Intuitively"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html",
    "href": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html",
    "title": "📐 Intro to Linear Programming",
    "section": "",
    "text": "library(blogdown)\nlibrary(gMOIP)\n# See: https://relund.github.io/gMOIP/index.html\nlibrary(knitr)\nlibrary(rgl)\nrgl::setupKnitr()\noptions(rgl.useNULL=TRUE)\nopts_chunk$set(\n  echo = FALSE,\n  collapse = TRUE,\n  #cache = TRUE, autodep = TRUE, \n  comment = \"#&gt;\",\n  fig.show = \"asis\", \n  warning=FALSE, message=FALSE, include = TRUE, \n  out.width = \"99%\", fig.width = 8, fig.align = \"center\", fig.asp = 0.62\n)",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Prescriptive Modelling",
      "📐 Intro to Linear Programming"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#introduction",
    "href": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#introduction",
    "title": "📐 Intro to Linear Programming",
    "section": "Introduction",
    "text": "Introduction\nWhat is Linear Programming?",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Prescriptive Modelling",
      "📐 Intro to Linear Programming"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#demonstration-of-level-curve",
    "href": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#demonstration-of-level-curve",
    "title": "📐 Intro to Linear Programming",
    "section": "Demonstration of Level Curve",
    "text": "Demonstration of Level Curve",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Prescriptive Modelling",
      "📐 Intro to Linear Programming"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#linear-programming-solver",
    "href": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#linear-programming-solver",
    "title": "📐 Intro to Linear Programming",
    "section": "Linear Programming Solver",
    "text": "Linear Programming Solver",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Prescriptive Modelling",
      "📐 Intro to Linear Programming"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#linear-programming-in-3d-view",
    "href": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#linear-programming-in-3d-view",
    "title": "📐 Intro to Linear Programming",
    "section": "Linear Programming in 3D view",
    "text": "Linear Programming in 3D view",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Prescriptive Modelling",
      "📐 Intro to Linear Programming"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#linear-programming-interactive",
    "href": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#linear-programming-interactive",
    "title": "📐 Intro to Linear Programming",
    "section": "Linear Programming Interactive",
    "text": "Linear Programming Interactive\nLet us say we have a Linear Programming problem with 3 variables: We define the model:\n\\[\nMaximise : 20x_1 + 10x_2 + 15x_3\\\\\nSubject \\ to \\\\\n\\\\\nx_1 + x_2 + x_3 &lt;= 10\\\\\n3x_1 + x_3 &lt;= 24\n\\]\nHere is the interactive LP Polytope:",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Prescriptive Modelling",
      "📐 Intro to Linear Programming"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#references",
    "href": "content/courses/Analytics/Prescriptive/Modules/10-IntroLinearProg/index.html#references",
    "title": "📐 Intro to Linear Programming",
    "section": "References",
    "text": "References\n\nVirginia Postrel, Operations Everything, Boston Globe, Hune 27, 2004. http://archive.boston.com/news/globe/ideas/articles/2004/06/27/operation_everything?pg=full",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Prescriptive Modelling",
      "📐 Intro to Linear Programming"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/10-Reports-flextable/index.html",
    "href": "content/courses/Analytics/Workflow/Modules/10-Reports-flextable/index.html",
    "title": "Using FlexDashboard in R",
    "section": "",
    "text": "R Tutorial",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Workflow",
      "Using FlexDashboard in R"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/10-Reports-flextable/index.html#references",
    "href": "content/courses/Analytics/Workflow/Modules/10-Reports-flextable/index.html#references",
    "title": "Using FlexDashboard in R",
    "section": "References",
    "text": "References\n\nFlexdashboard Basics https://rstudio.github.io/flexdashboard/articles/flexdashboard.html\nFlexdashboard Examples https://rstudio.github.io/flexdashboard/articles/examples.html\nShannon Haymond,Create laboratory business intelligence dashboards for free using R: A tutorial using the flexdashboard package, Journal of Mass Spectrometry and Advances in the Clinical Lab, Volume 23, 2022,Pages 39-43, ISSN 2667-145X, https://doi.org/10.1016/j.jmsacl.2021.12.002.\nhttps://posit.co/blog/flexdashboard-easy-interactive-dashboards-for-r/",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Workflow",
      "Using FlexDashboard in R"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/listing.html",
    "href": "content/courses/Analytics/Inference/listing.html",
    "title": "Statistical Inference",
    "section": "",
    "text": "Important\n\n\n\nStatistical inference is the process of drawing conclusions about the entire population based on the information in a sample.\n\n\nIn this Section we will examine samples from populations and find procedures for estimating parameters such as means and sd. We will also devise procedures for comparing means and variances across more than one population. The conditions that make these procedures possible and accurate will also be studied and we will find alternative methods when those assumptions breakdown.\nBased on our ideas of data and types of variables, here is a table of what we may infer, based on the underlying data:\n\nData Types and Inference\n\n\n\n\n\n\n\n\nVariable(s)\nEstimating What?\nPopulation Parameter\nSample Statistic\n\n\n\n\nSingle Qual variable\nProportion\np\n\\(\\hat{p}\\)\n\n\nSingle Quant variable\nMean\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\n\nTwo Qual Variables\nDifference in Proportions\n\\(p_1 -p_2\\)\n\\(\\hat{p_1} - \\hat{p_2}\\)\n\n\nOne Qual, one Quant\nDifference in Means\n\\(\\mu_1 - \\mu_2\\)\n\\(\\bar{x_1}-\\bar{x_2}\\)\n\n\nTwo Quant variables\nCorrelation\n\\(\\rho\\)\nr\n\n\n\nWe will examine inference procedures for all these cases.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/listing.html#what-is-inference",
    "href": "content/courses/Analytics/Inference/listing.html#what-is-inference",
    "title": "Statistical Inference",
    "section": "",
    "text": "Important\n\n\n\nStatistical inference is the process of drawing conclusions about the entire population based on the information in a sample.\n\n\nIn this Section we will examine samples from populations and find procedures for estimating parameters such as means and sd. We will also devise procedures for comparing means and variances across more than one population. The conditions that make these procedures possible and accurate will also be studied and we will find alternative methods when those assumptions breakdown.\nBased on our ideas of data and types of variables, here is a table of what we may infer, based on the underlying data:\n\nData Types and Inference\n\n\n\n\n\n\n\n\nVariable(s)\nEstimating What?\nPopulation Parameter\nSample Statistic\n\n\n\n\nSingle Qual variable\nProportion\np\n\\(\\hat{p}\\)\n\n\nSingle Quant variable\nMean\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\n\nTwo Qual Variables\nDifference in Proportions\n\\(p_1 -p_2\\)\n\\(\\hat{p_1} - \\hat{p_2}\\)\n\n\nOne Qual, one Quant\nDifference in Means\n\\(\\mu_1 - \\mu_2\\)\n\\(\\bar{x_1}-\\bar{x_2}\\)\n\n\nTwo Quant variables\nCorrelation\n\\(\\rho\\)\nr\n\n\n\nWe will examine inference procedures for all these cases.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/listing.html#an-idea-to-encourage-you-stats-lessons-from-sholay",
    "href": "content/courses/Analytics/Inference/listing.html#an-idea-to-encourage-you-stats-lessons-from-sholay",
    "title": "Statistical Inference",
    "section": "An Idea to Encourage You: Stats Lessons from Sholay!!",
    "text": "An Idea to Encourage You: Stats Lessons from Sholay!!\n\nGabbar: “Kitne Aadmi thay?\nStats Teacher: How many observations do you have? n &lt; 30 is a joke.\n\nGabbar: Kya Samajh kar aaye thay? Gabbar khus hoga? Sabaasi dega kya?\nStats Teacher: What are the levels in your Factors? Are they binary? Don’t do ANOVA just yet!\n\nGabbar: (Fires off three rounds ) Haan, ab theek hai!\nStats Teacher: Yes, now the dataset is balanced wrt the factor (Treatment and Control).\n\nGabbar: Is pistol mein teen zindagi aur teen maut bandh hai. Dekhte hain kisko kya milega.\nStats Teacher: This is our Research Question, for which we will Design an Experiment.\n\nGabbar: (Twirls the chambers of his revolver) “Hume kuchh nahi pataa!”\nStats Teacher: Let us perform a non-parametric Permutation Test for this Factor!\n\nGabbar: “Kamaal ho gaya!”\nStats Teacher: Fantastic! Our p-value is so small that we can reject the NULL Hypothesis!!\n\nGo and like this post at: https://www.linkedin.com/pulse/stat-lessons-from-sholay-arvind-venkatadri-wgtrf/?trackingId=c0b4UCTLRea6U%2Bj%2Bm4TCtw%3D%3D",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/190-TwoProp/two-props.html",
    "href": "content/courses/Analytics/Inference/Modules/190-TwoProp/two-props.html",
    "title": "🃏 Inferences Test for Two Proportions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(ggmosaic) # plotting mosaic plots for Categorical Data\n\n### Dataset from Chihara and Hesterberg's book (Second Edition)\nlibrary(resampledata)\nlibrary(vcd)",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🃏 Inferences Test for Two Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/190-TwoProp/two-props.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Inference/Modules/190-TwoProp/two-props.html#setting-up-r-packages",
    "title": "🃏 Inferences Test for Two Proportions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(ggmosaic) # plotting mosaic plots for Categorical Data\n\n### Dataset from Chihara and Hesterberg's book (Second Edition)\nlibrary(resampledata)\nlibrary(vcd)",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🃏 Inferences Test for Two Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/190-TwoProp/two-props.html#introduction",
    "href": "content/courses/Analytics/Inference/Modules/190-TwoProp/two-props.html#introduction",
    "title": "🃏 Inferences Test for Two Proportions",
    "section": "\n Introduction",
    "text": "Introduction\nWe saw from the diagram created by Allen Downey that there is only one test! We will now use this philosophy to develop a technique that allows us to mechanize several Statistical Models in that way, with nearly identical code.\nWe will use two packages in R, mosaic and the relatively new infer package, to develop our intuition for what are called permutation based statistical tests.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🃏 Inferences Test for Two Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/190-TwoProp/two-props.html#inference-for-proportions-case-study-1-gss2002-dataset",
    "href": "content/courses/Analytics/Inference/Modules/190-TwoProp/two-props.html#inference-for-proportions-case-study-1-gss2002-dataset",
    "title": "🃏 Inferences Test for Two Proportions",
    "section": "\n Inference for Proportions Case Study-1: GSS2002 dataset",
    "text": "Inference for Proportions Case Study-1: GSS2002 dataset\nWe can extend this idea to multiple proportions too.\nLet us try a dataset with Qualitative / Categorical data. This is the General Social Survey GSS dataset from the resampledata package, and we have people with different levels of Education stating their opinion on the Death Penalty. We want to know if these two Categorical variables have a correlation, i.e. can the opinions in favour of the Death Penalty be explained by the Education level?\nSince data is Categorical ( both variables ), we need to take counts in a table, and then implement a chi-square test. In the test, we will permute the Education variable to see if we can see how significant its effect size is.\n\ndata(GSS2002, package = \"resampledata\")\nglimpse(GSS2002)\n\nRows: 2,765\nColumns: 21\n$ ID            &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ Region        &lt;fct&gt; South Central, South Central, South Central, South Centr…\n$ Gender        &lt;fct&gt; Female, Male, Female, Female, Male, Male, Female, Female…\n$ Race          &lt;fct&gt; White, White, White, White, White, White, White, White, …\n$ Education     &lt;fct&gt; HS, Bachelors, HS, Left HS, Left HS, HS, Bachelors, HS, …\n$ Marital       &lt;fct&gt; Divorced, Married, Separated, Divorced, Divorced, Divorc…\n$ Religion      &lt;fct&gt; Inter-nondenominational, Protestant, Protestant, Protest…\n$ Happy         &lt;fct&gt; Pretty happy, Pretty happy, NA, NA, NA, Pretty happy, NA…\n$ Income        &lt;fct&gt; 30000-34999, 75000-89999, 35000-39999, 50000-59999, 4000…\n$ PolParty      &lt;fct&gt; \"Strong Rep\", \"Not Str Rep\", \"Strong Rep\", \"Ind, Near De…\n$ Politics      &lt;fct&gt; Conservative, Conservative, NA, NA, NA, Conservative, NA…\n$ Marijuana     &lt;fct&gt; NA, Not legal, NA, NA, NA, NA, NA, NA, Legal, NA, NA, NA…\n$ DeathPenalty  &lt;fct&gt; Favor, Favor, NA, NA, NA, Favor, NA, NA, Favor, NA, NA, …\n$ OwnGun        &lt;fct&gt; No, Yes, NA, NA, NA, Yes, NA, NA, Yes, NA, NA, NA, NA, N…\n$ GunLaw        &lt;fct&gt; Favor, Oppose, NA, NA, NA, Oppose, NA, NA, Oppose, NA, N…\n$ SpendMilitary &lt;fct&gt; Too little, About right, NA, About right, NA, Too little…\n$ SpendEduc     &lt;fct&gt; Too little, Too little, NA, Too little, NA, Too little, …\n$ SpendEnv      &lt;fct&gt; About right, About right, NA, Too little, NA, Too little…\n$ SpendSci      &lt;fct&gt; About right, About right, NA, Too little, NA, Too little…\n$ Pres00        &lt;fct&gt; Bush, Bush, Bush, NA, NA, Bush, Bush, Bush, Bush, NA, NA…\n$ Postlife      &lt;fct&gt; Yes, Yes, NA, NA, NA, Yes, NA, NA, Yes, NA, NA, NA, NA, …\n\ninspect(GSS2002)\n\n\ncategorical variables:  \n            name  class levels    n missing\n1         Region factor      7 2765       0\n2         Gender factor      2 2765       0\n3           Race factor      3 2765       0\n4      Education factor      5 2760       5\n5        Marital factor      5 2765       0\n6       Religion factor     13 2746      19\n7          Happy factor      3 1369    1396\n8         Income factor     24 1875     890\n9       PolParty factor      8 2729      36\n10      Politics factor      7 1331    1434\n11     Marijuana factor      2  851    1914\n12  DeathPenalty factor      2 1308    1457\n13        OwnGun factor      3  924    1841\n14        GunLaw factor      2  916    1849\n15 SpendMilitary factor      3 1324    1441\n16     SpendEduc factor      3 1343    1422\n17      SpendEnv factor      3 1322    1443\n18      SpendSci factor      3 1266    1499\n19        Pres00 factor      5 1749    1016\n20      Postlife factor      2 1211    1554\n                                    distribution\n1  North Central (24.7%) ...                    \n2  Female (55.6%), Male (44.4%)                 \n3  White (79.1%), Black (14.8%) ...             \n4  HS (53.8%), Bachelors (16.1%) ...            \n5  Married (45.9%), Never Married (25.6%) ...   \n6  Protestant (53.2%), Catholic (24.5%) ...     \n7  Pretty happy (57.3%) ...                     \n8  40000-49999 (9.1%) ...                       \n9  Ind (19.3%), Not Str Dem (18.9%) ...         \n10 Moderate (39.2%), Conservative (15.8%) ...   \n11 Not legal (64%), Legal (36%)                 \n12 Favor (68.7%), Oppose (31.3%)                \n13 No (65.5%), Yes (33.5%) ...                  \n14 Favor (80.5%), Oppose (19.5%)                \n15 About right (46.5%) ...                      \n16 Too little (73.9%) ...                       \n17 Too little (60%) ...                         \n18 About right (49.7%) ...                      \n19 Bush (50.6%), Gore (44.7%) ...               \n20 Yes (80.5%), No (19.5%)                      \n\nquantitative variables:  \n  name   class min  Q1 median   Q3  max mean       sd    n missing\n1   ID integer   1 692   1383 2074 2765 1383 798.3311 2765       0\n\nskimr::skim(GSS2002)\n\n\nData summary\n\n\nName\nGSS2002\n\n\nNumber of rows\n2765\n\n\nNumber of columns\n21\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n20\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nRegion\n0\n1.00\nFALSE\n7\nNor: 684, Sou: 486, Sou: 471, Mid: 435\n\n\nGender\n0\n1.00\nFALSE\n2\nFem: 1537, Mal: 1228\n\n\nRace\n0\n1.00\nFALSE\n3\nWhi: 2188, Bla: 410, Oth: 167\n\n\nEducation\n5\n1.00\nFALSE\n5\nHS: 1485, Bac: 443, Lef: 400, Gra: 230\n\n\nMarital\n0\n1.00\nFALSE\n5\nMar: 1269, Nev: 708, Div: 445, Wid: 247\n\n\nReligion\n19\n0.99\nFALSE\n13\nPro: 1460, Cat: 673, Non: 379, Chr: 65\n\n\nHappy\n1396\n0.50\nFALSE\n3\nPre: 784, Ver: 415, Not: 170\n\n\nIncome\n890\n0.68\nFALSE\n24\n400: 170, 300: 166, 250: 140, 500: 136\n\n\nPolParty\n36\n0.99\nFALSE\n8\nInd: 528, Not: 515, Not: 449, Str: 408\n\n\nPolitics\n1434\n0.48\nFALSE\n7\nMod: 522, Con: 210, Sli: 209, Sli: 159\n\n\nMarijuana\n1914\n0.31\nFALSE\n2\nNot: 545, Leg: 306\n\n\nDeathPenalty\n1457\n0.47\nFALSE\n2\nFav: 899, Opp: 409\n\n\nOwnGun\n1841\n0.33\nFALSE\n3\nNo: 605, Yes: 310, Ref: 9\n\n\nGunLaw\n1849\n0.33\nFALSE\n2\nFav: 737, Opp: 179\n\n\nSpendMilitary\n1441\n0.48\nFALSE\n3\nAbo: 615, Too: 414, Too: 295\n\n\nSpendEduc\n1422\n0.49\nFALSE\n3\nToo: 992, Abo: 278, Too: 73\n\n\nSpendEnv\n1443\n0.48\nFALSE\n3\nToo: 793, Abo: 439, Too: 90\n\n\nSpendSci\n1499\n0.46\nFALSE\n3\nAbo: 629, Too: 461, Too: 176\n\n\nPres00\n1016\n0.63\nFALSE\n5\nBus: 885, Gor: 781, Nad: 57, Oth: 16\n\n\nPostlife\n1554\n0.44\nFALSE\n2\nYes: 975, No: 236\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\nID\n0\n1\n1383\n798.33\n1\n692\n1383\n2074\n2765\n▇▇▇▇▇\n\n\n\n\nNote how all variables are Categorical !! Education has five levels, and of course DeathPenalty has three:\n\nGSS2002 %&gt;% count(Education)\n\n\n  \n\n\nGSS2002 %&gt;% count(DeathPenalty)\n\n\n  \n\n\n\nLet us drop NA entries in Education and Death Penalty and set up a Contingency Table.\n\ngss2002 &lt;- GSS2002 %&gt;% \n  dplyr::select(Education, DeathPenalty) %&gt;% \n  tidyr::drop_na(., c(Education, DeathPenalty))\n##\ngss_table &lt;- mosaic::tally(DeathPenalty ~ Education, data = gss2002)\ngss_table %&gt;% \n  addmargins()\n\n            Education\nDeathPenalty Left HS   HS Jr Col Bachelors Graduate  Sum\n      Favor      117  511     71       135       64  898\n      Oppose      72  200     16        71       50  409\n      Sum        189  711     87       206      114 1307\n\n\nContingency Table Plots\nThe Contingency Table can be plotted, as we have seen, using a mosaic plot using several packages:\n\n\nUsing ggformula\nUsing vcd\nUsing ggmosaic\n\n\n\nNeeds a little more work, to convert the Contingency Table into a tibble:\n\n# https://stackoverflow.com/questions/19233365/how-to-create-a-marimekko-mosaic-plot-in-ggplot2\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n\ngss_summary &lt;- gss2002 %&gt;%\n  mutate(\n    Education = factor(\n      Education,\n      levels = c(\"Bachelors\", \"Graduate\", \"Jr Col\", \"HS\", \"Left HS\"),\n      labels = c(\"Bachelors\", \"Graduate\", \"Jr Col\", \"HS\", \"Left HS\")\n    ),\n    DeathPenalty = as.factor(DeathPenalty)\n  ) %&gt;%\n  group_by(Education, DeathPenalty) %&gt;%\n  summarise(count = n()) %&gt;% # This is good for a chisq test\n  \n  # Add two more columns to facilitate mosaic/Marrimekko Plot\n  mutate(edu_count = sum(count), \n         edu_prop = count / sum(count)) %&gt;%\n  ungroup() \n###\ngf_col(edu_prop ~ Education, data = gss_summary,\n       width = ~ edu_count, \n       fill = ~ DeathPenalty,\n       stat = \"identity\", \n       position = \"fill\", \n       color = \"black\") %&gt;% \n  \n  gf_text(edu_prop ~ Education, \n          label = ~ scales::percent(edu_prop),\n          position = position_stack(vjust = 0.5)) %&gt;% \n  \n  gf_facet_grid(~ Education, \n                scales = \"free_x\", \n                space = \"free_x\") %&gt;% \n  \n  gf_theme(scale_fill_manual(values = c(\"orangered\", \"palegreen3\"))) \n\n\n\n\n\n\n\n\n\n\nvcd::mosaic(gss_table, gp = shading_hsv)\n\n\n\n\n\n\n\n\n\n\n#library(ggmosaic)\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n\nggplot(data = gss2002) +\n  geom_mosaic(aes(x = product(DeathPenalty, Education), \n                  fill = DeathPenalty))\n\n\n\n\n\n\n\n\n\n\nObserved Statistic: the \\(X^2\\) metric\nWhen there are multiple proportions involved, the \\(X^2\\) test is what is used.\n\n\nIntuitive Explanation\nCode\n\n\n\nLet us look at the Contingency Table that we have:\n\n\n\n\nContingency Table\n\n\nLeft HS\nHS\nJr Col\nBachelors\nGraduate\nSum\n\n\n\nFavor\n117\n511\n71\n135\n64\n898\n\n\nOppose\n72\n200\n16\n71\n50\n409\n\n\nSum\n189\n711\n87\n206\n114\n1307\n\n\n\n\n\n\nIn the chi-square test, we check whether the two ( or more ) categorical variables are independent. To do this we perform a simple check on the Contingency Table. We first re-compute the totals in each row and column, based on what we could expect if there was independence (NULL Hypothesis). If the two variables were independent, then there should be no difference between real and expected scores.\nHow do we know what scores to expect if there was no relationship between the variables?\nConsider the entry in location (1,1): 117. The number of expected entries there is probability of an entry landing in that square times the total number of entries:\n\n\\[\\begin{align}\n\n\\text{Expected Value at location[1,1]}\n&= p_{row_1} * p_{col_1} * \\text{Total Scores}\\\\\\\n&= \\frac{\\text{Row-1-Total}}{\\text{Total Scores}} * \\frac{\\text{Col-1-Total}}{\\text{Total Scores}} * \\text{Total Scores}\\\\\\\n&= \\frac{898}{1307} * \\frac{189}{1307} * 1307\\\\\\\n&= 130\n\n\n\\end{align}\\]\n\nProceeding in this way for all the 15 entries in the Contingency Table, we get the “Expected” Contingency Table. Here are both tables for comparison:\n\n\n\n\nExpected Contingency Table\n\n\nLeft HS\nHS\nJr Col\nBachelors\nGraduate\nSum\n\n\n\nFavor\n130\n489\n60\n142\n78\n898\n\n\nOppose\n59\n222\n27\n64\n36\n409\n\n\nSum\n189\n711\n87\n206\n114\n1307\n\n\n\n\n\n\n\n\n\n\nActual Contingency Table\n\n\nLeft HS\nHS\nJr Col\nBachelors\nGraduate\nSum\n\n\n\nFavor\n117\n511\n71\n135\n64\n898\n\n\nOppose\n72\n200\n16\n71\n50\n409\n\n\nSum\n189\n711\n87\n206\n114\n1307\n\n\n\n\n\n\nThe \\(X^2\\) statistic is sum of squared differences between Observed and Expected scores, scaled by the Expected Scores. For location [1,1] this would be: \\((117-130)^2/130\\). Do try to compute all of these and the \\(X^2\\) statistic by hand !!\n\n\nLet us now perform the base chisq test: We need a table and then the chisq test:\n\n# gss_table &lt;- tally(DeathPenalty ~ Education, data = gss2002)\n# gss_table\n\n# Get the observed chi-square statistic\nobservedChi2 &lt;- mosaic::chisq(tally(DeathPenalty ~ Education, data = gss2002))\nobservedChi2\n\nX.squared \n 23.45093 \n\n# Actual chi-square test\nstats::chisq.test(tally(DeathPenalty ~ Education, data = gss2002))\n\n\n    Pearson's Chi-squared test\n\ndata:  tally(DeathPenalty ~ Education, data = gss2002)\nX-squared = 23.451, df = 4, p-value = 0.0001029\n\n\nWe see that our observed \\(X^2 = 23.45\\).\n\n\n\nHypotheses Definition\nWhat would our Hypotheses be?\n\\(H_0: \\text{Education does not affect votes for Death Penalty}\\) \\(H_a: \\text{Education affects votes for Death Penalty}\\)\nPermutation Test for Education\n\nWe should now repeat the test with permutations on Education:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n\nnull_chisq &lt;- do(9999) * \n  chisq.test(tally(DeathPenalty ~ shuffle(Education), \n                   data = gss2002))\n\nhead(null_chisq)\n\n\n  \n\n\ngf_histogram( ~ X.squared, data = null_chisq) %&gt;% \n  \n  gf_vline(xintercept = observedChi2, \n           color = \"red\") \n\n\n\n\n\n\nprop1(~ X.squared &gt;= observedChi2, data = null_chisq)\n\nprop_TRUE \n    3e-04 \n\n\nThe p-value is well below our threshold of \\(0.05\\), so we would conclude that Education has a significant effect on DeathPenalty opinion!\n\n Inference for Proportions Case Study-2: TBD dataset\nTo be Written Up.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🃏 Inferences Test for Two Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/190-TwoProp/two-props.html#conclusion",
    "href": "content/courses/Analytics/Inference/Modules/190-TwoProp/two-props.html#conclusion",
    "title": "🃏 Inferences Test for Two Proportions",
    "section": "\n Conclusion",
    "text": "Conclusion\nIn our basic \\(X^2\\) test, we calculate the test statistic of \\(X^2\\) and look up a theoretical null distribution for that statistic, and see how unlikely our observed value is.\nWhy would a permutation test be a good idea here? With a permutation test, there are no assumptions of the null distribution: this is computed based on real data. We note in passing that, in this case, since the number of cases in each cell of the Contingency Table are fairly high ( &gt;= 5) the resulting NULL distribution is of the \\(X^2\\) variety.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🃏 Inferences Test for Two Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/190-TwoProp/two-props.html#references",
    "href": "content/courses/Analytics/Inference/Modules/190-TwoProp/two-props.html#references",
    "title": "🃏 Inferences Test for Two Proportions",
    "section": "\n References",
    "text": "References\n\nOpenIntro Modern Statistics: Chapter 17\nExploring the underlying theory of the chi-square test through simulation - part 1 https://www.rdatagen.net/post/a-little-intuition-and-simulation-behind-the-chi-square-test-of-independence/\nExploring the underlying theory of the chi-square test through simulation - part 2 https://www.rdatagen.net/post/a-little-intuition-and-simulation-behind-the-chi-square-test-of-independence-part-2/\nAn Online \\(\\Xi^2\\)-test calculator. https://www.statology.org/chi-square-test-of-independence-calculator/\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggmosaic\n0.3.3\nJeppson, Hofmann, and Cook (2021)\n\n\nresampledata\n0.3.1\nChihara and Hesterberg (2018)\n\n\nscales\n1.3.0\nWickham, Pedersen, and Seidel (2023)\n\n\nvcd\n1.4.12\n\nMeyer, Zeileis, and Hornik (2006); Zeileis, Meyer, and Hornik (2007); Meyer et al. (2023)\n\n\n\n\n\n\n\nChihara, Laura M., and Tim C. Hesterberg. 2018. Mathematical Statistics with Resampling and r. 2nd ed. Hoboken, NJ: John Wiley & Sons. https://sites.google.com/site/chiharahesterberg/home.\n\n\nJeppson, Haley, Heike Hofmann, and Di Cook. 2021. ggmosaic: Mosaic Plots in the “ggplot2” Framework. https://CRAN.R-project.org/package=ggmosaic.\n\n\nMeyer, David, Achim Zeileis, and Kurt Hornik. 2006. “The Strucplot Framework: Visualizing Multi-Way Contingency Tables with Vcd.” Journal of Statistical Software 17 (3): 1–48. https://doi.org/10.18637/jss.v017.i03.\n\n\nMeyer, David, Achim Zeileis, Kurt Hornik, and Michael Friendly. 2023. vcd: Visualizing Categorical Data. https://CRAN.R-project.org/package=vcd.\n\n\nWickham, Hadley, Thomas Lin Pedersen, and Dana Seidel. 2023. scales: Scale Functions for Visualization. https://CRAN.R-project.org/package=scales.\n\n\nZeileis, Achim, David Meyer, and Kurt Hornik. 2007. “Residual-Based Shadings for Visualizing (Conditional) Independence.” Journal of Computational and Graphical Statistics 16 (3): 507–25. https://doi.org/10.1198/106186007X237856.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🃏 Inferences Test for Two Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html",
    "title": "🃏 Inference for a Single Mean",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE,message = TRUE,warning = TRUE, fig.align = \"center\")\noptions(digits=2)\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(infer)\n### Dataset from Chihara and Hesterberg's book (Second Edition)\nlibrary(resampledata)\nlibrary(openintro) # datasets\nlibrary(explore) # New, Easy package for Stats Test and Viz, and other things",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🃏 Inference for a Single Mean"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html#setting-up-r-packages",
    "title": "🃏 Inference for a Single Mean",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE,message = TRUE,warning = TRUE, fig.align = \"center\")\noptions(digits=2)\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(infer)\n### Dataset from Chihara and Hesterberg's book (Second Edition)\nlibrary(resampledata)\nlibrary(openintro) # datasets\nlibrary(explore) # New, Easy package for Stats Test and Viz, and other things",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🃏 Inference for a Single Mean"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html#introduction",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html#introduction",
    "title": "🃏 Inference for a Single Mean",
    "section": "Introduction",
    "text": "Introduction\nWe saw from the diagram created by Allen Downey that there is only one test 1! We will now use this philosophy to develop a technique that allows us to mechanize several Statistical Models in that way, with nearly identical code.\nWe will use two packages in R, mosaic to develop our intuition for what are called bootstrap randomization based statistical tests. (There is also a more recent package called infer in R which can do pretty much all of this, including visualization. In my opinion, the code is a little too high-level and does not offer quite the detailed insight that the mosaic package does).",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🃏 Inference for a Single Mean"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html#case-study-1-toy-data",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html#case-study-1-toy-data",
    "title": "🃏 Inference for a Single Mean",
    "section": "\n Case Study #1: Toy data",
    "text": "Case Study #1: Toy data\nFirst we will use a toy dataset with three “imaginary” samples, \\(x, y, y2\\). Each is normally distributed and made up of 50 observations.\nWe start by creating a function that will allow us to produce samples of a given size (N) with a specified mean (mu) and standard deviation (sd).\n\nrnorm_fixed  &lt;- function(N, mu = 0, sd = 1) {\n  scale(rnorm(N))* sd + mu\n}\n\nWe create three variables: x ( explanatory) and y, y2 ( dependent ).\nset.seed(40) # for replication\n\n# Data as vectors ( for t.tests etc)\nx &lt;- rnorm_fixed(50, mu = 0.0, sd = 1) #explanatory\ny &lt;- rnorm_fixed(50, mu = 0.3, sd = 2) # dependent #1\ny2 &lt;- rnorm_fixed(50, mu = 0.5, sd = 1.5) # dependent #2\n\n# Make a tibble with all variables\nmydata_wide &lt;- tibble(x = x, y = y, y2 = y2)\n\n# Long form data\nmydata_long &lt;- \n  mydata_wide %&gt;%\n  pivot_longer(., cols = c(x,y,y2), \n               names_to = \"group\", \n               values_to = \"value\")\n\n# Long form data with only dependent variables\nmydata_long_y &lt;- \n  mydata_wide %&gt;% \n  select(-x) %&gt;% \n  pivot_longer(., cols = c(y,y2), \n               names_to = \"group\", \n               values_to = \"value\")\nmydata_wide\nmydata_long\nmydata_long_y",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🃏 Inference for a Single Mean"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html#signed-rank-values",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html#signed-rank-values",
    "title": "🃏 Inference for a Single Mean",
    "section": "“Signed Rank” Values",
    "text": "“Signed Rank” Values\nMost statistical tests use the actual values of the data variables. However, in some non-parametric statistical tests, the data are used in rank-transformed sense/order. In some cases the signed-rank of the data values is used instead of the data itself.\nSigned Rank is calculated as follows:\n1. Take the absolute value of each observation in a sample\n2. Place the ranks in order of (absolute magnitude). The smallest number has rank = 1 and so on.\n3. Give each of the ranks the sign of the original observation ( + or - )\n\nsigned_rank &lt;- function(x) {sign(x) * rank(abs(x))}",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🃏 Inference for a Single Mean"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html#introduction-to-inference-for-a-single-mean",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html#introduction-to-inference-for-a-single-mean",
    "title": "🃏 Inference for a Single Mean",
    "section": "\n Introduction to Inference for a Single Mean",
    "text": "Introduction to Inference for a Single Mean\nA series of tests deal with one mean value of a sample. The idea is to evaluate whether that mean is representative of the mean of the underlying population. Depending upon the nature of the (single) variable, the test that can be used are as follows:\n\n\n\n\n\nflowchart TD\n    A[Inference for Single Mean] --&gt;|Check Assumptions| B[Normality: Shapiro-Wilk Test shapiro.test\\n]\n    B --&gt; C{OK?}\n    C --&gt;|Yes\\n Parametric| D[t.test]\n    D &lt;--&gt;F[Linear Model\\n with Data] \n    C --&gt;|No\\n Non-Parametric| E[wilcox.test]\n    E &lt;--&gt; G[Linear Model\\n with\\n Signed-Ranks of Data]\n    C --&gt;|No\\n Non-Parametric| P[Bootstrap]\n    P &lt;--&gt; Q[Linear Model\\n with Signed-Rank\\n with Bootstrap]",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🃏 Inference for a Single Mean"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html#inspecting-and-charting-data",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html#inspecting-and-charting-data",
    "title": "🃏 Inference for a Single Mean",
    "section": "\n Inspecting and Charting Data",
    "text": "Inspecting and Charting Data\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nmydata_long %&gt;% \n  gf_density(~ value, group = ~ group, fill = ~ group) %&gt;% \n  gf_fitdistr(dist = \"dnorm\") %&gt;% \n  gf_facet_wrap(vars(group)) %&gt;% \n  gf_labs(title = \"Densities of Original Data Variables\",\n          subtitle =\"Compared with Normal Density\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations from Density Plots\n\n\n\n\nAll variables appear to be zero mean\n\n\n\\(x\\) seems to have lower \\(sd\\) than the other two\n\n\n\nTesting Assumptions in the Data\nInference\n\n\nThe t-test\nWilcoxon’s Signed-Rank Test\nLinear Model\nUsing bootstrap\nPlots\n\n\n\nA. Model\nA single number predicts y:\n\\[\ny = \\beta_0 + \\beta_1*x \\\\\n\\] \\[\n\\\\and\\ further \\\\\n\\] \\[\ny = \\beta_0\\\n\\]\nand the second term vanishes, since “there is no x”: all the x-values are made equal to zero in the linear model for a single variable !! The NULL Hypothesis therefore is:\n\\[\n\\ H_0: \\beta_0 = 0\n\\]\nNote that if we want the NULL hypothesis to be that the mean is other than zero, we can use\n\\[\nH_0:\\ \\beta_0 = \\mu \\ne 0\n\\] the lm(...., mu = some_number, ..) parameter in the command in the code.\nB. Code\nIf we compare the t.test with the appropriate lm model:\n\n# t-test\nt1 &lt;- t.test(y, mu = 0, alternative = \"two.sided\") %&gt;% broom::tidy()\nprint(t1)\n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1      0.3      1.06   0.294        49   -0.268     0.868 One Sampl… two.sided  \n\n\nSo even though y has a mean of 0.3, the confidence intervals straddle zero, and hence we cannot reject the NULL hypothesis that the true population, of which y is a sample, could have mean=0.\n\n\nSince we are dealing with the mean, the sign of the rank becomes important to use, in the case of a non-parametric single mean test.\nA. Model\n\\[\nsigned\\_rank(y) = \\beta_0\n\\]\n\\[\nH_0: \\beta_0 = 0\n\\]\nB. Code\n\n# Standard Wilcoxon Signed_Rank Test\nw1 &lt;- wilcox.test(y)\nw1\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  y\nV = 754, p-value = 0.2628\nalternative hypothesis: true location is not equal to 0\n\n# Wilcoxon test with lm\nw2 &lt;- lm(signed_rank(y) ~ 1 , data = mydata_wide)\nw2\n\n\nCall:\nlm(formula = signed_rank(y) ~ 1, data = mydata_wide)\n\nCoefficients:\n(Intercept)  \n       4.66  \n\n# t-test with signed_rank data\nw3 &lt;- t.test(signed_rank(y))\nw3\n\n\n    One Sample t-test\n\ndata:  signed_rank(y)\nt = 1.1277, df = 49, p-value = 0.265\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -3.644491 12.964491\nsample estimates:\nmean of x \n     4.66 \n\n\n\n\n\n# linear model\nlm1 &lt;- lm(y ~ 1, data = mydata_wide)\nlm1 %&gt;% summary()\n\n\nCall:\nlm(formula = y ~ 1, data = mydata_wide)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5554 -1.4845 -0.0392  1.5559  4.5119 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   0.3000     0.2828   1.061    0.294\n\nResidual standard error: 2 on 49 degrees of freedom\n\nlm1 %&gt;% confint()\n\n                 2.5 %    97.5 %\n(Intercept) -0.2683937 0.8683937\n\n\nThe confidence intervals for both the t.test and the lm model are identical.\nt-test confidence intervals:\nlinear model confidence intervals: -0.2683937, 0.8683937\n\n\nTBW\n\n\nWe can plot the y data both original and ranked to see where the mean lies in each case. The approximation to the true \\(\\beta_0\\) ( is good when the number of observations \\(n &gt;=50\\). Lindoloev has a simulation on this.. We can also plot the model using lm for both the original data and the sign-ranked data.\n\nt1\n\n\n  \n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\np1 &lt;- ggplot(mydata_wide, aes( x = 0, y = y)) +\n  geom_point(alpha = 0.4) +\n  geom_segment(data = t1, aes(y = estimate, \n                   yend = estimate, \n                   x = -0.2, xend = 0.2)) + \n  labs(title = \"Student's\\n t-Test\")\n\n# t-test using linear model\np2 &lt;- ggplot(mydata_wide, aes( x = 0, y = y)) +\n  geom_point(alpha = 0.4) +\n  geom_segment(aes(y = lm(y ~ 1)$coefficient, \n                   yend = lm(y ~ 1)$coefficient, \n                   x = -0.2, xend = 0.2)) + \n  labs(title = \"Student's\\n t-Test \\n using lm\")\n\n# Wilcoxon test, using signed-ranks of data\np3 &lt;- ggplot(mydata_wide, aes( x = 0, y = signed_rank(y))) +\n  geom_point(alpha = 0.4) +\n  geom_segment(aes(y = mean(signed_rank(y)), yend = mean(signed_rank(y)), x = -0.2, xend = 0.2)) + \n  labs(title = \"Wilcoxon \\nSigned-Rank\\n Test\")\n\n# Wilcoxon test, using signed-ranks of data, and lm\np4 &lt;- ggplot(mydata_wide, aes( x = 0, y = signed_rank(y))) +\n  geom_point(alpha = 0.4) +\n  geom_segment(aes(y = lm(signed_rank(y) ~1)$coefficient, \n                   yend = lm(signed_rank(y) ~1)$coefficient, \n                   x = -0.2, xend = 0.2)) + \n  labs(title = \"Wilcoxon \\n Signed-Rank \\n Test with lm\")\n\n\npatchwork::wrap_plots(p1,p2,p3,p4, nrow = 2, guides = \"collect\")\n\n\n\n\n\n\n\n\n\n\nAll the tests assert that the mean of y is not significantly different from zero.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🃏 Inference for a Single Mean"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html#case-study-2-exam-data",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html#case-study-2-exam-data",
    "title": "🃏 Inference for a Single Mean",
    "section": "\n Case Study #2: Exam data",
    "text": "Case Study #2: Exam data\nLet us now choose a dataset from the openintro package:\n\ndata(\"exam_grades\")\nexam_grades\n\n\n  \n\n\n\n\n Inspecting and Charting Data\nTesting Assumptions in the Data\nInference\n\n\nt.test\nWilcoxon test\nLinear Model\nUsing Bootstrap\nPlots",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🃏 Inference for a Single Mean"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html#conclusion",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html#conclusion",
    "title": "🃏 Inference for a Single Mean",
    "section": "\n Conclusion",
    "text": "Conclusion\nTBW",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🃏 Inference for a Single Mean"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html#sec-references",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html#sec-references",
    "title": "🃏 Inference for a Single Mean",
    "section": "\n References",
    "text": "References\n\nOpenIntro Modern Statistics, Chapter #17\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nexplore\n1.3.0\nKrasser (2024)\n\n\ninfer\n1.0.7\nCouch et al. (2021)\n\n\nopenintro\n2.4.0\nÇetinkaya-Rundel et al. (2022)\n\n\nresampledata\n0.3.1\nChihara and Hesterberg (2018)\n\n\nTeachHist\n0.2.1\nLange (2023)\n\n\nTeachingDemos\n2.13\nSnow (2024)\n\n\n\n\n\n\nÇetinkaya-Rundel, Mine, David Diez, Andrew Bray, Albert Y. Kim, Ben Baumer, Chester Ismay, Nick Paterno, and Christopher Barr. 2022. openintro: Data Sets and Supplemental Functions from “OpenIntro” Textbooks and Labs. https://CRAN.R-project.org/package=openintro.\n\n\nChihara, Laura M., and Tim C. Hesterberg. 2018. Mathematical Statistics with Resampling and r. 2nd ed. Hoboken, NJ: John Wiley & Sons. https://sites.google.com/site/chiharahesterberg/home.\n\n\nCouch, Simon P., Andrew P. Bray, Chester Ismay, Evgeni Chasnovski, Benjamin S. Baumer, and Mine Çetinkaya-Rundel. 2021. “infer: An R Package for Tidyverse-Friendly Statistical Inference.” Journal of Open Source Software 6 (65): 3661. https://doi.org/10.21105/joss.03661.\n\n\nKrasser, Roland. 2024. explore: Simplifies Exploratory Data Analysis. https://CRAN.R-project.org/package=explore.\n\n\nLange, Carsten. 2023. TeachHist: A Collection of Amended Histograms Designed for Teaching Statistics. https://CRAN.R-project.org/package=TeachHist.\n\n\nSnow, Greg. 2024. TeachingDemos: Demonstrations for Teaching and Learning. https://CRAN.R-project.org/package=TeachingDemos.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🃏 Inference for a Single Mean"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html#footnotes",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/single-mean.html#footnotes",
    "title": "🃏 Inference for a Single Mean",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://allendowney.blogspot.com/2016/06/there-is-still-only-one-test.html↩︎",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🃏 Inference for a Single Mean"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/70-PermTest/files/perm-tutorial.html",
    "href": "content/courses/Analytics/Inference/Modules/70-PermTest/files/perm-tutorial.html",
    "title": "Permutation Tests",
    "section": "",
    "text": "The mosaic package provides the shuffle() function as a synonym for sample(). When used without additional arguments, this will permute its first argument.\n\nShow the Code# library(mosaic)\nshuffle(1:10)\n\n [1]  6 10  3  2  5  7  4  1  8  9\n\n\nApplying shuffle() to an explanatory variable in a model allows us to test the null hypothesis that the explanatory variable has, in fact, no explanatory power. This idea can be used to test\n\nthe equivalence of two or more means,\nthe equivalence of two or more proportions,\nwhether a regression parameter is 0. (Correlations between two variables) For example:\n\nCoupled with mosaic::do() we can repeat a shuffle many times, computing a desired statistic each time we shuffle. The distribution of this computed statistic is a NULL distribution, which can then be compared with the observed statistic to decide upon the Hypothesis Test and p-value."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/70-PermTest/files/perm-tutorial.html#permutation-tests",
    "href": "content/courses/Analytics/Inference/Modules/70-PermTest/files/perm-tutorial.html#permutation-tests",
    "title": "Permutation Tests",
    "section": "Permutation Tests",
    "text": "Permutation Tests\nCase Study-1: Hot Wings Orders vs Gender\nA student conducted a study of hot wings and beer consumption at a Bar. She asked patrons at the bar to record their consumption of hot wings and beer over the course of several hours. She wanted to know if people who ate more hot wings would then drink more beer. In addition, she investigated whether or not gender had an impact on hot wings or beer consumption.\n\nShow the CodeBeerwings &lt;- read.csv(\"../../../../../../materials/data/resampling/Beerwings.csv\")\ninspect(Beerwings)\n\n\ncategorical variables:  \n    name     class levels  n missing\n1 Gender character      2 30       0\n                                   distribution\n1 F (50%), M (50%)                             \n\nquantitative variables:  \n      name   class min    Q1 median    Q3 max     mean        sd  n missing\n1       ID integer   1  8.25   15.5 22.75  30 15.50000  8.803408 30       0\n2 Hotwings integer   4  8.00   12.5 15.50  21 11.93333  4.784554 30       0\n3     Beer integer   0 24.00   30.0 36.00  48 26.20000 11.842064 30       0\n\n\nLet us calculate the observed difference in Hotwings consumption between Males and Females ( Gender)\n\nShow the Codemean(Hotwings ~ Gender, data = Beerwings)\n\n        F         M \n 9.333333 14.533333 \n\nShow the Codeobs_diff_wings &lt;- mosaic::diffmean(data = Beerwings, Hotwings ~ Gender)\nobs_diff_wings \n\ndiffmean \n     5.2 \n\n\n\nShow the Codegf_boxplot(data = Beerwings, Hotwings ~ Gender, title = \"Hotwings Consumption by Gender\")\n\n\n\n\n\n\n\nThe observed difference in mean consumption of Hotwings between Males and Females is 5.2. Could this have occurred by chance? Here is our formulation of the Hypotheses:\n\\[\nNULL\\ Hypothesis\\ H_0 =&gt; No\\ difference\\ between\\ means\\ across\\ groups\\\\\nAlternative\\ Hypothesis\\\nH_a =&gt;Significant\\ difference\\ between\\ the\\ means\\\n\\]\nSo we perform a Permutation Test to check:\n\nShow the Codenull_dist_wings &lt;- do(1000) * diffmean(Hotwings ~ shuffle(Gender), data = Beerwings)\nnull_dist_wings %&gt;% head()\n\n\n  \n\n\nShow the Codegf_histogram(data = null_dist_wings, ~ diffmean) %&gt;% \n  gf_vline(xintercept = obs_diff_wings, colour = \"red\")\n\n\n\n\n\n\nShow the Codeprop1(~ diffmean &gt;= obs_diff_wings, data = null_dist_wings)\n\n  prop_TRUE \n0.004995005 \n\n\nThe \\(\\color{red}{red\\ line}\\) shows the actual measured mean difference in Hot Wings consumption. The probability that our Permutation distribution is able to equal or exceed that number is \\(0.001998002\\) and we have to reject the Null Hypothesis that the means are identical.\nTo test whether eating more hotwings would lead to increased beer consumption, we need a regression model, which we can again test with a permutation test.\n\nShow the Codelm(Beer ~ Hotwings, data = Beerwings)\n\n\nCall:\nlm(formula = Beer ~ Hotwings, data = Beerwings)\n\nCoefficients:\n(Intercept)     Hotwings  \n      3.040        1.941  \n\n\nCase Study-2: Verizon\nThe following example is used throughout this article. Verizon was an Incumbent Local Exchange Carrier (ILEC), responsible for maintaining land-line phone service in certain areas. Verizon also sold long-distance service, as did a number of competitors, termed Competitive Local Exchange Carriers (CLEC). When something went wrong, Verizon was responsible for repairs, and was supposed to make repairs as quickly for CLEC long-distance customers as for their own. The New York Public Utilities Commission (PUC) monitored fairness by comparing repair times for Verizon and different CLECs, for different classes of repairs and time periods. In each case a hypothesis test was performed at the 1% significance level, to determine whether repairs for CLEC’s customers were significantly slower than for Verizon’s customers. There were hundreds of such tests. If substantially more than 1% of the tests were significant, then Verizon would pay large penalties. These tests were performed using t tests; Verizon proposed using permutation tests instead.\n\nShow the Codeverizon &lt;- read.csv(\"../../../../../../materials/data/resampling/Verizon.csv\")\ninspect(verizon)\n\n\ncategorical variables:  \n   name     class levels    n missing\n1 Group character      2 1687       0\n                                   distribution\n1 ILEC (98.6%), CLEC (1.4%)                    \n\nquantitative variables:  \n  name   class min   Q1 median   Q3   max     mean       sd    n missing\n1 Time numeric   0 0.75   3.63 7.35 191.6 8.522009 14.78848 1687       0\n\n\n\nShow the Codemean(Time ~ Group, data = verizon)\n\n     CLEC      ILEC \n16.509130  8.411611 \n\nShow the Codeobs_diff_verizon &lt;- diffmean(Time ~ Group, data = verizon)\nobs_diff_verizon\n\ndiffmean \n-8.09752 \n\n\n\nShow the Codenull_dist_verizon &lt;- do(1000) * diffmean(Time ~ shuffle(Group), data = verizon)\ngf_histogram(data = null_dist_verizon, ~ diffmean) %&gt;% \n  gf_vline(xintercept = obs_diff_wings, colour = \"red\")\n\n\n\n\n\n\nShow the Codeprop1(~ diffmean &gt;= obs_diff_wings, data = null_dist_verizon)\n\n  prop_TRUE \n0.007992008 \n\n\nCase Story-3: Recidivism\nDo criminals released after a jail term commit crimes again?\n\nShow the Coderecidivism &lt;- read.csv(\"../../../../../../materials/data/resampling/Recidivism.csv\")\ninspect(recidivism)\n\n\ncategorical variables:  \n     name     class levels     n missing\n1  Gender character      2 17019       3\n2     Age character      5 17019       3\n3   Age25 character      2 17019       3\n4 Offense character      2 17022       0\n5   Recid character      2 17022       0\n6    Type character      3 17022       0\n                                   distribution\n1 M (87.7%), F (12.3%)                         \n2 25-34 (36.6%), 35-44 (23.7%) ...             \n3 Over 25 (81.9%), Under 25 (18.1%)            \n4 Felony (80.6%), Misdemeanor (19.4%)          \n5 No (68.4%), Yes (31.6%)                      \n6 No Recidivism (68.4%), New (20.2%) ...       \n\nquantitative variables:  \n  name   class min  Q1 median  Q3  max     mean       sd    n missing\n1 Days integer   0 241    418 687 1095 473.3275 283.1393 5386   11636\n\n\nThere are some missing values in the variable  Age25. The  complete.cases command gives the row numbers where values are not missing. We create a new data frame omitting the rows where there is a missing value in the  ‘Age25’  variable.\n\nShow the Coderecidivism_na &lt;- recidivism %&gt;% tidyr::drop_na(Age25)\n\n\nAlso, the variable Recid is a factor variable coded “Yes” or “No”. We convert it to a numeric variable of 1’s and 0’s.\n\nShow the Coderecidivism_na &lt;- recidivism_na %&gt;% mutate(Recid2 = ifelse(Recid==\"Yes\", 1, 0))\n\nobs_diff_recid &lt;- diffmean( Recid2 ~ Age25, data = recidivism_na)\nobs_diff_recid\n\n  diffmean \n0.05919913 \n\nShow the Codenull_dist_recid &lt;- do(1000) * diffmean( Recid2 ~ shuffle(Age25), data = recidivism_na)\n\ngf_histogram( ~ diffmean, data = null_dist_recid) %&gt;% \n  gf_vline(xintercept = obs_diff_recid, colour = \"red\")\n\n\n\n\n\n\n\nCase Study-4: Matched Pairs: Results from a diving championship.\n\nShow the CodeDiving2017 &lt;- read.csv(\"../../../../../../materials/data/resampling/Diving2017.csv\")\nhead(Diving2017)\n\n\n  \n\n\nShow the Codeinspect(Diving2017)\n\n\ncategorical variables:  \n     name     class levels  n missing\n1    Name character     12 12       0\n2 Country character      8 12       0\n                                   distribution\n1  SI Yajie (8.3%) ...                         \n2 Canada (16.7%), China (16.7%) ...            \n\nquantitative variables:  \n       name   class    min       Q1  median      Q3   max    mean       sd  n\n1 Semifinal numeric 313.70 322.2000 325.625 356.575 382.8 338.500 22.94946 12\n2     Final numeric 283.35 318.5875 358.925 387.150 397.5 350.475 40.02204 12\n  missing\n1       0\n2       0\n\n\nThe data is made up of paired observations per swimmer. So we need to take the difference between the two swim records for each swimmer and then shuffle the differences to either polarity. Another way to look at this is to shuffle the records between Semifinal and Final on a per Swimmer basis.\n\nShow the CodeDiving2017\n\n\n  \n\n\nShow the CodeDiving2017 %&gt;% diffmean(data = ., Final ~ Semifinal, only.2 = FALSE)\n\n  318.7-313.7  320.55-318.7 322.75-320.55  325.5-322.75  325.75-325.5 \n       12.350       -63.050         5.225        85.125      -114.150 \n   346-325.75    355.15-346 360.85-355.15  367.5-360.85   382.8-367.5 \n      102.200       -54.150        28.600        31.950         4.050 \n\nShow the Codeobs_diff_swim &lt;- mean(~ Final - Semifinal, data = Diving2017)\nobs_diff_swim\n\n[1] 11.975\n\n\n\nShow the Codepolarity &lt;- c(rep(1, 6), rep(-1,6))\npolarity\n\n [1]  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1\n\nShow the Codenull_dist_swim &lt;- do(100000) * mean(data = Diving2017, \n                                    ~(Final - Semifinal) * resample(polarity,\n                                                    replace = TRUE))\nnull_dist_swim %&gt;% head()\n\n\n  \n\n\nShow the Codegf_histogram(data = null_dist_swim, ~mean) %&gt;% \n  gf_vline(xintercept = obs_diff_swim, colour = \"red\")\n\n\n\n\n\n\n\nCase Study #5: Flight Delays\nLaGuardia Airport (LGA) is one of three major airports that serves the New York City metropolitan area. In 2008, over 23 million passengers and over 375 000 planes flew in or out of LGA. United Airlines and America Airlines are two major airlines that schedule services at LGA. The data set FlightDelays contains information on all 4029 departures of these two airlines from LGA during May and June 2009.\n\nShow the CodeflightDelays &lt;- read.csv(\"../../../../../../materials/data/resampling/FlightDelays.csv\")\n\ninspect(flightDelays)\n\n\ncategorical variables:  \n         name     class levels    n missing\n1     Carrier character      2 4029       0\n2 Destination character      7 4029       0\n3  DepartTime character      5 4029       0\n4         Day character      7 4029       0\n5       Month character      2 4029       0\n6   Delayed30 character      2 4029       0\n                                   distribution\n1 AA (72.1%), UA (27.9%)                       \n2 ORD (44.3%), DFW (22.8%), MIA (15.1%) ...    \n3 8-Noon (26.1%), Noon-4pm (26%) ...           \n4 Fri (15.8%), Mon (15.6%), Tue (15.6%) ...    \n5 June (50.4%), May (49.6%)                    \n6 No (85.2%), Yes (14.8%)                      \n\nquantitative variables:  \n          name   class min   Q1 median   Q3  max      mean         sd    n\n1           ID integer   1 1008   2015 3022 4029 2015.0000 1163.21645 4029\n2     FlightNo integer  71  371    691  787 2255  827.1035  551.30939 4029\n3 FlightLength integer  68  155    163  228  295  185.3011   41.78783 4029\n4        Delay integer -19   -6     -3    5  693   11.7379   41.63050 4029\n  missing\n1       0\n2       0\n3       0\n4       0\n\n\nThe variables in the flightDelays dataset are:\n\nflightDelay dataset variables\n\n\n\n\n\nVariable\nDescription\n\n\n\nCarrier\nUA=United Airlines, AA=American Airlines\n\n\nFlightNo\nFlight number\n\n\nDestination\nAirport code\n\n\nDepartTime\nScheduled departure time in 4 h intervals\n\n\nDay\nDay of the Week\n\n\nMonth\nMay or June\n\n\nDelay\nMinutes flight delayed (negative indicates early departure)\n\n\nDelayed30\nDeparture delayed more than 30 min? Yes or No\n\n\nFlightLength\nLength of time of flight (minutes)\n\n\n\n\nLet us compute the proportion of times that each carrier’s flights was delayed more than 20 min. We will conduct a two-sided test to see if the difference in these proportions is statistically significant.\n\n\nShow the Codeprop(data = flightDelays, Delay &gt;= 20 ~ Carrier)\n\nprop_TRUE.AA prop_TRUE.UA \n   0.1713696    0.2226180 \n\nShow the Codeobs_diff_delay &lt;- diffprop(data = flightDelays, Delay &gt;= 20 ~ Carrier)\nobs_diff_delay\n\n  diffprop \n0.05124841 \n\n\nWe see carrier AA has a 17.13% chance of delays&gt;= 20, while UA has 22.26% chance. The difference is 5.12%. Is this statistically significant? We take the Delays for both Carriers and perform a permutation test by shuffle on the carrier variable:\n\nShow the Codenull_dist_delay &lt;- do(10000) * diffprop(data = flightDelays, Delay &gt;= 20 ~ shuffle(Carrier))\nnull_dist_delay %&gt;% head()\n\n\n  \n\n\nShow the Codegf_histogram(data = null_dist_delay, ~ diffprop) %&gt;% gf_vline(xintercept = obs_diff_delay, color = \"red\")\n\n\n\n\n\n\n\nIt appears that the difference indelay times is significant. We can compute the p-value based on this test:\n\nShow the Code2* mean(null_dist_delay &gt;= obs_diff_delay)\n\n[1] 2e-04\n\n\nwhich is very small. Hence we reject the null Hypothesis that there is no difference between carriers on delay times.\n\nCompute the variance in the flight delay lengths for each carrier. Conduct a test to see if the variance for United Airlines differs from that of American Airlines.\n\n\nShow the Codevar(data = flightDelays, Delay ~ Carrier)\n\n      AA       UA \n1606.457 2037.525 \n\nShow the Code# There is no readymade function in mosaic called `diffvar`...so...we construct one\nobs_diff_var &lt;- diff(var(data = flightDelays, Delay ~ Carrier))\nobs_diff_var\n\n      UA \n431.0677 \n\n\nThe difference in variances in Delay between the two carriers is \\(-431.0677\\). In our Permutation Test, we shuffle the Carrier variable:\n\nShow the Codeobs_diff_var &lt;- diff(var(data = flightDelays, Delay ~ Carrier))\nnull_dist_var &lt;-\n  do(10000) * diff(var(data = flightDelays, Delay ~ shuffle(Carrier)))\nnull_dist_var %&gt;% head()\n\n\n  \n\n\nShow the Code# The null distribution variable is called `UA`\ngf_histogram(data = null_dist_var, ~ UA) %&gt;% gf_vline(xintercept = obs_diff_delay, color = \"red\")\n\n\n\n\n\n\nShow the Code2 * mean(null_dist_var &gt;= obs_diff_var)\n\n[1] 0.301\n\n\nClearly there is no case for a significant difference in variances!\nCase Study #6: Walmart vs Target\nIs there a difference in the price of groceries sold by the two retailers Target and Walmart? The data set Groceries contains a sample of grocery items and their prices advertised on their respective web sites on one specific day.\n\nInspect the data set, then explain why this is an example of matched pairs data.\nCompute summary statistics of the prices for each store.\nConduct a permutation test to determine whether or not there is a difference in the mean prices.\nCreate a histogram bar-chart of the difference in prices. What is unusual about Quaker Oats Life cereal?\nRedo the hypothesis test without this observation. Do you reach the same conclusion?\n\n\nShow the Codegroceries &lt;- read.csv(\"../../../../../../materials/data/resampling/Groceries.csv\") %&gt;% mutate(Product = stringr::str_squish(Product))\nhead(groceries)\n\n\n  \n\n\nShow the Codeinspect(groceries)\n\n\ncategorical variables:  \n     name     class levels  n missing\n1 Product character     30 30       0\n2    Size character     24 30       0\n                                   distribution\n1 Annie's Macaroni & Cheese (3.3%) ...         \n2 18oz (10%), 12oz (6.7%) ...                  \n\nquantitative variables:  \n     name   class  min     Q1 median    Q3  max     mean       sd  n missing\n1  Target numeric 0.99 1.8275  2.545 3.140 7.99 2.762333 1.582128 30       0\n2 Walmart numeric 1.00 1.7600  2.340 2.955 6.98 2.705667 1.560211 30       0\n\n\nWe see that the comparison is to be made between two prices for the same product, and hence this is one more example of paired data, as in Case Study #4. Let us plot the prices for the products:\n\nShow the Codegf_col(data = groceries,\n       Target ~ Product,\n       fill = \"#0073C299\",\n       width = 0.5 ) %&gt;% \n  gf_col(data = groceries,\n         -Walmart ~ Product,\n         fill = \"#EFC00099\",\n         ylab = \"Prices\",\n         width = 0.5\n       ) %&gt;% \n  gf_col(data = groceries %&gt;% filter(Product == \"Quaker Oats Life Cereal Original\"), \n         -Walmart ~ Product,\n         fill = \"red\", \n         width = 0.5) %&gt;% \n  gf_theme(theme_classic()) %&gt;%\n  gf_theme(ggplot2::theme(axis.text.x = element_text(\n    size = 8,\n    face = \"bold\",\n    vjust = 0,\n    hjust = 1\n  ))) %&gt;% gf_theme(ggplot2::coord_flip())\n\n\n\n\n\n\n\nWe see that the price difference between Walmart and Target prices is highest for the Product named Quaker Oats Life Cereal Original. Let us check the mean difference in prices:\n\nShow the Codediffmean(data = groceries, Walmart ~ Target, only.2 = FALSE)\n\n   1-0.99    1.22-1 1.42-1.22 1.49-1.42 1.59-1.49 1.62-1.59 1.79-1.62 1.94-1.79 \n-0.580000  0.170000  0.210000 -0.100000  0.190000  0.070000  0.180000  0.160000 \n1.99-1.94 2.12-1.99 2.39-2.12  2.5-2.39  2.59-2.5 2.64-2.59 2.79-2.64 2.82-2.79 \n 0.090000  0.010000  0.200000  0.600000 -0.200000 -0.600000  0.660000  0.040000 \n2.99-2.82 3.19-2.99 3.49-3.19 3.99-3.49 4.79-3.99 7.19-4.79 7.99-7.19 \n 0.220000  1.263333 -1.183333 -0.480000  2.290000  2.190000  0.000000 \n\nShow the Codeobs_diff_price = mean( ~ Walmart - Target, data = groceries)\nobs_diff_price\n\n[1] -0.05666667\n\n\nLet us perform the pair-wise permutation test on prices, by shuffling the two store names:\n\nShow the Codepolarity &lt;- c(rep(1, 15), rep(-1,15))\npolarity\n\n [1]  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n[26] -1 -1 -1 -1 -1\n\nShow the Codenull_dist_price &lt;- do(100000) * mean(data = groceries, \n                                    ~(Walmart-Target) * resample(polarity,\n                                                    replace = TRUE))\nnull_dist_price %&gt;% head()\n\n\n  \n\n\nShow the Codegf_histogram(data = null_dist_price, ~mean) %&gt;% \n  gf_vline(xintercept = obs_diff_price, colour = \"red\")\n\n\n\n\n\n\nShow the Code2*(sum(null_dist_price &gt;= obs_diff_price + 1)/(100000+1)) #P-value\n\n[1] 0\n\n\nDoes not seem to be aby significant difference in prices…\nSuppose we knock off the Quaker Cereal data item…\n\nShow the Codewhich(groceries$Product == \"Quaker Oats Life Cereal Original\")\n\n[1] 2\n\nShow the Codegroceries_less &lt;- groceries[-2,]\ngroceries_less\n\n\n  \n\n\nShow the Codeobs_diff_price_less = mean( ~ Walmart - Target, data = groceries_less)\nobs_diff_price_less\n\n[1] -0.1558621\n\nShow the Codepolarity_less &lt;- c(rep(1, 15), rep(-1,14)) # Due to resampling this small bias makes no difference\nnull_dist_price_less &lt;- do(100000) * mean(data = groceries_less, \n                                    ~(Walmart-Target) * resample(polarity_less,\n                                                    replace = TRUE))\nnull_dist_price_less %&gt;% head()\n\n\n  \n\n\nShow the Codegf_histogram(data = null_dist_price_less, ~mean) %&gt;% \n  gf_vline(xintercept = obs_diff_price_less, colour = \"red\")\n\n\n\n\n\n\nShow the Code1- mean(null_dist_price_less &gt;= obs_diff_price_less) #P-value\n\n[1] 0.01566\n\n\nCase Study 7: Proportions between Categorical Variables\nLet us try a dataset with Qualitative / Categorical data. This is a General Social Survey dataset, and we have people with different levels of Education stating their opinion on the Death Penalty. We want to know if these two Categorical variables have a correlation, i.e. can the opinions in favour of the Death Penalty be explained by the Education level?\nSince data is Categorical, we need to take counts in a table, and then implement a chi-square test. In the test, we will permute the Education variable to see if we can see how significant its effect size is.\n\nShow the CodeGSS2002 &lt;- read.csv(\"../../../../../../materials/data/resampling/GSS2002.csv\")\ninspect(GSS2002)\n\n\ncategorical variables:  \n            name     class levels    n missing\n1         Region character      7 2765       0\n2         Gender character      2 2765       0\n3           Race character      3 2765       0\n4      Education character      5 2760       5\n5        Marital character      5 2765       0\n6       Religion character     13 2746      19\n7          Happy character      3 1369    1396\n8         Income character     24 1875     890\n9       PolParty character      8 2729      36\n10      Politics character      7 1331    1434\n11     Marijuana character      2  851    1914\n12  DeathPenalty character      2 1308    1457\n13        OwnGun character      3  924    1841\n14        GunLaw character      2  916    1849\n15 SpendMilitary character      3 1324    1441\n16     SpendEduc character      3 1343    1422\n17      SpendEnv character      3 1322    1443\n18      SpendSci character      3 1266    1499\n19        Pres00 character      5 1749    1016\n20      Postlife character      2 1211    1554\n                                    distribution\n1  North Central (24.7%) ...                    \n2  Female (55.6%), Male (44.4%)                 \n3  White (79.1%), Black (14.8%) ...             \n4  HS (53.8%), Bachelors (16.1%) ...            \n5  Married (45.9%), Never Married (25.6%) ...   \n6  Protestant (53.2%), Catholic (24.5%) ...     \n7  Pretty happy (57.3%) ...                     \n8  40000-49999 (9.1%) ...                       \n9  Ind (19.3%), Not Str Dem (18.9%) ...         \n10 Moderate (39.2%), Conservative (15.8%) ...   \n11 Not legal (64%), Legal (36%)                 \n12 Favor (68.7%), Oppose (31.3%)                \n13 No (65.5%), Yes (33.5%) ...                  \n14 Favor (80.5%), Oppose (19.5%)                \n15 About right (46.5%) ...                      \n16 Too little (73.9%) ...                       \n17 Too little (60%) ...                         \n18 About right (49.7%) ...                      \n19 Bush (50.6%), Gore (44.7%) ...               \n20 Yes (80.5%), No (19.5%)                      \n\nquantitative variables:  \n  name   class min  Q1 median   Q3  max mean       sd    n missing\n1   ID integer   1 692   1383 2074 2765 1383 798.3311 2765       0\n\n\nNote how all variables are Categorical !! Education has five levels:\n\nShow the CodeGSS2002 %&gt;% count(Education)\n\n\n  \n\n\nShow the CodeGSS2002 %&gt;% count(DeathPenalty)\n\n\n  \n\n\n\nLet us drop NA entries in Education and Death Penalty. And set up a table for the chi-square test.\n\nShow the Codegss2002 &lt;- GSS2002 %&gt;% \n  dplyr::select(Education, DeathPenalty) %&gt;% \n  tidyr::drop_na(., c(Education, DeathPenalty))\ndim(gss2002)\n\n[1] 1307    2\n\nShow the Codegss_summary &lt;- gss2002 %&gt;%\n  mutate(\n    Education = factor(\n      Education,\n      levels = c(\"Bachelors\", \"Graduate\", \"Jr Col\", \"HS\", \"Left HS\"),\n      labels = c(\"Bachelors\", \"Graduate\", \"Jr Col\", \"HS\", \"Left HS\")\n    ),\n    DeathPenalty = as.factor(DeathPenalty)\n  ) %&gt;%\n  group_by(Education, DeathPenalty) %&gt;%\n  summarise(count = n()) %&gt;% # This is good for a chisq test\n  \n  # Add two more columns to faciltate mosaic/Marrimekko Plot\n  # \n  mutate(edu_count = sum(count), \n         edu_prop = count / sum(count)) %&gt;%\n  ungroup() \n\ngss_summary\n\n\n  \n\n\nShow the Code# We can plot a heatmap-like `mosaic chart` for this table, using `ggplot`:\n# https://stackoverflow.com/questions/19233365/how-to-create-a-marimekko-mosaic-plot-in-ggplot2\n\nggplot(data = gss_summary, aes( x = Education, y = edu_prop)) +\n  geom_bar(aes(width = edu_count, fill = DeathPenalty), stat = \"identity\", position = \"fill\", colour = \"black\") +\n  geom_text(aes(label = scales::percent(edu_prop)), position = position_stack(vjust = 0.5)) +\n\n\n# if labels are desired\n facet_grid(~ Education, scales = \"free_x\", space = \"free_x\") + \n  theme(scale_fill_brewer(palette = \"RdYlGn\")) + \n  # theme(panel.spacing.x = unit(0, \"npc\")) + # if no spacing preferred between bars\n  theme_void() \n\n\n\n\n\n\n\nLet us now perform the base chisq test: We need a table and then the chisq test:\n\nShow the Codegss_table &lt;- tally(DeathPenalty ~ Education, data = gss2002)\ngss_table\n\n            Education\nDeathPenalty Bachelors Graduate  HS Jr Col Left HS\n      Favor        135       64 511     71     117\n      Oppose        71       50 200     16      72\n\nShow the Code# Get the observed chi-square statistic\nobservedChi2 &lt;- mosaic::chisq(tally(DeathPenalty ~ Education, data = gss2002))\nobservedChi2\n\nX.squared \n 23.45093 \n\nShow the Code# Actual chi-square test\nstats::chisq.test(tally(DeathPenalty ~ Education, data = gss2002))\n\n\n    Pearson's Chi-squared test\n\ndata:  tally(DeathPenalty ~ Education, data = gss2002)\nX-squared = 23.451, df = 4, p-value = 0.0001029\n\n\nWe should now repeat the test with permutations on Education:\n\nShow the Codenull_chisq &lt;- do(10000) * chisq.test(tally(DeathPenalty ~ shuffle(Education), data = gss2002))\n\nhead(null_chisq)\n\n\n  \n\n\nShow the Codegf_histogram( ~ X.squared, data = null_chisq) %&gt;% \n  gf_vline(xintercept = observedChi2, color = \"red\")\n\n\n\n\n\n\nShow the Codegf_histogram( ~ p.value, data = null_chisq, binwidth = 0.1, center = 0.05)\n\n\n\n\n\n\n\nSo we would conclude that Education has a significant effect on DeathPenalty opinion!"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/paired-means.html",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/paired-means.html",
    "title": "🃏 Inference for Comparing Two Paired Means",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = FALSE,message = TRUE, warning = TRUE, fig.align = \"center\")\noptions(scipen=5)\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(broom)\nlibrary(MKinfer) # Confidence Interval Computation\nlibrary(resampledata) ### Datasets from Chihara and Hesterberg's book \nlibrary(gt) # for tables",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🃏 Inference for Comparing Two Paired Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/paired-means.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/paired-means.html#setting-up-r-packages",
    "title": "🃏 Inference for Comparing Two Paired Means",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = FALSE,message = TRUE, warning = TRUE, fig.align = \"center\")\noptions(scipen=5)\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(broom)\nlibrary(MKinfer) # Confidence Interval Computation\nlibrary(resampledata) ### Datasets from Chihara and Hesterberg's book \nlibrary(gt) # for tables",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🃏 Inference for Comparing Two Paired Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/paired-means.html#inspecting-and-charting-data",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/paired-means.html#inspecting-and-charting-data",
    "title": "🃏 Inference for Comparing Two Paired Means",
    "section": "\n Inspecting and Charting Data",
    "text": "Inspecting and Charting Data\n\ndata(\"Diving2017\")\nDiving2017\nDiving2017_inspect &lt;- inspect(Diving2017)\nDiving2017_inspect$categorical\nDiving2017_inspect$quantitative\n\n\n  \n\n\n  \n\n\n  \n\n\n\nThe data is made up of paired observations per swimmer, one for the semi-final and one for the final race. There are 12 swimmers and therefore 12 paired records. How can we quickly visualize this data?\nFirst, histograms and densities of the two variables at hand:\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nDiving2017_long &lt;- Diving2017 %&gt;%\n  pivot_longer(\n    cols = c(Final, Semifinal),\n    names_to = \"race\",\n    values_to = \"scores\"\n  )\n\nDiving2017_long %&gt;%\n  gf_density( ~ scores,\n              fill = ~ race,\n              alpha = 0.5,\n              title = \"Diving Scores\") %&gt;%\n  gf_facet_grid( ~ race) %&gt;%\n  gf_fitdistr(dist = \"dnorm\")\n###\nDiving2017_long %&gt;%\n  gf_col(\n    fct_reorder(Name, scores) ~ scores,\n    fill = ~ race,\n    alpha = 0.5,\n    position = \"dodge\",\n    xlab = \"Scores\",\n    ylab = \"Name\",\n    title = \"Diving Scores\"\n  ) \n###\nDiving2017_long %&gt;%\n  gf_boxplot(\n    scores ~ race,\n    fill = ~ race,\n    alpha = 0.5,\n    xlab = \"Race\",\n    ylab = \"Scores\",\n    title = \"Diving Scores\"\n  ) \n\n\n\n\n\n\n\n\n\n\n\n\n\nWe see that:\n\nThe data are not normally distributed. With just such few readings (n &lt; 30) it was just possible…more readings would have helped. We will verify this aspect formally very shortly. \nThere is no immediately identifiable trend in score changes from one race to the other.\n\n\nA.  Check for Normality\nLet us also complete a check for normality: the shapiro.wilk test checks whether a Quant variable is from a normal distribution; the NULL hypothesis is that the data are from a normal distribution.\nshapiro.test(Diving2017$Final)\nshapiro.test(Diving2017$Semifinal)\n\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  Diving2017$Final\nW = 0.9184, p-value = 0.273\n\n\n    Shapiro-Wilk normality test\n\ndata:  Diving2017$Semifinal\nW = 0.86554, p-value = 0.05738\n\n\n\nHmmm….the Shapiro-Wilk test suggests that both scores are normally distributed, though Semifinal is probably marginally so.\nCan we check this with plots? We can plot Q-Q plots for both variables, and also compare both data with normally-distributed data generated with the same means and standard deviations:\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nset.rseed(1234)\nDiving2017 %&gt;% \n  mutate(Final_norm = rnorm(n = 12, \n                          mean = mean(Final), \n                          sd = sd(Final)),\n\n         Semifinal_norm = rnorm(n = 12, \n                                      mean = mean(Semifinal), \n                                      sd = sd(Semifinal))) %&gt;% \n  pivot_longer(cols = \n                 c(Semifinal, Final, Semifinal_norm,Final_norm),\n               names_to = \"score_type\", values_to = \"value\") %&gt;% \n  gf_boxplot(value ~ score_type, fill = ~ score_type, \n             show.legend = FALSE)\n###\nDiving2017_long %&gt;% \n  gf_qq(~ scores | race) %&gt;% \n  gf_qqline(ylab = \"scores\")\n\n\n\n\n\n\n\n\n\n\nWhile the boxplots are not very evocative, we see in the QQ-plots that the Final scores are closer to the straight line than the Semifinal scores. But it is perhaps still hard to accept the data as normally distributed…hmm.\nB.  Check for Variances\nLet us check if the two variables have similar variances: the var.test does this for us, with a NULL hypothesis that the variances are not significantly different:\nvar.test(scores ~ race, data = Diving2017_long,\n         conf.int = TRUE,conf.level = 0.95) %&gt;% \n  broom::tidy()\nqf(0.975,11,11)\n\n\n\n\n  \n\n\n\n[1] 3.473699\n\n\n\nThe variances are not significantly different, as seen by the \\(p.value = 0.08\\).\nSo to summarise our data checks:\n- data are normally distributed\n- variances are not significantly different",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🃏 Inference for Comparing Two Paired Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/paired-means.html#hypothesis",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/paired-means.html#hypothesis",
    "title": "🃏 Inference for Comparing Two Paired Means",
    "section": "\n Hypothesis",
    "text": "Hypothesis\nBased on the graph, how would we formulate our Hypothesis? We wish to infer whether there is any change in performance, per swimmer between the two races. So accordingly:\n\\[\nH_0: \\mu_{semifinal} = \\mu_{final}\\\\\n\\]\n\\[\nH_a: \\mu_{semifinal} \\ne \\mu_{final}\\\n\\]",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🃏 Inference for Comparing Two Paired Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/paired-means.html#observed-and-test-statistic",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/paired-means.html#observed-and-test-statistic",
    "title": "🃏 Inference for Comparing Two Paired Means",
    "section": "\n Observed and Test Statistic",
    "text": "Observed and Test Statistic\nWhat would be the test statistic we would use? The difference in means. Is the observed difference in the means between the two groups of scores non-zero? We use the diffmean function, with the argument only.2=FALSE to allow for paired data:\n\nobs_diff_swim &lt;- diffmean(scores ~ race, data = Diving2017_long, \n                          only.2 = FALSE) \n\n# Can use this also\n# formula method is better for permutation test!\n# obs_diff_swim &lt;- mean(~ (Final - Semifinal), data = Diving2017)\n\nobs_diff_swim\n\ndiffmean \n -11.975",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🃏 Inference for Comparing Two Paired Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/paired-means.html#inference",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/paired-means.html#inference",
    "title": "🃏 Inference for Comparing Two Paired Means",
    "section": "\n Inference",
    "text": "Inference\nType help(t.test) in your Console.\n\nUsing the paired t.test\nUsing non-parametric paired Wilcoxon test\nUsing the Linear Model Method\nUsing Permutation Tests\n\n\n\nSince the data variables satisfy the assumption of being normally distributed, and the variances are not significantly different, we may attempt the classical t.test with paired data. (we will use the mosaic variant).  Our model would be:\n\\[\nmean(Final(i) - Semifinal(i)) = \\beta_0 \\\\\n\\]\nAnd that \\[\nH_0: \\beta_0 = 0;\\\\\n\\\\\nH_a: \\beta_0 \\ne 0\n\\]\n\nmosaic::t.test(x = Diving2017$Semifinal, \n               y = Diving2017$Final,\n               paired = TRUE, var.equal = FALSE) %&gt;% broom::tidy()\n\n\n  \n\n\n\nThe confidence interval spans the zero value, and the p.value is a high \\(0.259\\), so there is no reason to accept alternative hypothesis that the means are different. Hence we say that there is no evidence of a difference between SemiFinal and Final scores.\n\n\nWell, we might consider ( based on knowledge of the sport ) that at least one of the variables does not meet the normality criteria, and though their variances are not significantly different. So we would attempt a non-parametric Wilcoxon test, that uses the signed-rank of the paired data differences, instead of the data variables. Our model would be:\n\\[\nmean(\\ sign.rank[\\ Final(i) - Semifinal(i)\\ ]\\ ) = \\beta_0 \\\\\n\\]\n\\[\nH_0 : \\beta_0 = 0;\\\\\nH_a: \\beta_0 \\ne 0\n\\]\n\nwilcox.test(x = Diving2017$Semifinal, \n            y = Diving2017$Final,\n               paired = TRUE,conf.int = TRUE,conf.level = 0.95) %&gt;% broom::tidy()\n\n\n  \n\n\n\nHere also with the p.value being \\(0.3804\\), we have no reason to accept the Alternative Hypothesis. The parametric t.test and the non-parametric wilcox.test agree in their inferences.\n\n\nWe can apply the linear-model-as-inference interpretation both to the original data and to the sign.rank data: \n\\[\nlm(y_i - x_i \\sim 1) = \\beta_0\\\\\n\\\\ and\\\\\nlm(\\ sign.rank[\\ Final(i) - Semifinal(i)\\ ] \\sim 1) = \\beta_0\\\\\n\\]\nAnd the Hypothesis for both interpretations would be:\\[\nH_0: \\beta_0 = 0\\\\\n\\\\\\\nH_a: \\beta_0 \\ne 0\\\\\n\\]\n\nlm(Semifinal - Final ~ 1, data = Diving2017) %&gt;% \n  broom::tidy(conf.int = TRUE, conf.level = 0.95)\n\n# Create a sign-rank function\nsigned_rank &lt;- function(x) {sign(x) * rank(abs(x))}\n\nlm(signed_rank(Semifinal - Final) ~ 1, \n                   data = Diving2017) %&gt;% \n  broom::tidy(conf.int = TRUE,conf.level = 0.95)\n\n\n  \n\n\n  \n\n\n\nWe observe that using the linear model method for the original scores and the sign-rank scores both sdo not permit us to reject the \\(H_0\\) Null Hypothesis, since p.values are high, and the confidence.intervals straddle \\(0\\).\n\n\nWe saw from the diagram created by Allen Downey that there is only one test! We will now use this philosophy to develop a technique that allows us to mechanize several Statistical Models in that way, with nearly identical code. For the specific data at hand, we need to shuffle the records between Semifinal and Final on a per Swimmer basis and take the test statistic (difference between the two swim records for each swimmer). Another way to look at this is to take the differences between Semifinal and Final scores and shuffle the differences to either polarity. We will follow this method in the code below:\npolarity &lt;- c(rep(1, 6), rep(-1, 6)) \n# 12 +/- 1s, \n# 6 each to make sure there is equal probability\npolarity\nnull_dist_swim &lt;- do(9999) *\n  mean(data = Diving2017,\n       ~ (Final - Semifinal) * mosaic::resample(polarity, replace = TRUE))\n\nnull_dist_swim\n\n\n\n [1]  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1\n\n\n\n  \n\n\n\n\nLet us plot the NULL distribution and compare it with the actual observed differences in the race times:\n# Set graph theme\ntheme_set(new = theme_custom())\n#\ngf_histogram(data = null_dist_swim, ~ mean) %&gt;%\n  gf_vline(xintercept = obs_diff_swim, colour = \"red\")\n###\ngf_ecdf(data = null_dist_swim, ~ mean) %&gt;%\n  gf_vline(xintercept = obs_diff_swim, colour = \"red\")\n###\nprop1(~ mean &lt;= obs_diff_swim, data = null_dist_swim)\n\n\n\n\n\n\n\n\n\n\n\nprop_TRUE \n   0.1254 \n\n\n\nHmm…so by generating 9999 shuffles of score-difference polarities, it does appear that we can not only obtain the current observed difference but even surpass it frequently. So it does seem that there is no difference in means between Semi-Final and Final swimming scores.\n\n\n\nAll Tests Together\nWe can put all the test results together to get a few more insights about the tests:\n\nmosaic::t.test(x = Diving2017$Semifinal, \n               y = Diving2017$Final,\n               paired = TRUE) %&gt;% broom::tidy() %&gt;% \n  gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"cyan\"), cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)) %&gt;% \n  tab_header(title = \"t.test\")\n\nlm(Semifinal - Final ~ 1, data = Diving2017) %&gt;% \n  broom::tidy(conf.int = TRUE, conf.level = 0.95) %&gt;% \n    gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"cyan\"),cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)) %&gt;% \n  tab_header(title = \"Linear Model\")\n\nwilcox.test(x = Diving2017$Semifinal, \n               y = Diving2017$Final,\n               paired = TRUE) %&gt;% broom::tidy() %&gt;% \n    gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"palegreen\"),cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)) %&gt;% \n  tab_header(title = \"Wilcoxon test\")\n\nlm(signed_rank(Semifinal - Final) ~ 1, \n                   data = Diving2017) %&gt;% \n  broom::tidy(conf.int = TRUE,conf.level = 0.95) %&gt;% \n    gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"palegreen\"),cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)) %&gt;% \n  tab_header(title = \"Linear Model with sign.rank\")\n\n\n\n\n\n\n\nt.test\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n-11.975\n-1.190339\n0.2589684\n11\n-34.11726\n10.16726\nPaired t-test\ntwo.sided\n\n\n\n\n\n\n\n\nLinear Model\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n(Intercept)\n-11.975\n10.06016\n-1.190339\n0.2589684\n-34.11726\n10.16726\n\n\n\n\n\n\n\n\nWilcoxon test\n\n\nstatistic\np.value\nmethod\nalternative\n\n\n\n27\n0.3803711\nWilcoxon signed rank exact test\ntwo.sided\n\n\n\n\n\n\n\n\nLinear Model with sign.rank\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n(Intercept)\n-2\n2.135558\n-0.9365236\n0.3691097\n-6.70033\n2.70033",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🃏 Inference for Comparing Two Paired Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/paired-means.html#footnotes",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/paired-means.html#footnotes",
    "title": "🃏 Inference for Comparing Two Paired Means",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://raw.githubusercontent.com/gadenbuie/tidyexplain/main/images/tidyr-pivoting.gif↩︎",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🃏 Inference for Comparing Two Paired Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/paired-means.html#inspecting-and-charting-data-1",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/paired-means.html#inspecting-and-charting-data-1",
    "title": "🃏 Inference for Comparing Two Paired Means",
    "section": "\n Inspecting and Charting Data",
    "text": "Inspecting and Charting Data\n\ndata(\"Groceries\")\nGroceries &lt;- Groceries %&gt;% \n  mutate(Product = stringr::str_squish(Product)) # Knock off extra spaces\nGroceries\nGroceries_inspect &lt;- inspect(Groceries)\nGroceries_inspect$categorical\nGroceries_inspect$quantitative\n\n\n  \n\n\n  \n\n\n  \n\n\n\nThere are just 30 prices for each vendor….just barely enough to get an idea of what the distribution might be. Let us plot histograms/densities of the two variables that we wish to compare. We will also overlay a Gaussian distribution for comparison:\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nGroceries_long &lt;- Groceries %&gt;%\n  pivot_longer(\n    cols = c(Walmart, Target),\n    names_to = \"store\",\n    values_to = \"prices\"\n  ) %&gt;% \n  mutate(store = as_factor(store))\n\nGroceries_long %&gt;%\n  gf_dhistogram( ~ prices,\n              fill = ~ store,\n              alpha = 0.5,\n              title = \"Grocery Costs\") %&gt;%\n  gf_facet_grid( ~ store) %&gt;%\n  gf_fitdistr(dist = \"dnorm\")\nGroceries_long %&gt;%\n  gf_density( ~ prices,\n              fill = ~ store,\n              alpha = 0.5,\n              title = \"Grocery Costs\") %&gt;%\n  gf_facet_grid( ~ store) %&gt;%\n  gf_fitdistr(dist = \"dnorm\")\n\n\n\n\n\n\n\n\n\n\nNot close to the Gaussian…there is clearly some skew to the right, with some items being very costly compared to the rest. More when we check the assumptions on data for the tests.\nHow about price differences, what we are interested in? Let us plot the prices for the products, as box plots after pivoting the data to long form, 1 and as bar charts:\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nGroceries_long %&gt;% \n  gf_boxplot(prices~ store, fill = ~ store) \n###\nGroceries_long %&gt;%\n  gf_col(fct_reorder(Product, prices) ~ prices,\n    fill = ~ store,\n    alpha = 0.5,\n    position = \"dodge\",\n    xlab = \"Prices\",\n    ylab = \"\",\n    title = \"Groceries Costs\"\n  ) %&gt;% \n  gf_col(data = Groceries_long %&gt;% filter(Product == \"Quaker Oats Life Cereal Original\"), fct_reorder(Product, prices) ~ prices, fill = ~store, position = \"dodge\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe see that the price difference between Walmart and Target prices is highest for the Product named Quaker Oats Life Cereal Original. Apart from this Product, the rest have no discernible trend either way. Let us check observed statistic (the mean difference in prices)\n\n\nobs_diff_price &lt;- diffmean(prices ~ store, \n                           data = Groceries_long, \n                           only.2 = FALSE)\n\n# Can also use\n# obs_diff_price &lt;-  mean( ~ Walmart - Target, data = Groceries)\n\nobs_diff_price\n\n\n\n  diffmean \n0.05666667 \n\n\n\n\n Hypothesis\nBased on the graph, how would we formulate our Hypothesis? We wish to infer whether there is any change in prices, per product between the two Store chains. So accordingly:\n\\[\nH_0: \\mu_{Walmart} = \\mu_{Target}\\\\\n\\]\n\\[\nH_a: \\mu_{Walmart} \\ne \\mu_{Target}\\\n\\]\nTesting for Assumptions on the Data\nThere are a few checks we need to make of our data, to decide what test procedure to use.\nA.  Check for Normality\n\nshapiro.test(Groceries$Walmart)\nshapiro.test(Groceries$Target)\n\n\n    Shapiro-Wilk normality test\n\ndata:  Groceries$Walmart\nW = 0.78662, p-value = 3.774e-05\n\n\n    Shapiro-Wilk normality test\n\ndata:  Groceries$Target\nW = 0.79722, p-value = 5.836e-05\n\n\nFor both tests, we see that the p.value is very small, indicating that the data are unlikely to be normally distributed. This means we cannot apply a standard paired t.test and need to use the non-parametric wilcox.test, that does not rely on the assumption of normality.\nB.  Check for Variances\nLet us check if the two variables have similar variances:\n\nvar.test(Groceries$Walmart, Groceries$Target)\n\n\n    F test to compare two variances\n\ndata:  Groceries$Walmart and Groceries$Target\nF = 0.97249, num df = 29, denom df = 29, p-value = 0.9406\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.4628695 2.0431908\nsample estimates:\nratio of variances \n         0.9724868 \n\n\nIt appears from the \\(p.value = 0.9\\) and the \\(Confidence Interval = [0.4629, 2.0432]\\) that we cannot reject the NULL Hypothesis that the variances are not significantly different.\n\n Inference\n\n\nUsing paired t.test\nUsing non-parametric paired Wilcoxon test\nUsing the Linear Model Method\nUsing Permutation Tests\n\n\n\nWell, the variables are not normally distributed, so a standard t.test is not advised, even if the variances are similar. We can still try:\n\nmosaic::t_test(Groceries$Walmart, Groceries$Target,paired = TRUE) %&gt;% \n  broom::tidy()\n\n\n  \n\n\n\nThe p.value is \\(0.64\\) ! And the Confidence Interval straddles \\(0\\). So the t.test gives us no reason to reject the Null Hypothesis that the means are similar. But can we really believe this, given the non-normality of data?\n\n\nHowever, we have seen that the data variables are not normally distributed. So a Wilcoxon Test, using signed-ranks, is indicated: (recall the model!)\n\n# For stability reasons, it may be advisable to use rounded data or to set digits.rank = 7, say, \n# such that determination of ties does not depend on very small numeric differences (see the example).\n\nwilcox.test(Groceries$Walmart, Groceries$Target, data = Groceries_long, \n            digits.rank = 7,\n            paired = TRUE, conf.int = TRUE, conf.level = 0.95) %&gt;% \n  broom::tidy()\n\n\n  \n\n\n\nThe Wilcoxon test result is very interesting: the p.value says there is a significant difference between the two store prices, and the confidence.interval also is unipolar…\n\n\nAs before we can do the linear model for both the original data and the sign.rank data. The test statistic is again the difference between the two variables:\n\nlm(Target- Walmart ~ 1, data = Groceries) %&gt;% \n  broom::tidy(conf.int = TRUE,conf.level = 0.95)\n\n# Create a sign-rank function\nsigned_rank &lt;- function(x) {sign(x) * rank(abs(x))}\n\nlm(signed_rank(Target - Walmart) ~ 1, \n                    data = Groceries) %&gt;% \n  broom::tidy(conf.int = TRUE,conf.level = 0.95)\n\n\n  \n\n\n  \n\n\n\nVery interesting results, but confirming what we saw earlier: The Linear Model with the original data reports no significant difference, but the linear model with sign-ranks, suggests there is a significant difference in means prices between stores!\n\n\nLet us perform the pair-wise permutation test on prices, by shuffling the two store names:\n\n# | layout: [[15, 85, 15]]\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n\npolarity &lt;- c(rep(1, 15), rep(-1,15))\n##\nnull_dist_price &lt;- do(9999) * \n                      mean(data = Groceries, \n                           ~ (Target - Walmart) * resample(polarity,\n                                                      replace = TRUE))\nnull_dist_price\n\n\n  \n\n\n##\ngf_histogram(data = null_dist_price, ~ mean) %&gt;% \n  gf_vline(xintercept = obs_diff_price, colour = \"red\")\n\n\n\n\n\n\nprop1(~ mean, data = null_dist_price)\n\nprop_-0.302 \n      2e-04 \n\n\nDoes not seem to be any significant difference in prices…\n\n\n\nAll Tests Together\nWe can put all the test results together to get a few more insights about the tests:\n\nmosaic::t_test(Groceries$Walmart, Groceries$Target,paired = TRUE) %&gt;% \n  broom::tidy() %&gt;%\n  gt() %&gt;%\n  tab_style(style = list(cell_fill(color = \"cyan\"),\n                         cell_text(weight = \"bold\")),\n            locations = cells_body(columns = p.value)) %&gt;%\n  tab_header(title = \"t.test\")\n###\nlm(Target - Walmart ~ 1, data = Groceries) %&gt;%\n  broom::tidy(conf.int = TRUE, conf.level = 0.95) %&gt;% gt() %&gt;%\n  tab_style(style = list(cell_fill(color = \"cyan\"),\n                         cell_text(weight = \"bold\")),\n            locations = cells_body(columns = p.value))  %&gt;%\n  tab_header(title = \"Linear Model\")\n###\nwilcox.test(Groceries$Walmart, Groceries$Target,\n  digits.rank = 7,\n  paired = TRUE,\n  conf.int = TRUE,\n  conf.level = 0.95\n) %&gt;%\n  broom::tidy() %&gt;% gt() %&gt;%\n  tab_style(style = list(cell_fill(color = \"palegreen\"),\n                         cell_text(weight = \"bold\")),\n            locations = cells_body(columns = p.value))  %&gt;%\n  tab_header(title = \"Wilcoxon Test\")\n###\nlm(signed_rank(Target - Walmart) ~ 1,\n   data = Groceries) %&gt;%\n  broom::tidy(conf.int = TRUE, conf.level = 0.95) %&gt;% gt() %&gt;%\n  tab_style(style = list(cell_fill(color = \"palegreen\"),\n                         cell_text(weight = \"bold\")),\n            locations = cells_body(columns = p.value)) %&gt;%\n  tab_header(title = \"Linear Model with Sign.Ranks\")\n\n\n\n\n\n\n\nt.test\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n-0.05666667\n-0.4704556\n0.6415488\n29\n-0.3030159\n0.1896825\nPaired t-test\ntwo.sided\n\n\n\n\n\n\n\n\nLinear Model\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n(Intercept)\n0.05666667\n0.1204506\n0.4704556\n0.6415488\n-0.1896825\n0.3030159\n\n\n\n\n\n\n\n\nWilcoxon Test\n\n\nestimate\nstatistic\np.value\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n-0.104966\n95\n0.01431746\n-0.1750051\n-0.03005987\nWilcoxon signed rank test with continuity correction\ntwo.sided\n\n\n\n\n\n\n\n\nLinear Model with Sign.Ranks\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n(Intercept)\n8.533333\n2.888834\n2.953902\n0.006167464\n2.625004\n14.44166\n\n\n\n\n\n\n\n\nClearly, the parametric tests do not detect a significant difference in prices, whereas the non-parametric tests do.\nSuppose we knock off the Quaker Cereal data item…(note the spaces in the product name)\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n\nset.seed(12345)\nGroceries_less &lt;- Groceries %&gt;%\n  filter(Product != \"Quaker Oats Life Cereal Original\")\n\nGroceries_less_long &lt;- Groceries_less %&gt;%\n  pivot_longer(\n    cols = c(Target, Walmart),\n    names_to = \"store\",\n    values_to = \"prices\"\n  )\n\nwilcox.test(Groceries_less$Walmart, Groceries_less$Target,paired = TRUE,\n  digits.rank = 7,\n  conf.int = TRUE,\n  conf.level = 0.95\n) %&gt;%\n  broom::tidy()\nobs_diff_price_less &lt;-\n  mean(~ (Target - Walmart), data = Groceries_less)\nobs_diff_price_less\npolarity_less &lt;- c(rep(1, 15), rep(-1, 14))\n# Due to resampling this small bias makes no difference\nnull_dist_price_less &lt;-\n  do(9999) * mean(data = Groceries_less,\n                  ~ (Target - Walmart) * resample(polarity_less,\n                                                  replace = TRUE))\n##\ngf_histogram(data = null_dist_price_less, ~ mean) %&gt;%\n  gf_vline(xintercept = obs_diff_price_less, colour = \"red\")\n##\nmean(null_dist_price_less &gt;= obs_diff_price_less)\n\n\n\n\n  \n\n\n\n[1] 0.1558621\n\n\n\n\n\n\n\n[1] 0.01370137\n\n\n\nWe see that removing the Quaker Oats product item from the data does give a significant difference in mean prices !!! That one price difference was in the opposite direction compared to the general trend in differences, so when it was removed, we obtained a truer picture of price differences.\nTry to do a regular parametric t.test with this reduced data!",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🃏 Inference for Comparing Two Paired Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/paired-means.html#conclusion",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/paired-means.html#conclusion",
    "title": "🃏 Inference for Comparing Two Paired Means",
    "section": "\n Conclusion",
    "text": "Conclusion\nWe have learnt how to perform inference for paired-means. We have looked at the conditions that make the regular t.test possible, and learnt what to do if the conditions of normality and equal variance are not met. We have also looked at how these tests can be understood as manifestations of the linear model, with data and sign-ranked data. It should also be fairly clear now that we can test for the equivalence of two paired means, using a very simple permutation tests. Given computing power, we can always mechanize this test very quickly to get our results. And that performing this test yields reliable results without having to rely on any assumption relating to underlying distributions and so on.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🃏 Inference for Comparing Two Paired Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/paired-means.html#references",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/paired-means.html#references",
    "title": "🃏 Inference for Comparing Two Paired Means",
    "section": "\n References",
    "text": "References\n\nRandall Pruim, Nicholas J. Horton, Daniel T. Kaplan, Start Teaching with R\n\nhttps://bcs.wiley.com/he-bcs/Books?action=index&itemId=111941654X&bcsId=11307\nhttps://statsandr.com/blog/wilcoxon-test-in-r-how-to-compare-2-groups-under-the-non-normality-assumption/\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\ngt\n0.10.1\n@gt\n\n\ninfer\n1.0.7\n@infer\n\n\nMKinfer\n1.2\n@MKinfer\n\n\nopenintro\n2.4.0\n@openintro",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🃏 Inference for Comparing Two Paired Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/inf_for_numerical_data.html",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/inf_for_numerical_data.html",
    "title": "Inference for numerical data",
    "section": "",
    "text": "In this lab, we will explore and visualize the data using the tidyverse suite of packages, and perform statistical inference using infer. The data can be found in the companion package for OpenIntro resources, openintro.\nLet’s load the packages.\n\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(openintro)\nlibrary(infer)\nlibrary(skimr)\n\n\nTo create your new lab report, in RStudio, go to New File -&gt; R Markdown… Then, choose From Template and then choose Lab Report for OpenIntro Statistics Labs from the list of templates.\n\nEvery two years, the Centers for Disease Control and Prevention conduct the Youth Risk Behavior Surveillance System (YRBSS) survey, where it takes data from high schoolers (9th through 12th grade), to analyze health patterns. You will work with a selected group of variables from a random sample of observations during one of the years the YRBSS was conducted.\nLoad the yrbss data set into your workspace.\n\ndata(yrbss)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/inf_for_numerical_data.html#getting-started",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/inf_for_numerical_data.html#getting-started",
    "title": "Inference for numerical data",
    "section": "",
    "text": "In this lab, we will explore and visualize the data using the tidyverse suite of packages, and perform statistical inference using infer. The data can be found in the companion package for OpenIntro resources, openintro.\nLet’s load the packages.\n\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(openintro)\nlibrary(infer)\nlibrary(skimr)\n\n\nTo create your new lab report, in RStudio, go to New File -&gt; R Markdown… Then, choose From Template and then choose Lab Report for OpenIntro Statistics Labs from the list of templates.\n\nEvery two years, the Centers for Disease Control and Prevention conduct the Youth Risk Behavior Surveillance System (YRBSS) survey, where it takes data from high schoolers (9th through 12th grade), to analyze health patterns. You will work with a selected group of variables from a random sample of observations during one of the years the YRBSS was conducted.\nLoad the yrbss data set into your workspace.\n\ndata(yrbss)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/inf_for_numerical_data.html#exploratory-data-analysis",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/inf_for_numerical_data.html#exploratory-data-analysis",
    "title": "Inference for numerical data",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nThere are observations on 13 different variables, some categorical and some numerical. The meaning of each variable can be found by bringing up the help file: type this in your console\n\n\n\n\n\n\nNote\n\n\n\nhelp(yrbss)\n\n\n\n\n\n\n\n\nNote\n\n\n\n1 . What are the cases in this data set? How many cases are there in our sample?\n\n\nYou will first start with analyzing the weight of the participants in kilograms: weight.\nUsing visualization and summary statistics, describe the distribution of weights. The inspect() function from the mosaic package produces nice summaries of the variables in the dataset, separating categorical (character) variables from quantitative variables.\n\nmosaic::inspect(yrbss)\n\n\ncategorical variables:  \n                      name     class levels     n missing\n1                   gender character      2 13571      12\n2                    grade character      5 13504      79\n3                 hispanic character      2 13352     231\n4                     race character      5 10778    2805\n5               helmet_12m character      6 13272     311\n6   text_while_driving_30d character      8 12665     918\n7  hours_tv_per_school_day character      7 13245     338\n8 school_night_hours_sleep character      7 12335    1248\n                                   distribution\n1 male (51.2%), female (48.8%)                 \n2 9 (26.6%), 12 (26.3%), 11 (23.6%) ...        \n3 not (74.4%), hispanic (25.6%)                \n4 White (59.5%) ...                            \n5 never (52.6%), did not ride (34.3%) ...      \n6 0 (37.8%), did not drive (36.7%) ...         \n7 2 (20.4%), &lt;1 (16.4%), 3 (16.1%) ...         \n8 7 (28.1%), 8 (21.8%), 6 (21.5%) ...          \n\nquantitative variables:  \n                  name   class   min    Q1 median    Q3    max      mean\n1                  age integer 12.00 15.00  16.00 17.00  18.00 16.157041\n2               height numeric  1.27  1.60   1.68  1.78   2.11  1.691241\n3               weight numeric 29.94 56.25  64.41 76.20 180.99 67.906503\n4 physically_active_7d integer  0.00  2.00   4.00  7.00   7.00  3.903005\n5 strength_training_7d integer  0.00  0.00   3.00  5.00   7.00  2.949948\n          sd     n missing\n1  1.2637373 13506      77\n2  0.1046973 12579    1004\n3 16.8982128 12579    1004\n4  2.5641046 13310     273\n5  2.5768522 12407    1176\n\n\nNext, consider the possible relationship between a high schooler’s weight and their physical activity. Plotting the data is a useful first step because it helps us quickly visualize trends, identify strong associations, and develop research questions.\nFirst, let’s create a new variable physical_3plus, which will be coded as either “yes” if the student is physically active for at least 3 days a week, and “no” if not. Recall that we have several missing data in that column, so we will (sadly) drop these before generating the new variable:\n\nyrbss &lt;- yrbss %&gt;% \n  drop_na() %&gt;% \n  mutate(physical_3plus = if_else(physically_active_7d &gt;= 2, \"yes\", \"no\"),\n         physical_3plus = factor(physical_3plus, \n                                 labels = c(\"yes\", \"no\"),\n                                 levels = c(\"yes\", \"no\")))\n# Let us check\nyrbss %&gt;% count(physical_3plus)\n\n\n  \n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nMake a side-by-side violin box plots of physical_3plus and weight.\nIs there a relationship between these two variables? What did you expect and why?\n\n\n\n\ngf_boxplot(weight ~ physical_3plus, \n          fill = ~ physical_3plus,\n          data = yrbss,\n          draw_quantiles = TRUE)\n\n\n\n\n\n\n\nThe box plots show how the medians of the two distributions compare, but we can also compare the means of the distributions using the following to first group the data by the physical_3plus variable, and then calculate the mean weight in these groups using the mean function while ignoring missing values by setting the na.rm argument to TRUE.\n\nyrbss %&gt;%\n  group_by(physical_3plus) %&gt;%\n  summarise(mean_weight = mean(weight, na.rm = TRUE))\n\n\n  \n\n\n\nThere is an observed difference, but is this difference large enough to deem it “statistically significant”? In order to answer this question we will conduct a hypothesis test."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/inf_for_numerical_data.html#inference",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/inf_for_numerical_data.html#inference",
    "title": "Inference for numerical data",
    "section": "Inference",
    "text": "Inference\n\n\n\n\n\n\nImportant\n\n\n\nAre all conditions necessary for inference satisfied? Comment on each. You can compute the group sizes with the summarize command above by defining a new variable with the definition n().\n\n\n\n\n\n\n\n\nNote\n\n\n\nWrite the hypotheses for testing if the average weights are different for those who exercise at least times a week and those who don’t.\nWrite here !\n\n\nWe will do this in two ways, just for fun: one using mosaic and the other using infer.\nBut first, we need to initialize the test, which we will save as obs_diff.\n\nobs_diff_infer &lt;- yrbss %&gt;%\n  specify(weight ~ physical_3plus) %&gt;%\n  calculate(stat = \"diff in means\", order = c(\"yes\", \"no\"))\nobs_diff_infer\n\n\n  \n\n\nobs_diff_mosaic &lt;- diffmean(~ weight | physical_3plus, data = yrbss)\nobs_diff_mosaic\n\n diffmean \n-1.694383 \n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that obs_diff_infer is a 1 X 1 dataframe; obs_diff_mosaic is a scalar!!\n\n\n\n\nInference Using `infer`\nInference Using `mosaic`\n\n\n\nNext, we will work through creating a permutation distribution using tools from the infer package.\nRecall that the specify() function is used to specify the variables you are considering (notated y ~x), and you can use the calculate() function to specify the statistic you want to calculate and the order of subtraction you want to use. For this hypothesis, the statistic you are searching for is the difference in means, with the order being yes - no.\nAfter you have calculated your observed statistic, you need to create a permutation distribution. This is the distribution that is created by shuffling the observed weights into new physical_3plus groups, labeled “yes” and “no”.\nWe will save the permutation distribution as null_dist.\n\nnull_dist &lt;- yrbss %&gt;%\n  specify(weight ~ physical_3plus) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  generate(reps = 1000, type = \"permute\") %&gt;%\n  calculate(stat = \"diff in means\", order = c(\"yes\", \"no\"))\n\nThe hypothesize() function is used to declare what the null hypothesis is. Here, we are assuming that student’s weight is independent of whether they exercise at least 3 days or not.\nWe should also note that the type argument within generate() is set to \"permute\". This ensures that the statistics calculated by the calculate() function come from a reshuffling of the data (not a resampling of the data)! Finally, the specify() and calculate() steps should look familiar, since they are the same as what we used to find the observed difference in means!\nWe can visualize this null distribution with the following code:\n\ngf_histogram(data = null_dist, ~ stat)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nAdd a vertical red line to the plot above, demonstrating where the observed difference in means (obs_diff_mosaic) falls on the distribution.\nHow many of these null_dist permutations have a difference at least as large (or larger) as obs_diff_mosaic?\n\n\n\nNow that you have calculated the observed statistic and generated a permutation distribution, you can calculate the p-value for your hypothesis test using the function get_p_value() from the infer package.\n\nnull_dist %&gt;%\n  get_p_value(obs_stat = obs_diff_infer, direction = \"two_sided\")\n\n\n  \n\n\n\n\n\n\n\nWhat warning message do you get? Why do you think you get this warning message?\nConstruct and record a confidence interval for the difference between the weights of those who exercise at least three times a week and those who don’t, and interpret this interval in context of the data.\n\n\n\n\nWe already have the observed difference, obs_diff_mosaic. Now we generate the null distribution using permutation, with mosaic:\n\nnull_dist_mosaic &lt;- do(1000) * diffmean(~ weight | shuffle(physical_3plus), data = yrbss)\n\nWe can also generate the histogram of the null distribution, compare that with the observed diffrence and compute the p-value and confidence intervals:\n\ngf_histogram(~ diffmean, data = null_dist_mosaic) %&gt;% \n  gf_vline(xintercept = obs_diff_mosaic, colour = \"red\")\n\n\n\n\n\n\n# p-value\nprop(~ diffmean &gt;= obs_diff_mosaic, data = null_dist_mosaic)\n\nprop_TRUE \n        1 \n\n# Confidence Intervals for p = 0.95\nmosaic::cdata(~ diffmean, p = 0.95, data = null_dist_mosaic)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/inf_for_numerical_data.html#more-practice",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/inf_for_numerical_data.html#more-practice",
    "title": "Inference for numerical data",
    "section": "More Practice",
    "text": "More Practice\n\nCalculate a 95% confidence interval for the average height in meters (height) and interpret it in context.\nCalculate a new confidence interval for the same parameter at the 90% confidence level. Comment on the width of this interval versus the one obtained in the previous exercise.\nConduct a hypothesis test evaluating whether the average height is different for those who exercise at least three times a week and those who don’t.\nNow, a non-inference task: Determine the number of different options there are in the dataset for the hours_tv_per_school_day there are.\nCome up with a research question evaluating the relationship between height or weight and sleep. Formulate the question in a way that it can be answered using a hypothesis test and/or a confidence interval. Report the statistical results, and also provide an explanation in plain language. Be sure to check all assumptions, state your \\(\\alpha\\) level, and conclude in context."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "",
    "text": "R Tutorial  \n\n\n  Datasets",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🎲 Samples, Populations, Statistics and Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#slides-and-tutorials",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#slides-and-tutorials",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "",
    "text": "R Tutorial  \n\n\n  Datasets",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🎲 Samples, Populations, Statistics and Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#setting-up-r-packages",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nset.seed(123456) # TO get repeatable graphs!\n\nlibrary(tidyverse) # Data Processing in R\nlibrary(mosaic) # Our workhorse for stats, sampling\nlibrary(skimr) # Good to Examine data\nlibrary(ggformula) # Formula interface for graphs\n\n# load the NHANES data library\nlibrary(NHANES)\nlibrary(infer)\nlibrary(cowplot) # ggplot themes and stacking of plots",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🎲 Samples, Populations, Statistics and Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#what-is-a-population",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#what-is-a-population",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "\n What is a Population?",
    "text": "What is a Population?\nA population is a collection of individuals or observations we are interested in. This is also commonly denoted as a study population. We mathematically denote the population’s size using upper-case N.\nA population parameter is some numerical summary about the population that is unknown but you wish you knew. For example, when this quantity is a mean like the mean height of all Bangaloreans, the population parameter of interest is the population mean.\nA census is an exhaustive enumeration or counting of all N individuals in the population. We do this in order to compute the population parameter’s value exactly. Of note is that as the number N of individuals in our population increases, conducting a census gets more expensive (in terms of time, energy, and money).\n\n\n\n\n\n\n Parameters\n\n\n\nPopulations Parameters are usually indicated by Greek Letters.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🎲 Samples, Populations, Statistics and Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#what-is-a-sample",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#what-is-a-sample",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "\n What is a Sample?",
    "text": "What is a Sample?\nSampling is the act of collecting a small subset from the population, which we generally do when we can’t perform a census. We mathematically denote the sample size using lower case n, as opposed to upper case N which denotes the population’s size. Typically the sample size n is much smaller than the population size N. Thus sampling is a much cheaper alternative than performing a census.\nA sample statistic, also known as a point estimate, is a summary statistic like a mean or standard deviation that is computed from a sample.\n\n\n\n\n\n\nWhy do we sample?\n\n\n\nBecause we cannot conduct a census ( not always ) — and sometimes we won’t even know how big the population is — we take samples. And we still want to do useful work for/with the population, after estimating its parameters, an act of generalizing from sample to population. So the question is, can we estimate useful parameters of the population, using just samples? Can point estimates serve as useful guides to population parameters?\nThis act of generalizing from sample to population is at the heart of statistical inference.\n\n\n\n\n\n\n\n\nAn Alliterative Mnemonic\n\n\n\nNOTE: there is an alliterative mnemonic here: Samples have Statistics; Populations have Parameters.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🎲 Samples, Populations, Statistics and Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#population-parameters-and-sample-statistics",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#population-parameters-and-sample-statistics",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "\n Population Parameters and Sample Statistics",
    "text": "Population Parameters and Sample Statistics\n\nParameters and Statistics\n\n\nPopulation Parameter\nSample Statistic\n\n\n\nMean\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\n\nStandard Deviation\n\\(\\sigma\\)\ns\n\n\nProportion\np\n\\(\\hat{p}\\)\n\n\nCorrelation\n\\(\\rho\\)\nr\n\n\nSlope (Regression)\n\\(\\beta_1\\)\n\\(b_1\\)\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.1. What is the mean commute time for workers in a particular city?\nA.1. The parameter is the mean commute time \\(\\mu\\) for a population containing all workers who work in the city. We estimate it using \\(\\bar{x}\\), the mean of the random sample of people who work in the city.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.2. What is the correlation between the size of dinner bills and the size of tips at a restaurant?\nA.2. The parameter is \\(\\rho\\) , the correlation between bill amount and tip size for a population of all dinner bills at that restaurant. We estimate it using r, the correlation from a random sample of dinner bills.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.3. How much difference is there in the proportion of 30 to 39-year-old residents who have only a cell phone (no land line phone) compared to 50 to 59-year-olds in the country?\nA.3. The population is all citizens of the country, and the parameter is \\(p_1 - p_2\\), the difference in proportion of 30 to 39-year-old residents who have only a cell phone (\\(p_1\\)) and the proportion with the same property among all 50 to 59-year olds (\\(p_2\\)). We estimate it using (\\(\\hat{p_1} - \\hat{p_2}\\)), the difference in sample proportions computed from random samples taken from each group.\n\n\nSample statistics vary and in the following we will estimate this uncertainty and decide how reliable they might be as estimates of population parameters.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🎲 Samples, Populations, Statistics and Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#case-study-1-sampling-the-nhanes-dataset",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#case-study-1-sampling-the-nhanes-dataset",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "\n Case Study #1: Sampling the NHANES dataset",
    "text": "Case Study #1: Sampling the NHANES dataset\nWe will first execute some samples from a known dataset. We load up the NHANES dataset and inspect it.\ndata(\"NHANES\")\n#mosaic::inspect(NHANES)\nskimr::skim(NHANES)\n\n\n\n\nData summary\n\n\nName\nNHANES\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n76\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n31\n\n\nnumeric\n45\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nSurveyYr\n0\n1.00\nFALSE\n2\n200: 5000, 201: 5000\n\n\nGender\n0\n1.00\nFALSE\n2\nfem: 5020, mal: 4980\n\n\nAgeDecade\n333\n0.97\nFALSE\n8\n40: 1398, 0-: 1391, 10: 1374, 20: 1356\n\n\nRace1\n0\n1.00\nFALSE\n5\nWhi: 6372, Bla: 1197, Mex: 1015, Oth: 806\n\n\nRace3\n5000\n0.50\nFALSE\n6\nWhi: 3135, Bla: 589, Mex: 480, His: 350\n\n\nEducation\n2779\n0.72\nFALSE\n5\nSom: 2267, Col: 2098, Hig: 1517, 9 -: 888\n\n\nMaritalStatus\n2769\n0.72\nFALSE\n6\nMar: 3945, Nev: 1380, Div: 707, Liv: 560\n\n\nHHIncome\n811\n0.92\nFALSE\n12\nmor: 2220, 750: 1084, 250: 958, 350: 863\n\n\nHomeOwn\n63\n0.99\nFALSE\n3\nOwn: 6425, Ren: 3287, Oth: 225\n\n\nWork\n2229\n0.78\nFALSE\n3\nWor: 4613, Not: 2847, Loo: 311\n\n\nBMICatUnder20yrs\n8726\n0.13\nFALSE\n4\nNor: 805, Obe: 221, Ove: 193, Und: 55\n\n\nBMI_WHO\n397\n0.96\nFALSE\n4\n18.: 2911, 30.: 2751, 25.: 2664, 12.: 1277\n\n\nDiabetes\n142\n0.99\nFALSE\n2\nNo: 9098, Yes: 760\n\n\nHealthGen\n2461\n0.75\nFALSE\n5\nGoo: 2956, Vgo: 2508, Fai: 1010, Exc: 878\n\n\nLittleInterest\n3333\n0.67\nFALSE\n3\nNon: 5103, Sev: 1130, Mos: 434\n\n\nDepressed\n3327\n0.67\nFALSE\n3\nNon: 5246, Sev: 1009, Mos: 418\n\n\nSleepTrouble\n2228\n0.78\nFALSE\n2\nNo: 5799, Yes: 1973\n\n\nPhysActive\n1674\n0.83\nFALSE\n2\nYes: 4649, No: 3677\n\n\nTVHrsDay\n5141\n0.49\nFALSE\n7\n2_h: 1275, 1_h: 884, 3_h: 836, 0_t: 638\n\n\nCompHrsDay\n5137\n0.49\nFALSE\n7\n0_t: 1409, 0_h: 1073, 1_h: 1030, 2_h: 589\n\n\nAlcohol12PlusYr\n3420\n0.66\nFALSE\n2\nYes: 5212, No: 1368\n\n\nSmokeNow\n6789\n0.32\nFALSE\n2\nNo: 1745, Yes: 1466\n\n\nSmoke100\n2765\n0.72\nFALSE\n2\nNo: 4024, Yes: 3211\n\n\nSmoke100n\n2765\n0.72\nFALSE\n2\nNon: 4024, Smo: 3211\n\n\nMarijuana\n5059\n0.49\nFALSE\n2\nYes: 2892, No: 2049\n\n\nRegularMarij\n5059\n0.49\nFALSE\n2\nNo: 3575, Yes: 1366\n\n\nHardDrugs\n4235\n0.58\nFALSE\n2\nNo: 4700, Yes: 1065\n\n\nSexEver\n4233\n0.58\nFALSE\n2\nYes: 5544, No: 223\n\n\nSameSex\n4232\n0.58\nFALSE\n2\nNo: 5353, Yes: 415\n\n\nSexOrientation\n5158\n0.48\nFALSE\n3\nHet: 4638, Bis: 119, Hom: 85\n\n\nPregnantNow\n8304\n0.17\nFALSE\n3\nNo: 1573, Yes: 72, Unk: 51\n\n\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nID\n0\n1.00\n61944.64\n5871.17\n51624.00\n56904.50\n62159.50\n67039.00\n71915.00\n▇▇▇▇▇\n\n\nAge\n0\n1.00\n36.74\n22.40\n0.00\n17.00\n36.00\n54.00\n80.00\n▇▇▇▆▅\n\n\nAgeMonths\n5038\n0.50\n420.12\n259.04\n0.00\n199.00\n418.00\n624.00\n959.00\n▇▇▇▆▃\n\n\nHHIncomeMid\n811\n0.92\n57206.17\n33020.28\n2500.00\n30000.00\n50000.00\n87500.00\n100000.00\n▃▆▃▁▇\n\n\nPoverty\n726\n0.93\n2.80\n1.68\n0.00\n1.24\n2.70\n4.71\n5.00\n▅▅▃▃▇\n\n\nHomeRooms\n69\n0.99\n6.25\n2.28\n1.00\n5.00\n6.00\n8.00\n13.00\n▂▆▇▂▁\n\n\nWeight\n78\n0.99\n70.98\n29.13\n2.80\n56.10\n72.70\n88.90\n230.70\n▂▇▂▁▁\n\n\nLength\n9457\n0.05\n85.02\n13.71\n47.10\n75.70\n87.00\n96.10\n112.20\n▁▃▆▇▃\n\n\nHeadCirc\n9912\n0.01\n41.18\n2.31\n34.20\n39.58\n41.45\n42.92\n45.40\n▁▂▇▇▅\n\n\nHeight\n353\n0.96\n161.88\n20.19\n83.60\n156.80\n166.00\n174.50\n200.40\n▁▁▁▇▂\n\n\nBMI\n366\n0.96\n26.66\n7.38\n12.88\n21.58\n25.98\n30.89\n81.25\n▇▆▁▁▁\n\n\nPulse\n1437\n0.86\n73.56\n12.16\n40.00\n64.00\n72.00\n82.00\n136.00\n▂▇▃▁▁\n\n\nBPSysAve\n1449\n0.86\n118.15\n17.25\n76.00\n106.00\n116.00\n127.00\n226.00\n▃▇▂▁▁\n\n\nBPDiaAve\n1449\n0.86\n67.48\n14.35\n0.00\n61.00\n69.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nBPSys1\n1763\n0.82\n119.09\n17.50\n72.00\n106.00\n116.00\n128.00\n232.00\n▂▇▂▁▁\n\n\nBPDia1\n1763\n0.82\n68.28\n13.78\n0.00\n62.00\n70.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys2\n1647\n0.84\n118.48\n17.49\n76.00\n106.00\n116.00\n128.00\n226.00\n▃▇▂▁▁\n\n\nBPDia2\n1647\n0.84\n67.66\n14.42\n0.00\n60.00\n68.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys3\n1635\n0.84\n117.93\n17.18\n76.00\n106.00\n116.00\n126.00\n226.00\n▃▇▂▁▁\n\n\nBPDia3\n1635\n0.84\n67.30\n14.96\n0.00\n60.00\n68.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nTestosterone\n5874\n0.41\n197.90\n226.50\n0.25\n17.70\n43.82\n362.41\n1795.60\n▇▂▁▁▁\n\n\nDirectChol\n1526\n0.85\n1.36\n0.40\n0.39\n1.09\n1.29\n1.58\n4.03\n▅▇▂▁▁\n\n\nTotChol\n1526\n0.85\n4.88\n1.08\n1.53\n4.11\n4.78\n5.53\n13.65\n▂▇▁▁▁\n\n\nUrineVol1\n987\n0.90\n118.52\n90.34\n0.00\n50.00\n94.00\n164.00\n510.00\n▇▅▂▁▁\n\n\nUrineFlow1\n1603\n0.84\n0.98\n0.95\n0.00\n0.40\n0.70\n1.22\n17.17\n▇▁▁▁▁\n\n\nUrineVol2\n8522\n0.15\n119.68\n90.16\n0.00\n52.00\n95.00\n171.75\n409.00\n▇▆▃▂▁\n\n\nUrineFlow2\n8524\n0.15\n1.15\n1.07\n0.00\n0.48\n0.76\n1.51\n13.69\n▇▁▁▁▁\n\n\nDiabetesAge\n9371\n0.06\n48.42\n15.68\n1.00\n40.00\n50.00\n58.00\n80.00\n▁▂▆▇▂\n\n\nDaysPhysHlthBad\n2468\n0.75\n3.33\n7.40\n0.00\n0.00\n0.00\n3.00\n30.00\n▇▁▁▁▁\n\n\nDaysMentHlthBad\n2466\n0.75\n4.13\n7.83\n0.00\n0.00\n0.00\n4.00\n30.00\n▇▁▁▁▁\n\n\nnPregnancies\n7396\n0.26\n3.03\n1.80\n1.00\n2.00\n3.00\n4.00\n32.00\n▇▁▁▁▁\n\n\nnBabies\n7584\n0.24\n2.46\n1.32\n0.00\n2.00\n2.00\n3.00\n12.00\n▇▅▁▁▁\n\n\nAge1stBaby\n8116\n0.19\n22.65\n4.77\n14.00\n19.00\n22.00\n26.00\n39.00\n▆▇▅▂▁\n\n\nSleepHrsNight\n2245\n0.78\n6.93\n1.35\n2.00\n6.00\n7.00\n8.00\n12.00\n▁▅▇▁▁\n\n\nPhysActiveDays\n5337\n0.47\n3.74\n1.84\n1.00\n2.00\n3.00\n5.00\n7.00\n▇▇▃▅▅\n\n\nTVHrsDayChild\n9347\n0.07\n1.94\n1.43\n0.00\n1.00\n2.00\n3.00\n6.00\n▇▆▂▂▂\n\n\nCompHrsDayChild\n9347\n0.07\n2.20\n2.52\n0.00\n0.00\n1.00\n6.00\n6.00\n▇▁▁▁▃\n\n\nAlcoholDay\n5086\n0.49\n2.91\n3.18\n1.00\n1.00\n2.00\n3.00\n82.00\n▇▁▁▁▁\n\n\nAlcoholYear\n4078\n0.59\n75.10\n103.03\n0.00\n3.00\n24.00\n104.00\n364.00\n▇▁▁▁▁\n\n\nSmokeAge\n6920\n0.31\n17.83\n5.33\n6.00\n15.00\n17.00\n19.00\n72.00\n▇▂▁▁▁\n\n\nAgeFirstMarij\n7109\n0.29\n17.02\n3.90\n1.00\n15.00\n16.00\n19.00\n48.00\n▁▇▂▁▁\n\n\nAgeRegMarij\n8634\n0.14\n17.69\n4.81\n5.00\n15.00\n17.00\n19.00\n52.00\n▂▇▁▁▁\n\n\nSexAge\n4460\n0.55\n17.43\n3.72\n9.00\n15.00\n17.00\n19.00\n50.00\n▇▅▁▁▁\n\n\nSexNumPartnLife\n4275\n0.57\n15.09\n57.85\n0.00\n2.00\n5.00\n12.00\n2000.00\n▇▁▁▁▁\n\n\nSexNumPartYear\n5072\n0.49\n1.34\n2.78\n0.00\n1.00\n1.00\n1.00\n69.00\n▇▁▁▁▁\n\n\n\n\n\n\nLet us create a NHANES (sub)-dataset without duplicated IDs and only adults:\n\nNHANES &lt;-\n  NHANES %&gt;%\n  distinct(ID, .keep_all = TRUE) \n\n#create a dataset of only adults\nNHANES_adult &lt;-  \n  NHANES %&gt;%\n  filter(Age &gt;= 18) %&gt;% drop_na(Height)\n\n\n An “Assumed” Population\n\n\n\n\n\n\nAn “Assumed” Population\n\n\n\nFor now, we will treat this dataset as our Population. So each variable in the dataset is a population for that particular quantity/category, with appropriate population parameters such as means, sd-s, and proportions.\n\n\nLet us calculate the population parameters for the Height data from our “assumed” population:\n\n# NHANES_adult is assumed population\npop_mean_height &lt;- mean(~ Height, data = NHANES_adult)\npop_sd_height &lt;- sd(~ Height, data = NHANES_adult)\n\npop_mean_height\n\n[1] 168.3497\n\npop_sd_height\n\n[1] 10.15705\n\n\n\n Sampling\nNow, we will sample ONCE from the NHANES Height variable. Let us take a sample of sample size 50. We will compare sample statistics with population parameters on the basis of this ONE sample of 50:\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n\nsample_height &lt;- sample(NHANES_adult, size = 50) %&gt;% \n  select(Height)\nsample_height\nsample_mean_height &lt;- mean(~ Height, data = sample_height)\nsample_mean_height\n# Plotting the histogram of this sample\nsample_height %&gt;% \n  gf_histogram(~ Height, bins = 10) %&gt;% \n  \n  gf_vline(xintercept = sample_mean_height, \n           color = \"red\") %&gt;% \n  \n  gf_vline(xintercept = pop_mean_height, \n           colour = \"blue\") %&gt;% \n  \n  gf_label(7 ~ (pop_mean_height + 8), \n          label = \"Population Mean\", \n          color = \"blue\") %&gt;% \n  \n  gf_label(7 ~ (sample_mean_height - 8), \n          label = \"Sample Mean\", color = \"red\") %&gt;% \n  gf_labs(title = \"Distribution and Mean of a Single Sample\")\n\n\n\n\n  \n\n\n\n[1] 165.866\n\n\n\n\n\n\n\n Repeated Samples and Sample Means\nOK, so the sample_mean_height is not too far from the pop_mean_height. Is this always true? Let us check: we will create 500 samples each of size 50. And calculate their mean as the sample statistic, giving us a data frame containing 500 sample means. We will then see if these 500 means lie close to the pop_mean_height:\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n\nsample_height_500 &lt;- do(500) * {\n  sample(NHANES_adult, size = 50) %&gt;%\n    select(Height) %&gt;%\n    summarise(\n      sample_mean_500 = mean(Height),\n      sample_min_500 = min(Height),\n      sample_max_500 = max(Height))\n}\n\nhead(sample_height_500)\ndim(sample_height_500)\nsample_height_500 %&gt;%\n  gf_point(.index ~ sample_mean_500, color = \"red\",\n           title = \"Sample Means are close to the Population Mean\",\n           subtitle = \"Sample Means are Random!\") %&gt;%\n  \n  gf_segment(\n    .index + .index ~ sample_min_500 + sample_max_500,\n    color = \"red\",\n    linewidth = 0.3,\n    alpha = 0.3,\n    ylab = \"Sample Index (1-500)\",\n    xlab = \"Sample Means\"\n  ) %&gt;%\n  \n  gf_vline(xintercept = ~ pop_mean_height, \n           color = \"blue\") %&gt;%\n  \n  gf_label(-15 ~ pop_mean_height, label = \"Population Mean\", \n           color = \"blue\") \n\n\n\n\n  \n\n\n\n[1] 500   5\n\n\n\n\n\n\n\n\n\n\n\n\nSample Means are a Random Variable\n\n\n\nThe sample-means are a random variable! And hence they will have a mean and sd. Do not get confused ;-D\n\n\nThe sample_means (red dots), are themselves random because the samples are random, of course. It appears that they are generally in the vicinity of the pop_mean (blue line).\n\n Distribution of Sample-Means\nSince the sample-means are themselves random variables, let’s plot the distribution of these 500 sample-means themselves, called a distribution of sample-means. We will also plot the position of the population mean pop_mean_height parameter, the mean of the Height variable.\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n\nsample_height_500 %&gt;% \n  gf_dhistogram(~ sample_mean_500,bins = 30, xlab = \"Height\") %&gt;% \n  \n  gf_vline(xintercept = pop_mean_height, \n           color = \"blue\") %&gt;% \n  \n  gf_label(0.01 ~ pop_mean_height, \n            label = \"Population Mean\", \n            color = \"blue\") %&gt;% \ngf_labs(title = \"Sampling Mean Distribution\")\n\n\n# How does this **distribution of sample-means** compare with the\n# overall distribution of the population?\n# \nsample_height_500 %&gt;% \n  gf_dhistogram(~ sample_mean_500, bins = 30,xlab = \"Height\") %&gt;% \n  \n  gf_vline(xintercept = pop_mean_height, \n           color = \"blue\") %&gt;% \n  \n   gf_label(0.01 ~ pop_mean_height, \n            label = \"Population Mean\", \n            color = \"blue\") %&gt;% \n\n  ## Add the population histogram\n  gf_histogram(~ Height, data = NHANES_adult, \n               alpha = 0.2, fill = \"blue\", \n               bins = 30) %&gt;% \n  \n  gf_label(0.025 ~ (pop_mean_height + 20), \n           label = \"Population Distribution\", color = \"blue\") %&gt;% \ngf_labs(title = \"Sampling Mean Distribution\", subtitle = \"Original Population overlay\")\n\n\n\n\n\nSample\n\n\n\n\n\nSample and Population\n\n\n\n\n\nDistributions\n\n\n\n\n Central Limit Theorem\nWe see in the Figure above that\n\nthe distribution of sample-means is centered around the pop_mean.\nThat the standard deviation of the distribution of sample means is less than that of the original population. But exactly what is it?\nAnd what is the kind of distribution?\n\nOne more experiment.\nNow let’s repeatedly sample Height and compute the sample mean, and look at the resulting histograms and Q-Q plots. (Q-Q plots check whether a certain distribution is close to being normal or not.)\nWe will use sample sizes of c(8, 16, ,32, 64) and generate 1000 samples each time, take the means and plot these 1000 means:\n\nset.seed(12345)\n\n\nsamples_height_08 &lt;- do(1000) * mean(resample(NHANES_adult$Height, size = 08))\n\nsamples_height_16 &lt;- do(1000) * mean(resample(NHANES_adult$Height, size = 16))\n\nsamples_height_32 &lt;- do(1000) * mean(resample(NHANES_adult$Height, size = 32))\n\nsamples_height_64 &lt;- do(1000) * mean(resample(NHANES_adult$Height, size = 64))\n\n# samples_height_128 &lt;- do(1000) * mean(resample(NHANES_adult$Height, size = 128))\n\n# Quick Check\nhead(samples_height_08)\n\n\n  \n\n\n\nNow let’s create separate Q-Q plots for the different sample sizes.\n\n# Now let's create separate Q-Q plots for the different sample sizes.\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n\np1 &lt;- gf_qq( ~ mean,data = samples_height_08,\n             title = \"N = 8\", \n             color = \"dodgerblue\") %&gt;%\n  gf_qqline()\n\np2 &lt;- gf_qq( ~ mean,data = samples_height_16,\n            title = \"N = 16\", \n            color = \"sienna\") %&gt;%\n  gf_qqline()\n\np3 &lt;- gf_qq( ~ mean,data = samples_height_32,\n            title = \"N = 32\", \n            color = \"palegreen\") %&gt;%\n  gf_qqline()\n\np4 &lt;- gf_qq( ~ mean,data = samples_height_64,\n            title = \"N = 64\", \n            color = \"violetred\") %&gt;%\n  gf_qqline()\n\ncowplot::plot_grid(p1, p2, p3, p4)\n\n\n\n\n\n\n\nLet us plot their individual histograms to compare them:\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n\n# Let us overlay their individual histograms to compare them:\np5 &lt;- gf_dhistogram(~ mean,\n              data = samples_height_08,\n              color = \"grey\",\n              fill = \"dodgerblue\",title = \"N = 8\") %&gt;%\n  gf_fitdistr(linewidth = 1) %&gt;%\n  gf_vline(xintercept = pop_mean_height, inherit = FALSE,\n           color = \"blue\") %&gt;%\n  gf_label(-0.025 ~ pop_mean_height, \n           label = \"Population Mean\", \n           color = \"blue\") %&gt;% \n  gf_theme(scale_y_continuous(expand = expansion(mult = c(0.08,0.02))))\n##\np6 &lt;- gf_dhistogram(~ mean,\n              data = samples_height_16,\n              color = \"grey\",\n              fill = \"sienna\",title = \"N = 16\") %&gt;%\n  gf_fitdistr(linewidth = 1) %&gt;%\n  gf_vline(xintercept = pop_mean_height,\n           color = \"blue\") %&gt;%\n  gf_label(-.025 ~ pop_mean_height, \n           label = \"Population Mean\", \n           color = \"blue\") %&gt;% \n  gf_theme(scale_y_continuous(expand = expansion(mult = c(0.08,0.02))))\n##\np7 &lt;- gf_dhistogram(~ mean,\n                    data = samples_height_32 ,\n                    na.rm = TRUE,\n                    color = \"grey\",\n                    fill = \"palegreen\",title = \"N = 32\") %&gt;%\n  gf_fitdistr(linewidth = 1) %&gt;%\n  gf_vline(xintercept = pop_mean_height,\n           color = \"blue\") %&gt;%\n  gf_label(-.025 ~ pop_mean_height, \n           label = \"Population Mean\", color = \"blue\") %&gt;% \n  gf_theme(scale_y_continuous(expand = expansion(mult = c(0.08,0.02))))\n\np8 &lt;- gf_dhistogram(~ mean, \n                    data = samples_height_64,\n                    na.rm = TRUE,\n                    color = \"grey\",\n                    fill = \"violetred\",title = \"N = 64\") %&gt;% \n  gf_fitdistr(linewidth = 1) %&gt;% \n  gf_vline(xintercept = pop_mean_height,\n         color = \"blue\") %&gt;%\n  gf_label(-.025 ~ pop_mean_height, \n           label = \"Population Mean\", color = \"blue\") %&gt;% \n  gf_theme(scale_y_continuous(expand = expansion(mult = c(0.08,0.02))))\n\n#patchwork::wrap_plots(p5,p6,p7,p8)\np5\np6\np7\np8\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd if we overlay the histograms:\n\n\n\n\n\n\n\n\nThe QQ plots show that the results become more normally distributed (i.e. following the straight line) as the samples get larger. From the histograms we learn that the sample-means are normally distributed around the population mean. This feels intuitively right because when we sample from the population, many values will be close to the population mean, and values far away from the mean will be increasingly scarce.\nLet us calculate the mean of the sample-means:\n\nmean(~ mean, data  = samples_height_08)\nmean(~ mean, data  = samples_height_16)\nmean(~ mean, data  = samples_height_32)\nmean(~ mean, data  = samples_height_64)\npop_mean_height\n\n[1] 168.0245\n[1] 168.4516\n[1] 168.3748\n[1] 168.3037\n[1] 168.3497\n\n\nAnd the sample sds:\n\nsd(~ mean, data  = samples_height_08)\nsd(~ mean, data  = samples_height_16)\nsd(~ mean, data  = samples_height_32)\nsd(~ mean, data  = samples_height_64)\n\n[1] 3.65638\n[1] 2.454437\n[1] 1.807573\n[1] 1.251853\n\n\n\n\n\n\n\n\nCentral Limit Theorem\n\n\n\nThis is the Central Limit Theorem (CLT)\n\nthe sample-means are normally distributed around the population mean.\nthe sample-means become “more normally distributed” with sample length, as shown by the (small but definite) improvements in the Q-Q plots with sample-size.\nthe sample-mean distributions narrow with sample length, i.e the sd decreases with increasing sample size.\n\nThis is regardless of the distribution of the population parameter itself.1",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🎲 Samples, Populations, Statistics and Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#standard-error",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#standard-error",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "\n Standard Error",
    "text": "Standard Error\nAs we saw above, the standard deviations of the sample-mean distributions reduce with sample size. In fact their SDs are defined by:\nsd = pop_sd/sqrt(sample_size)2 where sample-size here is one of c(8, 16,32,64)\nThe standard deviation of the sample-mean distribution is called the Standard Error. This statistic derived from the sample, will help us infer our population parameters with a precise estimate of the uncertainty involved.\n\\[\nStandard\\ Error\\ \\pmb {se} = \\frac{population\\ sd}{\\sqrt[]{sample\\ size}} \\\\\\\n\\pmb {se} = \\frac{\\sigma}{\\sqrt[]{n}}\n\\]\nIn our sampling experiments, the Standard Errors evaluate to:\npop_sd_height &lt;- sd(~ Height, data = NHANES_adult)\n\npop_sd_height/sqrt(8)\npop_sd_height/sqrt(16)\npop_sd_height/sqrt(32)\npop_sd_height/sqrt(64)\n\n\n\n[1] 3.591058\n[1] 2.539262\n[1] 1.795529\n[1] 1.269631\n\n\n\nAs seen, these are identical to the Standard Deviations of the individual sample-mean distributions.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🎲 Samples, Populations, Statistics and Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#confidence-intervals",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#confidence-intervals",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "\n Confidence intervals",
    "text": "Confidence intervals\nWhen we work with samples, we want to be able to speak with a certain degree of confidence about the population mean, based on the evaluation of one sample mean, not a large number of them. Given that sample-means are normally distributed around the population means, we can say that \\(68\\%\\) of all possible sample-mean lie within \\(\\pm SE\\) of the population mean; and further that \\(95 \\%\\) of all possible sample-mean lie within \\(\\pm 2*SE\\) of the population mean.\nThese two intervals \\(sample.mean \\pm SE\\) and \\(sample.mean \\pm 1.5*SE\\) are called the confidence intervals for the population mean, at levels \\(68\\%\\) and \\(95 \\%\\) probability respectively.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🎲 Samples, Populations, Statistics and Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#workflow",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#workflow",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "\n Workflow",
    "text": "Workflow\nThus if we want to estimate a population parameter:\n\nwe take one random sample from the population\nwe calculate the estimate from the sample\nwe calculate the sample-sd\nwe calculate the Standard Error as \\(\\frac{sample-sd}{\\sqrt[]{n}}\\)\n\nwe calculate 95% confidence intervals for the population parameter based on the formula\n\\(CI_{95\\%}= sample.mean \\pm 2*SE\\).\n\nSince Standard Error decreases with sample size, we need to make our sample of adequate size.( \\(n=30\\) seems appropriate in most cases. Why?)\nAnd we do not have to worry about the distribution of the population. It need not be normal !!",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🎲 Samples, Populations, Statistics and Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#an-interactive-sampling-app",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#an-interactive-sampling-app",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "\n An interactive Sampling app",
    "text": "An interactive Sampling app\nHere below is an interactive sampling app. Choose the number of samples you want from a normally-distributed population \\(N(\\mu = 0, \\sigma = 1)\\). Vary the sample size to see how the sample mean varies around the tru population mean.\n#| standalone: true\n#| viewerHeight: 600\n#| viewerWidth: 1000\nlibrary(shiny)\nlibrary(bslib)\n\n# Define UI for app that draws a histogram ----\nui &lt;- page_sidebar(\n  sidebar = sidebar(open = \"open\",\n    numericInput(\"n\", \"Sample count\", 100),\n    checkboxInput(\"pause\", \"Pause\", FALSE),\n  ),\n  plotOutput(\"plot\", width=800)\n)\n\nserver &lt;- function(input, output, session) {\n  data &lt;- reactive({\n    input$resample\n    if (!isTRUE(input$pause)) {\n      invalidateLater(1000)\n    }\n    rnorm(input$n)\n  })\n  \n  output$plot &lt;- renderPlot({\n    hist(data(),\n      breaks = 40,\n      xlim = c(-3, 3),\n      ylim = c(0, 1.5),\n      lty = \"blank\",\n      xlab = \"value\",\n      freq = FALSE,\n      main = \"\"\n    )\n    \n    x &lt;- seq(from = -3, to = 3, length.out = 500)\n    y &lt;- dnorm(x)\n    lines(x, y, lwd=1.5)\n    \n    lwd &lt;- 5\n    abline(v=0, col=\"red\", lwd=lwd, lty=2)\n    abline(v=mean(data()), col=\"blue\", lwd=lwd, lty=1)\n\n    legend(legend = c(\"Normal\", \"Mean\", \"Sample mean\"),\n      col = c(\"black\", \"red\", \"blue\"),\n      lty = c(1, 2, 1),\n      lwd = c(1, lwd, lwd),\n      x = 0.25,\n      y = 1.25\n    )\n  }, res=140)\n}\n\n# Create Shiny app ----\nshinyApp(ui = ui, server = server)\n\nWhat if we sample from not a normal distribution but say, a Poisson Distribution? Will we still see the sample mean hover around the population mean?",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🎲 Samples, Populations, Statistics and Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#an-interactive-app-for-the-clt",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#an-interactive-app-for-the-clt",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "An Interactive app for the CLT",
    "text": "An Interactive app for the CLT\nhttps://gallery.shinyapps.io/CLT_mean/",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🎲 Samples, Populations, Statistics and Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#references",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#references",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "\n References",
    "text": "References\n\nDiez, David M & Barr, Christopher D & Çetinkaya-Rundel, Mine, OpenIntro Statistics. https://www.openintro.org/book/os/\nStats Test Wizard. https://www.socscistatistics.com/tests/what_stats_test_wizard.aspx\nDiez, David M & Barr, Christopher D & Çetinkaya-Rundel, Mine: OpenIntro Statistics. Available online https://www.openintro.org/book/os/\nMåns Thulin, Modern Statistics with R: From wrangling and exploring data to inference and predictive modelling http://www.modernstatisticswithr.com/\nJonas Kristoffer Lindeløv, Common statistical tests are linear models (or: how to teach stats) https://lindeloev.github.io/tests-as-linear/\nCheatSheet https://lindeloev.github.io/tests-as-linear/linear_tests_cheat_sheet.pdf\nCommon statistical tests are linear models: a work through by Steve Doogue https://steverxd.github.io/Stat_tests/\nJeffrey Walker “Elements of Statistical Modeling for Experimental Biology”. https://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/\nAdam Loy, Lendie Follett & Heike Hofmann (2016) Variations of Q–Q Plots: The Power of Our Eyes!, The American Statistician, 70:2, 202-214, DOI: 10.1080/00031305.2015.1077728\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nNHANES\n2.1.0\nPruim (2015)\n\n\nregressinator\n0.1.3\nReinhart (2024)\n\n\nsmovie\n1.1.6\nNorthrop (2023)\n\n\nTeachHist\n0.2.1\nLange (2023)\n\n\nTeachingDemos\n2.13\nSnow (2024)\n\n\nvisualize\n4.5.0\nBalamuta (2023)\n\n\n\n\n\n\nBalamuta, James. 2023. visualize: Graph Probability Distributions with User Supplied Parameters and Statistics. https://CRAN.R-project.org/package=visualize.\n\n\nLange, Carsten. 2023. TeachHist: A Collection of Amended Histograms Designed for Teaching Statistics. https://CRAN.R-project.org/package=TeachHist.\n\n\nNorthrop, Paul J. 2023. smovie: Some Movies to Illustrate Concepts in Statistics. https://CRAN.R-project.org/package=smovie.\n\n\nPruim, Randall. 2015. NHANES: Data from the US National Health and Nutrition Examination Study. https://CRAN.R-project.org/package=NHANES.\n\n\nReinhart, Alex. 2024. regressinator: Simulate and Diagnose (Generalized) Linear Models. https://CRAN.R-project.org/package=regressinator.\n\n\nSnow, Greg. 2024. TeachingDemos: Demonstrations for Teaching and Learning. https://CRAN.R-project.org/package=TeachingDemos.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🎲 Samples, Populations, Statistics and Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#footnotes",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/sampling.html#footnotes",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "Footnotes",
    "text": "Footnotes\n\nThe `Height` variable seems to be normally distributed at population level. We will try other non-normal population variables as an exercise in the tutorials.↩︎\nOnce sample size = population, we have complete access to the population and there is no question of estimation error! So sample_sd = pop_sd!↩︎",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "🎲 Samples, Populations, Statistics and Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html",
    "title": "Inference for Two Independent Means",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic) # Our go-to package\nlibrary(ggformula)\nlibrary(infer) # An alternative package for inference using tidy data\nlibrary(broom) # Clean test results in tibble form\nlibrary(skimr) # data inspection\nlibrary(explore) # New, Easy package for Stats Test and Viz, and other things\nlibrary(resampledata3) # Datasets from Chihara and Hesterberg's book\nlibrary(openintro) # datasets\nlibrary(gt) # for tables\n\n\n\nflowchart TD\n    A[Inference for Two Independent Means] --&gt;|Check Assumptions| B[Normality: Shapiro-Wilk Test shapiro.test\\n Variances: Fisher F-test var.test]\n    B --&gt; C{OK?}\n    C --&gt;|Yes, both\\n Parametric| D[t.test]\n    D &lt;--&gt;F[Linear Model\\n Method] \n    C --&gt;|Yes, but not variance\\n Parametric| W[t.test with\\n Welch Correction]\n    W&lt;--&gt;F\n    C --&gt;|No\\n Non-Parametric| E[wilcox.test]\n    E &lt;--&gt; G[Linear Model\\n with\\n Signed-Ranks]\n    C --&gt;|No\\n Non-Parametric| P[Bootstrap\\n or\\n Permutation]\n    P &lt;--&gt; Q[Linear Model\\n with Signed-Rank\\n with Permutation]\n\n\n\n\nflowchart TD\n    A[Inference for Two Independent Means] --&gt;|Check Assumptions| B[Normality: Shapiro-Wilk Test shapiro.test\\n Variances: Fisher F-test var.test]\n    B --&gt; C{OK?}\n    C --&gt;|Yes, both\\n Parametric| D[t.test]\n    D &lt;--&gt;F[Linear Model\\n Method] \n    C --&gt;|Yes, but not variance\\n Parametric| W[t.test with\\n Welch Correction]\n    W&lt;--&gt;F\n    C --&gt;|No\\n Non-Parametric| E[wilcox.test]\n    E &lt;--&gt; G[Linear Model\\n with\\n Signed-Ranks]\n    C --&gt;|No\\n Non-Parametric| P[Bootstrap\\n or\\n Permutation]\n    P &lt;--&gt; Q[Linear Model\\n with Signed-Rank\\n with Permutation]",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "Inference for Two Independent Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#introduction",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#introduction",
    "title": "Inference for Two Independent Means",
    "section": "",
    "text": "flowchart TD\n    A[Inference for Two Independent Means] --&gt;|Check Assumptions| B[Normality: Shapiro-Wilk Test shapiro.test\\n Variances: Fisher F-test var.test]\n    B --&gt; C{OK?}\n    C --&gt;|Yes, both\\n Parametric| D[t.test]\n    D &lt;--&gt;F[Linear Model\\n Method] \n    C --&gt;|Yes, but not variance\\n Parametric| W[t.test with\\n Welch Correction]\n    W&lt;--&gt;F\n    C --&gt;|No\\n Non-Parametric| E[wilcox.test]\n    E &lt;--&gt; G[Linear Model\\n with\\n Signed-Ranks]\n    C --&gt;|No\\n Non-Parametric| P[Bootstrap\\n or\\n Permutation]\n    P &lt;--&gt; Q[Linear Model\\n with Signed-Rank\\n with Permutation]\n\n\n\n\nflowchart TD\n    A[Inference for Two Independent Means] --&gt;|Check Assumptions| B[Normality: Shapiro-Wilk Test shapiro.test\\n Variances: Fisher F-test var.test]\n    B --&gt; C{OK?}\n    C --&gt;|Yes, both\\n Parametric| D[t.test]\n    D &lt;--&gt;F[Linear Model\\n Method] \n    C --&gt;|Yes, but not variance\\n Parametric| W[t.test with\\n Welch Correction]\n    W&lt;--&gt;F\n    C --&gt;|No\\n Non-Parametric| E[wilcox.test]\n    E &lt;--&gt; G[Linear Model\\n with\\n Signed-Ranks]\n    C --&gt;|No\\n Non-Parametric| P[Bootstrap\\n or\\n Permutation]\n    P &lt;--&gt; Q[Linear Model\\n with Signed-Rank\\n with Permutation]",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "Inference for Two Independent Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#inspecting-and-charting-data",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#inspecting-and-charting-data",
    "title": "Inference for Two Independent Means",
    "section": "\n Inspecting and Charting Data",
    "text": "Inspecting and Charting Data\n\ndata(\"MathAnxiety\")\nMathAnxiety\nMathAnxiety_inspect &lt;- inspect(MathAnxiety)\nMathAnxiety_inspect$categorical\nMathAnxiety_inspect$quantitative\n\n\n  \n\n\n  \n\n\n  \n\n\n\nWe have ~600 data entries, and with 4 Quant variables; Age,AMAS, RCMAS, and AMAS; and two Qual variables, Gender and Grade. A simple dataset, with enough entries to make it worthwhile to explore as our first example.\n\n\n\n\n\n\nResearch Question\n\n\n\nIs there a difference between boy and girl “anxiety” levels for AMAS (test) in the MathAnxiety dataset?\n\n\nFirst, histograms, densities and counts of the variable we are interested in, after converting data into long format:\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nMathAnxiety %&gt;% \n  gf_density(~ AMAS,\n              fill = ~ Gender,\n              alpha = 0.5,\n              title = \"\") \nMathAnxiety %&gt;% \n  gf_boxplot(AMAS ~ Gender,\n              fill = ~ Gender,\n              alpha = 0.5,\n              title = \"\") \nMathAnxiety %&gt;% count(Gender)\nMathAnxiety %&gt;% group_by(Gender) %&gt;% summarise(mean = mean(AMAS))\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nThe distributions for anxiety scores for boys and girls overlap considerably and are very similar, though the boxplot for boys shows a significant outlier. Are they close to being normal distributions too? We should check.\nA.  Check for Normality\nStatistical tests for means usually require a couple of checks1 2:\n\nAre the data normally distributed?\n\nAre the data variances similar?\n\nLet us complete a check for normality: the shapiro.wilk test checks whether a Quant variable is from a normal distribution; the NULL hypothesis is that the data are from a normal distribution. We will also look at Q-Q plots for both variables:\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n\nMathAnxiety %&gt;%\n  gf_density( ~ AMAS,\n              fill = ~ Gender,\n              alpha = 0.5,\n              title = \"Math Anxiety scores for boys and girls\") %&gt;%\n  gf_facet_grid(~ Gender) %&gt;% \n  gf_fitdistr(dist = \"dnorm\") \nMathAnxiety%&gt;% \n  gf_qq(~ AMAS, color = ~ Gender) %&gt;% \n  gf_qqline(ylab = \"Math Anxiety Score..are they Normally Disributed?\") %&gt;%\n  gf_facet_wrap(~ Gender)  # independent y-axis\nboys_AMAS &lt;- MathAnxiety %&gt;% filter(Gender== \"Boy\") %&gt;% select(AMAS)\ngirls_AMAS &lt;- MathAnxiety %&gt;% filter(Gender== \"Girl\") %&gt;% select(AMAS)\n\nshapiro.test(boys_AMAS$AMAS)\nshapiro.test(girls_AMAS$AMAS)\n\n\n\n\n\n\n\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  boys_AMAS$AMAS\nW = 0.99043, p-value = 0.03343\n\n\n    Shapiro-Wilk normality test\n\ndata:  girls_AMAS$AMAS\nW = 0.99074, p-value = 0.07835\n\n\n\nThe distributions for anxiety scores for boys and girls are almost normal, visually speaking. With the Shapiro-Wilk test we find that the scores for girls are normally distributed, but the boys scores are not so.\n\n\n\n\n\n\nNote\n\n\n\nThe p.value obtained in the shapiro.wilk test suggests the chances of the data, given the Assumption that they are normally distributed.\n\n\nWe see that MathAnxiety contains discrete-level scores for anxiety for the two variables (for Boys and Girls) anxiety scores. The boys score has a significant outlier, which we saw earlier and perhaps that makes that variable lose out, perhaps narrowly.\nB.  Check for Variances\nLet us check if the two variables have similar variances: the var.test does this for us, with a NULL hypothesis that the variances are not significantly different:\nvar.test(AMAS ~ Gender, data = MathAnxiety, \n         conf.int = TRUE,conf.level = 0.95) %&gt;% \n  broom::tidy()\nqf(0.975,275,322)\n\n\n\n\n  \n\n\n\n[1] 1.254823\n\n\n\nThe variances are quite similar as seen by the \\(p.value = 0.82\\). We also saw it visually when we plotted the overlapped distributions earlier.\n\n\n\n\n\n\nConditions:\n\n\n\n\nThe two variables are not both normally distributed.\nThe two variances are significantly similar.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "Inference for Two Independent Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#hypothesis",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#hypothesis",
    "title": "Inference for Two Independent Means",
    "section": "\n Hypothesis",
    "text": "Hypothesis\nBased on the graphs, how would we formulate our Hypothesis? We wish to infer whether there is any difference in the mean anxiety score between Girls and Boys in the dataset MathAnxiety. So accordingly:\n\\[\nH_0: \\mu_{Boys} = \\mu_{Girls}\\\\\n\\] \\[\nH_a: \\mu_{Girls} \\ne \\mu_{Boys}\\\\\n\\]",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "Inference for Two Independent Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#observed-and-test-statistic",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#observed-and-test-statistic",
    "title": "Inference for Two Independent Means",
    "section": "\n Observed and Test Statistic",
    "text": "Observed and Test Statistic\nWhat would be the test statistic we would use? The difference in means. Is the observed difference in the means between the two groups of scores non-zero? We use the diffmean function:\n\nobs_diff_amas &lt;- diffmean(AMAS ~ Gender, data = MathAnxiety) \nobs_diff_amas\n\ndiffmean \n  1.7676 \n\n\nGirls’ AMAS anxiety scores are, on average, \\(1.76\\) points higher than those for Boys.\n\n\n\n\n\n\nOn Observed Difference Estimates\n\n\n\nDifferent tests here will show the difference as positive or negative, but with the same value! This depends upon the way the factor variable Gender is used, i.e. Boy-Girl or Girl-Boy…",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "Inference for Two Independent Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#inference",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#inference",
    "title": "Inference for Two Independent Means",
    "section": "\n Inference",
    "text": "Inference\nType help(wilcox.test) in your Console.\n\n\n\n\n\n\nUsing the Parametric t.test\nUsing the Mann-Whitney Test\nUsing the Linear Model Interpretation\nUsing the Permutation Test\n\n\n\nSince the data are not both normally distributed, though the variances similar, we typically cannot use a parametric t.test. However, we can still examine the results:\n\nmosaic::t_test(AMAS ~ Gender, \n               data = MathAnxiety) %&gt;% \n  broom::tidy()\n\n\n  \n\n\n\nThe p.value is \\(0.001\\) ! And the Confidence Interval does not straddle \\(0\\). So the t.test gives us good reason to reject the Null Hypothesis that the means are similar and that there is a significant difference between Boys and Girls when it comes to AMAS anxiety. But can we really believe this, given the non-normality of data?\n\n\nSince the data variables do not satisfy the assumption of being normally distributed, and though the variances are similar, we use the classical wilcox.test, which implements what we need here: the Mann-Whitney U test:3\n\nThe Mann-Whitney test as a test of mean ranks. It first ranks all your values from high to low, computes the mean rank in each group, and then computes the probability that random shuffling of those values between two groups would end up with the mean ranks as far apart as, or further apart, than you observed. No assumptions about distributions are needed so far. (emphasis mine)\n\n\\[\nmean(rank(AMAS_{Girls})) - mean(rank(AMAS_{Boys})) = \\beta_1;\\\\\n\\] \\[\nH_0: \\beta_1 = 0;\\\\\n\\\\\n\\] \\[\nH_a: \\beta_1 \\ne 0\n\\]\n\nwilcox.test(AMAS ~ Gender, data = MathAnxiety, \n            conf.int = TRUE, \n            conf.level = 0.95) %&gt;% \n  broom::tidy()\n\n\n  \n\n\n\nThe p.value is very similar, \\(0.00077\\), and again the Confidence Interval does not straddle \\(0\\), and we are hence able to reject the NULL hypothesis that the means are equal and accept the alternative hypothesis that there is a significant difference in mean anxiety scores between Boys and Girls.\n\n\nWe can apply the linear-model-as-inference interpretation to the ranked data data to implement the non-parametric test as a Linear Model:\n\\[\nlm(rank(AMAS) \\sim  gender) = \\beta_0 + \\beta_1 * gender\n\\] \\[\nH_0: \\beta_1 = 0\\\\\n\\] \\[\nH_a: \\beta_1 \\ne 0\\\\\n\\]\n\n\n\n\nlm(rank(AMAS) ~ Gender, \n   data = MathAnxiety) %&gt;% \n  broom::tidy(conf.int = TRUE,\n              conf.level = 0.95)\n\n\n  \n\n\n\n\n\n\n\n\n\nDummy Variables in lm\n\n\n\nNote how the Qual variable was used here in Linear Regression! The Gender variable was treated as a binary “dummy” variable4.\n\n\nHere too we see that the p.value for the slope term (“GenderGirl”) is significant at \\(7.4*10^{-4}\\).\n\n\nWe saw from the diagram created by Allen Downey that there is only one test5! We will now use this philosophy to develop a technique that allows us to mechanize several Statistical Models in that way, with nearly identical code.\nWe pretend that Gender has not effect on the AMAS anxiety scores. If this is our position, then the Gender labels are essentially meaningless, and we can pretend that any AMAS score belongs to a Boy or a Girl. This means we can mosaic::shuffle (permute) the Gender labels and see how uncommon our real data is. And we do not have to really worry about whether the data are normally distributed, or if their variances are nearly equal.\n\n\n\n\n\n\nImportant\n\n\n\nThe “pretend” position is exactly the NULL Hypothesis!! The “uncommon” part is the p.value under NULL!!\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nnull_dist_amas &lt;- \n  do(9999) * diffmean(data = MathAnxiety, AMAS ~ shuffle(Gender))\nnull_dist_amas\n###\n\ngf_histogram(data = null_dist_amas, ~ diffmean, bins = 25) %&gt;%\n  gf_vline(xintercept = obs_diff_amas, colour = \"red\") \n###\ngf_ecdf(data = null_dist_amas, ~ diffmean) %&gt;%\n  gf_vline(xintercept = obs_diff_amas, colour = \"red\") \n###\n1-prop1(~ diffmean &lt;= obs_diff_amas, data = null_dist_amas)\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\nprop_TRUE \n    7e-04 \n\n\n\nClearly the observed_diff_amas is much beyond anything we can generate with permutations with gender! And hence there is a significant difference in weights across gender!\n\n\n\nAll Tests Together\nWe can put all the test results together to get a few more insights about the tests:\n\nmosaic::t_test(AMAS ~ Gender, \n               data = MathAnxiety) %&gt;% \n  broom::tidy() %&gt;% \n  gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"cyan\"), cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)) %&gt;% \n  tab_header(title = \"t.test\")\n\nwilcox.test(AMAS ~ Gender, \n               data = MathAnxiety) %&gt;% \n  broom::tidy() %&gt;% \n  gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"cyan\"), cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)) %&gt;% \n  tab_header(title = \"wilcox.test\")\n\nlm(AMAS ~ Gender, \n   data = MathAnxiety) %&gt;% \n  broom::tidy(conf.int = TRUE,\n              conf.level = 0.95) %&gt;% \n  gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"cyan\"),cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)) %&gt;% \n  tab_header(title = \"Linear Model with Original Data\")\n\nlm(rank(AMAS) ~ Gender, \n   data = MathAnxiety) %&gt;% \n  broom::tidy(conf.int = TRUE,\n              conf.level = 0.95) %&gt;% \n  gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"cyan\"),cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)) %&gt;% \n  tab_header(title = \"Linear Model with Ranked Data\")\n\n\n\n\n\n\n\nt.test\n\n\nestimate\nestimate1\nestimate2\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n-1.7676\n21.16718\n22.93478\n-3.291843\n0.001055808\n580.2004\n-2.822229\n-0.7129706\nWelch Two Sample t-test\ntwo.sided\n\n\n\n\n\n\n\n\nwilcox.test\n\n\nstatistic\np.value\nmethod\nalternative\n\n\n\n37483\n0.0007736219\nWilcoxon rank sum test with continuity correction\ntwo.sided\n\n\n\n\n\n\n\n\nLinear Model with Original Data\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n21.16718\n0.3641315\n58.130602\n5.459145e-248\n20.4520482\n21.882317\n\n\nGenderGirl\n1.76760\n0.5364350\n3.295087\n1.042201e-03\n0.7140708\n2.821129\n\n\n\n\n\n\n\n\n\nLinear Model with Ranked Data\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n278.04644\n9.535561\n29.158898\n6.848992e-117\n259.31912\n296.7738\n\n\nGenderGirl\n47.64559\n14.047696\n3.391701\n7.405210e-04\n20.05668\n75.2345",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "Inference for Two Independent Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#footnotes",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#footnotes",
    "title": "Inference for Two Independent Means",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://stats.stackexchange.com/questions/2492/is-normality-testing-essentially-useless↩︎\nhttps://www.allendowney.com/blog/2023/01/28/never-test-for-normality/↩︎\nhttps://stats.stackexchange.com/q/113337↩︎\nhttps://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/intro-linear-models.html#a-linear-model-can-be-fit-to-data-with-continuous-discrete-or-categorical-x-variables↩︎\nhttps://allendowney.blogspot.com/2016/06/there-is-still-only-one-test.html↩︎\nhttps://stats.stackexchange.com/questions/92374/testing-large-dataset-for-normality-how-and-is-it-reliable↩︎\nhttps://stats.stackexchange.com/q/113337↩︎\nhttps://en.wikipedia.org/wiki/Dummy_variable_(statistics)↩︎",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "Inference for Two Independent Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#inspecting-and-charting-data-1",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#inspecting-and-charting-data-1",
    "title": "Inference for Two Independent Means",
    "section": "\n Inspecting and Charting Data",
    "text": "Inspecting and Charting Data\n\ndata(yrbss)\nyrbss\nyrbss_inspect &lt;- inspect(yrbss)\nyrbss_inspect$categorical\nyrbss_inspect$quantitative\n\n\n  \n\n\n  \n\n\n  \n\n\n\nWe have 13K data entries, and with 13 different variables, some Qual and some Quant. Many entries are missing too, typical of real-world data and something we will have to account for in our computations. The meaning of each variable can be found by bringing up the help file.Type this in your console: help(yrbss)\nIn this Case Study, our research question is:\n\n\n\n\n\n\nResearch Question\n\n\n\nDoes weight of highschoolers in this dataset vary with gender?",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "Inference for Two Independent Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#inspecting-and-charting-data-2",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#inspecting-and-charting-data-2",
    "title": "Inference for Two Independent Means",
    "section": "\n Inspecting and Charting Data",
    "text": "Inspecting and Charting Data\nFirst, histograms, densities and counts of the variable we are interested in:\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n\nyrbss_select_gender &lt;- yrbss %&gt;% \n  select(weight, gender) %&gt;% \n  drop_na(weight) # Sadly dropping off NA data\n###\nyrbss_select_gender %&gt;%\n  gf_density( ~ weight,\n              fill = ~ gender,\n              alpha = 0.5,\n              title = \"Highschoolers' Weights by Gender\") \n###\nyrbss_select_gender %&gt;%\n  gf_boxplot(weight ~ gender,\n              fill = ~ gender,\n              alpha = 0.5,\n              title = \"Highschoolers' Weights by Gender\") \n###\nyrbss_select_gender %&gt;% count(gender)\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nOverlapped Distribution plot shows some difference in the means; and the Boxplots show visible difference in the medians.\nA.  Check for Normality\nAs stated before, statistical tests for means usually require a couple of checks:\n\nAre the data normally distributed?\n\nAre the data variances similar?\n\nLet us also complete a visual check for normality, with plots since we cannot do a shapiro.test:\n\n\n\n\n\n\nShapiro-Wilks Test\n\n\n\nThe longest data it can take (in R) is 5000. Since our data is longer, we will cannot use this procedure and have to resort to visual means.\n\n\nLet us plot frequency distribution and Q-Q plots6 for both variables.\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n\nmale_student_weights &lt;- yrbss_select_gender %&gt;% \n  filter(gender == \"male\") %&gt;% \n  select(weight)\nfemale_student_weights &lt;- yrbss_select_gender %&gt;% \n  filter(gender == \"female\") %&gt;% \n  select(weight)\n#shapiro.test(male_student_weights$weight)\n#shapiro.test(female_student_weights$weight)\n\nyrbss_select_gender %&gt;%\n  gf_density( ~ weight,\n              fill = ~ gender,\n              alpha = 0.5,\n              title = \"Highschoolers' Weights by Gender\") %&gt;%\n  gf_facet_grid(~ gender) %&gt;% \n  gf_fitdistr(dist = \"dnorm\") \nyrbss_select_gender %&gt;% \n  gf_qq(~ weight | gender) %&gt;% \n  gf_qqline(ylab = \"scores\") \n\n\n\n\n\n\n\n\n\n\nDistributions are not too close to normal…perhaps a hint of a rightward skew, suggesting that there are some obese students.\nNo real evidence (visually) of the variables being normally distributed.\nB.  Check for Variances\nLet us check if the two variables have similar variances: the var.test does this for us, with a NULL hypothesis that the variances are not significantly different:\n\nvar.test( weight ~  gender, data = yrbss_select_gender, \n          conf.int = TRUE,\n          conf.level = 0.95) %&gt;% \n  broom::tidy()\n\n#qf(0.975,6164, 6413)\n\n\n  \n\n\n\nThe p.value being so small, we are able to reject the NULL Hypothesis that the variances of weight are nearly equal across the two exercise regimes.\n\n\n\n\n\n\nConditions\n\n\n\n\nThe two variables are not normally distributed.\nThe two variances are also significantly different.\n\n\n\nThis means that the parametric t.test must be eschewed in favour of the non-parametric wilcox.test. We will use that, and also attempt linear models with rank data, and a final permutation test.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "Inference for Two Independent Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#hypothesis-1",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#hypothesis-1",
    "title": "Inference for Two Independent Means",
    "section": "\n Hypothesis",
    "text": "Hypothesis\nBased on the graphs, how would we formulate our Hypothesis? We wish to infer whether there is difference in mean weight across gender. So accordingly:\n\\[\nH_0: \\mu_{male} = \\mu_{female}\\\\\n\\\\\\\nH_a: \\mu_{male} \\ne \\mu_{female}\\\n\\]",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "Inference for Two Independent Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#observed-and-test-statistic-1",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#observed-and-test-statistic-1",
    "title": "Inference for Two Independent Means",
    "section": "\n Observed and Test Statistic",
    "text": "Observed and Test Statistic\nWhat would be the test statistic we would use? The difference in means. Is the observed difference in the means between the two groups of scores non-zero? We use the diffmean function, from mosaic:\nobs_diff_gender &lt;- diffmean(weight ~ gender, data = yrbss_select_gender) \n\nobs_diff_gender\n\n\n\ndiffmean \n11.70089",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "Inference for Two Independent Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#inference-1",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#inference-1",
    "title": "Inference for Two Independent Means",
    "section": "\n Inference",
    "text": "Inference\nType help(wilcox.test) in your Console.\n\nUsing the Mann-Whitney test\nUsing the Linear Model\nUsing the Permutation Test\n\n\n\nSince the data variables do not satisfy the assumption of being normally distributed, and the variances are significantly different, we use the classical wilcox.test, which implements what we need here: the Mann-Whitney U test:7\n\nThe Mann-Whitney test as a test of mean ranks. It first ranks all your values from high to low, computes the mean rank in each group, and then computes the probability that random shuffling of those values between two groups would end up with the mean ranks as far apart as, or further apart, than you observed. No assumptions about distributions are needed so far. (emphasis mine)\n\nOur model would be:\n\\[\nmean(rank(Weight_{male})) - mean(rank(Weight_{female})) = \\beta_1;\\\\\nH_0: \\beta_1 = 0;\\\\\n\\\\\nH_a: \\beta_1 \\ne 0\n\\]\nRecall the earlier graph showing ranks of scores against Gender.\n\nwilcox.test(weight ~ gender, data = yrbss_select_gender, \n            conf.int = TRUE, \n            conf.level = 0.95) %&gt;% \n  broom::tidy()\n\n\n  \n\n\n\nThe p.value is negligible and we are able to reject the NULL hypothesis that the means are equal.\n\n\nWe can apply the linear-model-as-inference interpretation to the ranked data data to implement the non-parametric test as a Linear Model:\n\\[\nlm(rank(weight) \\sim  gender) = \\beta_0 + \\beta_1 * gender\n\\\\\nH_0: \\beta_1 = 0\\\\\n\\\\\\\nH_a: \\beta_1 \\ne 0\\\\\n\\]\n\n# Create a sign-rank function\n#signed_rank &lt;- function(x) {sign(x) * rank(abs(x))}\n\nlm(rank(weight) ~ gender, \n   data = yrbss_select_gender) %&gt;% \n  broom::tidy(conf.int = TRUE,\n              conf.level = 0.95)\n\n\n  \n\n\n\n\n\n\n\n\n\nDummy Variables in lm\n\n\n\nNote how the Qual variable was used here in Linear Regression! The gender variable was treated as a binary “dummy” variable8.\n\n\n\n\nFor the specific data at hand, we need to shuffle the records between Semifinal and Final on a per Swimmer basis and take the test statistic (difference between the two swim records for each swimmer). Another way to look at this is to take the differences between Semifinal and Final scores and shuffle the differences to either polarity. We will follow this method in the code below:\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n\nnull_dist_weight &lt;- \n  do(9999) * diffmean(data = yrbss_select_gender, weight ~ shuffle(gender))\nnull_dist_weight\n###\ngf_histogram(data = null_dist_weight, ~ diffmean, bins = 25) %&gt;%\n  gf_vline(xintercept = obs_diff_gender, colour = \"red\") \n###\ngf_ecdf(data = null_dist_weight, ~ diffmean) %&gt;%\n  gf_vline(xintercept = obs_diff_gender, colour = \"red\") \n###\nprop1(~ diffmean &lt;= obs_diff_gender, data = null_dist_weight)\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\nprop_TRUE \n        1 \n\n\n\nClearly the observed_diff_weight is much beyond anything we can generate with permutations with gender! And hence there is a significant difference in weights across gender!\n\n\n\nAll Tests Together\nWe can put all the test results together to get a few more insights about the tests:\n\nwilcox.test(weight ~ gender, data = yrbss_select_gender, \n            conf.int = TRUE, \n            conf.level = 0.95) %&gt;% \n  broom::tidy() %&gt;% \n  gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"cyan\"), cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)) %&gt;% \n  tab_header(title = \"wilcox.test\")\n\nlm(rank(weight) ~ gender, \n   data = yrbss_select_gender) %&gt;% \n  broom::tidy(conf.int = TRUE,\n              conf.level = 0.95) %&gt;% \n  gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"cyan\"),cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)) %&gt;% \n  tab_header(title = \"Linear Model with Ranked Data\")\n\n\n\n\n\n\n\nwilcox.test\n\n\nestimate\nstatistic\np.value\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n-11.33999\n10808212\n0\n-11.34003\n-10.87994\nWilcoxon rank sum test with continuity correction\ntwo.sided\n\n\n\n\n\n\n\n\nLinear Model with Ranked Data\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n4836.157\n42.52745\n113.71848\n0\n4752.797\n4919.517\n\n\ngendermale\n2851.246\n59.55633\n47.87478\n0\n2734.507\n2967.986\n\n\n\n\n\n\n\n\n\nThe wilcox.test and the linear model with rank data offer the same results. This is of course not surprising!",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "Inference for Two Independent Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#inspecting-and-charting-data-3",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#inspecting-and-charting-data-3",
    "title": "Inference for Two Independent Means",
    "section": "\n Inspecting and Charting Data",
    "text": "Inspecting and Charting Data\nWe can make distribution plots for weight by physical_3plus:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\ngf_boxplot(weight ~ physical_3plus, \n          fill = ~ physical_3plus,\n          data = yrbss_select_phy, xlab = \"Days of Exercise &gt;=3 \")\n\n\n\n\n\n\n###\ngf_density(~ weight,\n          fill = ~ physical_3plus,\n          data = yrbss_select_phy)\n\n\n\n\n\n\n\nThe box plots show how the medians of the two distributions compare, but we can also compare the means of the distributions using the following to first group the data by the physical_3plus variable, and then calculate the mean weight in these groups using the mean function while ignoring missing values by setting the na.rm argument to TRUE.\n\nyrbss_select_phy %&gt;%\n  group_by(physical_3plus) %&gt;%\n  summarise(mean_weight = mean(weight, na.rm = TRUE))\n\n\n  \n\n\n\nThere is an observed difference, but is this difference large enough to deem it “statistically significant”? In order to answer this question we will conduct a hypothesis test. But before that a few more checks on the data:\nA.  Check for Normality\nAs stated before, statistical tests for means usually require a couple of checks:\n\nAre the data normally distributed?\n\nAre the data variances similar?\n\nLet us also complete a visual check for normality,with plots since we cannot do a shapiro.test:\nyrbss_select_phy %&gt;%\n  gf_density( ~ weight,\n              fill = ~ physical_3plus,\n              alpha = 0.5,\n              title = \"Highschoolers' Weights by Exercise Frequency\") %&gt;%\n  gf_facet_grid(~ physical_3plus) %&gt;% \n  gf_fitdistr(dist = \"dnorm\") \n\n\n\n\n\n\n\nAgain, not normally distributed…\nWe can plot Q-Q plots for both variables, and also compare both data with normally-distributed data generated with the same means and standard deviations:\nyrbss_select_phy %&gt;% \n  gf_qq(~ weight | physical_3plus , color = ~ physical_3plus) %&gt;% \n  gf_qqline(ylab = \"Weight\") \n\n\n\n\n\n\n\nThe QQ-plots confirm that he tow data variables are not normally distributed.\nB.  Check for Variances\nLet us check if the two variables have similar variances: the var.test does this for us, with a NULL hypothesis that the variances are not significantly different:\nvar.test( weight ~ physical_3plus, data = yrbss_select_phy, \n          conf.int = TRUE,\n          conf.level = 0.95) %&gt;% \n  broom::tidy()\n\n# Critical F value\nqf(0.975,4021, 8341)\n\n\n\n\n  \n\n\n\n[1] 1.054398\n\n\n\nThe p.value states the probability of the data being what it is, assuming the NULL hypothesis that variances were similar. It being so small, we are able to reject this NULL Hypothesis that the variances of weight are nearly equal across the two exercise frequencies. (Compare the statistic in the var.test with the critical F-value)\n\n\n\n\n\n\nConditions\n\n\n\n\nThe two variables are not normally distributed.\nThe two variances are also significantly different.\n\n\n\nHence we will have to use non-parametric tests to infer if the means are similar.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "Inference for Two Independent Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#hypothesis-2",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#hypothesis-2",
    "title": "Inference for Two Independent Means",
    "section": "\n Hypothesis",
    "text": "Hypothesis\nBased on the graphs, how would we formulate our Hypothesis? We wish to infer whether there is difference in mean weight across physical_3plus. So accordingly:\n\\[\nH_0: \\mu_{physical-3plus-Yes} = \\mu_{physical-3plus-No}\\\\\n\\\\\\\nH_a: \\mu_{physical-3plus-Yes} \\ne \\mu_{physical-3plus-No}\\\\\n\\]",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "Inference for Two Independent Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#observed-and-test-statistic-2",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#observed-and-test-statistic-2",
    "title": "Inference for Two Independent Means",
    "section": "\n Observed and Test Statistic",
    "text": "Observed and Test Statistic\nWhat would be the test statistic we would use? The difference in means. Is the observed difference in the means between the two groups of scores non-zero? We use the diffmean function, from mosaic:\nobs_diff_phy &lt;- diffmean(weight ~ physical_3plus, data = yrbss_select_phy) \n\nobs_diff_phy\n\n\n\n diffmean \n-1.774584 \n\n\n\n\n Inference\n::: {.panel-tabset .nav-pills style=“background: whitesmoke;”}\nUsing parametric t.test\n\nWell, the variables are not normally distributed, and the variances are significantly different so a standard t.test is not advised. We can still try:\n\nmosaic::t_test(weight ~ physical_3plus,\n               var.equal = FALSE, # Welch Correction\n               data = yrbss_select_phy) %&gt;% \n  broom::tidy()\n\n\n  \n\n\n\nThe p.value is \\(8.9e-08\\) ! And the Confidence Interval is clear of \\(0\\). So the t.test gives us good reason to reject the Null Hypothesis that the means are similar. But can we really believe this, given the non-normality of data?\nUsing non-parametric paired Wilcoxon test\nHowever, we have seen that the data variables are not normally distributed. So a Wilcoxon Test, using signed-ranks, is indicated: (recall the model!)\n\n# For stability reasons, it may be advisable to use rounded data or to set digits.rank = 7, say, \n# such that determination of ties does not depend on very small numeric differences (see the example).\n\nwilcox.test(weight ~ physical_3plus,\n            conf.int = TRUE,\n            conf.level = 0.95,\n            data = yrbss_select_phy) %&gt;% \n  broom::tidy()\n\n\n  \n\n\n\nThe nonparametric wilcox.test also suggests that the means for weight across physical_3plus are significantly different.\nUsing the Linear Model Interpretation\nWe can apply the linear-model-as-inference interpretation to the ranked data data to implement the non-parametric test as a Linear Model:\n\\[\nlm(rank(weight) \\sim  physical.3plus) = \\beta_0 + \\beta_1 \\times physical.3plus\n\\\\\nH_0: \\beta_1 = 0\\\\\n\\\\\\\nH_a: \\beta_1 \\ne 0\\\\\n\\]\n\nlm(rank(weight) ~ physical_3plus, \n   data = yrbss_select_phy) %&gt;% \n  broom::tidy(conf.int = TRUE,\n              conf.level = 0.95)\n\n\n  \n\n\n\nHere too, the linear model using rank data arrives at a conclusion similar to that of the Mann-Whitney U test.\nUsing Permutation Tests\nFor this last Case Study, we will do this in two ways, just for fun: one using our familiar mosaic package, and the other using the package infer.\nBut first, we need to initialize the test, which we will save as obs_diff_**.\nobs_diff_infer &lt;- yrbss_select_phy %&gt;%\n  infer::specify(weight ~ physical_3plus) %&gt;%\n  infer::calculate(stat = \"diff in means\", order = c(\"yes\", \"no\"))\nobs_diff_infer\nobs_diff_mosaic &lt;- mosaic::diffmean(~ weight | physical_3plus, data = yrbss_select_phy)\nobs_diff_mosaic\nobs_diff_phy\n\n\n\n\n  \n\n\n\n diffmean \n-1.774584 \n\n\n diffmean \n-1.774584 \n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that obs_diff_infer is a 1 X 1 dataframe; obs_diff_mosaic is a scalar!!\n\n\n\nInference Using mosaic\n\n\nWe already have the observed difference, obs_diff_mosaic. Now we generate the null distribution using permutation, with mosaic:\n\nnull_dist_mosaic &lt;- do(999) * diffmean(~ weight | shuffle(physical_3plus), data = yrbss_select_phy)\n\nWe can also generate the histogram of the null distribution, compare that with the observed diffrence and compute the p-value and confidence intervals:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\ngf_histogram(~ diffmean, data = null_dist_mosaic) %&gt;% \n  gf_vline(xintercept = obs_diff_mosaic, colour = \"red\")\n\n\n\n\n\n\n# p-value\nprop(~ diffmean != obs_diff_mosaic, data = null_dist_mosaic)\n\nprop_TRUE \n        1 \n\n# Confidence Intervals for p = 0.95\nmosaic::cdata(~ diffmean, p = 0.95, data = null_dist_mosaic)",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "Inference for Two Independent Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#your-turn",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#your-turn",
    "title": "Inference for Two Independent Means",
    "section": "Your Turn",
    "text": "Your Turn\n\nCalculate a 95% confidence interval for the average height in meters (height) and interpret it in context.\nCalculate a new confidence interval for the same parameter at the 90% confidence level. Comment on the width of this interval versus the one obtained in the previous exercise.\nConduct a hypothesis test evaluating whether the average height is different for those who exercise at least three times a week and those who don’t.\nNow, a non-inference task: Determine the number of different options there are in the dataset for the hours_tv_per_school_day there are.\nCome up with a research question evaluating the relationship between height or weight and sleep. Formulate the question in a way that it can be answered using a hypothesis test and/or a confidence interval. Report the statistical results, and also provide an explanation in plain language. Be sure to check all assumptions, state your \\(\\alpha\\) level, and conclude in context.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "Inference for Two Independent Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#conclusion",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#conclusion",
    "title": "Inference for Two Independent Means",
    "section": "\n Conclusion",
    "text": "Conclusion\nWe have learnt how to perform inference for independent means. We have looked at the conditions that make the regular t.test possible, and learnt what to do if the conditions of normality and equal variance are not met. We have also looked at how these tests can be understood as manifestations of the linear model, with data and sign-ranked data. It should also be fairly clear now that we can test for the equivalence of two paired means, using a very simple permutation tests. Given computing power, we can always mechanize this test very quickly to get our results. And that performing this test yields reliable results without having to rely on any assumption relating to underlying distributions and so on.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "Inference for Two Independent Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#sec-references",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/two-means.html#sec-references",
    "title": "Inference for Two Independent Means",
    "section": "\n References",
    "text": "References\n\nRandall Pruim, Nicholas J. Horton, Daniel T. Kaplan, Start Teaching with R\n\nhttps://bcs.wiley.com/he-bcs/Books?action=index&itemId=111941654X&bcsId=11307\nhttps://statsandr.com/blog/wilcoxon-test-in-r-how-to-compare-2-groups-under-the-non-normality-assumption/\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nexplore\n1.3.0\n@explore\n\n\ninfer\n1.0.7\n@infer\n\n\nopenintro\n2.4.0\n@openintro\n\n\nresampledata\n0.3.1\n@resampledata\n\n\nTeachHist\n0.2.1\n@TeachHist\n\n\nTeachingDemos\n2.13\n@TeachingDemos",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Statistical Inference",
      "Inference for Two Independent Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/two-means-tutorial.html",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/two-means-tutorial.html",
    "title": "Permutation Tests for Two Means",
    "section": "",
    "text": "library(ggplot2)\nlibrary(dplyr)\nlibrary(mosaic)\n\nlibrary(resampledata)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/two-means-tutorial.html#setting-up-the-packages",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/two-means-tutorial.html#setting-up-the-packages",
    "title": "Permutation Tests for Two Means",
    "section": "",
    "text": "library(ggplot2)\nlibrary(dplyr)\nlibrary(mosaic)\n\nlibrary(resampledata)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/two-means-tutorial.html#case-study-1-verizon",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/two-means-tutorial.html#case-study-1-verizon",
    "title": "Permutation Tests for Two Means",
    "section": "Case Study-1: Verizon",
    "text": "Case Study-1: Verizon\nDoes Verizon create a difference in Repair Times between ILEC and CLEC systems?\n\ndata(\"Verizon\")\ninspect(Verizon)\n\n\ncategorical variables:  \n   name  class levels    n missing\n1 Group factor      2 1687       0\n                                   distribution\n1 ILEC (98.6%), CLEC (1.4%)                    \n\nquantitative variables:  \n  name   class min   Q1 median   Q3   max     mean       sd    n missing\n1 Time numeric   0 0.75   3.63 7.35 191.6 8.522009 14.78848 1687       0\n\n\nDescribe the Variables!\nHypothesis Specification\nWrite the Null and Alternate hypotheses here.\nNull Distribution Computation\nVerizon Conclusion"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/two-means-tutorial.html#case-story-2-recidivism",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/two-means-tutorial.html#case-story-2-recidivism",
    "title": "Permutation Tests for Two Means",
    "section": "Case Story-2: Recidivism",
    "text": "Case Story-2: Recidivism\nDo criminals released after a jail term commit crimes again? Does recidivism depend upon age?\n\ndata(\"Recidivism\")\ninspect(Recidivism)\n\n\ncategorical variables:  \n     name  class levels     n missing\n1  Gender factor      2 17019       3\n2     Age factor      5 17019       3\n3   Age25 factor      2 17019       3\n4    Race factor     10 16988      34\n5 Offense factor      2 17022       0\n6   Recid factor      2 17022       0\n7    Type factor      3 17022       0\n                                   distribution\n1 M (87.7%), F (12.3%)                         \n2 25-34 (36.6%), 35-44 (23.7%) ...             \n3 Over 25 (81.9%), Under 25 (18.1%)            \n4 White-NonHispanic (67%) ...                  \n5 Felony (80.6%), Misdemeanor (19.4%)          \n6 No (68.4%), Yes (31.6%)                      \n7 No Recidivism (68.4%), New (20.2%) ...       \n\nquantitative variables:  \n  name   class min  Q1 median  Q3  max     mean       sd    n missing\n1 Days integer   0 241    418 687 1095 473.3275 283.1393 5386   11636\n\n\nDescribe the variables!\nHypothesis Specification\nLet us see if the indidence of recidivism is dependent upon whether a person is aged less than or more than 25 years. Write the Null and Alternate hypotheses here.\n\\[\nH_0: \\mu_{recid-age-25-minus}\\ = \\mu_{recid-age-25-plus}\\\\\n\\]\n\\[\nH_a:\\mu_{recid-age-25-minus}\\ \\ne\\mu_{recid-age-25-plus}\\\\\n\\]\n\nRecidivism\n\n\n  \n\n\ninspect(Recidivism)\n\n\ncategorical variables:  \n     name  class levels     n missing\n1  Gender factor      2 17019       3\n2     Age factor      5 17019       3\n3   Age25 factor      2 17019       3\n4    Race factor     10 16988      34\n5 Offense factor      2 17022       0\n6   Recid factor      2 17022       0\n7    Type factor      3 17022       0\n                                   distribution\n1 M (87.7%), F (12.3%)                         \n2 25-34 (36.6%), 35-44 (23.7%) ...             \n3 Over 25 (81.9%), Under 25 (18.1%)            \n4 White-NonHispanic (67%) ...                  \n5 Felony (80.6%), Misdemeanor (19.4%)          \n6 No (68.4%), Yes (31.6%)                      \n7 No Recidivism (68.4%), New (20.2%) ...       \n\nquantitative variables:  \n  name   class min  Q1 median  Q3  max     mean       sd    n missing\n1 Days integer   0 241    418 687 1095 473.3275 283.1393 5386   11636\n\n\nAlso, the variable Recid is a factor variable coded “Yes” or “No”. We ought to convert it to a numeric variable of 1’s and 0’s. Why?\nNull Distribution for Recidivism\nRecidivism Conclusion\nCase Study #3: Flight Delays\nLaGuardia Airport (LGA) is one of three major airports that serves the New York City metropolitan area. In 2008, over 23 million passengers and over 375 000 planes flew in or out of LGA. United Airlines and America Airlines are two major airlines that schedule services at LGA. The data set FlightDelays contains information on all 4029 departures of these two airlines from LGA during May and June 2009.\n\ndata(\"FlightDelays\")\ninspect(FlightDelays)\n\n\ncategorical variables:  \n         name  class levels    n missing\n1     Carrier factor      2 4029       0\n2 Destination factor      7 4029       0\n3  DepartTime factor      5 4029       0\n4         Day factor      7 4029       0\n5       Month factor      2 4029       0\n6   Delayed30 factor      2 4029       0\n                                   distribution\n1 AA (72.1%), UA (27.9%)                       \n2 ORD (44.3%), DFW (22.8%), MIA (15.1%) ...    \n3 8-Noon (26.1%), Noon-4pm (26%) ...           \n4 Fri (15.8%), Mon (15.6%), Tue (15.6%) ...    \n5 June (50.4%), May (49.6%)                    \n6 No (85.2%), Yes (14.8%)                      \n\nquantitative variables:  \n          name   class min   Q1 median   Q3  max      mean         sd    n\n1           ID integer   1 1008   2015 3022 4029 2015.0000 1163.21645 4029\n2     FlightNo integer  71  371    691  787 2255  827.1035  551.30939 4029\n3 FlightLength integer  68  155    163  228  295  185.3011   41.78783 4029\n4        Delay integer -19   -6     -3    5  693   11.7379   41.63050 4029\n  missing\n1       0\n2       0\n3       0\n4       0\n\n\nThe variables in the FlightDelays dataset are:\nHypothesis Specification\nLet us compute the proportion of times that each carrier’s flights was delayed more than 20 min. We will conduct a two-sided test to see if the difference in these proportions is statistically significant.\nNull Distribution for FlightDelays\n\nwhich is very small. Hence we reject the null Hypothesis that there is no difference between carriers on delay times."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html",
    "title": "Tutorial: Permutation Testing for One Proportion",
    "section": "",
    "text": "We will use the datasets that are part of the resampledata package.1\n\nlibrary(tidyverse)\nlibrary(mosaic)\n\nlibrary(resampledata)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html#introduction",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html#introduction",
    "title": "Tutorial: Permutation Testing for One Proportion",
    "section": "",
    "text": "We will use the datasets that are part of the resampledata package.1\n\nlibrary(tidyverse)\nlibrary(mosaic)\n\nlibrary(resampledata)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html#case-study-1-verizon",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html#case-study-1-verizon",
    "title": "Tutorial: Permutation Testing for One Proportion",
    "section": "Case Study-1: Verizon",
    "text": "Case Study-1: Verizon\nDoes Verizon create a difference in Repair Times between ILEC and CLEC systems?\n\ndata(\"Verizon\")\ninspect(Verizon)\n\n\ncategorical variables:  \n   name  class levels    n missing\n1 Group factor      2 1687       0\n                                   distribution\n1 ILEC (98.6%), CLEC (1.4%)                    \n\nquantitative variables:  \n  name   class min   Q1 median   Q3   max     mean       sd    n missing\n1 Time numeric   0 0.75   3.63 7.35 191.6 8.522009 14.78848 1687       0\n\n\nDescribe the Variables!\nHypothesis Specification\nWrite the Null and Alternate hypotheses here.\nNull Distribution Computation\nVerizon Conclusion"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html#case-story-2-recidivism",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html#case-story-2-recidivism",
    "title": "Tutorial: Permutation Testing for One Proportion",
    "section": "Case Story-2: Recidivism",
    "text": "Case Story-2: Recidivism\nDo criminals released after a jail term commit crimes again? Does recidivism depend upon age?\n\ndata(\"Recidivism\")\ninspect(Recidivism)\n\n\ncategorical variables:  \n     name  class levels     n missing\n1  Gender factor      2 17019       3\n2     Age factor      5 17019       3\n3   Age25 factor      2 17019       3\n4    Race factor     10 16988      34\n5 Offense factor      2 17022       0\n6   Recid factor      2 17022       0\n7    Type factor      3 17022       0\n                                   distribution\n1 M (87.7%), F (12.3%)                         \n2 25-34 (36.6%), 35-44 (23.7%) ...             \n3 Over 25 (81.9%), Under 25 (18.1%)            \n4 White-NonHispanic (67%) ...                  \n5 Felony (80.6%), Misdemeanor (19.4%)          \n6 No (68.4%), Yes (31.6%)                      \n7 No Recidivism (68.4%), New (20.2%) ...       \n\nquantitative variables:  \n  name   class min  Q1 median  Q3  max     mean       sd    n missing\n1 Days integer   0 241    418 687 1095 473.3275 283.1393 5386   11636\n\n\nDescribe the variables!\nHypothesis Specification\nLet us see if the incidence of recidivism is dependent upon whether a person is aged less than or more than 25 years. Write the Null and Alternate hypotheses here.\n\nRecidivism\n\n\n  \n\n\n\nAlso, the variable Recid is a factor variable coded “Yes” or “No”. We ought to convert it to a numeric variable of 1’s and 0’s. Why?\nNull Distribution for Recidivism\nRecidivism Conclusion\nCase Study #3: Flight Delays\nLaGuardia Airport (LGA) is one of three major airports that serves the New York City metropolitan area. In 2008, over 23 million passengers and over 375 000 planes flew in or out of LGA. United Airlines and America Airlines are two major airlines that schedule services at LGA. The data set FlightDelays contains information on all 4029 departures of these two airlines from LGA during May and June 2009.\n\ndata(\"FlightDelays\")\ninspect(FlightDelays)\n\n\ncategorical variables:  \n         name  class levels    n missing\n1     Carrier factor      2 4029       0\n2 Destination factor      7 4029       0\n3  DepartTime factor      5 4029       0\n4         Day factor      7 4029       0\n5       Month factor      2 4029       0\n6   Delayed30 factor      2 4029       0\n                                   distribution\n1 AA (72.1%), UA (27.9%)                       \n2 ORD (44.3%), DFW (22.8%), MIA (15.1%) ...    \n3 8-Noon (26.1%), Noon-4pm (26%) ...           \n4 Fri (15.8%), Mon (15.6%), Tue (15.6%) ...    \n5 June (50.4%), May (49.6%)                    \n6 No (85.2%), Yes (14.8%)                      \n\nquantitative variables:  \n          name   class min   Q1 median   Q3  max      mean         sd    n\n1           ID integer   1 1008   2015 3022 4029 2015.0000 1163.21645 4029\n2     FlightNo integer  71  371    691  787 2255  827.1035  551.30939 4029\n3 FlightLength integer  68  155    163  228  295  185.3011   41.78783 4029\n4        Delay integer -19   -6     -3    5  693   11.7379   41.63050 4029\n  missing\n1       0\n2       0\n3       0\n4       0\n\n\nThe variables in the FlightDelays dataset are:\nHypothesis Specification\nLet us compute the proportion of times that each carrier’s flights was delayed more than 20 min. We will conduct a two-sided test to see if the difference in these proportions is statistically significant.\nNull Distribution for FlightDelays\n\nwhich is very small. Hence we reject the null Hypothesis that there is no difference between carriers on delay times."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html#references",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html#references",
    "title": "Tutorial: Permutation Testing for One Proportion",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html#footnotes",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html#footnotes",
    "title": "Tutorial: Permutation Testing for One Proportion",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://github.com/rudeboybert/resampledata↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html",
    "title": "EDA: Exploring Static Graphs for Distributions in R",
    "section": "",
    "text": "options(digits = 3)\nlibrary(tidyverse)\nlibrary(mosaic) # package for stats, simulations, and basic plots\nlibrary(mosaicData) # package containing datasets\nlibrary(ggformula) # package for professional looking plots, that use the formula interface from mosaic\nlibrary(skimr) # Summary statistics about variables in data frames\nlibrary(NHANES) # survey data collected by the US National Center for Health Statistics (NCHS)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html#setup-the-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html#setup-the-packages",
    "title": "EDA: Exploring Static Graphs for Distributions in R",
    "section": "",
    "text": "options(digits = 3)\nlibrary(tidyverse)\nlibrary(mosaic) # package for stats, simulations, and basic plots\nlibrary(mosaicData) # package containing datasets\nlibrary(ggformula) # package for professional looking plots, that use the formula interface from mosaic\nlibrary(skimr) # Summary statistics about variables in data frames\nlibrary(NHANES) # survey data collected by the US National Center for Health Statistics (NCHS)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html#introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html#introduction",
    "title": "EDA: Exploring Static Graphs for Distributions in R",
    "section": "\n Introduction",
    "text": "Introduction\nWe will create Distributions for data in R. As always, we will consistently use the Project Mosaic ecosystem of packages in R (mosaic, mosaicData and ggformula).\n\n\n\n\n\n\nTip\n\n\n\nNote the standard method for all commands from the mosaic package: goal( y ~ x | z, data = mydata, …)\nWith ggformula, one can create any graph/chart using: gf_geometry(y ~ x | z, data = mydata) OR mydata %&gt;% gf_geometry( y ~ x | z)\nThe second method may be preferable, especially if you have done some data manipulation first! More later!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html#case-study-1-galton-dataset-from-mosaicdata",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html#case-study-1-galton-dataset-from-mosaicdata",
    "title": "EDA: Exploring Static Graphs for Distributions in R",
    "section": "\n Case Study #1: Galton Dataset from mosaicData\n",
    "text": "Case Study #1: Galton Dataset from mosaicData\n\nLet us inspect what datasets are available in the package mosaicData. Type data(package = \"mosaicData\") in your Console to see what datasets are available.\nLet us choose the famous Galton dataset:\n\ndata(\"Galton\")\nGalton_inspect &lt;- inspect(Galton)\nGalton_inspect\n\n\ncategorical variables:  \n    name  class levels   n missing\n1 family factor    197 898       0\n2    sex factor      2 898       0\n                                   distribution\n1 185 (1.7%), 166 (1.2%), 66 (1.2%) ...        \n2 M (51.8%), F (48.2%)                         \n\nquantitative variables:  \n    name   class min Q1 median   Q3  max  mean   sd   n missing\n1 father numeric  62 68   69.0 71.0 78.5 69.23 2.47 898       0\n2 mother numeric  58 63   64.0 65.5 70.5 64.08 2.31 898       0\n3 height numeric  56 64   66.5 69.7 79.0 66.76 3.58 898       0\n4  nkids integer   1  4    6.0  8.0 15.0  6.14 2.69 898       0\n\n\nThe data is described as:\n\nA data frame with 898 observations on the following variables.\n\n\nfamily a factor with levels for each family\n\nfather the father’s height (in inches)\n\nmother the mother’s height (in inches)\n\nsex the child’s sex: F or M\n\nheight the child’s height as an adult (in inches)\n\nnkids the number of adult children in the family, or, at least, the number whose heights Galton recorded.\n\n\nThere is a lot of Description generated by the mosaic::inspect() command ! What can we say about the dataset and its variables? How big is the dataset? How many variables? What types are they, Quant or Qual? If they are Qual, what are the levels? Are they ordered levels? Discuss!\n\n Counts, and Charts with Counts\nNow that we know the variables, let us look at counts of data observations(rows). We know from our examination of variable types that counting of observations must be done on the basis of Qualitative variables. So let us count and plot the counts in bar charts.\n\n\n\n\n\n\nQuestion\n\n\n\nQ.1 How many families in the data for each value of nkids(i.e. Count of families by size)?\nGalton_counts &lt;- Galton %&gt;%\n  group_by(nkids) %&gt;% \n  summarise(children = n()) %&gt;% \n  # just to check\n  mutate(\n    No_of_families = as.integer(children/nkids),\n    # Why do we divide\n    \n    running_count_of_children = cumsum(children),\n    running_count_of_families = cumsum(No_of_families)) \nGalton_counts\nGalton_counts %&gt;% \n  gf_col(No_of_families ~ nkids) %&gt;% \n  gf_theme(theme_classic())\n\n\n\n\n  \n\n\n\n\n\n\n\nInsight: There are 32 1-kid families; and \\(128/8 = 16\\) 8-kid families! There is one great great 15-kid family. (Did you get the idea behind why we divide here?)\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.2. What is the count of Children by sex of the child and by family size nkids?\nGalton_counts_by_sex &lt;- Galton %&gt;% \n  mutate(family = as.integer(family)) %&gt;% \n  group_by(nkids, sex) %&gt;% \n  summarise(count_by_sex = n()) %&gt;% \n  ungroup() %&gt;% \n  group_by(sex) \nGalton_counts_by_sex\nGalton_counts_by_sex %&gt;% \n  gf_col(count_by_sex ~ nkids | sex, fill = ~sex) %&gt;% \n  gf_theme(theme_classic())\n\n\n\n\n  \n\n\n\n\n\n\n\nInsight: Hmm…decent gender balance overall, across family sizes nkids.\n\n\n\n\n\n\n\n\nFollow-up Question\n\n\n\nFollow up Question: How would we look for “gender balance” in individual families? Should we look at the family column ?\nGalton_gender_family &lt;- Galton %&gt;% \n  mutate(family = as.integer(family)) %&gt;% \n  group_by(family, sex) %&gt;% \n  summarise(count_by_sex = n()) %&gt;% \n  ungroup() %&gt;% \n  group_by(sex)\nGalton_gender_family\nGalton_gender_family %&gt;% \n  gf_col(count_by_sex ~ family | sex, fill = ~ sex) %&gt;% \n  gf_theme(theme_minimal())\n\n\n\n\n  \n\n\n\n\n\n\n\nInsight: The No of Children were distributed similarly across family sizenkids… However, this plot is too crowded and does not lead to any great insight. Using family ID was silly to plot against, wasn’t it? Not all exploratory plots will be “necessary” in the end. But they are part of the journey of getting better acquainted with the data!\n\n\n\n Stat Summaries and Distributions\nOK, on to the Quantitative variables now! What Questions might we have, that could relate not to counts by Qual variables, but to the numbers in Quant variables. Stat measures, like their ranges, max and min? Means, medians, distributions? And how these vary on the basis of Qual variables? All this using histograms and densities.\n\n\n\n\n\n\nSummary Stats\n\n\n\nAs Stigler[@stigler2016] said, summaries are the first thing to look at in data. skimr::skim has already given us a lot summary data for Quant variables. We can now use mosaic::favstats to develop these further, by slicing / facetting these wrt other Qual variables. Let us tabulate some quick stat summaries of the important variables in Galton.\n\n# summaries facetted by sex of child\nGalton_measures &lt;- favstats(~ height | sex, data = Galton)\nGalton_measures\n\n\n  \n\n\n\nInsight: We saw earlier that the mean height of the Children was 66 inches. However, are Sons taller than Daughters? Difference in mean height is 5 inches! AND…that was the same difference between fathers and mothers mean heights! Is it so simple then?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.4 How are the heights of the children distributed? Here is where we need a gf_histogram…\n\nGalton %&gt;% \n  gf_histogram(~ height,bins = 30) %&gt;% # ALWAYS try several settings for \"bins\"\n  gf_vline(xintercept = mean(Galton$height), color = \"red\") %&gt;% \n  gf_label(label = glue::glue(\"Mean Value\\n {mean(Galton$height)}\"),\n           gformula = 85 ~ 70) %&gt;% # Where do we want the label (y ~ x) %&gt;% \n  gf_labs(title = \"Galton Dataset\",\n          subtitle = \"\",\n          x = \"Heights of Children\",\n          y = \"Counts\",\n          caption = \"Using `ggformula` and Classic Theme\") %&gt;% \n  gf_theme(theme_classic())\n\n\n\n\n\n\n\nInsight: Fairly symmetric distribution…but there are a few very short and some very tall children! Always try to change the no. of bins to check of we are missing some pattern.\n\n\n#| standalone: true\n#| viewerHeight: 800\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  titlePanel(\"Interactive Histogram!\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\n        inputId = \"bins\",\n        label = \"Number of bins:\",\n        min = 1,\n        max = 50,\n        value = 30\n      )\n    ),\n    mainPanel(\n      plotOutput(outputId = \"distPlot\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  output$distPlot &lt;- renderPlot({\n    x &lt;- faithful$waiting\n    bins &lt;- seq(min(x), max(x), length.out = input$bins + 1)\n    hist(x,\n         breaks = bins, col = \"#75AADB\", border = \"white\",\n         xlab = \"Waiting time to next eruption (in mins)\",\n         main = \"Histogram of waiting times\"\n    )\n  })\n}\n\nshinyApp(ui = ui, server = server)\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.5 Is there a difference in height distributions between Male and Female children?(Quant variable sliced by Qual variable)\n\nmean_daughters &lt;- Galton_measures %&gt;% \n             filter(sex == \"F\") %&gt;% select(mean) %&gt;% as.numeric()\nmean_sons &lt;- Galton_measures %&gt;% \n             filter(sex == \"M\") %&gt;% select(mean) %&gt;% as.numeric()\nGalton %&gt;% \n  gf_density(~ height, group = ~sex, fill = ~ sex, alpha = 0.5) %&gt;% \n  gf_vline(xintercept = mean_daughters) %&gt;%\n  gf_vline(xintercept = mean_sons) %&gt;% \n  \n   gf_label(label = glue::glue(\"Average Daughters\\n {mean_daughters}\"),\n           gformula = 0.15 ~ 60, # Where do we want the label (y ~ x) \n           show.legend = FALSE,\n           fill = \"white\") %&gt;% \n  \n   gf_label(label = glue::glue(\"Average Sons\\n {mean_sons}\"),\n           gformula = 0.15 ~ 75,  # Where do we want the label (y ~ x) \n           show.legend = FALSE,\n           fill = \"white\") %&gt;% \n  gf_theme(theme_classic())\n\n\n\n\n\n\n\nInsight: There is a visible difference in average heights between girls and boys. Is that significant, however? We will need a statistical inference test to figure that out!! Claus Wilke1 says comparisons of Quant variables across groups are best made between densities and not histograms…\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.6 Are Mothers generally shorter than fathers?\n\nmean_fathers &lt;- Galton %&gt;% mosaic::mean(~ father, data = .) %&gt;% as.numeric()\n\nmean_mothers &lt;- Galton%&gt;% mean(~ mother, data = .) %&gt;% as.numeric()\n\nGalton %&gt;% \n  gf_density(~ father, fill = \"dodgerblue\", alpha = 0.3) %&gt;% \n  gf_density(~ mother, fill = \"red\", alpha = 0.3) %&gt;% \n    gf_vline(xintercept = mean_mothers) %&gt;%\n  gf_vline(xintercept = mean_fathers) %&gt;% \n  gf_labs(x = \"Parents' Heights\") %&gt;% \n    \n   gf_label(label = glue::glue(\" Average Mother \\n {mean_mothers}\"),\n           gformula = 0.15 ~ 60, # Where do we want the label (y ~ x) \n           show.legend = FALSE,\n           fill = \"white\") %&gt;% \n  \n   gf_label(label = glue::glue(\"Average Father\\n {mean_fathers}\"),\n           gformula = 0.15 ~ 75,  # Where do we want the label (y ~ x) \n           show.legend = FALSE,\n           fill = \"white\") %&gt;% \n  \n  gf_theme(theme_classic())\n\n\n\n\n\n\n\nInsight: Yes, moms are on average shorter than dads in this dataset. Again, is this difference statistically significant? We will find out in when we do Inference.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.7a. Are heights of children different based on the number of kids in the family? And For Male and Female children?\n\n# Boxplot series for daughters\ngf_boxplot(height ~ nkids, group = ~ nkids, fill = ~ sex, \n           data = Galton %&gt;% filter(sex == \"F\")) %&gt;% \n# Boxplot series for sons\n  gf_boxplot(height ~ nkids, group = ~ nkids, fill = ~ sex, \n             data = Galton %&gt;% filter(sex == \"M\")) %&gt;% \n  gf_labs(title = \"Daughters vs Sons: Who is Taller?\",\n          subtitle = \"Family Size does not change height disparity ;-D\",\n          x = \"Family Size\", y = \"Heights of Children\",\n          caption = \"Made with `ggformula` and Classic Theme\") %&gt;% \n  \n  gf_theme(theme_classic())\n\n\n\n\n\n\n\nInsight: So, at all family “strengths” (nkids), the sons are taller than the daughters, based on distributions. Box plots are used to show distributions of numeric data values and compare them between multiple groups (i.e Categorical Data, here sex and nkids).\n\n\n\n\n\n\n\n\nFollow-up Question\n\n\n\nQ. 8a. Is height difference between sons and daughters related to height difference between father and mother?\nDifferences between father and mother heights influencing height…this would be like height ~ (father-mother). This would be a relationship between two Quant variables. A histogram would not serve here and we plot this as a Scatter Plot:\nGalton_parents_diff &lt;- Galton %&gt;% \n  group_by(family,sex) %&gt;% \n  \n  # Parental Height Difference\n  mutate(diff_height = father - mother) %&gt;% \n  select(family, sex, height, diff_height) %&gt;% \n  ungroup() %&gt;% \n  group_by(sex)\nGalton_parents_diff\nGalton_parents_diff %&gt;% \n  gf_point(height ~ diff_height, color = ~ sex, data = Galton_parents_diff) %&gt;%\n  gf_lims(y = c(55,82)) %&gt;% # note: Y-AXIS does not go to zero!!\n  \n  gf_rect(55 + 80 ~ 0 + 18, fill = \"grey80\", \n          color = \"white\", alpha = 0.02) %&gt;%\n  gf_rect(55 + 80 ~ -5 + 0, fill = \"moccasin\", \n          color = \"white\", alpha = 0.02) %&gt;%   \n  \n  # Note the repetition!\n  gf_point(height ~ diff_height, color = ~ sex, data = Galton_parents_diff) %&gt;%\n  gf_smooth(method = \"lm\", se = FALSE) %&gt;% \n  gf_labs(x = \"Parental Height Difference\",\n          y = \"Children's Heights\") %&gt;% \n  \n  gf_label(label = glue::glue(\"Taller Mothers\"),\n           gformula = 75 ~ -2.5, # Where do we want the label (y ~ x) \n           show.legend = FALSE,\n           fill = \"white\") %&gt;% \n  \n  gf_label(label = glue::glue(\"Taller Fathers\"),\n           gformula = 75 ~ 15,  # Where do we want the label (y ~ x) \n           show.legend = FALSE,\n           fill = \"white\") %&gt;% \n  \n  gf_vline(xintercept = 0, linewidth = 2, \n           title = \"Does Parental Height Difference affect \n                    Children's Height?\",\n           caption = \"Note Y-axis does not go to zero\") %&gt;% \n  \n  gf_theme(theme_classic())\n\n\n\n\n  \n\n\n\n\n\n\n\nInsight: There seems no relationship, or a very small one, between children’s heights on the y-axis and the difference in parental height differences (father - mother) on the x-axis…\n\n\nAnd so on…..we can proceed from simple visualizations based on Questions to larger questions that demand inference and modelling. We hinted briefly on these in the above Case Study."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html#case-study-2-dataset-from-nhanes",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html#case-study-2-dataset-from-nhanes",
    "title": "EDA: Exploring Static Graphs for Distributions in R",
    "section": "\n Case Study #2: Dataset from NHANES\n",
    "text": "Case Study #2: Dataset from NHANES\n\nLet us try the NHANES dataset. Try help(NHANES) in your Console.\n\ndata(\"NHANES\")\n\n\n Look at the Data\n\nskimr::skim(NHANES)\n\n\nData summary\n\n\nName\nNHANES\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n76\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n31\n\n\nnumeric\n45\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nSurveyYr\n0\n1.00\nFALSE\n2\n200: 5000, 201: 5000\n\n\nGender\n0\n1.00\nFALSE\n2\nfem: 5020, mal: 4980\n\n\nAgeDecade\n333\n0.97\nFALSE\n8\n40: 1398, 0-: 1391, 10: 1374, 20: 1356\n\n\nRace1\n0\n1.00\nFALSE\n5\nWhi: 6372, Bla: 1197, Mex: 1015, Oth: 806\n\n\nRace3\n5000\n0.50\nFALSE\n6\nWhi: 3135, Bla: 589, Mex: 480, His: 350\n\n\nEducation\n2779\n0.72\nFALSE\n5\nSom: 2267, Col: 2098, Hig: 1517, 9 -: 888\n\n\nMaritalStatus\n2769\n0.72\nFALSE\n6\nMar: 3945, Nev: 1380, Div: 707, Liv: 560\n\n\nHHIncome\n811\n0.92\nFALSE\n12\nmor: 2220, 750: 1084, 250: 958, 350: 863\n\n\nHomeOwn\n63\n0.99\nFALSE\n3\nOwn: 6425, Ren: 3287, Oth: 225\n\n\nWork\n2229\n0.78\nFALSE\n3\nWor: 4613, Not: 2847, Loo: 311\n\n\nBMICatUnder20yrs\n8726\n0.13\nFALSE\n4\nNor: 805, Obe: 221, Ove: 193, Und: 55\n\n\nBMI_WHO\n397\n0.96\nFALSE\n4\n18.: 2911, 30.: 2751, 25.: 2664, 12.: 1277\n\n\nDiabetes\n142\n0.99\nFALSE\n2\nNo: 9098, Yes: 760\n\n\nHealthGen\n2461\n0.75\nFALSE\n5\nGoo: 2956, Vgo: 2508, Fai: 1010, Exc: 878\n\n\nLittleInterest\n3333\n0.67\nFALSE\n3\nNon: 5103, Sev: 1130, Mos: 434\n\n\nDepressed\n3327\n0.67\nFALSE\n3\nNon: 5246, Sev: 1009, Mos: 418\n\n\nSleepTrouble\n2228\n0.78\nFALSE\n2\nNo: 5799, Yes: 1973\n\n\nPhysActive\n1674\n0.83\nFALSE\n2\nYes: 4649, No: 3677\n\n\nTVHrsDay\n5141\n0.49\nFALSE\n7\n2_h: 1275, 1_h: 884, 3_h: 836, 0_t: 638\n\n\nCompHrsDay\n5137\n0.49\nFALSE\n7\n0_t: 1409, 0_h: 1073, 1_h: 1030, 2_h: 589\n\n\nAlcohol12PlusYr\n3420\n0.66\nFALSE\n2\nYes: 5212, No: 1368\n\n\nSmokeNow\n6789\n0.32\nFALSE\n2\nNo: 1745, Yes: 1466\n\n\nSmoke100\n2765\n0.72\nFALSE\n2\nNo: 4024, Yes: 3211\n\n\nSmoke100n\n2765\n0.72\nFALSE\n2\nNon: 4024, Smo: 3211\n\n\nMarijuana\n5059\n0.49\nFALSE\n2\nYes: 2892, No: 2049\n\n\nRegularMarij\n5059\n0.49\nFALSE\n2\nNo: 3575, Yes: 1366\n\n\nHardDrugs\n4235\n0.58\nFALSE\n2\nNo: 4700, Yes: 1065\n\n\nSexEver\n4233\n0.58\nFALSE\n2\nYes: 5544, No: 223\n\n\nSameSex\n4232\n0.58\nFALSE\n2\nNo: 5353, Yes: 415\n\n\nSexOrientation\n5158\n0.48\nFALSE\n3\nHet: 4638, Bis: 119, Hom: 85\n\n\nPregnantNow\n8304\n0.17\nFALSE\n3\nNo: 1573, Yes: 72, Unk: 51\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nID\n0\n1.00\n61944.64\n5871.17\n51624.00\n56904.50\n62159.50\n67039.00\n7.19e+04\n▇▇▇▇▇\n\n\nAge\n0\n1.00\n36.74\n22.40\n0.00\n17.00\n36.00\n54.00\n8.00e+01\n▇▇▇▆▅\n\n\nAgeMonths\n5038\n0.50\n420.12\n259.04\n0.00\n199.00\n418.00\n624.00\n9.59e+02\n▇▇▇▆▃\n\n\nHHIncomeMid\n811\n0.92\n57206.17\n33020.28\n2500.00\n30000.00\n50000.00\n87500.00\n1.00e+05\n▃▆▃▁▇\n\n\nPoverty\n726\n0.93\n2.80\n1.68\n0.00\n1.24\n2.70\n4.71\n5.00e+00\n▅▅▃▃▇\n\n\nHomeRooms\n69\n0.99\n6.25\n2.28\n1.00\n5.00\n6.00\n8.00\n1.30e+01\n▂▆▇▂▁\n\n\nWeight\n78\n0.99\n70.98\n29.13\n2.80\n56.10\n72.70\n88.90\n2.31e+02\n▂▇▂▁▁\n\n\nLength\n9457\n0.05\n85.02\n13.71\n47.10\n75.70\n87.00\n96.10\n1.12e+02\n▁▃▆▇▃\n\n\nHeadCirc\n9912\n0.01\n41.18\n2.31\n34.20\n39.58\n41.45\n42.92\n4.54e+01\n▁▂▇▇▅\n\n\nHeight\n353\n0.96\n161.88\n20.19\n83.60\n156.80\n166.00\n174.50\n2.00e+02\n▁▁▁▇▂\n\n\nBMI\n366\n0.96\n26.66\n7.38\n12.88\n21.58\n25.98\n30.89\n8.12e+01\n▇▆▁▁▁\n\n\nPulse\n1437\n0.86\n73.56\n12.16\n40.00\n64.00\n72.00\n82.00\n1.36e+02\n▂▇▃▁▁\n\n\nBPSysAve\n1449\n0.86\n118.15\n17.25\n76.00\n106.00\n116.00\n127.00\n2.26e+02\n▃▇▂▁▁\n\n\nBPDiaAve\n1449\n0.86\n67.48\n14.35\n0.00\n61.00\n69.00\n76.00\n1.16e+02\n▁▁▇▇▁\n\n\nBPSys1\n1763\n0.82\n119.09\n17.50\n72.00\n106.00\n116.00\n128.00\n2.32e+02\n▂▇▂▁▁\n\n\nBPDia1\n1763\n0.82\n68.28\n13.78\n0.00\n62.00\n70.00\n76.00\n1.18e+02\n▁▁▇▆▁\n\n\nBPSys2\n1647\n0.84\n118.48\n17.49\n76.00\n106.00\n116.00\n128.00\n2.26e+02\n▃▇▂▁▁\n\n\nBPDia2\n1647\n0.84\n67.66\n14.42\n0.00\n60.00\n68.00\n76.00\n1.18e+02\n▁▁▇▆▁\n\n\nBPSys3\n1635\n0.84\n117.93\n17.18\n76.00\n106.00\n116.00\n126.00\n2.26e+02\n▃▇▂▁▁\n\n\nBPDia3\n1635\n0.84\n67.30\n14.96\n0.00\n60.00\n68.00\n76.00\n1.16e+02\n▁▁▇▇▁\n\n\nTestosterone\n5874\n0.41\n197.90\n226.50\n0.25\n17.70\n43.82\n362.41\n1.80e+03\n▇▂▁▁▁\n\n\nDirectChol\n1526\n0.85\n1.36\n0.40\n0.39\n1.09\n1.29\n1.58\n4.03e+00\n▅▇▂▁▁\n\n\nTotChol\n1526\n0.85\n4.88\n1.08\n1.53\n4.11\n4.78\n5.53\n1.37e+01\n▂▇▁▁▁\n\n\nUrineVol1\n987\n0.90\n118.52\n90.34\n0.00\n50.00\n94.00\n164.00\n5.10e+02\n▇▅▂▁▁\n\n\nUrineFlow1\n1603\n0.84\n0.98\n0.95\n0.00\n0.40\n0.70\n1.22\n1.72e+01\n▇▁▁▁▁\n\n\nUrineVol2\n8522\n0.15\n119.68\n90.16\n0.00\n52.00\n95.00\n171.75\n4.09e+02\n▇▆▃▂▁\n\n\nUrineFlow2\n8524\n0.15\n1.15\n1.07\n0.00\n0.48\n0.76\n1.51\n1.37e+01\n▇▁▁▁▁\n\n\nDiabetesAge\n9371\n0.06\n48.42\n15.68\n1.00\n40.00\n50.00\n58.00\n8.00e+01\n▁▂▆▇▂\n\n\nDaysPhysHlthBad\n2468\n0.75\n3.33\n7.40\n0.00\n0.00\n0.00\n3.00\n3.00e+01\n▇▁▁▁▁\n\n\nDaysMentHlthBad\n2466\n0.75\n4.13\n7.83\n0.00\n0.00\n0.00\n4.00\n3.00e+01\n▇▁▁▁▁\n\n\nnPregnancies\n7396\n0.26\n3.03\n1.80\n1.00\n2.00\n3.00\n4.00\n3.20e+01\n▇▁▁▁▁\n\n\nnBabies\n7584\n0.24\n2.46\n1.32\n0.00\n2.00\n2.00\n3.00\n1.20e+01\n▇▅▁▁▁\n\n\nAge1stBaby\n8116\n0.19\n22.65\n4.77\n14.00\n19.00\n22.00\n26.00\n3.90e+01\n▆▇▅▂▁\n\n\nSleepHrsNight\n2245\n0.78\n6.93\n1.35\n2.00\n6.00\n7.00\n8.00\n1.20e+01\n▁▅▇▁▁\n\n\nPhysActiveDays\n5337\n0.47\n3.74\n1.84\n1.00\n2.00\n3.00\n5.00\n7.00e+00\n▇▇▃▅▅\n\n\nTVHrsDayChild\n9347\n0.07\n1.94\n1.43\n0.00\n1.00\n2.00\n3.00\n6.00e+00\n▇▆▂▂▂\n\n\nCompHrsDayChild\n9347\n0.07\n2.20\n2.52\n0.00\n0.00\n1.00\n6.00\n6.00e+00\n▇▁▁▁▃\n\n\nAlcoholDay\n5086\n0.49\n2.91\n3.18\n1.00\n1.00\n2.00\n3.00\n8.20e+01\n▇▁▁▁▁\n\n\nAlcoholYear\n4078\n0.59\n75.10\n103.03\n0.00\n3.00\n24.00\n104.00\n3.64e+02\n▇▁▁▁▁\n\n\nSmokeAge\n6920\n0.31\n17.83\n5.33\n6.00\n15.00\n17.00\n19.00\n7.20e+01\n▇▂▁▁▁\n\n\nAgeFirstMarij\n7109\n0.29\n17.02\n3.90\n1.00\n15.00\n16.00\n19.00\n4.80e+01\n▁▇▂▁▁\n\n\nAgeRegMarij\n8634\n0.14\n17.69\n4.81\n5.00\n15.00\n17.00\n19.00\n5.20e+01\n▂▇▁▁▁\n\n\nSexAge\n4460\n0.55\n17.43\n3.72\n9.00\n15.00\n17.00\n19.00\n5.00e+01\n▇▅▁▁▁\n\n\nSexNumPartnLife\n4275\n0.57\n15.09\n57.85\n0.00\n2.00\n5.00\n12.00\n2.00e+03\n▇▁▁▁▁\n\n\nSexNumPartYear\n5072\n0.49\n1.34\n2.78\n0.00\n1.00\n1.00\n1.00\n6.90e+01\n▇▁▁▁▁\n\n\n\n\n\nAgain, lots of data from skim, about the Quant and Qual variables. Spend a little time looking through this output.\n\nWhich variables could have been data that was given/stated by each respondent?\nAnd which ones could have been measured dependent data variables? Why do you think so?\nWhy is there so much missing data? Which variable are the most affected by this?\n\n\n Counts, and Charts with Counts\n\n\n\n\n\n\nQuestion\n\n\n\nQ.1 What are the Education levels and the counts of people with those levels?\n\nNHANES %&gt;% \n  group_by(Education) %&gt;% \n  summarise(total = n())\n\n\n  \n\n\n# This also works\n# tally(~Education, data = NHANES) %&gt;% as_tibble()\n\nInsight: The count goes up as we go from lower Education levels to higher. Need to keep that in mind. How do we understand the large number of NA entries? What could have led to these entries?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.2 How do counts of Education vs Work-status look like?\nNHANES %&gt;% \n  mutate(Education = as.factor(Education)) %&gt;% \n  group_by(Work,Education) %&gt;% \n  summarise(count = n())\nNHANES %&gt;% \n  group_by(Work, Education) %&gt;% \n  summarise(count = n()) %&gt;% \n  gf_col(count ~ Education, fill = ~ Work, position = \"dodge\") %&gt;% \n  gf_theme(theme_classic())\n\n\n\n\n  \n\n\n\n\n\n\n\nInsight: Clear increase in the number of Working people as Education goes from 8th Grade to College. No surprise. Are the NotWorking counts a surprise?\n\n\n\n Stat Summaries, Histograms, and Densities\n\n\n\n\n\n\nQuestion\n\n\n\nQ.3. What is the distribution of Physical Activity Days, across Gender? Across Education?\nNHANES %&gt;% \n  gf_histogram(~ PhysActiveDays | Gender, fill = ~ Gender) %&gt;% \n  gf_theme(theme_classic())\nNHANES %&gt;% \n  gf_histogram(~ PhysActiveDays | Education, fill = ~ Education) %&gt;% \n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\n\n\nInsight: Can we conclude anything here? The populations in each category are different, as indicated by the different heights of the bars, so what do we need to do? Take percentages or ratios of course, per-capita! How would one do that?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.3a. What is the distribution of Physical Activity Days, across Education and Sex, per capita?\nNHANES %&gt;% \n  group_by(Gender) %&gt;% \n  summarize(mean_active = mean(PhysActiveDays,na.rm = TRUE))\nNHANES %&gt;% \n  group_by(Education, Work) %&gt;% \n  summarize(mean_active = mean(PhysActiveDays,na.rm = TRUE))\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nInsight: Hmm..no great differences in per-capita physical activity. Females are marginally more active than males. No need to even plot this.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.4. How are people’s Ages distributed across levels of Education?\n# Recall there are missing data\ngf_boxplot(Age ~ Education,\n           fill = ~ Education, # Always a good idea to fill boxes\n           data = NHANES) %&gt;%\n  gf_theme(theme_classic()) %&gt;% \n  \n  # And to turn this into an interactive plot\n  plotly::ggplotly()\n\n\n\n\n\n\n\n\nInsight: Older age groups are somewhat more heavily represented in groups with lower educational status. College Graduates also have slightly narrow age….That is a nice Question for some Inferential Modelling. And how to interpret the NA group?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.5. How is Education distributed over Race?\nNHANES_by_Race1 &lt;- NHANES %&gt;% \n  group_by(Race1) %&gt;% \n  summarize(population = n())\nNHANES_by_Race1\nNHANES %&gt;% group_by(Education, Race1) %&gt;% \n  summarize( n = n()) %&gt;% \n  left_join(NHANES_by_Race1, by = c(\"Race1\" = \"Race1\")) %&gt;% \n  mutate(percapita_educated = (n/population)*100) %&gt;% \n  ungroup() %&gt;%  \n  group_by(Race1) %&gt;%\n  gf_col(percapita_educated ~ Education | Race1, fill = ~ Race1) %&gt;%\n  gf_refine(coord_flip()) %&gt;%  \n  gf_theme(theme_classic()) %&gt;% \n\n  # And to turn this into an interactive plot\n  plotly::ggplotly()\n\n\n\n\n  \n\n\n\n\n\n\n\n\nInsight: Blacks, Hispanics, and Mexicans tend to have fewer people with college degrees, as a percentage of their population. Asians and other immigrants have a significant tendency towards higher education!\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.6. What is the distribution of people’s BMI, split by Gender? By Race1?\n# One can also plot both histograms and densities in an overlay fashion,\n\nNHANES %&gt;% \n  group_by(Gender) %&gt;% \n  gf_density(~ BMI | Gender,fill = ~ Gender , alpha = 0.4) %&gt;% \n  gf_fitdistr(dist = \"dnorm\",color =  ~ Gender) %&gt;% \n  gf_theme(theme_classic()) %&gt;% \n\n  # And to turn this into an interactive plot\n  plotly::ggplotly()\nNHANES %&gt;% group_by(Race1) %&gt;% \n  gf_density(~ BMI | Race1, fill = ~ Race1) %&gt;% \n  gf_fitdistr(dist = \"dnorm\") %&gt;% \n  gf_theme(theme_classic()) %&gt;% \n\n  # And to turn this into an interactive plot\n  plotly::ggplotly()\n\n\n\n\n\n\n\n\n\n\n\n\nInsight: Blacks tend to have larger portions of their populations with larger BMI. So these races perhaps tend to obesity. By and large BMI distributions are normal.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.7. What is the distribution of people’s Testosterone level vs BMI? Split By Race1?\n\nNHANES %&gt;%  \n  gf_density_2d(Testosterone ~ BMI | Race1) %&gt;%\n  gf_theme(theme_classic()) %&gt;% \nplotly::ggplotly()\n\n\n\n\n\nInsight: Low testosterone levels exist across all BMI values, but healthy levels of T exists only over a smaller range of BMI."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html#case-study-3-a-complete-example",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html#case-study-3-a-complete-example",
    "title": "EDA: Exploring Static Graphs for Distributions in R",
    "section": "\n Case Study #3: A complete example",
    "text": "Case Study #3: A complete example\nHere is a dataset from Jeremy Singer-Vine’s blog, Data Is Plural. This is a list of all books banned in schools across the US.\n Download the data \n\nbanned &lt;- readxl::read_xlsx(path = \"../data/banned.xlsx\",\n                            sheet = \"Sorted by Author & Title\")\nbanned\n\n\n  \n\n\nnames(banned)\n\n [1] \"Author\"                    \"Title\"                    \n [3] \"Type of Ban\"               \"Secondary Author(s)\"      \n [5] \"Illustrator(s)\"            \"Translator(s)\"            \n [7] \"State\"                     \"District\"                 \n [9] \"Date of Challenge/Removal\" \"Origin of Challenge\"      \n\n\nClearly the variables are all Qualitative, except perhaps for Date of Challenge/Removal, (which in this case has been badly mangled by Excel) So we need to make counts based on the levels* of the Qual variables and plot Bar/Column charts.\nLet us quickly make some Stat Summaries ( using inspect)\n\nmosaic::inspect(banned)\n\n\ncategorical variables:  \n                        name     class levels    n missing\n1                     Author character    797 1586       0\n2                      Title character   1145 1586       0\n3                Type of Ban character      4 1586       0\n4        Secondary Author(s) character     61   98    1488\n5             Illustrator(s) character    192  364    1222\n6              Translator(s) character      9   10    1576\n7                      State character     26 1586       0\n8                   District character     86 1586       0\n9  Date of Challenge/Removal character     15 1586       0\n10       Origin of Challenge character      2 1586       0\n                                    distribution\n1  Kobabe, Maia (1.9%) ...                      \n2  Gender Queer: A Memoir (1.9%) ...            \n3  Banned Pending Investigation (46.1%) ...     \n4  Cast, Kristin (12.2%) ...                    \n5  Aly, Hatem (4.7%) ...                        \n6  Mlawer, Teresa (20%) ...                     \n7  Texas (45%), Pennsylvania (28.8%) ...        \n8  Central York (27.8%) ...                     \n9  44440 (28.8%), 44531 (28.3%) ...             \n10 Administrator (95.6%) ...                    \n\n\nInsight: Clearly the variables are all Qualitative, except perhaps for Date of Challenge/Removal, (which in this case has been badly mangled by Excel). So we need to make counts based on the* levels* of the Qual variables and plot Bar/Column charts. We will not find much use for histograms or densities.\nLet us try to answer this question:\n\n\n\n\n\n\nQuestion\n\n\n\nQ.1.What is the count of banned books by type? By US state?\nbanned %&gt;% count(`Type of Ban`, sort = TRUE)\nbanned %&gt;% count(State, sort = TRUE)\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nInsight: There are four types of book bans, and Banned Pending Investigation is the most frequent, but other types of bans are frequent too. Texas is the leader in book bans!\nFrom: https://en.wikipedia.org/wiki/Bible_Belt\n\nThe Bible Belt is a region of the Southern United States in which socially conservative Protestant Christianity plays a strong role in society. Church attendance across the denominations is generally higher than the nation’s average. The region contrasts with the religiously diverse Midwest and Great Lakes, and the Mormon corridor in Utah and southern Idaho.\n\n\n\nBible Belt\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nQ.2. What is the count of banned books by type and by US state?*\n\nbanned %&gt;% group_by(State, `Type of Ban`) %&gt;% \n  count(State, sort = TRUE) %&gt;% \n  slice_max(n = 10, order_by = n, with_ties = TRUE) %&gt;% \n  gf_col(reorder(State, n) ~ n, fill = ~`Type of Ban`,\n         xlab = \"No of Banned Books\", ylab = \"State\") %&gt;% \n  gf_refine(scale_fill_brewer(palette = \"Set2\")) %&gt;% \n  gf_theme(theme_classic())\n\n\n\n\n\n\n\nInsight: Clearly Texas is the leading state in book bans in schools. California clearly doesn’t bother itself with these things!!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html#conclusion",
    "title": "EDA: Exploring Static Graphs for Distributions in R",
    "section": "\n Conclusion",
    "text": "Conclusion\nAnd that is a wrap!! Try to work with this procedure:\n\nInspect the data using skim or inspect\n\nIdentify Qualitative and Quantitative variables\nNotice variables that have missing data and decide if that matters\nDevelop Counts of Observations for combinations of Qualitative variables (factors)\nDevelop Histograms and Densities, and slice them by Qualitative variables to develop faceted plots as needed\nContinue with other Descriptive Graphs as needed\nAt each step record the insight and additional questions!!\n\nOnwards with Correlations and other visuals…\nAnd then on the inference and modelling!!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html#references",
    "title": "EDA: Exploring Static Graphs for Distributions in R",
    "section": "\n References",
    "text": "References\n\nA detailed analysis of the NHANES dataset, https://awagaman.people.amherst.edu/stat230/Stat230CodeCompilationExampleCodeUsingNHANES.pdf"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/files/distributions.html#footnotes",
    "title": "EDA: Exploring Static Graphs for Distributions in R",
    "section": "Footnotes",
    "text": "Footnotes\n\nFundamentals of Data Visualization (clauswilke.com)↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html",
    "title": "📊 Distributions, Densities, Bar Plots, and Boxplots",
    "section": "",
    "text": "R (Static Viz)  \n\n  Radiant Tutorial \n  Datasets",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📊 Distributions, Densities, Bar Plots, and Boxplots"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#slides-and-tutorials",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#slides-and-tutorials",
    "title": "📊 Distributions, Densities, Bar Plots, and Boxplots",
    "section": "",
    "text": "R (Static Viz)  \n\n  Radiant Tutorial \n  Datasets",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📊 Distributions, Densities, Bar Plots, and Boxplots"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#using-web-r",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#using-web-r",
    "title": "📊 Distributions, Densities, Bar Plots, and Boxplots",
    "section": "\n Using web-R",
    "text": "Using web-R\nThis tutorial uses web-r that allows you to run all code within your browser, on all devices. Most code chunks herein are formatted in a tabbed structure ( like in an old-fashioned library) with duplicated code. The tabs in front have regular R code that will work when copy-pasted in your RStudio session. The tab “behind” has the web-R code that can work directly in your browser, and can be modified as well. The R code is also there to make sure you have original code to go back to, when you have made several modifications to the code on the web-r tabs and need to compare your code with the original!\nKeyboard Shortcuts\n\nRun selected code using either:\n\nmacOS: ⌘ + ↩︎/Return\n\nWindows/Linux: Ctrl + ↩︎/Enter\n\n\n\nRun the entire code by clicking the “Run code” button or pressing Shift+↩︎.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📊 Distributions, Densities, Bar Plots, and Boxplots"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#setting-up-r-packages",
    "title": "📊 Distributions, Densities, Bar Plots, and Boxplots",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\n R\n\n\n\noptions(paged.print = TRUE)\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(ggformula)\n\n#install.packages(\"remotes\")\n#library(remotes)\n#remotes::install_github(\"wilkelab/ggridges\")\nlibrary(ggridges)\nlibrary(skimr)\n\nlibrary(palmerpenguins) # Our new favourite dataset",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📊 Distributions, Densities, Bar Plots, and Boxplots"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#what-graphs-will-we-see-today",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#what-graphs-will-we-see-today",
    "title": "📊 Distributions, Densities, Bar Plots, and Boxplots",
    "section": "\n What graphs will we see today?",
    "text": "What graphs will we see today?\nSome of the very basic and commonly used plots for data are:\n\nBar and Column Charts\nHistograms and Frequency Distributions\nBox Plots\nRidge Plots ( Quant + Qual variables)\n\nLet us listen to the late great Hans Rosling from the Gapminder Project, which aims at telling stories of the world with data, to remove systemic biases about poverty, income and gender related issues.\n\n\n Graphing Packages in R\nThere are several Data Visualization packages, even systems, within R.\n\nBase R supports graph making out of the box;\nThe most well known is ggplot https://ggplot2-book.org/ which uses Leland Wilkinson’s concept of a “Grammar of Graphics”;\nThere is the lattice package https://lattice.r-forge.r-project.org/ which uses the “Trellis Graphics” concept framework for data visualization developed by R. A. Becker, W. S. Cleveland, et al.;\nAnd the grid package https://bookdown.org/rdpeng/RProgDA/the-grid-package.html that allows extremely fine control of shapes plotted on the graph.\n\nEach system has its benefits and learning complexities. We will look at plots created using the simpler and intuitive ggformula system that uses the popularggplot framework, but provides a simplified interface that is easy to recall and apply. While our first option will be to use ggformula, we will, where appropriate state ggplot code too for comparison.\nA quick reminder on how mosaic and ggformula work in a very similar fashion:\n\n\n\n\n\n\nmosaic and ggformula command template\n\n\n\nNote the standard method for all commands from the mosaic and ggformula packages: goal( y ~ x | z, data = _____)\nWith ggformula, one can create any graph/chart using: gf_***(y ~ x | z, data = _____)\nIn practice, we often use: dataframe %&gt;%  gf_***(y ~ x | z) which has cool benefits such as “autocompletion” of variable names, as we shall see. The “***” indicates what kind of graph you desire: histogram, bar, scatter, density; the “___” is the name of your dataset that you want to plot with.\n\n\n\n\n\n\n\n\nggplot command template\n\n\n\nThe ggplot2 template is used to identify the dataframe, identify the x and y axis, and define visualized layers:\nggplot(data = ---, mapping = aes(x = ---, y = ---)) + geom_----()\nNote: —- is meant to imply text you supply. e.g. function names, data frame names, variable names.\nIt is helpful to see the argument mapping, above. In practice, rather than typing the formal arguments, code is typically shorthanded to this:\ndataframe %&gt;%  ggplot(aes(xvar, yvar)) + geom_----()",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📊 Distributions, Densities, Bar Plots, and Boxplots"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#bar-charts-and-histograms",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#bar-charts-and-histograms",
    "title": "📊 Distributions, Densities, Bar Plots, and Boxplots",
    "section": "\n Bar Charts and Histograms",
    "text": "Bar Charts and Histograms\nBar Charts show counts of observations with respect to a Qualitative variable. For instance, a shop inventory with shirt-sizes. Each bar has a height proportional to the count per shirt-size, in this example.\nAlthough Histograms may look similar to Bar Charts, the two are different. First, histograms show continuous Quant data. By contrast, bar charts show categorical data, such as shirt-sizes, or apples, bananas, carrots, etc.\nHistograms are best to show the distribution of raw Quantitative data, by displaying the number of values that fall within defined ranges, often called buckets or bins. We use a Quant variable on the x-axis and the histogram shows us how frequently different values occur for that variable by showing counts/frequencies on the y-axis. The x-axis is typically broken up into “buckets” or ranges for the x-variable, And usually you can adjust the bucket ranges to explore frequency patterns. For example, you can shift histogram buckets from 0-1, 1-2, 2-3, etc. to 0-2, 2-4, etc. Histograms do not usually show spaces between buckets because the buckets represent contiguous ranges, while bar charts show spaces to separate each (unconnected) category/level within a Qual variable.\nTo complicate matters: Having said all that, the histogram is really a bar chart in disguise! You probably suspect that the “bucketing” of the Quant variable is tantamount to creating a Qual variable! Each bucket is a level in this fictitious bucketed Quant variable.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📊 Distributions, Densities, Bar Plots, and Boxplots"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#case-study-1-diamonds-dataset",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#case-study-1-diamonds-dataset",
    "title": "📊 Distributions, Densities, Bar Plots, and Boxplots",
    "section": "\n Case Study-1: diamonds dataset",
    "text": "Case Study-1: diamonds dataset\nWe will first look at at a dataset that is directly available in R, the diamonds dataset.\n\n Examine the Data\nAs per our Workflow, we will look at the data using all the three methods we have seen.\n\n\n R\n web-r\n\n\n\n\nglimpse(diamonds)\n\nRows: 53,940\nColumns: 10\n$ carat   &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.…\n$ cut     &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Ver…\n$ color   &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I,…\n$ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, …\n$ depth   &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64…\n$ table   &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58…\n$ price   &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 34…\n$ x       &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.…\n$ y       &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.…\n$ z       &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.…\n\n\n\nskim(diamonds)\n\n\nData summary\n\n\nName\ndiamonds\n\n\nNumber of rows\n53940\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\ncut\n0\n1\nTRUE\n5\nIde: 21551, Pre: 13791, Ver: 12082, Goo: 4906\n\n\ncolor\n0\n1\nTRUE\n7\nG: 11292, E: 9797, F: 9542, H: 8304\n\n\nclarity\n0\n1\nTRUE\n8\nSI1: 13065, VS2: 12258, SI2: 9194, VS1: 8171\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\ncarat\n0\n1\n0.80\n0.47\n0.2\n0.40\n0.70\n1.04\n5.01\n▇▂▁▁▁\n\n\ndepth\n0\n1\n61.75\n1.43\n43.0\n61.00\n61.80\n62.50\n79.00\n▁▁▇▁▁\n\n\ntable\n0\n1\n57.46\n2.23\n43.0\n56.00\n57.00\n59.00\n95.00\n▁▇▁▁▁\n\n\nprice\n0\n1\n3932.80\n3989.44\n326.0\n950.00\n2401.00\n5324.25\n18823.00\n▇▂▁▁▁\n\n\nx\n0\n1\n5.73\n1.12\n0.0\n4.71\n5.70\n6.54\n10.74\n▁▁▇▃▁\n\n\ny\n0\n1\n5.73\n1.14\n0.0\n4.72\n5.71\n6.54\n58.90\n▇▁▁▁▁\n\n\nz\n0\n1\n3.54\n0.71\n0.0\n2.91\n3.53\n4.04\n31.80\n▇▁▁▁▁\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nBusiness Insights on Examining the diamonds dataset\n\n\n\n\nThis is a large dataset (54K rows).\nThere are several Qualitative variables: cut, color and clarity. These have 5, 7, and 8 levels respectively. The fact that the class for these is ordered suggests that these are factors and that the levels have a sequence/order.\n\ncarat, price, x, y, z, depth and table are Quantitative variables.\nThere are no missing values for any variable, all are complete with 54K entries.\n\n\n\n\n Plotting Histograms\nLet’s plot some histograms.\n\n\nUsing ggformula\nUsing ggplot\n web-r\n\n\n\n## Set graph theme\ntheme_set(new = theme_custom())\n##\ngf_histogram(~ price, data = diamonds) %&gt;%\n  gf_labs(title = \"Plot A: Diamond Prices\",caption = \"ggformula\") \n###\ndiamonds %&gt;% \n  gf_histogram(~ price, \n               fill = ~ cut, \n               alpha = 0.3) %&gt;%\n  gf_labs(title = \"Plot B: Prices by Cut\",\n          caption = \"ggformula\")\n###\ndiamonds %&gt;% \n  gf_histogram(~ price, \n               fill = ~ cut) %&gt;%\n  gf_facet_wrap(~ cut) %&gt;%\n  gf_labs(title = \"Plot C: Prices by Filled and Facetted by Cut\",\n          caption = \"ggformula\") %&gt;%\n  gf_theme(theme(axis.text.x = element_text(angle = 45, hjust = 1)))\n###\ndiamonds %&gt;% \n  gf_histogram(~ price,\n                          fill = ~ cut, \n                          color = \"black\") %&gt;% \n  gf_facet_wrap(~ cut, scales = \"free_y\", nrow = 2) %&gt;%\n  gf_labs(title = \"Plot D: Prices Filled and Facetted by Cut\", \n          subtitle = \"Free y-scale\",\n          caption = \"ggformula\") %&gt;%\n  gf_theme(theme(axis.text.x = element_text(angle = 45, hjust = 1)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Set graph theme\ntheme_set(new = theme_custom())\n##\n\nggplot(data = diamonds) + \n  geom_histogram(aes(x = price)) +\n  labs(title = \"Plot A: Diamond Prices\",\n       caption = \"ggplot\")\n###\ndiamonds %&gt;% ggplot() + \n  geom_histogram(aes(x = price, \n                     fill = cut), \n                     alpha = 0.3) + \n  labs(title = \"Plot B: Prices by Cut\",\n       caption = \"ggplot\")\n###\ndiamonds  %&gt;% ggplot() + \n  geom_histogram(aes(price, fill = cut)) +\n  facet_wrap(facets = vars(cut)) + \n  labs(title = \"Plot C: Prices by Filled and Facetted by Cut\",\n       caption = \"ggplot\") + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n###\ndiamonds  %&gt;% ggplot() + \n  geom_histogram(aes(price, fill = cut), color = \"black\") +\n  facet_wrap(facets = vars(cut), scales = \"free_y\") +\n  labs(title = \"Plot D: Prices by Filled and Facetted by Cut\",\n       subtitle = \"Free y-scale\",\n       caption = \"ggplot\") + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nBusiness Insights from diamond Histograms\n\n\n\n\nThe price distribution is heavily skewed to the right. There are many diamonds that have low prices and relatively few diamonds that are very expensive. This long-tailed nature of the histogram holds true regardless of the cut of the diamond.\nSee the x-axis range for each plot in Plot D! Price ranges are the same regardless of cut !! Very surprising! So cut is perhaps not the only thing that determines price…\nFacetting the plot into small multiples helps look at patterns better: overlapping histograms are hard to decipher. Adding color defines the bars in the histogram very well.\n\n\n\n\n\n\n\n\n\nA Hypothesis\n\n\n\nThe surprise insight above should lead you to make a Hypothesis! You should decide whether you want to investigate this question further, making more graphs, as we will see. Here, we are making a Hypothesis that more than just cut determines the price of a diamond.\n\n\nAn Interactive App for Histograms\nType in your Console:\n\n```{r}\n#| eval: false\ninstall.packages(\"shiny\")\nlibrary(shiny)\nrunExample(\"01_hello\")      # an interactive histogram\n```\n\n\n Plotting Barcharts\nLet’s plot some bar graphs: recall that for bar charts, we need to choose Qual variables to count with!\n\n\nUsing ggformula\nUsing ggplot\n web-r\n\n\n\n## Set graph theme\ntheme_set(new = theme_custom())\n##\n\ngf_bar(~ cut, data = diamonds) %&gt;%\n  gf_labs(title = \"Plot A: Diamonds Counts of different Cuts\")\n###\ndiamonds %&gt;% \n  gf_bar( ~ cut, \n          fill = ~ cut) %&gt;%\n  gf_labs(title = \"Plot B: Diamonds Counts filled by Cut\")\n###\ndiamonds %&gt;% \n  gf_bar( ~ cut, \n          fill = ~ clarity, \n          position = \"stack\",\n          alpha = 0.3) %&gt;%\n  gf_labs(title = \"Plot C: Diamonds Counts by Cut filled by Clarity\",\n          subtitle = \"Stacked Bar Chart\")\n###\ndiamonds %&gt;% \n  gf_bar( ~ cut, \n          fill = ~ clarity, \n          position = \"dodge\",\n          color = \"black\") %&gt;%\n  gf_refine(scale_fill_viridis_d(option = \"turbo\")) %&gt;% # inferno, magma, cividis...etc\n  gf_labs(title = \"Plot D: Diamonds Counts by Cut filled by Clarity\",\n          subtitle = \"Dodged Bar Chart\",\n          caption = \"Turbo Palette from Viridis set\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Set graph theme\ntheme_set(new = theme_custom())\n##\n\ndiamonds %&gt;% gf_bar( ~ cut, \n                     fill = ~ clarity, \n                     position = \"dodge\", \n                     colour = \"black\") %&gt;%\n  gf_facet_wrap(vars(color),scales = \"free_y\") %&gt;%\n  gf_refine(scale_fill_viridis_d(option = \"turbo\")) %&gt;%\n  gf_theme(theme(axis.text.x = element_text(angle = 45,hjust = 1))) %&gt;%\n  gf_labs(title = \"Plot E: Diamonds Counts by Cut filled by Clarity\",\n          subtitle = \"Dodged Bar Chart Facetted by Color\",\n          caption = \"Turbo Palette from Viridis set\")\n\n\n\n\n\n\n\n\n\n## Set graph theme\ntheme_set(new = theme_custom())\n##\n\nggplot(diamonds)  + \n  geom_bar(aes(cut)) + \n  labs(title = \"Plot A: Diamonds Counts of different Cuts\")\n###\ndiamonds %&gt;% \n  ggplot() + \n  geom_bar(aes(cut, fill = cut) ) + \n  labs(title = \"Plot B: Diamonds Counts filled by Cut\")\n###\ndiamonds %&gt;% \n  ggplot() + \n  geom_bar(aes(cut, fill = clarity), \n               position = \"stack\",\n               alpha = 0.3) + \n  labs(title = \"Plot C: Diamonds Counts by Cut filled by Clarity\",\n       subtitle = \"Stacked Bar Chart\")\n###\ndiamonds %&gt;% \n  ggplot() + \n  geom_bar(aes(cut, fill = clarity), \n               position = \"dodge\",\n               color = \"black\") + \n  scale_fill_viridis_d(option = \"turbo\") +  # inferno, magma, cividis...etc\n  labs(title = \"Plot D: Diamonds Counts by Cut filled by Clarity\",\n       subtitle = \"Dodged Bar Chart\",\n       caption = \"Turbo Palette from Viridis set\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Set graph theme\ntheme_set(new = theme_custom())\n##\n\ndiamonds %&gt;% \n  ggplot() + \n  geom_bar(aes(cut, fill = clarity), \n               position = \"dodge\", \n               colour = \"black\") +\n  facet_wrap(vars(color), scales = \"free_y\") + \n  scale_fill_viridis_d(option = \"turbo\") +\n  theme(axis.text.x = element_text(angle = 45,hjust = 1)) + \n  labs(title = \"Plot E: Diamonds Counts by Cut filled by Clarity\",\n       subtitle = \"Dodged Bar Chart Facetted by Color\",\n       caption = \"Turbo Palette from Viridis set\")\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nBusiness Insights from diamond Bar Charts\n\n\n\nAs seen before the counts for different values of cut are not the same:\n\nthe dataset is not balanced.\nFrom Plot D, the counts per level of cut increase steadily, with Ideal cut dominating.\nAnd the counts at different values of clarity within each cut are also not equal; the levels VS1, VS2, VVS1, VVS2 tend to be higher.\nFrom Plot E, we have cut, clarity facetted by color. The overall counts are higher for the Ideal cut across all colors.\nAmong the colors, the color J seems to have the lowest counts, judging from the y-axis limits.\n\n\n\n\n\n\n\n\n\ngf_barvs gf_col (and geom_bar vs geom_col)\n\n\n\nThese two geometries in ggformula and ggplot do very similar things: give us a bar plot. There is one significant difference however: gf_bar/geom_bar performs counting internally, whereas gf_col/geom_col need the data to be counted beforehand. gf_counts() is another name for gf_bar(). See the code below:\n## Set graph theme\ntheme_set(new = theme_custom())\n##\n\ndiamonds %&gt;% \n  gf_bar(~ cut) %&gt;% # performs counts based on `cut` (Qual variable)\n                # default y-axis labelling is \"count\"\n                \n  gf_labs(title = \"Bar Plot Counts internally\")\n###\n\ndiamonds %&gt;% \n  # count gives a default variable called `n`\n  # We rename it to \"counts\". \n  # Note the quotation marks in the naming ceremony below\n  count(cut, name = \"How_Many\") %&gt;% \n  \n  # gf_col needs counted data\n  # we use the (re)named variable counts vs cut\n  gf_col(How_Many ~ cut) %&gt;% \n  gf_labs(title = \"Column Plot needs pre-counted data\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote also that gf_bar/geom_bar takes only ONE variable (for the x-axis), whereas gf_col/geom_col needs both X and Y variables since it simply plots columns.\nBoth are useful!\n\n\n\n\n\n\n\n\nAnd we can plot Proportions and Percentages too!\n\n\n\nAlso check out gf_props and gf_percents ! These are also very useful ggformula functions!\n## Set graph theme\ntheme_set(new = theme_custom())\n##\n\ngf_props(~ substance,\n  data = mosaicData::HELPrct, fill = ~ sex,\n  position = \"dodge\"\n) %&gt;%\n  gf_labs(title = \"Plotting Proportions using gf_props\")\n###\ngf_percents(~ substance,\n  data = mosaicData::HELPrct, fill = ~ sex,\n  position = \"dodge\"\n)%&gt;%\n  gf_labs(title = \"Plotting Percentages using gf_percents\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Plotting Densities\nYou might imagine a density chart as a histogram where the buckets are infinitesimally small, i.e. zero width. This may seem counter-intuitive, but densities have their uses in spotting the ranges in the data where there are more frequent values. In this, they serve a similar purpose as do histograms, but may offer insights not readily apparent with histograms, especially with default bucket widths.\n\n\nUsing ggformula\nUsing ggplot\n web-r\n\n\n\n## Set graph theme\ntheme_set(new = theme_custom())\n##\n\n\ngf_density(~ price, data = diamonds) %&gt;%\n  gf_labs(title = \"Plot A: Diamond Prices\",caption = \"ggformula\")\n###\ndiamonds %&gt;% gf_density(~ price, \n                          fill = ~ cut, \n                          color = ~ cut,\n                          alpha = 0.3) %&gt;%\n  gf_refine(scale_color_viridis_d(option = \"magma\",\n                                  aesthetics = c(\"colour\", \"fill\"))) %&gt;%\n  gf_labs(title = \"Plot B: Prices by Cut\",caption = \"ggformula\")\n###\ndiamonds %&gt;% gf_density(~ price,\n                          fill = ~ cut) %&gt;%\n  gf_facet_wrap(vars(cut)) %&gt;%\n  gf_labs(title = \"Plot C: Prices by Filled and Facetted by Cut\",caption = \"ggformula\") \n###\ndiamonds %&gt;% gf_density(~ price,\n                          fill = ~ cut, \n                        color = \"black\") %&gt;% \n  gf_facet_wrap(vars(cut), scales = \"free_y\", nrow = 2) %&gt;%\n  gf_labs(title = \"Plot D: Prices Filled and Facetted by Cut\", \n          subtitle = \"Free y-scale\", caption = \"ggformula\") %&gt;%\n  gf_theme(theme(axis.text.x = element_text(angle = 45,hjust = 1)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Set graph theme\ntheme_set(new = theme_custom())\n##\n\ndiamonds %&gt;% ggplot() + \n  geom_density(aes(price)) +\n  labs(title = \"Plot A: Diamond Prices\",caption = \"ggplot\")\n###\ndiamonds %&gt;% ggplot() + \n  geom_density(aes(price, \n                   fill = cut, \n                   color = cut),\n                   alpha = 0.3) +\n  scale_color_viridis_d(option = \"magma\",\n                        aesthetics = c(\"colour\", \"fill\")) + \n  labs(title = \"Plot B: Prices by Cut\",caption = \"ggplot\")\n###\ndiamonds %&gt;% ggplot() + \n  geom_density(aes(price,\n                   fill = cut)) + \n  facet_wrap(vars(cut)) + \n  labs(title = \"Plot C: Prices by Filled and Facetted by Cut\",\n      caption = \"ggplot\") \n###\ndiamonds %&gt;% ggplot() + \n  geom_density(aes(price,\n                   fill = cut), \n                   color = \"black\") + \n  facet_wrap(vars(cut), scales = \"free_y\", nrow = 2) + \n  labs(title = \"Plot D: Prices Filled and Facetted by Cut\", \n          subtitle = \"Free y-scale\", caption = \"ggplot\") + \n  theme(axis.text.x = element_text(angle = 45,hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nBusiness Insights from diamond Densities\n\n\n\nPretty much similar conclusions as with histograms. Although densities may not be used much in business contexts, they are better than histograms when comparing multiple distributions! So you should use thems!\n\n\n\n Box Plots\nMost plots do some internal calculations before creating the plots, such as “counts” in bar charts, and also “buckets + counts” in histograms.\nWith boxplots, the values of a Quant variable are ranked in increasing order of magnitude. The median and the interquartile range are also calculated, and so are the outliers. A typical box plot looks like this:\n\n\n\n\n\n\n\n\n\n\nUsing ggformula\nUsing ggplot\n web-r\n\n\n\n## Set graph theme\ntheme_set(new = theme_custom())\n##\n\ngf_boxplot(price ~ \"All Diamonds\", data = diamonds) %&gt;% \n  gf_labs(title = \"Plot A: Boxplot for Diamond Prices\")\n###\ndiamonds %&gt;% \n  gf_boxplot(price ~ cut) %&gt;% \n  gf_labs(title = \"Plot B: Price by Cut\")\n###\ndiamonds %&gt;% \n  gf_boxplot(price ~ cut, \n             fill = ~ cut, \n             color = ~ cut,\n             alpha = 0.3) %&gt;% \n  gf_labs(title = \"Plot C: Price by Cut\")\n###\ndiamonds %&gt;% \n  gf_boxplot(price ~ cut, \n             fill = ~ cut, \n             colour = ~ cut,             \n             alpha = 0.3) %&gt;% \n  gf_facet_wrap(vars(clarity)) %&gt;%\n  gf_labs(title = \"Plot D: Price by Cut facetted by Clarity\") %&gt;%\n  gf_theme(theme(axis.text.x = element_text(angle = 45,hjust = 1)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Set graph theme\ntheme_set(new = theme_custom())\n##\n\ndiamonds %&gt;% ggplot() + \n  geom_boxplot(aes(y = price)) + # note: y, not x\n  labs(title = \"Plot A: Boxplot for Diamond Prices\")\n###\ndiamonds %&gt;% ggplot() + \n  geom_boxplot(aes(cut, price)) + \n  labs(title = \"Plot B: Price by Cut\")\n###\ndiamonds %&gt;% ggplot() + \n  geom_boxplot(aes(cut, \n                   price, \n                   color = cut, fill = cut), alpha = 0.4) +\n  labs(title = \"Plot C: Price by Cut\")\n###\ndiamonds %&gt;% ggplot() + \n  geom_boxplot(aes(cut, \n                   price, \n                   color = cut, fill = cut), alpha = 0.4)  +  \n  facet_wrap(vars(clarity)) +\n  labs(title = \"Plot D: Price by Cut facetted by Clarity\") +\n  theme(axis.text.x = element_text(angle = 45,hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nBusiness Insights from Boxplots\n\n\n\n\nPlot A shows the presence of quite some high-end outliers; the median price is USD2500.\nWhen broken up by Qual variable cut we see that the median mileage for different drive trains are slightly different…but not steadily increasing, on the contrary the median price drops for Good, VeryGood cuts, increases for Premium but stragely again drops for Ideal cuts.\nThis could again mean that cut alone does not determine diamond prices…\nSimilar trends with Plot D showing facetted plots\n\n\n\n\n Ridge Plots\nSometimes we may wish to show the distribution/density of a Quant variable, against several levels of a Qual variable. For instance, the prices of different items of furniture, based on the furniture “style” variable. Or the sales of a particular line of products, across different shops or cities. We did this with both histograms and densities, by colouring based on a Qual variable, and by facetting using a Qual variable. There is a third way, using what is called a ridge plot. ggformula support this plot by importing/depending upon the ggridges package; however, ggplot itself appears to not have this capability.\n\n\nUsing ggformula\n web-r\n\n\n\n\n## Set graph theme\ntheme_set(new = theme_custom())\n##\n\n\ngf_density_ridges(drv ~ hwy, fill = ~ drv, \n                  alpha = 0.3, \n                  rel_min_height = 0.005, data = mpg) %&gt;% \n  gf_refine(scale_y_discrete(expand = c(0.01, 0)),\n            scale_x_continuous(expand = c(0.01, 0))) %&gt;% \n  gf_labs(title = \"Ridge Plot\")\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nBusiness Insights from mpg Ridge Plots\n\n\n\nThis is another way of visualizing multiple distributions, of a Quant variable at different levels of a Qual variable. We see that the distribution of hwy mileage varies substantially with drv type.\n\n\n\n Violin Plots\nOften one needs to view multiple densities at the same time. Ridge plots of course give us one option, where we get densities of a Quant variable split by a Qual variable. Another option is to generate a density plot facetted into small multiples using a Qual variable.\nYet another plot that allows comparison of multiple densities side by side is a violin plot. The violin plot combines the aspects of a boxplot(ranking of values, median, quantiles…) with a superimposed density plot. This allows us to look at medians, means, densities, and quantiles of a Quant variable with respect to another Qual variable. Let us see what this looks like!\n\n\nUsing ggformula\nUsing ggplot\n web-r\n\n\n\n## Set graph theme\ntheme_set(new = theme_custom())\n##\n\n\ngf_violin(price ~ \"All Diamonds\", data = diamonds, \n          draw_quantiles = c(0,.25,.50,.75)) %&gt;%\n  gf_labs(title = \"Plot A: Violin plot for Diamond Prices\")\n###\ndiamonds %&gt;% \n  gf_violin(price ~ cut,\n            draw_quantiles = c(0,.25,.50,.75)) %&gt;% \n  gf_labs(title = \"Plot B: Price by Cut\")\n###\ndiamonds %&gt;% \n  gf_violin(price ~ cut, \n             fill = ~ cut, \n             color = ~ cut,\n             alpha = 0.3,\n            draw_quantiles = c(0,.25,.50,.75)) %&gt;% \n  gf_labs(title = \"Plot C: Price by Cut\")\n###\ndiamonds %&gt;% \n  gf_violin(price ~ cut, \n             fill = ~ cut, \n             colour = ~ cut,             \n             alpha = 0.3,draw_quantiles = c(0,.25,.50,.75)) %&gt;% \n  gf_facet_wrap(vars(clarity)) %&gt;%\n  gf_labs(title = \"Plot D: Price by Cut facetted by Clarity\") %&gt;%\n  gf_theme(theme(axis.text.x = element_text(angle = 45,hjust = 1)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Set graph theme\ntheme_set(new = theme_custom())\n##\n\ndiamonds %&gt;% ggplot() + \n  geom_violin(aes(y = price, x = \"\"),\n              draw_quantiles = c(0,.25,.50,.75)) + # note: y, not x\n  labs(title = \"Plot A: violin for Diamond Prices\")\n###\ndiamonds %&gt;% ggplot() + \n  geom_violin(aes(cut, price),\n              draw_quantiles = c(0,.25,.50,.75)) + \n  labs(title = \"Plot B: Price by Cut\")\n###\ndiamonds %&gt;% ggplot() + \n  geom_violin(aes(cut, price, \n                  color = cut, fill = cut),\n              draw_quantiles = c(0,.25,.50,.75),\n              alpha = 0.4) +\n  labs(title = \"Plot C: Price by Cut\")\n###\ndiamonds %&gt;% ggplot() + \n  geom_violin(aes(cut, \n                   price, \n                   color = cut, fill = cut), \n              draw_quantiles = c(0,.25,.50,.75),\n              alpha = 0.4)  +  \n  facet_wrap(vars(clarity)) +\n  labs(title = \"Plot D: Price by Cut facetted by Clarity\") +\n  theme(axis.text.x = element_text(angle = 45,hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nBusiness Insights from diamond Violin Plots\n\n\n\nThe distribution for price is clearly long-tailed (skewed). The distributions also vary considerably based on both cut and clarity. These Qual variables clearly have a large effect on the prices of individual diamonds.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📊 Distributions, Densities, Bar Plots, and Boxplots"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#case-study-2-race-dataset",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#case-study-2-race-dataset",
    "title": "📊 Distributions, Densities, Bar Plots, and Boxplots",
    "section": "\n Case Study-2: race dataset",
    "text": "Case Study-2: race dataset\n\n Import data\nThe data come from the TidyTuesday, project, a weekly social learning project dedicated to gaining practical experience with R and data science. In this case the TidyTuesday data are based on International Trail Running Association (ITRA) data but inspired by Benjamin Nowak. We will use the TidyTuesday data that are on GitHub. Nowak’s data are also available on GitHub.\n\n R\n\n\n\nrace_df &lt;- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-10-26/race.csv\")\nrank_df &lt;- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-10-26/ultra_rankings.csv\")\n\nThe data has automatically been read into the webr session, so you can continue on to the next code chunk!\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n Examine the race Data\nLet us look at the dataset using all our three methods:\n\n\n R\n web-r\n\n\n\n\nglimpse(race_df)\n\nRows: 1,207\nColumns: 13\n$ race_year_id   &lt;dbl&gt; 68140, 72496, 69855, 67856, 70469, 66887, 67851, 68241,…\n$ event          &lt;chr&gt; \"Peak District Ultras\", \"UTMB®\", \"Grand Raid des Pyréné…\n$ race           &lt;chr&gt; \"Millstone 100\", \"UTMB®\", \"Ultra Tour 160\", \"PERSENK UL…\n$ city           &lt;chr&gt; \"Castleton\", \"Chamonix\", \"vielle-Aure\", \"Asenovgrad\", \"…\n$ country        &lt;chr&gt; \"United Kingdom\", \"France\", \"France\", \"Bulgaria\", \"Turk…\n$ date           &lt;date&gt; 2021-09-03, 2021-08-27, 2021-08-20, 2021-08-20, 2021-0…\n$ start_time     &lt;time&gt; 19:00:00, 17:00:00, 05:00:00, 18:00:00, 18:00:00, 17:0…\n$ participation  &lt;chr&gt; \"solo\", \"Solo\", \"solo\", \"solo\", \"solo\", \"solo\", \"solo\",…\n$ distance       &lt;dbl&gt; 166.9, 170.7, 167.0, 164.0, 159.9, 159.9, 163.8, 163.9,…\n$ elevation_gain &lt;dbl&gt; 4520, 9930, 9980, 7490, 100, 9850, 5460, 4630, 6410, 31…\n$ elevation_loss &lt;dbl&gt; -4520, -9930, -9980, -7500, -100, -9850, -5460, -4660, …\n$ aid_stations   &lt;dbl&gt; 10, 11, 13, 13, 12, 15, 5, 8, 13, 23, 13, 5, 12, 15, 0,…\n$ participants   &lt;dbl&gt; 150, 2300, 600, 150, 0, 300, 0, 200, 120, 100, 300, 50,…\n\nglimpse(rank_df)\n\nRows: 137,803\nColumns: 8\n$ race_year_id    &lt;dbl&gt; 68140, 68140, 68140, 68140, 68140, 68140, 68140, 68140…\n$ rank            &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, NA, NA, NA,…\n$ runner          &lt;chr&gt; \"VERHEUL Jasper\", \"MOULDING JON\", \"RICHARDSON Phill\", …\n$ time            &lt;chr&gt; \"26H 35M 25S\", \"27H 0M 29S\", \"28H 49M 7S\", \"30H 53M 37…\n$ age             &lt;dbl&gt; 30, 43, 38, 55, 48, 31, 55, 40, 47, 29, 48, 47, 52, 49…\n$ gender          &lt;chr&gt; \"M\", \"M\", \"M\", \"W\", \"W\", \"M\", \"W\", \"W\", \"M\", \"M\", \"M\",…\n$ nationality     &lt;chr&gt; \"GBR\", \"GBR\", \"GBR\", \"GBR\", \"GBR\", \"GBR\", \"GBR\", \"GBR\"…\n$ time_in_seconds &lt;dbl&gt; 95725, 97229, 103747, 111217, 117981, 118000, 120601, …\n\n\n\nskim(race_df)\n\n\nData summary\n\n\nName\nrace_df\n\n\nNumber of rows\n1207\n\n\nNumber of columns\n13\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n5\n\n\nDate\n1\n\n\ndifftime\n1\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nevent\n0\n1.00\n4\n57\n0\n435\n0\n\n\nrace\n0\n1.00\n3\n63\n0\n371\n0\n\n\ncity\n172\n0.86\n2\n30\n0\n308\n0\n\n\ncountry\n4\n1.00\n4\n17\n0\n60\n0\n\n\nparticipation\n0\n1.00\n4\n5\n0\n4\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\ndate\n0\n1\n2012-01-14\n2021-09-03\n2017-09-30\n711\n\n\nVariable type: difftime\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\nstart_time\n0\n1\n0 secs\n82800 secs\n05:00:00\n39\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nrace_year_id\n0\n1\n27889.65\n20689.90\n2320\n9813.5\n23565.0\n42686.00\n72496.0\n▇▃▃▂▂\n\n\ndistance\n0\n1\n152.62\n39.88\n0\n160.1\n161.5\n165.15\n179.1\n▁▁▁▁▇\n\n\nelevation_gain\n0\n1\n5294.79\n2872.29\n0\n3210.0\n5420.0\n7145.00\n14430.0\n▅▇▇▂▁\n\n\nelevation_loss\n0\n1\n-5317.01\n2899.12\n-14440\n-7206.5\n-5420.0\n-3220.00\n0.0\n▁▂▇▇▅\n\n\naid_stations\n0\n1\n8.63\n7.63\n0\n0.0\n9.0\n14.00\n56.0\n▇▆▁▁▁\n\n\nparticipants\n0\n1\n120.49\n281.83\n0\n0.0\n21.0\n150.00\n2900.0\n▇▁▁▁▁\n\n\n\n\n\n\nskim(rank_df)\n\n\nData summary\n\n\nName\nrank_df\n\n\nNumber of rows\n137803\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nrunner\n0\n1.00\n3\n52\n0\n73629\n0\n\n\ntime\n17791\n0.87\n8\n11\n0\n72840\n0\n\n\ngender\n30\n1.00\n1\n1\n0\n2\n0\n\n\nnationality\n0\n1.00\n3\n3\n0\n133\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nrace_year_id\n0\n1.00\n26678.70\n20156.18\n2320\n8670\n21795\n40621\n72496\n▇▃▃▂▂\n\n\nrank\n17791\n0.87\n253.56\n390.80\n1\n31\n87\n235\n1962\n▇▁▁▁▁\n\n\nage\n0\n1.00\n46.25\n10.11\n0\n40\n46\n53\n133\n▁▇▂▁▁\n\n\ntime_in_seconds\n17791\n0.87\n122358.26\n37234.38\n3600\n96566\n114167\n148020\n296806\n▁▇▆▁▁\n\n\n\n\n\n\n# inspect(race_df) # does not work with hms and difftime variables\ninspect(rank_df)\n\n\ncategorical variables:  \n         name     class levels      n missing\n1      runner character  73629 137803       0\n2        time character  72840 120012   17791\n3      gender character      2 137773      30\n4 nationality character    133 137803       0\n                                   distribution\n1 SMITH Mike (0%) ...                          \n2 48H 12M 58S (0%) ...                         \n3 M (84.7%), W (15.3%)                         \n4 USA (34.3%), FRA (21%), GBR (8%) ...         \n\nquantitative variables:  \n             name   class  min    Q1 median     Q3    max         mean\n1    race_year_id numeric 2320  8670  21795  40621  72496  26678.70126\n2            rank numeric    1    31     87    235   1962    253.55546\n3             age numeric    0    40     46     53    133     46.24826\n4 time_in_seconds numeric 3600 96566 114167 148020 296806 122358.25765\n           sd      n missing\n1 20156.17976 137803       0\n2   390.79821 120012   17791\n3    10.11424 137803       0\n4 37234.37552 120012   17791\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nBusiness Insights from race data\n\n\n\n\nWe have two datasets, one for races (race_df) and one for the ranking of athletes (rank_df).\nThere is atleast one common column between the two, the race_year_id variable.\nOverall, there are Qualitative variables such as country, city,gender, and participation. This last variables seems badly coded, with entries showing solo and Solo.\n\nQuantitative variables are rank, time,time_in_seconds, age from rank_df; and distance, elevation_gain, elevation_loss,particants, and aid_stations from race_df.\nWe have 1207 races and over 130K participants! But some races do show zero participants!! Is that an error in data entry?\n\n\n\n\n EDA with race datasets\n\n\n\n\n\n\nQuestion #1\n\n\n\nWhich countries host the maximum number of races? Which countries send the maximum number of participants??\n\n\n R\n web-r\n\n\n\n\nrace_df %&gt;% count(country) %&gt;% arrange(desc(n))\n\n\n  \n\n\nrank_df %&gt;% count(nationality) %&gt;% arrange(desc(n))\n\n\n  \n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nThe top three locations for races were the USA, UK, and France. These are also the countries that send the maximum number of participants, naturally!\n\n\n\n\n\n\n\n\nQuestion #2\n\n\n\nWhich countries have the maximum number of winners (top 3 ranks)?\n\n\n R\n web-r\n\n\n\n\nrank_df %&gt;% \n  filter(rank %in% c(1,2,3)) %&gt;%\n  count(nationality) %&gt;% arrange(desc(n))\n\n\n  \n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n1240 Participants from the USA have been top 3 finishers. Across all races…\n\n\n\n\n\n\n\n\nQuestion #3\n\n\n\nWhich countries have had the most top-3 finishes in the longest distance race?\nHere we see we have ranks in one dataset, and race details in another! How do we do this now? We have to join the two data frames into one data frame, using a common variable that uniquely identifies observations in both datasets.\n\n\n R\n web-r\n\n\n\nlongest_races &lt;- race_df %&gt;%\n  slice_max(n = 5, order_by = distance) # Longest distance races\nlongest_races\nlongest_races %&gt;%\n  left_join(., rank_df, by  = \"race_year_id\") %&gt;% # total participants in longest 4 races\n  filter(rank %in% c(1:10)) %&gt;% # Top 10 ranks\n  count(nationality) %&gt;% arrange(desc(n))\n\n\n\n\n  \n\n\n\n\n\n\n  \n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nWow….France has one the top 10 positions 26 times in the longest races… which take place in France, Thailand, Chad, Australia, and Portugal. So although the USA has the greatest number of top 10 finishes, when it comes to the longest races, it is 🇫🇷 vive la France!\n\n\n\n\n\n\n\n\nQuestion #4\n\n\n\nWhat is the distribution of the finishing times, across all races and all ranks?\n\n\n R\n web-r\n\n\n\n\n## Set graph theme\ntheme_set(new = theme_custom())\n##\n\nrank_df %&gt;%\n  gf_histogram(~ time_in_seconds, bins = 75) %&gt;%\n  gf_labs(title = \"Histogram of Race Times\")\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nSo the distribution is (very) roughly bell-shaped, spread over a 2X range. And some people may have dropped out of the race very early and hence we have a small bump close to zero time! The histogram shows three bumps…at least one reason is that the distances to be covered are not the same…but could there be other reasons? Like altitude_gained for example?\n\n\n\n\n\n\n\n\nQuestion #5\n\n\n\nWhat is the distribution of race distances?\n\n\n R\n web-r\n\n\n\n\n## Set graph theme\ntheme_set(new = theme_custom())\n##\n\nrace_df %&gt;%\n  gf_histogram(~ distance, bins =  50) %&gt;%\n  gf_labs(title = \"Histogram of Race Distances\")\n\n\n\n\n\n\n\nHmm…a closely clumped set of race distances, with some entries in between [0-150], but some are zero? Which are these?\n\nrace_df %&gt;%\n  filter(distance == 0)\n\n\n  \n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nHmm…a closely clumped set of race distances, with some entries in between [0-150], but some are zero? Which are these?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nCurious…some of these zero-distance races have had participants too! Perhaps these were cancelled events…all of them are stated to be 100 mile events…\n\n\n\n\n\n\n\n\nQuestion #6\n\n\n\nFor all races that have a distance around 150, what is the distribution of finishing times? Can these be split/facetted using start_time of the race (i.e. morning / evening) ?\n\n\n R\n web-r\n\n\n\nLet’s make a count of start times:\n\nrace_times &lt;- race_df %&gt;%\n  count(start_time) %&gt;% arrange(desc(n))\nrace_times\n\n\n  \n\n\n\nLet’s convert start_time into a factor with levels: early_morning(0200:0600), late_morning(0600:1000), midday(1000:1400), afternoon(1400: 1800), evening(1800:2200), and night(2200:0200)\n\n# Demo purposes only!\n\n## Set graph theme\ntheme_set(new = theme_custom())\n##\n\nrace_start_factor &lt;- race_df %&gt;%\n  mutate(\n    start_day_time =\n      case_when(\n        start_time &gt; hms(\"02:00:00\") &\n          start_time &lt;= hms(\"06:00:00\") ~ \"early_morning\",\n        \n        start_time &gt; hms(\"06:00:01\") &\n          start_time &lt;= hms(\"10:00:00\") ~ \"late_morning\",\n        \n        start_time &gt; hms(\"10:00:01\") &\n          start_time &lt;= hms(\"14:00:00\") ~ \"mid_day\",\n        \n        start_time &gt; hms(\"14:00:01\") &\n          start_time &lt;= hms(\"18:00:00\") ~ \"afternoon\",\n        \n        start_time &gt; hms(\"18:00:01\") &\n          start_time &lt;= hms(\"22:00:00\") ~ \"evening\",\n        \n        start_time &gt; hms(\"22:00:01\") &\n          start_time &lt;= hms(\"23:59:59\") ~ \"night\",\n        \n        start_time &gt;= hms(\"00:00:00\") &\n          start_time &lt;= hms(\"02:00:00\") ~ \"postmidnight\",\n        \n        .default =  \"other\"\n      )\n  ) %&gt;%\n  mutate(start_day_time = \n           as_factor(start_day_time) %&gt;%\n           fct_collapse(.f = ., \n               night = c(\"night\", \"postmidnight\")))\n##\n# Join with rank_df\nrace_start_factor %&gt;%\n  left_join(rank_df, by = \"race_year_id\") %&gt;%\n  drop_na(time_in_seconds) %&gt;%\n  gf_histogram(\n    ~ time_in_seconds,\n    bins = 75,\n    fill = ~ start_day_time,\n    color = ~ start_day_time,\n    alpha = 0.5\n  ) %&gt;%\n  gf_facet_wrap(vars(start_day_time), ncol = 2, scales = \"free_y\") %&gt;%\n  gf_labs(title = \"Race Times by Start-Time\")\n\n\n\n\n\n\n\n\n\nLet’s make a count of start times:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nLet’s convert start_time into a factor with levels: early_morning(0200:0600), late_morning(0600:1000), midday(1000:1400), afternoon(1400: 1800), evening(1800:2200), and night(2200:0200)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nWe see that finish times tend to be longer for afternoon and evening start races; these are lower for early morning and night time starts. Mid-day starts show a curious double hump in finish times that should be studied.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📊 Distributions, Densities, Bar Plots, and Boxplots"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#distributions-and-densities-in-the-wild",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#distributions-and-densities-in-the-wild",
    "title": "📊 Distributions, Densities, Bar Plots, and Boxplots",
    "section": "\n Distributions and Densities in the Wild",
    "text": "Distributions and Densities in the Wild\nBefore we conclude, let us look at a real world dataset: populations of countries. This dataset was taken from Kaggle https://www.kaggle.com/datasets/ulrikthygepedersen/populations. Click on the icon below to save the file into a subfolder called data in your project folder:\n Download the Populations data \npop &lt;- read_csv(\"data/populations.csv\")\npop\ninspect(pop)\n\n\n\n\n  \n\n\n\n\n\n\ncategorical variables:  \n          name     class levels     n missing\n1 country_code character    265 16400       0\n2 country_name character    265 16400       0\n                                   distribution\n1 ABW (0.4%), AFE (0.4%), AFG (0.4%) ...       \n2 Afghanistan (0.4%) ...                       \n\nquantitative variables:  \n   name   class  min       Q1  median       Q3        max         mean\n1  year numeric 1960   1975.0    1991     2006       2021 1.990529e+03\n2 value numeric 2646 986302.5 6731400 46024452 7888408686 2.140804e+08\n            sd     n missing\n1 1.789551e+01 16400       0\n2 7.040554e+08 16400       0\n\n\n\nLet us plot densities/histograms for value:\n## Set graph theme\ntheme_set(new = theme_custom())\n##\n\n##\ngf_histogram(~ value, data = pop, title = \"Long Tailed Histogram\") \n##\ngf_density(~ value, data = pop, title = \"Long Tailed Density\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese graphs convey very little to us: the data is very heavily skewed to the right and much of the chart is empty. There are many countries with small populations and a few countries with very large populations. Such distributions are also called “long tailed” distributions. To develop better insights with this data, we should transform the variable concerned, using say a “log” transformation:\n## Set graph theme\ntheme_set(new = theme_custom())\n##\n\ngf_histogram(~ log10(value), data = pop, title = \"Histogram with Log transformed x-variable\") \n##\ngf_density(~ log10(value), data = pop, title = \"Density with Log transformed x-variable\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBe prepared to transform your data with log or sqrt transformations when you see skewed distributions! Salaries, Instagram connections, number of customers vs Companies, net worth / valuation of Companies, extreme events on stock markets….all of these could have highly skewed distributions. In such a case, the standard statistics of mean/median/sd may not convey too much information.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📊 Distributions, Densities, Bar Plots, and Boxplots"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#z-scores",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#z-scores",
    "title": "📊 Distributions, Densities, Bar Plots, and Boxplots",
    "section": "\n Z-scores",
    "text": "Z-scores\nOften when we compute wish to compare distributions with different values for means and standard deviations, we resort to a scaling of the variables that are plotted in the respective distributions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlthough the densities all look the same, they are are quite different! The x-axis in each case has two scales: one is the actual value of the x-variable, and the other is the z-score which is calculated as:\n\\[\nz_x = \\frac{x - \\mu_{x}}{\\sigma_x}\n\\]\nWith similar distributions (i.e. normal distributions), we see that the variation in density is the same at the same values of z-score for each variable. However since the \\(\\mu_i\\) and \\(\\sigma_i\\) are different, the absolute value of the z-score is different for each variable. In the first plot (from the top left), \\(z = 1\\) corresponds to an absolute change of \\(5\\) units; it is \\(15\\) units in the plot directly below it.\nOur comparisons are done easily when we compare differences in probabilities at identical z-scores, or differences in z-scores at identical probabilities.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📊 Distributions, Densities, Bar Plots, and Boxplots"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#conclusion",
    "title": "📊 Distributions, Densities, Bar Plots, and Boxplots",
    "section": "\n Conclusion",
    "text": "Conclusion\n\nHistograms, Frequency Distributions, and Box Plots are used for Quantitative data variables\nHistograms “dwell upon” counts, ranges, means and standard deviations\n\nFrequency Density plots “dwell upon” probabilities and densities\n\nBox Plots “dwell upon” medians and Quartiles\n\nQualitative data variables can be plotted as counts, using Bar Charts, or using Heat Maps\nViolin Plots help us to visualize multiple distributions at the same time, as when we split a Quant variable wrt to the levels of a Qual variable.\nRidge Plots are density plots used for describing one Quant and one Qual variable (by inherent splitting)\nWe can split all these plots on the basis of another Qualitative variable.(Ridge Plots are already split)\nLong tailed distributions need care in visualization and in inference making!",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📊 Distributions, Densities, Bar Plots, and Boxplots"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#your-turn",
    "title": "📊 Distributions, Densities, Bar Plots, and Boxplots",
    "section": "\n Your Turn",
    "text": "Your Turn\n  Datasets\n\nClick on the Dataset Icon above, and unzip that archive. Try to make distribution plots with each of the three tools.\nA dataset from calmcode.io https://calmcode.io/datasets.html\n\nOld Faithful Data in R (Find it!)\n\ninspect the dataset in each case and develop a set of Questions, that can be answered by appropriate stat measures, or by using a chart to show the distribution.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📊 Distributions, Densities, Bar Plots, and Boxplots"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/20-Distributions/intro-dist.html#references",
    "title": "📊 Distributions, Densities, Bar Plots, and Boxplots",
    "section": "\n References",
    "text": "References\n\nSee the scrolly animation for a histogram at this website: Exploring Histograms, an essay by Aran Lunzer and Amelia McNamara https://tinlizzie.org/histograms/?s=09\n\nMinimal R using mosaic.https://cran.r-project.org/web/packages/mosaic/vignettes/MinimalRgg.pdf\n\nSebastian Sauer, Plotting multiple plots using purrr::map and ggplot \n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggridges\n0.5.6\nWilke (2024)\n\n\nNHANES\n2.1.0\nPruim (2015)\n\n\nTeachHist\n0.2.1\nLange (2023)\n\n\nTeachingDemos\n2.13\nSnow (2024)\n\n\nvisualize\n4.5.0\nBalamuta (2023)\n\n\n\n\n\n\nBalamuta, James. 2023. visualize: Graph Probability Distributions with User Supplied Parameters and Statistics. https://CRAN.R-project.org/package=visualize.\n\n\nLange, Carsten. 2023. TeachHist: A Collection of Amended Histograms Designed for Teaching Statistics. https://CRAN.R-project.org/package=TeachHist.\n\n\nPruim, Randall. 2015. NHANES: Data from the US National Health and Nutrition Examination Study. https://CRAN.R-project.org/package=NHANES.\n\n\nSnow, Greg. 2024. TeachingDemos: Demonstrations for Teaching and Learning. https://CRAN.R-project.org/package=TeachingDemos.\n\n\nWilke, Claus O. 2024. ggridges: Ridgeline Plots in “ggplot2”. https://CRAN.R-project.org/package=ggridges.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📊 Distributions, Densities, Bar Plots, and Boxplots"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/evol-flow.html",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/evol-flow.html",
    "title": "🕸 Change, Evolution, and Flow",
    "section": "",
    "text": "R Tutorial"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/evol-flow.html#slides-and-tutorials",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/evol-flow.html#slides-and-tutorials",
    "title": "🕸 Change, Evolution, and Flow",
    "section": "",
    "text": "R Tutorial"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/evol-flow.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/evol-flow.html#setting-up-r-packages",
    "title": "🕸 Change, Evolution, and Flow",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(tidyverse)\nlibrary(ggstream)\nlibrary(ggformula)\n\n# remotes::install_github(\"corybrunson/ggalluvial@main\", build_vignettes = TRUE)\nlibrary(ggalluvial)\nlibrary(ggsankeyfier)\n# install.packages(\"devtools\")\n# devtools::install_github(\"davidsjoberg/ggsankey\")\nlibrary(ggsankey)\nlibrary(networkD3)\nlibrary(echarts4r) # Interactive graphs"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/evol-flow.html#what-time-evolution-charts-can-we-plot",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/evol-flow.html#what-time-evolution-charts-can-we-plot",
    "title": "🕸 Change, Evolution, and Flow",
    "section": "\n What Time Evolution Charts can we plot?",
    "text": "What Time Evolution Charts can we plot?\nIn these cases, the x-axis is typically time…and we chart the variable of another Quant variable with respect to time, using a line geometry.\nLet is take a healthcare budget dataset from Our World in Data: We will plot graphs for 5 countries (India, China, Brazil, Russia, Canada ).\n\n\n\n\n\n\nAnd Introducting echarts4r\n\n\n\nWe will also build interactive versions of these charts using echarts4r!\n\n\nDownload this data by clicking on the button below:\n Download the Health data \n\nhealth &lt;-\n  read_csv(\"data/public-health-expenditure-share-GDP-OWID.csv\")\n\nhealth_filtered &lt;- health %&gt;%\n  filter(Entity %in% c(\n    \"India\",\n    \"China\",\n    \"United States\",\n    \"United Kingdom\",\n    \"Russia\",\n    \"Sweden\"\n  ))\n\n\n\nUsing ggformula\nUsing echarts4r\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\ngf_point(data = health_filtered,\n             public_health_expenditure_pc_gdp ~ Year, \n             colour = ~ Entity, \n             ylab = \"Healthcare Budget\\n as % of GDP\",\n         title = \"Line Charts to show Evolution (over Time )\") %&gt;% \n  gf_line() \n###\ngf_area(data = health_filtered,\n          public_health_expenditure_pc_gdp ~ Year, \n          fill = ~ Entity, alpha = 0.3, \n          ylab = \"Healthcare Budget\\n as % of GDP\",\n        title = \"Area Charts to show Evolution (over Time )\") %&gt;% \n  gf_line(colour = ~ Entity) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhealth_filtered %&gt;% group_by(Entity) %&gt;% \n  e_charts(Year) %&gt;% \n  e_scatter(public_health_expenditure_pc_gdp) %&gt;% \n  e_line(public_health_expenditure_pc_gdp) %&gt;% \n  e_x_axis(name = \"Year\", min = 1850, max = 2050) %&gt;% \n  e_y_axis(name = \"Public Health Expenditure\", \n           nameLocation = \"middle\", nameGap = 25) %&gt;% \n  e_tooltip()\n###\nhealth_filtered %&gt;% group_by(Entity) %&gt;% \n  e_charts(Year) %&gt;% \n  e_scatter(public_health_expenditure_pc_gdp) %&gt;% \n  e_area(public_health_expenditure_pc_gdp) %&gt;% \n  e_x_axis(name = \"Year\", min = 1850, max = 2050) %&gt;% \n  e_y_axis(name = \"Public Health Expenditure\", \n           nameLocation = \"middle\",nameGap = 25) %&gt;% \n  e_tooltip()"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/evol-flow.html#what-space-evolution-charts-can-we-plot",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/evol-flow.html#what-space-evolution-charts-can-we-plot",
    "title": "🕸 Change, Evolution, and Flow",
    "section": "\n What Space Evolution Charts can we plot?",
    "text": "What Space Evolution Charts can we plot?\nHere, the space can be any Qual variable, and we can chart another Quant or Qual variable move across levels of the first chosen Qual variable.\nFor instance we can contemplate enrollment at a University, and show how students move from course to course in a University. Or how customers drift from one category of products or brands to another….or the movement of cricket players from one IPL Team to another !!\nHere is what Thomas Lin Pedersen says:\n\nA parallel sets diagram is a type of visualisation showing the interaction between multiple categorical variables. If the variables have an intrinsic order the representation can be thought of as a Sankey Diagram. If each variable is a point in time it will resemble an Alluvial diagram.\n\n\n\n\n\n\nThe Qualitative variables being connected are mapped to stages/axes\n\nEach level within a Qual variable is mapped to nodes / strata / lodes;\nAnd the connections between the strata of the axes are called flows / links / alluvia.\n\n\nSuch diagrams are best used when you want to show a many-to-many mapping between two domains or multiple paths through a set of stages E.g Students going through multiple courses during a semester of study.\nHere is an example of a Sankey Diagram: This diagram show how energy is converted or transmitted before being consumed or lost: supplies are on the left, and demands are on the right. (Data: Department of Energy & Climate Change via Tom Counsell)1:\n\n\n\n\n\n\n\n\nSwitching to ggplot here\n\n\n\nFor the next few charts, there are (as yet) no equivalents in ggformula. Hence we will use ggplot.\n\n\n\n Case Study-1: Titanic Dataset\n# library(ggalluvial)\ndata(\"Titanic\")\nTitanic &lt;- Titanic %&gt;% as_tibble()\nTitanic\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nTable Form Data\n\n\n\nNote that this data is in tidy wide / table form, with separate columns for each Qualitative variable and a separate count column, which we saw when we examined Categorical Data. This is, in my opinion, intuitively the best form of data to plot a Sankey plot with. But there are other forms such as the tidy long form which we have been using practically all this while. You will find examples of on the ggalluvial website using tidy long form data. https://corybrunson.github.io/ggalluvial/\n\n\n\n\nUsing ggplot\nUsing echarts4r\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\n##\nTitanic %&gt;% ggplot(data = .,\n    \n# Select the Categorical Variables for the vertical Axes / Stages\n       aes(axis1 = Class, \n           axis2 = Sex, \n           axis3 = Age,\n           axis4 = Survived,\n           y = n), fill = \"white\") +\n  \n# Alluvials between Categorical Axes\n  geom_alluvium(aes(fill = Survived), \n                colour = \"black\", \n                linewidth = 0.25) +\n  \n# Vertical segments for each Categorical Variable2 \n  geom_stratum(colour = \"black\", \n               linewidth = 1, \n               fill = \"white\") + \n  \n# Labels for each \"level\" of the Categorical Axes\n  geom_text(stat = \"stratum\", \n            aes(label = after_stat(stratum))) +\n  \n\n  \n# Scales and Colours\n  scale_x_discrete(limits = c(\"Class\", \"Sex\", \"Age\"), \n                   expand = c(.2, .05)) +\n  \n  scale_fill_manual(values = c(\"red3\", \"springgreen3\")) + \n  \n  xlab(\"Demographic\") +\n  ggtitle(\"Passengers on the maiden voyage of the Titanic\",\n          \"Stratified by demographics and survival\")\n\n\n\n\n\n\n\n\nHere is how the package ggalluvial defines the elements of a typical alluvial plot:\n\nAn axis is a dimension (variable) along which the data are vertically arranged at a fixed horizontal position. The plot above uses three categorical axes: Class, Sex, and Age.\nThe groups at each axis are depicted as opaque blocks called strata. For example, the Class axis contains four strata: 1st, 2nd, 3rd, and Crew.\nHorizontal (x-) splines called alluvia span the entire width of the plot. In this plot, each alluvium corresponds to a fixed strata value of each axis variable, indicated by its vertical position at the axis, as well as of the Survived variable, indicated by its fill color.\nThe segments of the alluvia between pairs of adjacent axes are flows.\nThe alluvia intersect the strata at lodes. The lodes are not visualized in the above plot, but they can be inferred as filled rectangles extending the flows through the strata at each end of the plot or connecting the flows on either side of the center stratum.\n\n\n\n\nLet us make an interactive graph for this dataset using echarts4.\n\nClassSex &lt;- \n  Titanic %&gt;% group_by(Class, Sex) %&gt;% \n  summarise(cs = sum(n)) %&gt;% \n  ungroup() %&gt;% \n  rename(\"source\" = Class, \"target\" = Sex, \"value\" = cs)\n            \nSexAge &lt;- \n  Titanic %&gt;% group_by(Sex,Age) %&gt;% \n  summarise(sa = sum(n)) %&gt;% \n  ungroup() %&gt;% \n  rename(\"source\" = Sex, \"target\" = Age, \"value\" = sa)\n\nAgeSurvived &lt;- \n  Titanic %&gt;% group_by(Age,Survived) %&gt;% \n  summarise(as = sum(n)) %&gt;% \n  ungroup() %&gt;% \n  rename(\"source\" = Age, \"target\" = Survived, \"value\" = as)\n\nCombo &lt;- rbind(ClassSex, SexAge, AgeSurvived)\nCombo\n\n\n  \n\n\nCombo %&gt;% e_charts() %&gt;% \n  e_sankey(source, target, value) %&gt;% \n  e_title(\"Titanic: Who lived, and who didn't?\") %&gt;% \n  e_tooltip()\n\n\n\n\n\nThe process with echarts4r is quite different, since the data structure used by this package is different:\n\nThe echarts4r package needs to have source and target columns for axes, along with a value to determine the width of the alluvium. \nThe names in the source and target can repeat, and can appear in both source and target columns in order to create a multi-axis diagram. Hence the data needs to be inherently in long form.\nHowever, for the values, we need to manually calculate the aggregate totals for alluvia between each consecutive pairs of axes (i.e Qual variables). This is not done automatically in echarts4r, but it is with ggalluvial.\nSo we create grouped aggregate summaries for each pair of Qualitative variables that we wish to plot consecutively ( i.e as axis1, axis2…)\nStack these pair-wise alluvia totals into one combo data frame using rbind(), after renaming the variables to “source”, “target” and “value”.\n\nPhew! seems like too much work to do…I wonder if good, old-fashioned pivot-longer will get us here…"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/evol-flow.html#chord-diagram",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/evol-flow.html#chord-diagram",
    "title": "🕸 Change, Evolution, and Flow",
    "section": "\n Chord Diagram",
    "text": "Chord Diagram\n\n\n\n\nWe will explore this diagram when we explore network graphs with the tidygraph and ggraph packages.\nUsing ggsankeyfier !!\nTo be Written Up."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/evol-flow.html#dumbbell-plots",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/evol-flow.html#dumbbell-plots",
    "title": "🕸 Change, Evolution, and Flow",
    "section": "\n Dumbbell Plots",
    "text": "Dumbbell Plots\nA simple plot that can quickly indicate changes in multiple variables/aspects over either a time or a space variable is a dumbbell plot. This is a combination of scatter plot + a segment plot. Let us take our previously loaded health dataset and plot just the change in expenditure for multiple countries, across a time span of 8 years (2010 - 2018)\n\n\nUsing ggformula\nUsing ggplot\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\n##\n\nhealth_2010_2018 &lt;- health %&gt;%\n  # select Years 2010 and 2018\n  filter(Year %in% c(2010, 2018)) %&gt;%\n\n  # Make separate columns for each year, easier that way\n  # Though not essential\n  pivot_wider(\n    id_cols = c(Entity, Code),\n    names_from = Year,\n    names_prefix = \"Year\",\n    values_from = public_health_expenditure_pc_gdp\n  )\n\nhealth_2010_2018 %&gt;%   \n  # remove NA data across the data set\n  drop_na() %&gt;%\n  \n  # take the top 20 countries based on 2018 allocation\n  slice_max(n = 20, order_by = Year2018) %&gt;% \n  \n  gf_segment(Entity + Entity ~ Year2010 + Year2018,\n             colour = \"grey\",\n             linewidth = 2) %&gt;%\n  gf_point(Entity ~ Year2018,\n           colour = ~ \"2018\") %&gt;%\n  gf_point(Entity ~ Year2010,\n           colour = ~ \"2010\")\n## Can we do better?\n\nhealth_2010_2018 %&gt;%\n    # remove NA data across the data set\n  drop_na() %&gt;%\n    \n# take the top 20 countries based on 2018 allocation\n  slice_max(n = 20, order_by = Year2018) %&gt;% \n  \n# plot segments first\n  gf_segment(\n    reorder(Entity, Year2018) + reorder(Entity, Year2018) ~\n      Year2010 + Year2018,\n    colour = \"grey\",\n    linewidth = 2\n  ) %&gt;%\n  \n# Then plot points\n  gf_point(reorder(Entity, Year2018) ~ Year2018,\n           colour = ~ \"2018\",\n           size = 3) %&gt;%\n  gf_point(\n    reorder(Entity, Year2018) ~ Year2010,\n    colour = ~ \"2010\", size = 3,\n    xlab = \"Health Expenditure as Percentage of GDP\",\n    ylab = \"Country\",\ntitle = \"Healthcare Budgets Changes between 2010 to 2018\",\nsubtitle = \"Bars are Sorted\"\n  ) %&gt;% \n  gf_refine(scale_x_continuous(\n    breaks = scales::breaks_width(2), \n    labels = scales::label_percent(suffix = \"%\", scale = 1)),\n    \n    scale_colour_manual(name = \"Year\",values = c(\"red\", \"green\"))\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\n\nhealth_2010_2018 &lt;- health %&gt;%\n  # select Years 2010 and 2018\n  filter(Year %in% c(2010, 2018)) %&gt;%\n\n  # Make separate columns for each year, easier that way\n  # Though not essential\n  pivot_wider(\n    id_cols = c(Entity, Code),\n    names_from = Year,\n    names_prefix = \"Year\",\n    values_from = public_health_expenditure_pc_gdp\n  )\n\nhealth_2010_2018 %&gt;%   \n  # remove NA data across the data set\n  drop_na() %&gt;%\n  \n  # take the top 20 countries based on 2018 allocation\n  slice_max(n = 20, order_by = Year2018) %&gt;% \n  \n  ggplot() + \n  geom_segment(aes(y = Entity, yend = Entity, \n                   x = Year2010, xend = Year2018),\n             colour = \"grey\",\n             linewidth = 2) + \n  \n  geom_point(aes(y = Entity, x = Year2018, colour = \"2018\")) + \n  geom_point(aes(y = Entity, x = Year2010, colour = \"2010\"))\n## Can we do better?\n\nhealth_2010_2018 %&gt;%\n    # remove NA data across the data set\n  drop_na() %&gt;%\n    \n# take the top 20 countries based on 2018 allocation\n  slice_max(n = 20, order_by = Year2018) %&gt;% \n  ggplot() + \n# plot segments first\n  geom_segment(\n    aes(y = reorder(Entity, Year2018), yend = reorder(Entity, Year2018),\n      x = Year2010, xend = Year2018),\n    colour = \"grey\",\n    linewidth = 2\n  ) + \n  \n# Then plot points\n  geom_point(aes(y = reorder(Entity, Year2018), x = Year2018, \n                 colour = \"2018\"),size = 3) + \n  geom_point(aes(y = reorder(Entity, Year2018), x = Year2010, \n                 colour = \"2010\"), size = 3) + \n    labs(x = \"Health Expenditure as Percentage of GDP\",\n    y = \"Country\",title = \"Healthcare Budgets\", \n    subtitle = \"Changes between 2010 to 2018\"\n  ) + \n  scale_x_continuous(\n    breaks = breaks_width(2), \n    labels = scales::label_percent(suffix = \"%\", scale = 1)) + \n    \n    scale_colour_manual(name = \"Year\",values = c(\"red\", \"green\"))"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/evol-flow.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/evol-flow.html#conclusion",
    "title": "🕸 Change, Evolution, and Flow",
    "section": "\n Conclusion",
    "text": "Conclusion\nWe see that we can visualize “evolutions” over time and space. The evolutions can represent changes in the quantities of things, or their categorical affiliations or groups.\nWhat business data would you depict in this way? Revenue streams? Employment? Expenditures over time and market? There are many possibilities!\nNote also that the Bump Charts are a special case of alluvial/sankey charts where each node connects/flows to one other node.\n\n\n\n\nlogsUserNetworkAPI ServerCell TowerData ProcessorOnline PortalsatellitestransmitterStorageUIphone logsMake callpersistdisplayaccess"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/evol-flow.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/evol-flow.html#your-turn",
    "title": "🕸 Change, Evolution, and Flow",
    "section": "Your Turn",
    "text": "Your Turn\n\nWithin the ggalluvial package are two datasets, majors and vaccinations. Plot alluvial charts for both of these.\nGo to the American Life Panel Website where you will find many public datasets. Try to take one and make charts from it that we have learned in this Module."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/evol-flow.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/evol-flow.html#references",
    "title": "🕸 Change, Evolution, and Flow",
    "section": "\n References",
    "text": "References\n\n\nGlobal Migration, https://download.gsb.bund.de/BIB/global_flow/ A good example of the use of a Chord Diagram.ggalluvial cheatsheet,https://cheatography.com/seleven/cheat-sheets/ggalluvial/\nJohn Coene, Sankey plots with echarts4r, https://echarts4r.john-coene.com/articles/chart_types.html#sankey\nOther packages: Sankey plot | the R Graph Gallery (r-graph-gallery.com)\nAnother package: Sankey diagrams in ggplot2 with ggsankey | RCHARTS (r-charts.com)\nSankey Charts using networkD3: http://christophergandrud.github.io/networkD3\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\necharts4r\n0.4.5\nCoene (2023)\n\n\nggalluvial\n0.12.5\n@\n\n\nggsankey\n0.0.99999\nSjoberg (2024)\n\n\nggsankeyfier\n0.1.8\nde Vries (2024)\n\n\nggstream\n0.1.0\nSjoberg (2021)\n\n\nnetworkD3\n0.4\nAllaire et al. (2017)\n\n\n\n\n\n\nAllaire, J. J., Christopher Gandrud, Kenton Russell, and CJ Yetman. 2017. networkD3: D3 JavaScript Network Graphs from r. https://CRAN.R-project.org/package=networkD3.\n\n\nCoene, John. 2023. Echarts4r: Create Interactive Graphs with “Echarts JavaScript” Version 5. https://CRAN.R-project.org/package=echarts4r.\n\n\nde Vries, Pepijn. 2024. ggsankeyfier: Create Sankey and Alluvial Diagrams Using “ggplot2”. https://CRAN.R-project.org/package=ggsankeyfier.\n\n\nSjoberg, David. 2021. ggstream: Create Streamplots in “ggplot2”. https://CRAN.R-project.org/package=ggstream.\n\n\n———. 2024. ggsankey: Sankey, Alluvial and Sankey Bump Plots. https://github.com/davidsjoberg/ggsankey."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/evol-flow.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/evol-flow.html#footnotes",
    "title": "🕸 Change, Evolution, and Flow",
    "section": "Footnotes",
    "text": "Footnotes\n\nD3 JavaScript Network Graphs from R: christophergandrud.github.io/networkD3/↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html",
    "href": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html",
    "title": "🐉 Visualizing Survey Data",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic) # Our trusted friend\nlibrary(skimr)\nlibrary(vcd) # Michael Friendly's package, Visualizing Categorical Data\nlibrary(vcdExtra) # Categorical Data Sets\nlibrary(ggmosaic) # Mosaic Plots\nlibrary(resampledata) # More datasets\n\nlibrary(sjPlot) # Likert Scale Plots\nlibrary(sjlabelled) # Creating Labelled Data for Likert Plots\n\nlibrary(ggpubr) # Colours, Themes and new geometries in ggplot\nlibrary(ca) # Correspondence Analysis, for use some day\n\n## Making Tables\nlibrary(kableExtra) # html styled tables",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🐉 Visualizing Survey Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#sec-setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#sec-setting-up-r-packages",
    "title": "🐉 Visualizing Survey Data",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic) # Our trusted friend\nlibrary(skimr)\nlibrary(vcd) # Michael Friendly's package, Visualizing Categorical Data\nlibrary(vcdExtra) # Categorical Data Sets\nlibrary(ggmosaic) # Mosaic Plots\nlibrary(resampledata) # More datasets\n\nlibrary(sjPlot) # Likert Scale Plots\nlibrary(sjlabelled) # Creating Labelled Data for Likert Plots\n\nlibrary(ggpubr) # Colours, Themes and new geometries in ggplot\nlibrary(ca) # Correspondence Analysis, for use some day\n\n## Making Tables\nlibrary(kableExtra) # html styled tables",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🐉 Visualizing Survey Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#introduction",
    "title": "🐉 Visualizing Survey Data",
    "section": " Introduction",
    "text": "Introduction\nIn many business situations, we perform say customer surveys to get Likert Scale data, where several respondents rate a product or a service on a scale of Very much like, somewhat like, neutral, Dislike and Very much dislike, for example.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🐉 Visualizing Survey Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#plots-for-survey-data",
    "href": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#plots-for-survey-data",
    "title": "🐉 Visualizing Survey Data",
    "section": " Plots for Survey Data",
    "text": "Plots for Survey Data\nHow does this data look like, and how does one plot it? Let us consider a fictitious example, followed by a real world dataset.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🐉 Visualizing Survey Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#case-study-1-a-fictitious-app-survey-dataset",
    "href": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#case-study-1-a-fictitious-app-survey-dataset",
    "title": "🐉 Visualizing Survey Data",
    "section": " Case Study-1: A fictitious app Survey dataset",
    "text": "Case Study-1: A fictitious app Survey dataset\n\n\n\n\n\n\nA fictitious QuickEZ app\n\n\n\nWe are a start-up that has an app called QuickEZ for delivery of groceries. We conduct a survey of 200 people at a local store, with the following questions,\n\n“Have your heard of the QuickEZ app?”\n“Do you use the QuickEZ app?”\n“Do you find it easy to use the QuickEZ app?”\n“Will you continue to use the QuickEZ app?”\n\nwhere each questions is to be answered on a scale of : “always”, “often”, “sometimes”,“never”.\n\n\nSuch data may look for example as follows:\n\n\n\n\n\n\n\n\nFirst 10 Responses\n\n\nq1\nq2\nq3\nq4\n\n\n\n\n4\n2\n4\n4\n\n\n4\n3\n4\n1\n\n\n3\n2\n1\n2\n\n\n2\n3\n4\n1\n\n\n1\n1\n3\n4\n\n\n1\n3\n4\n4\n\n\n2\n4\n2\n2\n\n\n3\n3\n4\n1\n\n\n1\n1\n4\n4\n\n\n2\n1\n4\n4\n\n\n\n\n\n\n\n\ntibble [200 × 4] (S3: tbl_df/tbl/data.frame)\n $ q1: int [1:200] 4 4 3 2 1 1 2 3 1 2 ...\n  ..- attr(*, \"label\")= Named chr \"Have your heard of the QuickEZ app?\"\n  .. ..- attr(*, \"names\")= chr \"q1\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"always\" \"often\" \"sometimes\" \"never\"\n $ q2: int [1:200] 2 3 2 3 1 3 4 3 1 1 ...\n  ..- attr(*, \"label\")= Named chr \"Do you use the QuickEZ app?\"\n  .. ..- attr(*, \"names\")= chr \"q2\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"always\" \"often\" \"sometimes\" \"never\"\n $ q3: int [1:200] 4 4 1 4 3 4 2 4 4 4 ...\n  ..- attr(*, \"label\")= Named chr \"Do you find it easy to use the QuickEZ app?\"\n  .. ..- attr(*, \"names\")= chr \"q3\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"always\" \"often\" \"sometimes\" \"never\"\n $ q4: int [1:200] 4 1 2 1 4 4 2 1 4 4 ...\n  ..- attr(*, \"label\")= Named chr \"Will you continue to use the QuickEZ app?\"\n  .. ..- attr(*, \"names\")= chr \"q4\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"always\" \"often\" \"sometimes\" \"never\"\n\n\n\nThe columns here correspond to the 4 questions (q1-q4) and the rows contain the 200 responses, which have been coded as (1:4). Such data is also a form of Categorical data and we need to count and plot counts for each of the survey questions. Such a plot is called a Likert plot and it looks like this:\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\nBased on this chart, since it looks like about half the survey respondents have not heard of our app, we need more publicity, and many do not find it easy to use 😿, so we have serious re-design and user testing to do !! But at least those who have managed to get past the hurdles are stating they will continue to use the app, so it does the job, but we can make it easier to use.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🐉 Visualizing Survey Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#case-study-2-eurofam-survey-dataset",
    "href": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#case-study-2-eurofam-survey-dataset",
    "title": "🐉 Visualizing Survey Data",
    "section": " Case Study-2: EUROFAM Survey dataset",
    "text": "Case Study-2: EUROFAM Survey dataset\nHere is another example of Likert data from the healthcare industry.\nefc is a German data set from a European study titled EUROFAM study, on family care of older people. Following a common protocol, data were collected from national samples of approximately 1,000 family carers (i.e. caregivers) per country and clustered into comparable subgroups to facilitate cross-national analysis. The research questions in this EUROFAM study were:\n\n\nTo what extent do family carers of older people use support services or receive financial allowances across Europe? What kind of supports and allowances do they mainly use?\nWhat are the main difficulties carers experience accessing the services used? What prevents carers from accessing unused supports that they need? What causes them to stop using still-needed services?\nIn order to improve support provision, what can be understood about the service characteristics considered crucial by carers, and how far are these needs met? and,\nWhich channels or actors can provide the greatest help in underpinning future policy efforts to improve access to services/supports?\n\n\nWe will select the variables from the efc data set that related to coping (on part of care-givers) and plot their responses after inspecting them:\n```{r}\n#| label: efc_data\n#| layout-nrow: 2\n#| column: body-outset-right\ndata(efc,package = \"sjPlot\")\n\nefc %&gt;% \n  select(dplyr::contains(\"cop\")) %&gt;% \n  head(20) \n##\nefc %&gt;% \n  select(dplyr::contains(\"cop\")) %&gt;% \n  str()\n```\n\n\n\n\n  \n\n\n\n\n\n'data.frame':   908 obs. of  9 variables:\n $ c82cop1: num  3 3 2 4 3 2 4 3 3 3 ...\n  ..- attr(*, \"label\")= chr \"do you feel you cope well as caregiver?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"never\" \"sometimes\" \"often\" \"always\"\n $ c83cop2: num  2 3 2 1 2 2 2 2 2 2 ...\n  ..- attr(*, \"label\")= chr \"do you find caregiving too demanding?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"Never\" \"Sometimes\" \"Often\" \"Always\"\n $ c84cop3: num  2 3 1 3 1 3 4 2 3 1 ...\n  ..- attr(*, \"label\")= chr \"does caregiving cause difficulties in your relationship with your friends?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"Never\" \"Sometimes\" \"Often\" \"Always\"\n $ c85cop4: num  2 3 4 1 2 3 1 1 2 2 ...\n  ..- attr(*, \"label\")= chr \"does caregiving have negative effect on your physical health?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"Never\" \"Sometimes\" \"Often\" \"Always\"\n $ c86cop5: num  1 4 1 1 2 3 1 1 2 1 ...\n  ..- attr(*, \"label\")= chr \"does caregiving cause difficulties in your relationship with your family?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"Never\" \"Sometimes\" \"Often\" \"Always\"\n $ c87cop6: num  1 1 1 1 2 2 2 1 1 1 ...\n  ..- attr(*, \"label\")= chr \"does caregiving cause financial difficulties?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"Never\" \"Sometimes\" \"Often\" \"Always\"\n $ c88cop7: num  2 3 1 1 1 2 4 2 3 1 ...\n  ..- attr(*, \"label\")= chr \"do you feel trapped in your role as caregiver?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"Never\" \"Sometimes\" \"Often\" \"Always\"\n $ c89cop8: num  3 2 4 2 4 1 1 3 1 1 ...\n  ..- attr(*, \"label\")= chr \"do you feel supported by friends/neighbours?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"never\" \"sometimes\" \"often\" \"always\"\n $ c90cop9: num  3 2 3 4 4 1 4 3 3 3 ...\n  ..- attr(*, \"label\")= chr \"do you feel caregiving worthwhile?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"never\" \"sometimes\" \"often\" \"always\"\n\n\n\nThe coping related variables have responses on the Likert Scale (1,2,3,4) which correspond to (never, sometimes, often, always), and each variable also has a label defining each variable. The labels are actually ( and perhaps usually ) the questions in the survey.\nWe can plot this data using the plot_likert function from package sjPlot:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nefc %&gt;% \n  select(dplyr::contains(\"cop\")) %&gt;% \n  sjPlot::plot_likert(title = \"Caregiver Survey from EUROFAM\") \n\n\n\n\n\n\n\n\nMany questions here have strong negative responses. This may indicate that policy and publicity related efforts may be required.\n\n\n\n\n\n\nColours and Orientation in the Likert Plot\n\n\n\nOne could prefer (as I do) that “often” and “always” scores should be toward the right and “sometimes” and “never” scores towards the left. One can do this within the plot_likert command using:\nplot_likert(..., reverse.scale = TRUE)\nIf you want the colours to be reversed, then…\nplot_likert(..., reverse.colors = TRUE)\nTry these options now in your Console! (Note the American spelling color)",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🐉 Visualizing Survey Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#labelled-data",
    "href": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#labelled-data",
    "title": "🐉 Visualizing Survey Data",
    "section": " Labelled Data",
    "text": "Labelled Data\nNote how the y-axis has been populated with Survey Questions: this is an example of a labelled dataset, where not only do the variables have names i.e. column names, but also have longish text labels that add information to the data variables. The data values ( i.e scores) in the columns is also labelled as per the the Likert scale (Like/Dislike/Strongly Dislike OR never/sometimes/often/always) etc. These Likert scores are usually a set of contiguous integers.\n\n\n\n\n\n\nVariable Labels and Value Labels\n\n\n\nVariable label is human readable description of the variable. R supports rather long variable names and these names can contain even spaces and punctuation but short variables names make coding easier. Variable label can give a nice, long description of variable. With this description it is easier to remember what those variable names refer to.\n\nValue labels are similar to variable labels, but value labels are descriptions of the values a variable can take. Labeling values means we don’t have to remember if 1=Extremely poor and 7=Excellent or vice-versa. We can easily get dataset description and variables summary with info function.\n\n\nLet us manually create one such dataset, since this is a common-enough situation1 that we have survey data and then have to label the variables and the values before plotting. We will use the R package sjlabelled to label our data.2.\nIt is also possible to label the tibble, the columns, and the values in similar fashion using the labelr package.3\n\n#library(sjlabelled)\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nvariable_labels &lt;- c(\"Do you practice Analytics?\",\n                     \"Do you code in R?\",\n                     \"Have you published your R Code?\",\n                     \"Do you use Quarto as your Workflow in R?\",\n                     \"Will you use R at Work?\")\nvalue_labels = c(\"never\", \"sometimes\",\"often\",\"always\") #numerically 1:4\n\nmy_survey_data &lt;- \n  # Create toy survey data\n  # 200 responses to 5 questions\n  # responses on Likert Scale\n  # 1:4 = \"never\", \"sometimes\",\"often\",\"always\")\n\n  tibble(q1 = mosaic::sample(1:4, replace = TRUE, size = 200,\n                             prob = c(0.2, 0.2, 0.5, 0.1)),\n         q2 = mosaic::sample(1:4, replace = TRUE, size = 200,\n                             prob = c(0.3, 0.3, 0.3, 0.1)),\n         q3 = mosaic::sample(1:4, replace = TRUE, size = 200,\n                             prob = c(0.2, 0.1, 0.1, 0.6)),\n         q4 = mosaic::sample(1:4, replace = TRUE, size = 200,\n                             prob = c(0.4, 0.2, 0.1, 0.3)),\n         q5 = mosaic::sample(1:4, replace = TRUE, size = 200,\n                             prob = c(0.1, 0.2, 0.5, 0.2))) %&gt;%\n  \n  # Set VARIABLE labels\n  sjlabelled::set_label(x = .,\n                        label = variable_labels) %&gt;%\n  \n  # Now set VALUE labels\n  sjlabelled::set_labels(x = ., labels = value_labels)\n###\nhead(my_survey_data, 6)\n\n\n  \n\n\n###\nstr(my_survey_data)\n\ntibble [200 × 5] (S3: tbl_df/tbl/data.frame)\n $ q1: int [1:200] 1 3 1 3 3 1 3 4 3 1 ...\n  ..- attr(*, \"label\")= Named chr \"Do you practice Analytics?\"\n  .. ..- attr(*, \"names\")= chr \"q1\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"never\" \"sometimes\" \"often\" \"always\"\n $ q2: int [1:200] 3 1 3 2 1 2 3 3 1 1 ...\n  ..- attr(*, \"label\")= Named chr \"Do you code in R?\"\n  .. ..- attr(*, \"names\")= chr \"q2\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"never\" \"sometimes\" \"often\" \"always\"\n $ q3: int [1:200] 3 2 1 4 4 1 2 2 4 4 ...\n  ..- attr(*, \"label\")= Named chr \"Have you published your R Code?\"\n  .. ..- attr(*, \"names\")= chr \"q3\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"never\" \"sometimes\" \"often\" \"always\"\n $ q4: int [1:200] 1 1 1 4 1 4 1 1 1 1 ...\n  ..- attr(*, \"label\")= Named chr \"Do you use Quarto as your Workflow in R?\"\n  .. ..- attr(*, \"names\")= chr \"q4\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"never\" \"sometimes\" \"often\" \"always\"\n $ q5: int [1:200] 2 4 3 1 3 2 3 3 3 1 ...\n  ..- attr(*, \"label\")= Named chr \"Will you use R at Work?\"\n  .. ..- attr(*, \"names\")= chr \"q5\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"never\" \"sometimes\" \"often\" \"always\"\n\nplot_likert(my_survey_data, \n            title = \"Summary of Analytics Questionnaire\",\n            reverse.scale = TRUE,# Reverse score values on plot\n            reverse.colors = FALSE, # let the colors be\n            show.prc.sign = TRUE, # Show percentage sign\n            legend.pos = \"bottom\")\n\n\n\n\n\n\n\n\nIt seems many people in the survey plan to use R at work!! And have published R code as well. But Quarto seems to have mixed results! But of course this is a toy dataset!!\nSo there we are with Survey data analysis and plots!\nThere are a few other plots with this type of data, which are useful in very specialized circumstances. One example of this is the agreement plot which captures the agreement between two (sets) of evaluators, on ratings given on a shared ordinal scale to a set of items. An example from the field of medical diagnosis is the opinions of two specialists on a common set of patients. However, that is for a more advanced course!",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🐉 Visualizing Survey Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#conclusion",
    "title": "🐉 Visualizing Survey Data",
    "section": " Conclusion",
    "text": "Conclusion\nHow are the Likert Plots for Survey data different from Bar Plots? Not very much inherently; we can view the Likert Charts as a set of stacked bar charts, based on Likert-scale response counts. At a pinch we can make a Likert Plot with vanilla bar graphs, but the elegance and power of the packages sjPlot and sjlabelled is undeniable.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🐉 Visualizing Survey Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#your-turn",
    "title": "🐉 Visualizing Survey Data",
    "section": " Your Turn",
    "text": "Your Turn\n\nTake some of the categorical datasets from the vcd and vcdExtra packages and recreate the plots from this module. Go to https://vincentarelbundock.github.io/Rdatasets/articles/data.html and type “vcd” in the search box. You can directly load CSV files from there, using read_csv(\"url-to-csv\").\nIncluding Edible Insects in our Diet!\n\n Download the Edible Insects Dataset \nThere are several questions here for each “area” of preference for edible insects: experience, fear, concern for the environment, etc. Take all the columns marked as average as your data for your Likert Plot.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🐉 Visualizing Survey Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#references",
    "title": "🐉 Visualizing Survey Data",
    "section": " References",
    "text": "References\n\nMine Cetinkaya-Rundel and Johanna Hardin. An Introduction to Modern Statistics, Chapter 4. https://openintro-ims.netlify.app/explore-categorical.html\n\nUsing the strcplot command from vcd, https://cran.r-project.org/web/packages/vcd/vignettes/strucplot.pdf\n\nCreating Frequency Tables with vcd, https://cran.r-project.org/web/packages/vcdExtra/vignettes/A_creating.html\n\nCreating mosaic plots with vcd, https://cran.r-project.org/web/packages/vcdExtra/vignettes/D_mosaics.html\n\nMichael Friendly, Corrgrams: Exploratory displays for correlation matrices. The American Statistician August 19, 2002 (v1.5). https://www.datavis.ca/papers/corrgram.pdf\n\nVisualizing Categorical Data in R\n\nH. Riedwyl & M. Schüpbach (1994), Parquet diagram to plot contingency tables. In F. Faulbaum (ed.), Softstat ’93: Advances in Statistical Software, 293–299. Gustav Fischer, New York.\n\n\n\n R Package Citations\n\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\n\nggmosaic\n0.3.3\nJeppson, Hofmann, and Cook (2021)\n\n\nggpubr\n0.6.0\nKassambara (2023)\n\n\njanitor\n2.2.0\nFirke (2023)\n\n\nkableExtra\n1.4.0\nZhu (2024)\n\n\nresampledata\n0.3.1\nChihara and Hesterberg (2018)\n\n\nsjlabelled\n1.2.0\nLüdecke (2022)\n\n\nsjPlot\n2.8.16\nLüdecke (2024)\n\n\nvcd\n1.4.12\nMeyer, Zeileis, and Hornik (2006); Zeileis, Meyer, and Hornik (2007); Meyer et al. (2023)\n\n\nvcdExtra\n0.8.5\nFriendly (2023)\n\n\n\n\n\n\nChihara, Laura M., and Tim C. Hesterberg. 2018. Mathematical Statistics with Resampling and r. 2nd ed. Hoboken, NJ: John Wiley & Sons. https://sites.google.com/site/chiharahesterberg/home.\n\n\nFirke, Sam. 2023. janitor: Simple Tools for Examining and Cleaning Dirty Data. https://CRAN.R-project.org/package=janitor.\n\n\nFriendly, Michael. 2023. vcdExtra: “vcd” Extensions and Additions. https://CRAN.R-project.org/package=vcdExtra.\n\n\nJeppson, Haley, Heike Hofmann, and Di Cook. 2021. ggmosaic: Mosaic Plots in the “ggplot2” Framework. https://CRAN.R-project.org/package=ggmosaic.\n\n\nKassambara, Alboukadel. 2023. ggpubr: “ggplot2” Based Publication Ready Plots. https://CRAN.R-project.org/package=ggpubr.\n\n\nLüdecke, Daniel. 2022. sjlabelled: Labelled Data Utility Functions (Version 1.2.0). https://doi.org/10.5281/zenodo.1249215.\n\n\n———. 2024. sjPlot: Data Visualization for Statistics in Social Science. https://CRAN.R-project.org/package=sjPlot.\n\n\nMeyer, David, Achim Zeileis, and Kurt Hornik. 2006. “The Strucplot Framework: Visualizing Multi-Way Contingency Tables with Vcd.” Journal of Statistical Software 17 (3): 1–48. https://doi.org/10.18637/jss.v017.i03.\n\n\nMeyer, David, Achim Zeileis, Kurt Hornik, and Michael Friendly. 2023. vcd: Visualizing Categorical Data. https://CRAN.R-project.org/package=vcd.\n\n\nZeileis, Achim, David Meyer, and Kurt Hornik. 2007. “Residual-Based Shadings for Visualizing (Conditional) Independence.” Journal of Computational and Graphical Statistics 16 (3): 507–25. https://doi.org/10.1198/106186007X237856.\n\n\nZhu, Hao. 2024. kableExtra: Construct Complex Table with “kable” and Pipe Syntax. https://CRAN.R-project.org/package=kableExtra.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🐉 Visualizing Survey Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#footnotes",
    "title": "🐉 Visualizing Survey Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPiping Hot Data: Leveraging Labelled Data in R, https://www.pipinghotdata.com/posts/2020-12-23-leveraging-labelled-data-in-r/&gt;↩︎\nLabel Support in R:https://cran.r-project.org/web/packages/sjlabelled/index.html↩︎\nUsing the labelr package: https://cran.r-project.org/web/packages/labelr/vignettes/labelr-introduction.html↩︎",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🐉 Visualizing Survey Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html",
    "title": "The Grammar of Networks",
    "section": "",
    "text": "# Fonts\n# Run these next few commands IN YOUR CONSOLE once. \n#install.packages(\"extrafontdb\")\n#library(extrafont)\n#extrafont::font_import(paths = NULL, recursive = TRUE, prompt = TRUE,pattern = NULL)\n\n########################################\n# For General Data Manipulation\nlibrary(tidyverse)\n\n########################################\n# Network Analysis Library (Handle data and Viz)\nlibrary(igraph)\n\n########################################\n# For Network \"Manipulation\"\nlibrary(tidygraph)\n\n# For Network Visualization\nlibrary(ggraph)\nlibrary(graphlayouts)\nlibrary(visNetwork)\n\n# For \"Network\" Datasets\nlibrary(igraphdata)\n\n# Fonts\nlibrary(ggtext) # Claus Wilke's package\nlibrary(showtext)\nlibrary(fontawesome)\n# For repeatable layouts, some can be random!!\nset.seed(12345)",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "The Grammar of Networks"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html#setting-up-r-packages",
    "title": "The Grammar of Networks",
    "section": "",
    "text": "# Fonts\n# Run these next few commands IN YOUR CONSOLE once. \n#install.packages(\"extrafontdb\")\n#library(extrafont)\n#extrafont::font_import(paths = NULL, recursive = TRUE, prompt = TRUE,pattern = NULL)\n\n########################################\n# For General Data Manipulation\nlibrary(tidyverse)\n\n########################################\n# Network Analysis Library (Handle data and Viz)\nlibrary(igraph)\n\n########################################\n# For Network \"Manipulation\"\nlibrary(tidygraph)\n\n# For Network Visualization\nlibrary(ggraph)\nlibrary(graphlayouts)\nlibrary(visNetwork)\n\n# For \"Network\" Datasets\nlibrary(igraphdata)\n\n# Fonts\nlibrary(ggtext) # Claus Wilke's package\nlibrary(showtext)\nlibrary(fontawesome)\n# For repeatable layouts, some can be random!!\nset.seed(12345)",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "The Grammar of Networks"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html#introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html#introduction",
    "title": "The Grammar of Networks",
    "section": "\n Introduction",
    "text": "Introduction\nNetwork graphs show relationships between entities: what sort they are, how strong they are, and even of they change over time.\nWe will examine data structures pertaining both to the entities and the relationships between them and look at the data object that can combine these aspects together. Then we will see how these are plotted, what the structure of the plot looks like. There are also metrics that we can calculate for the network, based on its structure. We will of course examine geometric metaphors that can represent various classes of entities and their relationships.\nNetwork graphs can be rendered both as static and interactive and we will examine R packages that render both kinds of plots.\nThere is a another kind of structure: one that combines spatial and network data in one. We will defer that for a future module !",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "The Grammar of Networks"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html#what-kind-network-graphs-will-we-make",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html#what-kind-network-graphs-will-we-make",
    "title": "The Grammar of Networks",
    "section": "What kind Network graphs will we make?",
    "text": "What kind Network graphs will we make?\nHere is a network map of the characters in Victor Hugo’s Les Miserables:\n\n\nAnd this: the well known Zachary’s Karate Club dataset visualized as a network",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "The Grammar of Networks"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html#goals",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html#goals",
    "title": "The Grammar of Networks",
    "section": "\n Goals",
    "text": "Goals\nAt the end of this Lab session, we should:\n\nknow the types and structures of network data and be able to work with them\nunderstand the basics of modern network packages in R\nbe able to create network visualizations using tidygraph, ggraph( static visualizations ) and visNetwork (interactive visualizations)\nsee directions for how the network metaphor applies in a variety of domains (e.g. biology/ecology, ideas/influence, technology, transportation, to name a few)",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "The Grammar of Networks"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html#pedagogical-note",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html#pedagogical-note",
    "title": "The Grammar of Networks",
    "section": "\n Pedagogical Note",
    "text": "Pedagogical Note\nThe method followed will be based on PRIMM:\n\n\nPREDICT Inspect the code and guess at what the code might do, write predictions\n\n\nRUN the code provided and check what happens\n\nINFER what the parameters of the code do and write comments to explain. What bells and whistles can you see?\n\nMODIFY the parameters code provided to understand the options available. Write comments to show what you have aimed for and achieved.\n\nMAKE : take an idea/concept of your own, and graph it.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "The Grammar of Networks"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html#graph-metaphors",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html#graph-metaphors",
    "title": "The Grammar of Networks",
    "section": "\n Graph Metaphors",
    "text": "Graph Metaphors\nNetwork graphs are characterized by two key terms: nodes and edges\n\n\nNodes : Entities\n\nMetaphors: Individual People? Things? Ideas? Places? to be connected in the network.\nSynonyms: vertices. Nodes have IDs.\n\n\n\nEdges: Connections\n\nMetaphors: Interactions? Relationships? Influence? Letters sent and received? Dependence? between the entities.\nSynonyms: links, ties.\n\n\n\nIn R, we create network representations using node and edge information. One way in which these could be organized are:\n\n\nNode list: a data frame with a single column listing the node IDs found in the edge list. You can also add attribute columns to the data frame such as the names of the nodes or grouping variables. ( Type? Class? Family? Country? Subject? Race? )\n\n\nNode Table\n\n\n\n\n\n\n\nID\nNode Name\nAttribute? Qualities?Categories? Family? Country?Planet?\n\n\n1\nNed\nNursery School Teacher\n\n\n2\nJaguar Paw\nMain Character, Apocalypto\n\n\n3\nJohn Snow\nEpidemiologist\n\n\n\n\n\nEdge list: data frame containing two columns: source node and destination node of an edge. Source and Destination have node IDs.\n\nWeighted network graph: An edge list can also contain additional columns describing attributes of the edges such as a magnitude aspect for an edge. If the edges have a magnitude attribute the graph is considered weighted.\n\n\nEdges Table\n\nFrom\nTo\nRelationship\nWeightage\n\n\n\n1\n3\nFinancial Dealings\n6\n\n\n2\n1\nHistory Lessons\n2\n\n\n2\n3\nVaccination\n15\n\n\n\n\n\nLayout: A geometric arrangement of nodes and edges.\n\nMetaphors: Location? Spacing? Distance? Coordinates? Colour? Shape? Size? Provides visual insight due to the arrangement.\n\n\n\nLayout Algorithms : Method to arranges nodes and edges with the aim of optimizing some metric .\n\nMetaphors: Nodes are masses and edges are springs. The Layout algorithm minimizes the stretching and compressing of all springs.(BTW, are the Spring Constants K the same for all springs?…)\n\n\nDirected and undirected network graph: If the distinction between source and target is meaningful, the network is directed. If the distinction is not meaningful, the network is undirected. Directed edges represent an ordering of nodes, like a relationship extending from one node to another, where switching the direction would change the structure of the network. Undirected edges are simply links between nodes where order does not matter.\n\n\n\n\n\n\n\nExamples\n\n\n\n\nThe World Wide Web is an example of a directed network because hyperlinks connect one Web page to another, but not necessarily the other way around.\nCo-authorship networks represent examples of un-directed networks, where nodes are authors and they are connected by an edge if they have written a publication together\nWhen people send e-mail to each other, the distinction between the sender (source) and the recipient (target) is clearly meaningful, therefore the network is directed.\n\n\n\n\n\nConnected and Disconnected graphs: If there is some path from any node to any other node, the Networks is said to be Connected. Else, Disconnected.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "The Grammar of Networks"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html#predictruninfer-1",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html#predictruninfer-1",
    "title": "The Grammar of Networks",
    "section": "Predict/Run/Infer-1",
    "text": "Predict/Run/Infer-1\nUsing tidygraph and ggraph\n\ntidygraph and ggraph are modern R packages for network data. Graph Data setup and manipulation is done in tidygraph and graph visualization with ggraph.\n\n\ntidygraph Data -&gt; “Network Object” in R.\n\nggraph Network Object -&gt; Plots using a chosen layout/algo.\n\nBoth leverage the power of igraph, which is the Big Daddy of all network packages. We will be using the Grey’s Anatomy dataset in our first foray into networks.\nStep1. Read the data\nDownload these two datasets into your current project-&gt; data folder.\n Grey’s Anatomy Nodes \n Grey’s Anatomy Edges \ngrey_nodes &lt;- read_csv(\"files/data/grey_nodes.csv\")\ngrey_edges &lt;- read_csv(\"files/data/grey_edges.csv\")\n\ngrey_nodes\ngrey_edges\n\n\n\n\n  \n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #1\n\n\n\nLook at the output thumbnails. What attributes (i.e. extra information) are seen for Nodes and Edges?\n\n\nStep 2.Create a network object using tidygraph:\nKey function:\n\n\ntbl_graph(): (aka “tibble graph”). Key arguments: nodes, edges and directed. Note this is a very versatile command and can take many input forms, such as data structures that result from other packages. Type ?tbl_graph in the Console and see the Usage section.\n\n\nga &lt;- tbl_graph(nodes = grey_nodes, \n                edges = grey_edges, \n                directed = FALSE)\nga\n\n# A tbl_graph: 54 nodes and 57 edges\n#\n# An undirected simple graph with 4 components\n#\n# Node Data: 54 × 7 (active)\n   name               sex   race  birthyear position  season sign    \n   &lt;chr&gt;              &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   \n 1 Addison Montgomery F     White      1967 Attending      1 Libra   \n 2 Adele Webber       F     Black      1949 Non-Staff      2 Leo     \n 3 Teddy Altman       F     White      1969 Attending      6 Pisces  \n 4 Amelia Shepherd    F     White      1981 Attending      7 Libra   \n 5 Arizona Robbins    F     White      1976 Attending      5 Leo     \n 6 Rebecca Pope       F     White      1975 Non-Staff      3 Gemini  \n 7 Jackson Avery      M     Black      1981 Resident       6 Leo     \n 8 Miranda Bailey     F     Black      1969 Attending      1 Virgo   \n 9 Ben Warren         M     Black      1972 Other          6 Aquarius\n10 Henry Burton       M     White      1972 Non-Staff      7 Cancer  \n# ℹ 44 more rows\n#\n# Edge Data: 57 × 4\n   from    to weight type    \n  &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt;   \n1     5    47      2 friends \n2    21    47      4 benefits\n3     5    46      1 friends \n# ℹ 54 more rows\n\n\n\n\n\n\n\n\nQuestions and Inferences #2\n\n\n\nWhat information does the graph object contain? What attributes do the nodes have? What about the edges?\n\n\nStep 3. Plot using ggraph\n\n3a. Quick Plot: autograph() This is to check quickly is the data is imported properly and to decide upon going on to a more elaborate plotting.\n\nautograph(ga)\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #3\n\n\n\nDescribe this graph, in simple words here. Try to use some of the new domain words we have just acquired: nodes/edges, connected/disconnected, directed/undirected.\n\n\n3b. More elaborate plot\nKey functions:\n\n\nggraph(layout = \"......\"): Create classic node-edge diagrams; i.e. Sets up the graph. Rather like ggplot for networks!\n\nTwo kinds of geom: one set for nodes, and another for edges\n\ngeom_node_point(aes(.....)): Draws node as “points”. Alternatives are circle / arc_bar / tile / voronoi. Remember the geoms that we have seen before in Grammar of Graphics!\ngeom_edge_link0(aes(.....)): Draws edges as “links”. Alternatives are arc / bend / elbow / hive / loop / parallel / diagonal / point / span /tile.\ngeom_node_text(aes(label = ......), repel = TRUE): Adds text labels (non-overlapping). Alternatives are label /...\nlabs(title = \"....\", subtitle = \"....\", caption = \"....\"): Change main titles, axis labels and legend titles. We know this from our work with ggplot.\n\n\n# Write Comments next to each line \n# About what that line does for the overall graph\n\nggraph(graph = ga,  layout = \"kk\") +\n  #\n  geom_edge_link0(width = 2, color = \"pink\") +\n  #\n  geom_node_point(shape = 21, size = 8,\n                  fill = \"blue\",\n                  color = \"green\",\n                  stroke = 2) +\n  \n  labs(title = \"Whoo Hoo! My First Silly Grey's Anatomy graph in R!\",\n       subtitle = \"Why did I ever get in this course...\",\n       caption = \"Bro, they are doing cool things in the other classes...\\n And the show is even more cool!\") +\n  \n  set_graph_style(family = \"Roboto\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #4:\n\n\n\nWhat parameters have been changed here, compared to the earlier graph? Where do you see these changes in the code above?\n\n\nLet us Play with this graph and see if we can make some small changes. Colour? Fill? Width? Size? Stroke? Labs? Of course!\n\n# Change the parameters in each of the commands here to new ones\n# Use fixed values for colours or sizes...etc. \n\nggraph(graph = ga,  layout = \"kk\") + \n  geom_edge_link0(width = 2) + \n  geom_node_point(shape = 21, size = 4, \n                  fill = \"moccasin\", \n                  color = \"firebrick\", \n                  stroke = 2) +\n  labs(title = \"Whoo Hoo! My next silly Grey's Anatomy graph in R!\",\n       subtitle = \"Why did I ever get in this course...\",\n       caption = \"Bro, they are doing cool things in the other classes...\") + \nset_graph_style(family = \"Roboto\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #5\n\n\n\nWhat did the shape parameter achieve? What are the possibilities with shape? How about including alpha?\n\n\n3c. Aesthetic Mapping from Node and Edge attribute columns\nUp to now, we have assigned specific numbers to geometric aesthetics such as shape and size. Now we are ready ( maybe ?) change the meaning and significance of the entire graph and each element within it, and use aesthetics / metaphoric mappings to achieve new meanings or insights. Let us try using aes() inside each geom to map a variable to a geometric aspect.\nDon’t try to use more than 2 aesthetic mappings simultaneously!!\nThe node elements we can tweak are:\n\nTypes of Nodes: geom_node_****()\n\nNode Parameters: inside geom_node_****(aes(...............))\n-aes(alpha  = node-variable) : opacity; a value between 0 and 1\n-aes(shape  = node-variable) : node shape\n-aes(colour = node-variable) : node colour\n-aes(fill   = node-variable) : fill colour for node\n-aes(size   = node-variable) : size of node\n\nThe edge elements we can tweak are:\n\nType of Edges” geom_edge_****()\n\nEdge Parameters: inside geom_edge_****(aes(...............))\n-aes(colour = edge-variable) : colour of the edge\n-aes(width  = edge-variable) : width of the edge\n-aes(label  = some_variable) : labels for the edge\n\nType ?geom_node_point and ?geom-edge_link in your Console for more information.\n\nggraph(graph = ga, layout = \"fr\") +\n  geom_edge_link0(aes(width = weight)) + # change variable here\n  \n  geom_node_point(aes(color = race), size = 6) + # change variable here\n\n  labs(title = \"Whoo Hoo! Yet another Grey's Anatomy graph in R!\",\n       subtitle = \"Colouring Nodes by Attribute\",\n       caption = \"Grey's Anatomy\") +\n  \n  scale_edge_width(range = c(0.2,2)) +\n  set_graph_style(family = \"roboto\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #6\n\n\n\nDescribe some of the changes here. What types of edges worked? Which variables were you able to use for nodes and edges and how? What did not work with either of the two?\n\n\n\n# Arc diagram\n\nggraph(ga, layout = \"linear\") +\n  geom_edge_arc0(aes(width = weight), alpha = 0.8) +\n  scale_edge_width(range = c(0.2, 2)) +\n  geom_node_point(size = 2, colour = \"red\") +\n  labs(edge_width = \"Weight\",title = \"Grey's Anatomy\", subtitle = \"Arc Layout\") +\n  set_graph_style(family = \"Roboto\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #7\n\n\n\nHow does this graph look “metaphorically” different? Do you see a difference in the relationships between people here? Why?\n\n\n\n# Coord diagram, circular\nggraph(ga, layout = \"linear\", circular = TRUE) + # Note the layout!\n  geom_edge_arc0(aes(width = weight), alpha = 0.8) + \n  scale_edge_width(range = c(0.2, 2)) +\n  \n  geom_node_point(size = 3,colour = \"red\") + \n  geom_node_text(aes(label = name),\n                 repel = TRUE, size = 2,check_overlap = TRUE, \n                 max.overlaps = 25) +\n  labs(edge_width = \"Weight\")  +\n  theme(aspect.ratio = 1) +\n  set_graph_style(family = \"Roboto\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #8\n\n\n\nHow does this graph look “metaphorically” different? Do you see a difference in the relationships between people here? Why?",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "The Grammar of Networks"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html#hierarchical-layouts",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html#hierarchical-layouts",
    "title": "The Grammar of Networks",
    "section": "Hierarchical layouts",
    "text": "Hierarchical layouts\nThese provide for some alternative metaphorical views of networks. Note that not all layouts are possible for all datasets!!\n\n# set_graph_style()\n\n# This dataset contains the graph that describes the class \n# hierarchy for the Flare visualization library.\n# Type ?flare in your Console\nhead(flare$vertices)\n\n\n  \n\n\nhead(flare$edges)\n\n\n  \n\n\n# flare class hierarchy\ngraph &lt;-  tbl_graph(edges = flare$edges, nodes = flare$vertices)\n\n##\nset_graph_style(family = \"Roboto\")\n##\n\n# dendrogram\nggraph(graph, layout = \"dendrogram\") + \n  geom_edge_diagonal() + \n  labs(title = \"Dendrogram\") \n\n# circular dendrogram\nggraph(graph, layout = \"dendrogram\", circular = TRUE) + \n  geom_edge_diagonal0() + \n  geom_node_point(aes(filter = leaf)) + \n  coord_fixed()+ \n  labs(title = \"Circular Dendrogram\")\n\n# rectangular tree map\nggraph(graph, layout = \"treemap\", weight = size) + \n  geom_node_tile(aes(fill = depth), size = 0.25) + \n  scale_fill_distiller(palette = \"Pastel1\") + \n  labs(title = \"Rectangular Tree Map\")\n\n\n# circular tree map\nggraph(graph, layout = \"circlepack\", weight = size) + \n  geom_node_circle(aes(fill = depth), size = 0.25, n = 50) + \n  scale_fill_distiller(palette = \"Accent\") + \n  coord_fixed() + \n  labs(title = \"Circular Tree Map\")\n\n\n# icicle\nggraph(graph, layout = \"partition\") + \n  geom_node_tile(aes(y = -y, fill = depth)) + \n  scale_fill_distiller(palette = \"Set3\") + \n  labs(title = \"Icicle Chart\")\n\n# sunburst (circular icicle)\nggraph(graph, layout = \"partition\", circular = TRUE) +\n  geom_node_arc_bar(aes(fill = depth)) +\n  scale_fill_distiller(palette = \"Spectral\") + \n  coord_fixed() + \n  labs(title = \"Circular Icicle\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #9\n\n\n\nHow do graphs look “metaphorically” different? Do they reveal different aspects of the group? How?",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "The Grammar of Networks"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html#faceting",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html#faceting",
    "title": "The Grammar of Networks",
    "section": "Faceting",
    "text": "Faceting\nFaceting allows to create sub-plots according to the values of a qualitative attribute on nodes or edges.\n##\nset_graph_style(family = \"Roboto\",size = 8)\n##\n# facet edges by type\nggraph(ga,layout = \"linear\", circular = TRUE) + \n  geom_edge_link0(aes(color = type)) + \n  geom_node_point() +\n  facet_edges(~ type) +\n  th_foreground(border = TRUE) +\n  theme(aspect.ratio = 1)\n# facet nodes by sex\nggraph(ga,layout = \"linear\", circular = TRUE) + \n  geom_edge_link0() + \n  geom_node_point() +\n  facet_nodes(~race) +\n  th_foreground(border = TRUE) +\n  theme(aspect.ratio = 1)\n# facet both nodes and edges\nggraph(ga,layout = \"linear\", circular = TRUE) + \n  geom_edge_link0(aes(color = type)) + \n  geom_node_point() +\n  facet_graph(type ~ race) + \n  th_foreground(border = TRUE) +\n  theme(aspect.ratio = 1, legend.position = \"right\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #10\n\n\n\nDoes splitting up the main graph into sub-networks give you more insight? Describe some of these.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "The Grammar of Networks"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html#network-analysis-with-tidygraph",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html#network-analysis-with-tidygraph",
    "title": "The Grammar of Networks",
    "section": "Network analysis with tidygraph",
    "text": "Network analysis with tidygraph\nThe data frame graph representation can be easily augmented with metrics or statistics computed on the graph. Remember how we computed counts with the penguin dataset in Grammar of Graphics.\nBefore computing a metric on nodes or edges use the activate() function to activate either node or edge data frames. Use dplyr verbs (filter, arrange, mutate) to achieve your computation in the proper way.\nNetwork Centrality: Go-To and Go-Through People!\nCentrality is a an “ill-defined” metric of node and edge importance in a network. It is therefore calculated in many ways. Type ?centrality in your Console.\n\n\nStandards\n\nLet’s add a few columns to the nodes and edges based on network centrality measures:\n\nga %&gt;% \n  activate(nodes) %&gt;% \n  \n  # Node with  the most connections?\n  mutate(degree = centrality_degree(mode = c(\"in\"))) %&gt;% \n  filter(degree &gt; 0) %&gt;% \n  \n  activate(edges) %&gt;% \n  # \"Busiest\" edge?\n  mutate(betweenness = centrality_edge_betweenness())\n\n# A tbl_graph: 54 nodes and 57 edges\n#\n# An undirected simple graph with 4 components\n#\n# Edge Data: 57 × 5 (active)\n    from    to weight type         betweenness\n   &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt;\n 1     5    47      2 friends             20.3\n 2    21    47      4 benefits            44.7\n 3     5    46      1 friends             39  \n 4     5    41      1 friends             66.3\n 5    18    41      6 friends             39  \n 6    21    41     12 benefits            91.5\n 7    37    41      5 professional       164. \n 8    31    41      2 professional        98.8\n 9    20    31      3 professional        47.2\n10    17    31      4 friends            102. \n# ℹ 47 more rows\n#\n# Node Data: 54 × 8\n  name               sex   race  birthyear position  season sign   degree\n  &lt;chr&gt;              &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1 Addison Montgomery F     White      1967 Attending      1 Libra       3\n2 Adele Webber       F     Black      1949 Non-Staff      2 Leo         1\n3 Teddy Altman       F     White      1969 Attending      6 Pisces      4\n# ℹ 51 more rows\n\n\nPackages tidygraph and ggraph can be pipe-lined to perform analysis and visualization tasks in one go.\n\n##\nset_graph_style(family = \"Roboto\")\n##\nggraph(ga,layout = \"nicely\") +\n    geom_edge_link0(aes(alpha = centrality_edge_betweenness())) + \n    \n    geom_node_point(aes(colour = centrality_degree(), \n                        size = centrality_degree())) +\n    \n    geom_node_text(aes(label = name), repel = TRUE, size = 1.5) +\n    \n    scale_size(name = \"Degree\", range = c(0.5, 5)) + \n    \n    scale_color_gradient(name = \"Degree\", # SAME NAME!!\n                         low = \"blue\", high = \"red\", \n                         aesthetics = c(\"colour\", \"fill\"), \n                         guide = guide_legend(reverse = FALSE)) + \n    \n    scale_edge_alpha(name = \"Betweenness\", range = c(0.05, 1)) +\n    labs(title = \"Grey's Anatomy\", \n         subtitle = \"Nodes Scaled by Degree, Edges shaded by Betweenness\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #11\n\n\n\nHow do the Centrality Measures show up in the graph? Would you “agree” with the way we have done it? Try to modify the aesthetics by copy-pasting this chunk below and see how you can make an alternative representation.\n\n\nAnalysis and Visualizing Network Communities\nWho is close to whom? Which are the groups you can see?\n\n##\nset_graph_style(family = \"Roboto\")\n##\n# visualize communities of nodes\nga %&gt;% \n  activate(nodes) %&gt;%\n  mutate(community = as.factor(group_louvain())) %&gt;% \n  ggraph(layout = \"graphopt\") + \n  geom_edge_link0() + \n  geom_node_point(aes(color = community), size = 3) +\n  labs(title = \"Grey's Anatomy\", subtitle = \"Nodes Coloured by Community Detection Algorithm (Louvain)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions and Inferences #12\n\n\n\nIs the Community depiction clear? How would you do it, with which aesthetic? Copy Paste this chunk below and try.\n\n\nInteractive Graphs with visNetwork\n\nExploring the VisNetwork package. Make graphs wiggle and shake using tidy commands! The package implements interactivity using the physical metaphor of weights and springs we discussed earlier.\nThe visNetwork() function uses a nodes list and edges list to create an interactive graph. The nodes list must include an “id” column, and the edge list must have “from” and “to” columns. The function also plots the labels for the nodes, using the names of the cities from the “label” column in the node list.\nlibrary(visNetwork)\n\n# Prepare the data for plotting by visNetwork\ngrey_nodes\ngrey_edges\n# Relabel greys anatomy nodes and edges for VisNetwork\ngrey_nodes_vis &lt;- grey_nodes %&gt;% \n  rowid_to_column(var = \"id\") %&gt;% \n  rename(\"label\" = name) %&gt;% \n  mutate(sex = case_when(sex == \"F\" ~ \"Female\",\n                         sex == \"M\" ~ \"Male\")) %&gt;% \n  replace_na(., list(sex = \"Transgender?\")) %&gt;% \n  rename(\"group\" = sex)\ngrey_nodes_vis\ngrey_edges_vis &lt;- grey_edges %&gt;% \n  select(from, to) %&gt;% \n  left_join(., grey_nodes_vis, \n            by = c(\"from\" = \"label\")) %&gt;% \n  left_join(., grey_nodes_vis, \n            by = c(\"to\" = \"label\")) %&gt;%\n  select(\"from\"= id.x, \"to\" = id.y)\ngrey_edges_vis\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nUsing fontawesome icons\n\ngrey_nodes_vis %&gt;%\n\n  visNetwork(nodes = ., edges = grey_edges_vis) %&gt;% \n  visNodes(font = list(size = 40)) %&gt;% \n  \n  # Colour and icons for each of the gender-groups\n  visGroups(groupname = \"Female\", shape = \"icon\", \n            icon = list(code = \"f182\", size = 75, color = \"tomato\"),\n            shadow = list(enabled = TRUE)) %&gt;% \n  \n  visGroups(groupname = \"Male\", shape = \"icon\", \n            icon = list(code = \"f183\", size = 75, color = \"slateblue\"), \n            shadow = list(enabled = TRUE)) %&gt;% \n  \n  visGroups(groupname = \"Transgender?\", shape = \"icon\", \n            icon = list(code = \"f22c\", size = 75, color = \"fuchsia\"), \n            shadow = list(enabled = TRUE)) %&gt;% \n  \n  #visLegend() %&gt;%\n  #Add the fontawesome icons!!\n  addFontAwesome(version = \"4.7.0\") %&gt;% \n  \n  # Add Interaction Controls\n  visInteraction(navigationButtons = TRUE,\n                 hover = TRUE,\n                 selectConnectedEdges = TRUE,\n                 hoverConnectedEdges = TRUE,\n                 zoomView = TRUE)\n\n\n\n\n\nThere is another family of icons available in visNetwork, called ionicons. Let’s see how they look:\n\ngrey_nodes_vis %&gt;%\n\n  visNetwork(nodes = ., edges = grey_edges_vis,) %&gt;%\n  visLayout(randomSeed = 12345) %&gt;%\n  visNodes(font = list(size = 50)) %&gt;%\n  visEdges(color = \"green\") %&gt;%\n  visGroups(\n    groupname = \"Female\",\n    shape = \"icon\",\n    icon = list(\n      face = 'Ionicons',\n      code = \"f25d\",\n      color = \"fuchsia\",\n      size = 125\n    )\n  ) %&gt;%\n  \n  visGroups(\n    groupname = \"Male\",\n    shape = \"icon\",\n    icon = list(\n      face = 'Ionicons',\n      code = \"f202\",\n      color = \"green\",\n      size = 125\n    )\n  ) %&gt;%\n  \n  visGroups(\n    groupname = \"Transgender?\",\n    shape = \"icon\",\n    icon = list(\n      face = 'Ionicons',\n      code = \"f233\",\n      color = \"dodgerblue\",\n      size = 125\n    )\n  ) %&gt;%\n  visLegend() %&gt;%\n  addIonicons() %&gt;%\n  visInteraction(\n    navigationButtons = TRUE,\n    hover = TRUE,\n    selectConnectedEdges = TRUE,\n    hoverConnectedEdges = TRUE,\n    zoomView = TRUE\n  )\n\n\n\n\n\nSome idea of interactivity and controls with visNetwork:\n Star Wars Nodes \n Star Wars Edges \n\n# let's look again at the data\nstarwars_nodes &lt;- read_csv(\"files/data/star-wars-network-nodes.csv\")\nstarwars_edges &lt;- read_csv(\"files/data/star-wars-network-edges.csv\")\n\n# We need to rename starwars nodes dataframe and edge dataframe columns for visNetwork\nstarwars_nodes_vis &lt;- \n  starwars_nodes %&gt;% \n  rename(\"label\" = name)\n\n# Convert from and to columns to **node ids**\nstarwars_edges_vis &lt;- \n  starwars_edges %&gt;% \n  \n  # Matching Source &lt;- Source Node id (\"id.x\")\n  left_join(., starwars_nodes_vis, by = c(\"source\" = \"label\")) %&gt;% \n  \n  # Matching Target &lt;- Target Node id (\"id.y\")\n  left_join(., starwars_nodes_vis, by = c(\"target\" = \"label\")) %&gt;% \n  \n# Select \"id.x\" and \"id.y\" ONLY\n# Rename them as \"from\" and \"to\"\n# keep \"weight\" column for aesthetics of edges\n  select(\"from\" = id.x, \"to\" = id.y, \"value\" = weight)\n\n# Check everything once\nstarwars_nodes_vis\nstarwars_edges_vis\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nOk, let’s make things move and shake!!\n\nvisNetwork(nodes = starwars_nodes_vis,\n           edges = starwars_edges_vis) %&gt;% \n  visNodes(font = list(size = 30)) %&gt;% \n  visEdges(color = \"red\")\n\n\n\n\n\n\nvisNetwork(nodes = starwars_nodes_vis,\n           edges = starwars_edges_vis) %&gt;% \n  visNodes(font = list(size = 30), shape = \"icon\", \n           icon = list(code = \"f1e3\", size = 75)) %&gt;% \n  visEdges(color = list(color = \"red\", hover = \"green\", highlight = \"black\")) %&gt;% \n  visInteraction(hover = TRUE) %&gt;% \n  addFontAwesome(version = \"4.7.0\")",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "The Grammar of Networks"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html#your-assignments",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html#your-assignments",
    "title": "The Grammar of Networks",
    "section": "Your Assignments:",
    "text": "Your Assignments:\nMake-1 : With a readymade dataset\nStep 0. Sine qua non! Fire up a New Project! Always!\nStep 1. Fire up a new Quarto document. Fill in the YAML header.\nStep 2. Take any one of the “Make1-Datasets” datasets described below.\nStep 3. Document contents:\n\nIntroduce / Inspect in R your data and describe\nIntroduce your Purpose\nCreate graph objects\nTry different layouts\nWrite comments in the code\nWrite narrative in text with sections, bold ,italic etc.\n\nStep 4. Knit before you submit. Submit your entire project in a .zip file.\nMake1 - Datasets:\n\n\n\n\n\n\nAirline Data:\n\n\n\n Airlines Nodes \n Airlines Edges \nStart with this bit of code in your second chunk, after set up\n\n```{r}\n#| label: start up code for Airlines\n#| eval: false ## remove this!!\nairline_nodes &lt;-\n  read_csv(\"./mydatafolder/AIRLINES-NODES.csv\") %&gt;% \n  mutate(Id = Id + 1)\n\nairline_edges &lt;-\n  read_csv(\"./mydatafolder/AIRLINES-EDGES.csv\") %&gt;%\n  mutate(Source = Source + 1, Target = Target + 1)\n```\n\n\n\n\n\n\n\n\n\nThe Famous Zachary Karate Club dataset\n\n\n\n\nStart with pulling this data into your Quarto:\n\n\n```{r}\n#| eval: false ## remove this!\ndata(\"karate\",package= \"igraphdata\")\nkarate\n```\n\n\nTry ?karate in the console\n\nNote that this is not a set of nodes, nor edges, but already a graph-object!\n\nSo no need to create a graph object using tbl_graph.\n\nYou will need to just go ahead and plot using ggraph.\n\n\n\n\n\n\n\n\n\nGame of Thrones:\n\n\n\n GoT Networks \n\nStart with pulling this data into your Rmarkdown:\n\n\n```{r}\n#| label: start-up code for GoT\n#| eval: false ## remove this!!\n\nGoT &lt;- read_rds(\"data/GoT.RDS\")\n```\n\n\nNote that this is a list of 7 graphs from Game of Thrones.\nSelect one using GoT[[index]] where index = 1…7 and then plot directly.\nTry to access the nodes and edges and modify them using any attribute data\n\n\n\n\n\n\n\n\n\nOther Datasets\n\n\n\n\nChoose any other graph dataset from igraphdata\n\n(type ?igraphdata in console)\n\nAsk me for help if you need any\n\n\n\n\nMake-2: Literary Network with TV Show / Book / Story / Play\nYou need to create a Network Graph for your favourite Book, play, TV serial or Show. (E.g. Friends, BBT, or LB or HIMYM, B99, TGP, JTV…or Hamlet, Little Women , Pride and Prejudice, or LoTR)\n\nStep 1. Go to: Literary Networks for instructions.\n\nStep 2. Make your data using the instructions.\n\nIn the nodes excel, use id and names as your columns. Any other details in other columns to the right.\n\nIn your edges excel, use from and to as your first columns.\n\nEntries in these columns can be names or ids but be consistent and don’t mix.\n\n\n\nStep 3. Decide on 3 answers that you to seek and plan to make graphs for.\nStep 4. Create graph objects. Say 3 visualizations.\nStep 5. Write comments/answers in the code and narrative text. Add pictures from the web using Markdown syntax.\nStep 6. Write Reflection ( ok, a short one!) inside your Quarto document. Make sure it renders !!\nStep 7. Group Submission: Submit the render-able .qmd file AND the data. Quarto Markdown with joint authorship. Each person submits on their Assignments. All get the same grade on this one.\n\nAsk me for clarifications on what to do after you have read the Instructions in your group.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "The Grammar of Networks"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/networks.html#references",
    "title": "The Grammar of Networks",
    "section": "\n References",
    "text": "References\n\n\nHadley Wickham, Danielle Navarro, and Thomas Lin Pedersen, ggplot2: Elegant Graphics for Data Analysis. https://ggplot2-book.org/networks\n\nOmar Lizardo and Isaac Jilbert, Social Networks: An Introduction. https://bookdown.org/omarlizardo/_main/\n\nMark Hoffman, Methods for Network Analysis. https://bookdown.org/markhoff/social_network_analysis/\n\n\nStatistical Analysis of Network Data with R, 2nd Edition.https://github.com/kolaczyk/sand\n\n\nThomas Lin Pedersen - 1 giraffe, 2 giraffe,GO!\n\nTyner, Sam, François Briatte, and Heike Hofmann. 2017. “Network Visualization with ggplot2.” The R Journal 9 (1): 27–59. https://journal.r-project.org/archive/2017/RJ-2017-023/index.html\n\nNetwork Datasets https://icon.colorado.edu/#!/networks\n\nYunran Chen, Introduction to Network Analysis Using R\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggraph\n2.2.1\nPedersen (2024a)\n\n\nggtext\n0.1.2\nWilke and Wiernik (2022)\n\n\ngraphlayouts\n1.1.1\nDavid Schoch (2023)\n\n\nigraph\n2.0.3\n\nCsardi and Nepusz (2006); Csárdi et al. (2024)\n\n\n\nigraphdata\n1.0.1\nCsardi (2015)\n\n\nsand\n2.0.0\nKolaczyk and Csárdi (2020)\n\n\nshowtext\n0.9.7\nQiu and See file AUTHORS for details. (2024)\n\n\ntidygraph\n1.3.1\nPedersen (2024b)\n\n\nvisNetwork\n2.1.2\nAlmende B.V. and Contributors and Thieurmel (2022)\n\n\n\n\n\n\nAlmende B.V. and Contributors, and Benoit Thieurmel. 2022. visNetwork: Network Visualization Using “vis.js” Library. https://CRAN.R-project.org/package=visNetwork.\n\n\nCsardi, Gabor. 2015. igraphdata: A Collection of Network Data Sets for the “igraph” Package. https://CRAN.R-project.org/package=igraphdata.\n\n\nCsardi, Gabor, and Tamas Nepusz. 2006. “The Igraph Software Package for Complex Network Research.” InterJournal Complex Systems: 1695. https://igraph.org.\n\n\nCsárdi, Gábor, Tamás Nepusz, Vincent Traag, Szabolcs Horvát, Fabio Zanini, Daniel Noom, and Kirill Müller. 2024. igraph: Network Analysis and Visualization in r. https://doi.org/10.5281/zenodo.7682609.\n\n\nDavid Schoch. 2023. “graphlayouts: Layout Algorithms for Network Visualizations in r.” Journal of Open Source Software 8 (84): 5238. https://doi.org/10.21105/joss.05238.\n\n\nKolaczyk, Eric, and Gábor Csárdi. 2020. sand: Statistical Analysis of Network Data with r, 2nd Edition. https://CRAN.R-project.org/package=sand.\n\n\nPedersen, Thomas Lin. 2024a. ggraph: An Implementation of Grammar of Graphics for Graphs and Networks. https://CRAN.R-project.org/package=ggraph.\n\n\n———. 2024b. tidygraph: A Tidy API for Graph Manipulation. https://CRAN.R-project.org/package=tidygraph.\n\n\nQiu, Yixuan, and authors/contributors of the included software. See file AUTHORS for details. 2024. showtext: Using Fonts More Easily in r Graphs. https://CRAN.R-project.org/package=showtext.\n\n\nWilke, Claus O., and Brenton M. Wiernik. 2022. ggtext: Improved Text Rendering Support for “ggplot2”. https://CRAN.R-project.org/package=ggtext.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "The Grammar of Networks"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/05-NatureData/nature-data.html",
    "href": "content/courses/Analytics/Descriptive/Modules/05-NatureData/nature-data.html",
    "title": "🕶 Science, Human Experience, Experiments, and Data",
    "section": "",
    "text": "This tutorial uses web-r that allows you to run all code within your browser, on all devices. Most code chunks herein are formatted in a tabbed structure ( like in an old-fashioned library) with duplicated code. The tabs in front have regular R code that will work when copy-pasted in your RStudio session. The tab “behind” has the web-R code that can work directly in your browser, and can be modified as well. The R code is also there to make sure you have original code to go back to, when you have made several modifications to the code on the web-r tabs and need to compare your code with the original!\n\n\nRun selected code using either:\n\nmacOS: ⌘ + ↩︎/Return\n\nWindows/Linux: Ctrl + ↩︎/Enter\n\n\n\nRun the entire code by clicking the “Run code” button or pressing Shift+↩︎.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🕶 Science, Human Experience, Experiments, and Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/05-NatureData/nature-data.html#using-web-r",
    "href": "content/courses/Analytics/Descriptive/Modules/05-NatureData/nature-data.html#using-web-r",
    "title": "🕶 Science, Human Experience, Experiments, and Data",
    "section": "",
    "text": "This tutorial uses web-r that allows you to run all code within your browser, on all devices. Most code chunks herein are formatted in a tabbed structure ( like in an old-fashioned library) with duplicated code. The tabs in front have regular R code that will work when copy-pasted in your RStudio session. The tab “behind” has the web-R code that can work directly in your browser, and can be modified as well. The R code is also there to make sure you have original code to go back to, when you have made several modifications to the code on the web-r tabs and need to compare your code with the original!\n\n\nRun selected code using either:\n\nmacOS: ⌘ + ↩︎/Return\n\nWindows/Linux: Ctrl + ↩︎/Enter\n\n\n\nRun the entire code by clicking the “Run code” button or pressing Shift+↩︎.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🕶 Science, Human Experience, Experiments, and Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/05-NatureData/nature-data.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/05-NatureData/nature-data.html#setting-up-r-packages",
    "title": "🕶 Science, Human Experience, Experiments, and Data",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(tidyverse) # Data processing with tidy principles\nlibrary(mosaic) # Our go-to package for almost everything\nlibrary(ggformula) # Our plotting package\n\nlibrary(Lock5withR) # Some neat little datasets from a lovely textbook\nlibrary(kableExtra)",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🕶 Science, Human Experience, Experiments, and Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/05-NatureData/nature-data.html#sec-where-data",
    "href": "content/courses/Analytics/Descriptive/Modules/05-NatureData/nature-data.html#sec-where-data",
    "title": "🕶 Science, Human Experience, Experiments, and Data",
    "section": "\n Where does Data come from?",
    "text": "Where does Data come from?\nWe will need to form a basic understanding of basic scientific enterprise. Let us look at the slides. (Also embedded below!)",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🕶 Science, Human Experience, Experiments, and Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/05-NatureData/nature-data.html#why-visualize",
    "href": "content/courses/Analytics/Descriptive/Modules/05-NatureData/nature-data.html#why-visualize",
    "title": "🕶 Science, Human Experience, Experiments, and Data",
    "section": "\n Why Visualize?",
    "text": "Why Visualize?\n\nWe can digest information more easily when it is pictorial\nOur Working Memories are both short-term and limited in capacity. So a picture abstracts the details and presents us with an overall summary, an insight, or a story that is both easy to recall and easy on retention.\nData Viz includes shapes that carry strong cultural memories; and impressions for us. These cultural memories help us to use data viz in a universal way to appeal to a wide variety of audiences. (Do humans have a gene for geometry?1);\nIt helps sift facts and mere statements: for example:\n\n\n\n\n\n\nFigure 1: Rape Capital\n\n\n\n\n\n\n\nFigure 2: Data Reveals Crime",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🕶 Science, Human Experience, Experiments, and Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/05-NatureData/nature-data.html#what-are-data-types",
    "href": "content/courses/Analytics/Descriptive/Modules/05-NatureData/nature-data.html#what-are-data-types",
    "title": "🕶 Science, Human Experience, Experiments, and Data",
    "section": "\n What are Data Types?",
    "text": "What are Data Types?\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Data\n\n\n\nEach variable is a column; a column contains one kind of data. Each observation or case is a row.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🕶 Science, Human Experience, Experiments, and Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/05-NatureData/nature-data.html#sec-data-types",
    "href": "content/courses/Analytics/Descriptive/Modules/05-NatureData/nature-data.html#sec-data-types",
    "title": "🕶 Science, Human Experience, Experiments, and Data",
    "section": "\n How do we Spot Data Variable Types?",
    "text": "How do we Spot Data Variable Types?\nBy asking questions! Shown below is a table of different kinds of questions you could use to query a dataset. The variable or variables that “answer” the question would be in the category indicated by the question.\n\n\n\n\n\n\nNo\nPronoun\nAnswer\nVariable/Scale\nExample\nWhat Operations?\n\n\n\n1\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities, with Scale and a Zero Value.Differences and Ratios /Products are meaningful.\nQuantitative/Ratio\nLength,Height,Temperature in Kelvin,Activity,Dose Amount,Reaction Rate,Flow Rate,Concentration,Pulse,Survival Rate\nCorrelation\n\n\n2\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities with Scale. Differences are meaningful, but not products or ratios\nQuantitative/Interval\npH,SAT score(200-800),Credit score(300-850),SAT score(200-800),Year of Starting College\nMean,Standard Deviation\n\n\n3\nHow, What Kind, What Sort\nA Manner / Method, Type or Attribute from a list, with list items in some \" order\" ( e.g. good, better, improved, best..)\nQualitative/Ordinal\nSocioeconomic status (Low income, Middle income, High income),Education level (HighSchool, BS, MS, PhD),Satisfaction rating(Very much Dislike, Dislike, Neutral, Like, Very Much Like)\nMedian,Percentile\n\n\n4\nWhat, Who, Where, Whom, Which\nName, Place, Animal, Thing\nQualitative/Nominal\nName\nCount no. of cases,Mode\n\n\n\n\n\n\n\nAs you go from Qualitative to Quantitative data types in the table, I hope you can detect a movement from fuzzy groups/categories to more and more crystallized numbers.\n\n\nType of Variables\n\nEach variable/scale can be subjected to the operations of the previous group. In the words of S.S. Stevens\n\nthe basic operations needed to create each type of scale is cumulative: to an operation listed opposite a particular scale must be added all those operations preceding it.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🕶 Science, Human Experience, Experiments, and Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/05-NatureData/nature-data.html#some-examples-of-data-variables",
    "href": "content/courses/Analytics/Descriptive/Modules/05-NatureData/nature-data.html#some-examples-of-data-variables",
    "title": "🕶 Science, Human Experience, Experiments, and Data",
    "section": "Some Examples of Data Variables",
    "text": "Some Examples of Data Variables\nExample 1: AllCountries\n\n\n\nBase R\n web-r\n\n\n\n\nhead(AllCountries,5) %&gt;% arrange(desc(Internet))\n\n\n  \n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nQuestions\n\n\n\nQ1. How many people in Andorra have internet access?\nA1. This leads to the Internet variable, which is a Quantitative variable, a proportion.2 The answer is \\(70.5\\%\\).\n\n\nExample 2:StudentSurveys\n\n\n\nBase R\n web-r\n\n\n\n\nhead(StudentSurvey,5)\n\n\n  \n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nQuestions\n\n\n\nQ.1. What kind of students are these?\nA.1. The variables Gender, and Year both answer to this Question. And they are both Qualitative/Categorical variables, of course.\nQ.2. What is their status in their respective families?\nA.2. Hmm…they are either first-born, or second-born, or third…etc. While this is recorded as a number, it is still a Qualitative variable3! Think! Can you do math operations with BirthOrder? Like mean or median?\nQ.3.How big are the families?\nA.3. Clearly, the variable that answers is Siblings and since the question is synonymous with “how many”, this is a Quantitative variable.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🕶 Science, Human Experience, Experiments, and Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/05-NatureData/nature-data.html#what-is-a-data-visualization",
    "href": "content/courses/Analytics/Descriptive/Modules/05-NatureData/nature-data.html#what-is-a-data-visualization",
    "title": "🕶 Science, Human Experience, Experiments, and Data",
    "section": "\n What is a Data Visualization?",
    "text": "What is a Data Visualization?\n\n Data Viz = Data + Geometry\n\n Shapes\nData Visualization is the act of “mapping” a geometric aspect/aesthetic to a variable in data. The aesthetic then varies in accordance with the data variable, creating (part of) a chart.\nWhat might be the geometric aesthetics available to us? An aesthetic is a geometric property, such as x-coordinate, y-coordinate, length/breadth/height,radius,shape,size, linewidth, linetype, and even colour…\n\n\nCommon Geometric Aesthetics in Charts\n\n\n Mapping\nWhat does this “mapping” mean? That the geometric aesthetics are used to represent qualitative or quantitative variables from your data, by varying in accordance to the data variable.\nFor instance, length or height of a bar can be made proportional to theage or income of a person. Colour of points can be mapped to gender, with a unique colour for each gender. Position along an axis x can vary in accordance with a height variable and position along the y axis can vary with a bodyWeight variable.\n\n\n\n\n\n\n\n\nA chart may use more than one aesthetic: position, shape, colour, height and angle,pattern or texture to name several. Usually, each aesthetic is mapped to just one variable to ensure there is no cognitive error. There is of course a choice and you should be able to map any kind of variable to any geometric aspect/aesthetic that may be available.\n\n\n\n\n\n\nA Natural Mapping\n\n\n\nNote that here is also a “natural” mapping between aesthetic and [kind of variable] Section 6, Quantitative or Qualitative. For instance, shape is rarely mapped to a Quantitative variable; we understand this because the nature of variation between the Quantitative variable and the shape aesthetic is not similar (i.e. not continuous). Bad choices may lead to bad, or worse, misleading charts!\n\n\nIn the above chart, it is pretty clear what kind of variable is plotted on the x-axis and the y-axis. What about colour? Could this be considered a z-axis in the chart? There are also other aspects that you can choose (not explicitly shown here) such as the plot theme(colours, fonts, backgrounds etc), which may not be mapped to data, but are nonetheless choices to be made. We will get acquainted with this aspect as we build charts.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🕶 Science, Human Experience, Experiments, and Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/05-NatureData/nature-data.html#sec-data-viz",
    "href": "content/courses/Analytics/Descriptive/Modules/05-NatureData/nature-data.html#sec-data-viz",
    "title": "🕶 Science, Human Experience, Experiments, and Data",
    "section": "\n Basic Types of Charts",
    "text": "Basic Types of Charts\nWe can think of simple visualizations as combinations of aesthetics, mapped to combinations of variables. Some examples:\n\nGeometries , Combinations, and Graphs\n\n\n\n\n\n\n\nVariable #1\nVariable #2\nChart Names\nChart Shape\n\n\n\nQuant\nNone\nHistogram and Density\n\n\n\n\n\nQual\nNone\nBar Chart\n\n\n\nQuant\nQuant\nScatter Plot, Line Chart, Bubble Plot, Area Chart\n\n\n\n\n\nQuant\nQual\nPie Chart, Donut Chart, Column Chart, Box-Whisker Plot, Radar Chart, Bump Chart, Tree Diagram\n\n\n\n\n\nQual\nQual\nStacked Bar Chart, Mosaic Chart, Sankey, Chord Diagram, Network Diagram",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🕶 Science, Human Experience, Experiments, and Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/05-NatureData/nature-data.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/05-NatureData/nature-data.html#conclusion",
    "title": "🕶 Science, Human Experience, Experiments, and Data",
    "section": "\n Conclusion",
    "text": "Conclusion\nSo there we have it:\n\nQuestions lead to Types of Variables (Quant and Qual)\n\nFurther Questions lead to relationships between them, which we describe using Data Visualizations\n\nData Visualizations are Data mapped onto Geometry \nMultiple Variable-to-Geometry Mappings = A Complete Data Visualization\n\n\nYou might think of all these Questions, Answers, Mapping as being equivalent to metaphors as a language in itself. And indeed, in R we use a philosophy called the Grammar of Graphics! We will use this grammar in the R graphics packages that we will encounter.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🕶 Science, Human Experience, Experiments, and Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/05-NatureData/nature-data.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/05-NatureData/nature-data.html#references",
    "title": "🕶 Science, Human Experience, Experiments, and Data",
    "section": "\n References",
    "text": "References\n\nRandomized Trials:\n\n\n\n \nMartyn Shuttleworth, Lyndsay T Wilson (Jun 26, 2009). What is the Scientific Method? Retrieved Mar 12, 2024 from Explorable.com: https://explorable.com/what-is-the-scientific-method\nhttps://safetyculture.com/topics/design-of-experiments/\nOpen Intro Stats: Types of Variables\nLock, Lock, Lock, Lock, and Lock. Statistics: Unlocking the Power of Data, Third Edition, Wiley, 2021. https://www.wiley.com/en-br/Statistics:+Unlocking+the+Power+of+Data,+3rd+Edition-p-9781119674160)\nClaus Wilke. Fundamentals of Data Visualization. https://clauswilke.com/dataviz/\nTim C. Hesterberg (2015). What Teachers Should Know About the Bootstrap: Resampling in the Undergraduate Statistics Curriculum, The American Statistician, 69:4, 371-386, DOI:10.1080/00031305.2015.1089789. PDF here\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggformula\n0.12.0\nKaplan and Pruim (2023)\n\n\nLock5withR\n1.2.2\nPruim (2015)\n\n\nmosaic\n1.9.1\nPruim, Kaplan, and Horton (2017)\n\n\nTeachingDemos\n2.13\nSnow (2024)\n\n\n\n\n\n\nKaplan, Daniel, and Randall Pruim. 2023. ggformula: Formula Interface to the Grammar of Graphics. https://CRAN.R-project.org/package=ggformula.\n\n\nPruim, Randall. 2015. Lock5withR: Datasets for “Statistics: Unlocking the Power of Data”. https://CRAN.R-project.org/package=Lock5withR.\n\n\nPruim, Randall, Daniel T Kaplan, and Nicholas J Horton. 2017. “The Mosaic Package: Helping Students to ‘Think with Data’ Using r.” The R Journal 9 (1): 77–102. https://journal.r-project.org/archive/2017/RJ-2017-024/index.html.\n\n\nSnow, Greg. 2024. TeachingDemos: Demonstrations for Teaching and Learning. https://CRAN.R-project.org/package=TeachingDemos.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🕶 Science, Human Experience, Experiments, and Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/05-NatureData/nature-data.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/05-NatureData/nature-data.html#footnotes",
    "title": "🕶 Science, Human Experience, Experiments, and Data",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://www.xcode.in/genes-and-personality/how-genes-influence-your-math-ability/↩︎\nHow might this data have been obtained? By asking people in a survey and getting Yes/No answers!↩︎\nQualitative variables are called Factor variables in R, and are stored, internally, as numeric variables together with their levels. The actual values of the numeric variable are 1, 2, and so on.↩︎",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🕶 Science, Human Experience, Experiments, and Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html",
    "title": "🕔 Time Series",
    "section": "",
    "text": "TimeSeries Wrangling  \n\n  Time Series Analysis-WIP",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🕔 Time Series"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#sec-slides-and-tutorials",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#sec-slides-and-tutorials",
    "title": "🕔 Time Series",
    "section": "",
    "text": "TimeSeries Wrangling  \n\n  Time Series Analysis-WIP",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🕔 Time Series"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#using-web-r",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#using-web-r",
    "title": "🕔 Time Series",
    "section": "\n Using web-R",
    "text": "Using web-R\nThis tutorial uses web-r that allows you to run all code within your browser, on all devices. Most code chunks herein are formatted in a tabbed structure (like in an old-fashioned library), with duplicated code. The tabs in front have regular R code that will work when copy-pasted in your RStudio session. The tab “behind” has the web-R code that can work directly in your browser, and can be modified as well. The R code is also there to make sure you have original code to go back to, when you have made several modifications to the code on the web-r tabs and need to compare your code with the original!\nKeyboard Shortcuts\n\nRun selected code using either:\n\nmacOS: ⌘ + ↩︎/Return\n\nWindows/Linux: Ctrl + ↩︎/Enter\n\n\n\nRun the entire code by clicking the “Run code” button or pressing Shift+↩︎.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🕔 Time Series"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#setting-up-r-packages",
    "title": "🕔 Time Series",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(ggformula) # Our Formula based graphing package\nlibrary(skimr)\nlibrary(fpp3)\n\n# Wrangling\n# library(lubridate)  # Deal with dates. Loads with tidyverse\n# library(tsibble) # loads with ffp3\n# library(tsibbledata) # loads with fpp3\n\n# devtools::install_github(\"FinYang/tsdl\")\nlibrary(tsdl)\nlibrary(TSstudio)\nlibrary(timetk)\nlibrary(tsbox)\nlibrary(gghighlight) # Highlight specific parts of charts\n\nThe fpp3 packages loads a good few other packages:\n\n\n [1] \"cli\"         \"crayon\"      \"dplyr\"       \"fable\"       \"fabletools\" \n [6] \"feasts\"      \"ggplot2\"     \"lubridate\"   \"magrittr\"    \"purrr\"      \n[11] \"rstudioapi\"  \"tibble\"      \"tidyr\"       \"tsibble\"     \"tsibbledata\"",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🕔 Time Series"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#reminder-on-plotting",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#reminder-on-plotting",
    "title": "🕔 Time Series",
    "section": "Reminder on Plotting",
    "text": "Reminder on Plotting\n\n\n\n\n\n\nmosaic and ggformula command template\n\n\n\nNote the standard method for all commands from the mosaic and ggformula packages: goal( y ~ x | z, data = ___)\nWith ggformula, one can create any graph/chart using: gf_***(y ~ x | z, data =___ )\nIn practice, we often use: dataframe %&gt;% gf_***(y ~ x | z) which has cool benefits such as “autocompletion” of variable names, as we shall see. The “***” indicates what kind of graph you desire: histogram, bar, scatter, density; the “___” is the name of your dataset that you want to plot with.\n\n\n\n\n\n\n\n\nggplot command template\n\n\n\nThe ggplot2 template is used to identify the dataframe, identify the x and y axis, and define visualized layers:\nggplot(data = —, mapping = aes(x = —, y = —)) + geom_—-()\nNote: —- is meant to imply text you supply. e.g. function names, data frame names, variable names.\nIt is helpful to see the argument mapping, above. In practice, rather than typing the formal arguments, code is typically shorthanded to this:\ndataframe %&gt;% ggplot(aes(xvar, yvar)) + geom_—-()",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🕔 Time Series"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#introduction",
    "title": "🕔 Time Series",
    "section": "\n Introduction",
    "text": "Introduction\nAny metric that is measured over regular time intervals forms a time series. Analysis of Time Series is commercially important because of industrial need and relevance, especially with respect to Forecasting (Weather data, sports scores, population growth figures, stock prices, demand, sales, supply…). For example, in the graph shown below are the temperatures over time in two US cities:\n\n\nWhat can we do with Time Series? As with other datasets, we have to begin by answering fundamental questions, such as:\n\nWhat are the types of time series?\nHow do we visualize time series?\nHow might we summarize time series to get aggregate numbers, say by week, month, quarter or year?\nHow do we decompose the time series into level, trend, and seasonal components?\nHow might we make a model of the underlying process that creates these time series?\nHow do we make useful forecasts with the data we have?\n\nWe will first look at the multiple data formats for time series in R. Alongside we will look at the R packages that work with these formats and create graphs and measures using those objects. Then we examine data wrangling of time series, where we look at packages that offer dplyr-like ability to group and summarize time series using the time variable. We will finally look at obtaining the components of the time series and try our hand at modelling and forecasting.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🕔 Time Series"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#time-series-formats-conversion-and-plotting",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#time-series-formats-conversion-and-plotting",
    "title": "🕔 Time Series",
    "section": "\n Time Series Formats, Conversion, and Plotting",
    "text": "Time Series Formats, Conversion, and Plotting\nThere are multiple formats for time series data. The ones that we are likely to encounter most are:\n\nThe ts format: We may simply have a single series of measurements that are made over time, stored as a numerical vector. The stats::ts() function will convert a numeric vector into an R time series ts object, which is the most basic time series object in R. The base-R ts object is used by established packages forecast and is also supported by newer packages such as tsbox.\nThe tibble format: the simplest and most familiar data format is of course the standard tibble/data frame, with or without an explicit time column/variable to indicate that the other variables vary with time. The standard tibble object is used by many packages, e.g. timetk & modeltime.\nThe tsibble format: this is a new format for time series analysis. The special tsibble object (“time series tibble”) is used by fable, feasts and others from the tidyverts set of packages.\n\nThere are many other time-oriented data formats too…probably too many, such a tibbletime and TimeSeries objects. For now the best way to deal with these, should you encounter them, is to convert them (Using the package tsbox) to a tibble or a tsibble and work with these.\n\n\nStandards\n\nTo start, we will use simple ts data first, and then do another with a “vanilla” tibble format that we can plot as is. We will then look at a tibbledata that does have a time-oriented variable. We will then perform conversion to tsibble format to plot it, and then a final example with a ground-up tsibble dataset.\n\n Base-R ts format data\nThere are a few datasets in base R that are in ts format already.\n\n\n R\n web-r\n\n\n\n\nAirPassengers\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1949 112 118 132 129 121 135 148 148 136 119 104 118\n1950 115 126 141 135 125 149 170 170 158 133 114 140\n1951 145 150 178 163 172 178 199 199 184 162 146 166\n1952 171 180 193 181 183 218 230 242 209 191 172 194\n1953 196 196 236 235 229 243 264 272 237 211 180 201\n1954 204 188 235 227 234 264 302 293 259 229 203 229\n1955 242 233 267 269 270 315 364 347 312 274 237 278\n1956 284 277 317 313 318 374 413 405 355 306 271 306\n1957 315 301 356 348 355 422 465 467 404 347 305 336\n1958 340 318 362 348 363 435 491 505 404 359 310 337\n1959 360 342 406 396 420 472 548 559 463 407 362 405\n1960 417 391 419 461 472 535 622 606 508 461 390 432\n\nstr(AirPassengers)\n\n Time-Series [1:144] from 1949 to 1961: 112 118 132 129 121 135 148 148 136 119 ...\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nThis can be easily plotted using base R:\n\n\n R\n web-r\n\n\n\n\n# Base R\nplot(AirPassengers)\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nOne can see that there is an upward trend and also seasonal variations that also increase over time. This is an example of a multiplicative time series, which we will discuss later.\nLet us take data that is “time oriented” but not in ts format. We use the command ts to convert a numeric vector to ts format: the syntax of ts() is:\nSyntax: objectName &lt;- ts(data, start, end, frequency), where,\n\n\ndata : represents the data vector\n\nstart : represents the first observation in time series\n\nend : represents the last observation in time series\n\nfrequency : represents number of observations per unit time. For example 1=annual, 4=quarterly, 12=monthly, 7=weekly, etc.\n\nWe will pick simple numerical vector data ( i.e. not a time series ) ChickWeight:\n\n\n R\n web-r\n\n\n\n\ndata(ChickWeight)\nstr(ChickWeight)\n\nClasses 'nfnGroupedData', 'nfGroupedData', 'groupedData' and 'data.frame':  578 obs. of  4 variables:\n $ weight: num  42 51 59 64 76 93 106 125 149 171 ...\n $ Time  : num  0 2 4 6 8 10 12 14 16 18 ...\n $ Chick : Ord.factor w/ 50 levels \"18\"&lt;\"16\"&lt;\"15\"&lt;..: 15 15 15 15 15 15 15 15 15 15 ...\n $ Diet  : Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 1 1 1 1 1 1 1 1 1 1 ...\n - attr(*, \"formula\")=Class 'formula'  language weight ~ Time | Chick\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_EmptyEnv&gt; \n - attr(*, \"outer\")=Class 'formula'  language ~Diet\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_EmptyEnv&gt; \n - attr(*, \"labels\")=List of 2\n  ..$ x: chr \"Time\"\n  ..$ y: chr \"Body weight\"\n - attr(*, \"units\")=List of 2\n  ..$ x: chr \"(days)\"\n  ..$ y: chr \"(gm)\"\n\nhead(ChickWeight)\n\n\n  \n\n\n\n\n# Filter for Chick #1 and for Diet #1\nChickWeight_ts &lt;- ChickWeight %&gt;% \n  dplyr::filter(Chick == 1, Diet ==1) %&gt;% \n  dplyr::select(weight, Time)\n\n## stats::ts does not accept pipe format\nChickWeight_ts &lt;- stats::ts(ChickWeight_ts$weight, \n                            frequency = 2) \nstr(ChickWeight_ts)\n\n Time-Series [1:12] from 1 to 6.5: 42 51 59 64 76 93 106 125 149 171 ...\n\n\n\nplot(ChickWeight_ts) # Using base-R\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nWe see that the weights of a young chick specimen #1 increases over time.\n\ntibble data\nThe ts data format can handle only one time series; in the above example, we could not have plotted the weight of two chicks, if we had wanted to. If we want to plot/analyze multiple time series, based on say Qualitative variables, (e.g. sales figures over time across multiple products and locations) we need other data formats. Using the familiar tibble structure opens up new possibilities.\n\nWe can have multiple time series within a tibble (think of numerical time-series data like GDP, Population, Imports, Exports for multiple countries as with the gapminder1data we saw earlier).\nIt also allows for data processing with dplyr such as filtering and summarizing.\n\n\n\ngapminder data\n\n\n\n\n\ncountry\nyear\ngdpPercap\npop\nlifeExp\ncontinent\n\n\n\nAfghanistan\n1952\n779.4453\n8425333\n28.801\nAsia\n\n\nAfghanistan\n1957\n820.8530\n9240934\n30.332\nAsia\n\n\nAfghanistan\n1962\n853.1007\n10267083\n31.997\nAsia\n\n\nAfghanistan\n1967\n836.1971\n11537966\n34.020\nAsia\n\n\nAfghanistan\n1972\n739.9811\n13079460\n36.088\nAsia\n\n\n\n\n\n\nLet us read and inspect in the US births data from 2000 to 2014. Download this data by clicking on the icon below, and saving the downloaded file in a sub-folder called data inside your project.\n Download the US Births data \nRead this data in and inspect it.\n\n\n R\n web-r\n\n\n\n\nbirths_2000_2014 &lt;- read_csv(\"data/US_births_2000-2014_SSA.csv\")\nglimpse(births_2000_2014)\n\nRows: 5,479\nColumns: 5\n$ year          &lt;dbl&gt; 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 20…\n$ month         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ date_of_month &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ day_of_week   &lt;dbl&gt; 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3,…\n$ births        &lt;dbl&gt; 9083, 8006, 11363, 13032, 12558, 12466, 12516, 8934, 794…\n\ninspect(births_2000_2014)\n\n\nquantitative variables:  \n           name   class  min   Q1 median    Q3   max         mean          sd\n1          year numeric 2000 2003   2007  2011  2014  2006.999270    4.321085\n2         month numeric    1    4      7    10    12     6.522723    3.449075\n3 date_of_month numeric    1    8     16    23    31    15.730243    8.801151\n4   day_of_week numeric    1    2      4     6     7     3.999817    2.000502\n5        births numeric 5728 8740  12343 13082 16081 11350.068261 2325.821049\n     n missing\n1 5479       0\n2 5479       0\n3 5479       0\n4 5479       0\n5 5479       0\n\nskim(births_2000_2014)\n\n\nData summary\n\n\nName\nbirths_2000_2014\n\n\nNumber of rows\n5479\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nyear\n0\n1\n2007.00\n4.32\n2000\n2003\n2007\n2011\n2014\n▇▇▇▇▇\n\n\nmonth\n0\n1\n6.52\n3.45\n1\n4\n7\n10\n12\n▇▅▅▅▇\n\n\ndate_of_month\n0\n1\n15.73\n8.80\n1\n8\n16\n23\n31\n▇▇▇▇▆\n\n\nday_of_week\n0\n1\n4.00\n2.00\n1\n2\n4\n6\n7\n▇▃▃▃▇\n\n\nbirths\n0\n1\n11350.07\n2325.82\n5728\n8740\n12343\n13082\n16081\n▂▂▁▇▁\n\n\n\n\nbirths_2000_2014\n\n\n  \n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nThis is just a tibble containing a single data variable births that varies over time. All other variables, although depicting time, are numerical columns and not explicitly time columns. There are no Qualitative variables (yet!).\nPlotting tibble-oriented time data\n\n\nUsing ggformula\nUsing ggplot\n\n\n\nWe will now plot this using ggformula. Using the separate year/month/week and day_of_week / day_of_month columns, we can plot births over time, colouring by day_of_week, for example:\n# Set graph theme\ntheme_set(new = theme_custom())\n\n# grouping by day_of_week\nbirths_2000_2014 %&gt;% \n  gf_line(births ~ year, \n          group = ~ day_of_week, \n          color = ~ day_of_week) %&gt;% \n  gf_point(title = \"Births, By Day of Week\", \n           subtitle = \"Over the Years\") %&gt;% \n  gf_theme(scale_colour_distiller(palette = \"Paired\")) \n# Grouping by date_of_month\nbirths_2000_2014 %&gt;% \n  gf_line(births ~ year, \n          group = ~ date_of_month, \n          color = ~ date_of_month) %&gt;% \n  gf_point(title = \"Births, By Date of Month\",\n           subtitle = \"Over the Years\") %&gt;% \n  gf_theme(scale_colour_distiller(palette = \"Paired\")) \n\n\n\n\n\n\n\n\n\n\nNot particularly illuminating. This is because the data is daily and we have considerable variation over time, and here we have too much data to visualize.\nSummaries will help, so we could calculate the the mean births per month in each year and plot that:\n# Set graph theme\ntheme_set(new = theme_custom())\n\nbirths_2000_2014_monthly &lt;- births_2000_2014 %&gt;% \n  \n# Convert month to factor/Qual variable!\n# So that we can have discrete colours for each month\n# Using base::factor()\n# Could use forcats::as_factor() also\n\n  mutate(month = base::factor(month, labels = month.abb)) %&gt;%\n\n# `month.abb` is a built-in dataset containing names of months.\n\n  group_by(year, month) %&gt;% \n  summarise(mean_monthly_births = mean(births, na.rm = TRUE))\nbirths_2000_2014_monthly\n####\nbirths_2000_2014_monthly %&gt;% \n##\n  gf_line(mean_monthly_births ~ year, \n          group = ~ month, \n          colour = ~ month, linewidth = 1) %&gt;% \n##\n  gf_point(size = 1.5, \n           title = \"Summaries of Monthly Births over the years\") %&gt;% \n  \n## palette for 12 colours\n  gf_theme(scale_colour_brewer(palette = \"Paired\")) \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThese are graphs for the same month each year: we have a January graph and a February graph and so on. So…average births per month were higher in all months during 2005 to 2007 and have dropped since.\n\n\nWe can do similar graphs using day_of_week as our basis for grouping, instead of month:\n# Set graph theme\ntheme_set(new = theme_custom())\n\nbirths_2000_2014_weekly &lt;- births_2000_2014 %&gt;% \n  mutate(day_of_week = base::factor(day_of_week,\n          levels = c(1,2,3,4,5,6,7), \n          labels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"))) %&gt;% \n  group_by(year, day_of_week) %&gt;% \n  summarise(mean_daily_births = mean(births, na.rm = TRUE))\n##\nbirths_2000_2014_weekly\n##\nbirths_2000_2014_weekly %&gt;%   \n  gf_line(mean_daily_births ~ year, \n          group = ~ day_of_week, \n          colour = ~ day_of_week, \n          linewidth = 1,\n          data = .) %&gt;% \n  gf_point(size = 2, title = \"Births over the Years by Day of Week\") %&gt;% \n  # palette for 12 colours\n  gf_theme(scale_colour_brewer(palette = \"Paired\")) \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\nWe will now plot this using ggplot for completeness. Using the separate year/month/week and day_of_week / day_of_month columns, we can plot births over time, colouring by day_of_week, for example:\n# Set graph theme\ntheme_set(new = theme_custom())\n\n# grouping by day_of_week\nbirths_2000_2014 %&gt;% \n  ggplot(aes(year, births,\n             group = day_of_week, \n             color = day_of_week)) + \n  geom_line() +  \n  geom_point() +\n  labs(title = \"Births, By Day of Week\",\n       subtitle = \"Over the Years\") + \n  scale_colour_distiller(palette = \"Paired\")\n##\n\n# Grouping by date_of_month\nbirths_2000_2014 %&gt;% \n  ggplot(aes(year,births,color = date_of_month,\n             group = date_of_month)) + \n  geom_line() + \n  geom_point() + \n  labs(title = \"Births, By Date of Month\",\n       subtitle = \"Over the Years\") + \n  scale_colour_distiller(palette = \"Paired\") \n\n\n\n\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nbirths_2000_2014_monthly &lt;- births_2000_2014 %&gt;% \n  \n# Convert month to factor/Qual variable!\n# So that we can have discrete colours for each month\n# Using base::factor()\n# Could use forcats::as_factor() also\n  mutate(month = base::factor(month, labels = month.abb)) %&gt;%\n# `month.abb` is a built-in dataset containing names of months.\n\n  group_by(year, month) %&gt;% \n  summarise(mean_monthly_births = mean(births, na.rm = TRUE))\nbirths_2000_2014_monthly\nbirths_2000_2014_monthly %&gt;% \n  ggplot(aes(year, mean_monthly_births,\n             group = month, \n             colour = month)) + \n  geom_line(linewidth = 1) + \n  geom_point(size = 1.5) + \n  labs(title = \"Summaries of Monthly Births over the years\") + \n    \n  # palette for 12 colours\n  scale_colour_brewer(palette = \"Paired\") \n\n\n\n\n  \n\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nbirths_2000_2014_weekly &lt;- births_2000_2014 %&gt;% \n  mutate(day_of_week = base::factor(day_of_week,\n          levels = c(1,2,3,4,5,6,7), \n          labels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"))) %&gt;% \n  group_by(year, day_of_week) %&gt;% \n  summarise(mean_daily_births = mean(births, na.rm = TRUE))\nbirths_2000_2014_weekly\nbirths_2000_2014_weekly %&gt;%\n  ggplot(aes(year, mean_daily_births, \n             group = day_of_week,\n             colour = day_of_week)) + \n  geom_line(linewidth = 1) + \n  geom_point(size = 2) + \n    \n  # palette for 12 colours\n  scale_colour_brewer(palette = \"Paired\") +  \n  labs(title = \"Births over the Years by Day of Week\")\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n Small Multiples using gghighlight\nInstead of looking at multiple overlapping time series graphs, we could split these up into small multiples or facets and still retain the overall picture that is offered by the overlapping graphs. The trick here is the highlight one of the graphs at a time, while keeping all other graphs in the background. We can do this with the gghighlight package.\n\n#library(gghighlight)\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nbirths_2000_2014_monthly\n\n\n  \n\n\n###\nbirths_2000_2014_monthly %&gt;% ggplot() + \n  geom_line(aes(y = mean_monthly_births,x = year, group = month)) +\n  labs(x = \"Year\", y  =\"Mean Monthly Births over the Years\", \n       title = \"Mean Births by Month\", \n       caption = \"Using gghighlight package\") +\n\n### Add highlighting\n  gghighlight(\n    use_direct_label = F,\n    unhighlighted_params = list(colour = alpha(\"grey85\", 1))) +\n  \n### Add faceting\n  facet_wrap(vars(month))\n\n\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nbirths_2000_2014_weekly\n\n\n  \n\n\n###\nbirths_2000_2014_weekly %&gt;% ggplot() + \n  geom_line(aes(y = mean_daily_births, x = year, group = day_of_week)) +\n  labs(x = \"Year\", y  =\"Mean Daily Births over the Years\", \n       title = \"Mean Births by Day of Week\", \n       caption = \"Using gghighlight package\") +\n\n### Add highlighting\n  gghighlight(\n    use_direct_label = F,\n    unhighlighted_params = list(colour = alpha(\"grey85\", 1))) +\n  \n### Add faceting\n  facet_wrap(vars(day_of_week))\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy are fewer babies born on weekends?\n\n\n\nLooks like an interesting story here…there are significantly fewer births on average on Sat and Sun, over the years! Why? Should we watch Grey’s Anatomy ?\nAnd more births in September? That should be a no-brainer!! 😄\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that this is still using just tibble data, without converting it into a time series format. So far we are simply treating the year/month/day variables are simple variables and using dplyr to group and summarize. We have not created an explicit time or date variable.\n\n\nPlotting tibble time-series\nNow, we can convert the time-oriented columns in this dataset into a single date variable, giving us a proper tibble time-series:\n\nbirths_tibble_timeseries &lt;- \n  births_2000_2014 %&gt;%\n  mutate(date = lubridate::make_date( year, month, date_of_month)) %&gt;%\n\n## Drop off the individual columns ( year, month, day_of_month)\n  select(date, births)\n\nbirths_tibble_timeseries\n\n\n  \n\n\n\nNote that we have a proper date formatted column, as desired. This is a single time series, but if we had other Qualitative variables such as say city, we could easily have had multiple series here. We can plot this with ggformula/ggplot as we have done before, and with now with timetk:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nbirths_tibble_timeseries %&gt;%\n  gf_line(births ~ date)\n\nbirths_tibble_timeseries %&gt;%\n  ggplot(aes(date, births)) + geom_line()\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nbirths_tibble_timeseries %&gt;%\n  timetk::plot_time_series(.date_var = date, \n                           .value = births, \n                           .interactive = FALSE, \n                           .title = \"Births over Time\",\n                           .x_lab = \"Time\",\n                           .y_lab = \"Births\")\n\n\n\n\n\n\n\n\ntsibble data\nFinally, we have tsibble (“time series tibble”) format data, which contains three main components:\n\nan index variable that defines time;\na set of key variables, usually categorical, that define sets of observations, over time. This allows for each combination of the categorical variables to define a separate time series.\na set of quantitative variables, that represent the quantities that vary with time (i.e index)\n\nHere is Robert Hyndman’s video introducing tsibbles:\n\nThe package tsibbledata contains several ready made tsibble format data.  Let us try PBS, which is a dataset containing Monthly Medicare prescription data in Australia.Run data(package = \"tsibbledata\") in your Console to find out about these.\n\ndata(PBS, package = \"tsibbledata\")\nPBS\n\n\n  \n\n\nglimpse(PBS)\n\nRows: 67,596\nColumns: 9\nKey: Concession, Type, ATC1, ATC2 [336]\n$ Month      &lt;mth&gt; 1991 Jul, 1991 Aug, 1991 Sep, 1991 Oct, 1991 Nov, 1991 Dec,…\n$ Concession &lt;chr&gt; \"Concessional\", \"Concessional\", \"Concessional\", \"Concession…\n$ Type       &lt;chr&gt; \"Co-payments\", \"Co-payments\", \"Co-payments\", \"Co-payments\",…\n$ ATC1       &lt;chr&gt; \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\",…\n$ ATC1_desc  &lt;chr&gt; \"Alimentary tract and metabolism\", \"Alimentary tract and me…\n$ ATC2       &lt;chr&gt; \"A01\", \"A01\", \"A01\", \"A01\", \"A01\", \"A01\", \"A01\", \"A01\", \"A0…\n$ ATC2_desc  &lt;chr&gt; \"STOMATOLOGICAL PREPARATIONS\", \"STOMATOLOGICAL PREPARATIONS…\n$ Scripts    &lt;dbl&gt; 18228, 15327, 14775, 15380, 14371, 15028, 11040, 15165, 168…\n$ Cost       &lt;dbl&gt; 67877.00, 57011.00, 55020.00, 57222.00, 52120.00, 54299.00,…\n\n\nData Description: This is a large-ish dataset:Run PBS in your console\n\n67K observations\n336 combinations of key variables (Concession, Type, ATC1, ATC2) which are categorical, as foreseen.\nData appears to be monthly, as indicated by the 1M.\nthe time index variable is called Month, formatted as yearmonth, a new type of variable introduced in the tsibble package.\n\nNote that there are multiple Quantitative variables (Scripts,Cost), each sliced into 336 time-series, a feature which is not supported in the ts format, but is supported in a tsibble. The Qualitative Variables are described below. Type help(\"PBS\") in your Console.\nThe data is dis-aggregated/grouped using four keys:\n- Concession: Concessional scripts are given to pensioners, unemployed, dependents, and other card holders\n- Type: Co-payments are made until an individual’s script expenditure hits a threshold ($290.00 for concession, $1141.80 otherwise). Safety net subsidies are provided to individuals exceeding this amount.\n- ATC1: Anatomical Therapeutic Chemical index (level 1). 15 types\n- ATC2: Anatomical Therapeutic Chemical index (level 2). 84 types, nested inside ATC1.\nLet us simply plot Cost over time:\n\n\nUsing ggformula\nUsing ggplot\nUsing timetk\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nPBS %&gt;%\n  gf_point(Cost ~ Month, data = .) %&gt;% \n  gf_line(title = \"PBS Costs vs time\", caption = \"ggformula\") \n\n\n\n\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nPBS %&gt;% \n  ggplot(aes(Month, Cost)) + \n  geom_point() + \n  geom_line() + \n  labs(title = \"PBS Costs vs time\", caption = \"ggplot\")\n\n\n\n\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nPBS %&gt;%\n  timetk::plot_time_series(.date_var = Month, .value = Cost, \n                           .interactive = FALSE,\n                           .smooth = FALSE)\n\n\n\n\n\n\n\n\n\n\nThis basic plot is quite messy. Other than an overall rising trend and more vigorous variations pointing to a multiplicative process, we cannot say more. There is simply too much happening here and it is now time (sic!) for us to look at summaries of the data using dplyr-like verbs.\nWe will do that in the Section 1.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🕔 Time Series"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#time-series-heatmaps",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#time-series-heatmaps",
    "title": "🕔 Time Series",
    "section": "\n Time Series Heatmaps",
    "text": "Time Series Heatmaps\nHow about a heatmap? We can cook up a categorical variable based on the number of births (low, fine, high) and use that to create a heatmap:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nbirths_2000_2014 %&gt;%\n  mutate(birthrate = case_when(births &gt;= 10000 ~ \"high\",\n                               births &lt;= 8000 ~ \"low\",\n                               TRUE ~ \"fine\")) %&gt;%\n  mutate(birthrate = base::factor(birthrate, \n                                  labels = c(\"high\", \"fine\", \"low\"), \n                                  ordered = TRUE)) %&gt;%\n  \n  gf_tile(\n    data = .,\n    year ~ month,\n    fill = ~ birthrate,\n    color = \"black\"\n  ) %&gt;%\n  \n  gf_theme(scale_x_time(\n    breaks = 1:12,\n    labels = c(\"Jan\",\"Feb\",\"Mar\",\"Apr\",\n               \"May\",\"Jun\",\"Jul\",\"Aug\",\n               \"Sep\",\"Oct\",\"Nov\",\"Dec\")\n  )) %&gt;%\n  \n  gf_theme(scale_fill_brewer(type = \"qual\", palette = \"OrRd\", direction = -1))",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🕔 Time Series"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#conclusion",
    "title": "🕔 Time Series",
    "section": "\n Conclusion",
    "text": "Conclusion\nWe have seen a good few data formats for time series, and how to work with them and plot them.\nIn the Tutorial Section 1, we will explore:\n\nwrangling with Time series to produce grouped and filtered aggregates/summaries and plots with these\nhow to decompose time series into periodic and aperiodic components, which can be used to make business decisions.\nProducing Interactive Plots for Time Series\n\nmodelling and forecasting of time series.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🕔 Time Series"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#your-turn",
    "title": "🕔 Time Series",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nChoose some of the datasets in the tsdl and in the tsibbledata packages. (Install and load them first! )Plot basic, filtered and model-based graphs for these and interpret.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🕔 Time Series"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#references",
    "title": "🕔 Time Series",
    "section": "\n References",
    "text": "References\n\nRobert Hyndman, Forecasting: Principles and Practice (Third Edition).available online\n\n\nTime Series Analysis at Our Coding Club\n\n\nThe Nuclear Threat—The Shadow Peace, part 1\n\n\n11 Ways to Visualize Changes Over Time – A Guide\n\n\nWhat is seasonal adjustment and why is it used?\n\n\nThe start-at-zero rule\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nfpp3\n0.5\nHyndman (2023)\n\n\ngghighlight\n0.4.1\nYutani (2023)\n\n\ntimetk\n2.9.0\nDancho and Vaughan (2023)\n\n\ntsbox\n0.4.1\nSax (2021)\n\n\ntsdl\n0.1.0\nHyndman and Yang (2024)\n\n\ntsibble\n1.1.4\nWang, Cook, and Hyndman (2020)\n\n\ntsibbledata\n0.4.1\nO’Hara-Wild et al. (2022)\n\n\nTSstudio\n0.1.7\nKrispin (2023)\n\n\n\n\n\n\nDancho, Matt, and Davis Vaughan. 2023. timetk: A Tool Kit for Working with Time Series. https://CRAN.R-project.org/package=timetk.\n\n\nHyndman, Rob. 2023. Fpp3: Data for “Forecasting: Principles and Practice” (3rd Edition). https://CRAN.R-project.org/package=fpp3.\n\n\nHyndman, Rob, and Yangzhuoran Yang. 2024. tsdl: Time Series Data Library. https://github.com/FinYang/tsdl.\n\n\nKrispin, Rami. 2023. TSstudio: Functions for Time Series Analysis and Forecasting. https://CRAN.R-project.org/package=TSstudio.\n\n\nO’Hara-Wild, Mitchell, Rob Hyndman, Earo Wang, and Rakshitha Godahewa. 2022. tsibbledata: Diverse Datasets for “tsibble”. https://CRAN.R-project.org/package=tsibbledata.\n\n\nSax, Christoph. 2021. tsbox: Class-Agnostic Time Series in in R. https://docs.ropensci.org/tsbox/.\n\n\nWang, Earo, Dianne Cook, and Rob J Hyndman. 2020. “A New Tidy Data Structure to Support Exploration and Modeling of Temporal Data.” Journal of Computational and Graphical Statistics 29 (3): 466–78. https://doi.org/10.1080/10618600.2019.1695624.\n\n\nYutani, Hiroaki. 2023. gghighlight: Highlight Lines and Points in “ggplot2”. https://CRAN.R-project.org/package=gghighlight.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🕔 Time Series"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/time-v2.html#footnotes",
    "title": "🕔 Time Series",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://www.gapminder.org/data/↩︎",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🕔 Time Series"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html",
    "title": "🕔 Time Series Wrangling",
    "section": "",
    "text": "This tutorial uses web-r that allows you to run all code within your browser, on all devices. Most code chunks herein are formatted in a tabbed structure (like in an old-fashioned library), with duplicated code. The tabs in front have regular R code that will work when copy-pasted in your RStudio session. The tab “behind” has the web-R code that can work directly in your browser, and can be modified as well. The R code is also there to make sure you have original code to go back to, when you have made several modifications to the code on the web-r tabs and need to compare your code with the original! If you have messed up the code there, then you can hit the “recycle” button on the web-r tab to go back to the original!\n\n\nRun selected code using either:\n\nmacOS: ⌘ + ↩︎/Return\n\nWindows/Linux: Ctrl + ↩︎/Enter\n\n\n\nRun the entire code by clicking the “Run code” button or pressing Shift+↩︎."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#using-web-r",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#using-web-r",
    "title": "🕔 Time Series Wrangling",
    "section": "",
    "text": "This tutorial uses web-r that allows you to run all code within your browser, on all devices. Most code chunks herein are formatted in a tabbed structure (like in an old-fashioned library), with duplicated code. The tabs in front have regular R code that will work when copy-pasted in your RStudio session. The tab “behind” has the web-R code that can work directly in your browser, and can be modified as well. The R code is also there to make sure you have original code to go back to, when you have made several modifications to the code on the web-r tabs and need to compare your code with the original! If you have messed up the code there, then you can hit the “recycle” button on the web-r tab to go back to the original!\n\n\nRun selected code using either:\n\nmacOS: ⌘ + ↩︎/Return\n\nWindows/Linux: Ctrl + ↩︎/Enter\n\n\n\nRun the entire code by clicking the “Run code” button or pressing Shift+↩︎."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#setting-up-r-packages",
    "title": "🕔 Time Series Wrangling",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(ggformula) # Our Formula based graphing package\nlibrary(scales) # Some nice time-oriented scales in graphs!\nlibrary(tsibble)\nlibrary(timetk)\n\n# Datasets\nlibrary(tsibbledata)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#introduction",
    "title": "🕔 Time Series Wrangling",
    "section": "\n Introduction",
    "text": "Introduction\nWe have now arrived at the need to start from raw, multiple time series data and filter, group, and summarize these time series grasp their meaning, a process known as “wrangling”.\n\n\n\n\n\n\nWrangling with dplyr\n\n\n\nThe tutorial for wrangling using dplyr is here.\n\n\nHere, we will first use the births data we encountered earlier which had a single time series, and then proceed to a more complex example which has multiple time-series."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#time-series-wrangling",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#time-series-wrangling",
    "title": "🕔 Time Series Wrangling",
    "section": "\n Time-Series Wrangling",
    "text": "Time-Series Wrangling\nWe can do this in two ways, and with two packages:\n\n\n\n\n\n\nTwo Wrangling “Dimensions”\n\n\n\nFor all the above operations, we can either use time variable as the basis, by filtering for specific periods, or computing summaries over larger intervals of time e.g. month, quarter, year;\nAND/OR\nWe can do the same over space variables, i.e. the Qualitative variables that define individual time series, and based on which we can filter and and analyze these specific time series. Each unique setting of these Qualitative variables could potentially define a time series! There are 336 groups/combinations of them in PBS, but not all are unique timeseries, since some of the Qual variables are nested inside others, e.g ATC1_desc provides more info on each value of ATC1 and is not truly a separate Qual variable.\n\n\nAnd the packages are:\n\n\n\n\n\n\ntsibble has dplyr-like functions\n\n\n\nUsing tsibble data, the tsibble package has specialized filter and group_by functions to do with the index (i.e time) variable and the key variables, such as index_by() and group_by_key().\n(Filtering based on Qual variables can be done with dplyr. We can use dplyr functions such as group_by, mutate(), filter(), select() and summarise() to work with tsibble objects.)\n\n\n\n\n\n\n\n\ntimetk also has dplyr-like functions!\n\n\n\nUsing tibbles, timetk provides functions such as summarize_by_time, filter_by_time and slidify that are quite powerful. Again, as with tsibble, dplyr can always be used for other Qual variables (i.e non-time)."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#case-study-1-births-dataset",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#case-study-1-births-dataset",
    "title": "🕔 Time Series Wrangling",
    "section": "\n Case Study #1: Births Dataset",
    "text": "Case Study #1: Births Dataset\nAs a second example let us read and inspect in the now familiar US births data from 2000 to 2014. Download this data by clicking on the icon below, and saving the downloaded file in a sub-folder called data inside your project.\n Download the US Births data \n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n R\n web-r\n\n\n\n\n# Step1: Read the data\nbirths_2000_2014 &lt;- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/births/US_births_2000-2014_SSA.csv\")\n\n\n# Step2: Convert year + month + date_of_month to \"date\"\nbirths_timeseries &lt;- \n  births_2000_2014 %&gt;% \n  mutate(date = lubridate::make_date(year = year,\n                                     month = month,\n                                     day = date_of_month)) %&gt;%\n  select(date, births)\n\nbirths_timeseries\n\n\n  \n\n\nclass(births_timeseries)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n\n# Step3: Convert to tsibble\n# combine the year/month/date_of_month columns into a date\n# drop them thereafter\nbirths_tsibble &lt;- \n  births_2000_2014 %&gt;%\n  mutate(index = lubridate::make_date(year = year, \n                                      month = month,\n                                      day = date_of_month)) %&gt;%\n  tsibble::as_tsibble(index = index) %&gt;%\n  select(index, births)\n\nbirths_tsibble\n\n\n  \n\n\nclass(births_tsibble)\n\n[1] \"tbl_ts\"     \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nBoth data frames look identical, except for data class difference. This is DAILY data of course.\nLet us say we want to group by month and plot mean monthly births as before, but now using tsibble and timetk:\n\n\n R\n web-r\n\n\n\nLet us try a basic plot with both tsibble vs timetk packages.\n#column: body-outset-right\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nbirths_tsibble %&gt;%\n  gf_line(births ~ index, \n          data = ., \n          title = \"Basic tsibble plotted with ggformula\") \n# timetk **can** plot tsibbles. \nbirths_tsibble %&gt;% \n  timetk::plot_time_series(.date_var = index,\n                           .value = births, .interactive = FALSE,\n                           .title = \"Tsibble Plotted with timetk\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nWe can also group by month and get monthly aggregates to get another summary:\n\n\n R\n web-r\n\n\n\nHere we take Monthly Aggregates with both tsibble vs timetk:\n# column: body-outset-right\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nbirths_tsibble %&gt;% \n  tsibble::index_by(month_index = ~ tsibble::yearmonth(.)) %&gt;% \n  dplyr::summarise(mean_births = mean(births, na.rm = TRUE)) %&gt;% \n  gf_point(mean_births ~ month_index, \n           data = ., \n           title = \"Monthly Aggregate with tsibble + ggformula\") %&gt;% \n  gf_line() %&gt;% \n  gf_smooth(se = FALSE, method = \"loess\")  %&gt;% \n  gf_labs(x = \"Year\", y = \"Mean Monthly Births\")\nbirths_timeseries %&gt;% \n  \n  # cannot use tsibble here\n  # tsibble format cannot be summarized/wrangled by timetk\n\n  timetk::summarize_by_time(.date_var = date, \n                            .by = \"month\", \n                            month_mean = mean(births)) %&gt;% \n  timetk::plot_time_series(date, month_mean,\n                           .title = \"Monthly aggregate births with timetk\",\n                           .interactive = FALSE,\n                           .x_lab = \"year\", \n                           .y_lab = \"Mean Monthly Births\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nApart from the bump during in 2006-2007, there are also seasonal trends that repeat each year, which we glimpsed earlier.\ntsibble vs timetk: Annual Aggregates\nLet us try getting annual aggregates.\n\n\n R\n web-r\n\n\n\n# column: body-outset-right\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nbirths_tsibble %&gt;% \n  tsibble::index_by(year_index = ~ lubridate::year(.)) %&gt;% \n  ## tsibble does not have a \"year\" function? So using lubridate..\n  ## Summarize\n  dplyr::summarise(mean_births = mean(births, na.rm = TRUE)) %&gt;%\n  ##Plot\n  gf_point(mean_births ~ year_index, data = .) %&gt;% \n  gf_line() %&gt;% \n  gf_smooth(se = FALSE, method = \"loess\")\n####\nbirths_timeseries %&gt;%\n  ## Summarize\n  timetk::summarise_by_time(.date_var = date, \n                            .by = \"year\", \n                            mean = mean(births)) %&gt;% \n  ## Plot\n  timetk::plot_time_series(date, mean,\n                           .title = \"Yearly aggregate births with timetk\",\n                           .interactive = FALSE,\n                           .x_lab = \"year\", \n                           .y_lab = \"Mean Yearly Births\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n A small detour\nAh yes….errors. There is a curious interplay between dplyr and tsibble…they play together but not all the time, it would seem.\nThe original births tibble dataset allows dplyr:group_by + summarize:\n\n# The original dataset allows dplyr:group_by + summarize\nbirths_2000_2014 %&gt;% \n  dplyr::group_by(year) %&gt;% \n  summarise(mean_births = mean(births, na.rm = TRUE))\n\n\n  \n\n\n\nHowever, tsibble-converted data does not quite work with dplyr::group_by+summarize:\n\n```{r}\n#| label: Errors-2\n#| eval: false\n\n# This code will not work\nbirths_tsibble %&gt;% \n# Grouping does not work. Here is the problem\n  dplyr::group_by(index) %&gt;% \n\n# Trying to get Annual Birth Average as before\n# Should give 15 rows, one per year, but does not!\n  summarise(mean_births = mean(births, na.rm = TRUE)) \n```\n\nEven if we pull out the year information in index, it gives confusing results…\n\nbirths_tsibble %&gt;% \n# All right, try to pull the year info from `index` then\n  mutate(dplyr_year = lubridate::year(index)) %&gt;% \n# Grouping does not work\n  dplyr::group_by(dplyr_year) %&gt;% \n\n# Trying to get Annual Birth Average as before\n# Should give 15 rows, one per year, but does not!\n  summarise(mean_births = mean(births, na.rm = TRUE)) \n\n\n  \n\n\n\nThis grouping does not give a proper result (though it does show 15 groups.)\nUsing tsibble::index_by() and then dplyr::summarize() does the trick…so all right. The index_by() operation is different from that of dplyr::group_by()!\n\n# tsibble works with index_by + summarize\n# 15 rows, one for each year\nbirths_tsibble %&gt;% \n  # tsibble can get year info from index\n  tsibble::index_by(year_date = year(index)) %&gt;% \n  dplyr::summarise(mean_births = mean(births, na.rm = TRUE))"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#candle-stick-plots",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#candle-stick-plots",
    "title": "🕔 Time Series Wrangling",
    "section": "\n Candle-Stick Plots",
    "text": "Candle-Stick Plots\nHmm…can we try to plot boxplots over time (Candle-Stick Plots)? Over month, quarter or year?\n\n Monthly Box Plots\n\n\n R\n web-r\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nbirths_tsibble %&gt;%\n  index_by(month_index = ~ yearmonth(.)) %&gt;%\n  # 15 years\n  # No need to summarise, since we want boxplots per year / month\n  # Plot the groups\n  # 180 plots!!\n  gf_boxplot(births ~ index, group =  ~ month_index,\n             fill = ~ month_index,\n             data = ., \n             title = \"Boxplots of Births by Month\",\n             caption = \"tsibble + ggformula\") \n####\nbirths_tsibble %&gt;% # Can try births_timeseries too\n  timetk::plot_time_series_boxplot(index, births, .period = \"month\",\n                                   .plotly_slider = TRUE,\n                                   .title = \"Boxplots of Births by Month\",\n                                   .interactive = TRUE,\n                                   .x_lab = \"year\", \n                                   .y_lab = \"Mean Monthly Births\"\n                                   )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nWe see 180 boxplots…yes this is still too busy a plot for us to learn much from."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#quarterly-boxplots",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#quarterly-boxplots",
    "title": "🕔 Time Series Wrangling",
    "section": "\n Quarterly boxplots",
    "text": "Quarterly boxplots\n\n\n R\n web-r\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nbirths_tsibble %&gt;%\n  index_by(qrtr_index = ~ yearquarter(.)) %&gt;% # 60 quarters over 15 years\n  # No need to summarise, since we want boxplots per year / month\n  gf_boxplot(births ~ index, \n             group = ~ qrtr_index,\n             fill = ~ qrtr_index,\n             data = .) # 60 plots!!\n\n\n\n\n\n\n###\nbirths_tsibble %&gt;% # Can try births_timeseries too\n  timetk::plot_time_series_boxplot(\n                            index, births, .period = \"quarter\",\n                           .title = \"Quarterly births with timetk\",\n                           .interactive = TRUE,\n                           .plotly_slider = TRUE,\n                           .x_lab = \"year\",\n                           .y_lab = \"Mean Monthly Births\")\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nWe have 60 boxplots…over a period of 15 years, one box plot per quarter…\n\n Yearwise boxplots\n\n\n R\n web-r\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nbirths_tsibble %&gt;% \n  index_by(year_index = ~ lubridate::year(.)) %&gt;% # 15 years, 15 groups\n  # No need to summarise, since we want boxplots per year / month\n\n  gf_boxplot(births ~ index, \n              group = ~ year_index, \n              fill = ~ year_index, \n             data = .) %&gt;%  # plot the groups 15 plots\n  gf_theme(scale_fill_distiller(palette = \"Spectral\")) \n\n\n\n\n\n\n####\n\nbirths_tsibble %&gt;% \n  timetk::plot_time_series_boxplot(\n                            index, births, .period = \"year\",\n                           .title = \"Yearly aggregate births with timetk\",\n                           .interactive = TRUE,\n                           .plotly_slider = TRUE,\n                           .x_lab = \"year\",\n                           .y_lab = \"Births\")\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nThis looks much better…We can more easily see that 2006-2009 the births were somewhat higher, because the medians in these years are the highest."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#case-study-2-pbs-dataset",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#case-study-2-pbs-dataset",
    "title": "🕔 Time Series Wrangling",
    "section": "\n Case Study #2: PBS Dataset",
    "text": "Case Study #2: PBS Dataset\nWe previously encountered the PBS dataset from the tsibbledata package earlier, which is a dataset containing Monthly Medicare prescription data in Australia. We will resume from there:\n\n\n R\n web-r\n\n\n\n\ndata(\"PBS\", package = \"tsibbledata\")\nPBS\n\n\n  \n\n\nglimpse(PBS)\n\nRows: 67,596\nColumns: 9\nKey: Concession, Type, ATC1, ATC2 [336]\n$ Month      &lt;mth&gt; 1991 Jul, 1991 Aug, 1991 Sep, 1991 Oct, 1991 Nov, 1991 Dec,…\n$ Concession &lt;chr&gt; \"Concessional\", \"Concessional\", \"Concessional\", \"Concession…\n$ Type       &lt;chr&gt; \"Co-payments\", \"Co-payments\", \"Co-payments\", \"Co-payments\",…\n$ ATC1       &lt;chr&gt; \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\",…\n$ ATC1_desc  &lt;chr&gt; \"Alimentary tract and metabolism\", \"Alimentary tract and me…\n$ ATC2       &lt;chr&gt; \"A01\", \"A01\", \"A01\", \"A01\", \"A01\", \"A01\", \"A01\", \"A01\", \"A0…\n$ ATC2_desc  &lt;chr&gt; \"STOMATOLOGICAL PREPARATIONS\", \"STOMATOLOGICAL PREPARATIONS…\n$ Scripts    &lt;dbl&gt; 18228, 15327, 14775, 15380, 14371, 15028, 11040, 15165, 168…\n$ Cost       &lt;dbl&gt; 67877.00, 57011.00, 55020.00, 57222.00, 52120.00, 54299.00,…\n\n# inspect(PBS) # does not work since mosaic cannot handle tsibbles\n# skimr::skim(PBS) # does not work, need to investigate\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n Counts by Qual variables\nLet us first see how many observations there are for each combo of keys:\n\n\n R\n web-r\n\n\n\n\n## Types\nPBS %&gt;% \n  dplyr::count(Type) # 2 Types\n\n\n  \n\n\n## Concessions\nPBS %&gt;% count(Concession) # 2 Types\n\n\n  \n\n\n## ATC1\nPBS %&gt;% count(ATC1) # 15 ATC1 groups\n\n\n  \n\n\n## ATC2\nPBS %&gt;% count(ATC2) # 84 ATC2 groups\n\n\n  \n\n\n# dplyr grouping with ATC1 and ATC2\nPBS %&gt;% \n  dplyr::group_by(ATC1, ATC2) %&gt;% \n  count() # Still 84; ATC2 is nested in ATC1\n\n\n  \n\n\n## All possible groups\nPBS %&gt;% \n  group_by(ATC1, ATC2, Concession, Type) %&gt;% \n  count() # 336 overall groups\n\n\n  \n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nBusiness Insights from inspecting PBS\n\n\n\nThis is a large-ish dataset:\n\n67K observations\nQuant Variables: Two Quant variables (Scripts and Cost)\nTime Variable:\n\nData appears to be monthly, as indicated by the 1M.\nthe time index variable is called Month\n\nformatted as yearmonth, a new type of variable introduced in the tsibble package. yearmonth does not show in glimpse output!\n\n\nQual variables:\n\n\nConcession: Concessional and General (Concessional scripts are given to pensioners, unemployed, dependents, and other card holders)\n\nType: Co-payments and Safety Net\n\n\nATC1: Anatomical Therapeutic Chemical index (level 1).\n\n15 types\n\n\n\n\n\nATC2: Anatomical Therapeutic Chemical index (level 2).\n\n84 types, nested inside ATC1.\n\n\n\n\n\nRun PBS in your console\nWe will start with the familiar basic messy plot and work our way towards filtering, summaries, and averages.\n\n\n R\n web-r\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nPBS %&gt;% \n  gf_point(Cost ~ Month, data = .) %&gt;% \n  gf_line(title = \"PBS Costs vs time\", caption = \"ggformula\")\n####\nPBS %&gt;% ggplot(aes(Month, Cost)) + \n  geom_point() + \n  geom_line() + \n  labs(title = \"PBS Costs vs time\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nAs noted earlier, this basic plot is quite messy. Other than an overall rising trend and more vigorous variations pointing to a multiplicative process, we cannot say more. There is simply too much happening here and it is now time (sic!) for us to look at summaries of the data using dplyr-like verbs. We will perform summaries with tsibble and plots with ggformula first. Then we will use timetk to perform both operations.\n\n\n R\n web-r\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\n# Costs variable for a specific combo of Qual variables(keys)\nPBS %&gt;% \n  dplyr::filter(Concession == \"General\", \n                ATC1 == \"A\") %&gt;% \n  gf_line(Cost ~ Month, \n          colour = ~ Type, \n          data = .) %&gt;% \n  gf_point(title = \"Costs per Month for General A category patients\") %&gt;%\n  gf_refine(scale_y_continuous(labels = scales::label_comma()))\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\nAs can be seen:\n\nstrongly seasonal for both Types of graphs;\nseasonal variation increasing over the years, a clear sign of a multiplicative time series, especially for Safety net.\nUpward trend with both types of subsidies, Safety net and Co-payments.\n\nCo-payments type have some kind of dip around the year 2000…\nBut this is still messy and overwhelming and we could certainly use some summaries/aggregates/averages.\n\n\n\nWe can now use tsibble’s dplyr-like commands to develop summaries by year, quarter, month(original data): Look carefully at the new time variable created each time, and the size the data frame decrease with each aggregation:\n\n\n R\n web-r\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\n# Cost Summary by Month, which is the original data\n# New Variable Name to make grouping visible\nPBS_month &lt;-  PBS %&gt;% \n  dplyr::filter(Concession == \"General\", \n                ATC1 == \"A\") %&gt;% \n  tsibble::index_by(Month_Date = Month) %&gt;% \n  dplyr::summarise(across(.cols = c(Cost, Scripts),\n                          .fn = mean,\n                          .names = \"mean_{.col}\"))\n\nPBS_month\n\n\n  \n\n\n###\nPBS_month %&gt;% \n  mutate(Month_Date = as_date(Month_Date)) %&gt;%\n  gf_line(mean_Cost ~ Month_Date) %&gt;%\n  gf_line(mean_Scripts ~ Month_Date, \n          title = \"Mean Costs and Scripts for General + A category\",\n          subtitle = \"Means over General + A category \") %&gt;%\n  gf_refine(scale_y_continuous(labels = scales::label_comma()))\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\nAs can be seen: To Be Written Up !!!\n\n\n\n\n R\n web-r\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\n# Cost Summary by Quarter\nPBS_quarter &lt;- \n  PBS %&gt;% \n  tsibble::index_by(Quarter_Date = yearquarter(Month)) %&gt;% # And the change here!\n  dplyr::summarise(across(.cols = c(Cost, Scripts),\n                          .fn = mean,\n                          .names = \"mean_{.col}\"))\nPBS_quarter\n\n\n  \n\n\n###\nPBS_quarter %&gt;% \n  gf_line(mean_Cost ~ Quarter_Date) %&gt;%\n  gf_refine(scale_y_continuous(labels = scales::label_comma()))\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\nAs can be seen: TBD\n\n\n\n\n R\n web-r\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\n# Cost Summary by Year\nPBS_year &lt;- PBS %&gt;% \n  index_by(Year_Date = year(Month)) %&gt;% # Note this change!!!\n  dplyr::summarise(across(.cols = c(Cost, Scripts),\n                          .fn = mean,\n                          .names = \"mean_{.col}\"))\n\nPBS_year\n\n\n  \n\n\nPBS_year %&gt;% \n  gf_line(mean_Cost ~ Year_Date) %&gt;%\n  gf_refine(scale_y_continuous(labels = scales::label_comma()))\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\nAs can be seen: TBD\n\n\nUsing timetk\n\n\n\n\n\n\n\nThe time variable for timetk\n\n\n\nThe PBS-derivedtsibbles have their “time-oriented” variables formatted asyearmonth,yearquarter and dbl, as seen. We need to mutate these into a proper date format for the timetk package to summarise them successfully. (Plotting a tsibble with timetk is possible, as seen earlier.)\n\n\n\n\n R\n web-r\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nPBS %&gt;% \n  mutate(Month_Date = lubridate::as_date(Month)) %&gt;%\n##\n  timetk::summarise_by_time(.date_var = Month_Date,\n                            .by = \"month\",\n                            mean_Cost = mean(Cost)) %&gt;%\n##\n  timetk::plot_time_series(.date_var = Month_Date, \n                           .value = mean_Cost,\n                           .interactive = FALSE,\n                           .x_lab = \"Time\", .y_lab = \"Costs\",\n                           .title = \"Mean Costs by Month\") + \n  labs(caption = \"Tsibble Plotted with timetk\")\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n R\n web-r\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nPBS %&gt;% \n  mutate(Month_Date = lubridate::as_date(Month)) %&gt;%\n  as_tibble() %&gt;%\n  ##\n  timetk::summarise_by_time(.date_var = Month_Date,\n                            .by = \"quarter\",\n                            mean_Cost = mean(Cost)) %&gt;%\n  ##\n  timetk::plot_time_series(.date_var = Month_Date, \n                           .value = mean_Cost,\n                           .interactive = FALSE,\n                           .x_lab = \"Time\", .y_lab = \"Costs\",\n                           .title = \"Mean Costs by Quarter\") + \n  labs(caption = \"Tsibble Plotted with timetk\")\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n R\n web-r\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nPBS %&gt;% \n  mutate(Month_Date = lubridate::as_date(Month)) %&gt;%\n  as_tibble() %&gt;%\n  ##\n  timetk::summarise_by_time(.date_var = Month_Date,\n                            .by = \"year\",\n                            mean_Cost = mean(Cost)) %&gt;%\n  ##\n  timetk::plot_time_series(.date_var = Month_Date, \n                           .value = mean_Cost,\n                           .interactive = FALSE,\n                           .x_lab = \"Time\", .y_lab = \"Costs\",\n                           .title = \"Mean Costs by Year\") + \n  labs(caption = \"Tsibble Plotted with timetk\")\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#conclusion",
    "title": "🕔 Time Series Wrangling",
    "section": "\n Conclusion",
    "text": "Conclusion\nWe have learnt how to filter, summarize and compute various aggregate metrics from them and to plot these. Both tsibble and timetk offer similar capability here."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#your-turn",
    "title": "🕔 Time Series Wrangling",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nChoose some of the datasets in the tsdl and in the tsibbledata packages. Plot basic, filtered and summarized graphs for these and interpret."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#references",
    "title": "🕔 Time Series Wrangling",
    "section": "\n References",
    "text": "References\n\nRobert Hyndman, Forecasting: Principles and Practice (Third Edition). available online\n\n\nTime Series Analysis at Our Coding Club\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\ngapminder\n1.0.0\n(gapminder?)\n\n\ntimetk\n2.9.0\n(timetk?)\n\n\ntsibble\n1.1.4\n(tsibble?)\n\n\ntsibbledata\n0.4.1\n(tsibbledata?)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/200-EDAWorkflow/eda-workflow.html",
    "href": "content/courses/Analytics/Descriptive/Modules/200-EDAWorkflow/eda-workflow.html",
    "title": "EDA Workflow",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(palmerpenguins)\nlibrary(ggformula)\nlibrary(ggridges)\nlibrary(skimr)\n\n###\nlibrary(GGally)\nlibrary(corrplot)\nlibrary(corrgram)\n\n\n# Set graph theme\nggplot2::theme_set(new = theme_classic(base_size = 14, \n                                       base_family = \"sans\"))",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "EDA Workflow"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/200-EDAWorkflow/eda-workflow.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/200-EDAWorkflow/eda-workflow.html#setting-up-r-packages",
    "title": "EDA Workflow",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(palmerpenguins)\nlibrary(ggformula)\nlibrary(ggridges)\nlibrary(skimr)\n\n###\nlibrary(GGally)\nlibrary(corrplot)\nlibrary(corrgram)\n\n\n# Set graph theme\nggplot2::theme_set(new = theme_classic(base_size = 14, \n                                       base_family = \"sans\"))",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "EDA Workflow"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html",
    "title": "📎 Correlations",
    "section": "",
    "text": "Tutorial   \n  R (Interactive Graphs",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📎 Correlations"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#sec-slides-and-tutorials",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#sec-slides-and-tutorials",
    "title": "📎 Correlations",
    "section": "",
    "text": "Tutorial   \n  R (Interactive Graphs",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📎 Correlations"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#setting-up-r-packages",
    "title": "📎 Correlations",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(tidyverse) # Tidy data processing and plotting\nlibrary(ggformula) # Formula based plots\nlibrary(mosaic) # Our go-to package\nlibrary(skimr) # Another Data inspection package\nlibrary(kableExtra) # Making good tables with data\n\nlibrary(GGally) # Corr plots\nlibrary(corrplot) # More corrplots\nlibrary(ggExtra) # Making Combination Plots\n\n#devtools::install_github(\"rpruim/Lock5withR\")\nlibrary(Lock5withR) # Datasets\nlibrary(palmerpenguins) # A famous dataset\n\nlibrary(easystats) # Easy Statistical Analysis and Charts\nlibrary(correlation) # Different Types of Correlations\n# From the easystats collection of packages",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📎 Correlations"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#what-is-correlation",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#what-is-correlation",
    "title": "📎 Correlations",
    "section": "\n What is Correlation?",
    "text": "What is Correlation?\nOne of the basic Questions we would have of our data is: Does some variable depend upon another in some way? Does \\(y\\) vary with \\(x\\)? A Correlation Test is designed to answer exactly this question.\nThe word correlation is used in everyday life to denote some form of association. We might say that we have noticed a correlation between rainy days and reduced sales at supermarkets. However, in statistical terms we use correlation to denote association between two quantitative variables. We also assume that the association is linear, that one variable increases or decreases a fixed amount for a unit increase or decrease in the other. The other technique that is often used in these circumstances is regression, which involves estimating the best straight line to summarise the association.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📎 Correlations"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#pearson-correlation-coefficient",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#pearson-correlation-coefficient",
    "title": "📎 Correlations",
    "section": "\n Pearson Correlation coefficient",
    "text": "Pearson Correlation coefficient\nThe degree of association is measured by a correlation coefficient, denoted by r. It is sometimes called Pearson’s correlation coefficient after its originator and is a measure of linear association. (If a curved line is needed to express the relationship, other and more complicated measures of the correlation must be used.)\nThe correlation coefficient is measured on a scale that varies from + 1 through 0 to – 1. Complete correlation between two variables is expressed by either + 1 or -1. When one variable increases as the other increases the correlation is positive; when one decreases as the other increases it is negative.\nIn formal terms, the correlation between two variables \\(x\\) and \\(y\\) is defined as\n\\[\n\\rho = E\\left[\\frac{(x - \\mu_{x}) * (y - \\mu_{y})}{(\\sigma_x)*(\\sigma_y)}\\right]\n\\]\nwhere \\(E\\) is the expectation operator ( i.e taking mean ). Think of this as the average of the products of two scaled variables.\n\n\n\n\n\n\nPearson Correlation uses z-scores\n\n\n\nWe can see \\((x-\\mu_x)/\\sigma_x\\) is a centering and scaling of the variable \\(x\\). Recall from our discussion on Distributions that this is called the z-score of x.\n\n\nPearson correlation assumes that the relationship between the two variables is linear. There are of course many other types of correlation measures: some which work when this is not so. Type vignette(\"types\", package = \"correlation\") in your Console to see the vignette from the correlation package that discusses various types of correlation measures.\nA quick reminder on how mosaic and ggformula work in a very similar fashion:\n\n\n\n\n\n\nmosaic and ggformula command template\n\n\n\nNote the standard method for all commands from the mosaic and ggformula packages: goal( y ~ x | z, data = _____)\nWith ggformula, one can create any graph/chart using: gf_***(y ~ x | z, data = _____)\nIn practice, we often use: dataframe %&gt;%  gf_***(y ~ x | z) which has cool benefits such as “autocompletion” of variable names, as we shall see. The “***” indicates what kind of graph you desire: histogram, bar, scatter, density; the “___” is the name of your dataset that you want to plot with.\n\n\nAnd on the ggplot command syntax:\n\n\n\n\n\n\nggplot command template\n\n\n\nThe ggplot2 template is used to identify the dataframe, identify the x and y axis, and define visualized layers:\nggplot(data = ---, mapping = aes(x = ---, y = ---)) + geom_----()\nNote: —- is meant to imply text you supply. e.g. function names, data frame names, variable names.\nIt is helpful to see the argument mapping, above. In practice, rather than typing the formal arguments, code is typically shorthanded to this:\ndataframe %&gt;%  ggplot(aes(xvar, yvar)) + geom_----()",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📎 Correlations"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#case-study-1-hollywoodmovies2011-dataset",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#case-study-1-hollywoodmovies2011-dataset",
    "title": "📎 Correlations",
    "section": "\n Case Study-1: HollywoodMovies2011 dataset",
    "text": "Case Study-1: HollywoodMovies2011 dataset\nLet us look at the HollywoodMovies2011 dataset from the Lock5withR package.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📎 Correlations"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#inspecting-the-data",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#inspecting-the-data",
    "title": "📎 Correlations",
    "section": "\n Inspecting the Data",
    "text": "Inspecting the Data\n\n\n R\n web-r\n\n\n\n\nHollywoodMovies2011-&gt; movies\nglimpse(movies)\n\nRows: 136\nColumns: 14\n$ Movie             &lt;fct&gt; \"Insidious\", \"Paranormal Activity 3\", \"Bad Teacher\",…\n$ LeadStudio        &lt;fct&gt; Sony, Independent, Independent, Warner Bros, Relativ…\n$ RottenTomatoes    &lt;int&gt; 67, 68, 44, 96, 90, 93, 75, 35, 63, 69, 69, 49, 26, …\n$ AudienceScore     &lt;int&gt; 65, 58, 38, 92, 77, 84, 91, 58, 74, 73, 72, 57, 68, …\n$ Story             &lt;fct&gt; Monster Force, Monster Force, Comedy, Rivalry, Rival…\n$ Genre             &lt;fct&gt; Horror, Horror, Comedy, Fantasy, Comedy, Romance, Dr…\n$ TheatersOpenWeek  &lt;int&gt; 2408, 3321, 3049, 4375, 2918, 944, 2534, 3615, NA, 2…\n$ BOAverageOpenWeek &lt;int&gt; 5511, 15829, 10365, 38672, 8995, 6177, 10278, 23775,…\n$ DomesticGross     &lt;dbl&gt; 54.01, 103.66, 100.29, 381.01, 169.11, 56.18, 169.22…\n$ ForeignGross      &lt;dbl&gt; 43.00, 98.24, 115.90, 947.10, 119.28, 83.00, 30.10, …\n$ WorldGross        &lt;dbl&gt; 97.009, 201.897, 216.196, 1328.111, 288.382, 139.177…\n$ Budget            &lt;dbl&gt; 1.5, 5.0, 20.0, 125.0, 32.5, 17.0, 25.0, 80.0, 0.2, …\n$ Profitability     &lt;dbl&gt; 64.672667, 40.379400, 10.809800, 10.624888, 8.873292…\n$ OpeningWeekend    &lt;dbl&gt; 13.27, 52.57, 31.60, 169.19, 26.25, 5.83, 26.04, 85.…\n\n\n\nskimr::skim(movies)\n\n\nData summary\n\n\nName\nmovies\n\n\nNumber of rows\n136\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n4\n\n\nnumeric\n10\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nMovie\n0\n1\nFALSE\n136\n30 : 1, 50/: 1, A D: 1, A V: 1\n\n\nLeadStudio\n0\n1\nFALSE\n34\nInd: 32, War: 12, 20t: 9, Uni: 9\n\n\nStory\n0\n1\nFALSE\n22\nMon: 19, Com: 14, Que: 13, Lov: 12\n\n\nGenre\n0\n1\nFALSE\n9\nAct: 32, Com: 27, Dra: 21, Hor: 17\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nRottenTomatoes\n2\n0.99\n53.19\n26.98\n4.00\n29.25\n53.50\n78.00\n97.00\n▅▇▅▆▇\n\n\nAudienceScore\n1\n0.99\n61.60\n17.03\n24.00\n49.50\n61.00\n76.50\n93.00\n▂▆▇▇▆\n\n\nTheatersOpenWeek\n16\n0.88\n2828.48\n932.70\n3.00\n2550.00\n2995.00\n3400.50\n4375.00\n▁▁▂▇▃\n\n\nBOAverageOpenWeek\n16\n0.88\n8338.83\n10284.47\n1513.00\n3778.75\n5685.50\n8923.00\n93230.00\n▇▁▁▁▁\n\n\nDomesticGross\n2\n0.99\n63.22\n69.42\n0.02\n19.03\n37.35\n80.46\n381.01\n▇▂▁▁▁\n\n\nForeignGross\n15\n0.89\n96.92\n156.44\n0.24\n14.25\n47.00\n102.00\n947.10\n▇▁▁▁▁\n\n\nWorldGross\n2\n0.99\n150.74\n215.02\n0.03\n30.71\n76.66\n173.69\n1328.11\n▇▁▁▁▁\n\n\nBudget\n2\n0.99\n53.48\n49.17\n0.20\n20.25\n36.50\n70.00\n250.00\n▇▂▂▁▁\n\n\nProfitability\n2\n0.99\n3.31\n6.62\n0.00\n1.06\n2.20\n3.67\n64.67\n▇▁▁▁▁\n\n\nOpeningWeekend\n3\n0.98\n20.34\n24.81\n0.00\n7.71\n13.10\n25.00\n169.19\n▇▁▁▁▁\n\n\n\n\nskimr::skim(movies) %&gt;% \n  kableExtra::kbl() %&gt;% \n  kable_styling(bootstrap_options = c(\"hover\", \"striped\", \"responsive\"))\n\n\n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\nfactor.ordered\nfactor.n_unique\nfactor.top_counts\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\nfactor\nMovie\n0\n1.0000000\nFALSE\n136\n30 : 1, 50/: 1, A D: 1, A V: 1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nfactor\nLeadStudio\n0\n1.0000000\nFALSE\n34\nInd: 32, War: 12, 20t: 9, Uni: 9\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nfactor\nStory\n0\n1.0000000\nFALSE\n22\nMon: 19, Com: 14, Que: 13, Lov: 12\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nfactor\nGenre\n0\n1.0000000\nFALSE\n9\nAct: 32, Com: 27, Dra: 21, Hor: 17\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\nRottenTomatoes\n2\n0.9852941\nNA\nNA\nNA\n53.19403\n26.984673\n4.000\n29.250000\n53.500000\n78.00000\n97.00000\n▅▇▅▆▇\n\n\nnumeric\nAudienceScore\n1\n0.9926471\nNA\nNA\nNA\n61.60000\n17.033856\n24.000\n49.500000\n61.000000\n76.50000\n93.00000\n▂▆▇▇▆\n\n\nnumeric\nTheatersOpenWeek\n16\n0.8823529\nNA\nNA\nNA\n2828.47500\n932.702579\n3.000\n2550.000000\n2995.000000\n3400.50000\n4375.00000\n▁▁▂▇▃\n\n\nnumeric\nBOAverageOpenWeek\n16\n0.8823529\nNA\nNA\nNA\n8338.82500\n10284.468976\n1513.000\n3778.750000\n5685.500000\n8923.00000\n93230.00000\n▇▁▁▁▁\n\n\nnumeric\nDomesticGross\n2\n0.9852941\nNA\nNA\nNA\n63.22276\n69.417994\n0.020\n19.032500\n37.355000\n80.45750\n381.01000\n▇▂▁▁▁\n\n\nnumeric\nForeignGross\n15\n0.8897059\nNA\nNA\nNA\n96.92339\n156.437778\n0.240\n14.250000\n47.000000\n102.00000\n947.10000\n▇▁▁▁▁\n\n\nnumeric\nWorldGross\n2\n0.9852941\nNA\nNA\nNA\n150.74234\n215.018634\n0.025\n30.706000\n76.658500\n173.69100\n1328.11100\n▇▁▁▁▁\n\n\nnumeric\nBudget\n2\n0.9852941\nNA\nNA\nNA\n53.48134\n49.171503\n0.200\n20.250000\n36.500000\n70.00000\n250.00000\n▇▂▂▁▁\n\n\nnumeric\nProfitability\n2\n0.9852941\nNA\nNA\nNA\n3.31452\n6.616088\n0.000\n1.064754\n2.198864\n3.66705\n64.67267\n▇▁▁▁▁\n\n\nnumeric\nOpeningWeekend\n3\n0.9779412\nNA\nNA\nNA\n20.34331\n24.805660\n0.000\n7.710000\n13.100000\n25.00000\n169.19000\n▇▁▁▁▁\n\n\n\n\n\n\n\n\n\n\nmovies_describe &lt;- inspect(movies)\n##\nmovies_describe$categorical %&gt;%\n  kableExtra::kbl() %&gt;% \n  kable_styling(bootstrap_options = c(\"hover\", \"striped\", \"responsive\"))\n\n\n\n\n\nname\nclass\nlevels\nn\nmissing\ndistribution\n\n\n\nMovie\nfactor\n136\n136\n0\n30 Minutes or Less (0.7%) ...\n\n\nLeadStudio\nfactor\n34\n136\n0\nIndependent (23.5%) ...\n\n\nStory\nfactor\n22\n136\n0\nMonster Force (14%), Comedy (10.3%) ...\n\n\nGenre\nfactor\n9\n136\n0\nAction (23.5%), Comedy (19.9%) ...\n\n\n\n\n\n\n\n\n##\nmovies_describe$quantitative %&gt;%\n  kableExtra::kbl() %&gt;% \n  kable_styling(bootstrap_options = c(\"hover\", \"striped\", \"responsive\"))\n\n\n\n\n\nname\nclass\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\nRottenTomatoes\ninteger\n4.000\n29.250000\n53.500000\n78.00000\n97.00000\n53.19403\n26.984673\n134\n2\n\n\nAudienceScore\ninteger\n24.000\n49.500000\n61.000000\n76.50000\n93.00000\n61.60000\n17.033856\n135\n1\n\n\nTheatersOpenWeek\ninteger\n3.000\n2550.000000\n2995.000000\n3400.50000\n4375.00000\n2828.47500\n932.702579\n120\n16\n\n\nBOAverageOpenWeek\ninteger\n1513.000\n3778.750000\n5685.500000\n8923.00000\n93230.00000\n8338.82500\n10284.468976\n120\n16\n\n\nDomesticGross\nnumeric\n0.020\n19.032500\n37.355000\n80.45750\n381.01000\n63.22276\n69.417994\n134\n2\n\n\nForeignGross\nnumeric\n0.240\n14.250000\n47.000000\n102.00000\n947.10000\n96.92339\n156.437778\n121\n15\n\n\nWorldGross\nnumeric\n0.025\n30.706000\n76.658500\n173.69100\n1328.11100\n150.74234\n215.018634\n134\n2\n\n\nBudget\nnumeric\n0.200\n20.250000\n36.500000\n70.00000\n250.00000\n53.48134\n49.171503\n134\n2\n\n\nProfitability\nnumeric\n0.000\n1.064754\n2.198864\n3.66705\n64.67267\n3.31452\n6.616088\n134\n2\n\n\nOpeningWeekend\nnumeric\n0.000\n7.710000\n13.100000\n25.00000\n169.19000\n20.34331\n24.805660\n133\n3\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nBusiness Insights from Data Inspection\n\n\n\nmovies has 136 observations on the following 14 variables.\n\nMovie a factor with many levels\nLeadStudio a factor with many levels\nRottenTomatoes a numeric vector\nAudienceScore a numeric vector\nStory a factor with many levels\nGenre a factor with levels Action, Adventure, Animation, Comedy, Drama, Fantasy, Horror, Romance, Thriller.\n\nTheatersOpenWeek a numeric vector\nBOAverageOpenWeek a numeric vector\nDomesticGross a numeric vector\nForeignGross a numeric vector\nWorldGross a numeric vector\nBudget a numeric vector\nProfitability a numeric vector\nOpeningWeekend a numeric vector\n\nThere are no missing values in the Qual variables; but some entries in the Quant variables are missing. skim throws a warning that we may need to examine later.\n\n\nLet us look at the Quant variables: are these related in anyway? Could the relationship between any two Quant variables also depend upon the level of a Qual variable?",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📎 Correlations"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#what-graphs-will-we-see-today",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#what-graphs-will-we-see-today",
    "title": "📎 Correlations",
    "section": "\n What graphs will we see today?",
    "text": "What graphs will we see today?\nSome of the very basic and commonly used plots for data are:\n\nScatter Plot\nContour Plot\nScatter Plot with Confidence Ellipses\nPairwise Correlation Plots\nCorrelogram\nHeatmap",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📎 Correlations"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#scatter-plots",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#scatter-plots",
    "title": "📎 Correlations",
    "section": "\n Scatter Plots",
    "text": "Scatter Plots\nWhich are the numeric variables in movies?\n\n\n R\n web-r\n\n\n\n\nmovies_quant &lt;- movies %&gt;%\n  drop_na() %&gt;%\n  select(where(is.numeric))\nmovies_quant\n\n\n  \n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nNow let us plot their relationships.\n\n\nUsing ggformula\nUsing ggplot\n web-r\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nmovies %&gt;% \n  gf_point(DomesticGross ~ WorldGross) %&gt;% \n  gf_lm() %&gt;% \n  gf_labs(title = \"Scatter Plot\",\n          subtitle = \"Movie Gross Earnings: Domestics vs World\")\n##\nmovies %&gt;% \n  gf_point(Profitability ~ OpeningWeekend) %&gt;% \n  gf_lm() %&gt;% \n  gf_labs(title = \"Scatter Plot\",\n          subtitle = \"Movies: Does Opening Week Earnings indicate Profitability?\")\n##\nmovies %&gt;% \n  gf_point(RottenTomatoes ~ AudienceScore) %&gt;% \n  gf_lm() %&gt;% \n  gf_labs(title = \"Scatter Plot\",\n          subtitle = \"Movie Ratings: Tomatoes vs Audience\")\n##\n## We can split some of the scatter plots using one or other of the Qual variables. For instance, is the relationship between the two ratings the same, regardless of movie genre?\nmovies %&gt;%\n  drop_na() %&gt;%\n  gf_point(RottenTomatoes ~ AudienceScore, \n           color = ~ Genre) %&gt;%\n  gf_lm() %&gt;% \n  gf_labs(title = \"Scatter Plot\",\n          subtitle = \"Movie Ratings: Trends by Genre\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nmovies %&gt;% \n  ggplot(aes(x = DomesticGross,y = WorldGross)) + \n  geom_point() + \n  geom_lm() + \n  labs(title = \"Scatter Plot\",\n       subtitle = \"Movie Gross Earnings: Domestics vs World\")\n##\nmovies %&gt;% \n  ggplot(aes(OpeningWeekend,Profitability)) + \n  geom_point() + \n  geom_lm() + \n  labs(title = \"Scatter Plot\",\n       subtitle = \"Movies: Does Opening Week Earnings indicate Profitability?\")\n##\nmovies %&gt;% \n  ggplot(aes(AudienceScore,RottenTomatoes)) + \n  geom_point() + \n  geom_lm() +\n  labs(title = \"Scatter Plot\",\n       subtitle = \"Movie Ratings: Tomatoes vs Audience\")\n##\nmovies %&gt;%\n  drop_na() %&gt;%\n  ggplot(aes(RottenTomatoes, AudienceScore, color = Genre)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Scatter Plot\",\n       subtitle = \"Movie Ratings: Trends by Genre\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nBusiness Insight from movies scatter plots\n\n\n\nWe have fitted a trend line to each of the scatter plots.\n\n\nDomesticGross and World Gross are related, though there are fewer movies at the high end of DomesticGross…\n\nAudienceScore and RottenTomatoes seem clearly related…both increase together.\n\nOpeningWeek and Profitability are also related in a linear way. There are just two movies which have been extremely profitable..but they do not influence the slope of the trend line too much, because of their location midway in the range of OpeningWeek. Influence is something that is a key concept in Linear Regression.\nBy and large, there are only small variations in slope across Genres.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📎 Correlations"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#d-density-plot",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#d-density-plot",
    "title": "📎 Correlations",
    "section": "\n 2D Density Plot",
    "text": "2D Density Plot\n\n\nUsing ggformula\nUsing ggplot\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nmovies %&gt;% \n  drop_na() %&gt;%\n  gf_density_2d(WorldGross ~ DomesticGross) %&gt;% \n  gf_point() %&gt;%\n  gf_lm() %&gt;% \n  gf_labs(title = \"Scatter Plot\",\n        subtitle = \"Movie Gross Earnings: Domestics vs World\")\n##\nmovies %&gt;% \n  drop_na() %&gt;%\n  gf_density_2d(Profitability ~ OpeningWeekend) %&gt;% \n  gf_point() %&gt;%\n  gf_lm() %&gt;% \n  gf_labs(title = \"Scatter Plot\",\n          subtitle = \"Movies: Does Opening Week Earnings indicate Profitability?\")\n\n\n\n\n\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nmovies %&gt;% \n  drop_na() %&gt;%\n  gf_density_2d(RottenTomatoes ~ AudienceScore) %&gt;% \n  gf_point() %&gt;%\n  gf_lm(title = \"Scatter Plot\",\n        subtitle = \"Movie Ratings: Tomatoes vs Audience\")\n\n\n\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nmovies %&gt;% drop_na() %&gt;%\n  ggplot(aes(DomesticGross, WorldGross)) + \n  geom_density_2d() + \n  geom_point() + \n  geom_lm() + \n  labs(title = \"Scatter Plot\",\n       subtitle = \"Movie Gross Earnings: Domestics vs World\")\n##\nmovies %&gt;% drop_na() %&gt;%\n  ggplot(aes(OpeningWeekend,Profitability)) +\n  geom_density2d() + \n  geom_point() + \n  geom_lm() + \n  labs(title = \"Scatter Plot\",\n       subtitle = \"Movies: Does Opening Week Earnings indicate Profitability?\")\n\n\n\n\n\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nmovies %&gt;% drop_na() %&gt;%\n  ggplot(aes(AudienceScore, RottenTomatoes)) + \n  geom_density_2d(bins = 30) + \n  geom_point() + \n  stat_ellipse(level = 0.95, colour = \"grey\", linewidth = 1) + \n  stat_ellipse(level = 0.68, colour = \"firebrick\", linewidth = 1) +   \n  geom_lm() + \n  labs(title = \"Scatter Plot\",\n       subtitle = \"Movie Ratings: Tomatoes vs Audience\")\n\n\n\n\n\n\n\n\n\n\nHow might we interpret these 2D densities? Take a look at the diagram below. Which of the images matches the densities from or movie data?\n\n\n\n\n\n\n\nBusiness Insight from movies 2D plots\n\n\n\nWe can imagine the 2D density plot as a 3-dimensional structure, like hills/mountains, and these are depicted by the contour lines. The more concentrated and aligned these contours, the more likely that there would be significant correlation between the two.\nIn the plot above, we can see two such areas of concentration and these are also aligned roughly along a line.\n\n\n\n\n\n\n\n\nIndependent and Dependent Variables\n\n\n\nNote that we have rather arbitrarily taken AudienceScore as the independent variable, to be plotted on the x-axis, and RottenTomatoes on the y-axis. It could easily have been the other way around, based on our Research Question. Datasets are gathered with specific Research Hypotheses in mind, so check the help file and also with the person who gathered the data about what variable they are interested in!",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📎 Correlations"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#quantizing-correlation",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#quantizing-correlation",
    "title": "📎 Correlations",
    "section": "\n Quantizing Correlation",
    "text": "Quantizing Correlation\nSo we see that there are visible relationships between Quant variables. How do we quantize this relationship, into a correlation score?\nThere are two ways: using the GGally and corplot packages, and doing a formal correlation test with the mosaic package.\n\n\nUsing GGally\nUsing corrplot\n\n\n\nBy default, GGally::ggpairs() provides:\n\ntwo different comparisons of each pair of columns\ndisplays either the density or count of the respective variable along the diagonal. \nWith different parameter settings, the diagonal can be replaced with the axis values and variable labels.\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nnames(movies_quant)\n\n [1] \"RottenTomatoes\"    \"AudienceScore\"     \"TheatersOpenWeek\" \n [4] \"BOAverageOpenWeek\" \"DomesticGross\"     \"ForeignGross\"     \n [7] \"WorldGross\"        \"Budget\"            \"Profitability\"    \n[10] \"OpeningWeekend\"   \n\nGGally::ggpairs(\n  movies %&gt;% drop_na(),\n  # Select Quant variables only for now\n  columns = c(\n    \"RottenTomatoes\", \"AudienceScore\", \"DomesticGross\",\"ForeignGross\"),\n  \n  switch = \"both\",\n  # axis labels in more traditional locations(left and bottom)\n  \n  progress = FALSE,\n  # no compute progress messages needed\n  \n  # Choose the diagonal graphs (always single variable! Think!)\n  diag = list(continuous = \"barDiag\"),\n  # choosing histogram,not density\n  \n  # Choose lower triangle graphs, two-variable graphs\n  lower = list(continuous = wrap(\"smooth\", alpha = 0.3, se = FALSE)),\n  \n  title = \"Movies Data Correlations Plot #1\"\n) \n\n\n\n\n\n\n\n\n\n\n\n\n\nBusiness Insight from Pairs Plot#1\n\n\n\n\nAs we saw earlier from the Scatter Plot, AudienceScore and RottenTomatoes are well correlated, with a correlation score of \\(0.833\\)\n\n\nDomesticGross and ForeignGross are also extremely well correlated, with a score of \\(0.873\\).\nBoth these correlation scores are highly significant, with three stars. (We will speak of significance in a while.)\nNone of the other pairs of variables have good correlation scores.\nNote in passing that both the “Gross” related variables have highly skewed distributions. That is the nature of the movie business!\n\n\n\nLet us also try a few other variables, related to budget and profits. For instance, it would be interesting to see the relationship between Budget and Profitability and even either of the “gross” earnings and Profitability.\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nGGally::ggpairs(\n  movies %&gt;% drop_na(),\n  # Select Quant variables only for now\n  columns = c(\n    \"Budget\", \"Profitability\", \"DomesticGross\", \"ForeignGross\"),\n  \n  switch = \"both\",\n  # axis labels in more traditional locations(left and bottom)\n  \n  progress = FALSE,\n  # no compute progress messages needed\n  \n  # Choose the diagonal graphs (always single variable! Think!)\n  diag = list(continuous = \"barDiag\"),\n  # choosing histogram,not density\n  \n  # Choose lower triangle graphs, two-variable graphs\n  lower = list(continuous = wrap(\"smooth\", alpha = 0.3, se = FALSE)),\n  \n  title = \"Movies Data Correlations Plot #2\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nBusiness Insight from Pairs Plot #2\n\n\n\n\nThe Budget variable has good correlation scores with DomesticGross and ForeignGross\n\n\nProfitability and Budget seem to have a very slight negative correlation, but this does not appear to be significant.\n\n\n\n\n\nIn this chart, the correlation between pairs of variables is shown symbolically as coloured shapes or colours. Circles, Squares, and Ellipse for example.\n\nThe size, colour, and “orientation” of the shapes in question symbolically represent the strength and polarity of the correlation scores.\nThe direction of the semi-major axis + the colour of the ellipse indicate whether the correlation score is positive or negative;\nAnd the more eccentric the ellipse, the higher is the correlation score in value.\n\n\n\n\n\n\n\nNote\n\n\n\nWhereas GGally computes the correlation scores, corplot “merely” displays them in an evocative way. We need to compute the correlations a priori.\n\n\nNote also:\n\n\n\n\n\n\nTip\n\n\n\nR package corrplot provides a visual exploratory tool on correlation matrix that supports automatic variable reordering to help detect hidden patterns among variables. corrplot is very easy to use and provides a rich array of plotting options in visualization method, graphic layout, color, legend, text labels, etc. It also provides p-values and confidence intervals to help users determine the statistical significance of the correlations.\n\n\n\n# library(corrplot)\nmydata_cor &lt;- cor(movies_quant)\nmydata_cor %&gt;% \n  knitr::kable(caption = \"Correlation Scores Matrix\")\n\n\nCorrelation Scores Matrix\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRottenTomatoes\nAudienceScore\nTheatersOpenWeek\nBOAverageOpenWeek\nDomesticGross\nForeignGross\nWorldGross\nBudget\nProfitability\nOpeningWeekend\n\n\n\nRottenTomatoes\n1.0000000\n0.8329740\n-0.0873543\n0.1823480\n0.2085935\n0.0979132\n0.1356232\n-0.0147887\n0.1502764\n0.0986304\n\n\nAudienceScore\n0.8329740\n1.0000000\n0.0259118\n0.1851768\n0.3849406\n0.2557891\n0.3037927\n0.1268649\n0.1047582\n0.2695132\n\n\nTheatersOpenWeek\n-0.0873543\n0.0259118\n1.0000000\n0.0117674\n0.5981162\n0.4850569\n0.5344582\n0.5924941\n0.0547807\n0.5977724\n\n\nBOAverageOpenWeek\n0.1823480\n0.1851768\n0.0117674\n1.0000000\n0.4713164\n0.4522253\n0.4710352\n0.2880262\n0.0964176\n0.5043684\n\n\nDomesticGross\n0.2085935\n0.3849406\n0.5981162\n0.4713164\n1.0000000\n0.8725927\n0.9374780\n0.6497274\n0.1812387\n0.9232259\n\n\nForeignGross\n0.0979132\n0.2557891\n0.4850569\n0.4522253\n0.8725927\n1.0000000\n0.9880383\n0.6707613\n0.1230330\n0.8487202\n\n\nWorldGross\n0.1356232\n0.3037927\n0.5344582\n0.4710352\n0.9374780\n0.9880383\n1.0000000\n0.6830783\n0.1448857\n0.8962294\n\n\nBudget\n-0.0147887\n0.1268649\n0.5924941\n0.2880262\n0.6497274\n0.6707613\n0.6830783\n1.0000000\n-0.1437862\n0.6228180\n\n\nProfitability\n0.1502764\n0.1047582\n0.0547807\n0.0964176\n0.1812387\n0.1230330\n0.1448857\n-0.1437862\n1.0000000\n0.1713962\n\n\nOpeningWeekend\n0.0986304\n0.2695132\n0.5977724\n0.5043684\n0.9232259\n0.8487202\n0.8962294\n0.6228180\n0.1713962\n1.0000000\n\n\n\n\n## View the matrix\ncorrplot::corrplot(mydata_cor, method = \"number\", \n         number.cex = 0.6,\n         cl.cex = 0.6, tl.cex = 0.6)\n\n\n\n\n\n\n# Default plot with circles\ncorrplot(mydata_cor, \n         method = \"circle\",                  \n         main = \"Correlogram with Circles\")\n\n\n\n\n\n\n# Ellipse plot\ncorrplot(mydata_cor, \n                   method = \"ellipse\",\n                   main = \"Correlogram with Ellipes\")\n\n\n\n\n\n\n# Heatmap\ncorrplot(mydata_cor, method = \"color\", ## US Spelling only\n         main = \"Correlogram\")\n\n\n\n\n\n\n# Heatmap with numbers\ncorrplot.mixed(mydata_cor, \n               lower = \"color\", number.cex = 0.6,\n               cl.cex = 0.6, tl.cex = 0.6,\n               upper = \"number\",\n               tl.pos = \"l\",\n               main = \"Heatmap?\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nBusiness Insights from corplots\n\n\n\n\nMost of the variables here have positive correlations, many of them are significant\n\n\n\n\n\n\nDoing a Correlation Test\nCorrelations scores can be obtained by conducting a formal test in R. We will use the mosaic function cor_test to get these results:\n\nmosaic::cor_test(Profitability ~ Budget, data = movies) %&gt;% \n  broom::tidy() %&gt;% \n  knitr::kable(digits = 2,\n               caption = \"Movie Profitability vs Budget\")\n\n\nMovie Profitability vs Budget\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n-0.08\n-0.96\n0.34\n132\n-0.25\n0.09\nPearson’s product-moment correlation\ntwo.sided\n\n\n\nmosaic::cor_test(DomesticGross ~ Budget, data = movies) %&gt;% \n  broom::tidy() %&gt;% \n  knitr::kable(digits = 2,\n               caption = \"Movie Domestic Gross vs Budget\")\n\n\nMovie Domestic Gross vs Budget\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n0.7\n11.06\n0\n131\n0.6\n0.77\nPearson’s product-moment correlation\ntwo.sided\n\n\n\nmosaic::cor_test(ForeignGross ~ Budget, data = movies) %&gt;% \n  broom::tidy() %&gt;% \n  knitr::kable(digits = 2,\n               caption = \"Movie Foreign Gross vs Budget\")\n\n\nMovie Foreign Gross vs Budget\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n0.69\n10.22\n0\n118\n0.58\n0.77\nPearson’s product-moment correlation\ntwo.sided\n\n\n\n\n\n\n\n\n\n\nBusiness Insights from Correlation Tests\n\n\n\nThe budget and profitability are not well correlated, sadly. We see this from the p.value which is \\(0.34\\) and the confidence values for the correlation estimate which also cover \\(0\\).\nHowever, both DomesticGross and ForeignGross are well correlated with Budget. Look at the p.value (=0) and the confindence intervals which are unipolar.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📎 Correlations"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#a-new-combination-plot",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#a-new-combination-plot",
    "title": "📎 Correlations",
    "section": "A New Combination Plot…",
    "text": "A New Combination Plot…\nSometimes, a simple scatter, or density alone, or viewed next to one another is not adequate to develop, or convey, our insight. We might just need a combination density + scatter plot. Such a plot can be be constructed from the ground up using ggformula or ggplot; however, there is a nice package called ggExtra that allows the creation of a powerful combination plot:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nlibrary(ggExtra)\n\npenguins %&gt;% \n  drop_na() %&gt;% \n  gf_point(body_mass_g  ~ flipper_length_mm, colour = ~ species) %&gt;% \n  gf_smooth(method = \"lm\") %&gt;% \n  gf_labs(title = \"Scatter Plot with Marginal Densities\") %&gt;% \n  ggExtra::ggMarginal(type = \"density\", groupColour = TRUE, \n                      groupFill = TRUE, margins = \"both\")",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📎 Correlations"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#an-interactive-correlation-game",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#an-interactive-correlation-game",
    "title": "📎 Correlations",
    "section": "An Interactive Correlation Game",
    "text": "An Interactive Correlation Game\nHead off to this interactive game website where you can play with correlations!\nhttps://openintro.shinyapps.io/correlation_game/",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📎 Correlations"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#conclusions",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#conclusions",
    "title": "📎 Correlations",
    "section": "\n Conclusions",
    "text": "Conclusions\nOur workflow for evaluating correlations between target variable and other predictor variables uses several packages such as GGally, corrplot, and of course mosaic for correlation tests. We will explore more R code for these in the tutorials Section 1.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📎 Correlations"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/intro-corr.html#references",
    "title": "📎 Correlations",
    "section": "\n References",
    "text": "References\n\nMinimal R using mosaic. https://cran.r-project.org/web/packages/mosaic/vignettes/MinimalRgg.pdf\n\nAntoine Soetewey. Pearson, Spearman and Kendall correlation coefficients by hand https://www.r-bloggers.com/2023/09/pearson-spearman-and-kendall-correlation-coefficients-by-hand/\n\n\n\n\n R Package Citations\n\n\n\n\nx\n\n\n\nAden-Buie G (2023). epoxy: String Interpolation for Documents, Reports and Apps. R package version 1.0.0, https://CRAN.R-project.org/package=epoxy.\n\n\nAttali D, Baker C (2023). ggExtra: Add Marginal Histograms to ‘ggplot2’, and More ‘ggplot2’ Enhancements. R package version 0.10.1, https://CRAN.R-project.org/package=ggExtra.\n\n\nBates D, Maechler M, Jagan M (2024). Matrix: Sparse and Dense Matrix Classes and Methods. R package version 1.7-0, https://CRAN.R-project.org/package=Matrix.\n\n\nBen-Shachar MS, Lüdecke D, Makowski D (2020). “effectsize: Estimation of Effect Size Indices and Standardized Parameters.” Journal of Open Source Software, 5(56), 2815. doi:10.21105/joss.02815 https://doi.org/10.21105/joss.02815, https://doi.org/10.21105/joss.02815.\n\n\nFrancisco Rodriguez-Sanchez, Connor P. Jackson (2023). grateful: Facilitate citation of R packages. https://pakillo.github.io/grateful/.\n\n\nGrolemund G, Wickham H (2011). “Dates and Times Made Easy with lubridate.” Journal of Statistical Software, 40(3), 1-25. https://www.jstatsoft.org/v40/i03/.\n\n\nHorst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. doi:10.5281/zenodo.3960218 https://doi.org/10.5281/zenodo.3960218, R package version 0.1.0, https://allisonhorst.github.io/palmerpenguins/.\n\n\nKaplan D, Pruim R (2023). ggformula: Formula Interface to the Grammar of Graphics. R package version 0.12.0, https://CRAN.R-project.org/package=ggformula.\n\n\nLüdecke D, Ben-Shachar M, Patil I, Makowski D (2020). “Extracting, Computing and Exploring the Parameters of Statistical Models using R.” Journal of Open Source Software, 5(53), 2445. doi:10.21105/joss.02445 https://doi.org/10.21105/joss.02445.\n\n\nLüdecke D, Ben-Shachar M, Patil I, Waggoner P, Makowski D (2021). “performance: An R Package for Assessment, Comparison and Testing of Statistical Models.” Journal of Open Source Software, 6(60), 3139. doi:10.21105/joss.03139 https://doi.org/10.21105/joss.03139.\n\n\nLüdecke D, Ben-Shachar M, Patil I, Wiernik B, Bacher E, Thériault R, Makowski D (2022). “easystats: Framework for Easy Statistical Modeling, Visualization, and Reporting.” CRAN. R package, https://easystats.github.io/easystats/.\n\n\nLüdecke D, Patil I, Ben-Shachar M, Wiernik B, Waggoner P, Makowski D (2021). “see: An R Package for Visualizing Statistical Models.” Journal of Open Source Software, 6(64), 3393. doi:10.21105/joss.03393 https://doi.org/10.21105/joss.03393.\n\n\nLüdecke D, Waggoner P, Makowski D (2019). “insight: A Unified Interface to Access Information from Model Objects in R.” Journal of Open Source Software, 4(38), 1412. doi:10.21105/joss.01412 https://doi.org/10.21105/joss.01412.\n\n\nMakowski D, Ben-Shachar M, Lüdecke D (2019). “bayestestR: Describing Effects and their Uncertainty, Existence and Significance within the Bayesian Framework.” Journal of Open Source Software, 4(40), 1541. doi:10.21105/joss.01541 https://doi.org/10.21105/joss.01541, https://joss.theoj.org/papers/10.21105/joss.01541.\n\n\nMakowski D, Ben-Shachar M, Patil I, Lüdecke D (2020). “Estimation of Model-Based Predictions, Contrasts and Means.” CRAN. https://github.com/easystats/modelbased.\n\n\nMakowski D, Lüdecke D, Patil I, Thériault R, Ben-Shachar M, Wiernik B (2023). “Automated Results Reporting as a Practical Tool to Improve Reproducibility and Methodological Best Practices Adoption.” CRAN. https://easystats.github.io/report/.\n\n\nMakowski D, Wiernik B, Patil I, Lüdecke D, Ben-Shachar M (2022). “correlation: Methods for Correlation Analysis.” Version 0.8.3, https://CRAN.R-project.org/package=correlation. Makowski D, Ben-Shachar M, Patil I, Lüdecke D (2020). “Methods and Algorithms for Correlation Analysis in R.” Journal of Open Source Software, 5(51), 2306. doi:10.21105/joss.02306 https://doi.org/10.21105/joss.02306, https://joss.theoj.org/papers/10.21105/joss.02306.\n\n\nMoroz G (2020). Create check-fields and check-boxes with checkdown. https://CRAN.R-project.org/package=checkdown.\n\n\nMüller K, Wickham H (2023). tibble: Simple Data Frames. R package version 3.2.1, https://CRAN.R-project.org/package=tibble.\n\n\nPatil I, Makowski D, Ben-Shachar M, Wiernik B, Bacher E, Lüdecke D (2022). “datawizard: An R Package for Easy Data Preparation and Statistical Transformations.” Journal of Open Source Software, 7(78), 4684. doi:10.21105/joss.04684 https://doi.org/10.21105/joss.04684.\n\n\nPedersen T, Shemanarev M (2024). ragg: Graphic Devices Based on AGG. R package version 1.3.2, https://CRAN.R-project.org/package=ragg.\n\n\nPerrier V, Meyer F (2023). gfonts: Offline ‘Google’ Fonts for ‘Markdown’ and ‘Shiny’. R package version 0.2.0, https://CRAN.R-project.org/package=gfonts.\n\n\nPruim R (2015). Lock5withR: Datasets for ‘Statistics: Unlocking the Power of Data’. R package version 1.2.2, commit f2773d9f828b72882ed1c4d6b3b2e539a3b3b24a, https://github.com/rpruim/Lock5withR.\n\n\nPruim R, Kaplan D, Horton N (2023). mosaicData: Project MOSAIC Data Sets. R package version 0.20.4, https://CRAN.R-project.org/package=mosaicData.\n\n\nPruim R, Kaplan DT, Horton NJ (2017). “The mosaic Package: Helping Students to ‘Think with Data’ Using R.” The R Journal, 9(1), 77-102. https://journal.r-project.org/archive/2017/RJ-2017-024/index.html.\n\n\nQiu Y, details. aotifSfAf (2020). showtextdb: Font Files for the ‘showtext’ Package. R package version 3.0, https://CRAN.R-project.org/package=showtextdb.\n\n\nQiu Y, details. aotifSfAf (2024). sysfonts: Loading Fonts into R. R package version 0.8.9, https://CRAN.R-project.org/package=sysfonts.\n\n\nQiu Y, details. aotisSfAf (2024). showtext: Using Fonts More Easily in R Graphs. R package version 0.9-7, https://CRAN.R-project.org/package=showtext.\n\n\nR Core Team (2024). R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria. https://www.R-project.org/.\n\n\nSarkar D (2008). Lattice: Multivariate Data Visualization with R. Springer, New York. ISBN 978-0-387-75968-5, http://lmdvr.r-forge.r-project.org.\n\n\nSchloerke B, Cook D, Larmarange J, Briatte F, Marbach M, Thoen E, Elberg A, Crowley J (2024). GGally: Extension to ‘ggplot2’. R package version 2.2.1, https://CRAN.R-project.org/package=GGally.\n\n\nSievert C, Schloerke B, Cheng J (2024). thematic: Unified and Automatic ‘Theming’ of ‘ggplot2’, ‘lattice’, and ‘base’ R Graphics. R package version 0.1.5, https://CRAN.R-project.org/package=thematic.\n\n\nWaring E, Quinn M, McNamara A, Arino de la Rubia E, Zhu H, Ellis S (2022). skimr: Compact and Flexible Summaries of Data. R package version 2.1.5, https://CRAN.R-project.org/package=skimr.\n\n\nWei T, Simko V (2021). R package ‘corrplot’: Visualization of a Correlation Matrix. (Version 0.92), https://github.com/taiyun/corrplot.\n\n\nWickham H (2016). ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. ISBN 978-3-319-24277-4, https://ggplot2.tidyverse.org.\n\n\nWickham H (2023). forcats: Tools for Working with Categorical Variables (Factors). R package version 1.0.0, https://CRAN.R-project.org/package=forcats.\n\n\nWickham H (2023). stringr: Simple, Consistent Wrappers for Common String Operations. R package version 1.5.1, https://CRAN.R-project.org/package=stringr.\n\n\nWickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). “Welcome to the tidyverse.” Journal of Open Source Software, 4(43), 1686. doi:10.21105/joss.01686 https://doi.org/10.21105/joss.01686.\n\n\nWickham H, François R, Henry L, Müller K, Vaughan D (2023). dplyr: A Grammar of Data Manipulation. R package version 1.1.4, https://CRAN.R-project.org/package=dplyr.\n\n\nWickham H, Henry L (2023). purrr: Functional Programming Tools. R package version 1.0.2, https://CRAN.R-project.org/package=purrr.\n\n\nWickham H, Hester J, Bryan J (2024). readr: Read Rectangular Text Data. R package version 2.1.5, https://CRAN.R-project.org/package=readr.\n\n\nWickham H, Pedersen T, Seidel D (2023). scales: Scale Functions for Visualization. R package version 1.3.0, https://CRAN.R-project.org/package=scales.\n\n\nWickham H, Vaughan D, Girlich M (2024). tidyr: Tidy Messy Data. R package version 1.3.1, https://CRAN.R-project.org/package=tidyr.\n\n\nWilke C (2024). ggridges: Ridgeline Plots in ‘ggplot2’. R package version 0.5.6, https://CRAN.R-project.org/package=ggridges.\n\n\nZhu H (2024). kableExtra: Construct Complex Table with ‘kable’ and Pipe Syntax. R package version 1.4.0, https://CRAN.R-project.org/package=kableExtra.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "📎 Correlations"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html",
    "title": "Tutorial on Correlations in R",
    "section": "",
    "text": "We will create Tables for Correlations, and graphs for Correlations in R. As always, we will consistently use the Project Mosaic ecosystem of packages in R (mosaic, mosaicData and ggformula)."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#introduction",
    "title": "Tutorial on Correlations in R",
    "section": "",
    "text": "We will create Tables for Correlations, and graphs for Correlations in R. As always, we will consistently use the Project Mosaic ecosystem of packages in R (mosaic, mosaicData and ggformula)."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#setting-up-r-packages",
    "title": "Tutorial on Correlations in R",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(tidyverse)\nlibrary(mosaic) # package for stats, simulations, and basic plots\nlibrary(ggformula) # package for professional looking plots, that use the formula interface from mosaic\nlibrary(skimr)\nlibrary(GGally)\nlibrary(corrplot) # For Correlogram plots\nlibrary(broom) # to properly format stat test results\n\nlibrary(mosaicData) # package containing datasets\nlibrary(NHANES) # survey data collected by the US National Center for Health Statistics (NCHS)\n\nAll R functions seen in the code are clickable links that take you to online documentation about the function. Try!\n\n\n\n\n\n\nThe Formula interface\n\n\n\nNote the standard method for all commands from the mosaic package:\ngoal( y ~ x | z, data = mydata, …)\nWith ggformula, one can create any graph/chart using:\ngf_geometry(y ~ x | z, data = mydata)\nOR\nmydata %&gt;% gf_geometry( y ~ x | z )\nThe second method may be preferable, especially if you have done some data manipulation first! More about this later!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#case-study-1-galton-dataset-from-mosaicdata",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#case-study-1-galton-dataset-from-mosaicdata",
    "title": "Tutorial on Correlations in R",
    "section": "\n Case Study 1: Galton Dataset from mosaicData\n",
    "text": "Case Study 1: Galton Dataset from mosaicData\n\nLet us inspect what datasets are available in the package mosaicData. Run this command in your Console:\n\n# Run in Console\ndata(package = \"mosaicData\")\n\nThe popup tab shows a lot of datasets we could use. Let us continue to use the famous Galton dataset and inspect it:\n\ndata(\"Galton\")\n\n\n Inspecting the Data\nThe inspect command already gives us a series of statistical measures of different variables of interest. As discussed previously, we can retain the output of inspect and use it in our reports: (there are ways of dressing up these tables too)\n\ngalton_describe &lt;- inspect(Galton)\n\ngalton_describe$categorical\n\n\n  \n\n\ngalton_describe$quantitative\n\n\n  \n\n\n\nTry help(\"Galton\") in your Console. The dataset is described as:\n\nA data frame with 898 observations on the following variables.\n- family a factor with levels for each family\n- father the father’s height (in inches)\n- mother the mother’s height (in inches)\n- sex the child’s sex: F or M\n- height the child’s height as an adult (in inches)\n- nkids the number of adult children in the family, or, at least, the number whose heights Galton recorded.\n\nThere is a lot of Description generated by the mosaic::inspect() command ! Let us also look at the output of skim:\n\nskimr::skim(Galton)\n\n\nData summary\n\n\nName\nGalton\n\n\nNumber of rows\n898\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nfamily\n0\n1\nFALSE\n197\n185: 15, 166: 11, 66: 11, 130: 10\n\n\nsex\n0\n1\nFALSE\n2\nM: 465, F: 433\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nfather\n0\n1\n69.23\n2.47\n62\n68\n69.0\n71.0\n78.5\n▁▅▇▂▁\n\n\nmother\n0\n1\n64.08\n2.31\n58\n63\n64.0\n65.5\n70.5\n▂▅▇▃▁\n\n\nheight\n0\n1\n66.76\n3.58\n56\n64\n66.5\n69.7\n79.0\n▁▇▇▅▁\n\n\nnkids\n0\n1\n6.14\n2.69\n1\n4\n6.0\n8.0\n15.0\n▃▇▆▂▁\n\n\n\n\n\nWhat can we say about the dataset and its variables? How big is the dataset? How many variables? What types are they, Quant or Qual? If they are Qual, what are the levels? Are they ordered levels? Which variables could have relationships with others? Why? Write down these Questions!\n\n Correlations and Plots\nWhat Questions might we have, that we could answer with a Statistical Measure, or Correlation chart?\n\n\n\n\n\n\nQuestions\n\n\n\nHow does children’s height correlate with that of father and mother? Is this relationship also affected by sex of the child?\nWith this question, height becomes our target variable, which we should always plot on the dependent y-axis.\n\n\n\n# Pulling out the list of Quant variables from NHANES\ngalton_quant &lt;- galton_describe$quantitative\ngalton_quant$name\n\n[1] \"father\" \"mother\" \"height\" \"nkids\" \n\nGGally::ggpairs(\n  Galton,\n  \n  # Choose the variables we want to plot for\n  columns = c(\"father\", \"mother\", \"height\", \"nkids\"),\n  \n  switch = \"both\", # axis labels in more traditional locations\n  progress = FALSE, # no compute progress messages needed\n  \n  # Choose the diagonal graphs (always single variable! Think!)\n  diag = list(continuous = \"barDiag\"), # choosing histogram,not density\n  \n  # Choose lower triangle graphs, two-variable graphs\n  lower = list(continuous = wrap(\"smooth\", alpha = 0.1)),\n  \n  title = \"Galton Data Correlations Plot\"\n) + \n  \n  theme_bw()\n\n\n\n\n\n\n\nWe note that children’s height is correlated with that of father and mother. The correlations are both positive, and that with father seems to be the larger of the two. ( Look at the slopes of the lines and the values of the correlation scores. )\n\n\n\n\n\n\nQuestion\n\n\n\nWhat if we group the Quant variables based on a Qual variable, like sex of the child?\n\n\n\n# Pulling out the list of Quant variables from NHANES\ngalton_quant &lt;- galton_describe$quantitative\ngalton_quant$name\n\n[1] \"father\" \"mother\" \"height\" \"nkids\" \n\nGGally::ggpairs(\n  Galton,\n  \n  mapping = aes(colour = sex), # Colour by `sex`\n\n  # Choose the variables we want to plot for\n  columns = c(\"father\", \"mother\", \"height\", \"nkids\"),\n  switch = \"both\", # axis labels in more traditional locations\n  progress = FALSE, # no compute progress messages needed\n  \n  diag = list(continuous = \"barDiag\"),\n  \n  # Choose lower triangle graphs, two-variable graphs\n  lower = list(continuous = wrap(\"smooth\", alpha = 0.1)),\n  \n  title = \"Galton Data Correlations Plot\"\n) + \n  \n  theme_bw()\n\n\n\n\n\n\n\nThe split scatter plots are useful, as is the split histogram for height: Clearly the correlation of children’s height with father and mother is positive for both sex-es. The other plots, and even some of the correlations scores are not all useful! Just shows everything we can compute is not necessarily useful immediately.\nIn later modules we will see how to plot correlations when the number of variables is larger still.\n\n\n\n\n\n\nQuestion\n\n\n\nCan we plot a Correlogram for this dataset?\n\n\n\n#library(corrplot)\n\ngalton_num_var &lt;- Galton %&gt;% select(father, mother, height, nkids)\ngalton_cor &lt;- cor(galton_num_var)\ngalton_cor %&gt;%\n  corrplot(method = \"ellipse\",\n           type = \"lower\",\n           main = \"Correlogram for Galton dataset\")\n\n\n\n\n\n\n\nClearly height is positively correlated to father and mother; interestingly, height is negatively correlated ( slightly) with nkids.\n\n\n\n\n\n\nQuestion\n\n\n\nLet us confirm with a correlation test:\n\n\nWe will use the mosaic function cor_test to get these results:\n\nmosaic::cor_test(height ~ father, data = Galton) %&gt;% \n  broom::tidy() %&gt;% \n  knitr::kable(digits = 2,\n               caption = \"Children vs Fathers\")\n\n\nChildren vs Fathers\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n0.28\n8.57\n0\n896\n0.21\n0.33\nPearson’s product-moment correlation\ntwo.sided\n\n\n\n\n\nmosaic::cor_test(height ~ mother, data = Galton) %&gt;% \n  broom::tidy() %&gt;% \n    knitr::kable(digits = 2,\n               caption = \"Children vs Mothers\")\n\n\nChildren vs Mothers\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n0.2\n6.16\n0\n896\n0.14\n0.26\nPearson’s product-moment correlation\ntwo.sided\n\n\n\n\n\n\n\n\n\n\nCorrelation Scores and Uncertainty\n\n\n\nNote how the mosaic::cor_test() reports a correlation score estimate and the p-value for the same. There is also a confidence interval reported for the correlation score, an interval within which we are 95% sure that the true correlation value is to be found.\nNote that GGally::ggpairs() too reports the significance of the correlation scores estimates using *** or **. This indicates the p-value in the scores obtained by GGally; Presumably, there is an internal cor_test that is run for each pair of variables and the p-value and confidence levels are also computed internally.\n\n\nIn both cases, we used the formula \\(height \\sim other-variable\\), in keeping with our idea of height being the dependent, target variable..\nWe also see the p.value for the estimateed correlation is negligible, and the conf.low/conf.high interval does not straddle \\(0\\). These attest to the significance of the correlation score.\n\n\n\n\n\n\nQuestion\n\n\n\nWhat does this correlation look when split by sex of Child?\n\n\n\n# For the sons\n\nmosaic::cor_test(height ~ father,\n                 data = Galton %&gt;% filter(sex == \"M\")) %&gt;% \n  broom::tidy() %&gt;% knitr::kable(digits = 2,\n                                 caption = \"Sons vs Fathers\")\ncor_test(height ~ mother, \n         data = Galton %&gt;% filter(sex == \"M\")) %&gt;% \n  broom::tidy() %&gt;% knitr::kable(digits = 2,\n                                 caption = \"Sons vs Mothers\")\n\n# For the daughters\ncor_test(height ~ father, \n         data = Galton %&gt;% filter(sex == \"F\")) %&gt;% \n  broom::tidy() %&gt;% knitr::kable(digits = 2,\n                                 caption = \"Daughters vs Fathers\")\ncor_test(height ~ mother, \n         data = Galton %&gt;% filter(sex == \"F\")) %&gt;% \n  broom::tidy() %&gt;% knitr::kable(digits = 2,\n                                 caption = \"Daughters vs Mothers\")\n\n\nSons vs Fathers\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n0.39\n9.15\n0\n463\n0.31\n0.47\nPearson’s product-moment correlation\ntwo.sided\n\n\n\nSons vs Mothers\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n0.33\n7.63\n0\n463\n0.25\n0.41\nPearson’s product-moment correlation\ntwo.sided\n\n\n\nDaughters vs Fathers\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n0.46\n10.72\n0\n431\n0.38\n0.53\nPearson’s product-moment correlation\ntwo.sided\n\n\n\nDaughters vs Mothers\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n0.31\n6.86\n0\n431\n0.23\n0.4\nPearson’s product-moment correlation\ntwo.sided\n\n\n\n\nThe same observation as made above ( p.value and confidence intervals) applies here too and tells us that the estimated correlations are significant.\nVisualizing Uncertainty in Correlation Estimates\nWe can also visualize this uncertainty and the confidence levels in a plot too, using gf_errorbar and a handy set of functions within purrr which is part of the tidyverse. Assuming heights is the target variable we want to correlate every other (quantitative) variable against, we can proceed very quickly as follows: we will first plot correlation uncertainty for one pair of variables to develop the intuition, and then for all variables against the one target variable:\n\nmosaic::cor_test(height ~ mother, data = Galton) %&gt;% \n  broom::tidy() %&gt;% \n\n# We need a graph not a table \n# So comment out this line from the earlier code\n#knitr::kable(digits = 2,caption = \"Children vs Mothers\")\n\nrowid_to_column(var = \"index\") %&gt;% # Need an index to plot with\n  \n  # Uncertainty as error-bars\n  gf_errorbar(conf.high + conf.low ~ index, linewidth = 2) %&gt;% \n  \n  # Estimate as a point\n  gf_point(estimate ~ index, color = \"red\", size = 6) %&gt;% \n  \n  # Labels\n  gf_text(estimate ~ index - 0.2, \n             label = \"Correlation Score = estimate\") %&gt;% \n  gf_text(conf.high*0.98 ~ index - 0.25, \n           label = \"Upper Limit = estimate + conf.high\") %&gt;%   \n  gf_text(conf.low*1.04 ~ index - 0.25, \n           label = \"Lower Limit = estimate - conf.low\") %&gt;% \n  gf_theme(theme_bw())\n\n\n\n\n\n\n\nWe can now do this for all variables against the target variable height, which we identified in our research question. We will use the iteration capabilities offered by the tidyverse package, purrr:\n\nall_corrs &lt;- Galton %&gt;% \n  select(where(is.numeric)) %&gt;% \n  \n  # leave off height to get all the remaining ones\n  select(- height) %&gt;%  \n  \n  # perform a cor.test for all variables against height\n  purrr::map(.x = .,\n             .f = \\(x) cor.test(x, Galton$height)) %&gt;%\n  \n  # tidy up the cor.test outputs into a tidy data frame\n  map_dfr(broom::tidy, .id = \"predictor\") \n\nall_corrs\n\n\n  \n\n\nall_corrs %&gt;% \n  \n  # arrange the predictors in order of their correlation scores\n  # with the target variable (`height`)\n  # Add errorbars to show uncertainty ranges / confidence intervals\n  # Use errorbar width and linewidth fo emphasis\n  gf_errorbar(conf.high + conf.low ~ reorder(predictor, estimate),\n              color = ~ estimate,\n              width = 0.2,\n              linewidth = ~ -log10(p.value)) %&gt;% \n  \n  # All correlation estimates as points\n  gf_point(estimate ~ reorder(predictor, estimate), \n           color = \"black\") %&gt;% \n  \n  # Reference line at zero correlation score\n  gf_hline(yintercept = 0, color = \"grey\", linewidth = 2) %&gt;% \n  \n  # Themes,Titles, and Scales\n  gf_labs(x = NULL, y = \"Correlation with height in Galton\", \n          caption = \"Significance = - log10(p.value)\") %&gt;% \n  \n  gf_refine(\n    \n    # Scale for colour\n scale_colour_distiller(\"Correlation\", type = \"div\", palette = \"RdBu\"),\n            \n    # Scale for dumbbells!!\n    scale_linewidth_continuous(\"significance\",\n                                       range = c(0.5,4))) %&gt;% \n  gf_refine(guides(linewidth = guide_legend(reverse = TRUE))) %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\nWe can clearly see the size of the correlations and the confidence intervals marked in this plot. father has somewhat greater correlation with children’s height, as compared to mother. nkids seems to matter very little. This kind of plot will be very useful when we pursue linear regression models.\n\n\n\n\n\n\nQuestion\n\n\n\nHow can we show this correlation in a set of Scatter Plots + Regression Lines? Can we recreate Galton’s famous diagram?\n\n\n# For the sons\ngf_point(height ~ father, \n         data = Galton %&gt;% filter(sex == \"M\"),\n         title = \"Soms and Fathers\") %&gt;%\n  gf_smooth(method = \"lm\") %&gt;%\n  gf_theme(theme_minimal())\ngf_point(height ~ mother, \n         data = Galton %&gt;% filter(sex == \"M\"),\n         title = \"Sons and Mothers\") %&gt;%\n  gf_smooth(method = \"lm\") %&gt;%\n  gf_theme(theme_minimal())\n# For the daughters\ngf_point(height ~ father, \n         data = Galton %&gt;% filter(sex == \"F\"),\n         title = \"Daughters and Fathers\") %&gt;%\n  gf_smooth(method = \"lm\") %&gt;%\n  gf_theme(theme_minimal())\ngf_point(height ~ mother, \n         data = Galton %&gt;% filter(sex == \"F\"),\n         title = \"Daughters and Mothers\") %&gt;%\n  gf_smooth(method = \"lm\") %&gt;% \n  gf_theme(theme_minimal())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn approximation to Galton’s famous plot1 (see Wikipedia):\n\n\n\n\n\n\n\n\ngf_point(height ~ (father + mother)/2, data = Galton) %&gt;% \n  gf_smooth(method = \"lm\") %&gt;% \n  gf_density_2d(n = 8) %&gt;% \n  gf_abline(slope = 1) %&gt;% \n  gf_theme(theme_minimal())\ngf_point(height ~ (father + mother)/2, data = Galton) %&gt;% \n  gf_smooth(method = \"lm\") %&gt;% \n  gf_ellipse(level = 0.95, color = \"red\") %&gt;% \n    gf_ellipse(level = 0.75, color = \"blue\") %&gt;% \n    gf_ellipse(level = 0.5, color = \"green\") %&gt;% \n  gf_abline(slope = 1) %&gt;% \n  gf_theme(theme_minimal())\n\n\n\n\n\n\n\n\n\n\nHow would you interpret this plot2?"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#case-study2-dataset-from-nhanes",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#case-study2-dataset-from-nhanes",
    "title": "Tutorial on Correlations in R",
    "section": "\n Case Study#2: Dataset from NHANES\n",
    "text": "Case Study#2: Dataset from NHANES\n\nLet us look at the NHANES dataset from the package NHANES:\n\ndata(\"NHANES\")\n\n\n Inspecting the Data\nNHANES_describe &lt;- inspect(NHANES)\n\nNHANES_describe$categorical\nNHANES_describe$quantitative\nNHANES\nskimr::skim(NHANES)\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n\n\nData summary\n\n\nName\nNHANES\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n76\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n31\n\n\nnumeric\n45\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nSurveyYr\n0\n1.00\nFALSE\n2\n200: 5000, 201: 5000\n\n\nGender\n0\n1.00\nFALSE\n2\nfem: 5020, mal: 4980\n\n\nAgeDecade\n333\n0.97\nFALSE\n8\n40: 1398, 0-: 1391, 10: 1374, 20: 1356\n\n\nRace1\n0\n1.00\nFALSE\n5\nWhi: 6372, Bla: 1197, Mex: 1015, Oth: 806\n\n\nRace3\n5000\n0.50\nFALSE\n6\nWhi: 3135, Bla: 589, Mex: 480, His: 350\n\n\nEducation\n2779\n0.72\nFALSE\n5\nSom: 2267, Col: 2098, Hig: 1517, 9 -: 888\n\n\nMaritalStatus\n2769\n0.72\nFALSE\n6\nMar: 3945, Nev: 1380, Div: 707, Liv: 560\n\n\nHHIncome\n811\n0.92\nFALSE\n12\nmor: 2220, 750: 1084, 250: 958, 350: 863\n\n\nHomeOwn\n63\n0.99\nFALSE\n3\nOwn: 6425, Ren: 3287, Oth: 225\n\n\nWork\n2229\n0.78\nFALSE\n3\nWor: 4613, Not: 2847, Loo: 311\n\n\nBMICatUnder20yrs\n8726\n0.13\nFALSE\n4\nNor: 805, Obe: 221, Ove: 193, Und: 55\n\n\nBMI_WHO\n397\n0.96\nFALSE\n4\n18.: 2911, 30.: 2751, 25.: 2664, 12.: 1277\n\n\nDiabetes\n142\n0.99\nFALSE\n2\nNo: 9098, Yes: 760\n\n\nHealthGen\n2461\n0.75\nFALSE\n5\nGoo: 2956, Vgo: 2508, Fai: 1010, Exc: 878\n\n\nLittleInterest\n3333\n0.67\nFALSE\n3\nNon: 5103, Sev: 1130, Mos: 434\n\n\nDepressed\n3327\n0.67\nFALSE\n3\nNon: 5246, Sev: 1009, Mos: 418\n\n\nSleepTrouble\n2228\n0.78\nFALSE\n2\nNo: 5799, Yes: 1973\n\n\nPhysActive\n1674\n0.83\nFALSE\n2\nYes: 4649, No: 3677\n\n\nTVHrsDay\n5141\n0.49\nFALSE\n7\n2_h: 1275, 1_h: 884, 3_h: 836, 0_t: 638\n\n\nCompHrsDay\n5137\n0.49\nFALSE\n7\n0_t: 1409, 0_h: 1073, 1_h: 1030, 2_h: 589\n\n\nAlcohol12PlusYr\n3420\n0.66\nFALSE\n2\nYes: 5212, No: 1368\n\n\nSmokeNow\n6789\n0.32\nFALSE\n2\nNo: 1745, Yes: 1466\n\n\nSmoke100\n2765\n0.72\nFALSE\n2\nNo: 4024, Yes: 3211\n\n\nSmoke100n\n2765\n0.72\nFALSE\n2\nNon: 4024, Smo: 3211\n\n\nMarijuana\n5059\n0.49\nFALSE\n2\nYes: 2892, No: 2049\n\n\nRegularMarij\n5059\n0.49\nFALSE\n2\nNo: 3575, Yes: 1366\n\n\nHardDrugs\n4235\n0.58\nFALSE\n2\nNo: 4700, Yes: 1065\n\n\nSexEver\n4233\n0.58\nFALSE\n2\nYes: 5544, No: 223\n\n\nSameSex\n4232\n0.58\nFALSE\n2\nNo: 5353, Yes: 415\n\n\nSexOrientation\n5158\n0.48\nFALSE\n3\nHet: 4638, Bis: 119, Hom: 85\n\n\nPregnantNow\n8304\n0.17\nFALSE\n3\nNo: 1573, Yes: 72, Unk: 51\n\n\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nID\n0\n1.00\n61944.64\n5871.17\n51624.00\n56904.50\n62159.50\n67039.00\n71915.00\n▇▇▇▇▇\n\n\nAge\n0\n1.00\n36.74\n22.40\n0.00\n17.00\n36.00\n54.00\n80.00\n▇▇▇▆▅\n\n\nAgeMonths\n5038\n0.50\n420.12\n259.04\n0.00\n199.00\n418.00\n624.00\n959.00\n▇▇▇▆▃\n\n\nHHIncomeMid\n811\n0.92\n57206.17\n33020.28\n2500.00\n30000.00\n50000.00\n87500.00\n100000.00\n▃▆▃▁▇\n\n\nPoverty\n726\n0.93\n2.80\n1.68\n0.00\n1.24\n2.70\n4.71\n5.00\n▅▅▃▃▇\n\n\nHomeRooms\n69\n0.99\n6.25\n2.28\n1.00\n5.00\n6.00\n8.00\n13.00\n▂▆▇▂▁\n\n\nWeight\n78\n0.99\n70.98\n29.13\n2.80\n56.10\n72.70\n88.90\n230.70\n▂▇▂▁▁\n\n\nLength\n9457\n0.05\n85.02\n13.71\n47.10\n75.70\n87.00\n96.10\n112.20\n▁▃▆▇▃\n\n\nHeadCirc\n9912\n0.01\n41.18\n2.31\n34.20\n39.58\n41.45\n42.92\n45.40\n▁▂▇▇▅\n\n\nHeight\n353\n0.96\n161.88\n20.19\n83.60\n156.80\n166.00\n174.50\n200.40\n▁▁▁▇▂\n\n\nBMI\n366\n0.96\n26.66\n7.38\n12.88\n21.58\n25.98\n30.89\n81.25\n▇▆▁▁▁\n\n\nPulse\n1437\n0.86\n73.56\n12.16\n40.00\n64.00\n72.00\n82.00\n136.00\n▂▇▃▁▁\n\n\nBPSysAve\n1449\n0.86\n118.15\n17.25\n76.00\n106.00\n116.00\n127.00\n226.00\n▃▇▂▁▁\n\n\nBPDiaAve\n1449\n0.86\n67.48\n14.35\n0.00\n61.00\n69.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nBPSys1\n1763\n0.82\n119.09\n17.50\n72.00\n106.00\n116.00\n128.00\n232.00\n▂▇▂▁▁\n\n\nBPDia1\n1763\n0.82\n68.28\n13.78\n0.00\n62.00\n70.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys2\n1647\n0.84\n118.48\n17.49\n76.00\n106.00\n116.00\n128.00\n226.00\n▃▇▂▁▁\n\n\nBPDia2\n1647\n0.84\n67.66\n14.42\n0.00\n60.00\n68.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys3\n1635\n0.84\n117.93\n17.18\n76.00\n106.00\n116.00\n126.00\n226.00\n▃▇▂▁▁\n\n\nBPDia3\n1635\n0.84\n67.30\n14.96\n0.00\n60.00\n68.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nTestosterone\n5874\n0.41\n197.90\n226.50\n0.25\n17.70\n43.82\n362.41\n1795.60\n▇▂▁▁▁\n\n\nDirectChol\n1526\n0.85\n1.36\n0.40\n0.39\n1.09\n1.29\n1.58\n4.03\n▅▇▂▁▁\n\n\nTotChol\n1526\n0.85\n4.88\n1.08\n1.53\n4.11\n4.78\n5.53\n13.65\n▂▇▁▁▁\n\n\nUrineVol1\n987\n0.90\n118.52\n90.34\n0.00\n50.00\n94.00\n164.00\n510.00\n▇▅▂▁▁\n\n\nUrineFlow1\n1603\n0.84\n0.98\n0.95\n0.00\n0.40\n0.70\n1.22\n17.17\n▇▁▁▁▁\n\n\nUrineVol2\n8522\n0.15\n119.68\n90.16\n0.00\n52.00\n95.00\n171.75\n409.00\n▇▆▃▂▁\n\n\nUrineFlow2\n8524\n0.15\n1.15\n1.07\n0.00\n0.48\n0.76\n1.51\n13.69\n▇▁▁▁▁\n\n\nDiabetesAge\n9371\n0.06\n48.42\n15.68\n1.00\n40.00\n50.00\n58.00\n80.00\n▁▂▆▇▂\n\n\nDaysPhysHlthBad\n2468\n0.75\n3.33\n7.40\n0.00\n0.00\n0.00\n3.00\n30.00\n▇▁▁▁▁\n\n\nDaysMentHlthBad\n2466\n0.75\n4.13\n7.83\n0.00\n0.00\n0.00\n4.00\n30.00\n▇▁▁▁▁\n\n\nnPregnancies\n7396\n0.26\n3.03\n1.80\n1.00\n2.00\n3.00\n4.00\n32.00\n▇▁▁▁▁\n\n\nnBabies\n7584\n0.24\n2.46\n1.32\n0.00\n2.00\n2.00\n3.00\n12.00\n▇▅▁▁▁\n\n\nAge1stBaby\n8116\n0.19\n22.65\n4.77\n14.00\n19.00\n22.00\n26.00\n39.00\n▆▇▅▂▁\n\n\nSleepHrsNight\n2245\n0.78\n6.93\n1.35\n2.00\n6.00\n7.00\n8.00\n12.00\n▁▅▇▁▁\n\n\nPhysActiveDays\n5337\n0.47\n3.74\n1.84\n1.00\n2.00\n3.00\n5.00\n7.00\n▇▇▃▅▅\n\n\nTVHrsDayChild\n9347\n0.07\n1.94\n1.43\n0.00\n1.00\n2.00\n3.00\n6.00\n▇▆▂▂▂\n\n\nCompHrsDayChild\n9347\n0.07\n2.20\n2.52\n0.00\n0.00\n1.00\n6.00\n6.00\n▇▁▁▁▃\n\n\nAlcoholDay\n5086\n0.49\n2.91\n3.18\n1.00\n1.00\n2.00\n3.00\n82.00\n▇▁▁▁▁\n\n\nAlcoholYear\n4078\n0.59\n75.10\n103.03\n0.00\n3.00\n24.00\n104.00\n364.00\n▇▁▁▁▁\n\n\nSmokeAge\n6920\n0.31\n17.83\n5.33\n6.00\n15.00\n17.00\n19.00\n72.00\n▇▂▁▁▁\n\n\nAgeFirstMarij\n7109\n0.29\n17.02\n3.90\n1.00\n15.00\n16.00\n19.00\n48.00\n▁▇▂▁▁\n\n\nAgeRegMarij\n8634\n0.14\n17.69\n4.81\n5.00\n15.00\n17.00\n19.00\n52.00\n▂▇▁▁▁\n\n\nSexAge\n4460\n0.55\n17.43\n3.72\n9.00\n15.00\n17.00\n19.00\n50.00\n▇▅▁▁▁\n\n\nSexNumPartnLife\n4275\n0.57\n15.09\n57.85\n0.00\n2.00\n5.00\n12.00\n2000.00\n▇▁▁▁▁\n\n\nSexNumPartYear\n5072\n0.49\n1.34\n2.78\n0.00\n1.00\n1.00\n1.00\n69.00\n▇▁▁▁▁\n\n\n\n\n\n\nTry help(\"NHANES\") in your Console.\n\nThis is survey data collected by the US National Center for Health Statistics (NCHS) which has conducted a series of health and nutrition surveys since the early 1960’s. Since 1999 approximately 5,000 individuals of all ages are interviewed in their homes every year and complete the health examination component of the survey. The health examination is conducted in a mobile examination centre (MEC).\n\nThe dataset is described as: A data frame with 100000 observations on 76 variables. Some of these are:\n- Race1 and Race2: factors with 5 and 6 levels respectively\n- Education a factor with 5 levels\n- HHIncomeMid Total annual gross income for the household in US dollars.\n- Age\n- BMI: Body mass index (weight/height2 in kg/m2)\n- Height: Standing height in cm.\n- Weight: Weight in kg &gt; &gt; - Testosterone: Testosterone total (ng/dL) - PhysActiveDays: Number of days in a typical week that participant does moderate or vigorous-intensity activity.\n- CompHrsDay: Number of hours per day on average participant used a computer or gaming device over the past 30 days.\n\n\n\n\n\n\nMissing Data\n\n\n\nWhy do so many of the variables have missing entries? What could be your guess about the Experiment/Survey`?\n\n\nLet us make some counts of the data, since we have so many factors:\n\nNHANES %&gt;% count(Gender)\n\n\n  \n\n\nNHANES %&gt;% count(Race1)\n\n\n  \n\n\nNHANES %&gt;% count(Race3)\n\n\n  \n\n\nNHANES %&gt;% count(Education)\n\n\n  \n\n\nNHANES %&gt;% count(MaritalStatus)\n\n\n  \n\n\n\nThere is a good mix of factors and counts.\nNow we articulate our Research Questions:\n\n\n\n\n\n\nResearch Questions\n\n\n\n\nDoes Testosterone have a relationship with parameters such as BMI, Weight, Height, PhysActiveDays CompHrsDay and Age?\nDoes HHIncomeMid have a relationship with these same parameters? And with Gender?\nAre there any other pairwise correlations that we should note? (This is especially useful in choosing independent variables for multiple regression)\n\n\n\n( Yes we are concerned with men more than with the women, sadly.)\n\n Correlations and Plots\n\nGGally::ggpairs(NHANES, \n                # Choose the variables we want to plot for\n                columns = c(\"HHIncomeMid\", \"Weight\", \"Height\", \n                            \"BMI\", \"Gender\"), \n                \n                # LISTs of graphs needed at different locations\n                # For different combinations of variables \n                diag = list(continuous = \"barDiag\"),\n                lower = list(continuous = wrap(\"smooth\", alpha = 0.01)),\n                upper = list(continuous = \"cor\"),\n                \n                switch = \"both\", # axis labels in more traditional locations\n                progress = FALSE ) + # No compute progress bars needed\n  theme_bw()\n\n\n\n\n\n\n\nWe see that HHIncomeMid is Quantitative, discrete valued variable, since it is based on a set of median incomes for different ranges of income. BMI, Weight, Height are continuous Quant variables.\nHHIncomeMid also seems to be relatively unaffected by Weight; And is only mildly correlated with Height and BMI, as seen both by the correlation score magnitudes and the slopes of the trend lines.\nThere is a difference in the median income by Gender, but we will defer that kind of test for later, when we do Statistical Inference.\nUnsurprisingly, BMI and Weight have a strong relationship, as do Height and Weight; the latter is of course non-linear, since the Height levels off at a point.\n\nGGally::ggpairs(NHANES, \n                columns = c(\"Testosterone\", \"Weight\", \"Height\", \"BMI\"), \n                \n                diag = list(continuous = \"barDiag\"),\n                lower = list(continuous = wrap(\"smooth\", alpha = 0.01)),\n                upper = list(continuous = \"cor\"),\n                \n                switch = \"both\",\n                progress = FALSE ) +\n  theme_bw()\n\n\n\n\n\n\n\nIt is clear that Testosterone has strong relationships with Height and Weight but not so much with BMI.\n\n Visualizing Uncertainty in Correlation Estimates\nSince the pairs plot is fairly clear for both target variables, let us head to visualizing the significance and uncertainty in the correlation estimates.\n\nHHIncome_corrs &lt;- NHANES %&gt;% \n  select(where(is.numeric)) %&gt;% \n  \n  # leave off height to get all the remaining ones\n  select(- HHIncomeMid) %&gt;%  \n  \n  # perform a cor.test for all variables against height\n  purrr::map(.x = .,\n             .f = \\(x) cor.test(x, NHANES$HHIncomeMid)) %&gt;%\n  \n  # tidy up the cor.test outputs into a tidy data frame\n  map_dfr(broom::tidy, .id = \"predictor\") \n\nHHIncome_corrs\n\n\n  \n\n\nHHIncome_corrs %&gt;% \n  \n  # Reference line at zero correlation score\n  gf_hline(yintercept = 0, color = \"grey\", linewidth = 2) %&gt;% \n  \n  # arrange the predictors in order of their correlation scores\n  # with the target variable (`height`)\n  # Add errorbars to show uncertainty ranges / confidence intervals\n  # Use errorbar width and linewidth fo emphasis\n  gf_errorbar(conf.high + conf.low ~ reorder(predictor, estimate),\n              color = ~ estimate,\n              width = 0.2,\n              linewidth = ~ -log10(p.value + 0.001)) %&gt;% \n  \n  # All correlation estimates as points\n  gf_point(estimate ~ reorder(predictor, estimate), \n           color = \"black\") %&gt;% \n  \n  # Themes,Titles, and Scales\n  gf_labs(x = NULL, y = \"Correlations with HouseHold Median Income\", \n          caption = \"Significance = - log10(p.value)\") %&gt;% \n  gf_theme(theme_classic()) %&gt;%\n\n  \n  # Scale for colour\n  gf_refine(guides(linewidth = guide_legend(reverse = TRUE)),\n            scale_colour_distiller(\"Correlation\", type = \"div\", \n                                    palette = \"RdBu\"),\n            \n  # Scale for dumbbells!!\n  scale_linewidth_continuous(\"Significance\", range = c(0.05,2)),\n  \n  theme(axis.text.y = element_text(size = 6, hjust = 1)),\n  coord_flip()) \n\n\n\n\n\n\n\nIf we select just the variables from our Research Question:\n\nHHIncome_corrs_select &lt;- NHANES %&gt;% \n  select(Height, Weight, BMI) %&gt;% # Only change is here!\n  \n  # leave off height to get all the remaining ones\n  #select(- HHIncomeMid) %&gt;%  \n  \n  # perform a cor.test for all variables against height\n  purrr::map(.x = .,\n             .f = \\(x) cor.test(x, NHANES$HHIncomeMid)) %&gt;%\n  \n  # tidy up the cor.test outputs into a tidy data frame\n  map_dfr(broom::tidy, .id = \"predictor\") \n\nHHIncome_corrs_select\n\n\n  \n\n\nHHIncome_corrs_select %&gt;% \n  \n  # arrange the predictors in order of their correlation scores\n  # with the target variable (`height`)\n  # Add errorbars to show uncertainty ranges / confidence intervals\n  # Use errorbar width and linewidth fo emphasis\n  gf_errorbar(conf.high + conf.low ~ reorder(predictor, estimate),\n              color = ~ estimate,\n              width = 0.2,\n              linewidth = ~ -log10(p.value + 0.000001)) %&gt;% \n  \n  # All correlation estimates as points\n  gf_point(estimate ~ reorder(predictor, estimate), \n           color = \"black\") %&gt;% \n  \n  # Reference line at zero correlation score\n  gf_hline(yintercept = 0, color = \"grey\", linewidth = 2) %&gt;% \n  \n  # Themes,Titles, and Scales\n  gf_labs(x = NULL, y = \"Correlations with HouseHold Median Income\", \n          caption = \"Significance = - log10(p.value + 0.000001)\") %&gt;% \n  \n  gf_theme(theme_classic()) %&gt;%\n\n  \n  # Scale for colour\n  gf_refine(guides(linewidth = guide_legend(reverse = TRUE)),\n            scale_colour_distiller(\"Correlation\", type = \"div\", \n                                    palette = \"RdBu\"),\n            \n  # Scale for dumbbells!!\n  scale_linewidth_continuous(\"Significance\", range = c(0.05,2)),\n  \n  theme(axis.text.y = element_text(size = 8, hjust = 1)),\n  coord_flip()) \n\n\n\n\n\n\n\nSo we might say taller people make more money? And fatter people make slightly less money? Well, the magnitude of the correlations (aka effect size) are low so we would not imagine this to be a hypothesis that we can defend.\nLet us look at the Testosterone variable: trying all variables shows some paucity of observations ( due to missing data), so we will stick with our chosen variables:\n\nTestosterone_corrs &lt;- NHANES %&gt;%\n  select(Height, Weight, BMI) %&gt;%\n  \n  # leave off height to get all the remaining ones\n  #select(- Testosterone) %&gt;%\n  \n  # perform a cor.test for all variables against height\n  purrr::map(.x = .,\n             .f = \\(x) cor.test(x, NHANES$Testosterone)) %&gt;%\n  \n  # tidy up the cor.test outputs into a tidy data frame\n  map_dfr(broom::tidy, .id = \"predictor\")\n\nTestosterone_corrs\n\n\n  \n\n\nTestosterone_corrs %&gt;%\n  \n  # Reference line at zero correlation score\n  gf_hline(yintercept = 0,\n           color = \"grey\",\n           linewidth = 2) %&gt;%\n  \n  # arrange the predictors in order of their correlation scores\n  # with the target variable (`height`)\n  # Add errorbars to show uncertainty ranges / confidence intervals\n  # Use errorbar width and linewidth fo emphasis\n  gf_errorbar(\n    conf.high + conf.low ~ reorder(predictor, estimate),\n    color = ~ estimate,\n    width = 0.2,\n    linewidth = ~ -log10(p.value + 0.000001)\n  ) %&gt;%\n  \n  # All correlation estimates as points\n  gf_point(estimate ~ reorder(predictor, estimate),\n           color = \"black\") %&gt;%\n  \n  \n  # Themes,Titles, and Scales\n  gf_labs(x = NULL, y = \"Correlations with Testosterone Levels\",\n          caption = \"Significance = - log10(p.value + 0.000001)\") %&gt;%\n  \n  gf_theme(theme_classic()) %&gt;%\n  \n  \n  # Scale for colour\n  gf_refine(\n    guides(linewidth = guide_legend(reverse = TRUE)),\n    scale_colour_distiller(\"Correlation\", type = \"div\",\n                           palette = \"RdBu\"),\n    \n    # Scale for dumbbells!!\n    scale_linewidth_continuous(\"Significance\", range = c(0.05, 2)),\n    \n    theme(axis.text.y = element_text(size = 8, hjust = 1)),\n    coord_flip()\n  )"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#conclusion",
    "title": "Tutorial on Correlations in R",
    "section": "\n Conclusion",
    "text": "Conclusion\nWe have a decent Correlations related workflow in R:\n\nLoad the dataset\n\ninspect/skim/glimpse the dataset, identify Quant and Qual variables\nIdentify a target variable based on your knowledge of the data, how it was gathered, who gathered it and what was their intent\nDevelop Pair-Wise plots + Correlations using GGally::ggpairs()\n\nDevelop Correlogram corrplot::corrplot\n\nCheck everything with a cor_test: effect size,significance, confidence intervals\nUse purrr + cor.test to plot correlations and confidence intervals for multiple Quant predictor variables against the target variable\nPlot scatter plots using gf_point.\nAdd extra lines using gf_abline() to compare hypotheses that you may have."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#references",
    "title": "Tutorial on Correlations in R",
    "section": "\n References",
    "text": "References\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\ncorrplot\n0.92\nWei and Simko (2021)\n\n\nGGally\n2.2.1\nSchloerke et al. (2024)\n\n\nggformula\n0.12.0\nKaplan and Pruim (2023)\n\n\nmosaic\n1.9.1\nPruim, Kaplan, and Horton (2017)\n\n\nmosaicData\n0.20.4\nPruim, Kaplan, and Horton (2023)\n\n\nNHANES\n2.1.0\nPruim (2015)\n\n\nTeachHist\n0.2.1\nLange (2023)\n\n\nTeachingDemos\n2.13\nSnow (2024)\n\n\n\n\n\n\nKaplan, Daniel, and Randall Pruim. 2023. ggformula: Formula Interface to the Grammar of Graphics. https://CRAN.R-project.org/package=ggformula.\n\n\nLange, Carsten. 2023. TeachHist: A Collection of Amended Histograms Designed for Teaching Statistics. https://CRAN.R-project.org/package=TeachHist.\n\n\nPruim, Randall. 2015. NHANES: Data from the US National Health and Nutrition Examination Study. https://CRAN.R-project.org/package=NHANES.\n\n\nPruim, Randall, Daniel T Kaplan, and Nicholas J Horton. 2017. “The Mosaic Package: Helping Students to ‘Think with Data’ Using r.” The R Journal 9 (1): 77–102. https://journal.r-project.org/archive/2017/RJ-2017-024/index.html.\n\n\nPruim, Randall, Daniel Kaplan, and Nicholas Horton. 2023. mosaicData: Project MOSAIC Data Sets. https://CRAN.R-project.org/package=mosaicData.\n\n\nSchloerke, Barret, Di Cook, Joseph Larmarange, Francois Briatte, Moritz Marbach, Edwin Thoen, Amos Elberg, and Jason Crowley. 2024. GGally: Extension to “ggplot2”. https://CRAN.R-project.org/package=GGally.\n\n\nSnow, Greg. 2024. TeachingDemos: Demonstrations for Teaching and Learning. https://CRAN.R-project.org/package=TeachingDemos.\n\n\nWei, Taiyun, and Viliam Simko. 2021. R Package “corrplot”: Visualization of a Correlation Matrix. https://github.com/taiyun/corrplot."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#footnotes",
    "title": "Tutorial on Correlations in R",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttp://euclid.psych.yorku.ca/SCS/Gallery/images/galton-corr.jpg&gt;↩︎\nhttps://www.researchgate.net/figure/Galtons-smoothed-correlation-diagram-for-the-data-on-heights-of-parents-and-children_fig15_226400313↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/part-whole.html",
    "href": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/part-whole.html",
    "title": "🍕 Parts of a Whole",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(ggformula)\nlibrary(plotrix) # Fan, Pyramid Chart\n#devtools::install_github(\"zmeers/ggparliament\")\nlibrary(ggparliament) # Parliament Chart\nlibrary(ggpol) # Parliament, Arc-Bar and other interesting charts\nlibrary(data.tree) # Many plots related to heirarchical data\n# install.packages(\"waffle\", repos = \"https://cinc.rud.is\")\nlibrary(waffle)\nlibrary(tidygraph) # Trees, Dendros, and Circle Packings\nlibrary(ggraph) # Trees, Dendros, and Circle Packings\nlibrary(echarts4r) # Interactive Charts\n\nlibrary(patchwork) # Arrange your plots",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🍕 Parts of a Whole"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/part-whole.html#setting-up-the-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/part-whole.html#setting-up-the-packages",
    "title": "🍕 Parts of a Whole",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(ggformula)\nlibrary(plotrix) # Fan, Pyramid Chart\n#devtools::install_github(\"zmeers/ggparliament\")\nlibrary(ggparliament) # Parliament Chart\nlibrary(ggpol) # Parliament, Arc-Bar and other interesting charts\nlibrary(data.tree) # Many plots related to heirarchical data\n# install.packages(\"waffle\", repos = \"https://cinc.rud.is\")\nlibrary(waffle)\nlibrary(tidygraph) # Trees, Dendros, and Circle Packings\nlibrary(ggraph) # Trees, Dendros, and Circle Packings\nlibrary(echarts4r) # Interactive Charts\n\nlibrary(patchwork) # Arrange your plots",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🍕 Parts of a Whole"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/part-whole.html#what-graphs-will-we-see-today",
    "href": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/part-whole.html#what-graphs-will-we-see-today",
    "title": "🍕 Parts of a Whole",
    "section": "\n What Graphs will we see today?",
    "text": "What Graphs will we see today?\nThere are a good few charts available to depict things that constitute other bigger things. We will discuss a few of these: Pie, Fan, and Donuts; Waffle and Parliament charts; Trees, Dendrograms, and Circle Packings. (The last three visuals we will explore along with network diagrams in a later module.)",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🍕 Parts of a Whole"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/part-whole.html#pies-and-fans",
    "href": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/part-whole.html#pies-and-fans",
    "title": "🍕 Parts of a Whole",
    "section": "\n Pies and Fans",
    "text": "Pies and Fans\nSo let us start with “eating humble pie”: discussing a Pie chart first.\nA pie chart is a circle divided into sectors that each represent a proportion of the whole. It is often used to show percentage, where the sum of the sectors equals 100%.\nThe problem is that humans are pretty bad at reading angles. This ubiquitous chart is much vilified in the industry and bar charts that we have seen earlier, are viewed as better options. On the other hand, pie charts are ubiquitous in business circles, and are very much accepted! Do also read this spirited defense of pie charts here. https://speakingppt.com/why-tufte-is-flat-out-wrong-about-pie-charts/\nAnd we will also see that there is an attractive, and similar-looking alternative, called a fan chart which we will explore here.\n\n\nUsing Base R\nUsing ggformula\nUsing echarts4r\n\n\n\nBase R has a simple pie command that does the job. Let’s create some toy data first:\npie_data &lt;- tibble(\n  sales = c(0.12, 0.3, 0.26, 0.16, 0.04, 0.12), \n  \n  # Labels MUST be character entries for `pie` to work\n  labels = c(\"Blueberry\",\"Cherry\",\"Apple\",\"Boston Cream\",\n             \"Other\",\"Vanilla Cream\")\n  )\npie_data\npie(\n  x = pie_data$sales,\n  labels = pie_data$labels, # Character Vector is a MUST\n\n  # Pie is within a square of 1 X 1 units\n  # Reduce radius if needed to see labels properly\n  radius = 0.95,\n  \n  init.angle = 90, # First slice starts at 12 o'clock position\n  \n  # Change the default colours. Comment this and see what happens. \n  col =  grDevices::hcl.colors(palette = \"Plasma\", n = 6)\n  )\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nWe create a bar chart or a column chart as appropriate, with bars filled by category. The width parameter is set to 1 so that the bars touch. The bars have a fixed width along the x-axis; the height of the bar varies based on the number we wish to show. Then the coord_polar(theta = \"y\") converts the bar plot into a pie.\n# Set graph theme\ntheme_set(new = theme_custom())\n\n# Using gf_col since we have a count/value column already\npie_data %&gt;%\n  gf_col(sales ~ 1, fill = ~ labels, width = 1) \npie_data %&gt;%\n  gf_col(sales ~ 1, fill = ~ labels, width = 1)  %&gt;%\n  gf_refine(coord_polar(theta = \"y\"))\n# Using gf_bar since we don't have ready made counts\ngf_bar(data = mpg,\n       ~ 1,\n       fill = ~ drv,\n       color = \"black\", # border for the bars/slices\n       width =  1)\ngf_bar(data = mpg,\n       ~ 0.5,\n       fill = ~ drv,\n       color = \"black\", # border for the bars/slices\n       width =  1) %&gt;%\n  gf_theme(theme_minimal()) %&gt;%\n  gf_theme(theme(axis.line.y = element_blank(),\n                 axis.text.y = element_blank(),\n                 axis.title.y = element_blank())) %&gt;%\n  gf_refine(coord_polar(theta = \"y\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere is a basic interactive pie chart withecharts4r:\n\npie_data &lt;- tibble(\n  sales = c(0.12, 0.3, 0.26, 0.16, 0.04, 0.12), \n\n  labels = c(\"Blueberry\",\"Cherry\",\"Apple\",\"Boston Cream\",\"Other\",\n             \"Vanilla Cream\"))\npie_data %&gt;% \n  e_charts(x = labels) %&gt;% \n  e_pie(serie = sales, clockwise = TRUE, \n        startAngle = 90) %&gt;% \n  e_legend(list(orient = \"vertical\",\n                      left = \"right\")) %&gt;% \n  e_tooltip()\n\n\n\n\n\nWe can add more bells and whistles to the humble-pie chart, and make a Nightingale rosechart out of it:\npie_data &lt;- tibble(\n  sales = c(0.12, 0.3, 0.26, 0.16, 0.04, 0.12), \n\n  labels = c(\"Blueberry\",\"Cherry\",\"Apple\",\"Boston Cream\",\"Other\",\n             \"Vanilla Cream\"))\npie_data %&gt;% \n  e_charts(x = labels) %&gt;% \n  e_pie(serie = sales, clockwise = TRUE, \n        startAngle = 90, \n        roseType = \"area\") %&gt;% # try \"radius\"\n  \n  # Lets move the legend\n  e_legend(left = \"right\", orient = \"vertical\") %&gt;% \n  e_tooltip()\npie_data %&gt;% \n  e_charts(x = labels) %&gt;% \n  e_pie(serie = sales, clockwise = TRUE, \n        startAngle = 90, \n        roseType = \"radius\") %&gt;% \n  \n  # Lets move the legend\n  e_legend(left = \"right\", orient = \"vertical\") %&gt;% \n  e_tooltip()\n\n\n\n\n\n\n\n\n\n\n\n\nFor more information and customization look at https://echarts.apache.org/en/option.html#series-pie\n\n\n\nThe fan Plot\nThe fan plot (from the plotrix package) displays numerical values as arcs of overlapping sectors. This allows for more effective comparison:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nplotrix::fan.plot(\n  x = pie_data$sales,\n  labels = pie_data$labels,\n  \n  col = grDevices::hcl.colors(palette = \"Plasma\", n = 6),\n  shrink = 0.03,\n  # How much to shrink each successive sector\n\n  label.radius = 1.15,\n  main = \"Fan Plot of Ice Cream Flavours\",\n  # ticks = 360,\n  # if we want tick marks on the circumference\n  \n  max.span = pi\n)\n\n\n\n\n\n\n\nThere is no fan plot possible with echarts4r, as far as I know.\nThe Donut Chart\nThe donut chart suffers from the same defects as the pie, so should be used with discretion. The donut chart is essentially a gf_rect from ggformula, plotted on a polar coordinate set of of axes:\n\n\nUsing ggformula\nUsing echarts4r\n\n\n\nLet us make some toy data:\n# Set graph theme\ntheme_set(new = theme_custom())\n\n# Data\ndf &lt;- tibble(group = LETTERS[1:3],\n                 value = c(25, 20, 35))\n\ndf &lt;-\n  df %&gt;% \n  dplyr::mutate(fraction = value / sum(value), # percentages\n    ymax = cumsum(fraction), # cumulative percentages\n    ymin = lag(ymax, 1, default = 0),\n    # bottom edge of each\n    label = paste0(group, \"\\n value: \", value),\n    labelPosition = (ymax + ymin) / 2 # labels midway on arcs\n  )\n\ndf\ndf %&gt;%  \n  # gf_rect() formula: ymin + ymax ~ xmin + xmax\n  # Bars with varying thickness (y) proportional to data\n  # Fixed length x (2 to 4)\n  gf_rect(ymin + ymax ~ 2 + 4,\n          fill = ~ group, colour = \"black\") %&gt;%\n  \n  gf_label(labelPosition ~ 3.5, \n           label = ~ label,\n           size = 4) %&gt;%\n\n# When switching to polar coords:\n# x maps to radius\n# y maps to angle theta\n# so we create a \"hole\" in the radius, in x \n  gf_refine(coord_polar(theta = \"y\", \n                        direction = 1)) %&gt;% \n            # Up to here will give us a pie chart\n  \n  # Now to create the hole\n  # try to play with the \"0\"\n  # Recall x = [2,4]\n  gf_refine(xlim(c(-2, 5))) %&gt;% \n\n  gf_theme(theme = theme_void()) %&gt;% \n  gf_theme(legend.position = \"none\")\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nThe donut chart is simply a variant of the pie chart in echarts4r:\ndf &lt;- tibble(group = LETTERS[1:3],\n                 value = c(25, 20, 35))\n\ndf &lt;-\n  df %&gt;% \n  dplyr::mutate(fraction = value / sum(value), # percentages\n    ymax = cumsum(fraction), # cumulative percentages\n    ymin = lag(ymax, 1, default = 0),\n    # bottom edge of each\n    label = paste0(group, \"\\n value: \", value),\n    labelPosition = (ymax + ymin) / 2 # labels midway on arcs\n  )\ndf\ndf %&gt;% \n  e_charts(x = group, width = 400) %&gt;% \n  e_pie(serie = value, \n        clockwise = TRUE, \n        startAngle = 90,\n        \n        radius = c(\"50%\", \"70%\")\n        ) %&gt;% \n  \n  e_legend(left = \"right\", orient = \"vertical\") %&gt;% \n  e_tooltip()",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🍕 Parts of a Whole"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/part-whole.html#waffle-charts",
    "href": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/part-whole.html#waffle-charts",
    "title": "🍕 Parts of a Whole",
    "section": "\n Waffle Charts",
    "text": "Waffle Charts\nWaffle charts are often called “square pie charts” !\nHere we will need to step outside of ggformula and get into ggplot itself momentarily. (Always remember that ggformula is a simplified and intuitive method that runs on top of ggplot.) We will use the waffle package.\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\n# install.packages(\"waffle\", repos = \"https://cinc.rud.is\")\nlibrary(waffle)\n\n# Data\ndf &lt;- tibble(group = LETTERS[1:3],\n                 value = c(25, 20, 35))\ndf\n\n\n  \n\n\n# Waffle plot\n# Using ggplot, sadly not yet ggformula\nggplot(df, aes(fill = group, values = value)) +\n  geom_waffle(\n    n_rows = 8,\n    size = 0.33,\n    colour = \"white\",\n    na.rm = TRUE\n  ) +\n  scale_fill_manual(\n    name = NULL,\n    values = c(\"#BA182A\", \"#FF8288\", \"#FFDBDD\"),\n    labels = c(\"A\", \"B\", \"C\")\n  ) +\n  coord_equal() +\n  theme_void()",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🍕 Parts of a Whole"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/part-whole.html#parliament-charts",
    "href": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/part-whole.html#parliament-charts",
    "title": "🍕 Parts of a Whole",
    "section": "\n Parliament Charts",
    "text": "Parliament Charts\nThe package ggpol offers an interesting visualization in the shape of a array of “seats” in a parliament. (There is also a package called ggparliament which in my opinion is a bit cumbersome, having a two step procedure to convert data into “parliament form” etc. )\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\ndf &lt;- tibble(group = LETTERS[1:3],\n                 value = c(25, 20, 35))\n\n# Parliament Plot\nggplot(df) +\n  ggpol::geom_parliament(aes(seats = value, \n                             fill = group),\n                         r0 = 2, # inner radius\n                         r1 = 4 # Outer radius\n  ) + \n  scale_fill_manual(name = NULL,\n                    values = c(\"#BA182A\", \"#FF8288\", \"#FFDBDD\"),\n                    labels = c(\"A\", \"B\", \"C\")) +\n  coord_equal() +\n  theme_void()",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🍕 Parts of a Whole"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/part-whole.html#trees-dendrograms-and-circle-packings",
    "href": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/part-whole.html#trees-dendrograms-and-circle-packings",
    "title": "🍕 Parts of a Whole",
    "section": "Trees, Dendrograms, and Circle Packings",
    "text": "Trees, Dendrograms, and Circle Packings\nThere are still more esoteric plots to explore, if you are hell-bent on startling people ! There is an R package called ggraph, that can do these charts, and many more:\n\nggraph is an extension of ggplot2 aimed at supporting relational data structures such as networks, graphs, and trees. While it builds upon the foundation of ggplot2 and its API it comes with its own self-contained set of geoms, facets, etc., as well as adding the concept of layouts to the grammar.\n\nWe will explore these charts when we examine network diagrams. For now, we can quickly see what these diagrams look like. Although the R-code is visible to you, it may not make sense at the moment!\n\n Dendrograms\nFrom the R Graph Gallery Website :\n\nDendrograms can be built from:\n\nHierarchical dataset: think about a CEO managing team leads managing employees and so on.\nClustering result: clustering divides a set of individuals in group according to their similarity. Its result can be visualized as a tree.\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\n# create an edge list data frame giving the hierarchical structure of your individuals\nd1 &lt;- tibble(from = \"origin\", to = paste(\"group\", seq(1,5), sep = \"\"))\nd2 &lt;- tibble(from = rep(d1$to, each=5), to = paste(\"subgroup\", seq(1,25), sep=\"_\"))\nedges &lt;- rbind(d1, d2)\nedges\n\n\n  \n\n\n# Create a graph object \nmygraph1 &lt;- tidygraph::as_tbl_graph( edges )\n \n# Basic tree\np1 &lt;- ggraph(mygraph1, layout = 'dendrogram', circular = TRUE) + \n  geom_edge_diagonal() +\n  geom_node_point() +\n  theme_void()\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\n# create a data frame \ndata &lt;- tibble(\n  level1=\"CEO\",\n  level2=c( rep(\"boss1\",4), rep(\"boss2\",4)),\n  level3=paste0(\"mister_\", letters[1:8])\n)\n \n# transform it to a edge list!\nedges_level1_2 &lt;- data %&gt;% \n  select(level1, level2) %&gt;% unique %&gt;% rename(from=level1, to=level2)\n\nedges_level2_3 &lt;- data %&gt;% \n  select(level2, level3) %&gt;% unique %&gt;% rename(from=level2, to=level3)\n\nedge_list &lt;- rbind(edges_level1_2, edges_level2_3)\nedge_list\n\n\n  \n\n\n# Now we can plot that\nmygraph2 &lt;- as_tbl_graph(edge_list)\np2 &lt;- ggraph(mygraph2, layout = 'dendrogram', circular = FALSE) + \n  geom_edge_diagonal() +\n  geom_node_point() +\n  theme_void()\n\n\np1 + p2 + theme(aspect.ratio = 1)\n\n\n\n\n\n\n\nCircle Packing\n\nlibrary(tidygraph)\nlibrary(ggraph)\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\ngraph &lt;- tbl_graph(flare$vertices, flare$edges)\nset.seed(1)\nggraph(graph, 'circlepack', weight = size) + \n  geom_node_circle(aes(fill = as_factor(depth)), size = 0.25, n = 50) + \n  coord_fixed() +\n  scale_fill_discrete(name = \"Depth\") +\n  theme_void()",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🍕 Parts of a Whole"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/part-whole.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/part-whole.html#your-turn",
    "title": "🍕 Parts of a Whole",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nUse the penguins dataset from the palmerpenguins package and plot pies, fans, and donuts as appropriate.\nLook at the whigs and highschool datasets in the package ggraph. Plot Pies, Fans and if you are feeling confident, Trees, Dendrograms, and Circle Packings as appropriate for these.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🍕 Parts of a Whole"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/part-whole.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/part-whole.html#references",
    "title": "🍕 Parts of a Whole",
    "section": "\n References",
    "text": "References\n\nIaroslava.2020. A Parliament Diagram in R, https://datavizstory.com/a-parliament-diagram-in-r/\n\nVenn Diagrams in R, Venn diagram in ggplot2 | R CHARTS (r-charts.com)\n\nGenerate icon-array charts without code! https://iconarray.com\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\ndata.tree\n1.1.0\nGlur (2023)\n\n\necharts4r\n0.4.5\nCoene (2023)\n\n\nggparliament\n3.1.6\nHickman, Meers, and Leeper (2024)\n\n\nggpol\n0.0.7\nTiedemann (2020)\n\n\nggraph\n2.2.1\nPedersen (2024a)\n\n\nplotrix\n3.8.4\nJ (2006)\n\n\ntidygraph\n1.3.1\nPedersen (2024b)\n\n\nwaffle\n1.0.2\nRudis and Gandy (2023)\n\n\n\n\n\n\nCoene, John. 2023. Echarts4r: Create Interactive Graphs with “Echarts JavaScript” Version 5. https://CRAN.R-project.org/package=echarts4r.\n\n\nGlur, Christoph. 2023. data.tree: General Purpose Hierarchical Data Structure. https://CRAN.R-project.org/package=data.tree.\n\n\nHickman, Robert, Zoe Meers, and Thomas J. Leeper. 2024. ggparliament: Parliament Plots. https://github.com/zmeers/ggparliament.\n\n\nJ, Lemon. 2006. “Plotrix: A Package in the Red Light District of r.” R-News 6 (4): 8–12.\n\n\nPedersen, Thomas Lin. 2024a. ggraph: An Implementation of Grammar of Graphics for Graphs and Networks. https://CRAN.R-project.org/package=ggraph.\n\n\n———. 2024b. tidygraph: A Tidy API for Graph Manipulation. https://CRAN.R-project.org/package=tidygraph.\n\n\nRudis, Bob, and Dave Gandy. 2023. waffle: Create Waffle Chart Visualizations. https://CRAN.R-project.org/package=waffle.\n\n\nTiedemann, Frederik. 2020. ggpol: Visualizing Social Science Data with “ggplot2”. https://CRAN.R-project.org/package=ggpol.",
    "crumbs": [
      "Teaching",
      "Data Analytics",
      "Descriptive Analytics",
      "🍕 Parts of a Whole"
    ]
  },
  {
    "objectID": "content/work-related/fsp-manifesto/index.html",
    "href": "content/work-related/fsp-manifesto/index.html",
    "title": "My Teaching Manifesto",
    "section": "",
    "text": "This is a short Statement of Values, Beliefs, and Content in my Teaching.\n\nArvind Venkatadri.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#instructions",
    "href": "content/work-related/DSU/A1.html#instructions",
    "title": "A1",
    "section": "Instructions",
    "text": "Instructions\n\nEach Question in this Assignment is a chart.\nEach Chart is accompanied by a set of short questions.\nYour responses to these can be R-code, or text.\nPlease number your answers as 1.a, 1.b, 1.c…..2.a, 2.b…on your Answer Sheet.\nAll aboard? Let’s go!"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-1",
    "href": "content/work-related/DSU/A1.html#question-1",
    "title": "A1",
    "section": "Question 1",
    "text": "Question 1\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-1-1",
    "href": "content/work-related/DSU/A1.html#question-1-1",
    "title": "A1",
    "section": "Question 1",
    "text": "Question 1\n\n\n\nWhat is the ggplot geometry used in this graph?\nWhat do the colours mean?"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-2",
    "href": "content/work-related/DSU/A1.html#question-2",
    "title": "A1",
    "section": "Question 2",
    "text": "Question 2\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-3",
    "href": "content/work-related/DSU/A1.html#question-3",
    "title": "A1",
    "section": "Question 3",
    "text": "Question 3\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-4",
    "href": "content/work-related/DSU/A1.html#question-4",
    "title": "A1",
    "section": "Question 4",
    "text": "Question 4\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-5",
    "href": "content/work-related/DSU/A1.html#question-5",
    "title": "A1",
    "section": "Question 5",
    "text": "Question 5\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-6",
    "href": "content/work-related/DSU/A1.html#question-6",
    "title": "A1",
    "section": "Question 6",
    "text": "Question 6\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-7",
    "href": "content/work-related/DSU/A1.html#question-7",
    "title": "A1",
    "section": "Question 7",
    "text": "Question 7\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-8",
    "href": "content/work-related/DSU/A1.html#question-8",
    "title": "A1",
    "section": "Question 8",
    "text": "Question 8\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-9",
    "href": "content/work-related/DSU/A1.html#question-9",
    "title": "A1",
    "section": "Question 9",
    "text": "Question 9\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-10",
    "href": "content/work-related/DSU/A1.html#question-10",
    "title": "A1",
    "section": "Question 10",
    "text": "Question 10\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-11",
    "href": "content/work-related/DSU/A1.html#question-11",
    "title": "A1",
    "section": "Question 11",
    "text": "Question 11\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-12",
    "href": "content/work-related/DSU/A1.html#question-12",
    "title": "A1",
    "section": "Question 12",
    "text": "Question 12\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-13",
    "href": "content/work-related/DSU/A1.html#question-13",
    "title": "A1",
    "section": "Question 13",
    "text": "Question 13\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-14",
    "href": "content/work-related/DSU/A1.html#question-14",
    "title": "A1",
    "section": "Question 14",
    "text": "Question 14\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)"
  },
  {
    "objectID": "content/work-related/DSU/A1.html#question-15",
    "href": "content/work-related/DSU/A1.html#question-15",
    "title": "A1",
    "section": "Question 15",
    "text": "Question 15\n\n\n\nList the variables are used in this graph?\nIdentify their types. (Quantitative, Qualitative)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment#1-IA-EMBA-T4-2023, Arvind Venkatadri"
  },
  {
    "objectID": "content/work-related/fsp-portfolio-2022/index.html",
    "href": "content/work-related/fsp-portfolio-2022/index.html",
    "title": "Teaching in this post(?) - Pandemic Year 2021-2022",
    "section": "",
    "text": "This is a short Portfolio of Teaching Initiatives and Student Outcomes during this post(?)-pandemic year, 2021-2022, from Arvind Venkatadri.\n\n\n\n Back to top"
  },
  {
    "objectID": "readme.html",
    "href": "readme.html",
    "title": "Applied Metaphors: Learning TRIZ, Complexity, Data/Stats/ML using Metaphors",
    "section": "",
    "text": "Preparatory Work to moving my full website to Quarto!"
  },
  {
    "objectID": "readme.html#get-started-with-quarto",
    "href": "readme.html#get-started-with-quarto",
    "title": "Applied Metaphors: Learning TRIZ, Complexity, Data/Stats/ML using Metaphors",
    "section": "",
    "text": "Preparatory Work to moving my full website to Quarto!"
  }
]