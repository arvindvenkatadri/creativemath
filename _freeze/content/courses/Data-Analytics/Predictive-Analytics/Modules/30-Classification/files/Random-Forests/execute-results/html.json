{
  "hash": "d40d1dfcb227a03d264bfcff02b52730",
  "result": {
    "markdown": "---\ntitle: \"Random Forests\"\nauthor: \"Arvind Venkatadri\"\ndate: \"13/06/2020\"\noutput:\n  html_document:\n    df_print: paged\n    toc: yes\n    code_download: TRUE\neditor_options: \n  chunk_output_type: inline\n---\n\n\n\n\n## References:\n1. Machine Learning Basics - Random Forest at [Shirin's Playground](https://shirinsplayground.netlify.app/2018/10/ml_basics_rf/)\n2. \n\n## Penguin Random Forest Model with`randomForest`\n\nUsing the `penguins` dataset and Random Forest Classification.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_…¹ body_…² sex    year\n   <fct>   <fct>              <dbl>         <dbl>      <int>   <int> <fct> <int>\n 1 Adelie  Torgersen           39.1          18.7        181    3750 male   2007\n 2 Adelie  Torgersen           39.5          17.4        186    3800 fema…  2007\n 3 Adelie  Torgersen           40.3          18          195    3250 fema…  2007\n 4 Adelie  Torgersen           NA            NA           NA      NA <NA>   2007\n 5 Adelie  Torgersen           36.7          19.3        193    3450 fema…  2007\n 6 Adelie  Torgersen           39.3          20.6        190    3650 male   2007\n 7 Adelie  Torgersen           38.9          17.8        181    3625 fema…  2007\n 8 Adelie  Torgersen           39.2          19.6        195    4675 male   2007\n 9 Adelie  Torgersen           34.1          18.1        193    3475 <NA>   2007\n10 Adelie  Torgersen           42            20.2        190    4250 <NA>   2007\n# … with 334 more rows, and abbreviated variable names ¹​flipper_length_mm,\n#   ²​body_mass_g\n```\n:::\n\n```{.r .cell-code}\nsummary(penguins)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2                                 \n```\n:::\n\n```{.r .cell-code}\npenguins %>% skimr::skim()\n```\n\n::: {.cell-output-display}\nTable: Data summary\n\n|                         |           |\n|:------------------------|:----------|\n|Name                     |Piped data |\n|Number of rows           |344        |\n|Number of columns        |8          |\n|_______________________  |           |\n|Column type frequency:   |           |\n|factor                   |3          |\n|numeric                  |5          |\n|________________________ |           |\n|Group variables          |None       |\n\n\n**Variable type: factor**\n\n|skim_variable | n_missing| complete_rate|ordered | n_unique|top_counts                  |\n|:-------------|---------:|-------------:|:-------|--------:|:---------------------------|\n|species       |         0|          1.00|FALSE   |        3|Ade: 152, Gen: 124, Chi: 68 |\n|island        |         0|          1.00|FALSE   |        3|Bis: 168, Dre: 124, Tor: 52 |\n|sex           |        11|          0.97|FALSE   |        2|mal: 168, fem: 165          |\n\n\n**Variable type: numeric**\n\n|skim_variable     | n_missing| complete_rate|    mean|     sd|     p0|     p25|     p50|    p75|   p100|hist  |\n|:-----------------|---------:|-------------:|-------:|------:|------:|-------:|-------:|------:|------:|:-----|\n|bill_length_mm    |         2|          0.99|   43.92|   5.46|   32.1|   39.23|   44.45|   48.5|   59.6|▃▇▇▆▁ |\n|bill_depth_mm     |         2|          0.99|   17.15|   1.97|   13.1|   15.60|   17.30|   18.7|   21.5|▅▅▇▇▂ |\n|flipper_length_mm |         2|          0.99|  200.92|  14.06|  172.0|  190.00|  197.00|  213.0|  231.0|▂▇▃▅▂ |\n|body_mass_g       |         2|          0.99| 4201.75| 801.95| 2700.0| 3550.00| 4050.00| 4750.0| 6300.0|▃▇▆▃▂ |\n|year              |         0|          1.00| 2008.03|   0.82| 2007.0| 2007.00| 2008.00| 2009.0| 2009.0|▇▁▇▁▇ |\n:::\n\n```{.r .cell-code}\npenguins <- penguins %>% tidyr::drop_na()\n# Spent one hour trying to find `drop-na()` ( 14 June 2020)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# library(corrplot)\ncor <- penguins %>% select(is.numeric) %>% cor() \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Use of bare predicate functions was deprecated in tidyselect 1.1.0.\nℹ Please use wrap predicates in `where()` instead.\n  # Was:\n  data %>% select(is.numeric)\n\n  # Now:\n  data %>% select(where(is.numeric))\n```\n:::\n\n```{.r .cell-code}\ncor %>% corrplot(., method = \"ellipse\", order = \"hclust\",tl.cex = 0.5)\n```\n\n::: {.cell-output-display}\n![](Random-Forests_files/figure-html/EDA on penguins data-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# try these too:\n# cor %>% corrplot(., method = \"square\", order = \"hclust\",tl.cex = 0.5)\n# cor %>% corrplot(., method = \"color\", order = \"hclust\",tl.cex = 0.5)\n# cor %>% corrplot(., method = \"shade\", order = \"hclust\",tl.cex = 0.5)\n```\n:::\n\n\nNotes:\n- `flipper_length_mm` and `culmen_depth_mm` are negtively correlated at approx (-0.7)\n- `flipper_length_mm` and `body_mass_g` are positively correlated at approx 0.8\n\nSo we will use steps in the recipe to remove correlated variables. \n\n\n### Penguin Data Sampling and Recipe\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data Split\npenguin_split <- initial_split(penguins, prop = 0.6)\npenguin_train <- training(penguin_split)\npenguin_test <- testing(penguin_split)\npenguin_split\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Training/Testing/Total>\n<199/134/333>\n```\n:::\n\n```{.r .cell-code}\nhead(penguin_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 8\n  species   island bill_length_mm bill_depth_mm flipper_le…¹ body_…² sex    year\n  <fct>     <fct>           <dbl>         <dbl>        <int>   <int> <fct> <int>\n1 Adelie    Biscoe           37.9          18.6          172    3150 fema…  2007\n2 Gentoo    Biscoe           45.3          13.7          210    4300 fema…  2008\n3 Gentoo    Biscoe           45.2          14.8          212    5200 fema…  2009\n4 Chinstrap Dream            50.5          18.4          200    3400 fema…  2008\n5 Adelie    Dream            39.5          16.7          178    3250 fema…  2007\n6 Adelie    Dream            39.6          18.1          186    4450 male   2008\n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n```\n:::\n\n```{.r .cell-code}\n# Recipe\npenguin_recipe <- penguins %>% \n  recipe(species ~ .) %>% \n  step_normalize(all_numeric()) %>% # Scaling and Centering\n  step_corr(all_numeric()) %>%  # Handling correlated variables\n  prep()\n\n# Baking the data\npenguin_train_baked <-  penguin_train %>% \n  bake(object = penguin_recipe, new_data = .)\n\npenguin_test_baked <-  penguin_test %>% \n  bake(object = penguin_recipe, new_data = .)\n\nhead(penguin_train_baked)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 8\n  island bill_length_mm bill_depth_mm flipper_le…¹ body_…² sex      year species\n  <fct>           <dbl>         <dbl>        <dbl>   <dbl> <fct>   <dbl> <fct>  \n1 Biscoe         -1.11          0.729      -2.07    -1.31  fema… -1.28   Adelie \n2 Biscoe          0.239        -1.76        0.644    0.115 fema… -0.0517 Gentoo \n3 Biscoe          0.221        -1.20        0.787    1.23  fema…  1.18   Gentoo \n4 Dream           1.19          0.627      -0.0690  -1.00  fema… -0.0517 Chinst…\n5 Dream          -0.822        -0.236      -1.64    -1.19  fema… -1.28   Adelie \n6 Dream          -0.803         0.475      -1.07     0.302 male  -0.0517 Adelie \n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n```\n:::\n:::\n\n\n### Penguin Random Forest Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguin_model <- \n  rand_forest(trees = 100) %>% \n  set_engine(\"randomForest\") %>% \n  set_mode(\"classification\")\npenguin_model\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  trees = 100\n\nComputational engine: randomForest \n```\n:::\n\n```{.r .cell-code}\npenguin_fit <- \n  penguin_model %>% \n  fit(species ~ .,penguin_train_baked)\npenguin_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nparsnip model object\n\n\nCall:\n randomForest(x = maybe_data_frame(x), y = y, ntree = ~100) \n               Type of random forest: classification\n                     Number of trees: 100\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 0.5%\nConfusion matrix:\n          Adelie Chinstrap Gentoo class.error\nAdelie        88         0      0  0.00000000\nChinstrap      1        41      0  0.02380952\nGentoo         0         0     69  0.00000000\n```\n:::\n\n```{.r .cell-code}\n# iris_ranger <- \n#   rand_forest(trees = 100) %>% \n#   set_mode(\"classification\") %>% \n#   set_engine(\"ranger\") %>% \n#   fit(Species ~ ., data = iris_training_baked)\n```\n:::\n\n### Metrics for the Penguin Random Forest Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predictions\npredict(object = penguin_fit, new_data = penguin_test_baked) %>%  \n  dplyr::bind_cols(penguin_test_baked) %>% \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 134\nColumns: 9\n$ .pred_class       <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen, Biscoe, …\n$ bill_length_mm    <dbl> -0.6752636, -0.8581235, -0.8764095, 0.3670377, -0.94…\n$ bill_depth_mm     <dbl> 0.42409105, 1.74440040, 1.23658911, 2.20143056, 0.01…\n$ flipper_length_mm <dbl> -0.42573251, -0.78247365, -0.42573251, -0.49708074, …\n$ body_mass_g       <dbl> -1.188572125, -0.691810886, 0.581139791, -0.00876418…\n$ sex               <fct> female, male, male, male, male, female, male, male, …\n$ year              <dbl> -1.2818130, -1.2818130, -1.2818130, -1.2818130, -1.2…\n$ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n```\n:::\n\n```{.r .cell-code}\n# Prediction Accuracy Metrics\npredict(object = penguin_fit, new_data = penguin_test_baked) %>%  \n  dplyr::bind_cols(penguin_test_baked) %>% \n  yardstick::metrics(truth = species, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy multiclass     0.963\n2 kap      multiclass     0.940\n```\n:::\n\n```{.r .cell-code}\n# Prediction Probabilities\npenguin_fit_probs <- \n  predict(penguin_fit, penguin_test_baked, type = \"prob\") %>%\n  dplyr::bind_cols(penguin_test_baked)\nglimpse(penguin_fit_probs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 134\nColumns: 11\n$ .pred_Adelie      <dbl> 0.99, 1.00, 0.98, 0.57, 0.97, 1.00, 1.00, 0.94, 0.97…\n$ .pred_Chinstrap   <dbl> 0.01, 0.00, 0.00, 0.40, 0.03, 0.00, 0.00, 0.06, 0.03…\n$ .pred_Gentoo      <dbl> 0.00, 0.00, 0.02, 0.03, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen, Biscoe, …\n$ bill_length_mm    <dbl> -0.6752636, -0.8581235, -0.8764095, 0.3670377, -0.94…\n$ bill_depth_mm     <dbl> 0.42409105, 1.74440040, 1.23658911, 2.20143056, 0.01…\n$ flipper_length_mm <dbl> -0.42573251, -0.78247365, -0.42573251, -0.49708074, …\n$ body_mass_g       <dbl> -1.188572125, -0.691810886, 0.581139791, -0.00876418…\n$ sex               <fct> female, male, male, male, male, female, male, male, …\n$ year              <dbl> -1.2818130, -1.2818130, -1.2818130, -1.2818130, -1.2…\n$ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n```\n:::\n\n```{.r .cell-code}\n# Confusion Matrix\npenguin_fit$fit$confusion %>% tidy()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: 'tidy.numeric' is deprecated.\nSee help(\"Deprecated\")\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 1\n  x[,\"Adelie\"] [,\"Chinstrap\"] [,\"Gentoo\"] [,\"class.error\"]\n         <dbl>          <dbl>       <dbl>            <dbl>\n1           88              0           0           0     \n2            1             41           0           0.0238\n3            0              0          69           0     \n```\n:::\n\n```{.r .cell-code}\n# Gain Curves\npenguin_fit_probs %>% \n  yardstick::gain_curve(species, .pred_Adelie:.pred_Gentoo) %>%\n  autoplot()\n```\n\n::: {.cell-output-display}\n![](Random-Forests_files/figure-html/Penguin Model Predictions and Metrics-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# ROC Plot\npenguin_fit_probs%>%\n  roc_curve(species, .pred_Adelie:.pred_Gentoo) %>%\n  autoplot()\n```\n\n::: {.cell-output-display}\n![](Random-Forests_files/figure-html/Penguin Model Predictions and Metrics-2.png){width=672}\n:::\n:::\n\n### Using `broom` on the penguin model\n\n::: {.cell}\n\n```{.r .cell-code}\npenguin_split\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Training/Testing/Total>\n<199/134/333>\n```\n:::\n\n```{.r .cell-code}\npenguin_split %>% broom::tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 333 × 2\n     Row Data    \n   <int> <chr>   \n 1     1 Analysis\n 2     2 Analysis\n 3     4 Analysis\n 4     6 Analysis\n 5     8 Analysis\n 6     9 Analysis\n 7    10 Analysis\n 8    11 Analysis\n 9    12 Analysis\n10    13 Analysis\n# … with 323 more rows\n```\n:::\n\n```{.r .cell-code}\npenguin_recipe %>% broom::tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 6\n  number operation type      trained skip  id             \n   <int> <chr>     <chr>     <lgl>   <lgl> <chr>          \n1      1 step      normalize TRUE    FALSE normalize_QzNS6\n2      2 step      corr      TRUE    FALSE corr_6yjth     \n```\n:::\n\n```{.r .cell-code}\n# Following do not work for `random forest models` !! ;-()\n#penguin_model %>% tidy()\n#penguin_fit %>% tidy() \npenguin_model %>% str()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 7\n $ args                 :List of 3\n  ..$ mtry : language ~NULL\n  .. ..- attr(*, \".Environment\")=<environment: R_EmptyEnv> \n  ..$ trees: language ~100\n  .. ..- attr(*, \".Environment\")=<environment: R_EmptyEnv> \n  ..$ min_n: language ~NULL\n  .. ..- attr(*, \".Environment\")=<environment: R_EmptyEnv> \n $ eng_args             : Named list()\n  ..- attr(*, \"class\")= chr [1:2] \"quosures\" \"list\"\n $ mode                 : chr \"classification\"\n $ user_specified_mode  : logi TRUE\n $ method               : NULL\n $ engine               : chr \"randomForest\"\n $ user_specified_engine: logi TRUE\n - attr(*, \"class\")= chr [1:2] \"rand_forest\" \"model_spec\"\n```\n:::\n\n```{.r .cell-code}\npenguin_test_baked\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 134 × 8\n   island    bill_length_mm bill_depth_mm flipper…¹ body_m…² sex    year species\n   <fct>              <dbl>         <dbl>     <dbl>    <dbl> <fct> <dbl> <fct>  \n 1 Torgersen         -0.675        0.424     -0.426 -1.19    fema… -1.28 Adelie \n 2 Torgersen         -0.858        1.74      -0.782 -0.692   male  -1.28 Adelie \n 3 Torgersen         -0.876        1.24      -0.426  0.581   male  -1.28 Adelie \n 4 Torgersen          0.367        2.20      -0.497 -0.00876 male  -1.28 Adelie \n 5 Biscoe            -0.950        0.0178    -1.50  -0.506   male  -1.28 Adelie \n 6 Biscoe            -0.639        0.373     -0.997 -1.25    fema… -1.28 Adelie \n 7 Biscoe            -0.639        0.881     -1.50  -0.319   male  -1.28 Adelie \n 8 Dream             -1.24         0.475     -1.64  -0.381   male  -1.28 Adelie \n 9 Dream             -0.566        0.881     -1.21  -0.381   male  -1.28 Adelie \n10 Dream             -1.39        -0.0837    -0.426 -1.10    fema… -1.28 Adelie \n# … with 124 more rows, and abbreviated variable names ¹​flipper_length_mm,\n#   ²​body_mass_g\n```\n:::\n:::\n\n\n\n\n\n\n## Iris Random Forest Model with `ranger`\n\nUsing the `iris` dataset and Random Forest Classification.\nThis part uses `rsample` to split the data and the `recipes` to *prep* the data for model making. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n#set.seed(100)\niris_split <- rsample::initial_split(iris, prop = 0.6)\niris_split\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Training/Testing/Total>\n<90/60/150>\n```\n:::\n\n```{.r .cell-code}\niris_split %>% training() %>% glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 90\nColumns: 5\n$ Sepal.Length <dbl> 6.9, 7.7, 4.9, 5.2, 5.5, 6.6, 5.1, 6.2, 6.0, 6.0, 6.7, 6.…\n$ Sepal.Width  <dbl> 3.1, 3.0, 3.0, 3.4, 2.6, 3.0, 3.8, 2.9, 3.0, 2.7, 2.5, 2.…\n$ Petal.Length <dbl> 4.9, 6.1, 1.4, 1.4, 4.4, 4.4, 1.9, 4.3, 4.8, 5.1, 5.8, 5.…\n$ Petal.Width  <dbl> 1.5, 2.3, 0.2, 0.2, 1.2, 1.4, 0.4, 1.3, 1.8, 1.6, 1.8, 1.…\n$ Species      <fct> versicolor, virginica, setosa, setosa, versicolor, versic…\n```\n:::\n\n```{.r .cell-code}\niris_split %>% testing() %>% glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 60\nColumns: 5\n$ Sepal.Length <dbl> 4.7, 5.4, 4.8, 4.3, 5.7, 5.1, 5.4, 5.1, 4.8, 5.0, 4.7, 5.…\n$ Sepal.Width  <dbl> 3.2, 3.9, 3.0, 3.0, 4.4, 3.8, 3.4, 3.7, 3.4, 3.4, 3.2, 3.…\n$ Petal.Length <dbl> 1.3, 1.7, 1.4, 1.1, 1.5, 1.5, 1.7, 1.5, 1.9, 1.6, 1.6, 1.…\n$ Petal.Width  <dbl> 0.2, 0.4, 0.1, 0.1, 0.4, 0.3, 0.2, 0.4, 0.2, 0.4, 0.2, 0.…\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n```\n:::\n:::\n\n\n\n### Iris Data Pre-Processing: Creating the Recipe\n\nThe `recipes` package provides an interface that specializes in *data pre-processing*. Within the package, the functions that start, or execute, the data transformations are named after **cooking actions**. That makes the interface more user-friendly. For example:\n\n - `recipe()` - Starts a new set of transformations to be applied, similar to the `ggplot()` command. Its main argument is the model’s `formula`.\n\n - `prep()` - Executes the transformations on top of the data that is supplied (**typically, the training data**). Each data transformation is a `step()` function. ( Recall what we did with the `caret` package: *Centering, Scaling, Removing Correlated variables*...)\n\nNote that in order to avoid data leakage (e.g: transferring information from the train set into the test set), data should be “prepped” using the **train_tbl** only. <https://towardsdatascience.com/modelling-with-tidymodels-and-parsnip-bae2c01c131c>\nCRAN: The idea is that the preprocessing operations will all be **created** using the *training set* and then these steps will be **applied** to both the training and test set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Pre Processing the Training Data\n\niris_recipe <- \n  training(iris_split) %>% # Note: Using TRAINING data !!\n  recipe(Species ~ .)      # Note: Outcomes ~ Predictors !!\n\n# The data contained in the `data` argument need not be the training set; this data is only used to catalog the names of the variables and their types (e.g. numeric, etc.).\n```\n:::\n\n\nQ: How does the recipe \"figure\" out which are the outcomes and which are the predictors?\nA.The `recipe` command defines `Outcomes` and `Predictors` using the formula interface. ~~Not clear how this recipe \"figures\" out which are the outcomes and which are the predictors, when we have not yet specified them...~~\n\nQ. Why is the recipe not agnostic to data set? Is that a meaningful question?\nA. The use of the `training set` in the recipe command is just to declare the variables and specify the `roles` of the data, nothing else. `Roles` are open-ended and extensible. \nFrom <https://cran.r-project.org/web/packages/recipes/vignettes/Simple_Example.html> :\n\n> This document demonstrates some basic uses of recipes. First, some definitions are required:\n- **variables** are the original (raw) data columns in a data frame or tibble. For example, in a traditional formula Y ~ A + B + A:B, the variables are A, B, and Y.\n- **roles** define how variables will be used in the model. Examples are: `predictor` (independent variables), `response`, and `case weight`. This is meant to be open-ended and extensible.\n- **terms** are columns in a **design matrix** such as A, B, and A:B. These can be other derived entities that are grouped, such as a set of `principal components` or a set of columns, that define a `basis function` for a variable. These are synonymous with `features` in machine learning. Variables that have `predictor` roles would automatically be main `effect terms`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Apply the transformation steps\niris_recipe <- iris_recipe %>% \n  step_corr(all_predictors()) %>% \n  step_center(all_predictors(), -all_outcomes()) %>% \n  step_scale(all_predictors(), -all_outcomes()) %>% \n  prep()\n```\n:::\n\n\nThis has created the `recipe()` and prepped it too. \nWe now need to apply it to our datasets:\n\n- Take `training` data and `bake()` it to prepare it for modelling. \n- Do the same for the `testing` set. \n\n\n::: {.cell}\n\n```{.r .cell-code}\niris_training_baked <- \n  iris_split %>% \n  training() %>% \n  bake(iris_recipe,.)\niris_training_baked\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 90 × 4\n   Sepal.Length Sepal.Width Petal.Width Species   \n          <dbl>       <dbl>       <dbl> <fct>     \n 1        1.29       0.191       0.442  versicolor\n 2        2.30      -0.0478      1.54   virginica \n 3       -1.23      -0.0478     -1.35   setosa    \n 4       -0.852      0.909      -1.35   setosa    \n 5       -0.474     -1.00        0.0291 versicolor\n 6        0.914     -0.0478      0.304  versicolor\n 7       -0.979      1.87       -1.07   setosa    \n 8        0.409     -0.287       0.167  versicolor\n 9        0.157     -0.0478      0.855  virginica \n10        0.157     -0.765       0.580  versicolor\n# … with 80 more rows\n```\n:::\n\n```{.r .cell-code}\niris_testing_baked <- \n  iris_split %>% \n  testing() %>% \n  bake(iris_recipe,.)\niris_testing_baked \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 60 × 4\n   Sepal.Length Sepal.Width Petal.Width Species\n          <dbl>       <dbl>       <dbl> <fct>  \n 1       -1.48       0.430        -1.35 setosa \n 2       -0.600      2.10         -1.07 setosa \n 3       -1.36      -0.0478       -1.48 setosa \n 4       -1.99      -0.0478       -1.48 setosa \n 5       -0.222      3.30         -1.07 setosa \n 6       -0.979      1.87         -1.21 setosa \n 7       -0.600      0.909        -1.35 setosa \n 8       -0.979      1.63         -1.07 setosa \n 9       -1.36       0.909        -1.35 setosa \n10       -1.10       0.909        -1.07 setosa \n# … with 50 more rows\n```\n:::\n:::\n\n\n### Iris Model Training using `parsnip`\n\nDifferent ML packages provide different interfaces (APIs ) to do the same thing  (e.g random forests). The `tidymodels` package provides a consistent interface to invoke a wide variety of packages supporting a wide variety of models. \n\nThe `parsnip` package is a successor to `caret`. \n\nTo model with `parsnip`:\n1. Pick a `model` : \n2. Set the `engine`\n3. Set the `mode` (if needed): *Classification* or *Regression*\n\nCheck [here](https://tidymodels.github.io/parsnip/articles/articles/Models.html) for models available in `parsnip`. \n\n - Mode: *classification* and *regression* in `parsnip`, each using a variety of models. ( **Which Way**). This defines the form of the output. \n \n - Engine: The `engine` is the **R package** that is invoked by `parsnip` to execute the model. E.g `glm`, `glmnet`,`keras`.( **How** ) `parsnip` provides **wrappers** for models from these packages. \n \n - Model: is the **specific technique** used for the modelling task. E.g `linear_reg()`, `logistic_reg()`, `mars`, `decision_tree`, `nearest_neighbour`...(What model). \n \n and models have:\n - `hyperparameters`: that are numerical or factor variables that `tune` the model ( Like the alpha beta parameters for Bayesian priors)\n\nWe can use the `random forest` model to **classify** the iris into species. Here Species is the `Outcome` variable and the rest are `predictor` variables. The `random forest` model is provided by the `ranger` package, to which `tidymodels/parsnip` provides a simple and consistent interface.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ranger)\niris_ranger <- \n  rand_forest(trees = 100) %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"ranger\") %>% \n  fit(Species ~ ., data = iris_training_baked)\n```\n:::\n\n\n`ranger` can generate random forest models for `classification`, `regression`, `survival`( time series, time to event stuff). `Extreme Forests` are also supported, wherein all points in the dataset are used ( instead of bootstrap samples) along with `feature bagging`. \nWe can also run the same model using the `randomForest` package:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(randomForest,quietly = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nrandomForest 4.7-1.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nType rfNews() to see new features/changes/bug fixes.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'randomForest'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:ranger':\n\n    importance\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:ggplot2':\n\n    margin\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:dplyr':\n\n    combine\n```\n:::\n\n```{.r .cell-code}\niris_rf <- \n  rand_forest(trees = 100) %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"randomForest\") %>% \n  fit(Species ~ ., data = iris_training_baked)\n```\n:::\n\n### Iris Predictions\n\nThe `predict()` function run against a `parsnip` model returns a prediction `tibble`. By default, the prediction variable is called `.pred_class`. \n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(object = iris_ranger, new_data = iris_testing_baked) %>%  \n  dplyr::bind_cols(iris_testing_baked) %>% \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 60\nColumns: 5\n$ .pred_class  <fct> setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n$ Sepal.Length <dbl> -1.4832286, -0.6000206, -1.3570560, -1.9879188, -0.221502…\n$ Sepal.Width  <dbl> 0.43043433, 2.10434559, -0.04782604, -0.04782604, 3.29999…\n$ Petal.Width  <dbl> -1.3471818, -1.0719346, -1.4848053, -1.4848053, -1.071934…\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n```\n:::\n:::\n\n\n### Iris Classification Model Validation\n\nWe use `metrics()` function from the `yardstick` package to evaluate how good the model is. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(iris_ranger, iris_testing_baked) %>%\n  dplyr::bind_cols(iris_testing_baked) %>% \n  yardstick::metrics(truth = Species, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy multiclass     0.917\n2 kap      multiclass     0.873\n```\n:::\n:::\n\n\nWe can also check the metrics for `randomForest` model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(iris_rf, iris_testing_baked) %>%\n  dplyr::bind_cols(iris_testing_baked) %>% \n  yardstick::metrics(truth = Species, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy multiclass     0.917\n2 kap      multiclass     0.873\n```\n:::\n:::\n\n\n### Iris Per-Classifier Metrics\n\nWe can use the parameter `type = \"prob\"` in the `predict()` function to obtain a probability score on each prediction. \n**TBD: How is this prob calculated?** Possible answer: the Random Forest model outputs its answer by majority voting across n trees. Each of the possible answers( i.e. predictions) for a particular test datum gets a share of the vote, that represents its probability. Hence each dataum in the test vector can show a probability for the \"winning\" answer. ( Quite possibly we can get the probabilities for *all possible outcomes* for **each test datum**)\n\n\n::: {.cell}\n\n```{.r .cell-code}\niris_ranger_probs <- \n  predict(iris_ranger, iris_testing_baked, type = \"prob\") %>%\n  dplyr::bind_cols(iris_testing_baked)\nglimpse(iris_ranger_probs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 60\nColumns: 7\n$ .pred_setosa     <dbl> 0.944043651, 0.979428571, 0.935642857, 0.945642857, 0…\n$ .pred_versicolor <dbl> 0.02800000, 0.02057143, 0.06042857, 0.05042857, 0.048…\n$ .pred_virginica  <dbl> 0.027956349, 0.000000000, 0.003928571, 0.003928571, 0…\n$ Sepal.Length     <dbl> -1.4832286, -0.6000206, -1.3570560, -1.9879188, -0.22…\n$ Sepal.Width      <dbl> 0.43043433, 2.10434559, -0.04782604, -0.04782604, 3.2…\n$ Petal.Width      <dbl> -1.3471818, -1.0719346, -1.4848053, -1.4848053, -1.07…\n$ Species          <fct> setosa, setosa, setosa, setosa, setosa, setosa, setos…\n```\n:::\n\n```{.r .cell-code}\niris_rf_probs <- \n  predict(iris_rf, iris_testing_baked, type = \"prob\") %>%\n  dplyr::bind_cols(iris_testing_baked)\nglimpse(iris_rf_probs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 60\nColumns: 7\n$ .pred_setosa     <dbl> 0.99, 0.99, 0.97, 0.99, 0.98, 1.00, 0.96, 1.00, 1.00,…\n$ .pred_versicolor <dbl> 0.00, 0.01, 0.03, 0.01, 0.02, 0.00, 0.04, 0.00, 0.00,…\n$ .pred_virginica  <dbl> 0.01, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,…\n$ Sepal.Length     <dbl> -1.4832286, -0.6000206, -1.3570560, -1.9879188, -0.22…\n$ Sepal.Width      <dbl> 0.43043433, 2.10434559, -0.04782604, -0.04782604, 3.2…\n$ Petal.Width      <dbl> -1.3471818, -1.0719346, -1.4848053, -1.4848053, -1.07…\n$ Species          <fct> setosa, setosa, setosa, setosa, setosa, setosa, setos…\n```\n:::\n\n```{.r .cell-code}\n# Tabulating the probabilities\nftable(iris_rf_probs$.pred_versicolor)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  0 0.01 0.02 0.03 0.04 0.06 0.07 0.09 0.1 0.11 0.12 0.14 0.17 0.18 0.28 0.31 0.42 0.49 0.56 0.75 0.76 0.81 0.84 0.86 0.93 0.95 0.97 0.98\n                                                                                                                                         \n 12    4    4    5    3    3    2    1   1    2    1    1    1    1    1    1    1    1    1    1    1    1    1    2    1    2    2    3\n```\n:::\n\n```{.r .cell-code}\nftable(iris_rf_probs$.pred_virginica)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  0 0.01 0.02 0.05 0.06 0.14 0.16 0.18 0.24 0.43 0.51 0.58 0.69 0.72 0.81 0.82 0.83 0.85 0.86 0.87 0.88 0.9 0.93 0.94 0.96 0.97 0.98\n                                                                                                                                    \n 20    8    1    1    1    2    1    1    1    1    1    1    1    1    1    1    1    1    2    1    3   1    1    2    1    1    3\n```\n:::\n\n```{.r .cell-code}\nftable(iris_rf_probs$.pred_setosa)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  0 0.01 0.03 0.04 0.07 0.08 0.1 0.11 0.12 0.94 0.96 0.97 0.98 0.99  1\n                                                                      \n 20    8    3    1    1    1   1    1    2    1    2    2    1    9  7\n```\n:::\n:::\n\n```\n\n### Iris Classifier: Gain and ROC Curves\n\nWe can plot gain and ROC curves for each of these models\n\n::: {.cell}\n\n```{.r .cell-code}\niris_ranger_probs %>% \n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %>% \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 125\nColumns: 5\n$ .level          <chr> \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"set…\n$ .n              <dbl> 0, 3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 15, 19, 21, 22, 24…\n$ .n_events       <dbl> 0, 3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 15, 19, 21, 22, 22…\n$ .percent_tested <dbl> 0.000000, 5.000000, 6.666667, 8.333333, 10.000000, 11.…\n$ .percent_found  <dbl> 0.00000, 13.63636, 18.18182, 22.72727, 27.27273, 31.81…\n```\n:::\n\n```{.r .cell-code}\niris_ranger_probs %>% \n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %>% \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](Random-Forests_files/figure-html/Gain and ROC Curves `ranger`-1.png){width=672}\n:::\n\n```{.r .cell-code}\niris_ranger_probs %>% \n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %>% \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 128\nColumns: 4\n$ .level      <chr> \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\"…\n$ .threshold  <dbl> -Inf, 0.000000000, 0.002222222, 0.003333333, 0.004444444, …\n$ specificity <dbl> 0.0000000, 0.0000000, 0.2894737, 0.3684211, 0.3947368, 0.5…\n$ sensitivity <dbl> 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0…\n```\n:::\n\n```{.r .cell-code}\niris_ranger_probs %>% \n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %>% \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](Random-Forests_files/figure-html/Gain and ROC Curves `ranger`-2.png){width=672}\n:::\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\niris_rf_probs %>% \n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %>% \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 73\nColumns: 5\n$ .level          <chr> \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"set…\n$ .n              <dbl> 0, 7, 16, 17, 19, 21, 22, 24, 25, 26, 27, 28, 29, 32, …\n$ .n_events       <dbl> 0, 7, 16, 17, 19, 21, 22, 22, 22, 22, 22, 22, 22, 22, …\n$ .percent_tested <dbl> 0.000000, 11.666667, 26.666667, 28.333333, 31.666667, …\n$ .percent_found  <dbl> 0.00000, 31.81818, 72.72727, 77.27273, 86.36364, 95.45…\n```\n:::\n\n```{.r .cell-code}\niris_rf_probs %>% \n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %>% \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](Random-Forests_files/figure-html/Gain and ROC Curves `randomForest`-1.png){width=672}\n:::\n\n```{.r .cell-code}\niris_rf_probs %>% \n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %>% \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 76\nColumns: 4\n$ .level      <chr> \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\"…\n$ .threshold  <dbl> -Inf, 0.00, 0.01, 0.03, 0.04, 0.07, 0.08, 0.10, 0.11, 0.12…\n$ specificity <dbl> 0.0000000, 0.0000000, 0.5263158, 0.7368421, 0.8157895, 0.8…\n$ sensitivity <dbl> 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0…\n```\n:::\n\n```{.r .cell-code}\niris_rf_probs %>% \n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %>% \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](Random-Forests_files/figure-html/Gain and ROC Curves `randomForest`-2.png){width=672}\n:::\n:::\n\n\n\n### Iris Classifier: Metrics\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(iris_ranger, iris_testing_baked, type = \"prob\") %>% \n  bind_cols(predict(iris_ranger,iris_testing_baked)) %>% \n  bind_cols(select(iris_testing_baked,Species)) %>% \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 60\nColumns: 5\n$ .pred_setosa     <dbl> 0.944043651, 0.979428571, 0.935642857, 0.945642857, 0…\n$ .pred_versicolor <dbl> 0.02800000, 0.02057143, 0.06042857, 0.05042857, 0.048…\n$ .pred_virginica  <dbl> 0.027956349, 0.000000000, 0.003928571, 0.003928571, 0…\n$ .pred_class      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setos…\n$ Species          <fct> setosa, setosa, setosa, setosa, setosa, setosa, setos…\n```\n:::\n\n```{.r .cell-code}\npredict(iris_ranger, iris_testing_baked, type = \"prob\") %>% \n  bind_cols(predict(iris_ranger,iris_testing_baked)) %>% \n  bind_cols(select(iris_testing_baked,Species)) %>% \n  yardstick::metrics(data = ., truth = Species, estimate = .pred_class, ... = .pred_setosa:.pred_virginica)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 3\n  .metric     .estimator .estimate\n  <chr>       <chr>          <dbl>\n1 accuracy    multiclass     0.917\n2 kap         multiclass     0.873\n3 mn_log_loss multiclass     0.228\n4 roc_auc     hand_till      0.981\n```\n:::\n\n```{.r .cell-code}\n# And for the `randomForest`method\n\npredict(iris_rf, iris_testing_baked, type = \"prob\") %>% \n  bind_cols(predict(iris_ranger,iris_testing_baked)) %>% \n  bind_cols(select(iris_testing_baked,Species)) %>% \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 60\nColumns: 5\n$ .pred_setosa     <dbl> 0.99, 0.99, 0.97, 0.99, 0.98, 1.00, 0.96, 1.00, 1.00,…\n$ .pred_versicolor <dbl> 0.00, 0.01, 0.03, 0.01, 0.02, 0.00, 0.04, 0.00, 0.00,…\n$ .pred_virginica  <dbl> 0.01, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,…\n$ .pred_class      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setos…\n$ Species          <fct> setosa, setosa, setosa, setosa, setosa, setosa, setos…\n```\n:::\n\n```{.r .cell-code}\npredict(iris_rf, iris_testing_baked, type = \"prob\") %>% \n  bind_cols(predict(iris_ranger,iris_testing_baked)) %>% \n  bind_cols(select(iris_testing_baked,Species)) %>% \n  yardstick::metrics(data = ., truth = Species, estimate = .pred_class, ... = .pred_setosa:.pred_virginica)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 3\n  .metric     .estimator .estimate\n  <chr>       <chr>          <dbl>\n1 accuracy    multiclass     0.917\n2 kap         multiclass     0.873\n3 mn_log_loss multiclass     0.202\n4 roc_auc     hand_till      0.981\n```\n:::\n:::\n",
    "supporting": [
      "Random-Forests_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}