{
  "hash": "d40d1dfcb227a03d264bfcff02b52730",
  "result": {
    "markdown": "---\ntitle: \"Random Forests\"\nauthor: \"Arvind Venkatadri\"\ndate: \"13/06/2020\"\noutput:\n  html_document:\n    df_print: paged\n    toc: yes\n    code_download: TRUE\neditor_options: \n  chunk_output_type: inline\n---\n\n\n\n\n## References:\n1. Machine Learning Basics - Random Forest at [Shirin's Playground](https://shirinsplayground.netlify.app/2018/10/ml_basics_rf/)\n2. \n\n## Penguin Random Forest Model with`randomForest`\n\nUsing the `penguins` dataset and Random Forest Classification.\n\n\n::: {.cell hash='Random-Forests_cache/html/penguins_166a3fde59863545cfbeaa1fdf1af161'}\n\n```{.r .cell-code}\npenguins\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_…¹ body_…² sex    year\n   <fct>   <fct>              <dbl>         <dbl>      <int>   <int> <fct> <int>\n 1 Adelie  Torgersen           39.1          18.7        181    3750 male   2007\n 2 Adelie  Torgersen           39.5          17.4        186    3800 fema…  2007\n 3 Adelie  Torgersen           40.3          18          195    3250 fema…  2007\n 4 Adelie  Torgersen           NA            NA           NA      NA <NA>   2007\n 5 Adelie  Torgersen           36.7          19.3        193    3450 fema…  2007\n 6 Adelie  Torgersen           39.3          20.6        190    3650 male   2007\n 7 Adelie  Torgersen           38.9          17.8        181    3625 fema…  2007\n 8 Adelie  Torgersen           39.2          19.6        195    4675 male   2007\n 9 Adelie  Torgersen           34.1          18.1        193    3475 <NA>   2007\n10 Adelie  Torgersen           42            20.2        190    4250 <NA>   2007\n# … with 334 more rows, and abbreviated variable names ¹​flipper_length_mm,\n#   ²​body_mass_g\n```\n:::\n\n```{.r .cell-code}\nsummary(penguins)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2                                 \n```\n:::\n\n```{.r .cell-code}\npenguins %>% skimr::skim()\n```\n\n::: {.cell-output-display}\nTable: Data summary\n\n|                         |           |\n|:------------------------|:----------|\n|Name                     |Piped data |\n|Number of rows           |344        |\n|Number of columns        |8          |\n|_______________________  |           |\n|Column type frequency:   |           |\n|factor                   |3          |\n|numeric                  |5          |\n|________________________ |           |\n|Group variables          |None       |\n\n\n**Variable type: factor**\n\n|skim_variable | n_missing| complete_rate|ordered | n_unique|top_counts                  |\n|:-------------|---------:|-------------:|:-------|--------:|:---------------------------|\n|species       |         0|          1.00|FALSE   |        3|Ade: 152, Gen: 124, Chi: 68 |\n|island        |         0|          1.00|FALSE   |        3|Bis: 168, Dre: 124, Tor: 52 |\n|sex           |        11|          0.97|FALSE   |        2|mal: 168, fem: 165          |\n\n\n**Variable type: numeric**\n\n|skim_variable     | n_missing| complete_rate|    mean|     sd|     p0|     p25|     p50|    p75|   p100|hist  |\n|:-----------------|---------:|-------------:|-------:|------:|------:|-------:|-------:|------:|------:|:-----|\n|bill_length_mm    |         2|          0.99|   43.92|   5.46|   32.1|   39.23|   44.45|   48.5|   59.6|▃▇▇▆▁ |\n|bill_depth_mm     |         2|          0.99|   17.15|   1.97|   13.1|   15.60|   17.30|   18.7|   21.5|▅▅▇▇▂ |\n|flipper_length_mm |         2|          0.99|  200.92|  14.06|  172.0|  190.00|  197.00|  213.0|  231.0|▂▇▃▅▂ |\n|body_mass_g       |         2|          0.99| 4201.75| 801.95| 2700.0| 3550.00| 4050.00| 4750.0| 6300.0|▃▇▆▃▂ |\n|year              |         0|          1.00| 2008.03|   0.82| 2007.0| 2007.00| 2008.00| 2009.0| 2009.0|▇▁▇▁▇ |\n:::\n\n```{.r .cell-code}\npenguins <- penguins %>% tidyr::drop_na()\n# Spent one hour trying to find `drop-na()` ( 14 June 2020)\n```\n:::\n\n::: {.cell hash='Random-Forests_cache/html/EDA on penguins data_46662858013011bb5eaf1a7bfdcfaf30'}\n\n```{.r .cell-code}\n# library(corrplot)\ncor <- penguins %>% select(is.numeric) %>% cor() \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Use of bare predicate functions was deprecated in tidyselect 1.1.0.\nℹ Please use wrap predicates in `where()` instead.\n  # Was:\n  data %>% select(is.numeric)\n\n  # Now:\n  data %>% select(where(is.numeric))\n```\n:::\n\n```{.r .cell-code}\ncor %>% corrplot(., method = \"ellipse\", order = \"hclust\",tl.cex = 0.5)\n```\n\n::: {.cell-output-display}\n![](Random-Forests_files/figure-html/EDA on penguins data-1.png){width=4200}\n:::\n\n```{.r .cell-code}\n# try these too:\n# cor %>% corrplot(., method = \"square\", order = \"hclust\",tl.cex = 0.5)\n# cor %>% corrplot(., method = \"color\", order = \"hclust\",tl.cex = 0.5)\n# cor %>% corrplot(., method = \"shade\", order = \"hclust\",tl.cex = 0.5)\n```\n:::\n\n\nNotes:\n- `flipper_length_mm` and `culmen_depth_mm` are negtively correlated at approx (-0.7)\n- `flipper_length_mm` and `body_mass_g` are positively correlated at approx 0.8\n\nSo we will use steps in the recipe to remove correlated variables. \n\n\n### Penguin Data Sampling and Recipe\n\n::: {.cell hash='Random-Forests_cache/html/Penguin Data Sampling and Recipe_b8a93d54cfddfa887e44935867c29d37'}\n\n```{.r .cell-code}\n# Data Split\npenguin_split <- initial_split(penguins, prop = 0.6)\npenguin_train <- training(penguin_split)\npenguin_test <- testing(penguin_split)\npenguin_split\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Training/Testing/Total>\n<199/134/333>\n```\n:::\n\n```{.r .cell-code}\nhead(penguin_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 8\n  species   island bill_length_mm bill_depth_mm flipper_le…¹ body_…² sex    year\n  <fct>     <fct>           <dbl>         <dbl>        <int>   <int> <fct> <int>\n1 Adelie    Dream            41.1          19            182    3425 male   2007\n2 Adelie    Biscoe           41.6          18            192    3950 male   2008\n3 Gentoo    Biscoe           40.9          13.7          214    4650 fema…  2007\n4 Adelie    Dream            42.2          18.5          180    3550 fema…  2007\n5 Chinstrap Dream            43.2          16.6          187    2900 fema…  2007\n6 Adelie    Dream            34            17.1          185    3400 fema…  2008\n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n```\n:::\n\n```{.r .cell-code}\n# Recipe\npenguin_recipe <- penguins %>% \n  recipe(species ~ .) %>% \n  step_normalize(all_numeric()) %>% # Scaling and Centering\n  step_corr(all_numeric()) %>%  # Handling correlated variables\n  prep()\n\n# Baking the data\npenguin_train_baked <-  penguin_train %>% \n  bake(object = penguin_recipe, new_data = .)\n\npenguin_test_baked <-  penguin_test %>% \n  bake(object = penguin_recipe, new_data = .)\n\nhead(penguin_train_baked)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 8\n  island bill_length_mm bill_depth_mm flipper_le…¹ body_…² sex      year species\n  <fct>           <dbl>         <dbl>        <dbl>   <dbl> <fct>   <dbl> <fct>  \n1 Dream          -0.529        0.932        -1.35   -0.971 male  -1.28   Adelie \n2 Biscoe         -0.438        0.424        -0.640  -0.319 male  -0.0517 Adelie \n3 Biscoe         -0.566       -1.76          0.930   0.550 fema… -1.28   Gentoo \n4 Dream          -0.328        0.678        -1.50   -0.816 fema… -1.28   Adelie \n5 Dream          -0.145       -0.287        -0.997  -1.62  fema… -1.28   Chinst…\n6 Dream          -1.83        -0.0329       -1.14   -1.00  fema… -0.0517 Adelie \n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n```\n:::\n:::\n\n\n### Penguin Random Forest Model\n\n\n::: {.cell hash='Random-Forests_cache/html/Penguin Random Forest Model_93a1b3e89d2cf81b28d058c244e1fb05'}\n\n```{.r .cell-code}\npenguin_model <- \n  rand_forest(trees = 100) %>% \n  set_engine(\"randomForest\") %>% \n  set_mode(\"classification\")\npenguin_model\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  trees = 100\n\nComputational engine: randomForest \n```\n:::\n\n```{.r .cell-code}\npenguin_fit <- \n  penguin_model %>% \n  fit(species ~ .,penguin_train_baked)\npenguin_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nparsnip model object\n\n\nCall:\n randomForest(x = maybe_data_frame(x), y = y, ntree = ~100) \n               Type of random forest: classification\n                     Number of trees: 100\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 0.5%\nConfusion matrix:\n          Adelie Chinstrap Gentoo class.error\nAdelie        85         0      0  0.00000000\nChinstrap      1        40      0  0.02439024\nGentoo         0         0     73  0.00000000\n```\n:::\n\n```{.r .cell-code}\n# iris_ranger <- \n#   rand_forest(trees = 100) %>% \n#   set_mode(\"classification\") %>% \n#   set_engine(\"ranger\") %>% \n#   fit(Species ~ ., data = iris_training_baked)\n```\n:::\n\n### Metrics for the Penguin Random Forest Model\n\n\n::: {.cell hash='Random-Forests_cache/html/Penguin Model Predictions and Metrics_5c890d14e4544366e037f89661ba63dc'}\n\n```{.r .cell-code}\n# Predictions\npredict(object = penguin_fit, new_data = penguin_test_baked) %>%  \n  dplyr::bind_cols(penguin_test_baked) %>% \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 134\nColumns: 9\n$ .pred_class       <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen, Biscoe, …\n$ bill_length_mm    <dbl> -0.6752636, -0.9312674, -0.5289757, -1.7541369, -1.4…\n$ bill_depth_mm     <dbl> 0.42409105, 0.32252879, 0.22096653, 0.62721557, 1.03…\n$ flipper_length_mm <dbl> -0.42573251, -1.42460769, -1.35325946, -1.21056301, …\n$ body_mass_g       <dbl> -1.18857213, -0.72285846, -1.25066728, -1.09542939, …\n$ sex               <fct> female, female, female, female, female, male, male, …\n$ year              <dbl> -1.2818130, -1.2818130, -1.2818130, -1.2818130, -1.2…\n$ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n```\n:::\n\n```{.r .cell-code}\n# Prediction Accuracy Metrics\npredict(object = penguin_fit, new_data = penguin_test_baked) %>%  \n  dplyr::bind_cols(penguin_test_baked) %>% \n  yardstick::metrics(truth = species, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy multiclass     0.985\n2 kap      multiclass     0.976\n```\n:::\n\n```{.r .cell-code}\n# Prediction Probabilities\npenguin_fit_probs <- \n  predict(penguin_fit, penguin_test_baked, type = \"prob\") %>%\n  dplyr::bind_cols(penguin_test_baked)\nglimpse(penguin_fit_probs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 134\nColumns: 11\n$ .pred_Adelie      <dbl> 0.99, 0.98, 0.98, 1.00, 0.98, 0.99, 0.96, 0.99, 0.99…\n$ .pred_Chinstrap   <dbl> 0.01, 0.02, 0.01, 0.00, 0.00, 0.00, 0.01, 0.00, 0.00…\n$ .pred_Gentoo      <dbl> 0.00, 0.00, 0.01, 0.00, 0.02, 0.01, 0.03, 0.01, 0.01…\n$ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen, Biscoe, …\n$ bill_length_mm    <dbl> -0.6752636, -0.9312674, -0.5289757, -1.7541369, -1.4…\n$ bill_depth_mm     <dbl> 0.42409105, 0.32252879, 0.22096653, 0.62721557, 1.03…\n$ flipper_length_mm <dbl> -0.42573251, -1.42460769, -1.35325946, -1.21056301, …\n$ body_mass_g       <dbl> -1.18857213, -0.72285846, -1.25066728, -1.09542939, …\n$ sex               <fct> female, female, female, female, female, male, male, …\n$ year              <dbl> -1.2818130, -1.2818130, -1.2818130, -1.2818130, -1.2…\n$ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n```\n:::\n\n```{.r .cell-code}\n# Confusion Matrix\npenguin_fit$fit$confusion %>% tidy()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: 'tidy.numeric' is deprecated.\nSee help(\"Deprecated\")\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 1\n  x[,\"Adelie\"] [,\"Chinstrap\"] [,\"Gentoo\"] [,\"class.error\"]\n         <dbl>          <dbl>       <dbl>            <dbl>\n1           85              0           0           0     \n2            1             40           0           0.0244\n3            0              0          73           0     \n```\n:::\n\n```{.r .cell-code}\n# Gain Curves\npenguin_fit_probs %>% \n  yardstick::gain_curve(species, .pred_Adelie:.pred_Gentoo) %>%\n  autoplot()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\nℹ The deprecated feature was likely used in the yardstick package.\n  Please report the issue at <https://github.com/tidymodels/yardstick/issues>.\n```\n:::\n\n::: {.cell-output-display}\n![](Random-Forests_files/figure-html/Penguin Model Predictions and Metrics-1.png){width=4200}\n:::\n\n```{.r .cell-code}\n# ROC Plot\npenguin_fit_probs%>%\n  roc_curve(species, .pred_Adelie:.pred_Gentoo) %>%\n  autoplot()\n```\n\n::: {.cell-output-display}\n![](Random-Forests_files/figure-html/Penguin Model Predictions and Metrics-2.png){width=4200}\n:::\n:::\n\n### Using `broom` on the penguin model\n\n::: {.cell hash='Random-Forests_cache/html/Using `broom` on the penguin model_683641eda069305a1368c40c233f7660'}\n\n```{.r .cell-code}\npenguin_split\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Training/Testing/Total>\n<199/134/333>\n```\n:::\n\n```{.r .cell-code}\npenguin_split %>% broom::tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 333 × 2\n     Row Data    \n   <int> <chr>   \n 1     1 Analysis\n 2     2 Analysis\n 3     4 Analysis\n 4     5 Analysis\n 5     7 Analysis\n 6     9 Analysis\n 7    10 Analysis\n 8    11 Analysis\n 9    12 Analysis\n10    13 Analysis\n# … with 323 more rows\n```\n:::\n\n```{.r .cell-code}\npenguin_recipe %>% broom::tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 6\n  number operation type      trained skip  id             \n   <int> <chr>     <chr>     <lgl>   <lgl> <chr>          \n1      1 step      normalize TRUE    FALSE normalize_zCUDv\n2      2 step      corr      TRUE    FALSE corr_WPWs7     \n```\n:::\n\n```{.r .cell-code}\n# Following do not work for `random forest models` !! ;-()\n#penguin_model %>% tidy()\n#penguin_fit %>% tidy() \npenguin_model %>% str()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 7\n $ args                 :List of 3\n  ..$ mtry : language ~NULL\n  .. ..- attr(*, \".Environment\")=<environment: R_EmptyEnv> \n  ..$ trees: language ~100\n  .. ..- attr(*, \".Environment\")=<environment: R_EmptyEnv> \n  ..$ min_n: language ~NULL\n  .. ..- attr(*, \".Environment\")=<environment: R_EmptyEnv> \n $ eng_args             : Named list()\n  ..- attr(*, \"class\")= chr [1:2] \"quosures\" \"list\"\n $ mode                 : chr \"classification\"\n $ user_specified_mode  : logi TRUE\n $ method               : NULL\n $ engine               : chr \"randomForest\"\n $ user_specified_engine: logi TRUE\n - attr(*, \"class\")= chr [1:2] \"rand_forest\" \"model_spec\"\n```\n:::\n\n```{.r .cell-code}\npenguin_test_baked\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 134 × 8\n   island    bill_length_mm bill_depth_mm flipper_…¹ body_…² sex    year species\n   <fct>              <dbl>         <dbl>      <dbl>   <dbl> <fct> <dbl> <fct>  \n 1 Torgersen         -0.675        0.424      -0.426  -1.19  fema… -1.28 Adelie \n 2 Torgersen         -0.931        0.323      -1.42   -0.723 fema… -1.28 Adelie \n 3 Torgersen         -0.529        0.221      -1.35   -1.25  fema… -1.28 Adelie \n 4 Torgersen         -1.75         0.627      -1.21   -1.10  fema… -1.28 Adelie \n 5 Biscoe            -1.48         1.03       -0.854  -0.506 fema… -1.28 Adelie \n 6 Biscoe            -1.06         0.475      -1.14   -0.319 male  -1.28 Adelie \n 7 Biscoe            -0.950        0.0178     -1.50   -0.506 male  -1.28 Adelie \n 8 Biscoe            -0.620        0.729      -1.28   -0.816 male  -1.28 Adelie \n 9 Biscoe            -0.639        0.881      -1.50   -0.319 male  -1.28 Adelie \n10 Dream             -0.822       -0.236      -1.64   -1.19  fema… -1.28 Adelie \n# … with 124 more rows, and abbreviated variable names ¹​flipper_length_mm,\n#   ²​body_mass_g\n```\n:::\n:::\n\n\n\n\n\n\n## Iris Random Forest Model with `ranger`\n\nUsing the `iris` dataset and Random Forest Classification.\nThis part uses `rsample` to split the data and the `recipes` to *prep* the data for model making. \n\n\n::: {.cell hash='Random-Forests_cache/html/Pre-process data_7d25f9742f56f53c14234b062da991de'}\n\n```{.r .cell-code}\n#set.seed(100)\niris_split <- rsample::initial_split(iris, prop = 0.6)\niris_split\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Training/Testing/Total>\n<90/60/150>\n```\n:::\n\n```{.r .cell-code}\niris_split %>% training() %>% glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 90\nColumns: 5\n$ Sepal.Length <dbl> 5.2, 5.1, 6.1, 5.2, 6.8, 4.6, 5.4, 5.8, 7.7, 6.5, 5.1, 6.…\n$ Sepal.Width  <dbl> 4.1, 3.5, 2.6, 3.5, 2.8, 3.4, 3.9, 2.6, 3.0, 3.0, 3.3, 2.…\n$ Petal.Length <dbl> 1.5, 1.4, 5.6, 1.5, 4.8, 1.4, 1.7, 4.0, 6.1, 5.8, 1.7, 4.…\n$ Petal.Width  <dbl> 0.1, 0.2, 1.4, 0.2, 1.4, 0.3, 0.4, 1.2, 2.3, 2.2, 0.5, 1.…\n$ Species      <fct> setosa, setosa, virginica, setosa, versicolor, setosa, se…\n```\n:::\n\n```{.r .cell-code}\niris_split %>% testing() %>% glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 60\nColumns: 5\n$ Sepal.Length <dbl> 4.7, 5.4, 4.8, 4.3, 5.8, 5.7, 5.1, 5.4, 5.1, 4.7, 5.5, 5.…\n$ Sepal.Width  <dbl> 3.2, 3.7, 3.0, 3.0, 4.0, 3.8, 3.8, 3.4, 3.7, 3.2, 4.2, 3.…\n$ Petal.Length <dbl> 1.3, 1.5, 1.4, 1.1, 1.2, 1.7, 1.5, 1.7, 1.5, 1.6, 1.4, 1.…\n$ Petal.Width  <dbl> 0.2, 0.2, 0.1, 0.1, 0.2, 0.3, 0.3, 0.2, 0.4, 0.2, 0.2, 0.…\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n```\n:::\n:::\n\n\n\n### Iris Data Pre-Processing: Creating the Recipe\n\nThe `recipes` package provides an interface that specializes in *data pre-processing*. Within the package, the functions that start, or execute, the data transformations are named after **cooking actions**. That makes the interface more user-friendly. For example:\n\n - `recipe()` - Starts a new set of transformations to be applied, similar to the `ggplot()` command. Its main argument is the model’s `formula`.\n\n - `prep()` - Executes the transformations on top of the data that is supplied (**typically, the training data**). Each data transformation is a `step()` function. ( Recall what we did with the `caret` package: *Centering, Scaling, Removing Correlated variables*...)\n\nNote that in order to avoid data leakage (e.g: transferring information from the train set into the test set), data should be “prepped” using the **train_tbl** only. <https://towardsdatascience.com/modelling-with-tidymodels-and-parsnip-bae2c01c131c>\nCRAN: The idea is that the preprocessing operations will all be **created** using the *training set* and then these steps will be **applied** to both the training and test set.\n\n\n::: {.cell hash='Random-Forests_cache/html/Declare Variables_4e2c2aa26318033ea2ebff082822ef31'}\n\n```{.r .cell-code}\n# Pre Processing the Training Data\n\niris_recipe <- \n  training(iris_split) %>% # Note: Using TRAINING data !!\n  recipe(Species ~ .)      # Note: Outcomes ~ Predictors !!\n\n# The data contained in the `data` argument need not be the training set; this data is only used to catalog the names of the variables and their types (e.g. numeric, etc.).\n```\n:::\n\n\nQ: How does the recipe \"figure\" out which are the outcomes and which are the predictors?\nA.The `recipe` command defines `Outcomes` and `Predictors` using the formula interface. ~~Not clear how this recipe \"figures\" out which are the outcomes and which are the predictors, when we have not yet specified them...~~\n\nQ. Why is the recipe not agnostic to data set? Is that a meaningful question?\nA. The use of the `training set` in the recipe command is just to declare the variables and specify the `roles` of the data, nothing else. `Roles` are open-ended and extensible. \nFrom <https://cran.r-project.org/web/packages/recipes/vignettes/Simple_Example.html> :\n\n> This document demonstrates some basic uses of recipes. First, some definitions are required:\n- **variables** are the original (raw) data columns in a data frame or tibble. For example, in a traditional formula Y ~ A + B + A:B, the variables are A, B, and Y.\n- **roles** define how variables will be used in the model. Examples are: `predictor` (independent variables), `response`, and `case weight`. This is meant to be open-ended and extensible.\n- **terms** are columns in a **design matrix** such as A, B, and A:B. These can be other derived entities that are grouped, such as a set of `principal components` or a set of columns, that define a `basis function` for a variable. These are synonymous with `features` in machine learning. Variables that have `predictor` roles would automatically be main `effect terms`.\n\n\n\n::: {.cell hash='Random-Forests_cache/html/Prep the recipe_26ebc103cafe6ef9478140f80592f315'}\n\n```{.r .cell-code}\n# Apply the transformation steps\niris_recipe <- iris_recipe %>% \n  step_corr(all_predictors()) %>% \n  step_center(all_predictors(), -all_outcomes()) %>% \n  step_scale(all_predictors(), -all_outcomes()) %>% \n  prep()\n```\n:::\n\n\nThis has created the `recipe()` and prepped it too. \nWe now need to apply it to our datasets:\n\n- Take `training` data and `bake()` it to prepare it for modelling. \n- Do the same for the `testing` set. \n\n\n::: {.cell hash='Random-Forests_cache/html/Pre-Processing_2_26b4d13f0ab66070e3a85e4bbe89fbff'}\n\n```{.r .cell-code}\niris_training_baked <- \n  iris_split %>% \n  training() %>% \n  bake(iris_recipe,.)\niris_training_baked\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 90 × 4\n   Sepal.Length Sepal.Width Petal.Width Species   \n          <dbl>       <dbl>       <dbl> <fct>     \n 1      -0.759        2.41      -1.48   setosa    \n 2      -0.878        1.02      -1.35   setosa    \n 3       0.307       -1.08       0.193  virginica \n 4      -0.759        1.02      -1.35   setosa    \n 5       1.14        -0.616      0.193  versicolor\n 6      -1.47         0.782     -1.22   setosa    \n 7      -0.522        1.95      -1.10   setosa    \n 8      -0.0487      -1.08      -0.0644 versicolor\n 9       2.20        -0.150      1.35   virginica \n10       0.780       -0.150      1.22   virginica \n# … with 80 more rows\n```\n:::\n\n```{.r .cell-code}\niris_testing_baked <- \n  iris_split %>% \n  testing() %>% \n  bake(iris_recipe,.)\niris_testing_baked \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 60 × 4\n   Sepal.Length Sepal.Width Petal.Width Species\n          <dbl>       <dbl>       <dbl> <fct>  \n 1      -1.35         0.316       -1.35 setosa \n 2      -0.522        1.48        -1.35 setosa \n 3      -1.23        -0.150       -1.48 setosa \n 4      -1.83        -0.150       -1.48 setosa \n 5      -0.0487       2.18        -1.35 setosa \n 6      -0.167        1.71        -1.22 setosa \n 7      -0.878        1.71        -1.22 setosa \n 8      -0.522        0.782       -1.35 setosa \n 9      -0.878        1.48        -1.10 setosa \n10      -1.35         0.316       -1.35 setosa \n# … with 50 more rows\n```\n:::\n:::\n\n\n### Iris Model Training using `parsnip`\n\nDifferent ML packages provide different interfaces (APIs ) to do the same thing  (e.g random forests). The `tidymodels` package provides a consistent interface to invoke a wide variety of packages supporting a wide variety of models. \n\nThe `parsnip` package is a successor to `caret`. \n\nTo model with `parsnip`:\n1. Pick a `model` : \n2. Set the `engine`\n3. Set the `mode` (if needed): *Classification* or *Regression*\n\nCheck [here](https://tidymodels.github.io/parsnip/articles/articles/Models.html) for models available in `parsnip`. \n\n - Mode: *classification* and *regression* in `parsnip`, each using a variety of models. ( **Which Way**). This defines the form of the output. \n \n - Engine: The `engine` is the **R package** that is invoked by `parsnip` to execute the model. E.g `glm`, `glmnet`,`keras`.( **How** ) `parsnip` provides **wrappers** for models from these packages. \n \n - Model: is the **specific technique** used for the modelling task. E.g `linear_reg()`, `logistic_reg()`, `mars`, `decision_tree`, `nearest_neighbour`...(What model). \n \n and models have:\n - `hyperparameters`: that are numerical or factor variables that `tune` the model ( Like the alpha beta parameters for Bayesian priors)\n\nWe can use the `random forest` model to **classify** the iris into species. Here Species is the `Outcome` variable and the rest are `predictor` variables. The `random forest` model is provided by the `ranger` package, to which `tidymodels/parsnip` provides a simple and consistent interface.\n\n\n::: {.cell hash='Random-Forests_cache/html/Random Forest Model with `ranger`_6a6f4f8646a1425497b5607596c5bd6e'}\n\n```{.r .cell-code}\nlibrary(ranger)\niris_ranger <- \n  rand_forest(trees = 100) %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"ranger\") %>% \n  fit(Species ~ ., data = iris_training_baked)\n```\n:::\n\n\n`ranger` can generate random forest models for `classification`, `regression`, `survival`( time series, time to event stuff). `Extreme Forests` are also supported, wherein all points in the dataset are used ( instead of bootstrap samples) along with `feature bagging`. \nWe can also run the same model using the `randomForest` package:\n\n\n::: {.cell hash='Random-Forests_cache/html/Random Forest Model with `randomForest`_7401d8b828ddad43c3d38d30f84c7ec4'}\n\n```{.r .cell-code}\nlibrary(randomForest,quietly = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nrandomForest 4.7-1.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nType rfNews() to see new features/changes/bug fixes.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'randomForest'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:ranger':\n\n    importance\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:ggplot2':\n\n    margin\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:dplyr':\n\n    combine\n```\n:::\n\n```{.r .cell-code}\niris_rf <- \n  rand_forest(trees = 100) %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"randomForest\") %>% \n  fit(Species ~ ., data = iris_training_baked)\n```\n:::\n\n### Iris Predictions\n\nThe `predict()` function run against a `parsnip` model returns a prediction `tibble`. By default, the prediction variable is called `.pred_class`. \n\n\n::: {.cell hash='Random-Forests_cache/html/Predictions_4114309da986b7690fa337c471016d9a'}\n\n```{.r .cell-code}\npredict(object = iris_ranger, new_data = iris_testing_baked) %>%  \n  dplyr::bind_cols(iris_testing_baked) %>% \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 60\nColumns: 5\n$ .pred_class  <fct> setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n$ Sepal.Length <dbl> -1.35138822, -0.52239642, -1.23296082, -1.82509781, -0.04…\n$ Sepal.Width  <dbl> 0.31591693, 1.48118431, -0.15019002, -0.15019002, 2.18034…\n$ Petal.Width  <dbl> -1.3532022, -1.3532022, -1.4820786, -1.4820786, -1.353202…\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n```\n:::\n:::\n\n\n### Iris Classification Model Validation\n\nWe use `metrics()` function from the `yardstick` package to evaluate how good the model is. \n\n\n\n::: {.cell hash='Random-Forests_cache/html/metrics `ranger`_fd11520c26da43cc9ad794e3f4b45c73'}\n\n```{.r .cell-code}\npredict(iris_ranger, iris_testing_baked) %>%\n  dplyr::bind_cols(iris_testing_baked) %>% \n  yardstick::metrics(truth = Species, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy multiclass     0.967\n2 kap      multiclass     0.950\n```\n:::\n:::\n\n\nWe can also check the metrics for `randomForest` model:\n\n\n::: {.cell hash='Random-Forests_cache/html/metrics `randomForest`_59d0efc6406a15254a559a080799fb2b'}\n\n```{.r .cell-code}\npredict(iris_rf, iris_testing_baked) %>%\n  dplyr::bind_cols(iris_testing_baked) %>% \n  yardstick::metrics(truth = Species, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy multiclass     0.95 \n2 kap      multiclass     0.925\n```\n:::\n:::\n\n\n### Iris Per-Classifier Metrics\n\nWe can use the parameter `type = \"prob\"` in the `predict()` function to obtain a probability score on each prediction. \n**TBD: How is this prob calculated?** Possible answer: the Random Forest model outputs its answer by majority voting across n trees. Each of the possible answers( i.e. predictions) for a particular test datum gets a share of the vote, that represents its probability. Hence each dataum in the test vector can show a probability for the \"winning\" answer. ( Quite possibly we can get the probabilities for *all possible outcomes* for **each test datum**)\n\n\n::: {.cell hash='Random-Forests_cache/html/Classification Probabilities_41c36faee1b4ba00e2f08c3534d743e2'}\n\n```{.r .cell-code}\niris_ranger_probs <- \n  predict(iris_ranger, iris_testing_baked, type = \"prob\") %>%\n  dplyr::bind_cols(iris_testing_baked)\nglimpse(iris_ranger_probs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 60\nColumns: 7\n$ .pred_setosa     <dbl> 0.99147619, 0.96175794, 0.94710119, 0.94710119, 0.730…\n$ .pred_versicolor <dbl> 0.005666667, 0.020416667, 0.031291667, 0.031291667, 0…\n$ .pred_virginica  <dbl> 0.002857143, 0.017825397, 0.021607143, 0.021607143, 0…\n$ Sepal.Length     <dbl> -1.35138822, -0.52239642, -1.23296082, -1.82509781, -…\n$ Sepal.Width      <dbl> 0.31591693, 1.48118431, -0.15019002, -0.15019002, 2.1…\n$ Petal.Width      <dbl> -1.3532022, -1.3532022, -1.4820786, -1.4820786, -1.35…\n$ Species          <fct> setosa, setosa, setosa, setosa, setosa, setosa, setos…\n```\n:::\n\n```{.r .cell-code}\niris_rf_probs <- \n  predict(iris_rf, iris_testing_baked, type = \"prob\") %>%\n  dplyr::bind_cols(iris_testing_baked)\nglimpse(iris_rf_probs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 60\nColumns: 7\n$ .pred_setosa     <dbl> 0.99, 0.97, 0.95, 0.95, 0.73, 0.81, 0.98, 0.98, 0.98,…\n$ .pred_versicolor <dbl> 0.01, 0.00, 0.02, 0.02, 0.22, 0.14, 0.00, 0.02, 0.00,…\n$ .pred_virginica  <dbl> 0.00, 0.03, 0.03, 0.03, 0.05, 0.05, 0.02, 0.00, 0.02,…\n$ Sepal.Length     <dbl> -1.35138822, -0.52239642, -1.23296082, -1.82509781, -…\n$ Sepal.Width      <dbl> 0.31591693, 1.48118431, -0.15019002, -0.15019002, 2.1…\n$ Petal.Width      <dbl> -1.3532022, -1.3532022, -1.4820786, -1.4820786, -1.35…\n$ Species          <fct> setosa, setosa, setosa, setosa, setosa, setosa, setos…\n```\n:::\n\n```{.r .cell-code}\n# Tabulating the probabilities\nftable(iris_rf_probs$.pred_versicolor)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n 0 0.01 0.02 0.03 0.05 0.06 0.08 0.12 0.13 0.14 0.15 0.17 0.22 0.24 0.26 0.28 0.29 0.3 0.34 0.5 0.62 0.64 0.68 0.72 0.74 0.75 0.76 0.77 0.78 0.79 0.81 0.84 0.88 0.9 0.94 0.95 0.96 0.97 1\n                                                                                                                                                                                          \n 8    4    8    1    2    1    1    1    1    1    1    1    1    1    1    1    1   1    1   1    1    1    1    2    1    1    1    1    1    1    2    1    1   1    1    1    1    2 1\n```\n:::\n\n```{.r .cell-code}\nftable(iris_rf_probs$.pred_virginica)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n 0 0.02 0.03 0.05 0.06 0.09 0.11 0.12 0.14 0.16 0.18 0.21 0.24 0.26 0.28 0.31 0.36 0.38 0.5 0.66 0.7 0.71 0.72 0.76 0.83 0.85 0.87 0.88 0.92 0.94 0.95 0.97 0.98 0.99\n                                                                                                                                                                     \n 8    6    8    4    2    1    1    1    1    1    2    1    1    2    1    1    1    1   1    1   1    1    1    1    1    1    1    1    1    1    2    1    1    1\n```\n:::\n\n```{.r .cell-code}\nftable(iris_rf_probs$.pred_setosa)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  0 0.01 0.02 0.05 0.11 0.12 0.68 0.73 0.81 0.95 0.97 0.98 0.99  1\n                                                                  \n 28    6    1    1    1    2    1    1    1    5    1    7    3  2\n```\n:::\n:::\n\n```\n\n### Iris Classifier: Gain and ROC Curves\n\nWe can plot gain and ROC curves for each of these models\n\n::: {.cell hash='Random-Forests_cache/html/Gain and ROC Curves `ranger`_4d61a502168be25c4ca5da6162ae106e'}\n\n```{.r .cell-code}\niris_ranger_probs %>% \n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %>% \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 137\nColumns: 5\n$ .level          <chr> \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"set…\n$ .n              <dbl> 0, 2, 4, 5, 9, 10, 11, 12, 13, 17, 18, 19, 20, 21, 22,…\n$ .n_events       <dbl> 0, 2, 4, 5, 9, 10, 11, 12, 13, 17, 18, 19, 20, 21, 21,…\n$ .percent_tested <dbl> 0.000000, 3.333333, 6.666667, 8.333333, 15.000000, 16.…\n$ .percent_found  <dbl> 0.00000, 9.52381, 19.04762, 23.80952, 42.85714, 47.619…\n```\n:::\n\n```{.r .cell-code}\niris_ranger_probs %>% \n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %>% \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](Random-Forests_files/figure-html/Gain and ROC Curves `ranger`-1.png){width=4200}\n:::\n\n```{.r .cell-code}\niris_ranger_probs %>% \n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %>% \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 140\nColumns: 4\n$ .level      <chr> \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\"…\n$ .threshold  <dbl> -Inf, 0.000000000, 0.000625000, 0.001000000, 0.003958333, …\n$ specificity <dbl> 0.0000000, 0.0000000, 0.3589744, 0.4102564, 0.4615385, 0.4…\n$ sensitivity <dbl> 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0…\n```\n:::\n\n```{.r .cell-code}\niris_ranger_probs %>% \n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %>% \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](Random-Forests_files/figure-html/Gain and ROC Curves `ranger`-2.png){width=4200}\n:::\n:::\n\n\n::: {.cell hash='Random-Forests_cache/html/Gain and ROC Curves `randomForest`_7b01ecf604009f93d9a4958c371ca205'}\n\n```{.r .cell-code}\niris_rf_probs %>% \n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %>% \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 90\nColumns: 5\n$ .level          <chr> \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"set…\n$ .n              <dbl> 0, 2, 5, 12, 13, 18, 19, 20, 21, 23, 24, 25, 26, 32, 6…\n$ .n_events       <dbl> 0, 2, 5, 12, 13, 18, 19, 20, 21, 21, 21, 21, 21, 21, 2…\n$ .percent_tested <dbl> 0.000000, 3.333333, 8.333333, 20.000000, 21.666667, 30…\n$ .percent_found  <dbl> 0.000000, 9.523810, 23.809524, 57.142857, 61.904762, 8…\n```\n:::\n\n```{.r .cell-code}\niris_rf_probs %>% \n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %>% \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](Random-Forests_files/figure-html/Gain and ROC Curves `randomForest`-1.png){width=4200}\n:::\n\n```{.r .cell-code}\niris_rf_probs %>% \n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %>% \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 93\nColumns: 4\n$ .level      <chr> \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\"…\n$ .threshold  <dbl> -Inf, 0.00, 0.01, 0.02, 0.05, 0.11, 0.12, 0.68, 0.73, 0.81…\n$ specificity <dbl> 0.0000000, 0.0000000, 0.7179487, 0.8717949, 0.8974359, 0.9…\n$ sensitivity <dbl> 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0…\n```\n:::\n\n```{.r .cell-code}\niris_rf_probs %>% \n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %>% \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](Random-Forests_files/figure-html/Gain and ROC Curves `randomForest`-2.png){width=4200}\n:::\n:::\n\n\n\n### Iris Classifier: Metrics\n\n\n::: {.cell hash='Random-Forests_cache/html/unnamed-chunk-1_9b31ebd3746bc3bdfec62b04dab56cec'}\n\n```{.r .cell-code}\npredict(iris_ranger, iris_testing_baked, type = \"prob\") %>% \n  bind_cols(predict(iris_ranger,iris_testing_baked)) %>% \n  bind_cols(select(iris_testing_baked,Species)) %>% \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 60\nColumns: 5\n$ .pred_setosa     <dbl> 0.99147619, 0.96175794, 0.94710119, 0.94710119, 0.730…\n$ .pred_versicolor <dbl> 0.005666667, 0.020416667, 0.031291667, 0.031291667, 0…\n$ .pred_virginica  <dbl> 0.002857143, 0.017825397, 0.021607143, 0.021607143, 0…\n$ .pred_class      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setos…\n$ Species          <fct> setosa, setosa, setosa, setosa, setosa, setosa, setos…\n```\n:::\n\n```{.r .cell-code}\npredict(iris_ranger, iris_testing_baked, type = \"prob\") %>% \n  bind_cols(predict(iris_ranger,iris_testing_baked)) %>% \n  bind_cols(select(iris_testing_baked,Species)) %>% \n  yardstick::metrics(data = ., truth = Species, estimate = .pred_class, ... = .pred_setosa:.pred_virginica)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 3\n  .metric     .estimator .estimate\n  <chr>       <chr>          <dbl>\n1 accuracy    multiclass     0.967\n2 kap         multiclass     0.950\n3 mn_log_loss multiclass     0.221\n4 roc_auc     hand_till      0.987\n```\n:::\n\n```{.r .cell-code}\n# And for the `randomForest`method\n\npredict(iris_rf, iris_testing_baked, type = \"prob\") %>% \n  bind_cols(predict(iris_ranger,iris_testing_baked)) %>% \n  bind_cols(select(iris_testing_baked,Species)) %>% \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 60\nColumns: 5\n$ .pred_setosa     <dbl> 0.99, 0.97, 0.95, 0.95, 0.73, 0.81, 0.98, 0.98, 0.98,…\n$ .pred_versicolor <dbl> 0.01, 0.00, 0.02, 0.02, 0.22, 0.14, 0.00, 0.02, 0.00,…\n$ .pred_virginica  <dbl> 0.00, 0.03, 0.03, 0.03, 0.05, 0.05, 0.02, 0.00, 0.02,…\n$ .pred_class      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setos…\n$ Species          <fct> setosa, setosa, setosa, setosa, setosa, setosa, setos…\n```\n:::\n\n```{.r .cell-code}\npredict(iris_rf, iris_testing_baked, type = \"prob\") %>% \n  bind_cols(predict(iris_ranger,iris_testing_baked)) %>% \n  bind_cols(select(iris_testing_baked,Species)) %>% \n  yardstick::metrics(data = ., truth = Species, estimate = .pred_class, ... = .pred_setosa:.pred_virginica)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 3\n  .metric     .estimator .estimate\n  <chr>       <chr>          <dbl>\n1 accuracy    multiclass     0.967\n2 kap         multiclass     0.950\n3 mn_log_loss multiclass     0.197\n4 roc_auc     hand_till      0.988\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\r\n<script src=\"../../../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}