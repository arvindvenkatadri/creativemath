{
  "hash": "573936146e792e7ce5889350aee98ddb",
  "result": {
    "markdown": "---\ntitle: ML - Clustering\ndate: 19/July/2022\ndate-modified: \"2023-03-15\"\nabstract: \"We will look at the basic models for Clustering of Data.\"\norder: 40\ntags:\n  - Machine Learning\n  - Orange\n  - kNN Algorithm\n  - k Means Algorithm\n---\n\n\n## Introduction\n\nQuoting from <http://baoqiang.org/?p=579>\n\n### k-Nearest-Neighbour and K-Means clustering\n\nThese two are arguably the two commonly used cluster methods. One of the\nreasons is that they are easy to use and also somehow straightforward.\nSo how do they work?\n\n**k-Nearest-Neighbour**: Provide N n-dimension entries with known\nassociated classes for each entry, the number of classes is k, that is,\n$$\n\\{\\vec{x_i}, y_i\\} ,\\ \\vec{x_i} \\in\\ {\\Re^{n}}\\ , y_i\\ = \\{c_1,...c_k\\}, \ni = 1...N\n$$\n\nFor a new entry $\\vec{v_j}$, to which class should it belong? We need\nuse a distance measure to get the k closest entries of the new entry\n\\vec{v_{j}}, the final decision is *simple majority vote* based the\nclosest k neighbors. The distance metric could be euclidean or other\nsimilar ones.\n\n<iframe width=\"100%\" height=\"735\" frameborder=\"0\" src=\"https://observablehq.com/embed/16bc2b3dcb13d1cd@289?cells=viewof+numTrain%2Cviewof+k%2CPlot\">\n\n</iframe>\n\n**K-means**: Given N n-dimension entries and classify them in k classes.\nAt first, we *randomly* choose k entries and assign them to k clusters.\nThey are the seed classes. Then we calculate the distance between each\nentry and each class. Each entry will be assigned into one class in\nterms of the its distance to each class, i.e., assign the entry to its\nclosest class. After the assignment is complete, we then calculate the\ncentroid of each class based on their new members. After the centroid\ncalculation, we go back to the distance calculation and therefore new\nround classification. We stop the iteration when there is\nconvergence,i.e,, no new centroid and classification.\n\nThe two methods are all *semi-supervised learning algorithms* because\nthey do need we provide the number of clusters prior the clustering.\n\n<iframe width=\"100%\" height=\"853\" frameborder=\"0\" src=\"https://observablehq.com/embed/ab4e983a61997013?cells=viewof+seed%2Cviewof+spread%2Cviewof+num_centroids%2Cviewof+selection%2Cviewof+stepslider\">\n\n</iframe>\n\n## Workflow using Orange\n\n## Workflow using Radiant\n\n## Workflow using R\n\n## Conclusion\n\n## References\n\n1.  K-means Cluster Analysis. [UC Business Analytics R Programming\n    Guide](https://uc-r.github.io/)\n    <https://uc-r.github.io/kmeans_clustering#optimal>\n\n2.  Thean C Lim. Clustering: k-means, k-means ++ and gganimate.\n    <https://theanlim.rbind.io/post/clustering-k-means-k-means-and-gganimate/>\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../../../../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\r\n<script src=\"../../../../../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}