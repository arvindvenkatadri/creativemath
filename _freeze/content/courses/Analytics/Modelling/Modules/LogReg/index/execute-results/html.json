{
  "hash": "f1ff00e098d364d4a4445d1ef55c863a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Modelling with Logistic Regression\"\nabstract: \"Predicting Qualitative Target Variables\"\ndate: 13/Apr/2023\ndate-modified: \"2024-06-25\"\norder: 20\nimage: featured.png\nimage-alt: \"\"\ncategories: \n  - Logistic Regression\n  - Qualitative Variable\n  - Probability\n  - Odds\n  - Log Transformation\nbibliography: \n  - grateful-refs.bib\ncitation: true\n#suppress-bibliography: true\neditor: \n  markdown: \n    wrap: 72\n---\n\n\n\n## {{< iconify noto-v1 package >}} Setting up R Packages\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(mosaic)\nlibrary(infer)\nlibrary(regressinator) # pedagogic tool for GLMs\nlibrary(GLMsData) # Datasets from Dunn and Smyth\nlibrary(HSAUR3) # Datasets from Everitt and Hothorn\nlibrary(prettyglm) # create beautiful coefficient summaries of generalised linear models.\n# remotes::install_github(\"UCLATALL/JMRData\")\n# library(JMRData)\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\n## {{< iconify openmoji japanese-symbol-for-beginner >}} Introduction\n\nSometimes the dependent variable is an either/or categorization. For\nexample, the variable we want to predict might be `won` or `lost` the\ncontest, `has an ailment` or `not`, `voted` or `not` in the last\nelection, or `graduated` from college or `not`. There might even be more\nthan two categories such as voted for Congress, BJP, or Independent; or\nnever smoker, former smoker, or current smoker.\n\nWe saw with the **General Linear Model** that it models the **mean** of\na target *Quantitative* variable as a linear weighted sum of the\npredictor variables:\n\n$$\ny \\sim N(x_i^T * \\beta, ~~\\sigma^2)\n$$\n\nThis model is considered to be **general** because of the dependence on\npotentially *more than one explanatory variable*, v.s. the **simple**\nlinear model:[^1] $y = \\beta_0 + \\beta_1*x_1 + \\epsilon$. The general\nlinear model gives us model \"shapes\" that start from a simple straight\nline to a *p-dimensional hyperplane*.\n\n[^1]: <https://statmath.wu.ac.at/courses/heather_turner/glmCourse_001.pdf>\n\nAlthough a very useful framework, there are some situations where\ngeneral linear models are not appropriate:\n\n-   the range of Y is restricted (e.g. binary, count)\n-   the variance of Y depends on the mean (Taylor's Law)[^2]\n\n[^2]: <https://en.wikipedia.org/wiki/Taylor%27s_law>\n\nHow do we use the familiar *linear model* framework when the\ntarget/dependent variable is *Categorical*?\n\n### Linear Models for Categorical Targets?\n\nRecall that we spoke of `dummy **predictor** variables` for our linear\nmodels and how we would **dummy code** them using numerical values, such\nas 0 and 1, or +1 and -1. Could we try the same way for a **target**\ncategorical variable?\n\n$$\nY_i = \\beta_0 + \\beta_1*Xi + \\epsilon_i\\\\ \\nonumber\nwhere\\\\\n\\begin{align}\nY_i &= 0 ~ if ~ \"No\"\\\\ \\nonumber\n    &= 1 ~ if ~\"Yes\"  \\nonumber\n\\end{align}\n$$\n\nSadly this seems to not work for categorical dependent variables using a\nsimple linear model as before. Consider the Credit Card `Default` data\nfrom the package `ISLR`.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 10,000\nColumns: 4\n$ default <fct> No, No, No, No, No, No, No, No, No, No, No, No, No, No, No, No…\n$ student <fct> No, Yes, No, No, No, Yes, No, Yes, No, No, Yes, Yes, No, No, N…\n$ balance <dbl> 729.5265, 817.1804, 1073.5492, 529.2506, 785.6559, 919.5885, 8…\n$ income  <dbl> 44361.625, 12106.135, 31767.139, 35704.494, 38463.496, 7491.55…\n```\n\n\n:::\n:::\n\n\n\nWe see `balance` and `income` are quantitative predictors; `student` is\na qualitative predictor, and `default` is a qualitative target variable.\nIf we naively use a linear model equation as\n`model = lm(default ~ balance, data = Default)` and plot it, then...\n\n::: grid\n::: g-col-6\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Naive Linear Model](index_files/figure-html/fig-naive-linear-model-1.png){#fig-naive-linear-model fig-align='center' width=2100}\n:::\n:::\n\n\n:::\n\n::: g-col-6\n...it is pretty much clear from @fig-naive-linear-model that something\nis very odd. (no pun intended! See below!) If the only possible values\nfor `default` are $No = 0$ and $Yes = 1$, how could we interpret\npredicted value of, say, $Y_i = 0.25$ or $Y_i = 1.55$, or perhaps\n$Y_i = -0.22$? Anything other than Yes/No is hard to interpret!\n:::\n:::\n\n### {{< iconify ic baseline-report-problem >}} {{< iconify ant-design solution-outlined >}} Problems...and Solutions\n\nWhere do we go from here?\n\nLet us state what we might desire of our model:\n\n1.  **Model Equation**: Despite this setback, we would still like our\n    model to be as close as possible to the familiar linear model\n    equation.\\\n2.  **Predictors and Weights**: We have quantitative **predictors** so\n    we still want to use a linear-weighted sum for the RHS (i.e\n    predictor side) of the model equation.\n\nWhat can we try to make this work? Especially for the LHS (i.e the\ntarget side)?\n\n3.  **Making the LHS continuous**: What can we try? In dummy encoding\n    our target variable, we found a range of \\[0,1\\], which is the same\n    range for a **probability** value! Could we try to use **probability\n    of the outcome** as our target, even though we are interested in\n    binary outcomes? This would still leave us with a range of $[0,1]$\n    for the target variable, as before.\n\n::: callout-note\n::: grid\n::: g-col-6\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-html/fig-binomial-mean-variance-1.png){#fig-binomial-mean-variance fig-align='center' width=2100}\n:::\n:::\n\n\n:::\n\n::: g-col-6\n#### Binomially distributed target variable\n\nIf we map our Categorical/Qualitative target variable into a\nQuantitative probability, we need immediately to look at the [**LINE**\nassumptions in linear\nregression](../LinReg/index.qmd#sec-assumptions-in-linear-models).\n\nIn linear regression, we assume a normally distributed target variable,\ni.e. the errors around the predicted value are normally distributed.\nWith a categorical target variable with two levels $0$ and $1$ it would\nbe impossible for the errors $e_i = Y_i - \\hat{Y_i}$ to have a *normal\ndistribution*, as assumed for the statistical tests to be valid. The\nerrors are bounded by $[0,1]$! One candidate for the error distribution\nin this case is the *binomial distribution*, whose mean and variance are\n`p` and `np(1-p)` respectively.\n\nNote immediately that **the binomial variance moves with the mean**!\n:::\n:::\n\nThe LINE assumption of *normality* is clearly violated. And from\n@fig-binomial-mean-variance, extreme probabilities (near 1 or 0) are\nmore stable (i.e., have less error variance) than middle probabilities.\nSo the model has *\"built-in\" heteroscedasticity*, which we need to\ncounter with transformations such as the $log()$ function. More on this\nlater.\n:::\n\n4.  **Odds**?: How would one \"extend\" the range of a target variable\n    from \\[0,1\\] to $[-\\infty, \\infty]$ ? One step would be to try the\n    **odds of the outcome**, instead of trying to predict the outcomes\n    directly (Yes or No), or their probabilities $[0,1]$.\n\n::: callout-note\n## Odds\n\nOdds of an event with probability `p` of occurrence is defined as\n$Odds = p/(1-p)$. As can be seen, the odds are the *ratio* of two\nprobabilities, that of the event and its complement. In the `Default`\ndataset just considered, the odds of default and the odds of non-default\ncan be calculated as:\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"default\"],\"name\":[1],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"n\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"No\",\"2\":\"9667\"},{\"1\":\"Yes\",\"2\":\"333\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n$$\n\\begin{align}\nOddsDefault &=p(noDefault)/(1-p(noDefault))\\\\ \\nonumber\n            &= 0.9667/(1-0.9667)\\\\ \\nonumber\n            &= 29.0303\\\\\n\\end{align}\n$$\n\nand `OddsNoDefault` = $1/29.0303 = 0.03444709$.\n\nNow, *odds* cover half of real number line, i.e. $[0, \\infty]$ !\nClearly, when the probability `p` of an event is $0$, the odds are\n$0$...and when it nears $1$, the odds tend to $\\infty$. So we have\n**transformed** a simple probability that lies between $[0,1]$ to odds\nlying between $[0, \\infty]$. That's one step towards making a linear\nmodel possible; we have \"removed\" one of the limits on our linear\nmodel's prediction range by using `Odds` as our target variable.\n:::\n\n5.  **Transformation using `log()`?**: We need one more leap of faith:\n    how do we convert a $[0, \\infty]$ range to a $[-\\infty, \\infty]$?\n    Can we try a log transformation?\n\n$$\nlog([0, \\infty]) ~ = ~ [-\\infty, \\infty]\n$$ This extends the range of our Qualitative target to the same as with\na Quantitative target!\n\nThere is an additional benefit if this `log()` transformation: the\n**Error Distributions with Odds targets**. See @fig-odds-plot below.\nOdds are a necessarily nonlinear function of probability; the slope of\n`Odds ~ probability` also depends upon the probability itself, as we saw\nwith the probability curve @fig-binomial-mean-variance.\n\n\n\n::: {#fig-odds-plot .cell layout-ncol=\"2\" layout-align=\"center\"}\n::: {.cell-output-display}\n![Odds](index_files/figure-html/fig-odds-plot-1.png){#fig-odds-plot-1 fig-align='center' width=2100}\n:::\n\n::: {.cell-output-display}\n![Log Odds](index_files/figure-html/fig-odds-plot-2.png){#fig-odds-plot-2 fig-align='center' width=2100}\n:::\n\nOdds Plot\n:::\n\n\n\nTo understand this issue intuitively, consider what happens to, say, a\n5% change in the odds ratio near 1.0 compared to more extreme odds\nratios, @fig-odds-plot-1 . If the odds ratio is $1.0$, then the\nprobabilities `p` and `1-p` are $0.5$, and $0.5$. A 20% increase in the\nodds ratio to $1.20$ would correspond to probabilities of $0.545$ and\n$0.455$. However, if the original probabilities were $0.9$ and $0.1$ for\nan odds ratio $9$, then a 20% increase to $10.8$ would correspond to\nprobabilities of $0.915$ and $0.085$, a much smaller change in the\nprobabilities. The `log` transformation provides a more linear\nrelationship, which is what we desire.\n\nSo in our model, instead of modeling *odds* as the dependent variable,\nwe will use $log(odds)$, also known as the **logit**, defined as:\n\n$$\n\\begin{align}\nlog(odds_i) &= log[(p_i)/(1-p_i)]\\\\ \\nonumber\n            &= logit(p_i)\\\\ \n\\end{align}\n$$\n\n5.  **Estimation of Model Parameters**: The last problem to solve is\n    that because we have made so many transformations to get to the\n    `logits` that we want to model, the logic of minimizing the **sum of\n    squared errors(SSE)** is no longer appropriate.\n\n::: callout-note\nThe probabilities for `default` are $0$ and $1$...the `log(odds)` will\nmap respectively to $-\\infty$ and $\\infty$. So if we naively try to take\nresiduals, we will find that they are **all** $\\infty$ !! Hence $SSE$\ncannot be computed and we need another way to assess the quality of our\nmodel.\n:::\n\nInstead, we will have to use **maximum likelihood estimation(MLE)** to\nestimate the models, and we will use the $X^2$ (\"chi-squared\") statistic\ninstead of `t` and `F`to evaluate the model comparisons. The *maximum\nlikelihood method* maximizes the probability of obtaining the data at\nhand against every choice of model parameters $\\beta_i$.\n\nThis is our **Logistic Regression Model**, which uses a Quantitative\nPredictor variable to predict a Categorical target variable. We write\nthe model as ( for the `Default` dataset:\n\n$$\n\\begin{align}\nlogit(default) & = \\beta_0 + \\beta_1 * balance&\\\\  \\nonumber\nlog(p(default)/(1-p(default))) & = \\beta_0+\\beta_1 * balance&\\\\ \\nonumber\ntherefore\\\\\np(default) & = \\frac{exp(\\beta_0 + \\beta_1 * balance)}{1 + exp(\\beta_0 + \\beta_1 * balance)}\\\\\n\\end{align}\n$$\n\nFrom the Eqn.4 above it should be clear that a *unit increase* in\n`balance` should increase the odds of `default` by $\\beta_1$ units.\n\nThe RHS of Eqn.5 is a *sigmoid* function of the weighted sum of\npredictors and is limited to the range \\[0,1\\]. The parameters $\\beta_i$\nneed to be estimated using maximum likelihood methods.\n\n\n\n::: {#fig-model-plots .cell layout-ncol=\"3\" layout-align=\"center\"}\n::: {.cell-output-display}\n![naive linear regression model](index_files/figure-html/fig-model-plots-1.png){#fig-model-plots-1 fig-align='center' width=2100}\n:::\n\n::: {.cell-output-display}\n![logistic regression model](index_files/figure-html/fig-model-plots-2.png){#fig-model-plots-2 fig-align='center' width=2100}\n:::\n\n::: {.cell-output-display}\n![log odds gives linear models](index_files/figure-html/fig-model-plots-3.png){#fig-model-plots-3 fig-align='center' width=2100}\n:::\n\nModel Plots\n:::\n\n\n\n## {{< iconify simple-icons hypothesis >}} Logistic Regression Models as Hypothesis Tests\n\nTo Be Written Up.\n\n::: {.content-hidden when-format=\"pdf\"}\n## {{< iconify tdesign function-curve >}} Generalized Linear Model\n\n::: callout-important\nA **generalized linear model** is made up of a linear predictor:\n\n$$\n\\eta_i = \\beta_0 + \\beta_1x_{1i} + ... + \\beta_px_{pi}\n$$\n\nand two functions:\n\n-   a link function that describes how the mean, $E(Y_i) = \\mu_i$,\n    depends on the linear predictor:\\\n\n$$\n    g(\\mu_i) = \\eta_i\n$$\n\n-   a variance function that describes how the variance, $var(Y_i)$\n    depends on the mean:\\\n\n$$\n    var(Y_i) = \\Phi*V(\\mu_i)\n$$\n\nwhere the dispersion parameter $\\Phi$ is a constant.\n:::\n\nFor example we can obtain our *general linear model* with the following\nchoice:\n\n$$\n\\begin{align}\n& g(\\mu_i) = \\mu_i\\\\\n& Phi = 1\n\\end{align}\n$$\n\nIf now we assume that the *target* variable $Y_i$ is a **binomial**,\ni.e. a two-valued variable:\\\n\n$$\n\\begin{align}\n & Y_i = binom(n_i,p_i)\\\\\n & mean(Y_i) = n_ip_i\\\\\n & var(Y_i) = n_ip_i(1-p_i)\n\\end{align}\n$$\n\nNow, we wish to model the **proportions** $Y_i/n_i$, as our **target**.\nThen we can state that:\\\n\n$$\n\\begin{align}\nmean(Y_i/n_i) = p_i := \\mu_i\\\\\nvar(Y_i/n_i) = var(Y_i)/n_i^2 = \\frac{p_i(1-p_i)}{n_i} := \\sigma_i^2\\\\\n\\end{align}\n$$\n\nInspecting the above, we can write:\n\n$$\n\\sigma_i^2 = \\frac{\\mu_i(1-\\mu_i)}{n_i}\n$$\n\nand since the link function needs to map ${[-\\infty, \\infty]}$ to\n${[0,1]}$, we use the `logit` function:\n\n$$\ng(\\mu_i) = logit(\\mu_i) = log(\\frac{\\mu_i}{1-\\mu_i})\n$$\n:::\n\n## {{< iconify flat-color-icons workflow >}} Workflow: Read the Data\n\nLet us now read in the data and check for these assumptions as part of\nour Workflow.\n\n::: callout-note\n## Research Question\n\nTo Be Written Up.\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata(\"earinf\", package = \"GLMsData\")\ninspect(earinf)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\ncategorical variables:  \n  name  class levels   n missing                                  distribution\n1 Swim factor      2 287       0 Occas (50.2%), Freq (49.8%)                  \n2  Loc factor      2 287       0 Beach (51.2%), NonBeach (48.8%)              \n3  Age factor      3 287       0 15-19 (48.8%), 20-24 (27.5%) ...             \n4  Sex factor      2 287       0 Male (65.5%), Female (34.5%)                 \n\nquantitative variables:  \n      name   class min Q1 median Q3 max      mean        sd   n missing\n1 NumInfec integer   0  0      0  2  17 1.3867596 2.3385412 287       0\n2    Infec integer   0  0      0  1   1 0.4738676 0.5001888 287       0\n```\n\n\n:::\n\n```{.r .cell-code}\nglimpse(earinf)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 287\nColumns: 6\n$ Swim     <fct> Occas, Occas, Occas, Occas, Occas, Occas, Occas, Occas, Occas…\n$ Loc      <fct> NonBeach, NonBeach, NonBeach, NonBeach, NonBeach, NonBeach, N…\n$ Age      <fct> 15-19, 15-19, 15-19, 15-19, 15-19, 15-19, 15-19, 15-19, 15-19…\n$ Sex      <fct> Male, Male, Male, Male, Male, Male, Male, Male, Male, Male, M…\n$ NumInfec <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2…\n$ Infec    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n```\n\n\n:::\n\n```{.r .cell-code}\nskimr::skim(earinf)\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |       |\n|:------------------------|:------|\n|Name                     |earinf |\n|Number of rows           |287    |\n|Number of columns        |6      |\n|_______________________  |       |\n|Column type frequency:   |       |\n|factor                   |4      |\n|numeric                  |2      |\n|________________________ |       |\n|Group variables          |None   |\n\n\n**Variable type: factor**\n\n|skim_variable | n_missing| complete_rate|ordered | n_unique|top_counts                 |\n|:-------------|---------:|-------------:|:-------|--------:|:--------------------------|\n|Swim          |         0|             1|FALSE   |        2|Occ: 144, Fre: 143         |\n|Loc           |         0|             1|FALSE   |        2|Bea: 147, Non: 140         |\n|Age           |         0|             1|FALSE   |        3|15-: 140, 20-: 79, 25-: 68 |\n|Sex           |         0|             1|FALSE   |        2|Mal: 188, Fem: 99          |\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate| mean|   sd| p0| p25| p50| p75| p100|hist  |\n|:-------------|---------:|-------------:|----:|----:|--:|---:|---:|---:|----:|:-----|\n|NumInfec      |         0|             1| 1.39| 2.34|  0|   0|   0|   2|   17|▇▁▁▁▁ |\n|Infec         |         0|             1| 0.47| 0.50|  0|   0|   0|   1|    1|▇▁▁▁▇ |\n\n\n:::\n:::\n\n\n\n## {{< iconify flat-color-icons workflow >}} Workflow: EDA\n\nTo Be Written Up.\n\n## {{< iconify flat-color-icons workflow >}} Workflow: Model Building\n\n::: {.panel-tabset .nav-pills style=\"background: whitesmoke;\"}\n### {{< iconify mingcute code-fill >}} Model Code\n\nTo Be Written Up.\n\n### {{< iconify mdi thinking >}} Logistic Regression Model Intuitive {#sec-lg-intuitive}\n\nTo Be Written Up.\n\n### {{< iconify material-symbols slideshow-sharp >}} Logistic Regression Models Manually Demonstrated\n\nTo Be Written Up.\n\n### {{< iconify iconoir stats-report >}} Using Other Packages {#sec--using-other-packages}\n\nTo Be Written Up.\n:::\n\n## {{< iconify flat-color-icons workflow >}} Workflow: Model Checking and Diagnostics {#sec-diagnostics}\n\n### {{< iconify ic twotone-rule >}} Checks for Uncertainty\n\nTo Be Written Up.\n\n## {{< iconify fluent-mdl2 decision-solid >}} Conclusions\n\nSo our Linear Modelling workflow might look like this: we have not seen\nall stages yet, but that is for another course module or tutorial!\n\n\n\n```{mermaid}\n%%| echo: false\nflowchart TD\n    A[(A: Data)] -->|mosaic  +  ggformula|B[B:EDA] \n    B --> |corrplot +  corrgram  + ggformula + purrr + cor.test| C(C: Check Relationships)\n    C --> D[D: Decide on Simple/Complex Model]\n    D --> E{E: Is the Model Possible?}\n    E --> |Yes| G[G: Build Model]\n    E -->|Nope| F[F: Transform Variables]\n    E -->|Nope| K[K: Try Multiple Regression <br> and/or Interaction Terms]\n    K --> D\n    F --> D\n    G --> H{H: Check Model Diagnostics}\n    H --> |Problems| D\n    H --> |All   good| I(Interpret Your Model)\n    I --> J(((Apply the Model for Predictions)))\n    \n```\n\n\n\n## {{< iconify ooui references-rtl >}} References {#sec-references}\n\n1.  Judd, Charles M. & McClelland, Gary H. & Ryan, Carey S. *Data\n    Analysis: A Model Comparison Approach to Regression, ANOVA, and\n    Beyond.* Routledge, Aug 2017. Chapter 14.\n\n2.  <https://yury-zablotski.netlify.app/post/how-logistic-regression-works/>\n\n3.  <https://uc-r.github.io/logistic_regression>\n\n4.  <https://francisbach.com/self-concordant-analysis-for-logistic-regression/>\n\n5.  <https://statmath.wu.ac.at/courses/heather_turner/glmCourse_001.pdf>\n\n6.  <https://jasp-stats.org/2022/06/30/generalized-linear-models-glm-in-jasp/>\n\n7.  P. Bingham, N.Q. Verlander, M.J. Cheal (2004). *John Snow, William\n    Farr and the 1849 outbreak of cholera that affected London: a\n    reworking of the data highlights the importance of the water\n    supply*. Public Health Volume 118, Issue 6, September 2004, Pages\n    387-394. <u>[Read the\n    PDF.](https://sci-hub.se/https://doi.org/10.1016/j.puhe.2004.05.007)</u>\n\n8.  <https://peopleanalytics-regression-book.org/bin-log-reg.html>\n\n::: {#refs style=\"font-size: 60%;\"}\n###### {{< iconify lucide package-check >}} R Package Citations\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\nPackage         Version   Citation       \n--------------  --------  ---------------\nggtext          0.1.2     @ggtext        \nGLMsData        1.4       @GLMsData      \nHSAUR3          1.0.14    @HSAUR3        \nprettyglm       1.0.1     @prettyglm     \nregressinator   0.1.3     @regressinator \n\n\n:::\n:::\n\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../../../../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../../../../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}