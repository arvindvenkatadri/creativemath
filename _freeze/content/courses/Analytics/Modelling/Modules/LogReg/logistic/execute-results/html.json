{
  "hash": "5f7edc179a02cf1b5487378bde965b84",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Modelling with Logistic Regression\"\nabstract: \"Predicting Qualitative Target Variables\"\ndate: 13/Apr/2023\ndate-modified: \"2024-03-13\"\norder: 20\nformat:\n  html:\n    html-math-method: katex\nimage: featured.png\nimage-alt: \"\"\ncategories: \n  - Logistic Regression\n  - Qualitative Variable\n  - Probability\n  - Odds\n  - Log Transformation\nbibliography: \n  - grateful-refs.bib\ncitation: true\n#suppress-bibliography: true\n---\n\n\n## {{< iconify noto-v1 package >}} Setting up R Packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(mosaic)\nlibrary(infer)\nlibrary(regressinator) # pedagogic tool for GLMs\nlibrary(GLMsData) # Datasets from Dunn and Smyth\nlibrary(HSAUR3) # Datasets from Everitt and Hothorn\nlibrary(prettyglm) # create beautiful coefficient summaries of generalised linear models.\n# remotes::install_github(\"UCLATALL/JMRData\")\n# library(JMRData)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\n## {{< iconify openmoji japanese-symbol-for-beginner >}} Introduction\n\nSometimes the dependent variable is an either/or categorization. For\nexample, the variable we want to predict might be `won` or `lost` the\ncontest, `has an ailment` or `not`, `voted` or `not` in the last\nelection, or `graduated` from college or `not`. There might even be more\nthan two categories such as voted for Congress, BJP, or Independent; or\nnever smoker, former smoker, or current smoker.\n\nWe saw with the **general linear model** that it models the **mean** of\na target *Quantitative* variable as a linear weighted sum of the\npredictor variables:\n\n$$\ny \\sim N(x_i^T * \\beta, ~~\\sigma^2)\n$$ \n\nThis model is considered to be **general** because of the dependence on\npotentially *more than one explanatory variable*, v.s. the **simple**\nlinear model:[^1] $y = \\beta_0 + \\beta_1*x_1 + \\epsilon$. The general\nlinear model gives us model \"shapes\" that start from a simple straight\nline to a *p-dimensional hyperplane*. \n\nAlthough a very useful framework, there are some situations where\ngeneral linear models are not appropriate:\n\n-   the range of Y is restricted (e.g. binary, count)\n-   the variance of Y depends on the mean (Taylor's Law)[^2]\n\nHow do we use the familiar *linear model* framework when the target/dependent variable is *Categorical*?\n\n### Linear Models for Categorical Targets?\n\nRecall that we spoke of `dummy **predictor** variables` for our linear\nmodels and how we would **dummy code** them using numerical values, such\nas 0 and 1, or +1 and -1. Could we try the same way for a **target**\ncategorical variable?\n\n$$\nY_i = \\beta_0 + \\beta_1*Xi + \\epsilon_i\\\\ \\nonumber\nwhere\\\\\n\\begin{align}\nY_i &= 0 ~ if ~ \"No\"\\\\ \\nonumber\n    &= 1 ~ if ~\"Yes\"  \\nonumber\n\\end{align}\n$$\n\nSadly this seems to not work for categorical dependent variables using a\nsimple linear model as before. Consider the Credit Card `Default` data\nfrom the package `ISLR`.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 10,000\nColumns: 4\n$ default <fct> No, No, No, No, No, No, No, No, No, No, No, No, No, No, No, No…\n$ student <fct> No, Yes, No, No, No, Yes, No, Yes, No, No, Yes, Yes, No, No, N…\n$ balance <dbl> 729.5265, 817.1804, 1073.5492, 529.2506, 785.6559, 919.5885, 8…\n$ income  <dbl> 44361.625, 12106.135, 31767.139, 35704.494, 38463.496, 7491.55…\n```\n\n\n:::\n:::\n\n\nWe see `balance` and `income` are quantitative predictors; `student` is\na qualitative predictor, and `default` is a qualitative target variable.\nIf we naively use a linear model equation as\n`model = lm(default ~ balance, data = Default)` and plot it, then...\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Naive Linear Model](logistic_files/figure-html/fig-naive-linear-model-1.png){#fig-naive-linear-model width=2100}\n:::\n:::\n\n\n...it is pretty much clear from @fig-naive-linear-model that something is\nvery odd. (no pun intended! See below!) If the only possible values for\n`default` are $No = 0$ and $Yes = 1$, how could we interpret predicted\nvalue of, say, $Y_i = 0.25$ or $Y_i = 1.55$, or perhaps $Y_i = -0.22$? \nAnything other than Yes/No is hard to interpret!\n\n### {{< iconify ic baseline-report-problem >}} {{< iconify ant-design solution-outlined >}} Problems...and Solutions\n\nWhere do we go from here?\n\nLet us state what we might desire of our model:\n\n1.  **Model Equation**: Despite this setback, we would still like\n    our model to be as close as possible to the familiar linear model\n    equation.\\\n2.  **Predictors and Weights**: We have quantitative **predictors** so\n    we still want to use a linear-weighted sum for the RHS (i.e\n    predictor side) of the model equation.\n\nAnd for the LHS (i.e the target side)?\n\n3.  **Probability**: What can we try? Even though we are interested in\n    binary outcomes, we might try to use **probability of the outcome** as our target. However, this still leaves us with a range of \\[0,1\\] for the target variable, as before. How about **odds of the outcome**, instead of trying to predict the outcomes directly (Yes or No), or their probabilities \\[0,1\\]?\n\n::: callout-note\n## Odds\n\nOdds of an event with probability `p` of occurrence is defined as\n$Odds = p/(1-p)$. As can be seen, the odds are the *ratio* of two probabilities, that of the event and its complement.\nIn the `Default` dataset just considered, the odds of default and the odds of non-default can be calculated as:\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"default\"],\"name\":[1],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"n\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"No\",\"2\":\"9667\"},{\"1\":\"Yes\",\"2\":\"333\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n$$\n\\begin{align}\nOddsDefault &=p(noDefault)/(1-p(noDefault))\\\\ \\nonumber\n            &= 0.9667/(1-0.9667)\\\\ \\nonumber\n            &= 29.0303\\\\\n\\end{align}\n$$\n\nand `OddsNoDefault` = $1/29.0303 = 0.03444709$.\n\nNow, *odds* cover the entire real number line, i.e. [$-\\infty$, $\\infty$] !\nClearly, when the probability `p` of an event is $0$, the odds are $-\\infty$...and when it nears $1$, the odds tend to $\\infty$. So we have\n**transformed** a simple probability that lies between $[0,1]$ to odds\nlying between $[-\\infty, \\infty]$. That's a great step towards making a linear model possible; we have \"removed\" the limits on our linear model's\nprediction range by using `Odds` as our target variable.\n:::\n\n4.  **Error Distributions with Odds targets**: Odds are a necessarily\n    nonlinear function of probability; the slope of `Odds ~ probability`\n    depends upon the probability itself. So a percentage change in the\n    Odds would lead to different amounts of change in the probabilities,\n    depending on the value of the probability.\n\n\n::: {#fig-odds-plot .cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![Odds](logistic_files/figure-html/fig-odds-plot-1.png){#fig-odds-plot-1 width=2100}\n:::\n\n::: {.cell-output-display}\n![Log Odds](logistic_files/figure-html/fig-odds-plot-2.png){#fig-odds-plot-2 width=2100}\n:::\n\nOdds Plot\n:::\n\n\nTo understand this issue intuitively, consider what happens to, say, a\n5% change in the odds ratio near 1.0 compared to more extreme odds\nratios, @fig-odds-plot-1 . If the odds ratio is $1.0$, then the\nprobabilities `p` and `1-p` are $0.5$, and $0.5$. A 20% increase in the\nodds ratio to $1.20$ would correspond to probabilities of $0.545$ and\n$0.455$. However, if the original probabilities were $0.9$ and $0.1$ for\nan odds ratio $9$, then a 20% increase to $10.8$ would correspond to\nprobabilities of $0.915$ and $0.085$, a much smaller change in the\nprobabilities. Hence, extreme probabilities (near 1 or 0) are more\nstable (i.e., have less error) than middle probabilities.\n\nThis should remind us of the [**LINE** assumptions in linear\nregression](../LinReg/LinReg.qmd#sec-assumptions-in-linear-models) where we assume that the errors (in prediction) are normally distributed with common variance, across different values of the independent/predictor variable. The solution to this *heteroscedasticity* problem is similar here to what we discussed there: the *log transformation*, @fig-odds-plot-2. This transformation works because it makes the same percentage changes equivalent no matter what the starting value of the odds or the probabilities; logs convert multipliers/divisors to sums/differences and the graph is linear for the most part.\n\nSo in our model, instead of modeling *odds* as the dependent variable,\nwe will use $log(odds)$, also known as the **logit**, defined as:\n\n$$\n\\begin{align}\nlog(odds_i) &= log[(p_i)/(1-p_i)]\\\\ \\nonumber\n            &= logit(p_i)\\\\ \n\\end{align}\n$$\n\n::: callout-note\n### Binomially distributed target variable\n\nIn linear regression, we assume a normally distributed target variable.\nWith a categorical target variable with two levels $0$ and $1$ it would be impossible for the errors $e_i = Y_i - \\hat{Y_i}$ to have a *normal distribution*, as assumed for the statistical tests to be valid. The\nerrors are bounded by \\[0,1\\]!\nOne candidate for the error distribution in this case is the *binomial distribution*, whose mean and variance are `p` and `np(1-p)` respectively. Note immediately that the variance moves with the mean! So the model has \"built-in\" heteroscedasticity, which we need to counter with transformations such as the $log()$ function. More on this later.\n:::\n\n5.  **Estimation of Model Parameters**: The last problem to solve is\n    that because we have made so many transformations to get to the\n    `logits` that we want to model, the logic of minimizing the **sum of squared errors(SSE)** is no longer appropriate.\n    \n::: callout-note\nThe probabilities for `default` are $0$ and $1$...the `log(odds)` will map respectively to $-\\infty$ and $\\infty$. So if we naively try to take residuals, we will find that they are **all** $\\infty$ !! Hence $SSE$ cannot be computed and we need another way to assess the quality of our model. \n:::\n\nInstead, we will have to use **maximum likelihood estimation(MLE)** to estimate the models, and we will use the $X^2$ (\"chi-squared\") statistic instead of `t` and `F`to evaluate the model comparisons. The *maximum likelihood method* maximizes the probability of obtaining the data at hand against every choice of model parameters $\\beta_i$. \n    \nThis is our **Logistic Regression Model**, which uses a Quantitative\nPredictor variable to predict a Categorical target variable. We write the model as ( for the `Default` dataset:\n\n$$\n\\begin{align}\nlogit(default) & = \\beta_0 + \\beta_1 * balance&\\\\  \\nonumber\nlog(p(default)/(1-p(default))) & = \\beta_0+\\beta_1 * balance&\\\\ \\nonumber\ntherefore\\\\\np(default) & = \\frac{exp(\\beta_0 + \\beta_1 * balance)}{1 + exp(\\beta_0 + \\beta_1 * balance)}\\\\\n\\end{align}\n$$\n\nFrom the Eqn.4 above it should be clear that a *unit increase* in `balance` should increase the odds of `default` by $\\beta_1$ units. \n\nThe RHS of Eqn.5 is a *sigmoid* function of the weighted sum of predictors and is limited to the range [0,1]. The parameters $\\beta_i$ need to be estimated using maximum likelihood methods. \n\n\n::: {#fig-model-plots .cell .column-screen-inset-right layout-ncol=\"3\"}\n::: {.cell-output-display}\n![naive linear regression model](logistic_files/figure-html/fig-model-plots-1.png){#fig-model-plots-1 width=2100}\n:::\n\n::: {.cell-output-display}\n![logistic regression model](logistic_files/figure-html/fig-model-plots-2.png){#fig-model-plots-2 width=2100}\n:::\n\n::: {.cell-output-display}\n![log odds gives linear models](logistic_files/figure-html/fig-model-plots-3.png){#fig-model-plots-3 width=2100}\n:::\n\nModel Plots\n:::\n\n\n\n## {{< iconify simple-icons hypothesis >}} Logistic Regression Models as Hypothesis Tests   \nTo Be Written Up.\n\n::: {.content-hidden when-format=\"html\"}\n## {{< iconify tdesign function-curve >}} Generalized Linear Model\n\n::: callout-important\nA **generalized linear model** is made up of a linear predictor:\n\n$$\n\\eta_i = \\beta_0 + \\beta_1x_{1i} + ... + \\beta_px_{pi}\n$$\n\nand two functions:\n\n-   a link function that describes how the mean, $E(Y_i) = \\mu_i$,\n    depends on the linear predictor:\\\n\n    $$\n    g(\\mu_i) = \\eta_i\n    $$\n\n-   a variance function that describes how the variance, $var(Y_i)$\n    depends on the mean:\\\n\n    $$\n    var(Y_i) = \\Phi*V(\\mu_i)\n    $$\n\nwhere the dispersion parameter $\\Phi$ is a constant.\n:::\n\nFor example we can obtain our *general linear model* with the following\nchoice:\n\n$$\n\\begin{align}\n& g(\\mu_i) = \\mu_i\\\\\n& Phi = 1\n\\end{align}\n$$\n\nIf now we assume that the *target* variable $Y_i$ is a **binomial**,\ni.e. a two-valued variable:\\\n\n$$\n\\begin{align}\n & Y_i = binom(n_i,p_i)\\\\\n & mean(Y_i) = n_ip_i\\\\\n & var(Y_i) = n_ip_i(1-p_i)\n\\end{align}\n$$\n\nNow, we wish to model the **proportions** $Y_i/n_i$, as our **target**.\nThen we can state that:\\\n\n$$\n\\begin{align}\nmean(Y_i/n_i) = p_i \\coloneqq \\mu_i\\\\\nvar(Y_i/n_i) = var(Y_i)/n_i^2 = \\frac{p_i(1-p_i)}{n_i} \\coloneqq \\sigma_i^2\\\\\n\\end{align}\n$$\n\nInspecting the above, we can write:\n\n$$\n\\sigma_i^2 = \\frac{\\mu_i(1-\\mu_i)}{n_i}\n$$\n\nand since the link function needs to map ${[-\\infty, \\infty]}$ to\n${[0,1]}$, we use the `logit` function:\n\n$$\ng(\\mu_i) = logit(\\mu_i) = log(\\frac{\\mu_i}{1-\\mu_i})\n$$\n:::\n\n\n\n## {{< iconify flat-color-icons workflow >}} Workflow: Read the Data\n\n\nLet us now read in the data and check for these assumptions as part of\nour Workflow.\n\n::: callout-note\n## Research Question\n\nTo Be Written Up. \n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"earinf\", package = \"GLMsData\")\ninspect(earinf)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\ncategorical variables:  \n  name  class levels   n missing                                  distribution\n1 Swim factor      2 287       0 Occas (50.2%), Freq (49.8%)                  \n2  Loc factor      2 287       0 Beach (51.2%), NonBeach (48.8%)              \n3  Age factor      3 287       0 15-19 (48.8%), 20-24 (27.5%) ...             \n4  Sex factor      2 287       0 Male (65.5%), Female (34.5%)                 \n\nquantitative variables:  \n      name   class min Q1 median Q3 max      mean        sd   n missing\n1 NumInfec integer   0  0      0  2  17 1.3867596 2.3385412 287       0\n2    Infec integer   0  0      0  1   1 0.4738676 0.5001888 287       0\n```\n\n\n:::\n\n```{.r .cell-code}\nglimpse(earinf)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 287\nColumns: 6\n$ Swim     <fct> Occas, Occas, Occas, Occas, Occas, Occas, Occas, Occas, Occas…\n$ Loc      <fct> NonBeach, NonBeach, NonBeach, NonBeach, NonBeach, NonBeach, N…\n$ Age      <fct> 15-19, 15-19, 15-19, 15-19, 15-19, 15-19, 15-19, 15-19, 15-19…\n$ Sex      <fct> Male, Male, Male, Male, Male, Male, Male, Male, Male, Male, M…\n$ NumInfec <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2…\n$ Infec    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n```\n\n\n:::\n\n```{.r .cell-code}\nskimr::skim(earinf)\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |       |\n|:------------------------|:------|\n|Name                     |earinf |\n|Number of rows           |287    |\n|Number of columns        |6      |\n|_______________________  |       |\n|Column type frequency:   |       |\n|factor                   |4      |\n|numeric                  |2      |\n|________________________ |       |\n|Group variables          |None   |\n\n\n**Variable type: factor**\n\n|skim_variable | n_missing| complete_rate|ordered | n_unique|top_counts                 |\n|:-------------|---------:|-------------:|:-------|--------:|:--------------------------|\n|Swim          |         0|             1|FALSE   |        2|Occ: 144, Fre: 143         |\n|Loc           |         0|             1|FALSE   |        2|Bea: 147, Non: 140         |\n|Age           |         0|             1|FALSE   |        3|15-: 140, 20-: 79, 25-: 68 |\n|Sex           |         0|             1|FALSE   |        2|Mal: 188, Fem: 99          |\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate| mean|   sd| p0| p25| p50| p75| p100|hist  |\n|:-------------|---------:|-------------:|----:|----:|--:|---:|---:|---:|----:|:-----|\n|NumInfec      |         0|             1| 1.39| 2.34|  0|   0|   0|   2|   17|▇▁▁▁▁ |\n|Infec         |         0|             1| 0.47| 0.50|  0|   0|   0|   1|    1|▇▁▁▁▇ |\n\n\n:::\n:::\n\n\n## {{< iconify flat-color-icons workflow >}} Workflow: EDA\nTo Be Written Up.\n\n\n## {{< iconify flat-color-icons workflow >}} Workflow: Model Building\n\n:::{.panel-tabset .nav-pills style=\"background: whitesmoke;\"} \n\n### {{< iconify mingcute code-fill >}} Model Code\nTo Be Written Up.\n\n### {{< iconify mdi thinking >}} Logistic Regression Model Intuitive {#sec-lg-intuitive}\nTo Be Written Up.\n\n### {{< iconify material-symbols slideshow-sharp >}} Logistic Regression Models Manually Demonstrated\nTo Be Written Up.\n\n### {{< iconify iconoir stats-report >}} Using Other Packages {#sec--using-other-packages}\nTo Be Written Up.\n:::\n\n\n## {{< iconify flat-color-icons workflow >}} Workflow: Model Checking and Diagnostics {#sec-diagnostics}\n\n### {{< iconify ic twotone-rule >}} Checks for Uncertainty\nTo Be Written Up.\n\n\n## {{< iconify fluent-mdl2 decision-solid >}} Conclusions\n\nSo our Linear Modelling workflow might look like this: we have not seen\nall stages yet, but that is for another course module or tutorial!\n\n\n```{mermaid}\n%%| echo: false\nflowchart TD\n    A[(A: Data)] -->|mosaic  +  ggformula|B[B:EDA] \n    B --> |corrplot +  corrgram  + ggformula + purrr + cor.test| C(C: Check Relationships)\n    C --> D[D: Decide on Simple/Complex Model]\n    D --> E{E: Is the Model Possible?}\n    E --> |Yes| G[G: Build Model]\n    E -->|Nope| F[F: Transform Variables]\n    E -->|Nope| K[K: Try Multiple Regression <br> and/or Interaction Terms]\n    K --> D\n    F --> D\n    G --> H{H: Check Model Diagnostics}\n    H --> |Problems| D\n    H --> |All   good| I(Interpret Your Model)\n    I --> J(((Apply the Model for Predictions)))\n    \n```\n\n\n## {{< iconify ooui references-rtl >}} References {#sec-references}\n\n1. Judd, Charles M. & McClelland, Gary H. & Ryan, Carey S. *Data\n    Analysis: A Model Comparison Approach to Regression, ANOVA, and\n    Beyond.* Routledge, Aug 2017. Chapter 14.\n    \n1. <https://yury-zablotski.netlify.app/post/how-logistic-regression-works/>\n1. <https://uc-r.github.io/logistic_regression>\n\n1. <https://francisbach.com/self-concordant-analysis-for-logistic-regression/>\n\n1. <https://statmath.wu.ac.at/courses/heather_turner/glmCourse_001.pdf>\n\n1. <https://jasp-stats.org/2022/06/30/generalized-linear-models-glm-in-jasp/>\n\n1. P. Bingham, N.Q. Verlander, M.J. Cheal (2004). *John Snow, William Farr and the 1849 outbreak of cholera that affected London: a reworking of the data highlights the importance of the water supply*. Public Health\nVolume 118, Issue 6, September 2004, Pages 387-394. <u>[Read the PDF.](https://sci-hub.se/https://doi.org/10.1016/j.puhe.2004.05.007)</u>\n\n::: {#refs style=\"font-size: 60%;\"}\n###### {{< iconify lucide package-check >}} R Package Citations\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\nPackage         Version   Citation       \n--------------  --------  ---------------\nggtext          0.1.2     @ggtext        \nGLMsData        1.4       @GLMsData      \nHSAUR3          1.0.14    @HSAUR3        \nprettyglm       1.0.1     @prettyglm     \nregressinator   0.1.3     @regressinator \n\n\n:::\n:::\n\n\n\n\n\n\n\n:::\n\n\n[^1]: <https://statmath.wu.ac.at/courses/heather_turner/glmCourse_001.pdf>\n\n[^2]: <https://en.wikipedia.org/wiki/Taylor%27s_law>\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../../../../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../../../../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}