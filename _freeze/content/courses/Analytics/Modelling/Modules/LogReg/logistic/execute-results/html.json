{
  "hash": "f5bfb83c1fc2a1d5368afbd935a26b19",
  "result": {
    "markdown": "---\ntitle: \"Modelling Logistic Regression\"\nauthor: \"Arvind Venkatadri\"\ndate: 13/Apr/2023\ndate-modified: \"2023-04-13\"\norder: 20\nimage: preview.png\nimage-alt: By Rudityas on Glazestock.com\ncategories: \n  - Logistic Regression\n  - Qualitative Variable\n  - Probability\nfilters:\n  - nutshell\n---\n\n::: {.cell hash='logistic_cache/html/setup_3287d339a6dd18dd76c6dd3e9f28247f'}\n\n```{.r .cell-code}\nknitr::opts_chunk$set(echo = TRUE,warning = FALSE,message = FALSE)\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(mosaic)\nlibrary(infer)\n```\n:::\n\n::: {.cell hash='logistic_cache/html/unnamed-chunk-2_ce494021beacd500194c456b0db61ec0'}\n<style type=\"text/css\">\n\n/*https://stackoverflow.com/questions/73546631/how-to-format-tabset-font-in-quarto\n*/\n\n/*\n.panel-tabset .nav-item {\n   font-size: 20px;\n   font-style: italic\n    }\n*/\n</style>\n:::\n\n\n## Introduction\n\nFollowing the ANOVA tutorial at [Our Coding\nClub](https://ourcodingclub.github.io/tutorials/anova/).\n\nSuppose we have three sales strategies on our website, to sell a certain\nproduct, say men's shirts. We have website interaction over several\nmonths. How do we know which strategy makes people buy the fastest ?\n\nIf there is a University course that is offered in parallel in three\ndifferent classrooms, is there a difference between the average marks\nobtained by students in each of the classrooms?\n\nIn each case we have a set of observations in each category: Interaction\nTime vs Sales Strategy in the first example, and Student Marks vs\nClassroom in the second. We can take *mean* scores in each category and\ndecide to compare them. How do we make the comparisons? One way would be\nto compare them pair-wise. But with this rapidly becomes intractable and\nalso dangerous: with increasing number of `groups`, the number of\nmean-comparisons becomes very large $N\\choose 2$ and with each\ncomparison the possibility of some difference showing up, *just by\nchance*, increases! And we end up making the wrong inference and perhaps\nthe wrong decision.\n\nThe trick is of course to make comparisons **all at once** and ANOVA is\nthe technique that allows us to do just that. In this tutorial, we will\ncompare the Hatching Time of frog spawn[^1], at three different lab\ntemperatures.\n\nIn this tutorial, our research question is:\n\n::: callout-note\n## Research Question\n\nHow does frogspawn hatching time vary with temperature?\n:::\n\n### Read the Data\n\nDownload the data by clicking the button below:\n\n\n{{< downloadthis data/frogs.csv dname=\"frogs\" label=\"Download the frogs data\" icon=\"database-fill-down\" type=\"info\" >}}\n\n\n::: {.cell hash='logistic_cache/html/unnamed-chunk-3_674276e03f54e67c8106635b3b9c1719'}\n\n```{.r .cell-code}\nfrogs_orig <- read_csv(\"data/frogs.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 60 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): Frogspawn sample id, Temperature13, Temperature18, Temperature25\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nfrogs_orig\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"Frogspawn sample id\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Temperature13\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Temperature18\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Temperature25\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1\",\"2\":\"24\",\"3\":\"NA\",\"4\":\"NA\"},{\"1\":\"2\",\"2\":\"NA\",\"3\":\"21\",\"4\":\"NA\"},{\"1\":\"3\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"18\"},{\"1\":\"4\",\"2\":\"26\",\"3\":\"NA\",\"4\":\"NA\"},{\"1\":\"5\",\"2\":\"NA\",\"3\":\"22\",\"4\":\"NA\"},{\"1\":\"6\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"14\"},{\"1\":\"7\",\"2\":\"27\",\"3\":\"NA\",\"4\":\"NA\"},{\"1\":\"8\",\"2\":\"NA\",\"3\":\"22\",\"4\":\"NA\"},{\"1\":\"9\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"15\"},{\"1\":\"10\",\"2\":\"27\",\"3\":\"NA\",\"4\":\"NA\"},{\"1\":\"11\",\"2\":\"NA\",\"3\":\"21\",\"4\":\"NA\"},{\"1\":\"12\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"15\"},{\"1\":\"13\",\"2\":\"26\",\"3\":\"NA\",\"4\":\"NA\"},{\"1\":\"14\",\"2\":\"NA\",\"3\":\"20\",\"4\":\"NA\"},{\"1\":\"15\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"16\"},{\"1\":\"16\",\"2\":\"27\",\"3\":\"NA\",\"4\":\"NA\"},{\"1\":\"17\",\"2\":\"NA\",\"3\":\"20\",\"4\":\"NA\"},{\"1\":\"18\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"16\"},{\"1\":\"19\",\"2\":\"27\",\"3\":\"NA\",\"4\":\"NA\"},{\"1\":\"20\",\"2\":\"NA\",\"3\":\"19\",\"4\":\"NA\"},{\"1\":\"21\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"17\"},{\"1\":\"22\",\"2\":\"25\",\"3\":\"NA\",\"4\":\"NA\"},{\"1\":\"23\",\"2\":\"NA\",\"3\":\"21\",\"4\":\"NA\"},{\"1\":\"24\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"17\"},{\"1\":\"25\",\"2\":\"26\",\"3\":\"NA\",\"4\":\"NA\"},{\"1\":\"26\",\"2\":\"NA\",\"3\":\"23\",\"4\":\"NA\"},{\"1\":\"27\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"17\"},{\"1\":\"28\",\"2\":\"28\",\"3\":\"NA\",\"4\":\"NA\"},{\"1\":\"29\",\"2\":\"NA\",\"3\":\"21\",\"4\":\"NA\"},{\"1\":\"30\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"17\"},{\"1\":\"31\",\"2\":\"24\",\"3\":\"NA\",\"4\":\"NA\"},{\"1\":\"32\",\"2\":\"NA\",\"3\":\"21\",\"4\":\"NA\"},{\"1\":\"33\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"18\"},{\"1\":\"34\",\"2\":\"26\",\"3\":\"NA\",\"4\":\"NA\"},{\"1\":\"35\",\"2\":\"NA\",\"3\":\"22\",\"4\":\"NA\"},{\"1\":\"36\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"14\"},{\"1\":\"37\",\"2\":\"27\",\"3\":\"NA\",\"4\":\"NA\"},{\"1\":\"38\",\"2\":\"NA\",\"3\":\"22\",\"4\":\"NA\"},{\"1\":\"39\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"15\"},{\"1\":\"40\",\"2\":\"27\",\"3\":\"NA\",\"4\":\"NA\"},{\"1\":\"41\",\"2\":\"NA\",\"3\":\"21\",\"4\":\"NA\"},{\"1\":\"42\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"15\"},{\"1\":\"43\",\"2\":\"26\",\"3\":\"NA\",\"4\":\"NA\"},{\"1\":\"44\",\"2\":\"NA\",\"3\":\"20\",\"4\":\"NA\"},{\"1\":\"45\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"16\"},{\"1\":\"46\",\"2\":\"27\",\"3\":\"NA\",\"4\":\"NA\"},{\"1\":\"47\",\"2\":\"NA\",\"3\":\"20\",\"4\":\"NA\"},{\"1\":\"48\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"16\"},{\"1\":\"49\",\"2\":\"27\",\"3\":\"NA\",\"4\":\"NA\"},{\"1\":\"50\",\"2\":\"NA\",\"3\":\"19\",\"4\":\"NA\"},{\"1\":\"51\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"17\"},{\"1\":\"52\",\"2\":\"25\",\"3\":\"NA\",\"4\":\"NA\"},{\"1\":\"53\",\"2\":\"NA\",\"3\":\"21\",\"4\":\"NA\"},{\"1\":\"54\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"17\"},{\"1\":\"55\",\"2\":\"26\",\"3\":\"NA\",\"4\":\"NA\"},{\"1\":\"56\",\"2\":\"NA\",\"3\":\"23\",\"4\":\"NA\"},{\"1\":\"57\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"17\"},{\"1\":\"58\",\"2\":\"28\",\"3\":\"NA\",\"4\":\"NA\"},{\"1\":\"59\",\"2\":\"NA\",\"3\":\"21\",\"4\":\"NA\"},{\"1\":\"60\",\"2\":\"NA\",\"3\":\"NA\",\"4\":\"17\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nOur response variable is the hatching `Time`. Our explanatory variable\nis a *factor*, Temperature, with 3 levels: 13°C, 18°C and 25°C.\nDifferent samples of spawn were subject to each of these temperatures\nrespectively. The data is badly organized, with a separate column for\neach Temperature, with NA entries since not all samples of spawn could\nbe subject to all temperatures. Hence, we should `pivot_longer()` this\ndata, so all Time readings are in one column, and also convert the\nTemperature into a `factor`:\n\n\n::: {.cell hash='logistic_cache/html/unnamed-chunk-4_033f5a211e6b318524298376f3c1b47c'}\n\n```{.r .cell-code}\nfrogs_long <- frogs_orig %>% \n  pivot_longer(., cols = starts_with(\"Temperature\"),\n               cols_vary = \"fastest\", # new in pivot_longer\n               names_to = \"Temp\",\n               values_to = \"Time\") %>% \n  drop_na() %>% \n  \n  # knock off the unnecessary \"Temperature\" word everywhere\n  separate_wider_regex(cols = Temp,\n                       patterns = c(\"Temperature\", \n                                     TempFac = \"\\\\d+\"), \n                       cols_remove = TRUE) %>% \n  \n  # Convert Temp into TempFac, a 3-level factor\n  mutate(TempFac = factor(x = TempFac,\n                              levels = c(13,18,25), \n                              labels = c(\"13\", \"18\", \"25\"))) %>% \n  rename(\"Id\" = `Frogspawn sample id`)\n\nfrogs_long\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"Id\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"TempFac\"],\"name\":[2],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Time\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1\",\"2\":\"13\",\"3\":\"24\"},{\"1\":\"2\",\"2\":\"18\",\"3\":\"21\"},{\"1\":\"3\",\"2\":\"25\",\"3\":\"18\"},{\"1\":\"4\",\"2\":\"13\",\"3\":\"26\"},{\"1\":\"5\",\"2\":\"18\",\"3\":\"22\"},{\"1\":\"6\",\"2\":\"25\",\"3\":\"14\"},{\"1\":\"7\",\"2\":\"13\",\"3\":\"27\"},{\"1\":\"8\",\"2\":\"18\",\"3\":\"22\"},{\"1\":\"9\",\"2\":\"25\",\"3\":\"15\"},{\"1\":\"10\",\"2\":\"13\",\"3\":\"27\"},{\"1\":\"11\",\"2\":\"18\",\"3\":\"21\"},{\"1\":\"12\",\"2\":\"25\",\"3\":\"15\"},{\"1\":\"13\",\"2\":\"13\",\"3\":\"26\"},{\"1\":\"14\",\"2\":\"18\",\"3\":\"20\"},{\"1\":\"15\",\"2\":\"25\",\"3\":\"16\"},{\"1\":\"16\",\"2\":\"13\",\"3\":\"27\"},{\"1\":\"17\",\"2\":\"18\",\"3\":\"20\"},{\"1\":\"18\",\"2\":\"25\",\"3\":\"16\"},{\"1\":\"19\",\"2\":\"13\",\"3\":\"27\"},{\"1\":\"20\",\"2\":\"18\",\"3\":\"19\"},{\"1\":\"21\",\"2\":\"25\",\"3\":\"17\"},{\"1\":\"22\",\"2\":\"13\",\"3\":\"25\"},{\"1\":\"23\",\"2\":\"18\",\"3\":\"21\"},{\"1\":\"24\",\"2\":\"25\",\"3\":\"17\"},{\"1\":\"25\",\"2\":\"13\",\"3\":\"26\"},{\"1\":\"26\",\"2\":\"18\",\"3\":\"23\"},{\"1\":\"27\",\"2\":\"25\",\"3\":\"17\"},{\"1\":\"28\",\"2\":\"13\",\"3\":\"28\"},{\"1\":\"29\",\"2\":\"18\",\"3\":\"21\"},{\"1\":\"30\",\"2\":\"25\",\"3\":\"17\"},{\"1\":\"31\",\"2\":\"13\",\"3\":\"24\"},{\"1\":\"32\",\"2\":\"18\",\"3\":\"21\"},{\"1\":\"33\",\"2\":\"25\",\"3\":\"18\"},{\"1\":\"34\",\"2\":\"13\",\"3\":\"26\"},{\"1\":\"35\",\"2\":\"18\",\"3\":\"22\"},{\"1\":\"36\",\"2\":\"25\",\"3\":\"14\"},{\"1\":\"37\",\"2\":\"13\",\"3\":\"27\"},{\"1\":\"38\",\"2\":\"18\",\"3\":\"22\"},{\"1\":\"39\",\"2\":\"25\",\"3\":\"15\"},{\"1\":\"40\",\"2\":\"13\",\"3\":\"27\"},{\"1\":\"41\",\"2\":\"18\",\"3\":\"21\"},{\"1\":\"42\",\"2\":\"25\",\"3\":\"15\"},{\"1\":\"43\",\"2\":\"13\",\"3\":\"26\"},{\"1\":\"44\",\"2\":\"18\",\"3\":\"20\"},{\"1\":\"45\",\"2\":\"25\",\"3\":\"16\"},{\"1\":\"46\",\"2\":\"13\",\"3\":\"27\"},{\"1\":\"47\",\"2\":\"18\",\"3\":\"20\"},{\"1\":\"48\",\"2\":\"25\",\"3\":\"16\"},{\"1\":\"49\",\"2\":\"13\",\"3\":\"27\"},{\"1\":\"50\",\"2\":\"18\",\"3\":\"19\"},{\"1\":\"51\",\"2\":\"25\",\"3\":\"17\"},{\"1\":\"52\",\"2\":\"13\",\"3\":\"25\"},{\"1\":\"53\",\"2\":\"18\",\"3\":\"21\"},{\"1\":\"54\",\"2\":\"25\",\"3\":\"17\"},{\"1\":\"55\",\"2\":\"13\",\"3\":\"26\"},{\"1\":\"56\",\"2\":\"18\",\"3\":\"23\"},{\"1\":\"57\",\"2\":\"25\",\"3\":\"17\"},{\"1\":\"58\",\"2\":\"13\",\"3\":\"28\"},{\"1\":\"59\",\"2\":\"18\",\"3\":\"21\"},{\"1\":\"60\",\"2\":\"25\",\"3\":\"17\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n::: {.cell hash='logistic_cache/html/unnamed-chunk-5_376f76cb50cf683ad2e03a0fa2f6d827'}\n\n```{.r .cell-code}\nfrogs_long %>% count(TempFac)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"TempFac\"],\"name\":[1],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"n\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"13\",\"2\":\"20\"},{\"1\":\"18\",\"2\":\"20\"},{\"1\":\"25\",\"2\":\"20\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nSo we have `20` samples for Hatching `Time` per `TempFac` setting.\n\n## EDA\n\nLet us set a plot theme:\n\n\n::: {.cell hash='logistic_cache/html/unnamed-chunk-6_87acf8044efd188789e78a814e6fa0c8'}\n\n```{.r .cell-code}\n# Data visualisation\n\ntheme_frogs <- function(){  # Creating a function\n  theme_classic() +  # Using pre-defined theme as base\n  theme(axis.text.x = element_text(size = 12, face = \"bold\"),  # Customizing axes text      \n        axis.text.y = element_text(size = 12, face = \"bold\"),\n        axis.title = element_text(size = 14, face = \"bold\"),  # Customizing axis title\n        panel.grid = element_blank(),  # Taking off the default grid\n        plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), units = , \"cm\"),\n        legend.text = element_text(size = 12, face = \"italic\"),  # Customizing legend text\n        legend.title = element_text(size = 12, face = \"bold\"),  # Customizing legend title\n        legend.position = \"right\",  # Customizing legend position\n        plot.caption = element_text(size = 12))  # Customizing plot caption\n}                                                                              \n```\n:::\n\n\nLet us plot some histograms of Hatching Time:\n\n\n::: {.cell hash='logistic_cache/html/unnamed-chunk-7_cd5c811cef38c891a0af61e4d0264203'}\n\n```{.r .cell-code}\ngf_histogram(data = frogs_long, \n             ~ Time, \n             fill = ~ TempFac,\n             stat = \"count\") %>% \n  gf_vline(xintercept = ~ mean(Time)) %>% \n  gf_labs(x = \"Hatching Time\") %>% \n  gf_theme(theme = theme_frogs()) %>% \n  gf_theme(guides(fill = guide_legend(title = \"Temperature level (°C)\")))\n```\n\n::: {.cell-output-display}\n![](logistic_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nWe should also look at boxplots\n\n\n::: {.cell hash='logistic_cache/html/unnamed-chunk-8_ffe10a435a93a2ecf6166a3b0cb61295'}\n\n```{.r .cell-code}\ngf_boxplot(data = frogs_long, \n             Time ~ TempFac, \n             fill = ~ TempFac) %>% \n  gf_vline(xintercept = ~ mean(Time)) %>% \n  gf_labs(x = \"Hatching Time\") %>% \n  gf_theme(theme = theme_frogs()) %>% \n  gf_theme(guides(fill = guide_legend(title = \"Temperature level (°C)\")))\n```\n\n::: {.cell-output-display}\n![](logistic_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n## ANOVA\n\nWe will first execute the ANOVA test with code and evaluate the results.\nThen we will do an intuitive walk through of the process and finally,\nhand-calculate entire analysis for clear understanding.\n\n::: panel-tabset\n### ANOVA Test with Code\n\nR offers a very simple command to execute an ANOVA test: Note the\nfamiliar `formula` of stating the variables:\n\n\n::: {.cell hash='logistic_cache/html/ANOVA with code_ad17c84271b6fc4a9e3a448f6d81079f'}\n\n```{.r .cell-code}\nfrogs_anova <- aov(Time ~ TempFac, data = frogs_long)\nfrogs_anova %>% broom::tidy()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"term\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"df\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"sumsq\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"meansq\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"statistic\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"p.value\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"TempFac\",\"2\":\"2\",\"3\":\"1020.933\",\"4\":\"510.466667\",\"5\":\"385.8966\",\"6\":\"7.357304e-34\"},{\"1\":\"Residuals\",\"2\":\"57\",\"3\":\"75.400\",\"4\":\"1.322807\",\"5\":\"NA\",\"6\":\"NA\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\nsummary(frogs_anova)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Df Sum Sq Mean Sq F value Pr(>F)    \nTempFac      2 1020.9   510.5   385.9 <2e-16 ***\nResiduals   57   75.4     1.3                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nThe effect of Temperature on Hatching time is significant, with a\np-value of $<2e-16$. The F-statistic for the ANOVA test is given by\n$385.9$, which is very high. Clearly `Temperature` has a very\nsignificant effect on the hatching `Time`.\n\nTo find which specific value of `TempFac` has the most effect will\nrequire *pairwise comparison* of the group means, using a standard\nt-test. The confidence level for such repeated comparisons will need\nwhat is called **Bonferroni correction**[^2] to prevent us from\ndetecting a significant (pair-wise) difference simply by chance. To do\nthis we take $\\alpha$ the confidence level used and divide it by $K$,\nthe number of pair-wise comparisons we intend to make. So using an\n$\\alpha = 0.05$ for ANOVA, the pairwise comparisons in our current data\nwill have to use $\\alpha/3 = 0.0166$ as the confidence level. We will\ndiscuss this more in the section titled \"ANOVA Intuitive\".\n\n### ANOVA Intuitive {#sec-anova-intuitive}\n\nAll that is very well, but what is happening under the hood of the\n`aov()` command?\n\nConsider a data set with a single Quant and a single Qual variable. The\nQual variable has two levels, the Quant data has 20 observations per\nQual level.\n\n\n::: {.cell hash='logistic_cache/html/unnamed-chunk-10_6601551f345158ed2e9f0d9c689fe1aa'}\n\n```{.r .cell-code}\nlibrary(patchwork)\nggplot2::theme_set(theme_classic())\ndata = tibble(index = 1:40,\n              qual = c(rep(x = \"A\", 20),rep(x = \"B\", 20)),\n              quant = c(rnorm(n = 20, mean = 0, sd = 2), \n                        rnorm(n = 20, mean = 10, sd = 2)))\ndata\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"index\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"qual\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"quant\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1\",\"2\":\"A\",\"3\":\"1.5031194\"},{\"1\":\"2\",\"2\":\"A\",\"3\":\"-1.7766906\"},{\"1\":\"3\",\"2\":\"A\",\"3\":\"-1.3825398\"},{\"1\":\"4\",\"2\":\"A\",\"3\":\"-0.3468375\"},{\"1\":\"5\",\"2\":\"A\",\"3\":\"0.3625619\"},{\"1\":\"6\",\"2\":\"A\",\"3\":\"0.1951818\"},{\"1\":\"7\",\"2\":\"A\",\"3\":\"-0.6462781\"},{\"1\":\"8\",\"2\":\"A\",\"3\":\"-0.7925901\"},{\"1\":\"9\",\"2\":\"A\",\"3\":\"-1.1541974\"},{\"1\":\"10\",\"2\":\"A\",\"3\":\"2.6323013\"},{\"1\":\"11\",\"2\":\"A\",\"3\":\"1.0637340\"},{\"1\":\"12\",\"2\":\"A\",\"3\":\"0.3768169\"},{\"1\":\"13\",\"2\":\"A\",\"3\":\"1.0516894\"},{\"1\":\"14\",\"2\":\"A\",\"3\":\"-1.7612288\"},{\"1\":\"15\",\"2\":\"A\",\"3\":\"-1.1904746\"},{\"1\":\"16\",\"2\":\"A\",\"3\":\"2.8020170\"},{\"1\":\"17\",\"2\":\"A\",\"3\":\"-2.8128426\"},{\"1\":\"18\",\"2\":\"A\",\"3\":\"-1.2232292\"},{\"1\":\"19\",\"2\":\"A\",\"3\":\"-3.7628075\"},{\"1\":\"20\",\"2\":\"A\",\"3\":\"-0.4961231\"},{\"1\":\"21\",\"2\":\"B\",\"3\":\"11.7917737\"},{\"1\":\"22\",\"2\":\"B\",\"3\":\"8.0229819\"},{\"1\":\"23\",\"2\":\"B\",\"3\":\"8.2046096\"},{\"1\":\"24\",\"2\":\"B\",\"3\":\"9.3392437\"},{\"1\":\"25\",\"2\":\"B\",\"3\":\"8.8161561\"},{\"1\":\"26\",\"2\":\"B\",\"3\":\"12.7288097\"},{\"1\":\"27\",\"2\":\"B\",\"3\":\"10.8695861\"},{\"1\":\"28\",\"2\":\"B\",\"3\":\"8.3597275\"},{\"1\":\"29\",\"2\":\"B\",\"3\":\"9.9294077\"},{\"1\":\"30\",\"2\":\"B\",\"3\":\"8.1061924\"},{\"1\":\"31\",\"2\":\"B\",\"3\":\"14.4843940\"},{\"1\":\"32\",\"2\":\"B\",\"3\":\"12.1203565\"},{\"1\":\"33\",\"2\":\"B\",\"3\":\"12.2032339\"},{\"1\":\"34\",\"2\":\"B\",\"3\":\"12.6546263\"},{\"1\":\"35\",\"2\":\"B\",\"3\":\"11.8010631\"},{\"1\":\"36\",\"2\":\"B\",\"3\":\"11.0093019\"},{\"1\":\"37\",\"2\":\"B\",\"3\":\"8.6862196\"},{\"1\":\"38\",\"2\":\"B\",\"3\":\"6.1641772\"},{\"1\":\"39\",\"2\":\"B\",\"3\":\"10.2772063\"},{\"1\":\"40\",\"2\":\"B\",\"3\":\"9.7397888\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\noverall_mean <- data %>%\n             summarise(overall_mean = mean(quant))\n#overall_mean\n\ngrouped_means <- data %>%\n  group_by(qual) %>% \n             summarise(grouped_means = mean(quant))\n#grouped_means\n\np1 <- gf_point(quant ~ index, \n               color = ~ qual,\n         data = data) %>% \n  gf_hline(yintercept =  ~ overall_mean,\n             data = overall_mean) %>% \n  gf_segment(data = data, \n             color = \"black\",\n             overall_mean$overall_mean + quant ~ index + index)\n\np2 <- gf_point(quant ~ index, \n         group = ~ qual, \n         colour = ~ qual, \n         data = data) %>% \n  \n  gf_hline(yintercept = ~ mean, \n           colour = ~ qual,\n           data = data %>% \n             group_by(qual) %>% \n             summarise(mean = mean(quant))) %>% \n  gf_segment(data = data %>% filter(qual == \"A\"),\n             grouped_means$grouped_means[1] + quant ~ index + index\n             ) %>% \n    gf_segment(data = data %>% filter(qual == \"B\"),\n             grouped_means$grouped_means[2] + quant ~ index + index\n             ) \n\np1 + p2 + patchwork::plot_annotation(tag_levels = c(\"A\", \"B\"))\n```\n\n::: {.cell-output-display}\n![](logistic_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nIn Fig A, the *horizontal* black line is the overall mean of `quant`,\ndenoted as $\\mu_{tot}$. The vertical black lines to the points show the\ndepartures of each point from this overall mean. The sum of *squares* of\nthese vertical black lines in Fig A is called the **Total Sum of\nSquares** (SST).\n\n$$\nSST = \\Sigma (y - \\mu_{tot})^2\n$$ {#eq-SST}\n\nIf there are $k$ levels in `qual` and $n$ observations $y_ n$ for each\nlevel, we can also write:\n\n$$\nSST = \n\\sum_{i=1}^{kn}y_i^2 - \\frac{ \\left( \\sum_{i=1}^{kn}\ny_i \\right)^2}{kn}\n$$\n\nIn Fig B, the *horizontal* green and red lines are the means of the\nindividual groups, respectively $\\mu_A$ and $\\mu_B$. The green and red\nvertical lines are the departures, or errors, of each point from *its\nown group-mean*. The sum of the *squares* of the green and red lines is\ncalled the **Total Error Sum of Squares** (SSE).\n\n$$\nSSE = \\Sigma [(y - \\mu_i)^2 +... (y - \\mu_k)^2]\n$$ {#eq-SSE}\n\nIf the $\\mu_A$ and $\\mu_B$ are different from $\\mu_{tot}$, then what\nwould be the relationship between $SSA$ and $SSE$ ? Clearly if the all\nmeans are identical then the $SST$ and $SSE$ are equal, since the two\ncoloured lines would be in the same place as the black line. It should\nbe clear that if $\\mu_A$ and $\\mu_B$ are different from the overall\nmean, then $SSE < SST$.\n\nSo, when we desire to detect if the two groups are different in their\nmeans, we take the difference:\n\n$$\nSSA = SST - SSE\n$$ {#eq-SSA}\n\n$SSA$ is called the **Treatment Sum of Squares** and is a measure the\ndifferences in means of observations at different levels of the factor.\n\n$SSA$ can also directly be re-written in a very symmetric fashion as:\n\n\\$\\$ SSA =\n\\frac{\\sum_{i=1}^{k} \\left( \\sum_{j=1}^{n}y_{ij}\\right)^2 }{n} -\n\\frac{\\left( \\sum_{i=1}^{kn}\n\ny_i \\right)^2}{kn}\n\n\\$\\$ {#eg-SSA}\n\nNote that in the first term, we are calculating sums of observations\nwithin each group in the inner summation, which is like a per-group\nmean(without the division). The outer summation takes the sum of squares\nof these undivided summations and divides by $n$.\n\nComparing $SSA$ and $SSE$ now provides us with a method that helps us\ndecide whether these means are different. The logic is that we compare\nglobal differences and local differences. The comparison is of course in\nthe form of a ratio, the F-statistic. Since each of these measures uses\na different sets of observations, the comparison is done after scaling\neach of $SSA$ and $SSE$ by the number of observations influencing them.\nThis means that we need to divide each of $SSA$ and $SSE$ by their\n*degrees of freedom*, which gives us a ratio of **variances**:\n\n$$\nF_{stat} = \\frac{SSA / df_{SSA}}{SSE / df_{SSE}}\n$$\n\nwhere $df_{SSA}$ and $df_{SSE}$ are respectively the degrees of freedom\nin $SSA$ and $SSE$. And so we are in effect deciding if means are\nsignificantly different by analyzing (a ratio of) variances! Hence\n*AN-alysis O-f VA-riance*, ANOVA.\n\nIn order to find which of the means is significantly different from\nothers, we need to make a pair-wise comparison of the means, applying\nthe Bonferroni correction as stated before.\n\n### ANOVA Manually Demonstrated (Apologies to Spinoza)\n\nLet us hand-calculate the numbers so we know what the test is doing.\n\nHere is the SST:\n\n\n::: {.cell hash='logistic_cache/html/SST Total Sum of Squares_62d5643a60d3c13c408ebabf1641e279'}\n\n```{.r .cell-code}\n# Calculate overall sum squares SST\n\n\nfrogs_overall <- frogs_long %>% \n  summarise(mean_time = mean(Time), \n            # Overall mean across all readings\n            # The Black Line\n            \n            SST = sum((Time - mean_time)^2),\n            n = n())\nfrogs_overall\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"mean_time\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"SST\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"n\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"21.16667\",\"2\":\"1096.333\",\"3\":\"60\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\nSST <- frogs_overall$SST\nSST\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1096.333\n```\n:::\n:::\n\n\nAnd here is our plot to understand the SST:\n\n\n::: {.cell hash='logistic_cache/html/Frogs SST Graph_7a1aa3c0ad785c2a2a7cd06bbfa50fe2'}\n\n```{.r .cell-code}\nfrogs_plot <- frogs_long %>% \n  arrange(TempFac) %>% \n           rowid_to_column(var = \"index\")\n\nfrogs_mean <- frogs_long %>%\n             summarise(overall_mean = mean(Time))\n\nfrogs_grouped_means <- frogs_long %>%\n  group_by(TempFac) %>% \n             summarise(grouped_means = mean(Time))\n\ngf_point(Time ~ index, \n         color = ~ TempFac,\n         data = frogs_plot) %>% \n  \n gf_hline(yintercept =  ~ overall_mean,\n             data = frogs_mean) %>% \n  \n  gf_segment(data = frogs_plot, \n             color = \"black\",\n             frogs_mean$overall_mean + Time ~ index + index)\n```\n\n::: {.cell-output-display}\n![](logistic_files/figure-html/Frogs SST Graph-1.png){width=672}\n:::\n:::\n\n\nAnd here is the SSE:\n\n\n::: {.cell hash='logistic_cache/html/SSE Within Group Sum of Squares_194e51d5d288da64415baece600b28b2'}\n\n```{.r .cell-code}\n# Calculate sums of square errors *within* each group\n# with respect to individual group means\n\nfrogs_within_groups <- frogs_long %>% \n  group_by(TempFac) %>% \n   summarise(mean_time = mean(Time),\n            variance_time = var(Time),\n            group_error_squares = sum((Time - mean_time)^2),\n            n = n())\nfrogs_within_groups\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"TempFac\"],\"name\":[1],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"mean_time\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"variance_time\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"group_error_squares\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"n\"],\"name\":[5],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"13\",\"2\":\"26.3\",\"3\":\"1.273684\",\"4\":\"24.2\",\"5\":\"20\"},{\"1\":\"18\",\"2\":\"21.0\",\"3\":\"1.263158\",\"4\":\"24.0\",\"5\":\"20\"},{\"1\":\"25\",\"2\":\"16.2\",\"3\":\"1.431579\",\"4\":\"27.2\",\"5\":\"20\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\nfrogs_SSE <- frogs_within_groups %>% \n  summarise(SSE = sum(group_error_squares))\n\nSSE <- frogs_SSE$SSE\nSSE\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 75.4\n```\n:::\n:::\n\n::: {.cell hash='logistic_cache/html/Frogs SSE Plot_74c31efbee1e040a3c883d0381d2a970'}\n\n```{.r .cell-code}\nfrogs_plot <- frogs_long %>% \n  arrange(TempFac) %>% \n           rowid_to_column(var = \"index\")\n\nfrogs_mean <- frogs_long %>%\n             summarise(overall_mean = mean(Time))\n\nfrogs_grouped_means <- frogs_long %>%\n  group_by(TempFac) %>% \n             summarise(grouped_means = mean(Time))\n\ngf_point(Time ~ index, \n         group = ~ TempFac, \n         colour = ~ TempFac, \n         data = frogs_plot) %>% \n  \n  gf_hline(yintercept = ~ grouped_means, \n           colour = ~ TempFac,\n           data = frogs_grouped_means) %>% \n  \n  gf_segment(data = frogs_plot %>% filter(TempFac == 13 ),\n             frogs_grouped_means$grouped_means[1] + Time ~ index + index\n             ) %>% \n  \n  gf_segment(data = frogs_plot %>% filter(TempFac == 18),\n             frogs_grouped_means$grouped_means[2] + Time ~ index + index\n             ) %>% \n  \n  gf_segment(data = frogs_plot %>% filter(TempFac == 25),\n             frogs_grouped_means$grouped_means[3] + Time ~ index + index\n             ) \n```\n\n::: {.cell-output-display}\n![](logistic_files/figure-html/Frogs SSE Plot-1.png){width=672}\n:::\n:::\n\n\nOK, we have $SST$ and $SSE$, so let's get $SSA$:\n\n\n::: {.cell hash='logistic_cache/html/unnamed-chunk-15_26cdb86f4fa5f7c7b330e814cdf0926c'}\n\n```{.r .cell-code}\nSST\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1096.333\n```\n:::\n\n```{.r .cell-code}\nSSE\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 75.4\n```\n:::\n\n```{.r .cell-code}\nSSA <- SST - SSE\nSSA\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1020.933\n```\n:::\n:::\n\n\nWe have $SST = 1096$, $SSE = 75.4$ and therefore $SSA = 1020.9$.\n\nIn order to calculate the F-Statistic, we need to compute the variances,\nusing these sum of squares. We obtain variances by dividing by their\n*Degrees of Freedom*:\n\n$$\nF_{stat} = \\frac{SSA / df_{SSA}}{SSE / df_{SSE}}\n$$\n\nwhere $df_{SSA}$ and $df_{SSE}$ are respectively the degrees of freedom\nin SSA and SSE.\n\nLet us calculate these Degrees of Freedom. With $k = 2$ levels in the\nfactor `TempFac`, and $n = 20$ points per level, $SST$ clearly has\ndegree of freedom $kn-1$, since it uses all observations but loses one\ndegree to calculate the global mean. (If each level did not have the\nsame number of points $n$, we simply take all observations less one as\nthe degrees of freedom for $SST$).\n\n$SSE$ has $k*(n-1)$ as degrees of freedom, since each of the $k$ groups\nthere are $n$ observations and each group loses one degree to calculate\nits own group mean.\n\nAnd therefore $SSA$ has $k-1$ degrees of freedom.\n\nWe can still calculate these in R, for the sake of method and clarity:\n\n\n::: {.cell hash='logistic_cache/html/Degrees of Freedom_2c65dd0291d7cc9af7c405d1e186a723'}\n\n```{.r .cell-code}\n# Error Sum of Squares SSE\ndf_SSE <- frogs_long %>% \n  \n  # Takes into account \"unbalanced\" situations\n  group_by(TempFac) %>% \n  summarise(per_group_df_SSE = n() - 1) %>% \n  summarise(df_SSE = sum(per_group_df_SSE)) %>% as.numeric()\n\n\n## Overall Sum of Squares SST\ndf_SST <- frogs_long %>% \n  summarise(df_SST = n() - 1) %>% as.integer()\n\n\n# Treatment Sum of Squares SSA\nk <- length(unique(frogs_long$TempFac))\ndf_SSA <- k - 1\n```\n:::\n\n\nThe degrees of freedom for the quantities are:\n\n\n::: {.cell hash='logistic_cache/html/unnamed-chunk-17_e10e19e0ea64edf6410bda91602ab609'}\n\n```{.r .cell-code}\ndf_SST\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 59\n```\n:::\n\n```{.r .cell-code}\ndf_SSE\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 57\n```\n:::\n\n```{.r .cell-code}\ndf_SSA\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2\n```\n:::\n:::\n\n\nNow we are ready to compute the F-statistic:\n\n\n::: {.cell hash='logistic_cache/html/unnamed-chunk-18_395baa88a6be35df66b9d9ad80f91895'}\n\n```{.r .cell-code}\n# Finally F_Stat!\n## Combine the sum-square_error for each level of the factor\n# weighted by degrees of freedom per level\n# Which are of course equal here ;-D\n\nMSE <- frogs_within_groups %>% \n  summarise(mean_square_error = sum(group_error_squares/df_SSE)) %>% \n  as.numeric()\nMSE\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.322807\n```\n:::\n\n```{.r .cell-code}\nMSA <- SSA/df_SSA # This is OK\nMSA\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 510.4667\n```\n:::\n\n```{.r .cell-code}\nF_stat <- MSA/MSE\nF_stat\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 385.8966\n```\n:::\n:::\n\n\nThe F-stat is compared with a **critical value** of the F-statistic,\nwhich is computed using the formula for the f-distribution in R. As with\nour hypothesis tests, we set the significance level to 0.95, and quote\nthe two relevant degrees of freedom as parameters to `qf()` which\ncomputes the critical F value as a **quartile**:\n\n\n::: {.cell hash='logistic_cache/html/unnamed-chunk-19_9811562fcbf22aa37a6f9d9b7cc24a84'}\n\n```{.r .cell-code}\nF_crit <-  qf(p = 0.95,     # Significance level is 5%\n              df1 = df_SSA, # Numerator degrees of freedom \n              df2 = df_SSE) # Denominator degrees of freedom\nF_crit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.158843\n```\n:::\n\n```{.r .cell-code}\nF_stat\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 385.8966\n```\n:::\n:::\n\n\nThe F_crit value can also be seen in a plot[^3]:\n\n\n::: {.cell hash='logistic_cache/html/unnamed-chunk-20_f6f64f405a700ccccc7786733923d468'}\n\n```{.r .cell-code}\nmosaic::pdist(dist = \"f\",\n              q = 3.158843, \n              df1=df_SSA, df2=df_SSE)\n```\n\n::: {.cell-output-display}\n![](logistic_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.95\n```\n:::\n:::\n\n\nAny value of F more than the F_crit occurs with smaller probability than\n0,05. Our F_stat is much higher than F_crit, by orders of magnitude! And\nso we can say with confidence that Temperature has a significant effect\non spawn Time.\n\nAnd that is how ANOVA computes!\n:::\n\n## Checking ANOVA Assumptions\n\nANOVA makes 3 fundamental assumptions:\n\na.  Data are normally distributed.\n\nb.  Variances are homogeneous.\n\nc.  Observations are independent.\n\nWe can check these using checks and graphs:\n\n#### Checks for Normality\n\nThe `shapiro.wilk` test tests if a vector of numeric data is normally\ndistributed and rejects the hypothesis of normality when the\n[p-value](https://variation.com/wp-content/distribution_analyzer_help/hs132.htm)\nis less than or equal to 0.05. \n\n\n::: {.cell hash='logistic_cache/html/Check for Normality_b7d4286145d9a05fb4eeca04e10aa881'}\n\n```{.r .cell-code}\nshapiro.test(x = frogs_long$Time)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  frogs_long$Time\nW = 0.92752, p-value = 0.001561\n```\n:::\n:::\n\n\nThe p-value is very low and we cannot reject the (alternative)\nhypothesis that the overall data is **not** normal. How about normality\nat each level of the factor?\n\n\n::: {.cell hash='logistic_cache/html/unnamed-chunk-22_9eaaa7ba787b095c82e3c57d1cd3fd14'}\n\n```{.r .cell-code}\nfrogs_grouped <- frogs_long %>% \n  group_by(TempFac) %>% \n  nest(.key = \"list\") \n\nfrogs_grouped %>% \n  pluck(\"list\", 1) %>% # naming the nested column \"list\"\n  select(Time) %>% \n  as_vector() %>% \n  shapiro.test(.)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  .\nW = 0.88954, p-value = 0.02638\n```\n:::\n\n```{.r .cell-code}\n# OK now we are set for group-wise Shapiro-Wilk testing:\n\nfrogs_grouped %>% \n  mutate(shaptest = \n           purrr::map(.x = list, # Column name is \"list\"\n                      .f = \\(.x) select(.data = .x, \n                                        Time) %>% \n                                 as_vector() %>% \n                                 shapiro.test(.)),\n         \n         params = map(.x = shaptest,\n                      .f = \\(.x) broom::tidy(.x))) %>% \n  \n  select(TempFac, params) %>% \n  unnest(cols = params)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"TempFac\"],\"name\":[1],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"statistic\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"p.value\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"method\"],\"name\":[4],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"13\",\"2\":\"0.8895426\",\"3\":\"0.02637682\",\"4\":\"Shapiro-Wilk normality test\"},{\"1\":\"18\",\"2\":\"0.9254425\",\"3\":\"0.12614802\",\"4\":\"Shapiro-Wilk normality test\"},{\"1\":\"25\",\"2\":\"0.8978947\",\"3\":\"0.03766278\",\"4\":\"Shapiro-Wilk normality test\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nThe `shapiro.wilk` test makes a NULL Hypothesis that the data **are**\nnormally distributed and estimates the probability that this could have\nhappened by chance. Except for `TempFac = 18` the p-values are less than\n0.05 and we can reject the NULL hypothesis that each of these is\nnormally distributed. Perhaps this is a sign that we need more than 20\nsamples per factor level.\n\nWe can also check the residuals post-model:\n\n\n::: {.cell hash='logistic_cache/html/unnamed-chunk-23_cfb7405682d1f29a1506c0a77e269789'}\n\n```{.r .cell-code}\nfrogs_anova$residuals %>% \n  as_tibble() %>% \n  gf_histogram(~ value,data = .) %>% \n  gf_theme(theme = theme_frogs())\n```\n\n::: {.cell-output-display}\n![](logistic_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n\n```{.r .cell-code}\nfrogs_anova$residuals %>%\n  as_tibble() %>% \n  gf_qq(~ value, data = .) %>% \n  gf_qqstep() %>% \n  gf_qqline() %>% \n  gf_theme(theme = theme_frogs())\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: The following aesthetics were dropped during statistical transformation: sample\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n```\n:::\n\n::: {.cell-output-display}\n![](logistic_files/figure-html/unnamed-chunk-23-2.png){width=672}\n:::\n\n```{.r .cell-code}\nshapiro.test(frogs_anova$residuals)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  frogs_anova$residuals\nW = 0.94814, p-value = 0.01275\n```\n:::\n:::\n\n\nUnsurprisingly, the residuals are also not normally distributed either.\n\n### Check for Similar Variance\n\nResponse data with different variances at different levels of an\n*explanatory* variable are said to exhibit **heteroscedasticity**. This\nviolates one of the assumptions of ANOVA.\n\nTo check if the `Time` readings are similar in `variance` across levels\nof `TempFac`, we can use the Levene Test, or since our per-group\nobservations are not normally distributed, a non-parametric rank-based\nFligner-Killeen test. The NULL hypothesis is that the data **are** with\nsimilar variances. The tests assess how probable this is with the given\ndata assuming this NULL hypothesis:\n\n\n::: {.cell hash='logistic_cache/html/unnamed-chunk-24_86fb0303683c5947c2f0f197ad89c4ab'}\n\n```{.r .cell-code}\nfrogs_long %>% \n  group_by(TempFac) %>% \n  summarise(variance = var(Time))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"TempFac\"],\"name\":[1],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"variance\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"13\",\"2\":\"1.273684\"},{\"1\":\"18\",\"2\":\"1.263158\"},{\"1\":\"25\",\"2\":\"1.431579\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\n# Not too different...OK on with the test\nfligner.test(Time ~ TempFac, data = frogs_long)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tFligner-Killeen test of homogeneity of variances\n\ndata:  Time by TempFac\nFligner-Killeen:med chi-squared = 0.53898, df = 2, p-value = 0.7638\n```\n:::\n\n```{.r .cell-code}\nDescTools::LeveneTest(Time ~ TempFac, data = frogs_long)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"Df\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"F value\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Pr(>F)\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"2\",\"2\":\"0.3931034\",\"3\":\"0.6767746\",\"_rn_\":\"group\"},{\"1\":\"57\",\"2\":\"NA\",\"3\":\"NA\",\"_rn_\":\"\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nIt seems that there is no cause for concern here; the data do not have\nsignificantly different variances.\n\n### Independent Observations\n\nThis is an experiment *design* concern; the way the data is gathered\nmust be specified such that data for each level of the factors ( factor\ncombinations if there are more than one) should be independent.\n\n## Effect Size\n\nThe simplest way to find the actual `effect sizes` detected by an ANOVA\ntest is to use (paradoxically) the `summary.lm()` command:\n\n\n::: {.cell hash='logistic_cache/html/unnamed-chunk-25_06f9d30d49a6b962b0625a3e564c9eb3'}\n\n```{.r .cell-code}\nsummary.lm(frogs_anova) %>% broom::tidy()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"term\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"estimate\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"std.error\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"statistic\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"p.value\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"(Intercept)\",\"2\":\"26.3\",\"3\":\"0.2571777\",\"4\":\"102.26394\",\"5\":\"2.781059e-66\"},{\"1\":\"TempFac18\",\"2\":\"-5.3\",\"3\":\"0.3637041\",\"4\":\"-14.57228\",\"5\":\"7.081214e-21\"},{\"1\":\"TempFac25\",\"2\":\"-10.1\",\"3\":\"0.3637041\",\"4\":\"-27.76982\",\"5\":\"8.187867e-35\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nIt may take a bit of effort to understand this. First the `TempFac` is\narranged in order of levels, and the `mean` at the $`TempFac` = 13$ is\ntitled `intercept`. That is $26.3$. The other two means for levels $18$\nand $25$ are stated as **differences** from this intercept, $-5.3$ and\n$-10.1$ respectively. The `p.value` for all these effect sizes is well\nbelow the desired confidence level of $0.05$.\n\nWe can easily plot bar-chart with error bars for the effect size:\n\n\n::: {.cell hash='logistic_cache/html/unnamed-chunk-26_e80f5f67615c06d74cfafbe2664ceb1c'}\n\n```{.r .cell-code}\ntidy_anova <- frogs_anova %>% \n  summary.lm() %>% \n  broom::tidy()\ntidy_anova\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"term\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"estimate\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"std.error\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"statistic\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"p.value\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"(Intercept)\",\"2\":\"26.3\",\"3\":\"0.2571777\",\"4\":\"102.26394\",\"5\":\"2.781059e-66\"},{\"1\":\"TempFac18\",\"2\":\"-5.3\",\"3\":\"0.3637041\",\"4\":\"-14.57228\",\"5\":\"7.081214e-21\"},{\"1\":\"TempFac25\",\"2\":\"-10.1\",\"3\":\"0.3637041\",\"4\":\"-27.76982\",\"5\":\"8.187867e-35\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\n# Merging group averages with `std.error`\nfrogs_long %>% \n  group_by(TempFac) %>% \n  summarise(mean = mean(Time)) %>% \n  cbind(std.error = tidy_anova$std.error) %>% \n  mutate(hi = mean + std.error,\n         lo = mean - std.error) %>% \n  gf_col(data = ., mean ~ TempFac, \n         fill = \"grey\", \n         color = \"black\") %>% \n  gf_point(mean ~ TempFac, \n           color = \"red\", \n           size = 4) %>% \n    gf_errorbar(hi + lo ~ TempFac,\n                color = \"blue\",\n                linetype = \"dashed\")\n```\n\n::: {.cell-output-display}\n![](logistic_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n\n## ANOVA using Permutation Tests\n\nWe wish to establish the significance of the effect size due to each of\nthe levels in `TempFac`. From the normality tests conducted earlier we\nsee that except at one level of `TempFac`, the times are are not\nnormally distributed. Hence we opt for a Permutation Test to check for\nsignificance of effect.\n\nAs remarked in Ernst[^4], the non-parametric permutation test can be\nboth *exact* and also **intuitively easier** for students to grasp.\nPermutations are easily executed in R, using packages such as\n`mosaic`[^5].\n\nWe proceed with a Permutation Test for `TempFac`. We shuffle the levels\n(13, 18, 25) randomly between the Times and repeat the ANOVA test each\ntime and calculate the F-statistic. The Null distribution is the\ndistribution of the F-statistic over the many permutations and the\np-value is given by the proportion of times the F-statistic equals or\nexceeds that observed.\n\nWe will use `mosaic` and also try with `infer`.\n\n::: panel-tabset\n#### Using `mosaic`\n\n`mosaic` offers an easy and intuitive way of doing a repeated\npermutation test, using the `do()` command. We will `shuffle` the\n`TempFac` factor to jumble up the `Time` observations, 10000 times. Each\ntime we shuffle, we compute the F_statistic and record it. We then plot\nthe 10000 F-statistics and compare that with the real-world observation\nof `F-stat`.\n\n\n::: {.cell hash='logistic_cache/html/permutation test for ANOVA with mosaic_90bc829c05c6f4e897cac1958ee2884f'}\n\n```{.r .cell-code}\nobs_F_stat <- frogs_anova %>% \n  broom::tidy() %>% \n  select(statistic)\nobserved_mosaic <- obs_F_stat$statistic[1]\n\nnull_dist_mosaic <- do(10000) * aov(Time ~ shuffle(TempFac), data = frogs_long)\nnull_dist_mosaic %>% head()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"source\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"df\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"SS\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"MS\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"F\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"pval\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\".row\"],\"name\":[7],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\".index\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"shuffle(TempFac)\",\"2\":\"2\",\"3\":\"41.23333\",\"4\":\"20.61667\",\"5\":\"1.1137807\",\"6\":\"0.3353541\",\"7\":\"1\",\"8\":\"1\",\"_rn_\":\"shuffle(TempFac)...1\"},{\"1\":\"Residuals\",\"2\":\"57\",\"3\":\"1055.10000\",\"4\":\"18.51053\",\"5\":\"NA\",\"6\":\"NA\",\"7\":\"2\",\"8\":\"1\",\"_rn_\":\"Residuals...2\"},{\"1\":\"shuffle(TempFac)\",\"2\":\"2\",\"3\":\"25.83333\",\"4\":\"12.91667\",\"5\":\"0.6877627\",\"6\":\"0.5068221\",\"7\":\"1\",\"8\":\"2\",\"_rn_\":\"shuffle(TempFac)...3\"},{\"1\":\"Residuals\",\"2\":\"57\",\"3\":\"1070.50000\",\"4\":\"18.78070\",\"5\":\"NA\",\"6\":\"NA\",\"7\":\"2\",\"8\":\"2\",\"_rn_\":\"Residuals...4\"},{\"1\":\"shuffle(TempFac)\",\"2\":\"2\",\"3\":\"70.93333\",\"4\":\"35.46667\",\"5\":\"1.9715233\",\"6\":\"0.1486260\",\"7\":\"1\",\"8\":\"3\",\"_rn_\":\"shuffle(TempFac)...5\"},{\"1\":\"Residuals\",\"2\":\"57\",\"3\":\"1025.40000\",\"4\":\"17.98947\",\"5\":\"NA\",\"6\":\"NA\",\"7\":\"2\",\"8\":\"3\",\"_rn_\":\"Residuals...6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\nnull_dist_mosaic %>% drop_na() %>% \n  select(F) %>% \n  gf_histogram(data = ., ~ F, \n               fill = ~ F >= observed_mosaic,\n               title = \"Null Distribution of ANOVA F-statistic\" )\n```\n\n::: {.cell-output-display}\n![](logistic_files/figure-html/permutation test for ANOVA with mosaic-1.png){width=672}\n:::\n:::\n\n\nThe Null distribution of the F_statistic under permutation shows it\nnever crosses the real-world observed value, testifying the strength of\nthe effect of `TempFac` on hatching `Time`. And the p-value is:\n\n\n::: {.cell hash='logistic_cache/html/unnamed-chunk-28_05304bf1c059d296337b48945b568890'}\n\n```{.r .cell-code}\np_value <- mean(null_dist_mosaic$F >= observed_mosaic, na.rm = TRUE)\np_value\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n:::\n\n\n#### Using `infer`\n\nWe calculate the observed F-stat with `infer`, which also has a very\ndirect, if verbose, syntax for doing permutation tests:\n\n\n::: {.cell hash='logistic_cache/html/ANOVA F-statistic with infer_76eb468b05555813222269f940d2c77d'}\n\n```{.r .cell-code}\nobserved_infer <- frogs_long %>% \n  specify(Time ~ TempFac) %>% \n  hypothesise(null = \"independence\") %>% \n  calculate(stat = \"F\")\nobserved_infer\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"stat\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"385.8966\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nWe see that the observed F-Statistic is of course $385.8966$ as before.\nNow we use `infer` to generate a NULL distribution using permutation of\nthe factor `TempFac`:\n\n\n::: {.cell hash='logistic_cache/html/Permutation using infer_4593ba9370f7390d4e30345570db6e85'}\n\n```{.r .cell-code}\nnull_dist_infer <- frogs_long %>% \n  specify(Time ~ TempFac) %>% \n  hypothesise(null = \"independence\") %>% \n  generate(reps = 1000, type = \"permute\") %>% \n  calculate(stat = \"F\")\n\nhead(null_dist_infer)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"replicate\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"stat\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1\",\"2\":\"0.57640052\"},{\"1\":\"2\",\"2\":\"0.25529174\"},{\"1\":\"3\",\"2\":\"0.04252307\"},{\"1\":\"4\",\"2\":\"0.10523666\"},{\"1\":\"5\",\"2\":\"0.56287787\"},{\"1\":\"6\",\"2\":\"0.14982578\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\nnull_dist_infer %>% \n  visualise()\n```\n\n::: {.cell-output-display}\n![](logistic_files/figure-html/Permutation using infer-1.png){width=672}\n:::\n:::\n\n\nAs seen, the `infer` based permutation test also shows that the\npermutationally generated F-statistics are nowhere near that which was\nobserved. The effect of `TempFac` is very strong.\n:::\n\n## Conclusions\n\nWe have discussed ANOVA as a means of modelling the effects of a\nCategorical variable on a Continuous (Quant) variable. ANOVA can be\ncarried out using the standard formula `aov` when assumptions on\ndistributions, variances, and independence are met. Permutation ANOVA\ntests can be carried out when these assumptions do not quite hold.\n\n## References\n\n1.  The ANOVA tutorial at [Our Coding\n    Club](https://ourcodingclub.github.io/tutorials/anova/).\n2.  Michael Crawley, The R Book,second edition, 2013. Chapter 11.\n3.  David C Howell, [Permutation Tests for Factorial ANOVA\n    Designs](https://www.uvm.edu/~statdhtx/StatPages/Permutation%20Anova/PermTestsAnova.html)\n4.  Marti Anderson, [Permutation tests for univariate or multivariate\n    analysis of variance and\n    regression](https://www.academia.edu/50056272/Permutation_tests_for_univariate_or_multivariate_analysis_of_variance_and_regression?auto=download)\n\n[^1]: https://www.openintro.org/go/?id=anova-supplement&referrer=/book/ahss/index.php\n\n[^2]: https://www.openintro.org/go/?id=anova-supplement&referrer=/book/ahss/index.php\n\n[^3]: Pruim R, Kaplan DT, Horton NJ (2017). \"The mosaic Package: Helping\n    Students to 'Think with Data' Using R.\" The R Journal, 9(1),\n    77--102.\n    https://journal.r-project.org/archive/2017/RJ-2017-024/index.html.\n\n[^4]: Ernst, Michael D. 2004. \"Permutation Methods: A Basis for Exact\n    Inference.\" Statistical Science 19 (4): 676--85.\n    doi:10.1214/088342304000000396.\n\n[^5]: Pruim R, Kaplan DT, Horton NJ (2017). \"The mosaic Package: Helping\n    Students to 'Think with Data' Using R.\" The R Journal, 9(1),\n    77--102.\n    https://journal.r-project.org/archive/2017/RJ-2017-024/index.html.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../../../../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\r\n<script src=\"../../../../../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}