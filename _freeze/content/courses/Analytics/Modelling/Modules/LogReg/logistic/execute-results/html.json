{
  "hash": "dde58a66db59e4acee37f830b3e62e78",
  "result": {
    "markdown": "---\ntitle: \"Modelling with Logistic Regression\"\nauthor: \"Arvind Venkatadri\"\ndate: 13/Apr/2023\ndate-modified: \"2023-09-15\"\norder: 20\nformat:\n  html:\n    html-math-method: katex\nimage: preview.png\nimage-alt: \"\"\ncategories: \n  - Logistic Regression\n  - Qualitative Variable\n  - Probability\n  - Odds\n  - Log Transformation\n---\n\n\n## {{< iconify noto-v1 package >}} Setting up R Packages\n\n\n::: {.cell hash='logistic_cache/html/setup_1810c5e962bfe54ee3896706040db6aa'}\n\n```{.r .cell-code}\nknitr::opts_chunk$set(echo = TRUE,warning = FALSE,message = FALSE)\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(mosaic)\nlibrary(infer)\nlibrary(regressinator) # pedagogic tool for GLMs\nlibrary(GLMsData)\nlibrary(prettyglm)\n#remotes::install_github(\"UCLATALL/JMRData\")\n#library(JMRData)\n```\n:::\n\n::: {.cell hash='logistic_cache/html/plot theme_4d33be43f669319d88a593261669fe4f'}\n\n```{.r .cell-code}\n# Let us set a plot theme for Data visualization\n\nmy_theme <- function(){  # Creating a function\n  theme_classic() +  # Using pre-defined theme as base\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        axis.text.x = element_text(size = 10, face = \"bold\"),  \n        # Customizing axes text      \n        axis.text.y = element_text(size = 10, face = \"bold\"),\n        axis.title = element_text(size = 12, face = \"bold\"),  \n        # Customizing axis title\n        panel.grid = element_blank(),  # Taking off the default grid\n        plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), units = , \"cm\"),\n        legend.text = element_text(size = 8, face = \"italic\"),  \n        # Customizing legend text\n        legend.title = element_text(size = 10, face = \"bold\"),  \n        # Customizing legend title\n        legend.position = \"right\",  # Customizing legend position\n        plot.caption = element_text(size = 8))  # Customizing plot caption\n}   \n```\n:::\n\n\n## {{< iconify openmoji japanese-symbol-for-beginner >}} Introduction\n\nSometimes the dependent variable is an either/or categorization. For\nexample, the variable we want to predict might be `won` or `lost` the\ncontest, `has an ailment` or `not`, `voted` or `not` in the last\nelection, or `graduated` from college or `not`. There might even be more\nthan two categories such as voted for Congress, BJP, or Independent; or\nnever smoker, former smoker, or current smoker.\n\nWe saw with the **general linear model** that it models the **mean** of\na target *Quantitative* variable as a linear weighted sum of the\npredictor variables:\n\n$$\ny \\sim N(x_i^T * \\beta, ~~\\sigma^2)\n$$ \n\nThis model is considered to be **general** because of the dependence on\npotentially more than one explanatory variable, v.s. the **simple**\nlinear model:[^1] $y = \\beta_0 + \\beta_1*x_1 + \\epsilon$. The general\nlinear model gives us model \"shapes\" that start from a simple straight\nline to a *p-dimensional hyperplane*. \n\nAlthough a very useful framework, there are some situations where\ngeneral linear models are not appropriate:\n\n-   the range of Y is restricted (e.g. binary, count)\n-   the variance of Y depends on the mean (Taylor's Law)[^2]\n\nHow do we use the familiar *linear model* framework when the target/dependent variable is *Categorical*?\n\n### Linear Models for Categorical Targets?\n\nRecall that we spoke of `dummy **predictor** variables` for our linear\nmodels and how we would **dummy code** them using numerical values, such\nas 0 and 1, or +1 and -1. Could we try the same way for a **target**\ncategorical variable?\n\n$$\nY_i = \\beta_0 + \\beta_1*Xi + \\epsilon_i\\\\ \\nonumber\nwhere\\\\\n\\begin{align}\nY_i &= 0 ~ if ~ \"No\"\\\\ \\nonumber\n    &= 1 ~ if ~\"Yes\"  \\nonumber\n\\end{align}\n$$\n\nSadly this seems to not work for categorical dependent variables using a\nsimple linear model as before. Consider the Credit Card `Default` data\nfrom the package `ISLR`.\n\n\n::: {.cell hash='logistic_cache/html/unnamed-chunk-3_a5833e1a2cc39825993dec9ab6c813ee'}\n::: {.cell-output .cell-output-stdout}\n```\nRows: 10,000\nColumns: 4\n$ default <fct> No, No, No, No, No, No, No, No, No, No, No, No, No, No, No, No…\n$ student <fct> No, Yes, No, No, No, Yes, No, Yes, No, No, Yes, Yes, No, No, N…\n$ balance <dbl> 729.5265, 817.1804, 1073.5492, 529.2506, 785.6559, 919.5885, 8…\n$ income  <dbl> 44361.625, 12106.135, 31767.139, 35704.494, 38463.496, 7491.55…\n```\n:::\n:::\n\n\nWe see `balance` and `income` are quantitative predictors; `student` is\na qualitative predictor, and `default` is a qualitative target variable.\nIf we naively use a linear model equation as\n`model = lm(default ~ balance, data = Default)` and plot it, then...\n\n\n::: {.cell hash='logistic_cache/html/fig-naive-linear-model_4d7e2aeb43ca2d6013cd4c1d14f80691'}\n::: {.cell-output-display}\n![Naive Linear Model](logistic_files/figure-html/fig-naive-linear-model-1.png){#fig-naive-linear-model width=2100}\n:::\n:::\n\n\n...it is pretty much clear from @fig-naive-linear-model that something is\nvery odd. (no pun intended! See below!) If the only possible values for\n`default` are $No = 0$ and $Yes = 1$, how could we interpret predicted\nvalue of, say, $Y_i = 0.25$ or $Y_i = 1.55$, or perhaps $Y_i = -0.22$? Anything other than Yes/No is hard to interpret!\n\n### {{< iconify ic baseline-report-problem >}} {{< iconify ant-design solution-outlined >}} Problems...and Solutions\n\nWhere do we go from here?\n\nLet us state what we might desire of our model:\n\n1.  **Model Equation**: Despite this setback, we would still like\n    our model to be as close as possible to the familiar linear model\n    equation.\\\n2.  **Predictors and Weights**: We have quantitative **predictors** so\n    we still want to use a linear-weighted sum for the RHS (i.e\n    predictor side) of the model equation.\n\nAnd for the LHS (i.e the target side)?\n\n3.  **Probability**: What can we try? Even though we are interested in\n    binary outcomes, we might try to use **probability of the outcome** as our target. However, this still leaves us with a range of \\[0,1\\] for the target variable, as before. How about **odds of the outcome**, instead of trying to predict the outcomes directly (Yes or No), or their probabilities \\[0,1\\]?\n\n::: callout-note\n## Odds\n\nOdds of an event with probability `p` of occurrence is defined as\n$Odds = p/(1-p)$. As can be seen, the odds are the *ratio* of two probabilities, that of the event and its complement.\nIn the `Default` dataset just considered, the odds of default and the odds of non-default can be calculated as:\n\n\n::: {.cell hash='logistic_cache/html/unnamed-chunk-5_ec56f246addd48993f6cbea8c1649318'}\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"default\"],\"name\":[1],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"n\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"No\",\"2\":\"9667\"},{\"1\":\"Yes\",\"2\":\"333\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n$$\n\\begin{align}\nOddsDefault &=p(noDefault)/(1-p(noDefault))\\\\ \\nonumber\n            &= 0.9667/(1-0.9667)\\\\ \\nonumber\n            &= 29.0303\\\\\n\\end{align}\n$$\n\nand `OddsNoDefault` = $1/29.0303 = 0.03444709$.\n\nNow, *odds* cover the entire real number line, i.e. [$-\\infty$, $\\infty$] !\nClearly, when the probability `p` of an event is $0$, the odds are $-\\infty$...and when it nears $1$, the odds tend to $\\infty$. So we have\n**transformed** a simple probability that lies between $[0,1]$ to odds\nlying between $[-\\infty, \\infty]$. That's a great step towards making a linear model possible; we have \"removed\" the limits on our linear model's\nprediction range by using `Odds` as our target variable.\n:::\n\n4.  **Error Distributions with Odds targets**: Odds are a necessarily\n    nonlinear function of probability; the slope of `Odds ~ probability`\n    depends upon the probability itself. So a percentage change in the\n    Odds would lead to different amounts of change in the probabilities,\n    depending on the value of the probability.\n\n\n::: {#fig-odds-plot .cell layout-ncol=\"2\" hash='logistic_cache/html/fig-odds-plot_3e9da080deb1971f37904c8f5ed95273'}\n::: {.cell-output-display}\n![Odds](logistic_files/figure-html/fig-odds-plot-1.png){#fig-odds-plot-1 width=2100}\n:::\n\n::: {.cell-output-display}\n![Log Odds](logistic_files/figure-html/fig-odds-plot-2.png){#fig-odds-plot-2 width=2100}\n:::\n\nOdds Plot\n:::\n\n\nTo understand this issue intuitively, consider what happens to, say, a\n5% change in the odds ratio near 1.0 compared to more extreme odds\nratios, @fig-odds-plot-1 . If the odds ratio is $1.0$, then the\nprobabilities `p` and `1-p` are $0.5$, and $0.5$. A 20% increase in the\nodds ratio to $1.20$ would correspond to probabilities of $0.545$ and\n$0.455$. However, if the original probabilities were $0.9$ and $0.1$ for\nan odds ratio $9$, then a 20% increase to $10.8$ would correspond to\nprobabilities of $0.915$ and $0.085$, a much smaller change in the\nprobabilities. Hence, extreme probabilities (near 1 or 0) are more\nstable (i.e., have less error) than middle probabilities.\n\nThis should remind us of the [**LINE** assumptions in linear\nregression](../LinReg/LinReg.qmd#sec-assumptions-in-linear-models) where we assume that the errors (in prediction) are normally distributed with common variance, across different values of the independent/predictor variable. The solution to this *heteroscedasticity* problem is similar here to what we discussed there: the *log transformation*, @fig-odds-plot-2. This transformation works because it makes the same percentage changes equivalent no matter what the starting value of the odds or the probabilities; logs convert multipliers/divisors to sums/differences and the graph is linear for the most part.\n\nSo in our model, instead of modeling *odds* as the dependent variable,\nwe will use $log(odds)$, also known as the **logit**, defined as:\n\n$$\n\\begin{align}\nlog(odds_i) &= log[(p_i)/(1-p_i)]\\\\ \\nonumber\n            &= logit(p_i)\\\\ \n\\end{align}\n$$\n\n::: callout-note\n### Binomially distributed target variable\n\nIn linear regression, we assume a normally distributed target variable.\nWith a categorical target variable with two levels $0$ and $1$ it would be impossible for the errors $e_i = Y_i - \\hat{Y_i}$ to have a *normal distribution*, as assumed for the statistical tests to be valid. The\nerrors are bounded by \\[0,1\\]!\nOne candidate for the error distribution in this case is the *binomial distribution*, whose mean and variance are `p` and `np(1-p)` respectively. Note immediately that the variance moves with the mean! So the model has \"built-in\" heteroscedasticity, which we need to counter with transformations such as the $log()$ function. More on this later.\n:::\n\n5.  **Estimation of Model Parameters**: The last problem to solve is\n    that because we have made so many transformations to get to the\n    `logits` that we want to model, the logic of minimizing the **sum of squared errors(SSE)** is no longer appropriate.\n    \n::: callout-note\nThe probabilities for `default` are $0$ and $1$...the `log(odds)` will map respectively to $-\\infty$ and $\\infty$. So if we naively try to take residuals, we will find that they are **all** $\\infty$ !! Hence $SSE$ cannot be computed and we need another way to assess the quality of our model. \n:::\n\nInstead, we will have to use **maximum likelihood estimation(MLE)** to estimate the models, and we will use the $X^2$ (\"chi-squared\") statistic instead of `t` and `F`to evaluate the model comparisons. The *maximum likelihood method* maximizes the probability of obtaining the data at hand against every choice of model parameters $\\beta_i$. \n    \nThis is our **Logistic Regression Model**, which uses a Quantitative\nPredictor variable to predict a Categorical target variable. We write the model as ( for the `Default` dataset:\n\n$$\n\\begin{align}\nlogit(default) & = \\beta_0 + \\beta_1 * balance&\\\\  \\nonumber\nlog(p(default)/(1-p(default))) & = \\beta_0+\\beta_1 * balance&\\\\ \\nonumber\ntherefore\\\\\np(default) & = \\frac{exp(\\beta_0 + \\beta_1 * balance)}{1 + exp(\\beta_0 + \\beta_1 * balance)}\\\\\n\\end{align}\n$$\n\nFrom the Eqn.4 above it should be clear that a *unit increase* in `balance` should increase the odds of `default` by $\\beta_1$ units. \n\nThe RHS of Eqn.5 is a *sigmoid* function of the weighted sum of predictors and is limited to the range [0,1]. The parameters $\\beta_i$ need to be estimated using maximum likelihood methods. \n\n\n::: {#fig-model-plots .cell .column-screen-inset-right layout-ncol=\"3\" hash='logistic_cache/html/fig-model-plots_716901a633e599984dea9db3e1750e06'}\n::: {.cell-output-display}\n![naive linear regression model](logistic_files/figure-html/fig-model-plots-1.png){#fig-model-plots-1 width=2100}\n:::\n\n::: {.cell-output-display}\n![logistic regression model](logistic_files/figure-html/fig-model-plots-2.png){#fig-model-plots-2 width=2100}\n:::\n\n::: {.cell-output-display}\n![log odds gives linear models](logistic_files/figure-html/fig-model-plots-3.png){#fig-model-plots-3 width=2100}\n:::\n\nModel Plots\n:::\n\n\n\n## {{< iconify simple-icons hypothesis >}} Logistic Regression Models as Hypothesis Tests   \n\n\n::: {.content-hidden when-format=\"html\"}\n## {{< iconify tdesign function-curve >}} Generalized Linear Model\n\n::: callout-important\nA **generalized linear model** is made up of a linear predictor:\n\n$$\n\\eta_i = \\beta_0 + \\beta_1x_{1i} + ... + \\beta_px_{pi}\n$$\n\nand two functions:\n\n-   a link function that describes how the mean, $E(Y_i) = \\mu_i$,\n    depends on the linear predictor:\\\n\n    $$\n    g(\\mu_i) = \\eta_i\n    $$\n\n-   a variance function that describes how the variance, $var(Y_i)$\n    depends on the mean:\\\n\n    $$\n    var(Y_i) = \\Phi*V(\\mu_i)\n    $$\n\nwhere the dispersion parameter $\\Phi$ is a constant.\n:::\n\nFor example we can obtain our *general linear model* with the following\nchoice:\n\n$$\n\\begin{align}\n& g(\\mu_i) = \\mu_i\\\\\n& Phi = 1\n\\end{align}\n$$\n\nIf now we assume that the *target* variable $Y_i$ is a **binomial**,\ni.e. a two-valued variable:\\\n\n$$\n\\begin{align}\n & Y_i = binom(n_i,p_i)\\\\\n & mean(Y_i) = n_ip_i\\\\\n & var(Y_i) = n_ip_i(1-p_i)\n\\end{align}\n$$\n\nNow, we wish to model the **proportions** $Y_i/n_i$, as our **target**.\nThen we can state that:\\\n\n$$\n\\begin{align}\nmean(Y_i/n_i) = p_i \\coloneqq \\mu_i\\\\\nvar(Y_i/n_i) = var(Y_i)/n_i^2 = \\frac{p_i(1-p_i)}{n_i} \\coloneqq \\sigma_i^2\\\\\n\\end{align}\n$$\n\nInspecting the above, we can write:\n\n$$\n\\sigma_i^2 = \\frac{\\mu_i(1-\\mu_i)}{n_i}\n$$\n\nand since the link function needs to map ${[-\\infty, \\infty]}$ to\n${[0,1]}$, we use the `logit` function:\n\n$$\ng(\\mu_i) = logit(\\mu_i) = log(\\frac{\\mu_i}{1-\\mu_i})\n$$\n:::\n\n\n\n## {{< iconify flat-color-icons workflow >}} Workflow: Read the Data\n\n\nLet us now read in the data and check for these assumptions as part of\nour Workflow.\n\n::: callout-note\n## Research Question\n\nHow do we predict the price of a house in Boston, based on other\nparameters Quantitative parameters such as area, location, rooms, and\ncrime-rate in the neighbourhood?\n:::\n\n\n::: {.cell hash='logistic_cache/html/unnamed-chunk-8_cc60b580f7e6d67b37111c0306d18443'}\n\n```{.r .cell-code}\ndata(\"turbines\", package = \"GLMsData\")\ninspect(turbines)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nquantitative variables:  \n      name   class min     Q1 median   Q3  max        mean         sd  n\n1    Hours integer 400 1600.0   2600 3600 4600 2581.818182 1357.80572 11\n2 Turbines integer  13   33.5     39   41   73   39.272727   14.79250 11\n3 Fissures integer   0    4.5      7   15   22    9.636364    7.97838 11\n  missing\n1       0\n2       0\n3       0\n```\n:::\n\n```{.r .cell-code}\nglimpse(turbines)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 11\nColumns: 3\n$ Hours    <int> 400, 1000, 1400, 1800, 2200, 2600, 3000, 3400, 3800, 4200, 46…\n$ Turbines <int> 39, 53, 33, 73, 30, 39, 42, 13, 34, 40, 36\n$ Fissures <int> 0, 4, 2, 7, 5, 9, 9, 6, 22, 21, 21\n```\n:::\n\n```{.r .cell-code}\nskimr::skim(turbines)\n```\n\n::: {.cell-output-display}\n<table style='width: auto;'\n      class='table table-condensed'>\n<caption>Data summary</caption>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Name </td>\n   <td style=\"text-align:left;\"> turbines </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Number of rows </td>\n   <td style=\"text-align:left;\"> 11 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Number of columns </td>\n   <td style=\"text-align:left;\"> 3 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> _______________________ </td>\n   <td style=\"text-align:left;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Column type frequency: </td>\n   <td style=\"text-align:left;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> numeric </td>\n   <td style=\"text-align:left;\"> 3 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> ________________________ </td>\n   <td style=\"text-align:left;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Group variables </td>\n   <td style=\"text-align:left;\"> None </td>\n  </tr>\n</tbody>\n</table>\n\n\n**Variable type: numeric**\n\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> skim_variable </th>\n   <th style=\"text-align:right;\"> n_missing </th>\n   <th style=\"text-align:right;\"> complete_rate </th>\n   <th style=\"text-align:right;\"> mean </th>\n   <th style=\"text-align:right;\"> sd </th>\n   <th style=\"text-align:right;\"> p0 </th>\n   <th style=\"text-align:right;\"> p25 </th>\n   <th style=\"text-align:right;\"> p50 </th>\n   <th style=\"text-align:right;\"> p75 </th>\n   <th style=\"text-align:right;\"> p100 </th>\n   <th style=\"text-align:left;\"> hist </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Hours </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 2581.82 </td>\n   <td style=\"text-align:right;\"> 1357.81 </td>\n   <td style=\"text-align:right;\"> 400 </td>\n   <td style=\"text-align:right;\"> 1600.0 </td>\n   <td style=\"text-align:right;\"> 2600 </td>\n   <td style=\"text-align:right;\"> 3600 </td>\n   <td style=\"text-align:right;\"> 4600 </td>\n   <td style=\"text-align:left;\"> ▅▅▅▅▇ </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Turbines </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 39.27 </td>\n   <td style=\"text-align:right;\"> 14.79 </td>\n   <td style=\"text-align:right;\"> 13 </td>\n   <td style=\"text-align:right;\"> 33.5 </td>\n   <td style=\"text-align:right;\"> 39 </td>\n   <td style=\"text-align:right;\"> 41 </td>\n   <td style=\"text-align:right;\"> 73 </td>\n   <td style=\"text-align:left;\"> ▂▇▇▂▂ </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Fissures </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 9.64 </td>\n   <td style=\"text-align:right;\"> 7.98 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 4.5 </td>\n   <td style=\"text-align:right;\"> 7 </td>\n   <td style=\"text-align:right;\"> 15 </td>\n   <td style=\"text-align:right;\"> 22 </td>\n   <td style=\"text-align:left;\"> ▇▇▅▁▇ </td>\n  </tr>\n</tbody>\n</table>\n:::\n:::\n\n\n## {{< iconify flat-color-icons workflow >}} Workflow: EDA\n\n## {{< iconify flat-color-icons workflow >}} Workflow: Model Building\n\n:::{.panel-tabset .nav-pills style=\"background: whitesmoke;\"} \n\n### {{< iconify mingcute code-fill >}} Model Code\nWIP\n\n### {{< iconify mdi thinking >}} Logistic Regression Model Intuitive {#sec-lg-intuitive}\nWIP\n\n### {{< iconify material-symbols slideshow-sharp >}} Logistic Regression Models Manually Demonstrated\nWIP\n\n### {{< iconify iconoir stats-report >}} Using Other Packages {#sec--using-other-packages}\nWIP\n:::\n\n\n## {{< iconify flat-color-icons workflow >}} Workflow: Model Checking and Diagnostics {#sec-diagnostics}\n\n### {{< iconify ic twotone-rule >}} Checks for Uncertainty\n\n## {{< iconify fluent-mdl2 decision-solid >}} Conclusions\n\nSo our Linear Modelling workflow might look like this: we have not seen\nall stages yet, but that is for another course module or tutorial!\n\n\n```{mermaid}\n%%| echo: false\nflowchart TD\n    A[(A: Data)] -->|mosaic  +  ggformula|B[B:EDA] \n    B --> |corrplot +  corrgram  + ggformula + purrr + cor.test| C(C: Check Relationships)\n    C --> D[D: Decide on Simple/Complex Model]\n    D --> E{E: Is the Model Possible?}\n    E --> |Yes| G[G: Build Model]\n    E -->|Nope| F[F: Transform Variables]\n    E -->|Nope| K[K: Try Multiple Regression <br> and/or Interaction Terms]\n    K --> D\n    F --> D\n    G --> H{H: Check Model Diagnostics}\n    H --> |Problems| D\n    H --> |All   good| I(Interpret Your Model)\n    I --> J(((Apply the Model for Predictions)))\n    \n```\n\n\n## {{< iconify ooui references-rtl >}} References {#sec-references}\n\n1.  Judd, Charles M. & McClelland, Gary H. & Ryan, Carey S. *Data\n    Analysis: A Model Comparison Approach to Regression, ANOVA, and\n    Beyond.* Routledge, Aug 2017. Chapter 14.\n\n2.  <https://yury-zablotski.netlify.app/post/how-logistic-regression-works/>\n\n3.  <https://uc-r.github.io/logistic_regression>\n\n4.  <https://francisbach.com/self-concordant-analysis-for-logistic-regression/>\n\n5.  <https://statmath.wu.ac.at/courses/heather_turner/glmCourse_001.pdf>\n\n6.  <https://jasp-stats.org/2022/06/30/generalized-linear-models-glm-in-jasp/>\n\n7. Fowler J (2023). *prettyglm: Pretty Summaries of Generalized Linear Model Coefficients*. R package version 1.0.1, <https://jared-fowler.github.io/prettyglm/>.\n\n[^1]: <https://statmath.wu.ac.at/courses/heather_turner/glmCourse_001.pdf>\n\n[^2]: <https://en.wikipedia.org/wiki/Taylor%27s_law>\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../../../../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\r\n<script src=\"../../../../../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}