{
  "hash": "2cc33aabfcfccd4dce506812090529ee",
  "result": {
    "markdown": "---\ntitle: \"\\U0001F0CF Testing for Two Independent Means\"\nauthor: \"Arvind Venkatadri\"\ndate: 10/Nov/2022\nsubtitle: \"\"\ndate-modified: \"2023-05-29\"\norder: 110\ntags:\n- Permutation\n- Monte Carlo Simulation\n- Random Number Generation\n- Distributions\n- Generating Parallel Worlds\n---\n\n::: {.cell hash='two-means_cache/html/setup_796b5bf8ed86337f00a50b9daae636e0'}\n\n```{.r .cell-code}\nknitr::opts_chunk$set(echo = FALSE,message = TRUE,\n                      warning = TRUE, \n                      fig.align = \"center\")\nlibrary(tidyverse)\nlibrary(mosaic)\n\n### Datasets from Chihara and Hesterberg's book (Second Edition)\nlibrary(resampledata)\n```\n:::\n\n\n## {{< fa folder-open >}} Slides and Tutorials\n\n| <a href=\"./files/two-means.qmd\"><i class=\"fa-brands                     \n                     fa-r-project\"></i> R Tutorial</a>                    | <a href=\"./files/two-means.ows\"> <iconify-icon icon=\"fluent-emoji:orange-circle\"></iconify-icon> Orange Tutorial</a> | <a href=\"./files/two-means.rda\"> <i class=\"fa-solid fa-person-rays\"></i> Radiant Tutorial</a>  | <a href=\"./files/data/sim-data.zip\"> <i class=\"fa-solid fa-database\"></i> Datasets</a> |\n|-------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------|\n\n## Introduction\n\nWe saw from the diagram created by Allen Downey that *there is only one\ntest*! We will now use this philosophy to develop a technique that\nallows us to mechanize several *Statistical Models* in that way, with\nnearly identical code.\n\nWe will use two packages in R, `mosaic` and the relatively new `infer`\npackage, to develop our intuition for what are called **permutation**\nbased statistical tests.\n\n## Hypothesis Testing using Permutation\n\nFrom Reference #1:\n\n> Hypothesis testing can be thought of as a 4-step process:\n>\n> 1.  State the null and alternative hypotheses.\n>\n> 2.  Compute a test statistic.\n>\n> 3.  Determine the p-value.\n>\n> 4.  Draw a conclusion.\n>\n>     In a traditional introductory statistics course, once this general\n>     framework has been mastered, the main work is in **applying the\n>     correct formula** to compute the standard test statistics in step\n>     2 and using a table or computer to **determine the p-value** based\n>     on the known (usually approximate) **theoretical distribution of\n>     the test statistic** under the null hypothesis.\n>\n>     In a **simulation-based approach**, steps 2 and 3 change. In Step\n>     2, it is no longer required that the test statistic be normalized\n>     to conform with a known, named distribution. Instead, natural test\n>     statistics, like the difference between two sample means $y1 − y2$\n>     can be used.\n>\n>     In Step 3, we use **randomization to approximate the sampling\n>     distribution of the test statistic**. Our lady tasting tea example\n>     demonstrates how this can be done from first principles. More\n>     typically, we will use randomization **to create new simulated\n>     data sets** ( \"*Parallel Worlds*\") that are like our original data\n>     in some ways, but make the null hypothesis true. For each\n>     simulated data set, we calculate our test statistic, just as we\n>     did for the original sample. Together, this collection of test\n>     statistics computed from the simulated samples constitute our\n>     randomization distribution.\n>\n>     When creating a randomization distribution, we will attempt to\n>     satisfy 3 guiding principles.\n>\n> 5.  Be consistent with the null hypothesis. We need to **simulate a\n>     world** in which the null hypothesis is true. If we don't do this,\n>     we won't be testing our null hypothesis.\n>\n> 6.  Use the data in the **original sample**. The original data should\n>     shed light on some aspects of the distribution that are not\n>     determined by null hypothesis. For example, a null hypothesis\n>     about a mean doesn't tell us about the shape of the population\n>     distribution, but the data give us some indication.\n>\n> 7.  Reflect the way the original data were collected.\n\nFrom Chihara and Hesterberg:\n\n> This is the core idea of statistical significance or classical\n> hypothesis testing -- to calculate how often pure random chance would\n> give an effect as large as that observed in the data, in the absence\n> of any real effect. If that probability is small enough, we conclude\n> that the data provide convincing evidence of a real effect.\n\n### Permutations tests using mosaic::`shuffle()`\n\nThe `mosaic` package provides the `shuffle()` function as a synonym for\n`sample()`. When used without additional arguments, this will permute\nits first argument.\n\n\n::: {.cell hash='two-means_cache/html/unnamed-chunk-2_524b2b0fcb0fc4b39240583565d48e5c'}\n\n```{.r .cell-code}\nshuffle(1:10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 10  1  5  8  3  9  7  2  4  6\n```\n:::\n:::\n\n\nApplying shuffle() to an *explanatory variable* in a model allows us to\ntest the null hypothesis that the explanatory variable has, in fact, no\nexplanatory power. This idea can be used to test\n\n-   the equivalence of two or more means,\n-   the equivalence of two or more proportions,\n-   whether a regression parameter is 0.\n\nWe will now see examples of each of these models using Permutations.\n\n## Testing for Two or More Means\n\n### Case Study-1: Hot Wings Orders vs Gender (From Chihara and Hesterberg)\n\nA student conducted a study of hot wings and beer consumption at a Bar.\nShe asked patrons at the bar to record their consumption of hot wings\nand beer over the course of several hours. She wanted to know if people\nwho ate more hot wings would then drink more beer. In addition, she\ninvestigated whether or not gender had an impact on hot wings or beer\nconsumption. Is the mean order value related to the gender of the person\nwho is ordering?\n\n\n::: {.cell hash='two-means_cache/html/unnamed-chunk-3_497b12f7fd3956741d404e8ebab158eb'}\n<iframe src=\"https://images.rawpixel.com/image_400/cHJpdmF0ZS9zdGF0aWMvaW1hZ2Uvd2Vic2l0ZS8yMDIyLTA0L2xyL3B4NzE4NDYzLWltYWdlLWt3dnY3emV5LmpwZw.jpg\" width=\"672\" height=\"400px\" data-external=\"1\"></iframe>\n:::\n\n::: {.cell hash='two-means_cache/html/unnamed-chunk-4_7803543952524127b493627741afdfa2'}\n\n```{.r .cell-code}\ndata(\"Beerwings\")\ninspect(Beerwings)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\ncategorical variables:  \n    name  class levels  n missing                                  distribution\n1 Gender factor      2 30       0 F (50%), M (50%)                             \n\nquantitative variables:  \n      name   class min    Q1 median    Q3 max     mean        sd  n missing\n1       ID integer   1  8.25   15.5 22.75  30 15.50000  8.803408 30       0\n2 Hotwings integer   4  8.00   12.5 15.50  21 11.93333  4.784554 30       0\n3     Beer integer   0 24.00   30.0 36.00  48 26.20000 11.842064 30       0\n```\n:::\n:::\n\n\nLet us calculate the observed difference in `Hotwings` consumption\nbetween Males and Females ( `Gender`): (using the `mosaic` package)\n\n\n::: {.cell hash='two-means_cache/html/unnamed-chunk-5_e95a822701259eefecc82678b63f2396'}\n\n```{.r .cell-code}\nmosaic::mean(Hotwings ~ Gender, data = Beerwings)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        F         M \n 9.333333 14.533333 \n```\n:::\n\n```{.r .cell-code}\nobs_diff_wings <- mosaic::diffmean(data = Beerwings, Hotwings ~ Gender)\nobs_diff_wings \n```\n\n::: {.cell-output .cell-output-stdout}\n```\ndiffmean \n     5.2 \n```\n:::\n:::\n\n::: {.cell hash='two-means_cache/html/unnamed-chunk-6_1699d97fcc5967b475f2f17cefa08c55'}\n\n```{.r .cell-code}\ngf_boxplot(data = Beerwings, Hotwings ~ Gender, title = \"Hotwings Consumption by Gender\") %>%  gf_theme(theme_classic)\n```\n\n::: {.cell-output-display}\n![](two-means_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nThe observed difference in mean consumption of Hotwings between Males\nand Females is 5.2. There is also a \"visible\" difference in *medians* as\nseen from the pair of box plots above.\n\nCould this have occurred by chance? Here is our formulation of the\nHypotheses:\n\n$$\nH_0: \\mu_M\\ = \\mu_F\\\\\nH_a: \\mu_M\\ \\ne \\mu_F\\\\\n$$Note that we have a **two-sided** test: we want to check for\ndifferences in mean order value, *either way*. So we perform a\nPermutation Test to check: we create a **null distribution** of the\ndifferences in mean by a `shuffle` operation on `gender`:\n\n\n::: {.cell hash='two-means_cache/html/unnamed-chunk-7_1692258faaf7d0af11c9facb09b0015f'}\n\n```{.r .cell-code}\nnull_dist_wings <- do(1000) * diffmean(Hotwings ~ shuffle(Gender), data = Beerwings)\nnull_dist_wings %>% head()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"diffmean\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"-2.6666667\",\"_rn_\":\"1\"},{\"1\":\"0.5333333\",\"_rn_\":\"2\"},{\"1\":\"1.8666667\",\"_rn_\":\"3\"},{\"1\":\"0.4000000\",\"_rn_\":\"4\"},{\"1\":\"-1.4666667\",\"_rn_\":\"5\"},{\"1\":\"-2.4000000\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\ngf_histogram(data = null_dist_wings, ~ diffmean) %>% \n  gf_vline(xintercept = obs_diff_wings, colour = \"red\") %>% gf_theme(theme_classic())\n```\n\n::: {.cell-output-display}\n![](two-means_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n```{.r .cell-code}\ngf_ecdf(data = null_dist_wings, ~ diffmean) %>% \n  gf_vline(xintercept = obs_diff_wings, colour = \"red\") %>% gf_theme(theme_classic())\n```\n\n::: {.cell-output-display}\n![](two-means_files/figure-html/unnamed-chunk-7-2.png){width=672}\n:::\n\n```{.r .cell-code}\nprop1(~ diffmean >= obs_diff_wings, data = null_dist_wings)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  prop_TRUE \n0.002997003 \n```\n:::\n:::\n\n\nThe $\\color{red}{red\\ line}$ shows the actual measured mean difference\nin Hot Wings consumption. The probability that our Permutation\ndistribution is able to equal or exceed that number is $0.001998002$ and\nwe have to reject the Null Hypothesis that the means are identical.\n\n### Matched Pairs: Results from a diving championship.\n\nSometimes the data is collected on the same set of individual\ncategories, e.g. scores by sport persons in two separate tournaments, or\nsales of identical items in two separate locations of a chain store.\nHere we have swimming records across a Semi-Final and a Final:\n\n\n::: {.cell hash='two-means_cache/html/unnamed-chunk-8_4d0c95e1f8ac86695b85172f7c90561c'}\n\n```{.r .cell-code}\ndata(\"Diving2017\")\nhead(Diving2017)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"Name\"],\"name\":[1],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Country\"],\"name\":[2],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Semifinal\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Final\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"CHEONG Jun Hoong\",\"2\":\"Malaysia\",\"3\":\"325.50\",\"4\":\"397.50\",\"_rn_\":\"1\"},{\"1\":\"SI Yajie\",\"2\":\"China\",\"3\":\"382.80\",\"4\":\"396.00\",\"_rn_\":\"2\"},{\"1\":\"REN Qian\",\"2\":\"China\",\"3\":\"367.50\",\"4\":\"391.95\",\"_rn_\":\"3\"},{\"1\":\"KIM Mi Rae\",\"2\":\"North Korea\",\"3\":\"346.00\",\"4\":\"385.55\",\"_rn_\":\"4\"},{\"1\":\"WU Melissa\",\"2\":\"Australia\",\"3\":\"318.70\",\"4\":\"370.20\",\"_rn_\":\"5\"},{\"1\":\"KIM Kuk Hyang\",\"2\":\"North Korea\",\"3\":\"360.85\",\"4\":\"360.00\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\ninspect(Diving2017)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\ncategorical variables:  \n     name  class levels  n missing\n1    Name factor     12 12       0\n2 Country factor      8 12       0\n                                   distribution\n1  SI Yajie (8.3%) ...                         \n2 Canada (16.7%), China (16.7%) ...            \n\nquantitative variables:  \n       name   class    min       Q1  median      Q3   max    mean       sd  n\n1 Semifinal numeric 313.70 322.2000 325.625 356.575 382.8 338.500 22.94946 12\n2     Final numeric 283.35 318.5875 358.925 387.150 397.5 350.475 40.02204 12\n  missing\n1       0\n2       0\n```\n:::\n:::\n\n\nThe data is made up of **paired** observations per swimmer. So we need\nto take the difference between the two swim records for *each* swimmer\nand then *shuffle the differences to either polarity*. Another way to\nlook at this is to shuffle the records between `Semifinal` and `Final`\non a per Swimmer basis. There are 12 swimmers and therefore 12 paired\nrecords.\n\nIn order to ensure that the records are `paired`, we use the argument\n`only.2=FALSE` in the `diffmean` function:\n\n\n::: {.cell hash='two-means_cache/html/unnamed-chunk-9_81af9d05cd0145a568ae49b6165d1589'}\n\n```{.r .cell-code}\nDiving2017\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"Name\"],\"name\":[1],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Country\"],\"name\":[2],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"Semifinal\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Final\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"CHEONG Jun Hoong\",\"2\":\"Malaysia\",\"3\":\"325.50\",\"4\":\"397.50\"},{\"1\":\"SI Yajie\",\"2\":\"China\",\"3\":\"382.80\",\"4\":\"396.00\"},{\"1\":\"REN Qian\",\"2\":\"China\",\"3\":\"367.50\",\"4\":\"391.95\"},{\"1\":\"KIM Mi Rae\",\"2\":\"North Korea\",\"3\":\"346.00\",\"4\":\"385.55\"},{\"1\":\"WU Melissa\",\"2\":\"Australia\",\"3\":\"318.70\",\"4\":\"370.20\"},{\"1\":\"KIM Kuk Hyang\",\"2\":\"North Korea\",\"3\":\"360.85\",\"4\":\"360.00\"},{\"1\":\"ITAHASHI Minami\",\"2\":\"Japan\",\"3\":\"313.70\",\"4\":\"357.85\"},{\"1\":\"BENFEITO Meaghan\",\"2\":\"Canada\",\"3\":\"355.15\",\"4\":\"331.40\"},{\"1\":\"PAMG Pandelela\",\"2\":\"Malaysia\",\"3\":\"322.75\",\"4\":\"322.40\"},{\"1\":\"CHAMANDY Olivia\",\"2\":\"Canada\",\"3\":\"320.55\",\"4\":\"307.15\"},{\"1\":\"PARRATTO Jessica\",\"2\":\"USA\",\"3\":\"322.75\",\"4\":\"302.35\"},{\"1\":\"MURILLO URREA Carolina\",\"2\":\"Colombia\",\"3\":\"325.75\",\"4\":\"283.35\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\nDiving2017 %>% diffmean(data = ., Final ~ Semifinal, only.2 = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  318.7-313.7  320.55-318.7 322.75-320.55  325.5-322.75  325.75-325.5 \n       12.350       -63.050         5.225        85.125      -114.150 \n   346-325.75    355.15-346 360.85-355.15  367.5-360.85   382.8-367.5 \n      102.200       -54.150        28.600        31.950         4.050 \n```\n:::\n\n```{.r .cell-code}\nobs_diff_swim <- mean(~ Final - Semifinal, data = Diving2017)\nobs_diff_swim\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 11.975\n```\n:::\n:::\n\n\nHow would we formulate our Hypothesis?\n\n$$\nH_0: \\mu_{semifinal} = \\mu_{final}\\\\\nH_a: \\mu_{semifinal} \\ne \\mu_{final}\\\n$$\n\n\n::: {.cell hash='two-means_cache/html/unnamed-chunk-10_86421680e092499a1d59666c3004c690'}\n\n```{.r .cell-code}\npolarity <- c(rep(1, 6), rep(-1, 6)) \n# 12 +/- 1s, \n# 6 each to make sure there is equal probability\npolarity\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1]  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1\n```\n:::\n\n```{.r .cell-code}\nnull_dist_swim <- do(100000) *\n  mean(data = Diving2017,\n       ~ (Final - Semifinal) * resample(polarity, replace = TRUE))\n\nnull_dist_swim %>% head()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"mean\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"-6.566667\",\"_rn_\":\"1\"},{\"1\":\"8.808333\",\"_rn_\":\"2\"},{\"1\":\"3.783333\",\"_rn_\":\"3\"},{\"1\":\"-4.816667\",\"_rn_\":\"4\"},{\"1\":\"9.525000\",\"_rn_\":\"5\"},{\"1\":\"-2.025000\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\ngf_histogram(data = null_dist_swim, ~ mean) %>%\n  gf_vline(xintercept = obs_diff_swim, colour = \"red\")\n```\n\n::: {.cell-output-display}\n![](two-means_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n\n```{.r .cell-code}\ngf_ecdf(data = null_dist_swim, ~ mean) %>%\n  gf_vline(xintercept = obs_diff_swim, colour = \"red\")\n```\n\n::: {.cell-output-display}\n![](two-means_files/figure-html/unnamed-chunk-10-2.png){width=672}\n:::\n\n```{.r .cell-code}\nprop1(~ mean >= obs_diff_swim, data = null_dist_swim)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nprop_TRUE \n0.1287887 \n```\n:::\n:::\n\n\nHmm...so by generating 100000 shufflings of score differences, with\npolarities, it does appear that we can not only obtain the current\nobserved difference but even surpass it frequently. So it does seem that\nthere is no difference in means between Semi-Final and Final swimming\nscores.\n\n### Walmart vs Target\n\nIs there a difference in the price of Groceries sold by the two\nretailers Target and Walmart? The data set `Groceries` contains a sample\nof grocery items and their prices advertised on their respective web\nsites on one specific day.\n\na)  Inspect the data set, then explain why this is an example of matched\n    pairs data.\nb)  Compute summary statistics of the prices for each store.\nc)  Conduct a permutation test to determine whether or not there is a\n    difference in the mean prices.\nd)  Create a ~~histogram~~ bar-chart of the difference in prices. What\n    is unusual about Quaker Oats Life cereal?\ne)  Redo the hypothesis test without this observation. Do you reach the\n    same conclusion?\n\n\n::: {.cell hash='two-means_cache/html/unnamed-chunk-11_12483df4897fd0c3dbead7dc698e7608'}\n\n```{.r .cell-code}\ndata(\"Groceries\")\nGroceries <- Groceries %>% mutate(Product = stringr::str_squish(Product))\nhead(Groceries)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"Product\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"Size\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"Target\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Walmart\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Units\"],\"name\":[5],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"UnitType\"],\"name\":[6],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"Kellogg NutriGrain Bars\",\"2\":\"8 bars\",\"3\":\"2.50\",\"4\":\"2.78\",\"5\":\"8\",\"6\":\"bars\",\"_rn_\":\"1\"},{\"1\":\"Quaker Oats Life Cereal Original\",\"2\":\"18oz\",\"3\":\"3.19\",\"4\":\"6.01\",\"5\":\"18\",\"6\":\"oz\",\"_rn_\":\"2\"},{\"1\":\"General Mills Lucky Charms\",\"2\":\"11.50oz\",\"3\":\"3.19\",\"4\":\"2.98\",\"5\":\"11\",\"6\":\"oz\",\"_rn_\":\"3\"},{\"1\":\"Quaker Oats Old Fashioned\",\"2\":\"18oz\",\"3\":\"2.82\",\"4\":\"2.68\",\"5\":\"18\",\"6\":\"oz\",\"_rn_\":\"4\"},{\"1\":\"Nabisco Oreo Cookies\",\"2\":\"14.3oz\",\"3\":\"2.99\",\"4\":\"2.98\",\"5\":\"14\",\"6\":\"oz\",\"_rn_\":\"5\"},{\"1\":\"Nabisco Chips Ahoy\",\"2\":\"13oz\",\"3\":\"2.64\",\"4\":\"1.98\",\"5\":\"13\",\"6\":\"oz\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\ninspect(Groceries)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\ncategorical variables:  \n      name     class levels  n missing\n1  Product character     30 30       0\n2     Size character     24 30       0\n3    Units character     16 30       0\n4 UnitType character      3 30       0\n                                   distribution\n1 Annie's Macaroni & Cheese (3.3%) ...         \n2 18oz (10%), 12oz (6.7%) ...                  \n3 10 (10%), 15 (10%), 16 (10%) ...             \n4 oz (93.3%), bars (3.3%) ...                  \n\nquantitative variables:  \n     name   class  min     Q1 median    Q3  max     mean       sd  n missing\n1  Target numeric 0.99 1.8275  2.545 3.140 7.99 2.762333 1.582128 30       0\n2 Walmart numeric 1.00 1.7600  2.340 2.955 6.98 2.705667 1.560211 30       0\n```\n:::\n:::\n\n\nWe see that the comparison is to be made between two prices for the\n*same* product, and hence this is one more example of `paired data`, as\nin Case Study #4. Let us plot the prices for the products:\n\n\n::: {.cell hash='two-means_cache/html/unnamed-chunk-12_15de12eacf7fae70b9d620fb46a3df34'}\n\n```{.r .cell-code}\ngf_col(data = Groceries,\n       Target ~ Product,\n       fill = \"#0073C299\",\n       width = 0.5 ) %>% \n  gf_col(data = Groceries,\n         -Walmart ~ Product,\n         fill = \"#EFC00099\",\n         ylab = \"Prices\",\n         width = 0.5\n       ) %>% \n  gf_col(data = Groceries %>% filter(Product == \"Quaker Oats Life Cereal Original\"), \n         -Walmart ~ Product,\n         fill = \"red\", \n         width = 0.5) %>% \n  gf_theme(theme_classic()) %>%\n  gf_theme(ggplot2::theme(axis.text.x = element_text(\n    size = 8,\n    face = \"bold\",\n    vjust = 0,\n    hjust = 1\n  ))) %>% gf_theme(ggplot2::coord_flip())\n```\n\n::: {.cell-output-display}\n![](two-means_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nWe see that the price difference between Walmart and Target prices is\nhighest for the `Product` named `Quaker Oats Life Cereal Original`. Let\nus check the mean difference in prices:\n\n\n::: {.cell hash='two-means_cache/html/unnamed-chunk-13_d06d5697a29b0a2691a8b800b5a990d8'}\n\n```{.r .cell-code}\ndiffmean(data = Groceries, Walmart ~ Target, only.2 = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   1-0.99    1.22-1 1.42-1.22 1.49-1.42 1.59-1.49 1.62-1.59 1.79-1.62 1.94-1.79 \n-0.580000  0.170000  0.210000 -0.100000  0.190000  0.070000  0.180000  0.160000 \n1.99-1.94 2.12-1.99 2.39-2.12  2.5-2.39  2.59-2.5 2.64-2.59 2.79-2.64 2.82-2.79 \n 0.090000  0.010000  0.200000  0.600000 -0.200000 -0.600000  0.660000  0.040000 \n2.99-2.82 3.19-2.99 3.49-3.19 3.99-3.49 4.79-3.99 7.19-4.79 7.99-7.19 \n 0.220000  1.263333 -1.183333 -0.480000  2.290000  2.190000  0.000000 \n```\n:::\n\n```{.r .cell-code}\nobs_diff_price = mean( ~ Walmart - Target, data = Groceries)\nobs_diff_price\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -0.05666667\n```\n:::\n:::\n\n\nLet us perform the pair-wise permutation test on prices, by shuffling\nthe two store names:\n\n\n::: {.cell hash='two-means_cache/html/unnamed-chunk-14_ff32ad6390e7caa92f350749bb2030f8'}\n\n```{.r .cell-code}\npolarity <- c(rep(1, 15), rep(-1,15))\npolarity\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1]  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n[26] -1 -1 -1 -1 -1\n```\n:::\n\n```{.r .cell-code}\nnull_dist_price <- do(100000) * mean(data = Groceries, \n                                    ~(Walmart-Target) * resample(polarity,\n                                                    replace = TRUE))\nnull_dist_price %>% head()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"mean\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"-0.10733333\",\"_rn_\":\"1\"},{\"1\":\"-0.07866667\",\"_rn_\":\"2\"},{\"1\":\"-0.04533333\",\"_rn_\":\"3\"},{\"1\":\"-0.16000000\",\"_rn_\":\"4\"},{\"1\":\"0.06066667\",\"_rn_\":\"5\"},{\"1\":\"0.05533333\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\ngf_histogram(data = null_dist_price, ~mean) %>% \n  gf_vline(xintercept = obs_diff_price, colour = \"red\")\n```\n\n::: {.cell-output-display}\n![](two-means_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n\n```{.r .cell-code}\n2*(sum(null_dist_price >= obs_diff_price + 1)/(100000+1)) #P-value\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n:::\n\n\nDoes not seem to be any significant difference in prices...\n\nSuppose we knock off the Quaker Cereal data item...\n\n\n::: {.cell hash='two-means_cache/html/unnamed-chunk-15_7be0fb0640315f05a8197104912fb852'}\n\n```{.r .cell-code}\nwhich(Groceries$Product == \"Quaker Oats Life Cereal Original\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2\n```\n:::\n\n```{.r .cell-code}\nGroceries_less <- Groceries[-2,]\nGroceries_less\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"Product\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"Size\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"Target\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Walmart\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Units\"],\"name\":[5],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"UnitType\"],\"name\":[6],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"Kellogg NutriGrain Bars\",\"2\":\"8 bars\",\"3\":\"2.50\",\"4\":\"2.78\",\"5\":\"8\",\"6\":\"bars\",\"_rn_\":\"1\"},{\"1\":\"General Mills Lucky Charms\",\"2\":\"11.50oz\",\"3\":\"3.19\",\"4\":\"2.98\",\"5\":\"11\",\"6\":\"oz\",\"_rn_\":\"3\"},{\"1\":\"Quaker Oats Old Fashioned\",\"2\":\"18oz\",\"3\":\"2.82\",\"4\":\"2.68\",\"5\":\"18\",\"6\":\"oz\",\"_rn_\":\"4\"},{\"1\":\"Nabisco Oreo Cookies\",\"2\":\"14.3oz\",\"3\":\"2.99\",\"4\":\"2.98\",\"5\":\"14\",\"6\":\"oz\",\"_rn_\":\"5\"},{\"1\":\"Nabisco Chips Ahoy\",\"2\":\"13oz\",\"3\":\"2.64\",\"4\":\"1.98\",\"5\":\"13\",\"6\":\"oz\",\"_rn_\":\"6\"},{\"1\":\"Doritos Nacho Cheese Chips\",\"2\":\"10oz\",\"3\":\"3.99\",\"4\":\"2.50\",\"5\":\"10\",\"6\":\"oz\",\"_rn_\":\"7\"},{\"1\":\"Cheez-it Original Baked\",\"2\":\"21oz\",\"3\":\"4.79\",\"4\":\"4.79\",\"5\":\"21\",\"6\":\"oz\",\"_rn_\":\"8\"},{\"1\":\"Swiss Miss Hot Chocolate\",\"2\":\"10 count\",\"3\":\"1.49\",\"4\":\"1.28\",\"5\":\"10\",\"6\":\"count\",\"_rn_\":\"9\"},{\"1\":\"Tazo Chai Classic Latte Black Tea\",\"2\":\"32 oz\",\"3\":\"3.49\",\"4\":\"2.98\",\"5\":\"32\",\"6\":\"oz\",\"_rn_\":\"10\"},{\"1\":\"Annie's Macaroni & Cheese\",\"2\":\"6oz\",\"3\":\"1.79\",\"4\":\"1.72\",\"5\":\"6\",\"6\":\"oz\",\"_rn_\":\"11\"},{\"1\":\"Rice A Roni Chicken\",\"2\":\"6.9oz\",\"3\":\"1.00\",\"4\":\"1.00\",\"5\":\"6\",\"6\":\"oz\",\"_rn_\":\"12\"},{\"1\":\"Zatarain's Jambalaya Rice Mix\",\"2\":\"8oz\",\"3\":\"1.62\",\"4\":\"1.54\",\"5\":\"8\",\"6\":\"oz\",\"_rn_\":\"13\"},{\"1\":\"SPAM Original Lunch Meat\",\"2\":\"12oz\",\"3\":\"2.79\",\"4\":\"2.64\",\"5\":\"12\",\"6\":\"oz\",\"_rn_\":\"14\"},{\"1\":\"Campbell's Chicken Noodle Soup\",\"2\":\"10.75oz\",\"3\":\"0.99\",\"4\":\"1.58\",\"5\":\"10\",\"6\":\"oz\",\"_rn_\":\"15\"},{\"1\":\"Dinty Moore Hearty Meals Beef Stew\",\"2\":\"15oz\",\"3\":\"1.99\",\"4\":\"1.98\",\"5\":\"15\",\"6\":\"oz\",\"_rn_\":\"16\"},{\"1\":\"Hormel Chili with Beans\",\"2\":\"15oz\",\"3\":\"1.94\",\"4\":\"1.88\",\"5\":\"15\",\"6\":\"oz\",\"_rn_\":\"17\"},{\"1\":\"Dole Pineapple Chunks\",\"2\":\"20 oz\",\"3\":\"1.59\",\"4\":\"1.47\",\"5\":\"20\",\"6\":\"oz\",\"_rn_\":\"18\"},{\"1\":\"Skippy Creamy Peanut Butter\",\"2\":\"16.3oz\",\"3\":\"2.59\",\"4\":\"2.58\",\"5\":\"16\",\"6\":\"oz\",\"_rn_\":\"19\"},{\"1\":\"Smucker's Strawberry Preserve\",\"2\":\"18oz\",\"3\":\"2.99\",\"4\":\"2.84\",\"5\":\"18\",\"6\":\"oz\",\"_rn_\":\"20\"},{\"1\":\"Heinz Tomato Ketchup\",\"2\":\"32oz\",\"3\":\"2.99\",\"4\":\"2.88\",\"5\":\"32\",\"6\":\"oz\",\"_rn_\":\"21\"},{\"1\":\"Near East Couscous Toasted Pine Nuts mix\",\"2\":\"5.6oz\",\"3\":\"2.12\",\"4\":\"1.98\",\"5\":\"5\",\"6\":\"oz\",\"_rn_\":\"22\"},{\"1\":\"Barilla Angel Hair Pasta\",\"2\":\"16oz\",\"3\":\"1.42\",\"4\":\"1.38\",\"5\":\"16\",\"6\":\"oz\",\"_rn_\":\"23\"},{\"1\":\"Betty Crocker Super Moist Chocolate Fudge Cake Mix\",\"2\":\"15.25oz\",\"3\":\"1.22\",\"4\":\"1.17\",\"5\":\"15\",\"6\":\"oz\",\"_rn_\":\"24\"},{\"1\":\"Kraft Jet-Puffed Marshmllows\",\"2\":\"16oz\",\"3\":\"1.99\",\"4\":\"1.96\",\"5\":\"16\",\"6\":\"oz\",\"_rn_\":\"25\"},{\"1\":\"Dunkin' Donuts Original Blend Medium Roast Ground Coffee\",\"2\":\"12oz\",\"3\":\"7.19\",\"4\":\"6.98\",\"5\":\"12\",\"6\":\"oz\",\"_rn_\":\"26\"},{\"1\":\"Dove Promises Milk Chocolate\",\"2\":\"8.87oz\",\"3\":\"3.19\",\"4\":\"3.50\",\"5\":\"8\",\"6\":\"oz\",\"_rn_\":\"27\"},{\"1\":\"Skittles\",\"2\":\"41oz\",\"3\":\"7.99\",\"4\":\"6.98\",\"5\":\"41\",\"6\":\"oz\",\"_rn_\":\"28\"},{\"1\":\"Vlasic Kosher Dill Pickle Spears\",\"2\":\"24oz\",\"3\":\"2.39\",\"4\":\"2.18\",\"5\":\"24\",\"6\":\"oz\",\"_rn_\":\"29\"},{\"1\":\"Vlasic Old Fashioned Sauerkraut\",\"2\":\"32oz\",\"3\":\"1.99\",\"4\":\"1.97\",\"5\":\"32\",\"6\":\"oz\",\"_rn_\":\"30\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\nobs_diff_price_less = mean( ~ Walmart - Target, data = Groceries_less)\nobs_diff_price_less\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -0.1558621\n```\n:::\n\n```{.r .cell-code}\npolarity_less <- c(rep(1, 15), rep(-1,14)) # Due to resampling this small bias makes no difference\nnull_dist_price_less <- do(100000) * mean(data = Groceries_less, \n                                    ~(Walmart-Target) * resample(polarity_less,\n                                                    replace = TRUE))\nnull_dist_price_less %>% head()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"mean\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"0.06758621\",\"_rn_\":\"1\"},{\"1\":\"0.02137931\",\"_rn_\":\"2\"},{\"1\":\"-0.06620690\",\"_rn_\":\"3\"},{\"1\":\"0.07931034\",\"_rn_\":\"4\"},{\"1\":\"-0.03172414\",\"_rn_\":\"5\"},{\"1\":\"-0.01655172\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\ngf_histogram(data = null_dist_price_less, ~mean) %>% \n  gf_vline(xintercept = obs_diff_price_less, colour = \"red\")\n```\n\n::: {.cell-output-display}\n![](two-means_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n\n```{.r .cell-code}\n1- mean(null_dist_price_less >= obs_diff_price_less) #P-value\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.01535\n```\n:::\n:::\n\n\n## Conclusion\n\nIt should be fairly clear now that we can test for the equivalence of\ntwo means, using a very simple permutation tests. Given computing power,\nwe can always mechanize this test very quickly to get our results. And\nthat performing this test yields reliable results without having to rely\non any assumption relating to underlying distributions and so on.\n\n## References\n\n1.  Randall Pruim, Nicholas J. Horton, Daniel T. Kaplan, [*Start\n    Teaching with\n    R*](https://github.com/ProjectMOSAIC/LittleBooks/raw/master/Starting/MOSAIC-StartTeaching.pdf)\n2.  https://bcs.wiley.com/he-bcs/Books?action=index&itemId=111941654X&bcsId=11307\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../../../../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\r\n<script src=\"../../../../../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}