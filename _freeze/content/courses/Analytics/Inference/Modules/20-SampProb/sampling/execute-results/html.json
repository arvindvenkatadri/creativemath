{
  "hash": "144fe9fe55c5eab047b04bcdbf2ad4d5",
  "result": {
    "markdown": "---\ntitle: \"\\U0001F3B2 Samples, Populations, Statistics and Inference\"\nauthor: \"Arvind Venkatadri\"\ndate: 25/Nov/2022\ndate-modified: \"2023-05-28\"\norder: 20\nabstract: \"How much ~~Land~~ Data does a Man need?\"\nimage: preview.jpg\ncategories:\n- Sampling\n- Central Limit Theorem\n- Standard Error\n- Confidence Intervals\n---\n\n\n## {{< fa folder-open >}} Slides and Tutorials\n\n|                                                                                        |                                                                                                                     |                                                                                               |                                                                                        |\n|----------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------|\n| <a href=\"./files/sampling.qmd\"><i class=\"fa-brands                                     \n                                     fa-r-project\"></i> R Tutorial</a>                   | <a href=\"./files/sampling.ows\"> <iconify-icon icon=\"fluent-emoji:orange-circle\"></iconify-icon> Orange Tutorial</a> | <a href=\"./files/sampling.rda\"> <i class=\"fa-solid fa-person-rays\"></i> Radiant Tutorial</a>  | <a href=\"./files/data/qdd-data.zip\"> <i class=\"fa-solid fa-database\"></i> Datasets</a> |\n\n## {{< iconify noto-v1 package >}} Setting up R Packages\n\n\n::: {.cell hash='sampling_cache/html/setup_7aaad631b973cdac6863258f61aad66e'}\n\n```{.r .cell-code}\nset.seed(123456)\n\nlibrary(tidyverse) # Data Processing in R\nlibrary(mosaic) # Our workhorse for stats, sampling\nlibrary(ggformula) # Formula interface for graphs\n\n# load the NHANES data library\nlibrary(NHANES)\n\nlibrary(infer) # tidy workflow for statistical inference\nlibrary(gt) # Create tidy tables to report data\nlibrary(cowplot) # ggplot themes and stacking of plots\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='sampling_cache/html/unnamed-chunk-2_668d216e4de1001cdfef14471dbeb42b'}\n\n:::\n\n\n## What is a Population?\n\nA *population* is a collection of individuals or observations we are\ninterested in. This is also commonly denoted as a study population. We\nmathematically denote the population's size using upper-case `N`.\n\nA *population parameter* is some numerical summary about the population\nthat is unknown but you wish you knew. For example, when this quantity\nis a mean like the average height of all Bangaloreans, the population\nparameter of interest is the population mean.\n\n::: callout-important\nPopulations *P*arameters are usually indicated by Greek Letters.\n:::\n\nA *census* is an exhaustive enumeration or counting of all N individuals\nin the population. We do this in order to compute the population\nparameter's value exactly. Of note is that as the number N of\nindividuals in our population increases, conducting a census gets more\nexpensive (in terms of time, energy, and money).\n\n## What is a Sample?\n\nSampling is the act of collecting a sample from the population, which we\ngenerally do when we can't perform a census. We mathematically denote\nthe sample size using lower case `n`, as opposed to upper case N which\ndenotes the population's size. Typically the sample size n is much\nsmaller than the population size N. Thus sampling is a much cheaper\nalternative than performing a census.\n\nA **sample statistic**, also known as a *point estimate*, is a summary\nstatistic like a mean or standard deviation that is computed from a\nsample.\n\n## Why do we sample?\n\nBecause we cannot conduct a census ( not always ) --- and sometimes we\nwon't even know how big the population is --- we take samples. And we\n*still* want to do useful work for/with the population, after\n*estimating its parameters, an act of generalizing* from sample to\npopulation. So the question is, **can we estimate useful parameters of\nthe population, using just samples? Can point estimates serve as useful\nguides to population parameters?**\n\nThis act of generalizing from sample to population is at the heart of\n**statistical inference**.\n\n::: callout-important\n## Mnemonic\n\nNOTE: there is an\n[*alliterative*](https://www.grammarly.com/blog/alliteration/)\n[*mnemonic*](https://www.merriam-webster.com/dictionary/mnemonic) here:\n\n**S**amples have **S**tatistics; **P**opulations have **P**arameters.\n:::\n\n## Sampling\n\nWe will first execute some samples from a known dataset. We load up the\nNHANES dataset and inspect it.\n\n\n::: {.cell hash='sampling_cache/html/unnamed-chunk-3_9712405643f761d42e4ec1a9dc9ddcaf'}\n\n```{.r .cell-code}\ndata(\"NHANES\")\nmosaic::inspect(NHANES)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\ncategorical variables:  \n               name  class levels     n missing\n1          SurveyYr factor      2 10000       0\n2            Gender factor      2 10000       0\n3         AgeDecade factor      8  9667     333\n4             Race1 factor      5 10000       0\n5             Race3 factor      6  5000    5000\n6         Education factor      5  7221    2779\n7     MaritalStatus factor      6  7231    2769\n8          HHIncome factor     12  9189     811\n9           HomeOwn factor      3  9937      63\n10             Work factor      3  7771    2229\n11 BMICatUnder20yrs factor      4  1274    8726\n12          BMI_WHO factor      4  9603     397\n13         Diabetes factor      2  9858     142\n14        HealthGen factor      5  7539    2461\n15   LittleInterest factor      3  6667    3333\n16        Depressed factor      3  6673    3327\n17     SleepTrouble factor      2  7772    2228\n18       PhysActive factor      2  8326    1674\n19         TVHrsDay factor      7  4859    5141\n20       CompHrsDay factor      7  4863    5137\n21  Alcohol12PlusYr factor      2  6580    3420\n22         SmokeNow factor      2  3211    6789\n23         Smoke100 factor      2  7235    2765\n24        Smoke100n factor      2  7235    2765\n25        Marijuana factor      2  4941    5059\n26     RegularMarij factor      2  4941    5059\n27        HardDrugs factor      2  5765    4235\n28          SexEver factor      2  5767    4233\n29          SameSex factor      2  5768    4232\n30   SexOrientation factor      3  4842    5158\n31      PregnantNow factor      3  1696    8304\n                                    distribution\n1  2009_10 (50%), 2011_12 (50%)                 \n2  female (50.2%), male (49.8%)                 \n3   40-49 (14.5%),  0-9 (14.4%) ...             \n4  White (63.7%), Black (12%) ...               \n5  White (62.7%), Black (11.8%) ...             \n6  Some College (31.4%) ...                     \n7  Married (54.6%), NeverMarried (19.1%) ...    \n8  more 99999 (24.2%) ...                       \n9  Own (64.7%), Rent (33.1%) ...                \n10 Working (59.4%), NotWorking (36.6%) ...      \n11 NormWeight (63.2%), Obese (17.3%) ...        \n12 18.5_to_24.9 (30.3%) ...                     \n13 No (92.3%), Yes (7.7%)                       \n14 Good (39.2%), Vgood (33.3%) ...              \n15 None (76.5%), Several (16.9%) ...            \n16 None (78.6%), Several (15.1%) ...            \n17 No (74.6%), Yes (25.4%)                      \n18 Yes (55.8%), No (44.2%)                      \n19 2_hr (26.2%), 1_hr (18.2%) ...               \n20 0_to_1_hr (29%), 0_hrs (22.1%) ...           \n21 Yes (79.2%), No (20.8%)                      \n22 No (54.3%), Yes (45.7%)                      \n23 No (55.6%), Yes (44.4%)                      \n24 Non-Smoker (55.6%), Smoker (44.4%)           \n25 Yes (58.5%), No (41.5%)                      \n26 No (72.4%), Yes (27.6%)                      \n27 No (81.5%), Yes (18.5%)                      \n28 Yes (96.1%), No (3.9%)                       \n29 No (92.8%), Yes (7.2%)                       \n30 Heterosexual (95.8%), Bisexual (2.5%) ...    \n31 No (92.7%), Yes (4.2%) ...                   \n\nquantitative variables:  \n              name   class      min        Q1    median        Q3        max\n1               ID integer 51624.00 56904.500 62159.500 67039.000  71915.000\n2              Age integer     0.00    17.000    36.000    54.000     80.000\n3        AgeMonths integer     0.00   199.000   418.000   624.000    959.000\n4      HHIncomeMid integer  2500.00 30000.000 50000.000 87500.000 100000.000\n5          Poverty numeric     0.00     1.240     2.700     4.710      5.000\n6        HomeRooms integer     1.00     5.000     6.000     8.000     13.000\n7           Weight numeric     2.80    56.100    72.700    88.900    230.700\n8           Length numeric    47.10    75.700    87.000    96.100    112.200\n9         HeadCirc numeric    34.20    39.575    41.450    42.925     45.400\n10          Height numeric    83.60   156.800   166.000   174.500    200.400\n11             BMI numeric    12.88    21.580    25.980    30.890     81.250\n12           Pulse integer    40.00    64.000    72.000    82.000    136.000\n13        BPSysAve integer    76.00   106.000   116.000   127.000    226.000\n14        BPDiaAve integer     0.00    61.000    69.000    76.000    116.000\n15          BPSys1 integer    72.00   106.000   116.000   128.000    232.000\n16          BPDia1 integer     0.00    62.000    70.000    76.000    118.000\n17          BPSys2 integer    76.00   106.000   116.000   128.000    226.000\n18          BPDia2 integer     0.00    60.000    68.000    76.000    118.000\n19          BPSys3 integer    76.00   106.000   116.000   126.000    226.000\n20          BPDia3 integer     0.00    60.000    68.000    76.000    116.000\n21    Testosterone numeric     0.25    17.700    43.820   362.410   1795.600\n22      DirectChol numeric     0.39     1.090     1.290     1.580      4.030\n23         TotChol numeric     1.53     4.110     4.780     5.530     13.650\n24       UrineVol1 integer     0.00    50.000    94.000   164.000    510.000\n25      UrineFlow1 numeric     0.00     0.403     0.699     1.221     17.167\n26       UrineVol2 integer     0.00    52.000    95.000   171.750    409.000\n27      UrineFlow2 numeric     0.00     0.475     0.760     1.513     13.692\n28     DiabetesAge integer     1.00    40.000    50.000    58.000     80.000\n29 DaysPhysHlthBad integer     0.00     0.000     0.000     3.000     30.000\n30 DaysMentHlthBad integer     0.00     0.000     0.000     4.000     30.000\n31    nPregnancies integer     1.00     2.000     3.000     4.000     32.000\n32         nBabies integer     0.00     2.000     2.000     3.000     12.000\n33      Age1stBaby integer    14.00    19.000    22.000    26.000     39.000\n34   SleepHrsNight integer     2.00     6.000     7.000     8.000     12.000\n35  PhysActiveDays integer     1.00     2.000     3.000     5.000      7.000\n36   TVHrsDayChild integer     0.00     1.000     2.000     3.000      6.000\n37 CompHrsDayChild integer     0.00     0.000     1.000     6.000      6.000\n38      AlcoholDay integer     1.00     1.000     2.000     3.000     82.000\n39     AlcoholYear integer     0.00     3.000    24.000   104.000    364.000\n40        SmokeAge integer     6.00    15.000    17.000    19.000     72.000\n41   AgeFirstMarij integer     1.00    15.000    16.000    19.000     48.000\n42     AgeRegMarij integer     5.00    15.000    17.000    19.000     52.000\n43          SexAge integer     9.00    15.000    17.000    19.000     50.000\n44 SexNumPartnLife integer     0.00     2.000     5.000    12.000   2000.000\n45  SexNumPartYear integer     0.00     1.000     1.000     1.000     69.000\n           mean           sd     n missing\n1  6.194464e+04 5.871167e+03 10000       0\n2  3.674210e+01 2.239757e+01 10000       0\n3  4.201239e+02 2.590431e+02  4962    5038\n4  5.720617e+04 3.302028e+04  9189     811\n5  2.801844e+00 1.677909e+00  9274     726\n6  6.248918e+00 2.277538e+00  9931      69\n7  7.098180e+01 2.912536e+01  9922      78\n8  8.501602e+01 1.370503e+01   543    9457\n9  4.118068e+01 2.311483e+00    88    9912\n10 1.618778e+02 2.018657e+01  9647     353\n11 2.666014e+01 7.376579e+00  9634     366\n12 7.355973e+01 1.215542e+01  8563    1437\n13 1.181550e+02 1.724817e+01  8551    1449\n14 6.748006e+01 1.435480e+01  8551    1449\n15 1.190902e+02 1.749636e+01  8237    1763\n16 6.827826e+01 1.378078e+01  8237    1763\n17 1.184758e+02 1.749133e+01  8353    1647\n18 6.766455e+01 1.441978e+01  8353    1647\n19 1.179292e+02 1.717719e+01  8365    1635\n20 6.729874e+01 1.495839e+01  8365    1635\n21 1.978980e+02 2.265045e+02  4126    5874\n22 1.364865e+00 3.992581e-01  8474    1526\n23 4.879220e+00 1.075583e+00  8474    1526\n24 1.185161e+02 9.033648e+01  9013     987\n25 9.792946e-01 9.495143e-01  8397    1603\n26 1.196759e+02 9.016005e+01  1478    8522\n27 1.149372e+00 1.072948e+00  1476    8524\n28 4.842289e+01 1.568050e+01   629    9371\n29 3.334838e+00 7.400700e+00  7532    2468\n30 4.126493e+00 7.832971e+00  7534    2466\n31 3.026882e+00 1.795341e+00  2604    7396\n32 2.456954e+00 1.315227e+00  2416    7584\n33 2.264968e+01 4.772509e+00  1884    8116\n34 6.927531e+00 1.346729e+00  7755    2245\n35 3.743513e+00 1.836358e+00  4663    5337\n36 1.938744e+00 1.434431e+00   653    9347\n37 2.197550e+00 2.516667e+00   653    9347\n38 2.914123e+00 3.182672e+00  4914    5086\n39 7.510165e+01 1.030337e+02  5922    4078\n40 1.782662e+01 5.326660e+00  3080    6920\n41 1.702283e+01 3.895010e+00  2891    7109\n42 1.769107e+01 4.806103e+00  1366    8634\n43 1.742870e+01 3.716551e+00  5540    4460\n44 1.508507e+01 5.784643e+01  5725    4275\n45 1.342330e+00 2.782688e+00  4928    5072\n```\n:::\n:::\n\n\nLet us create a NHANES dataset without duplicated IDs and only adults:\n\n\n::: {.cell hash='sampling_cache/html/unnamed-chunk-4_e1cd90295b3b8d7bf1af875ba807fded'}\n\n```{.r .cell-code}\nNHANES <-\n  NHANES %>%\n  distinct(ID, .keep_all = TRUE) \n\n#create a dataset of only adults\nNHANES_adult <-  NHANES %>%\n  filter(Age >= 18) %>%\n  drop_na(Height)\n\nglimpse(NHANES_adult)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 4,790\nColumns: 76\n$ ID               <int> 51624, 51630, 51647, 51654, 51656, 51657, 51666, 5166…\n$ SurveyYr         <fct> 2009_10, 2009_10, 2009_10, 2009_10, 2009_10, 2009_10,…\n$ Gender           <fct> male, female, female, male, male, male, female, male,…\n$ Age              <int> 34, 49, 45, 66, 58, 54, 58, 50, 33, 60, 56, 57, 54, 3…\n$ AgeDecade        <fct>  30-39,  40-49,  40-49,  60-69,  50-59,  50-59,  50-5…\n$ AgeMonths        <int> 409, 596, 541, 795, 707, 654, 700, 603, 404, 721, 677…\n$ Race1            <fct> White, White, White, White, White, White, Mexican, Wh…\n$ Race3            <fct> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ Education        <fct> High School, Some College, College Grad, Some College…\n$ MaritalStatus    <fct> Married, LivePartner, Married, Married, Divorced, Mar…\n$ HHIncome         <fct> 25000-34999, 35000-44999, 75000-99999, 25000-34999, m…\n$ HHIncomeMid      <int> 30000, 40000, 87500, 30000, 100000, 70000, 87500, 175…\n$ Poverty          <dbl> 1.36, 1.91, 5.00, 2.20, 5.00, 2.20, 2.03, 1.24, 1.27,…\n$ HomeRooms        <int> 6, 5, 6, 5, 10, 6, 10, 4, 11, 5, 10, 9, 3, 6, 6, 10, …\n$ HomeOwn          <fct> Own, Rent, Own, Own, Rent, Rent, Rent, Rent, Own, Own…\n$ Work             <fct> NotWorking, NotWorking, Working, NotWorking, Working,…\n$ Weight           <dbl> 87.4, 86.7, 75.7, 68.0, 78.4, 74.7, 57.5, 84.1, 93.8,…\n$ Length           <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ HeadCirc         <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ Height           <dbl> 164.7, 168.4, 166.7, 169.5, 181.9, 169.4, 148.1, 177.…\n$ BMI              <dbl> 32.22, 30.57, 27.24, 23.67, 23.69, 26.03, 26.22, 26.6…\n$ BMICatUnder20yrs <fct> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ BMI_WHO          <fct> 30.0_plus, 30.0_plus, 25.0_to_29.9, 18.5_to_24.9, 18.…\n$ Pulse            <int> 70, 86, 62, 60, 62, 76, 94, 74, 96, 84, 64, 70, 64, 6…\n$ BPSysAve         <int> 113, 112, 118, 111, 104, 134, 127, 142, 128, 152, 95,…\n$ BPDiaAve         <int> 85, 75, 64, 63, 74, 85, 83, 68, 74, 100, 69, 89, 41, …\n$ BPSys1           <int> 114, 118, 106, 124, 108, 136, NA, 138, 126, 154, 94, …\n$ BPDia1           <int> 88, 82, 62, 64, 76, 86, NA, 66, 80, 98, 74, 82, 48, 8…\n$ BPSys2           <int> 114, 108, 118, 108, 104, 132, 134, 142, 128, 150, 94,…\n$ BPDia2           <int> 88, 74, 68, 62, 72, 88, 82, 74, 74, 98, 70, 88, 42, 8…\n$ BPSys3           <int> 112, 116, 118, 114, 104, 136, 120, 142, NA, 154, 96, …\n$ BPDia3           <int> 82, 76, 60, 64, 76, 82, 84, 62, NA, 102, 68, 90, 40, …\n$ Testosterone     <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ DirectChol       <dbl> 1.29, 1.16, 2.12, 0.67, 0.96, 1.16, 1.14, 1.06, 0.91,…\n$ TotChol          <dbl> 3.49, 6.70, 5.82, 4.99, 4.24, 6.41, 4.78, 5.22, 5.59,…\n$ UrineVol1        <int> 352, 77, 106, 113, 163, 215, 29, 64, 155, 238, 26, 13…\n$ UrineFlow1       <dbl> NA, 0.094, 1.116, 0.489, NA, 0.903, 0.299, 0.190, 0.5…\n$ UrineVol2        <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 86, NA, NA, N…\n$ UrineFlow2       <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 0.43, NA, NA,…\n$ Diabetes         <fct> No, No, No, No, No, No, No, No, No, No, No, No, No, N…\n$ DiabetesAge      <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ HealthGen        <fct> Good, Good, Vgood, Vgood, Vgood, Fair, NA, Good, Fair…\n$ DaysPhysHlthBad  <int> 0, 0, 0, 10, 0, 4, NA, 0, 3, 7, 3, 0, 0, 3, 0, 2, 0, …\n$ DaysMentHlthBad  <int> 15, 10, 3, 0, 0, 0, NA, 0, 7, 0, 0, 0, 0, 4, 0, 30, 0…\n$ LittleInterest   <fct> Most, Several, None, None, None, None, NA, None, Seve…\n$ Depressed        <fct> Several, Several, None, None, None, None, NA, None, N…\n$ nPregnancies     <int> NA, 2, 1, NA, NA, NA, NA, NA, NA, NA, 4, 2, NA, NA, N…\n$ nBabies          <int> NA, 2, NA, NA, NA, NA, NA, NA, NA, NA, 3, 2, NA, NA, …\n$ Age1stBaby       <int> NA, 27, NA, NA, NA, NA, NA, NA, NA, NA, 26, 32, NA, N…\n$ SleepHrsNight    <int> 4, 8, 8, 7, 5, 4, 5, 7, 6, 6, 7, 8, 6, 5, 6, 4, 5, 7,…\n$ SleepTrouble     <fct> Yes, Yes, No, No, No, Yes, No, No, No, Yes, No, No, Y…\n$ PhysActive       <fct> No, No, Yes, Yes, Yes, Yes, Yes, Yes, No, No, Yes, Ye…\n$ PhysActiveDays   <int> NA, NA, 5, 7, 5, 1, 2, 7, NA, NA, 7, 3, 3, NA, 2, NA,…\n$ TVHrsDay         <fct> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CompHrsDay       <fct> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ TVHrsDayChild    <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CompHrsDayChild  <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ Alcohol12PlusYr  <fct> Yes, Yes, Yes, Yes, Yes, Yes, NA, No, Yes, Yes, Yes, …\n$ AlcoholDay       <int> NA, 2, 3, 1, 2, 6, NA, NA, 3, 6, 1, 1, 2, NA, 12, NA,…\n$ AlcoholYear      <int> 0, 20, 52, 100, 104, 364, NA, 0, 104, 36, 12, 312, 15…\n$ SmokeNow         <fct> No, Yes, NA, No, NA, NA, Yes, NA, No, No, NA, No, NA,…\n$ Smoke100         <fct> Yes, Yes, No, Yes, No, No, Yes, No, Yes, Yes, No, Yes…\n$ Smoke100n        <fct> Smoker, Smoker, Non-Smoker, Smoker, Non-Smoker, Non-S…\n$ SmokeAge         <int> 18, 38, NA, 13, NA, NA, 17, NA, NA, 16, NA, 18, NA, N…\n$ Marijuana        <fct> Yes, Yes, Yes, NA, Yes, Yes, NA, No, No, NA, No, Yes,…\n$ AgeFirstMarij    <int> 17, 18, 13, NA, 19, 15, NA, NA, NA, NA, NA, 18, NA, N…\n$ RegularMarij     <fct> No, No, No, NA, Yes, Yes, NA, No, No, NA, No, No, No,…\n$ AgeRegMarij      <int> NA, NA, NA, NA, 20, 15, NA, NA, NA, NA, NA, NA, NA, N…\n$ HardDrugs        <fct> Yes, Yes, No, No, Yes, Yes, NA, No, No, No, No, No, N…\n$ SexEver          <fct> Yes, Yes, Yes, Yes, Yes, Yes, NA, Yes, Yes, Yes, Yes,…\n$ SexAge           <int> 16, 12, 13, 17, 22, 12, NA, NA, 27, 20, 20, 18, 14, 2…\n$ SexNumPartnLife  <int> 8, 10, 20, 15, 7, 100, NA, 9, 1, 1, 2, 5, 20, 1, 20, …\n$ SexNumPartYear   <int> 1, 1, 0, NA, 1, 1, NA, 1, 1, NA, 1, 1, 2, 1, 3, 1, NA…\n$ SameSex          <fct> No, Yes, Yes, No, No, No, NA, No, No, No, No, No, No,…\n$ SexOrientation   <fct> Heterosexual, Heterosexual, Bisexual, NA, Heterosexua…\n$ PregnantNow      <fct> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n```\n:::\n:::\n\n\n::: callout-important\n## Population\n\nFor now, we will **treat** this dataset as our **Population**. So each\nvariable in the dataset is a population for that particular\nquantity/category, with appropriate *population parameters* such as\nmeans, sd-s, and proportions.\n:::\n\nLet us calculate the **population parameters** for the `Height` data:\n\n\n::: {.cell hash='sampling_cache/html/unnamed-chunk-5_7806e910669eb32ce2948e1fa556b910'}\n\n```{.r .cell-code}\npop_mean_height <- mean(~ Height, data = NHANES_adult)\npop_sd_height <- sd(~ Height, data = NHANES_adult)\n\npop_mean_height\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 168.3497\n```\n:::\n\n```{.r .cell-code}\npop_sd_height\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 10.15705\n```\n:::\n:::\n\n\nNow, we will sample **ONCE** from the NHANES `Height` variable. Let us\ntake a sample of `sample size` 50. We will compare **sample statistics**\nwith **population parameters** on the basis of this ONE sample of 50:\n\n\n::: {.cell fig.id='true' hash='sampling_cache/html/unnamed-chunk-6_b795ff41cafaee3783774ce1593ad335'}\n\n```{.r .cell-code}\nsample_height <- sample(NHANES_adult, size = 50) %>% \n  select(Height)\nsample_height\n```\n\n::: {.cell-output-display}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"Height\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"172.0\"},{\"1\":\"158.8\"},{\"1\":\"171.0\"},{\"1\":\"159.1\"},{\"1\":\"156.5\"},{\"1\":\"170.2\"},{\"1\":\"148.5\"},{\"1\":\"158.7\"},{\"1\":\"167.7\"},{\"1\":\"158.6\"},{\"1\":\"154.3\"},{\"1\":\"173.9\"},{\"1\":\"154.2\"},{\"1\":\"164.3\"},{\"1\":\"174.9\"},{\"1\":\"149.4\"},{\"1\":\"164.2\"},{\"1\":\"168.1\"},{\"1\":\"171.9\"},{\"1\":\"180.1\"},{\"1\":\"156.5\"},{\"1\":\"157.6\"},{\"1\":\"162.7\"},{\"1\":\"176.0\"},{\"1\":\"167.5\"},{\"1\":\"157.9\"},{\"1\":\"150.2\"},{\"1\":\"164.0\"},{\"1\":\"162.6\"},{\"1\":\"171.2\"},{\"1\":\"180.6\"},{\"1\":\"167.1\"},{\"1\":\"188.5\"},{\"1\":\"185.1\"},{\"1\":\"137.3\"},{\"1\":\"161.5\"},{\"1\":\"185.3\"},{\"1\":\"188.3\"},{\"1\":\"153.7\"},{\"1\":\"160.8\"},{\"1\":\"157.3\"},{\"1\":\"174.2\"},{\"1\":\"178.8\"},{\"1\":\"169.1\"},{\"1\":\"151.8\"},{\"1\":\"171.5\"},{\"1\":\"160.8\"},{\"1\":\"160.0\"},{\"1\":\"176.9\"},{\"1\":\"182.1\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n\nSingle-Sample Mean and Population Mean\n:::\n\n```{.r .cell-code}\nsample_mean_height <- mean(~ Height, data = sample_height)\nsample_mean_height\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 165.866\n```\n:::\n\n```{.r .cell-code}\n# Plotting the histogram of this sample\nsample_height %>% \n  gf_histogram(~ Height) %>% \n  \n  gf_vline(xintercept = sample_mean_height, \n           color = \"red\") %>% \n  \n  gf_vline(xintercept = pop_mean_height, \n           colour = \"blue\") %>% \n  \n  gf_text(1 ~ (pop_mean_height + 5), \n          label = \"Population Mean Height\", \n          color = \"blue\") %>% \n  \n  gf_text(2 ~ (sample_mean_height-5), \n          label = \"Sample Mean Height\", color = \"red\")  %>% \n  \n  gf_theme(theme = theme_minimal)\n```\n\n::: {.cell-output-display}\n![Single-Sample Mean and Population Mean](sampling_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nOK, so the `sample_mean_height` is not too far from the\n`pop_mean_height`. Is this always true? Let us check: we will create 500\nsamples each of size 50. And calculate their mean as the *sample\nstatistic*, giving us a dataframe containing 5000 `sample means`. We\nwill then compare if these 500 means are close to the `pop_mean_height`:\n\n\n::: {.cell layout-align=\"center\" hash='sampling_cache/html/unnamed-chunk-7_4b92556c1f673b0bf3b6bea4d58b57c8'}\n\n```{.r .cell-code}\nsample_height_500 <- do(500) * {\n  sample(NHANES_adult, size = 50) %>%\n    select(Height) %>%\n    summarise(\n      sample_mean_500 = mean(Height),\n      sample_min_500 = min(Height),\n      sample_max_500 = max(Height))\n}\n\nhead(sample_height_500)\n```\n\n::: {.cell-output-display}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"sample_mean_500\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"sample_min_500\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"sample_max_500\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\".row\"],\"name\":[4],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\".index\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"166.988\",\"2\":\"144.4\",\"3\":\"188.8\",\"4\":\"1\",\"5\":\"1\"},{\"1\":\"166.738\",\"2\":\"143.6\",\"3\":\"190.8\",\"4\":\"1\",\"5\":\"2\"},{\"1\":\"166.864\",\"2\":\"146.6\",\"3\":\"192.3\",\"4\":\"1\",\"5\":\"3\"},{\"1\":\"170.150\",\"2\":\"146.6\",\"3\":\"191.6\",\"4\":\"1\",\"5\":\"4\"},{\"1\":\"169.000\",\"2\":\"149.3\",\"3\":\"190.7\",\"4\":\"1\",\"5\":\"5\"},{\"1\":\"165.948\",\"2\":\"143.4\",\"3\":\"188.3\",\"4\":\"1\",\"5\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n\nMultiple Sample-Means and Population Mean\n:::\n\n```{.r .cell-code}\ndim(sample_height_500)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 500   5\n```\n:::\n\n```{.r .cell-code}\nsample_height_500 %>%\n  gf_point(.index ~ sample_mean_500, color = \"red\") %>%\n  \n  gf_segment(\n    .index + .index ~ sample_min_500 + sample_max_500,\n    color = \"red\",\n    size = 0.3,\n    alpha = 0.3,\n    ylab = \"Sample Index (1-500)\",\n    xlab = \"Sample Means\"\n  ) %>%\n  \n  gf_vline(xintercept = ~ pop_mean_height, \n           color = \"blue\") %>%\n  \n  gf_label(-15 ~ pop_mean_height, label = \"Population Mean\", \n           color = \"blue\") %>% \n  \n  gf_theme(theme = theme_minimal)\n```\n\n::: {.cell-output-display}\n![Multiple Sample-Means and Population Mean](sampling_files/figure-html/unnamed-chunk-7-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nThe `sample_mean`s (red dots), are themselves random because the samples\nare random, of course. It appears that they are generally in the\nvicinity of the `pop_mean` (blue line).\n\n### Distribution of Sample-Means\n\nSince the **sample-means** are themselves random variables, let's plot\nthe **distribution** of these 5000 sample-means themselves, called a **a\ndistribution of sample-means**.\n\n::: Note\nNOTE: this **a distribution of sample-means** will *itself* have a mean\nand standard deviation. Do not get confused ;-D\n:::\n\nWe will also plot the position of the population mean `pop_mean_height`\nparameter, the means of the `Height` variable.\n\n\n::: {.cell hash='sampling_cache/html/Sampling-Mean-Distribution_cc758f2f820d545d2754512e04c0de87'}\n\n```{.r .cell-code}\nsample_height_500 %>% \n  gf_dhistogram(~ sample_mean_500) %>% \n  \n  gf_vline(xintercept = pop_mean_height, \n           color = \"blue\") %>% \n  \n   gf_label(0.01 ~ pop_mean_height, \n            label = \"Population Mean\", \n            color = \"blue\")\n```\n\n::: {.cell-output-display}\n![Sampling Mean Distribution](sampling_files/figure-html/Sampling-Mean-Distribution-1.png){width=672}\n:::\n:::\n\n\nHow does this **distribution of sample-means** compare with that of the\noverall distribution of the population?\n\n\n::: {.cell hash='sampling_cache/html/unnamed-chunk-9_b94e7a06c42c9af99f0d3564036bebc3'}\n\n```{.r .cell-code}\nsample_height_500 %>% \n  gf_dhistogram(~ sample_mean_500) %>% \n  \n  gf_vline(xintercept = pop_mean_height, \n           color = \"blue\") %>% \n  \n   gf_label(0.01 ~ pop_mean_height, \n            label = \"Population Mean\", \n            color = \"blue\") %>% \n\n  ## Add the population histogram\n  gf_histogram(~ Height, data = NHANES_adult, \n               alpha = 0.2, fill = \"blue\", \n               bins = 50) %>% \n  \n  gf_label(0.025 ~ (pop_mean_height + 20), \n           label = \"Population Distribution\", color = \"blue\")\n```\n\n::: {.cell-output-display}\n![Sampling Means and Population Distributions](sampling_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n### Central limit theorem\n\nWe see in the Figure above that\n\n-   the *distribution of sample-means* is centered around mean =\n    `pop_mean`.\n\n-   That the standard deviation of the *distribution of sample means* is\n    less than that of the original population. But exactly what is it?\n\n-   And what is the kind of distribution?\n\nOne more experiment.\n\nNow let's repeatedly sample `Height` and compute the sample mean, and\nlook at the resulting histograms and Q-Q plots. ( Q-Q plots check\nwhether a certain distribution is close to being normal or not.)\n\nWe will use sample sizes of `c(16, 32, 64, 128)` and generate 1000\nsamples each time, take the means and plot these 1000 means:\n\n\n::: {.cell hash='sampling_cache/html/unnamed-chunk-10_026051157a38937e27ac1de64d472231'}\n\n```{.r .cell-code}\nset.seed(12345)\n\n\nsamples_height_16 <- do(1000) * mean(resample(NHANES_adult$Height, size = 16))\n\nsamples_height_32 <- do(1000) * mean(resample(NHANES_adult$Height, size = 32))\n\nsamples_height_64 <- do(1000) * mean(resample(NHANES_adult$Height, size = 64))\n\nsamples_height_128 <- do(1000) * mean(resample(NHANES_adult$Height, size = 128))\n\n# Quick Check\nhead(samples_height_16)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"mean\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"168.0500\",\"_rn_\":\"1\"},{\"1\":\"166.1000\",\"_rn_\":\"2\"},{\"1\":\"165.5375\",\"_rn_\":\"3\"},{\"1\":\"167.5625\",\"_rn_\":\"4\"},{\"1\":\"165.2375\",\"_rn_\":\"5\"},{\"1\":\"169.0250\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\n### do(1000,) * mean(resample(NHANES_adult$Height, size = 16)) produces a data frame with a variable named mean.\n###\n```\n:::\n\n\nNow let's create separate `Q-Q plots` for the different sample sizes.\n\n\n::: {.cell hash='sampling_cache/html/unnamed-chunk-11_81e3112dc5028eb439440ce49ce28ee6'}\n\n```{.r .cell-code}\n# Now let's create separate Q-Q plots for the different sample sizes.\n#\np1 <- gf_qq( ~ mean,data = samples_height_16,\n             title = \"N = 16\", \n             color = \"cornsilk\") %>%\n  gf_qqline()\n\np2 <- gf_qq( ~ mean,data = samples_height_32,\n            title = \"N = 32\", \n            color = \"sienna\") %>%\n  gf_qqline()\n\np3 <- gf_qq( ~ mean,data = samples_height_32,\n            title = \"N = 64\", \n            color = \"tomato2\") %>%\n  gf_qqline()\n\np4 <- gf_qq( ~ mean,data = samples_height_128,\n            title = \"N = 128\", \n            color = \"violetred\") %>%\n  gf_qqline()\n\ncowplot::plot_grid(p1, p2, p3, p4)\n```\n\n::: {.cell-output-display}\n![](sampling_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nLet us plot their individual histograms to compare them:\n\n\n::: {.cell hash='sampling_cache/html/unnamed-chunk-12_a7b71a01eea8758bc341a2cdf38a4cb7'}\n::: {.cell-output-display}\n![](sampling_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nAnd if we overlay the histograms:\n\n\n::: {.cell hash='sampling_cache/html/unnamed-chunk-13_cc7a6eb0f1663fdb5521298b9aea4d37'}\n::: {.cell-output-display}\n![](sampling_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nThis shows that the results become more normally distributed (i.e.\nfollowing the straight line) as the samples get larger. Hence we learn\nthat:\n\n-   the sample-means are normally distributed around the *population\n    mean*. This is because when we sample from the population, many\n    values will be close to the *population mean*, and values far away\n    from the mean will be increasingly scarce.\n\n\n::: {.cell hash='sampling_cache/html/unnamed-chunk-14_717d8eaf462ff2d61c7acd289a466a86'}\n\n```{.r .cell-code}\nmean(~ mean, data  = samples_height_16)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 168.306\n```\n:::\n\n```{.r .cell-code}\nmean(~ mean, data  = samples_height_32)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 168.4349\n```\n:::\n\n```{.r .cell-code}\nmean(~ mean, data  = samples_height_64)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 168.3184\n```\n:::\n\n```{.r .cell-code}\nmean(~ mean, data  = samples_height_128)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 168.366\n```\n:::\n\n```{.r .cell-code}\npop_mean_height\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 168.3497\n```\n:::\n:::\n\n\n::: callout-note\n## Central Limit Theorem\n\nThis is the **Central Limit Theorem (CLT)**\n\n-   the sample-means become \"more normally distributed\" with sample\n    length, as shown by the (small but definite) improvements in the Q-Q\n    plots with sample-size.\n-   the sample-mean distributions narrow with sample length.\n-   This is regardless of the distribution of the *population* itself.\n:::\n\nThe `Height` variable seems to be normally distributed at population\nlevel. We will try other non-normal population variables as an\nexercise).\n\nAs we saw above, the standard deviations of the sample-mean\ndistributions reduce with sample size. In fact their SDs are defined by:\n\n`sd = pop_sd/sqrt(sample_size)` where sample-size here is one of\n`c(16,32,64,128)`\n\n\n::: {.cell hash='sampling_cache/html/unnamed-chunk-15_ae919172452480ddb1e2cb7b5bab2e1b'}\n\n```{.r .cell-code}\nsd(~ mean, data  = samples_height_16)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.578355\n```\n:::\n\n```{.r .cell-code}\nsd(~ mean, data  = samples_height_32)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.834979\n```\n:::\n\n```{.r .cell-code}\nsd(~ mean, data  = samples_height_64)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.280014\n```\n:::\n\n```{.r .cell-code}\nsd(~ mean, data  = samples_height_128)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9096318\n```\n:::\n:::\n\n\nThe standard deviation of the **sample-mean distribution** is called the\n**Standard Error**. This statistic derived from the sample, will help us\ninfer our population parameters with a precise estimate of the\n*uncertainty* involved.\n\n$$\nStandard\\ Error\\ \\pmb {se} = \\frac{population\\ sd}{\\sqrt[]{sample\\ size}} \\\\\\\n\\pmb {se} = \\frac{\\sigma}{\\sqrt[]{n}}\n$$\n\nIn our sampling experiments, the Standard Errors evaluate to:\n\n\n::: {.cell hash='sampling_cache/html/unnamed-chunk-16_a58412df148a9e433a85404aeed43b48'}\n\n```{.r .cell-code}\npop_sd_height <- sd(~ Height, data = NHANES_adult)\n\npop_sd_height/sqrt(16)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.539262\n```\n:::\n\n```{.r .cell-code}\npop_sd_height/sqrt(32)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.795529\n```\n:::\n\n```{.r .cell-code}\npop_sd_height/sqrt(64)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.269631\n```\n:::\n\n```{.r .cell-code}\npop_sd_height/sqrt(128)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8977646\n```\n:::\n:::\n\n\nAs seen, these are identical to the Standard Deviations of the\nindividual sample-mean distributions.\n\n## Confidence intervals\n\nWhen we work with samples, we want to be able to speak with a certain\ndegree of confidence about the **population mean**, based on the\nevaluation of **one** sample mean,not a whole large number of them. Give\nthat sample-means are normally distributed around the **population\nmeans**, we can say that $68\\%$ of *all possible sample-mean* lie within\n$\\pm SE$of the *population mean*; and further that $95 \\%$ of *all\npossible sample-mean* lie within $\\pm 1.5*SE$ of the *population mean*.\n\nThese two intervals $sample.mean \\pm SE$ and $sample.mean \\pm 1.5*SE$\nare called the **confidence intervals** for the population mean, at\nlevels $68\\%$ and $95 \\%$ probability respectively.\n\nThus if we want to estimate a *population mean*:\n\n-   we take one sample from the population\n-   we calculate the sample-mean\n-   we calculate the sample-sd\n-   we calculate the Standard Error as $\\frac{sample-sd}{\\sqrt[]{n}}$\n-   we calculate 95% confidence intervals for the *population mean*\n    based on the formula above.\n\n## References\n\n1.  Diez, David M & Barr, Christopher D & Çetinkaya-Rundel, Mine,\n    *OpenIntro Statistics*. <https://www.openintro.org/book/os/>\n\n2.  Stats Test Wizard.\n    <https://www.socscistatistics.com/tests/what_stats_test_wizard.aspx>\n\n3.  Diez, David M & Barr, Christopher D & Çetinkaya-Rundel, Mine:\n    *OpenIntro Statistics*. Available online\n    <https://www.openintro.org/book/os/>\n\n4.  Måns Thulin, *Modern Statistics with R: From wrangling and exploring\n    data to inference and predictive modelling*\n    <http://www.modernstatisticswithr.com/>\n\n5.  Jonas Kristoffer Lindeløv, Common statistical tests are linear\n    models (or: how to teach stats)\n    <https://lindeloev.github.io/tests-as-linear/>\n\n6.  CheatSheet\n    <https://lindeloev.github.io/tests-as-linear/linear_tests_cheat_sheet.pdf>\n\n7.  Common statistical tests are linear models: a work through by Steve\n    Doogue <https://steverxd.github.io/Stat_tests/>\n\n8.  Jeffrey Walker \"Elements of Statistical Modeling for Experimental\n    Biology\".\n    <https://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/>\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../../../../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\r\n<script src=\"../../../../../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}