{
  "hash": "3fb0c455e4607623e2eb8cc66bb5c65d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ndate: 23/Nov/2024\ndate-modified: \"2024-12-17\"\ntitle: \"MLPs and Backpropagation\"\norder: 40\nsummary: \ntags:\n- Neural Nets\n- Back Propagation\n- Gradient\n\n---\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n## {{< iconify icons8 idea >}} How does an MLP Learn?\n\n\n- **Training Set**: Training is over several known input-output pairs (\"training data\")\n- **Training Epoch**: For each input, the signals propagate forward until we have an output\n- **Error Calculation**: Output is compared with **desired output**, to calculate *error*\n- **Backpropagation**: Errors need to be *sent backward from the output* to input, where we unravel the error from layer $l$ to layer $l-1$. (like apportioning blame !!). \n- **Error-to-Cost**: How does error at a given neuron relate to overall Cost?\n- **Differentiate**: Evaluate the *effect* of each weight/bias on ~~the (apportioned) error~~ overall Cost. (Slope!!)\n- **Gradient Descent**: Adapt the weights/biases with a small step in the **opposite direction** to the slope\n\n\n### Assumptions\n\n1. **Training Error**: We can calculate overall training Cost as the average Cost taken over all input samples.\n$$\nC = \\frac{1}{n}*\\Sum_{x}C_x\n$$\n\n2. **Cost Function**: We will minimize the Cost function which is assumed to be a function of (all) outputs of a NN.\n\n3. **Global Minimum**: Cost function has a global minimum! (Bowl shaped surface which we can descend)\n\n\n## Here Comes the ~~Rain~~ Maths Again!\n\n1. Rosenblatt-Nielsen's Demon: \n  - messes/perturbs with input to the sigmoid function at a neuron. (Weighted Sum)\n  - Error = Slope * perturbation; \n  - However, **Error ~= Slope** when we allow that the perturbation is a fixed amplitude.\n  - Still a product of slopes ;-O\n  \n$$\n{\\delta_j}^L = \\frac{\\delta C}{\\delta {a_j}^L} * \\sigma ({z_j}^L)\n$${#eq-bp1}\n\n:::: {.columns}\n\n::: {.column width=\"48%\"}\n\n\n{{< video https://youtu.be/Ilg3gGewQ5U?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi >}}\n\n\n\n:::\n\n::: {.column width=\"4%\"}\n:::\n\n::: {.column width=\"48%\"}\n\n\n{{< video https://youtu.be/tIeHLnjs5U8?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi >}}\n\n\n\n:::\n::::\n\n## Neural Nets in Code\n\n::: {.panel-tabset .nav-pills style=\"background: whitesmoke;\"}\n\n### Using p5.js\n\n\n### Using R\nUsing `torch`.\n\n:::\n\n\n## References\n\n1. Tariq Rashid. *Make your own Neural Network*. [PDF Online](https://github.com/harshitkgupta/StudyMaterial/blob/master/Make%20Your%20Own%20Neural%20Network%20(Tariq%20Rashid)%20-%20%7BCHB%20Books%7D.pdf)\n1. Mathoverflow. *Intuitive Crutches for Higher Dimensional Thinking*. <https://mathoverflow.net/questions/25983/intuitive-crutches-for-higher-dimensional-thinking>\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../../../../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../../../../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}