{
  "hash": "3661134935806e5703abfab88721c7ed",
  "result": {
    "engine": "knitr",
    "markdown": "---\ndate: 23/Nov/2024\ndate-modified: \"2024-12-30\"\ntitle: \"The Multilayer Perceptron\"\norder: 30\nsummary: \ntags:\n- Neural Nets\n- Hidden Layers\nfilters:\n  - d2\nd2:\n  layout: elk\n  theme: \"CoolClassics\"\n  sketch: true\n---\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n## {{< iconify icons8 idea >}} What is a Multilayer Perceptron?\n\nThis was our bare bones Perceptron, or neuron as we will refer to it\nhenceforth:\n\n::: {.cell}\n\n```{.d2 .cell-code}\ndirection: right\ngrid-gap: 10\ntitle: The Perceptron {\n  shape: text\n  near: top-center\n  style: {\n    font-size: 30\n    italic: true\n  }\n}\nin1.style.opacity: 0\nin2.style.opacity: 0\nin3.style.opacity: 0\nin4.style.opacity: 0\nin5.style.opacity: 0\nin6.style.opacity: 0\n1.shape: circle\n2.shape: circle\n3.shape: circle\n4.shape: circle\n5.shape: circle\n6.shape: circle\nact.shape: circle\nact {\n  icon: ./carbon--chart-logistic-regression.svg\n}\nout.style.opacity: 0\n###\nin1 -> 1\nin2 -> 2\nin3 -> 3\nin4 -> 4\nin5 -> 5\nin6 -> 6\n\n1 -> act\n2 -> act\n3 -> act\n4 -> act\n5 -> act\n6 -> act\n\nact -> out\n\n```\n:::\n\n<br> We add (one or more ) **hidden layers** to setup a *Multilayer\nPerceptron*:\n\n::: {.cell}\n\n```{.d2 .cell-code}\ntitle: Multilayer Perceptron {\n  shape: text\n  near: top-center\n  style: {\n    font-size: 30\n    italic: true\n  }\n}\ndirection: right\ngrid-columns: 5\ngrid-rows: 3\n###\nin1: \"in1\" {shape: circle\n     style: {\n      font-size: 25\n      stroke: white\n      fill: white\n    }}\nin2: \"in2\" {shape: circle\n     style: {\n      font-size: 25\n      stroke: white\n      fill: white\n    }}\nin3: \"in3\" {shape: circle\n     style: {\n      font-size: 25\n      stroke: white\n      fill: white\n    }}\ni1: {shape: circle\n     style: {\n      font-size: 25\n      fill: white\n    }}\ni2: {shape: circle\n     style: {\n      font-size: 25\n      fill: white\n    }}\ni3: {shape: circle\n     style: {\n      font-size: 25\n      fill: white\n    }}\nh1: {shape: circle\n     style: {\n      stroke: blue\n      font-color: blue\n      font-size: 25\n      stroke-dash: 2\n      stroke-width: 6\n      fill: white\n    }}\nh2: {shape: circle\n     style: {\n      stroke: blue\n      font-color: blue\n      font-size: 25\n      stroke-dash: 2\n      stroke-width: 6\n      fill: white\n    }}\nh3: {shape: circle\n     style: {\n      stroke: blue\n      font-color: blue\n      font-size: 25\n      stroke-dash: 2\n      stroke-width: 6\n      fill: white\n    }}\no1: {shape: circle\n     style: {\n      font-size: 25\n      fill: white\n    }}\no2: {shape: circle\n     style: {\n      font-size: 25\n      fill: white\n    }}\no3: {shape: circle\n     style: {\n      font-size: 25\n      fill: white\n    }}\nout1: \"out1\" {shape: circle\n     style: {\n      font-size: 25\n      stroke: white\n      fill: white\n    }}\nout2: \"out2\" {shape: circle\n     style: {\n      font-size: 25\n      stroke: white\n      fill: white\n    }}\nout3: \"out3\" {shape: circle\n     style: {\n      font-size: 25\n      stroke: white\n      fill: white\n    }}\n###\nin1 -> i1\nin2 -> i2\nin3 -> i3\ni1 -> h1\ni1 -> h2\ni1 -> h3\ni2 -> h1\ni2 -> h2\ni2 -> h3\ni3 -> h1\ni3 -> h2\ni3 -> h3\nh1 -> o1\nh2 -> o1\nh3 -> o1\nh1 -> o2\nh2 -> o2\nh3 -> o2\nh1 -> o3\nh2 -> o3\nh3 -> o3\n\no1 -> out1\no2 -> out2\no3 -> out3\n\n```\n:::\n\n<br>\n\n-   Here, `i1`, `i2`, and `i3` are *input neurons*: they are simply\n    inputs and are drawn as circles in the literature.\n-   The `h1`, `h2`, `h3` are neuron in the so-called **hidden layer**;\n    *hidden* because they are not inputs!\n-   The neurons `o1`, `o2`, and `o3` are **output neurons**.\n-   The signals/information flows from left to right in the diagram. And\n    we have shown every neuron connected to everyone in the next layer\n    downstream.\n\nHow do we mathematically, and concisely, express the operation of the\nMLP? Let us setup a notation for the MLP weights.\n\n-   $l$ : layer index;\n-   $j$, $k$ : neuron index in two adjacent layers\n-   $W^l_{jk}$ (i.e. $W^{layer}_{{dest}~{source}}$) : weight from $k$th neuron / $(l−1)$th layer to $j$th\n    neuron / $l$th layer;\n-   $b^l_j$ : *bias* of the $j$th neuron in the $l$th layer.\n-   $a^l_j$ : activation (output) of $j$th neuron / $l$th layer.\n\n\n::: {.cell}\n\n```{.d2 .cell-code}\ntitle: Data Processing between Layers {\n  shape: text\n  near: top-center\n  style: {\n    font-size: 60\n    italic: true\n  }\n}\n\ndirection: right\ngrid-columns: 3\ngrid-gap: 400\n\nlayer-1: \"Layer-1 : Index(k)\" {\n  grid-columns: 1\n  grid-gap: 100\n  style: {\n      font-size: 30\n    }\n  1 {shape: circle\n     style: {\n      font-size: 45\n    }}\n  2 {shape: circle\n     style: {\n      font-size: 45\n    }}\n  3 {shape: circle\n     style: {\n      font-size: 45\n    }}\n}\nlayer-2: \"Layer-2 : Index(j)\" {\n  grid-columns: 1\n  grid-gap: 100\n  style: {\n      font-size: 30\n    }\n  h1 {shape: circle\n     style: {\n      font-size: 45\n    }}\n  h2 {shape: circle\n     style: {\n      font-size: 45\n    }}\n  h3 {shape: circle\n     style: {\n      font-size: 45\n    }}\n}\n\nlayer-3: {\n  grid-columns: 1\n  grid-gap: 100\n  style: {\n    opacity: 0\n  }\n    a21: \"a21\" {shape: circle\n     style: {\n      font-size: 45\n      stroke: white\n      fill: white\n    }}\n    a22: \"a22\" {shape: circle\n     style: {\n      font-size: 45\n      stroke: white\n      fill: white\n    }}\n    a23: \"a23\" {shape: circle\n     style: {\n      font-size: 45\n      stroke: white\n      fill: white\n     }\n     }\n}\nlayer-1.1 -> layer-2.h1: W11 {\n  source-arrowhead.label: a11\n  style: {\n    font-size: 45\n    fill: LightBlue\n    stroke: FireBrick\n    stroke-width: 9\n    animated: true\n  }\n}\nlayer-1.1 -> layer-2.h2 {style:{stroke: grey}}\nlayer-1.1 -> layer-2.h3 {style:{stroke: grey}}\nlayer-1.2 -> layer-2.h1: W12 {\n  source-arrowhead.label: a12\n  style: {\n    font-size: 45\n    fill: LightBlue\n    stroke: FireBrick\n    stroke-width: 9\n    animated: true\n  }\n}\nlayer-1.2 -> layer-2.h2 {style:{stroke: grey}}\nlayer-1.2 -> layer-2.h3 {style:{stroke: grey}}\nlayer-1.3 -> layer-2.h1: W13 {\n  source-arrowhead.label: a13\n  style: {\n    font-size: 45\n    fill: LightBlue\n    stroke: FireBrick\n    stroke-width: 9\n    animated: true\n  }\n}\nlayer-1.3 -> layer-2.h2 {style:{stroke: grey}}\nlayer-1.3 -> layer-2.h3 {style:{stroke: grey}}\n\nlayer-2.h1 -> layer-3.a21 { style: {stroke-width: 9\n         stroke: FireBrick}}\nlayer-2.h2 -> layer-3.a22 {style:{stroke: grey}}\nlayer-2.h3 -> layer-3.a23 {style:{stroke: grey}}\n\n```\n:::\n\n\n<br> We can write the outputs of the `layer-2` as:\n\n$$\n\\begin{align}\n(j = 1): ~ a^2_1 = sigmoid~(~\\color{red}{W^2_{11}*a^1_1} + \\color{skyblue}{W^2_{12}*a^1_2} + \\color{forestgreen}{W^2_{13}*a^1_3}~ + b^2_1)\\\\\n(j = 2): ~ a^2_2 = sigmoid~(~W^2_{21}*a^1_1 + W^2_{22}*a^1_2 + W^2_{23}*a^1_3~ + b^2_2 )\\\\\n(j = 3): ~ a^2_3 = sigmoid~(~W^2_{31}*a^1_1 + W^2_{32}*a^1_2 + W^2_{33}*a^1_3~ + b^2_3)\\\\\n\\end{align}\n$$ \n\nIn (dreaded?) matrix notation :\n\n$$\n\\begin{bmatrix}\na^2_1\\\\\na^2_2\\\\\na^2_3\\\\\n\\end{bmatrix} = \nsigmoid~\\Bigg(\n\\begin{bmatrix}\n\\color{red}{W^2_{11}} & \\color{skyblue}{W^2_{12}} & \\color{forestgreen}{W^2_{13}}\\\\\nW^2_{21} & W^2_{22} & W^2_{23}\\\\\nW^2_{31} & W^2_{32} & W^2_{33}\\\\\n\\end{bmatrix} * \n\\begin{bmatrix}\n\\color{red}{a^1_1}\\\\\n\\color{skyblue}{a^1_2}\\\\\n\\color{forestgreen}{a^1_3}\\\\\n\\end{bmatrix} +\n\\begin{bmatrix}\nb^2_1\\\\\nb^2_2\\\\\nb^2_3\\\\\n\\end{bmatrix}\n\\Bigg)\n$$ \n\n\nIn compact notation we write, in general:\n\n$$\nA^l = \\sigma\\Bigg(W^lA^{l-1} + B^l\\Bigg)\n$$\n\n\n\n$$\na^l_j=σ(\\sum_kW^l_{jk} * a^{l−1}_k+b^l_j)\n$$ {#eq-forward-prop}\n\n## MLPs in Code\n\n::: {.panel-tabset .nav-pills style=\"background: whitesmoke \"}\n### Using p5.js\n\n### Using R\n\nUsing `torch`.\n:::\n\n## References\n\n1.  Tariq Rashid. *Make your own Neural Network*. [PDF\n    Online](https://github.com/harshitkgupta/StudyMaterial/blob/master/Make%20Your%20Own%20Neural%20Network%20(Tariq%20Rashid)%20-%20%7BCHB%20Books%7D.pdf)\n2.  Mathoverflow. *Intuitive Crutches for Higher Dimensional Thinking*.\n    <https://mathoverflow.net/questions/25983/intuitive-crutches-for-higher-dimensional-thinking>\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../../../../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../../../../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}