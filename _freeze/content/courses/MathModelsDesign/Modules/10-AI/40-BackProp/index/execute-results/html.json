{
  "hash": "72198cfe041b8db4aacef0b2ca5fd234",
  "result": {
    "engine": "knitr",
    "markdown": "---\ndate: 23/Nov/2024\ndate-modified: \"2024-12-27\"\ntitle: \"MLPs and Backpropagation\"\norder: 30\nsummary: \ntags:\n- Neural Nets\n- Back Propagation\n- Gradient\n\n---\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n\n\n## {{< iconify icons8 idea >}} What is Backpropagation?\n\n- Recall that we use a **training dataset** that contains inputs and **known** outputs, to adapt the weights of neural network (NN).\n\n- Before we adapt these, we need to *take the error at the outputs** and affix responsibility, or *blame* to each of the weights in the NN.\n\n- This process of taking the output error and *sending it backward* through the network in a **proportionate manner** is called **Backpropagation** in the NN literature. It is one of the most important steps in training an NN. \n\n\n## Backpropagation Intuitively Demonstrated\n\n\n::: {.cell}\n\n```{.d2 .cell-code}\ndirection: right\ngrid-columns: 6\ngrid-rows: 3\n###\nin1.style.opacity: 0\nin2.style.opacity: 0\nin3.style.opacity: 0\n1.shape: circle\n2.shape: circle\n3.shape: circle\nh1.shape: circle\nh2.shape: circle\nh3.shape: circle\no1.shape: circle\n# o1 {\n#   icon: https://icons.terrastruct.com/infra/019-network.svg\n# }\no2.shape: circle\no3.shape: circle\nout1.style.opacity: 0\nout2.style.opacity: 0\nout3.style.opacity: 0\n###\nin1 -> 1\nin2 -> 2\nin3 -> 3\n1 -> h1: w21 {\n  style: {\n    stroke: deepskyblue}\n}\n1 -> h2: W21 {\n  style: {\n  fill: LightBlue\n  stroke: FireBrick\n  stroke-width: 2\n  animated: true\n  }\n}\n1 -> h3\n2 -> h1\n2 -> h2\n2 -> h3\n3 -> h1\n3 -> h2\n3 -> h3\nh1 -> o1\nh2 -> o1\nh3 -> o1\nh1 -> o2\nh2 -> o2\nh3 -> o2\nh1 -> o3\nh2 -> o3\nh3 -> o3\n\no1 -> out1\no2 -> out2\no3 -> out3\n\n```\n:::\n\n\n### Using `d2r`\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## library(d2r)\n# Helper function for file paths\nfig_path <- \\(x) {\n  paste0(knitr::opts_chunk$get(\"fig.path\"), x)\n}\n\noptions(\"d2r.pad\" = 20)\noptions(\"d2r.direction\" = \"right\")\nsimple_diagram <- d2_diagram(\n  c(\"R\" = \"D2\")\n)\nd2_include(\n  simple_diagram,\n  fig_path(\"simple_diagram.png\")\n)\n```\n:::\n\n\n\n\n\n\n\n\n\n\n## Here Comes the ~~Rain~~ Maths Again!\n\nNow, we are ready (maybe?) to watch these two very beautifully made videos on Backpropagation. One is of course from Dan Shiffman, and the other from Grant Sanderson a.ka. 3Blue1Brown.\n\n:::: {.columns}\n\n::: {.column width=\"48%\"}\n\n\n\n\n\n{{< video https://youtu.be/QJoa0JYaX1I?list=PLRqwX-V7Uu6Y7MdSCaIfsxc561QI0U0Tb >}}\n\n\n\n\n\n\n\n\n\n:::\n\n::: {.column width=\"4%\"}\n:::\n\n::: {.column width=\"48%\"}\n\n\n\n\n\n{{< video https://youtu.be/tIeHLnjs5U8?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi >}}\n\n\n\n\n\n\n\n\n\n:::\n::::\n\n## Backpropagation in Code\n\n::: {.panel-tabset .nav-pills style=\"background: whitesmoke \"}\n\n### Using p5.js\n\n\n### Using R\nUsing `torch`.\n\n:::\n\n\n## References\n\n1. Tariq Rashid. *Make your own Neural Network*. [PDF Online](https://github.com/harshitkgupta/StudyMaterial/blob/master/Make%20Your%20Own%20Neural%20Network%20(Tariq%20Rashid)%20-%20%7BCHB%20Books%7D.pdf)\n1. Mathoverflow. *Intuitive Crutches for Higher Dimensional Thinking*. <https://mathoverflow.net/questions/25983/intuitive-crutches-for-higher-dimensional-thinking>\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../../../../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../../../../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}