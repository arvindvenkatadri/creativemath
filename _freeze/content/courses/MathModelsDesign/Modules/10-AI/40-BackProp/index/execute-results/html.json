{
  "hash": "4051b2ca59ef2cd83f3d0534f18ac98e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ndate: 23/Nov/2024\ndate-modified: \"2024-12-28\"\ntitle: \"MLPs and Backpropagation\"\norder: 30\nsummary: \ntags:\n- Neural Nets\n- Back Propagation\n- Gradient\n\n---\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n\n### How does an MLP Learn?\n\n$$\n\\begin{bmatrix}\na^2_1\\\\\na^2_2\\\\\na^2_3\\\\\n\\end{bmatrix} = \nsigmoid~\\Bigg(\n\\begin{bmatrix}\n\\color{red}{W^2_{11}} & \\color{skyblue}{W^2_{21}} & \\color{forestgreen}{W^2_{31}}\\\\\nW^2_{12} & W^2_{22} & W^2_{32}\\\\\nW^2_{13} & W^2_{23} & W^2_{33}\\\\\n\\end{bmatrix} * \n\\begin{bmatrix}\n\\color{red}{a^1_1}\\\\\n\\color{skyblue}{a^1_2}\\\\\n\\color{forestgreen}{a^1_3}\\\\\n\\end{bmatrix} +\n\\begin{bmatrix}\nb^2_1\\\\\nb^2_2\\\\\nb^2_3\\\\\n\\end{bmatrix}\n\\Bigg)\n$$\n\nSee how the connections between neurons are marked by **weights**: these multiply the signal from the previous neuron. The multiplied/weighted products are added up in the neuron, and the sum is given to the activation block therein. \n\nSo learning?\n\nThe only controllable variables in a neural network are these weights! So learning involves adapting these wwights so that they can perform a sueful function. \n\n### What is the Learning Process?\n\nThe process of *adapting the weights* of a neural network can be described in the following steps:\n\n- **Training Set**: Training is over several known input-output pairs (\"training data\")\n- **Training Epoch**: For each input, the signals propagate forward until we have an output\n- **Error Calculation**: Output is compared with **desired output**, to calculate *error*\n- **Backpropagation**: Errors need to be *sent backward from the output* to input, where we unravel the error from layer $l$ to layer $l-1$. (like apportioning blame !!). \n- **Error-to-Cost**: How does error at *any* given neuron relate to the idea of an **overall Cost** function?\n- **Differentiate**: Evaluate the *effect* of each weight/bias on ~~the (apportioned) error~~ overall Cost. (Slope!!)\n- **Gradient Descent**: Adapt the weights/biases with a small step in the **opposite direction** to the slope \n\n\n## What is the Cost Function?\n\nWe define the **cost** or **objective** function as:\n\n$$\nC(W, b) = \\frac{1}{2n}\\sum^n_{k=1}(y(k) - d(k))^2\n$${#eq-cost-function}\n\nWe take each of the outputs $y(k)$ over $n$ training samples and $d(k) are the desired outputs for each of the training samples. \n\nThe Cost Function is of course dependent upon the *Weights* and the *biases*, and is to be minimized by adapting these. Using the sum of *squared errors*, along with the *linear* operations in the nn guarantees that the Cost Function (usually) has one global, minimum. \n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n## Assumptions in the Training Process\n\n1. **Cost Function**: We will minimize the Cost function which is assumed to be a function of (all) outputs of a NN.\n\n1. **Training Error**: We can calculate overall training Cost as the average Cost taken over all/several input samples.\n$$\nC = \\frac{1}{n}*\\sum_{x}C_x\n$$\nIn practice, the input samples are not presented one by one, but in batches, called *minbatches*. This is cheaper by way of computation since we adapt the weights on a per-batch basis, and also allows to perform *averaging* of errors over each batch. However is also seems to complicate the housekeeping necessary to manage batches. ( We will not, in the interest of time, deal with this idea more. It is dealt with in the references below. )\n\n1. **Global Minimum**: Cost function has a global minimum! (Bowl shaped surface which we can descend)\n\n\n## Here Comes the ~~Rain~~ Maths Again!\n\n1. Rosenblatt-Nielsen's Demon: \n  - messes/perturbs with input to the sigmoid function at a neuron. (Weighted Sum)\n  - Error = Slope * perturbation  \n  - However, **Error ~= Slope** when we allow that the perturbation is a fixed amplitude.\n  - Still a product of slopes  -O\n  \n$$\n{\\delta_j}^L = \\frac{\\delta C}{\\delta {a_j}^L} * \\sigma ({z_j}^L)\n$${#eq-bp1}\n\n:::: {.columns}\n\n::: {.column width=\"48%\"}\n\n\n\n{{< video https://youtu.be/Ilg3gGewQ5U?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi >}}\n\n\n\n\n\n:::\n\n::: {.column width=\"4%\"}\n:::\n\n::: {.column width=\"48%\"}\n\n\n\n{{< video https://youtu.be/tIeHLnjs5U8?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi >}}\n\n\n\n\n\n:::\n::::\n\n\n\n\n## {{< iconify icons8 idea >}} What is Backpropagation?\n\n- Recall that we use a **training dataset** that contains inputs and **known** outputs, to adapt the weights of neural network (NN).\n\n- Before we adapt these, we need to *take the error at the outputs** and affix responsibility, or *blame* to each of the weights in the NN.\n\n- This process of taking the output error and *sending it backward* through the network in a **proportionate manner** is called **Backpropagation** in the NN literature. It is one of the most important steps in training an NN. \n\n\n## Backpropagation Intuitively Demonstrated\n\n\n::: {.cell}\n\n```{.d2 .cell-code}\ndirection: right\ngrid-columns: 6\ngrid-rows: 3\n###\nin1.style.opacity: 0\nin2.style.opacity: 0\nin3.style.opacity: 0\n1.shape: circle\n2.shape: circle\n3.shape: circle\nh1.shape: circle\nh2.shape: circle\nh3.shape: circle\no1.shape: circle\n# o1 {\n#   icon: https://icons.terrastruct.com/infra/019-network.svg\n# }\no2.shape: circle\no3.shape: circle\nout1.style.opacity: 0\nout2.style.opacity: 0\nout3.style.opacity: 0\n###\nin1 -> 1\nin2 -> 2\nin3 -> 3\n1 -> h1: w21 {\n  style: {\n    stroke: deepskyblue}\n}\n1 -> h2: W21 {\n  style: {\n  fill: LightBlue\n  stroke: FireBrick\n  stroke-width: 2\n  animated: true\n  }\n}\n1 -> h3\n2 -> h1\n2 -> h2\n2 -> h3\n3 -> h1\n3 -> h2\n3 -> h3\nh1 -> o1\nh2 -> o1\nh3 -> o1\nh1 -> o2\nh2 -> o2\nh3 -> o2\nh1 -> o3\nh2 -> o3\nh3 -> o3\n\no1 -> out1\no2 -> out2\no3 -> out3\n\n```\n:::\n\n\n## Here Comes the ~~Rain~~ Maths Again!\n\nNow, we are ready (maybe?) to watch these two very beautifully made videos on Backpropagation. One is of course from Dan Shiffman, and the other from Grant Sanderson a.ka. 3Blue1Brown.\n\n:::: {.columns}\n\n::: {.column width=\"48%\"}\n\n\n\n{{< video https://youtu.be/QJoa0JYaX1I?list=PLRqwX-V7Uu6Y7MdSCaIfsxc561QI0U0Tb >}}\n\n\n\n\n\n:::\n\n::: {.column width=\"4%\"}\n:::\n\n::: {.column width=\"48%\"}\n\n\n\n{{< video https://youtu.be/tIeHLnjs5U8?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi >}}\n\n\n\n\n\n:::\n::::\n\n## Backpropagation in Code\n\n::: {.panel-tabset .nav-pills style=\"background: whitesmoke \"}\n\n### Using p5.js\n\n\n### Using R\nUsing `torch`.\n\n:::\n\n\n## References\n\n1. Tariq Rashid. *Make your own Neural Network*. [PDF Online](https://github.com/harshitkgupta/StudyMaterial/blob/master/Make%20Your%20Own%20Neural%20Network%20(Tariq%20Rashid)%20-%20%7BCHB%20Books%7D.pdf)\n1. Mathoverflow. *Intuitive Crutches for Higher Dimensional Thinking*. <https://mathoverflow.net/questions/25983/intuitive-crutches-for-higher-dimensional-thinking>\n1. Interactive Backpropagation Explainer <https://xnought.github.io/backprop-explainer/>\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../../../../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../../../../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}