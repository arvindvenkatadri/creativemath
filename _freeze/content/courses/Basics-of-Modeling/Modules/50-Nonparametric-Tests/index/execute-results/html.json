{
  "hash": "4a2315dbf8f95ba26af4ec75008f4f1f",
  "result": {
    "markdown": "---\ntitle: \"Nonparametric Tests\"\nauthor: \"Arvind Venkatadri\"\ndate: 26/Nov/2022\ndate-modified: \"2023-01-12\"\ncode-fold: true\ncode-summary: \"Show the Code\"\ncode-line-numbers: true\ncode-tools: true\nkeywords: Statistics ; Tests; p-value; Feynman Technique\nabstract: This course is intended to assist with making statistically significant insights that drive design decisions. This document deals with the basics of stats. The method followed is that of Jonas Lindoloev, wherein every stat test is treated as a linear model y = mx + c.\nexecute: \n  freeze: true\n---\n\n\n\n\n## Nonparametric Tests Workflow in Orange\n\n## Nonparametric Tests Workflow in Radiant\n\n## Nonparametric Tests Workflow in R\n\n# References\n\n1.  *Common statistical tests are linear models (or: how to teach\n    stats)* by [Jonas Kristoffer\n    Lindeløv](https://lindeloev.github.io/tests-as-linear/)\n2.  [CheatSheet](https://lindeloev.github.io/tests-as-linear/linear_tests_cheat_sheet.pdf)\n3.  *Common statistical tests are linear models: a work through* by\n    [Steve Doogue](https://steverxd.github.io/Stat_tests/)\n4.  [Jeffrey Walker \"Elements of Statistical Modeling for Experimental\n    Biology\"](https://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/)\n5.  Text: Diez, David M & Barr, Christopher D & Çetinkaya-Rundel, Mine:\n    [OpenIntro Statistics](https://www.openintro.org/book/os/)\n6.  Modern Statistics with R: From wrangling and exploring data to\n    inference and predictive modelling by [Måns\n    Thulin](http://www.modernstatisticswithr.com/)\n\n# Structure of this document\n\nWe will follow the following structure: Each kind of Test is described\nin a separate Chapter. The Test *Model* is laid out in formula\n$y = mx + c$ and in *Code*.\n\nThe Structure looks like this:\n\n## Test {.tabset}\n\n### Model\n\nExplanation, formula etc.\n\n### Code\n\nWith Toy Data; Graphs\n\n### Example\n\nWith another \"real world\" data set; Graphs\n\n# Data\n\n## Sample Values\n\nMost examples in this exposition are based on three \"imaginary\" samples,\n$x, y, y2$. Each is normally distributed and made up of 50 observations.\n\nWe start by creating a function that will allow us to produce samples of\na given size (N) with a specified mean (mu) and standard deviation (sd):\nNote: this gives a *matrix* of numbers, as opposed to a vector using\n`rnorm` by itself. The data are created both in vector form and tibble\nform for flexibility of use in diverse packages and formulae in what\nfollows.\n\n\n::: {.cell layout-align=\"centre\"}\n\n```{.r .cell-code}\nrnorm_fixed  <- function(N, mu = 0, sd = 1) {\n  scale(rnorm(N))* sd + mu\n}\n```\n:::\n\n\nWe create three variables: x ( explanatory) and y, y2 ( dependent ).\n\n\n::: {.cell layout-align=\"centre\"}\n\n```{.r .cell-code}\nset.seed(40) # for replication\n\n# Data as vectors ( for t.tests etc)\nx <- rnorm_fixed(50, mu = 0.0, sd = 1) #explanatory\ny <- rnorm_fixed(50, mu = 0.3, sd = 2) # dependent #1\ny2 <- rnorm_fixed(50, mu = 0.5, sd = 1.5) # dependent #2\n\n# Make a tibble with all variables\nmydata_wide <- tibble(x = x, y = y, y2 = y2)\n\n# Long form data\nmydata_long <- \n  mydata_wide %>%\n  pivot_longer(., cols = c(x,y,y2), \n               names_to = \"group\", \n               values_to = \"value\")\n\n# Long form data with only dependent variables\nmydata_long_y <- \n  mydata_wide %>% \n  select(-x) %>% \n  pivot_longer(., cols = c(y,y2), \n               names_to = \"group\", \n               values_to = \"value\")\nmydata_wide\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 50 × 3\n    x[,1]  y[,1] y2[,1]\n    <dbl>  <dbl>  <dbl>\n 1  0.427  1.98   0.103\n 2  0.444  1.27  -0.281\n 3 -0.828 -0.944  0.624\n 4 -0.799  0.103  5.43 \n 5 -0.323  2.19   0.168\n 6 -1.24   0.192 -0.549\n 7 -1.36   4.81   0.309\n 8  1.62   1.55  -0.990\n 9 -0.292  3.36  -0.172\n10 -1.25   1.92  -0.158\n# … with 40 more rows\n```\n:::\n\n```{.r .cell-code}\nmydata_long\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 150 × 2\n   group value[,1]\n   <chr>     <dbl>\n 1 x         0.427\n 2 y         1.98 \n 3 y2        0.103\n 4 x         0.444\n 5 y         1.27 \n 6 y2       -0.281\n 7 x        -0.828\n 8 y        -0.944\n 9 y2        0.624\n10 x        -0.799\n# … with 140 more rows\n```\n:::\n\n```{.r .cell-code}\nmydata_long_y\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 100 × 2\n   group value[,1]\n   <chr>     <dbl>\n 1 y         1.98 \n 2 y2        0.103\n 3 y         1.27 \n 4 y2       -0.281\n 5 y        -0.944\n 6 y2        0.624\n 7 y         0.103\n 8 y2        5.43 \n 9 y         2.19 \n10 y2        0.168\n# … with 90 more rows\n```\n:::\n:::\n\n\n## \"Signed Rank\" Values\n\nMost statistical tests use the **actual values** of the data variables.\nHowever, in some *non-parametric* statistical tests, the data are used\nin **rank-transformed** sense/order. In some cases the **signed-rank**\nof the data values is used instead of the data itself.\n\nSigned Rank is calculated as follows:\\\n1. Take the absolute value of each observation in a sample\\\n2. Place the <u>*ranks*</u> in order of (absolute magnitude). The\nsmallest number has *rank = 1* and so on.\\\n3. Give each of the ranks the sign of the original observation ( + or -\n)\n\n\n::: {.cell layout-align=\"centre\"}\n\n```{.r .cell-code}\nsigned_rank <- function(x) {sign(x) * rank(abs(x))}\n```\n:::\n\n\n## Plotting Original and Signed Rank Data\n\nA quick plot:\n\n\n::: {.cell layout-align=\"centre\"}\n\n```{.r .cell-code}\np1 <- ggplot(mydata_long,aes(x = group, y = value)) +\n  geom_jitter(width = 0.02, height = 0,aes(colour = group), size = 4) +\n  geom_segment(data = mydata_wide, aes(y = 0, yend = 0, \n                                       x = .75, \n                                       xend = 1.25 )) + \n  geom_text(aes(x = 1, y = 0.5, label = \"0\")) +\n  geom_segment(data = mydata_wide, aes(y = 0.3, yend = 0.3, \n                                       x = 1.75 , \n                                       xend = 2.25 )) + \n  geom_text(aes(x = 2, y = 0.6, label = \"0.3\")) +\n  geom_segment(data = mydata_wide, aes(y = 0.5, yend = 0.5, \n                                       x = 2.75, \n                                       xend = 3.25 )) + \n  geom_text(aes(x = 3, y = 0.8, label = \"0.5\")) +\n  labs(title = \"Original Data\") +\n  ylab(\"Response Variable\")\n\np2 <- mydata_long %>% \n  group_by(group) %>% \n  mutate( s_value = signed_rank(value)) %>% \n  ggplot(., aes(x = group, y = s_value)) + \n  geom_jitter(width = 0.02, height = 0,aes(colour = group), size = 4) + \n  stat_summary(fun = \"mean\", geom = \"point\", colour = \"red\", \n               size = 8) + \n  labs(title = \"Signed Rank of Data\") +\n  ylab(\"Signed Rank of Response Variable\")\n\npatchwork::wrap_plots(p1,p2, nrow = 1, guides = \"collect\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/data_plots-1.png){fig-align='centre' width=672}\n:::\n:::\n\n\n## How does Sign-Rank data work?\n\nTBD: need to add some explanation here.\n\n# The Linear ( Regression ) Model {.tabset}\n\nThe premise here is that many common statistical tests are special cases\nof the linear model.\n\nA linear model estimates the relationship between one *continuous* or\n*ordinal* variable (dependent variable or \"response\" ) and one or more\nother variables ( explanatory variable or \"predictors\" ). It is assumed\nthat the relationship is linear:\n\n$$\ny = \\beta_0 + \\beta_1 *x\n$$ $\\beta_0$ is the *intercept* and $\\beta_1$ is the slope of the linear\nfit, that **predicts** the value of y based the value of x. Each\nprediction leaves a small \"residual\" error between the actual and\npredicted values. $\\beta_0$ and $\\beta_1$ are calculated based on\nminimizing the *sum of square*s of these residuals, and hence this\nmethod is called \"ordinary least squares\" regression.\n\n![Least Squares](OLS.png)\n\nThe net *area* of all the shaded squares is minimized in the calculation\nof $\\beta_0$ and $\\beta_1$. It is also possible that there is more than\none explanatory variable: this is **multiple regression.**\n\n$$\ny = \\beta_0 + \\beta_1*x_1 + \\beta_2*x_2 ...+ \\beta_n*x_n\n$$\n\nwhere each of the $\\beta_i$ are slopes defining the relationship between\ny and $x_i$. Together, the RHS of that equation defines an n-dimensional\n*plane*.\n\nAs per Lindoloev, many statistical tests, going from one-sample t-tests\nto two-way ANOVA, are special cases of this system.\n\nAlso see [Jeffrey Walker \"A\nlinear-model-can-be-fit-to-data-with-continuous-discrete-or-categorical-x-variables\"](https://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/intro-linear-models.html#a-linear-model-can-be-fit-to-data-with-continuous-discrete-or-categorical-x-variables)\n\n## Linear Models as Hypothesis Tests\n\nUsing linear models is based on the idea of **Testing of Hypotheses**.\nThe Hypothesis Testing method typically defines a NULL Hypothesis where\nthe statements read as \"**there is no relationship**\" between the\nvariables at hand, explanatory and responses. The Alternative Hypothesis\ntypically states that there *is* a relationship between the variables.\n\nAccordingly, in fitting a linear model, we follow the process as\nfollows:\n\n1.  Make the following hypotheses: $$\n    y = \\beta_0 + \\beta_1 *x \\\\\n    NULL\\ Hypothesis\\ H_0 => x\\ and\\ y\\ are\\ unrelated.\\ (\\beta_1 = 0)\n    $$ $$\n    y = \\beta_0 + \\beta_1 *x \\\\\n    Alternate\\ Hypothesis\\ H_1 => x\\ and\\ y\\ are\\ linearly\\ related\\ (\\beta_1 \\ne 0)\n    $$\n2.  We \"assume\" that $H_0$ is true.\n3.  We calculate $\\beta_1$.\n4.  We then find probability **p** that \\[$\\beta_1 = Estimated\\ Value$\\]\n    **when the NULL Hypothesis** is **assumed** TRUE. This is the\n    **p-value**. If that probability is **p\\>=0.05**, we say we \"cannot\n    reject\" $H_0$ and there is unlikely to be significant linear\n    relationship.\n\nHowever, if **p\\<= 0.05** can we reject the NULL hypothesis, and say\nthat there could be a significant linear relationship,because\n$\\beta_1 = Estimated\\ Value$ by mere chance under $H_0$ is very small.\n\nPheeew !!\n\n## Linear Models in R\n\n`lm()` is the function to create linear models in R. In R we are lazy\nand write :\n\n$$\ny \\sim 1 + x\\\\\nwhich\\ reads\\ like\\\\\ny = 1*number + x* another\\ number\n$$\n\nNote: there are very many ways in which linear models can be coded in R.\nSee [Vito Ricci on\nCRAN.](https://cran.r-project.org/doc/contrib/Ricci-refcard-regression.pdf)\n\n\n::: {.cell layout-align=\"centre\"}\n\n```{.r .cell-code}\n# using lm()\nlm(y ~ 1 + x, data = mydata_wide) %>% \n  summary() %>%   \n  print(digits = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ 1 + x, data = mydata_wide)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3.93980 -1.09967  0.12548  1.27904  3.88372 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)  0.30000    0.27799  1.0792   0.2859\nx           -0.46355    0.28081 -1.6507   0.1053\n\nResidual standard error: 1.9657 on 48 degrees of freedom\nMultiple R-squared:  0.05372,\tAdjusted R-squared:  0.034006 \nF-statistic:  2.725 on 1 and 48 DF,  p-value: 0.10532\n```\n:::\n:::\n\n\nSince the *p-value* is \\>=0.05, we *fail to reject the NULL Hypothesis\nthat there is no relationship between x and y*.\n\n## Assumptions in Linear Models\n\n1.  **L**: $\\color{blue}{linear}$ relationship\n2.  **I**: Errors are **independent** (across observations)\n3.  **N**: y is $\\color{red}{normally}$ distributed at each \"level\" of\n    x.\n4.  **E**: equal variance at all levels of x. No *heteroscedasticity*.\n    ![OLS Assumptions](ols_assumptions.png)\n\nLet us now see which standard statistical tests can be re-formulated as\nLinear Models.\n\n# Tests for Correlation {.tabset}\n\nCorrelation **r** is a measure of *strength* and *direction* of *linear\nassociation* between two variables. **r** is between \\[-1,+1\\], with 0\nimplying no association/correlation.\n\nFrom this definition, the *linear model* lends itself in a\nstraightforward way as a model to interpret *correlation*. Intuitively,\nthe slope of the linear model could be related to the correlation\nbetween y and x.\n\nNow we look at the numbers.\n\n## Pearson Correlation {.tabset}\n\n### Model\n\nThe model for Pearson Correlation tests is exactly the Linear Model:\n\n$$\ny = \\beta_0 + \\beta_1 * x \n\\\\\nH_0: \\beta_1 = 0\n$$\n\nSee the Code section for further insights into the relationship between\nthe Correlation Score and the Slope of the Linear Model.\n\n### Code\n\n\n::: {.cell layout-align=\"centre\"}\n\n```{.r .cell-code}\n# Pearson (built-in test)\ncor <- cor.test(y,x,method = \"pearson\") %>% \n  broom::tidy() %>% select(estimate, p.value)\n\n# Linear Model\nlin <- lm(y ~ 1 + x, data = mydata_wide) %>% \n  broom::tidy() %>% select(estimate, p.value)\n\n# Scaled linear model\nlin_scl <- lm(scale(y) ~ 1 + scale(x), data = mydata_wide) %>% \n  broom::tidy() %>% select(estimate, p.value)\n\nprint(cor)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 2\n  estimate p.value\n     <dbl>   <dbl>\n1   -0.232   0.105\n```\n:::\n\n```{.r .cell-code}\nprint(lin)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 2\n  estimate p.value\n     <dbl>   <dbl>\n1    0.3     0.286\n2   -0.464   0.105\n```\n:::\n\n```{.r .cell-code}\nprint(lin_scl)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 2\n   estimate p.value\n      <dbl>   <dbl>\n1 -9.06e-17   1    \n2 -2.32e- 1   0.105\n```\n:::\n\n```{.r .cell-code}\n# All together\nrbind(cor, lin, lin_scl) %>% print()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 2\n   estimate p.value\n      <dbl>   <dbl>\n1 -2.32e- 1   0.105\n2  3   e- 1   0.286\n3 -4.64e- 1   0.105\n4 -9.06e-17   1    \n5 -2.32e- 1   0.105\n```\n:::\n:::\n\n\nNotes: 1. The *p-value* for Pearson Correlation and that for the *slope*\nin the linear model is the same ( 0.1053 ). Which means we cannot reject\nthe NULL hypothesis of \"no relationship\".\n\n2.  Here is the relationship between the slope and correlation:\n\n$$\nSlope\\ \\beta_1 = \\frac{sd_y}{sd_x} * r\n$$\n\nWhen both x and y have the same standard deviation, the slope and\ncorrelation are the same. Here, since x has twice the `sd` of y, the\nratio of **slope** = -0.4635533 to **r** = -0.2317767 is\n0.5. Hence a linear model using `scale()` for both variables will show\nslope = **r**.\n\nSlope_Scaled: -0.2317767 = Correlation: -0.2317767\n\n### Example\n\nWe choose to look at the `gpa_study_hours` dataset. It has two numeric\ncolumns `gpa` and `study_hours`:\n\n\n::: {.cell layout-align=\"centre\"}\n\n```{.r .cell-code}\nglimpse(gpa_study_hours)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 193\nColumns: 2\n$ gpa         <dbl> 4.000, 3.800, 3.930, 3.400, 3.200, 3.520, 3.680, 3.400, 3.…\n$ study_hours <dbl> 10, 25, 45, 10, 4, 10, 24, 40, 10, 10, 30, 7, 15, 60, 10, …\n```\n:::\n:::\n\n::: {.cell layout-align=\"centre\"}\n\n```{.r .cell-code}\n# Checks for Normal/Symmetric distributions\np1 <- ggplot(gpa_study_hours) + geom_histogram(aes(gpa))\np2 <- ggplot(gpa_study_hours) + geom_histogram(aes(study_hours))\np3 <- ggplot(gpa_study_hours) + geom_point(aes(gpa, study_hours))\n(p1 + p2) / p3\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/Pearson_example_1-1.png){fig-align='centre' width=672}\n:::\n:::\n\n\nHmm...not normally distributed, and the relationship is also not linear,\nand there is some evidence of heterscedasticity, so Pearson correlation\nwould not be the best idea here.\n\n\n::: {.cell layout-align=\"centre\"}\n\n```{.r .cell-code}\n# Pearson Correlation as Linear Model\nlm(gpa ~ study_hours, data = gpa_study_hours) %>% summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = gpa ~ study_hours, data = gpa_study_hours)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.95130 -0.19456  0.03879  0.21708  0.73872 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 3.527997   0.037424  94.272   <2e-16 ***\nstudy_hours 0.003328   0.001794   1.855   0.0652 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2837 on 191 degrees of freedom\nMultiple R-squared:  0.01769,\tAdjusted R-squared:  0.01255 \nF-statistic:  3.44 on 1 and 191 DF,  p-value: 0.06517\n```\n:::\n\n```{.r .cell-code}\n# Other ways using other packages\nmosaic::cor_test(gpa ~ study_hours, data = gpa_study_hours)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPearson's product-moment correlation\n\ndata:  gpa and study_hours\nt = 1.8548, df = 191, p-value = 0.06517\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.008383868  0.269196552\nsample estimates:\n      cor \n0.1330138 \n```\n:::\n\n```{.r .cell-code}\nstatsExpressions::corr_test(data = gpa_study_hours, \n                            x = study_hours, \n                            y = gpa)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 14\n  parameter1  paramet…¹ effec…² estim…³ conf.…⁴ conf.low conf.…⁵ stati…⁶ df.er…⁷\n  <chr>       <chr>     <chr>     <dbl>   <dbl>    <dbl>   <dbl>   <dbl>   <int>\n1 study_hours gpa       Pearso…   0.133    0.95 -0.00838   0.269    1.85     191\n# … with 5 more variables: p.value <dbl>, method <chr>, n.obs <int>,\n#   conf.method <chr>, expression <list>, and abbreviated variable names\n#   ¹​parameter2, ²​effectsize, ³​estimate, ⁴​conf.level, ⁵​conf.high, ⁶​statistic,\n#   ⁷​df.error\n```\n:::\n:::\n\n\nThe correlation estimate is $0.133$; the `p-value` is 0.065 and the\nconfidence interval includes 0. Hence we fail to reject the NULL\nhypothesis that `study_hours` and `gpa` have no relationship.\n\nWe can use a later package `ggstaplot` to plot this:\n\n\n::: {.cell layout-align=\"centre\"}\n\n```{.r .cell-code}\nggstatsplot::ggscatterstats(data = gpa_study_hours, \n                            x = study_hours, \n                            y = gpa,\n                            type = \"robust\",\n                            marginal = TRUE,\n                            title = \"GPA vs Study Hours\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRegistered S3 method overwritten by 'ggside':\n  method from   \n  +.gg   ggplot2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/Pearson_example_3-1.png){fig-align='centre' width=672}\n:::\n:::\n\n\n## Spearman Correlation {.tabset}\n\n### Model\n\nIn some cases the **LINE** assumptions may not hold. Nonlinear\nrelationships, non-normally distributed data ( with large outliers ) and\nworking with *ordinal* rather than continuous data: these situations\nnecessitate the use of Spearman's *ranked* correlation scores.\n(**Ranked**, not **sign-ranked**.)\n\n$$\nrank(y) = \\beta_0 + \\beta_1 * rank(x) \\\\\nH_0: \\beta_1 = 0\n$$\n\nSpearman correlation = Pearson correlation using the rank of the data\nobservations. Let's check how this holds for a our x and y data:\n\n\n::: {.cell layout-align=\"centre\"}\n\n```{.r .cell-code}\n# Plot the data\np1 <- ggplot(mydata_wide, aes(x, y)) + \n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  ggtitle(\" Pearson Correlation\\n and Linear Models\")\n\n# Plot ranked data\np2 <- mydata_wide %>% \n  mutate(x_rank = rank(x),\n         y_rank = rank(y)) %>%\n  ggplot(.,aes(x_rank, y_rank)) + \n  geom_point(shape = 15, size = 2) +\n  geom_smooth(method = \"lm\") + \n  ggtitle(\" Spearman Ranked Correlation\\n and Linear Models\")\n\npatchwork::wrap_plots(p1,p2, nrow = 1, guides = \"collect\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/Spearman_Plot-1.png){fig-align='centre' width=672}\n:::\n:::\n\n\nSlopes are almost identical, \\~ 0.25.\n\n### Code\n\n\n::: {.cell layout-align=\"centre\"}\n\n```{.r .cell-code}\n# Spearman\ncor1 <- cor.test(y,x, method = \"spearman\") %>% \n  broom::tidy() %>% select(estimate, p.value)\n\n# Pearson using ranks\ncor2 <- cor.test(rank(y), rank(x), method = \"pearson\") %>% \nbroom::tidy() %>% select(estimate, p.value)\n\n# Linear Models using rank\ncor3 <- lm(rank(y) ~ 1 + rank(x),data = mydata_wide) %>% \n  broom::tidy() %>% select(estimate, p.value)\n\nrbind(cor1, cor2, cor3) %>% print()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 2\n  estimate  p.value\n     <dbl>    <dbl>\n1   -0.227 1.13e- 1\n2   -0.227 1.14e- 1\n3   31.3   9.11e-10\n4   -0.227 1.14e- 1\n```\n:::\n:::\n\n\nNotes:\n\n1.  When ranks are used, the slope of the linear model ($\\beta_1$) has\n    the same value as the correlation coefficient ( $\\rho$ ).\n\n2.  Note that the slope from the linear model now has an intuitive\n    interpretation: **the number of ranks y changes for each change in\n    rank of x**. ( Ranks are \"independent\" of `sd` )\n\n### Example\n\nWe examine the `cars93` data, where the numeric variables of interest\nare `weight` and `price`.\n\n\n::: {.cell layout-align=\"centre\"}\n\n```{.r .cell-code}\ncars93 %>% \n  ggplot(aes(weight, price)) + \n  geom_point() + geom_smooth(method = \"lm\", se = FALSE, lty = 2) + \n  labs(title = \"Car Weight and Car Price have a nonlinear relationship\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/Spearman_example_1-1.png){fig-align='centre' width=672}\n:::\n:::\n\n\nLet us try a Spearman Correlation score for these variables, since the\ndata are not linearly related and the variance of `price` also is not\nconstant over `weight`\n\n\n::: {.cell layout-align=\"centre\"}\n\n```{.r .cell-code}\ncor.test(cars93$price, cars93$weight, method = \"spearman\") %>% broom::tidy()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in cor.test.default(x, y, ...): Cannot compute exact p-value with ties\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 5\n  estimate statistic  p.value method                          alternative\n     <dbl>     <dbl>    <dbl> <chr>                           <chr>      \n1    0.883     3074. 1.07e-18 Spearman's rank correlation rho two.sided  \n```\n:::\n\n```{.r .cell-code}\n# Using linear Model\nlm(rank(price) ~ rank(weight), data = cars93) %>% summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = rank(price) ~ rank(weight), data = cars93)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-20.0676  -3.0135   0.7815   3.6926  20.4099 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   3.22074    2.05894   1.564    0.124    \nrank(weight)  0.88288    0.06514  13.554   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.46 on 52 degrees of freedom\nMultiple R-squared:  0.7794,\tAdjusted R-squared:  0.7751 \nF-statistic: 183.7 on 1 and 52 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\n# Stats Plot\nggstatsplot::ggscatterstats(data = cars93, x = weight, \n                            y = price,\n                            type = \"nonparametric\",\n                            title = \"Cars93: Weight vs Price\",\n                            subtitle = \"Spearman Correlation\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/Spearman_example_2-1.png){fig-align='centre' width=672}\n:::\n:::\n\n\nWe see that using ranks of the `price` variable, we obtain a Spearman's\n$\\rho = 0.882$ with a `p-value` that is very small. Hence we are able to\nreject the NULL hypothesis and state that there is a relationship\nbetween these two variables. The **linear** relationship is evaluated as\na correlation of `0.882`.\n\n# Tests on One Mean {.tabset}\n\nA series of tests deal with one mean value of a sample. The idea is to\nevaluate whether that mean is representative of the mean of the\nunderlying population.\n\nThis uses the *Student's t-test* for parametric data and the *Wilcoxon\nsigned-rank test* for non-parametric data.\n\nTests can involve a *single sample* or *paired samples*.\n\n## The Student's t-test with one sample {.tabset}\n\n### Model\n\nA single number predicts y:\n\n$$ \ny = \\beta_0 + \\beta_1*x \\\\\n\\\\and\\ further \\ actually\\\\\ny = \\beta_0\n$$\n\nand the second term vanishes, since \"there is no x\": all the x-values\nare made equal to zero in the linear model !! The NULL Hypothesis\ntherefore is:\n\n$$\n\\ H_0: \\beta_0 = 0\n$$\n\nThis NULL Hypothesis makes sense, because in the accompanying linear\nmodel all values of the <u>explanatory</u> variable x are zero, and\ntherefore the NULL Hypothesis for the model should be that y also should\nbe zero mean. Note that if we **want** the NULL hypothesis to be that\nthe mean is other than zero, we can use the\n`lm(...., mu = some_number, ..)` parameter in the command.\n\n### Code\n\nIf we compare the `t.test` with the appropriate `lm` model:\n\n\n::: {.cell layout-align=\"centre\"}\n\n```{.r .cell-code}\n# t-test\nt1 <- t.test(y, mu = 0, alternative = \"two.sided\")\nprint(t1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tOne Sample t-test\n\ndata:  y\nt = 1.0607, df = 49, p-value = 0.294\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.2683937  0.8683937\nsample estimates:\nmean of x \n      0.3 \n```\n:::\n\n```{.r .cell-code}\n# linear model\nlm1 <- lm(y ~ 1, data = mydata_wide)\nlm1 %>% summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ 1, data = mydata_wide)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5554 -1.4845 -0.0392  1.5559  4.5119 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   0.3000     0.2828   1.061    0.294\n\nResidual standard error: 2 on 49 degrees of freedom\n```\n:::\n\n```{.r .cell-code}\nlm1 %>% confint()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 2.5 %    97.5 %\n(Intercept) -0.2683937 0.8683937\n```\n:::\n:::\n\n\nThe confidence intervals for both the `t.test` and the `lm` model are\nidentical.\n\nt-test confidence intervals: -0.2683937, 0.8683937\\\nlinear model confidence intervals: -0.2683937, 0.8683937\n\nSo even though y has a mean of 0.3, the confidence intervals straddle\nzero, and hence we cannot reject the NULL hypothesis that the true\npopulation, of which y is a sample, could have mean=0.\n\n### Example\n\n\n::: {.cell layout-align=\"centre\"}\n\n```{.r .cell-code}\nexam_grades\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 233 × 6\n   semester sex   exam1 exam2 exam3 course_grade\n   <chr>    <chr> <dbl> <dbl> <dbl>        <dbl>\n 1 2000-1   Man    84.5  69.5  86.5         76.3\n 2 2000-1   Man    80    74    67           75.4\n 3 2000-1   Man    56    70    71.5         67.1\n 4 2000-1   Man    64    61    67.5         63.5\n 5 2000-1   Man    90.5  72.5  75           72.4\n 6 2000-1   Man    74    78.5  84.5         71.4\n 7 2000-1   Man    60.5  44    58           56.1\n 8 2000-1   Man    89    82    88           78.0\n 9 2000-1   Woman  87.5  86.5  95           82.9\n10 2000-1   Man    91    98    88           89.1\n# … with 223 more rows\n```\n:::\n:::\n\n\n## Wilcoxon's Signed-Rank Test {.tabset}\n\nSince we are dealing with the **mean**, the *sign* of the rank becomes\nimportant to use, in the case of a non-parametric single mean test.\n\n### Model\n\n$$\nsigned\\_rank(y) = \\beta_0 \\\\\nH_0: \\beta_0 = 0\n$$\n\n### Code\n\n\n::: {.cell layout-align=\"centre\"}\n\n```{.r .cell-code}\n# Standard Wilcoxon Signed_Rank Test\nw1 <- wilcox.test(y)\nw1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tWilcoxon signed rank test with continuity correction\n\ndata:  y\nV = 754, p-value = 0.2628\nalternative hypothesis: true location is not equal to 0\n```\n:::\n\n```{.r .cell-code}\n# Wilcoxon test with lm\nw2 <- lm(signed_rank(y) ~ 1 , data = mydata_wide)\nw2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = signed_rank(y) ~ 1, data = mydata_wide)\n\nCoefficients:\n(Intercept)  \n       4.66  \n```\n:::\n\n```{.r .cell-code}\n# t-test with signed_rank data\nw3 <- t.test(signed_rank(y))\nw3\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tOne Sample t-test\n\ndata:  signed_rank(y)\nt = 1.1277, df = 49, p-value = 0.265\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -3.644491 12.964491\nsample estimates:\nmean of x \n     4.66 \n```\n:::\n:::\n\n\nWe can plot the y data both original and ranked to see where the mean\nlies in each case. The approximation to the true \\$\\beta\\_0 ( is good\nwhen the number of observations N is \\>=50. Lindoloev has a [simulation\non\nthis.](https://lindeloev.github.io/tests-as-linear/simulations/simulate_wilcoxon.html).\nWe can also plot the model using *lm* for both the original data and the\nsign-ranked data.\n\n### Example\n\n### Plots for both t-test and Wilcoxon test\n\n\n::: {.cell layout-align=\"centre\"}\n\n```{.r .cell-code}\np1 <- ggplot(mydata_wide, aes( x = 0, y = y)) +\n  geom_point(alpha = 0.4) +\n  geom_segment(aes(y = t1$estimate, \n                   yend = t1$estimate, \n                   x = -0.2, xend = 0.2)) + \n  labs(title = \"Student's\\n t-Test\")\n\n# t-test using linear model\np2 <- ggplot(mydata_wide, aes( x = 0, y = y)) +\n  geom_point(alpha = 0.4) +\n  geom_segment(aes(y = lm(y ~ 1)$coefficient, \n                   yend = lm(y ~ 1)$coefficient, \n                   x = -0.2, xend = 0.2)) + \n  labs(title = \"Student's\\n t-Test \\n using lm\")\n\n# Wilcoxon test, using signed-ranks of data\np3 <- ggplot(mydata_wide, aes( x = 0, y = signed_rank(y))) +\n  geom_point(alpha = 0.4) +\n  geom_segment(aes(y = mean(signed_rank(y)), yend = mean(signed_rank(y)), x = -0.2, xend = 0.2)) + \n  labs(title = \"Wilcoxon \\nSigned-Rank\\n Test\")\n\n# Wilcoxon test, using signed-ranks of data, and lm\np4 <- ggplot(mydata_wide, aes( x = 0, y = signed_rank(y))) +\n  geom_point(alpha = 0.4) +\n  geom_segment(aes(y = lm(signed_rank(y) ~1)$coefficient, \n                   yend = lm(signed_rank(y) ~1)$coefficient, \n                   x = -0.2, xend = 0.2)) + \n  labs(title = \"Wilcoxon \\n Signed-Rank \\n Test with lm\")\n\n\npatchwork::wrap_plots(p1,p2,p3,p4, nrow = 1, guides = \"collect\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/Mean_Related_Plots-1.png){fig-align='centre' width=672}\n:::\n:::\n\n\n## Paired Sample t-test {.tabset}\n\nWe use this when we have two samples and the observations from one\nsample can be \"paired\" with observations in the other sample. Controlled\nstudies for interventions/measures, such as before/after kind of data,\ncomparisons between two interventions on the same set of subjects, and\ntwo measurements made on the same subjects using different methods etc.\n\n### Model\n\n$$\ny_2 - y_1 = \\beta_0 \\\\\nH_0 : \\beta_0 = 0\n$$\n\nThe NULL Hypothesis is that there is no difference, **either way**,\nbetween the two samples. Again, in the linear model, we assume as before\nthat \"the explanatory x variable has been equated to zero.\n\nWe therefore set `two.sided` and `mu = 0` in the `t.test`.\n\n### Code\n\n\n::: {.cell layout-align=\"centre\"}\n\n```{.r .cell-code}\n# Using paired t-test\nt2 <- t.test(y2, y, paired = TRUE, mu = 0, \n             alternative = \"two.sided\")\nt2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPaired t-test\n\ndata:  y2 and y\nt = 0.54264, df = 49, p-value = 0.5898\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -0.54067  0.94067\nsample estimates:\nmean difference \n            0.2 \n```\n:::\n\n```{.r .cell-code}\n# linear model\nlm2 <- lm(y2-y ~ 1, data = mydata_wide)\nlm2 %>% summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y2 - y ~ 1, data = mydata_wide)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.7027 -2.1872 -0.1367  1.3835  5.7262 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   0.2000     0.3686   0.543     0.59\n\nResidual standard error: 2.606 on 49 degrees of freedom\n```\n:::\n\n```{.r .cell-code}\nlm2 %>% confint()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               2.5 %  97.5 %\n(Intercept) -0.54067 0.94067\n```\n:::\n:::\n\n\nBoth tests report the difference to be 0.2. However the p-value in both\ntests is about 0.6, so the result is not statistically significant.\n\n### Example\n\n## Wilcoxon Paired Test {.tabset}\n\nWhen the original data is not normally distributed or has outliers etc,.\nwe use a nonparametric Wilcoxon paired test. The difference between the\npaired and unpaired Wilcoxon test is that the test is run on the\nsigned-ranks of the **pairwise differences** y2- y.\n\n### Model\n\n\\$\\$ signed_rank(y2 - y1) = \\beta\\_0 \\\\\n\nH_0: \\beta\\_0 = 0\n\n\\$\\$\n\n### Code\n\n\n::: {.cell layout-align=\"centre\"}\n\n```{.r .cell-code}\n# Paired Wilcoxon Test\nw4 <- wilcox.test(y, y2, paired = TRUE)\nw4\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tWilcoxon signed rank test with continuity correction\n\ndata:  y and y2\nV = 608, p-value = 0.7795\nalternative hypothesis: true location shift is not equal to 0\n```\n:::\n\n```{.r .cell-code}\n# Linear Model\nlm4 <- lm(signed_rank(y2-y) ~ 1 , data = mydata_wide)\nlm4 %>% summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = signed_rank(y2 - y) ~ 1, data = mydata_wide)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-48.18 -27.93   0.82  22.57  48.82 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)    1.180      4.182   0.282    0.779\n\nResidual standard error: 29.57 on 49 degrees of freedom\n```\n:::\n\n```{.r .cell-code}\n# t-test with Signed Rank\nt4 <- t.test(signed_rank(y2-y), mu = 0 , alternative = \"two.sided\")\nt4\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tOne Sample t-test\n\ndata:  signed_rank(y2 - y)\nt = 0.28214, df = 49, p-value = 0.779\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -7.224733  9.584733\nsample estimates:\nmean of x \n     1.18 \n```\n:::\n:::\n\n\nHere too, the p-values reported by the three tests are p = 0.779 so the\ndifference reported is not significant.\n\n### Example\n\n# Tests with Two Means {.tabset}\n\nWhen we have two independent samples and these are *not paired* as\nearlier. The intent is to test if there is a significant difference to\ntheir means. An example is identical measurements on two sets of\nsubjects.\n\nThere is considerable discussion on what test to use when: 1.If\nvariances are equal between samples, observations are normally\ndistributed within groups, no outliers, then `independent t-test` 2. If\nunequal variance, `t-test` with `Welch correction` ( Welch's t-test) 3.\nIf samples are not normal, then nonparametric test ( Wilcoxon with two\nsamples = `Mann-Whitney test`). Symmetry of the distribution is assumed\nhere.\n\n## Dummy Group Variable Concept\n\nAn important construct here is the **dummy variable**. When there is\nmore than one group in the data, a dummy *categorical variable* is set\nup, whose entries specify the group ID. The group IDs are **still\nnumerical** ( as with factors, remember ). The dummy variable is plotted\non the x-axis. The consecutive IDs in the dummy variable x are separated\nby *1*. Hence the *between-groups difference* in the stat measures\ncomputed on y are numerically equivalent to the **slope** in the linear\nmodel. Thus the *dummy variable* allows us to \\*\\*mathematically\\* use\nthe linear model, as presented in the equations above.\n\nDummy variables become even more useful when the explanatory variable (\n\"x-planatory\" ) is already categorical, as with ANOVA and friends.\n\nWe can visualize this as follows:\n\n\n::: {.cell layout-align=\"centre\"}\n\n```{.r .cell-code}\nmydata_wide_new <- \n  tibble(y1 = rnorm(50, mean = 0, sd = 0.5),\n         y2 = rnorm(50, mean = 1.2, sd = 0.5)) %>%\n  pivot_longer(cols = c(y1, y2),\n               names_to = \"variable\",  \n               values_to = \"values\") %>% \n  cbind(group = rep(0:1, 50))\n\nmydata_wide_new %>% \n  ggplot(aes(x = group, y = values)) + \n  geom_point() + \n  stat_summary(fun = \"mean\", colour = \"red\", size = 4, geom =\"point\") +\n  stat_summary(fun = \"mean\", geom= \"line\", colour = \"blue\", lty = 2) +\n  xlab(\"Dummy Variable to show groups\") +\n  ylab(\"y1 and y2, on the same scale\") +\n  scale_x_discrete(name = \"Dummy Variable x_i  [0,1]\",\n                limits = c(0,1)) +\n  annotate(\"text\", x = 0, y = 1.5, label = \"Difference in means \\n equals slope in linear model\") \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Continuous limits supplied to discrete scale.\nℹ Did you mean `limits = factor(...)` or `scale_*_continuous()`?\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/Independent_Data_Plots-1.png){fig-align='centre' width=672}\n:::\n\n```{.r .cell-code}\n# Need to use `glue` here to add more annotations\n# Math annotation on graphs\n```\n:::\n\n\n## Model\n\n$$\ny_i = \\beta_0 + \\beta_1 * x_i \\\\\nwhere\\\\\nx_i= \\left\\{\\begin{matrix}\n1\\ when\\ x\\ \\in\\ Group 1\\\\ \n0\\ when\\ x\\ \\in\\ Group2\n\\end{matrix}\\right.\n$$\n\n#  {.unnumbered}\n\nLet us now look at the tests.\n\n## Independent t-test {.tabset}\n\n### Model\n\nThe assumptions here are: - both data sets are normally distributed.\nSmall samples may be assumned to be `t-distributed` - variances are the\nsame - no outliers - observations across data sets are independent\n(obviously)\n\n$$\ny_i = \\beta_0 + \\beta_1 * x_i \\\\\nwhere \\\\\nx_i= \\left\\{\\begin{matrix}\n1\\ when\\ x\\ \\in\\ Group 1\\\\ \n0\\ when\\ x\\ \\in\\ Group2\n\\end{matrix}\\right.\n\\\\\nH_0 : \\beta_1 = 0\n$$\n\nThe t.test computes a statistic as follows:\n\n$$\nt \\ \\ = \\mod(\\bar{x_1}\\ - \\bar{x_2}) / std.error(\\bar{x_1}\\ - \\bar{x_2}) \\\\\n= \\mod(\\bar{x_1} - \\bar{x_2})\\ / \\sqrt{s_x^2 /n_x + s_y^2/n_y}  \\\\\nand\\\\\ndf = n_1 + n_2 - 2\\ \\ \\ \\ ( degrees\\ of\\ freedom)\n$$\n\nThe t-test uses an approximation to the sampling distribution of the\ndifference in sample means based on the Central Limit Theorem, which\nensures that for sufficiently large samples, the sampling distribution\nwill be very close to Normal. The mean of the sampling distribution will\nbe the difference in (underlying) population means, and the variance of\nthe sampling distribution will be the standard error of the difference\nin sample means.\n\nFor the `t-statistic`, note that the numerator of the formula is the\ndifference between means. The denominator is a measurement of\nexperimental error in the two groups combined. The wider the difference\nbetween means, the more confident you are in the data. The more\nexperimental error you have, the less confident you are in the data.\nThus the higher the value of t, the greater the confidence that there is\na difference.\n\nThe `t-statistic` has a t-distribution. It is compared to a *critical\nvalue* of t, for a given probability value ( 0.05 usually). If the\ncalculated t exceeds the critical value, we can assert that the NULL\nHypothesis can be rejected and there could be a significant difference\nin means. ![t-distribution curves](t-distribution.png)\n\n### Code\n\n\n::: {.cell layout-align=\"centre\"}\n\n```{.r .cell-code}\n# Independent t-test\nt5 <- t.test(y2, y, var.equal = TRUE)\nt5 %>% tidy() \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 10\n  estim…¹ estim…² estim…³ stati…⁴ p.value param…⁵ conf.…⁶ conf.…⁷ method alter…⁸\n    <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl> <chr>  <chr>  \n1     0.2     0.5     0.3   0.566   0.573      98  -0.502   0.902 Two S… two.si…\n# … with abbreviated variable names ¹​estimate, ²​estimate1, ³​estimate2,\n#   ⁴​statistic, ⁵​parameter, ⁶​conf.low, ⁷​conf.high, ⁸​alternative\n```\n:::\n\n```{.r .cell-code}\n# Welch test when variances are not equal\nt6 <- t.test(y2,y, var.equal = FALSE)\nt6 %>% tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 10\n  estim…¹ estim…² estim…³ stati…⁴ p.value param…⁵ conf.…⁶ conf.…⁷ method alter…⁸\n    <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl> <chr>  <chr>  \n1     0.2     0.5     0.3   0.566   0.573    90.9  -0.502   0.902 Welch… two.si…\n# … with abbreviated variable names ¹​estimate, ²​estimate1, ³​estimate2,\n#   ⁴​statistic, ⁵​parameter, ⁶​conf.low, ⁷​conf.high, ⁸​alternative\n```\n:::\n\n```{.r .cell-code}\n# Linear Model with Dummy variable\n# lm(value ~ 1 + group, data = mydata_long_y) # also works\nlm6 <- lm(value ~ 1 + I(group == \"y2\"), data = mydata_long_y)\nlm6 %>% tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 5\n  term                     estimate std.error statistic p.value\n  <chr>                       <dbl>     <dbl>     <dbl>   <dbl>\n1 \"(Intercept)\"                 0.3     0.25      1.2     0.233\n2 \"I(group == \\\"y2\\\")TRUE\"      0.2     0.354     0.566   0.573\n```\n:::\n:::\n\n\nWe get the same estimates for means of y and y2 ( 0.3 and 0.5\nrespectively).\n\n### Example\n\n## Welch's t-test {.tabset}\n\n### Model\n\nWelch's test we have already explored as a variant of the t.test for two\nmeans, with variances unequal.\n\nWelch's test is also stated as a *Generalized* Linear Model using, not\n`lm` but the `gls` command from the `nlme` package. This is explained on\n[StackExchange](https://stats.stackexchange.com/questions/142685/equivalent-to-welchs-t-test-in-gls-framework)\nbut we need not digress now.\n\nWhen the variances are unequal, there is a difference in the t-statistic\nthat is computed *only* when the group \\*\\*sizes\\* are different (\nDenominator of t-statistic ).\n\nFrom StackExchange:\\\n$$\nt_w =\\ \\frac {\\bar{x_1}-\\bar{x_2}}{\\sqrt{{s_1^2/n_1}{s_2^2/n_2}}} \n\\\\\\\\\n=\\ \\frac{\\bar{x_1}-\\bar{x_2}} {{\\sqrt{\\frac{s_1^2 + s_2^2}{n}}}} \n\\\\\\\\\n=\\ \\frac{\\bar{x_1}-\\bar{x_2}} {{\\sqrt{\\frac{s_1^2 + s_2^2} {2} * (\\frac{2}{n})}}} \n\\\\\\\\\n=\\ t_s\n$$\n\nSo under these conditions the t-statistic for the Welch t-test is the\nsame as that for the standard t-test.\n\n### Code\n\n\n::: {.cell layout-align=\"centre\"}\n\n```{.r .cell-code}\nt7 <- t.test(y, y2, var.equal = FALSE)\n```\n:::\n\n\n### Example\n\n## Mann-Whitney U Test {.tabset}\n\n(Wilcoxon Independent Sample test)\n\nAs before, when sample groups are not normally distributed, and when\nvariances are different, and they are outliers, a nonparametric rank-sum\ntest is preferred. This is the same as the Wilcoxon Test for independent\nvariables, and is called the Mann-Whitney Test.\n\n### Model\n\nAs before:\n\n$$\nrank(y_i) = \\beta_0 + \\beta_1 * rank(x_i) \\\\\nwhere \\\\\nx_i= \\left\\{\\begin{matrix}\n1\\ when\\ x\\ \\in\\ Group 1\\\\ \n0\\ when\\ x\\ \\in\\ Group2\n\\end{matrix}\\right.\n\\\\\nH_0 : \\beta_1 = 0\n$$\n\n### Code\n\n\n::: {.cell layout-align=\"centre\"}\n\n```{.r .cell-code}\n# As Wilcoxon Test\nw5 <- wilcox.test(y2,y, paired = FALSE)\nw5 %>% tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 4\n  statistic p.value method                                            alternat…¹\n      <dbl>   <dbl> <chr>                                             <chr>     \n1      1264   0.926 Wilcoxon rank sum test with continuity correction two.sided \n# … with abbreviated variable name ¹​alternative\n```\n:::\n\n```{.r .cell-code}\n# As Linear model\nlm5 <- lm(rank(value) ~ 1 + I(group == \"y2\"), \n          data = mydata_long_y)\nlm5 %>% tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 5\n  term                     estimate std.error statistic  p.value\n  <chr>                       <dbl>     <dbl>     <dbl>    <dbl>\n1 \"(Intercept)\"              50.2        4.12   12.2    2.50e-21\n2 \"I(group == \\\"y2\\\")TRUE\"    0.560      5.83    0.0960 9.24e- 1\n```\n:::\n:::\n\n\nNot clear how to explain this. Need to dig more into Mann-Whitney.\n\n### Example\n\n# Tests with Three or More Means {.tabset}\n\n## Model\n\nANOVAs are linear models with (only) categorical predictors so they\nsimply extend everything we did above, relying heavily on dummy coding.\n\n## One-way ANOVA {.tabset}\n\n### Model\n\nOne mean for each group predicts y.\n\n$$\ny=\\beta_0 + \\beta_1*x_1 + \\beta_2*x_2...+\\beta_n*x_n\\\\\nH0:y=β0\n$$\n\nwhere $x_i$ are indicators (x=0 or x=1) where at most one $x_i=1$ while\nall others are $x_i=0$.\n\nNotice how this is just \"more of the same\" of what we already did in\nother models above. When there are only two groups, this model is\n$y=β0+β1∗x$, i.e. the *independent t-test*. If there is only one group,\nit is $y=β0$, i.e. the *one-sample t-test*. This makes *one-way ANOVA* a\n**multiple regression** model.\n\nThis is easy to see in the visualization below - just cover up a few\ngroups and see that it matches the other visualizations above. Let's\nvisualize this using toy data:\n\n\n::: {.cell layout-align=\"centre\"}\n\n```{.r .cell-code}\nN = 15\nD_anova1 = data.frame(\n  y = c(\n    rnorm_fixed(N, 0.5, 0.3),\n    rnorm_fixed(N, 0, 0.3),\n    rnorm_fixed(N, 1, 0.3),\n    rnorm_fixed(N, 0.8, 0.3)\n  ),\n  x = rep(0:3, each = 15)\n)\nymeans = aggregate(y~x, D_anova1, mean)$y\nP_anova1 = ggplot(D_anova1, aes(x=x, y=y)) + \n  stat_summary(fun.y=mean, geom = \"errorbar\", aes(ymax = ..y.., ymin = ..y.., color='intercepts'), lwd=2) + \n  \n  geom_segment(x = -10, xend = 100, \n               y = 0.5, yend = 0.5, \n               lwd = 2, aes(color = 'beta_0')) +\n  geom_segment(x = 0, xend = 1, \n               y = ymeans[1], yend = ymeans[2], \n               lwd = 2, aes(color = 'betas')) +\n  geom_segment(x = 1, xend = 2, \n               y = ymeans[1], yend = ymeans[3], \n               lwd = 2, aes(color = 'betas')) +\n  geom_segment(x = 2, xend = 3, \n               y = ymeans[1], yend = ymeans[4], \n               lwd = 2, aes(color = 'betas')) +\n  \n  scale_color_manual(name = NULL, \n                     values = c(\"blue\", \"red\", \"darkblue\"),\n                     labels=c(bquote(beta[0]*\" (group 1 mean)\"),\n                              bquote(beta[1]*\", \"*beta[2]*\", \n                                     etc. (slopes/differences to \"*beta[0]*\")\"),\n      bquote(beta[0]*\"+\"*beta[1]*\", \"*beta[0]*\"+\"*beta[2]*\", etc. (group 2, 3, ... means)\")\n    )\n  )\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: The `fun.y` argument of `stat_summary()` is deprecated as of ggplot2 3.3.0.\nℹ Please use the `fun` argument instead.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n```\n:::\n\n```{.r .cell-code}\nP_anova1\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: The dot-dot notation (`..y..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(y)` instead.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/Visualizing_One_way_ANOVA-1.png){fig-align='centre' width=672}\n:::\n:::\n\n\n### Code\n\n### Example\n\n## Two-way ANOVA {.tabset}\n\n### Model\n\n### Code\n\n### Example\n\n## ANCOVA {.tabset}\n\n### Model\n\n### Code\n\n### Example\n\n# Proportions {.tabset}\n\n## Model\n\n## Discrete Variables {.tabset}\n\n### Model\n\n### Code\n\n### Example\n\n## Continuous Variables {.tabset}\n\n### Model\n\n### Code\n\n### Example\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}