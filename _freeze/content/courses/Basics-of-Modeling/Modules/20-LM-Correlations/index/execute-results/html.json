{
  "hash": "4d05fb093f10c4cc19364c468d43df21",
  "result": {
    "markdown": "---\ntitle: \"Linear Model: Correlation Test\"\nauthor: \"Arvind Venkatadri\"\ndate: 25/Nov/2022\ndate-modified: \"2023-01-12\"\norder: 20\nformat: html\nexecute: \n  freeze: auto\ncode-fold: true\ncode-summary: \"Show the Code\"\ncode-line-numbers: true\ncode-copy: true\ncode-tools: true\nkeywords: Statistics ; Tests; p-value; Feynman Technique\nabstract: \"This module is intended to assist with making statistically significant insights that drive business decisions. This document deals with the basics of stats. The method followed is that of Jonas Lindoloev, wherein every stat test is treated as a linear model $y = mx + c$\"\n---\n\n\n{{< fa folder-open >}} Slides and Tutorials\n\n<a href=\"./files/cortests.qmd\">\n<i class=\"fa-solid fa-file-powerpoint fa-2x\"></i></a> .nbsp;.nbsp;\n\n<a href=\"./files/cortests.ows\">\n<iconify-icon icon=\"icon-park-solid:orange\"></iconify-icon></a>\n.nbsp;.nbsp;\n\n<a href=\"./files/cortestss.rda\">\n<i class=\"fa-solid fa-person-rays\"></i></a> .nbsp;.nbsp;\n\n<a href=\"./data/data.zip\"> <i class=\"fa-solid fa-database\"></i></a>\n\n\n\n\n\n## The Linear Model\n\nThe premise here is that **many common statistical tests are special\ncases of the linear model**. A **linear model** estimates the\nrelationship between dependent variable or \"response\" variable ($y$) and\nan explanatory variable or \"predictor\" ($x$). It is assumed that the\nrelationship is **linear**. $\\beta_0$ is the *intercept* and $\\beta_1$\nis the slope of the linear fit, that **predicts** the value of y based\nthe value of x.\n\n$$\ny = \\beta_0 + \\beta_1 *x\n$$\n\n## The Correlation Test\n\nTBD: Some introductory text here on Correlation itself)\n\nOne of the basic Questions we would have of our data is: Does some\nvariable depend upon another in some way? Does $y$ depend upon $x$?\n\nA **Correlation Test** is designed to answer exactly this question.\n\nLet us now see how a Correlation Test can be re-formulated as a Linear\nModel + Hypothesis Test.\n\n### Some Toy Data\n\nMost examples in this exposition are based on three \"imaginary\" samples,\n$x, y1, y2$. Each is normally distributed and made up of 50\nobservations. The means and the sds are, respectively:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 2\n     mu    sd\n  <dbl> <dbl>\n1   0     1  \n2   0.3   2  \n3   0.5   1.5\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\nLet us look at our toy data in three ways:\n\n1.  All three variables:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 50 × 3\n    x[,1] y1[,1] y2[,1]\n    <dbl>  <dbl>  <dbl>\n 1  0.427  1.98   0.103\n 2  0.444  1.27  -0.281\n 3 -0.828 -0.944  0.624\n 4 -0.799  0.103  5.43 \n 5 -0.323  2.19   0.168\n 6 -1.24   0.192 -0.549\n 7 -1.36   4.81   0.309\n 8  1.62   1.55  -0.990\n 9 -0.292  3.36  -0.172\n10 -1.25   1.92  -0.158\n# … with 40 more rows\n```\n:::\n:::\n\n\n2.  Variables stacked and labelled (Note: `group` is now a Qual variable\n    !!)\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 150 × 2\n   group value[,1]\n   <chr>     <dbl>\n 1 x         0.427\n 2 y1        1.98 \n 3 y2        0.103\n 4 x         0.444\n 5 y1        1.27 \n 6 y2       -0.281\n 7 x        -0.828\n 8 y1       -0.944\n 9 y2        0.624\n10 x        -0.799\n# … with 140 more rows\n```\n:::\n:::\n\n\n3.  Same as 2, but only for the dependent `y` variables:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 100 × 2\n   group value[,1]\n   <chr>     <dbl>\n 1 y1        1.98 \n 2 y2        0.103\n 3 y1        1.27 \n 4 y2       -0.281\n 5 y1       -0.944\n 6 y2        0.624\n 7 y1        0.103\n 8 y2        5.43 \n 9 y1        2.19 \n10 y2        0.168\n# … with 90 more rows\n```\n:::\n:::\n\n\n# Tests for Correlation\n\nCorrelation **r** is a measure of *strength* and *direction* of *linear\nassociation* between two variables. **r** is between $[-1,+1]$, with $0$\nimplying no association/correlation.\n\nFrom this definition, the *linear model* lends itself in a\nstraightforward way as a model to interpret *correlation*. Intuitively,\nthe slope of the linear model could be related to the correlation\nbetween y and x.\n\nNow we look at the numbers.\n\n## Pearson Correlation\n\n### Model\n\nThe model for Pearson Correlation tests is exactly the Linear Model:\n\n$$\n\\begin{aligned}\ny = \\beta_0 + \\beta_1 \\times x\\\\\n\\\\\nH_0: Null\\ Hypothesis\\ => \\beta_1 = 0\\\\\\\nH_a: Alternate\\ Hypothesis\\ => \\beta_1 \\ne 0\\\\\n\\end{aligned}\n$$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  term                  estimate p.value\n  <chr>                    <dbl>   <dbl>\n1 Pearson Correlation r   -0.232   0.105\n```\n:::\n:::\n\n\nUsing the *linear model* method we get:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 3\n  term   estimate p.value\n  <chr>     <dbl>   <dbl>\n1 beta_0    0.3     0.286\n2 beta_1   -0.464   0.105\n```\n:::\n:::\n\n\nWhy are $r$ and $\\beta_1$ different, though the `p-value` is\nsuspiciously the same!?\n\nDid we miss a factor of $\\frac{-0.463}{-0.231} = 2$ somewhere...??\n\nLet us **scale** the variables to within `{-1, +1}` : (subtract the mean\nand divide by sd) and re-do the Linear Model with **scaled** versions\n$x$ and $y$:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 3\n  term    estimate p.value\n  <chr>      <dbl>   <dbl>\n1 beta_0 -9.06e-17   1    \n2 beta_1 -2.32e- 1   0.105\n```\n:::\n:::\n\n\nSo we conclude:\n\n1.  **When both x and y have the same standard deviation, the slope from\n    the linear model and the Pearson correlation are the same**. Here,\n    since x has twice the `sd` of y, the ratio of **slope** =\n    -0.4635533 to **r** = -0.2317767 is 0.5.\n\n2.  There is this relationship between the **slope in the linear model**\n    and **Pearson correlation**:\n\n$$\nSlope\\ \\beta_1 = \\frac{sd_y}{sd_x} * r\n$$\n\nThe slope is usually much more interpretable and informative than the\ncorrelation coefficient.\n\n2.  Hence a linear model using `scale()` for both variables will show\n    slope = **r**.\n\nSlope_Scaled: -0.2317767 = Correlation: -0.2317767\n\n3.  Finally, the *p-value* for Pearson Correlation and that for the\n    *slope* in the linear model is the same ($0.1053$). Which means we\n    cannot reject the NULL hypothesis of \"no relationship\".\n\n### Example\n\nTBD\n\n## Spearman Correlation\n\n### Model\n\nIn some cases the **LINE** assumptions may not hold.\n\nNonlinear relationships, non-normally distributed data ( with large\n**outliers** ) and working with *ordinal* rather than continuous data:\nthese situations necessitate the use of Spearman's *ranked* correlation\nscores. (**Ranked**, not **sign-ranked**.).\n\nSee the example below: We choose to look at the `gpa_study_hours`\ndataset. It has two numeric columns `gpa` and `study_hours`:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\nRows: 193\nColumns: 2\n$ gpa         <dbl> 4.000, 3.800, 3.930, 3.400, 3.200, 3.520, 3.680, 3.400, 3.…\n$ study_hours <dbl> 10, 25, 45, 10, 4, 10, 24, 40, 10, 10, 30, 7, 15, 60, 10, …\n```\n:::\n:::\n\n\nWe can plot this:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-html/Pearson_example_3-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nHmm...not normally distributed, and there is a sort of increasing\nrelationship, however is it linear? And there is some evidence of\nheteroscedasticity, so the LINE assumptions are clearly in violation.\nPearson correlation would not be the best idea here.\n\nLet us quickly try it anyway, using a Linear Model for the **scaled**\n`gpa` and `study_hours` variables, from where we get:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n    term     estimate    p.value        2.5 %    97.5 %\n1 beta_0 3.047040e-16 1.00000000 -0.141087199 0.1410872\n2 beta_1 1.330138e-01 0.06517072 -0.008440359 0.2744679\n```\n:::\n:::\n\n\nThe correlation estimate is $0.133$; the `p-value` is $0.065$ (and the\n`confidence interval` includes $0$).\n\nHence we fail to reject the NULL hypothesis that `study_hours` and `gpa`\nhave no relationship. But can this be right?\n\nShould we use another test, that does not **need** the LINE assumptions?\n\n## \"Signed Rank\" Values\n\nMost statistical tests use the **actual values** of the data variables.\nHowever, in some *non-parametric* statistical tests, the data are used\nin **rank-transformed** sense/order. (In some cases the **signed-rank**\nof the data values is used instead of the data itself.)\n\n**Signed Rank** is calculated as follows:\n\n1.  Take the absolute value of each observation in a sample\n\n2.  Place the <u>*ranks*</u> in order of (absolute magnitude). The\n    smallest number has *rank = 1* and so on. This gives is **ranked\n    data**.\n\n3.  Give each of the ranks the sign of the original observation ( + or\n    -). This gives us **signed** ranked data.\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n## Plotting Original and Signed Rank Data\n\nLet us see how this might work by comparing data and its signed-rank\nversion...A quick set of plots:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-html/data_plots-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nSo the means of the **ranks** three separate variables seem to be in the\nsame order as the means of the data variables themselves.\n\nHow about associations between data? Do ranks reflect well what the data\nmight?\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-html/Spearman_Plot-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nThe slopes are almost identical, $0.25$ for both original data and\nranked data for $y1\\sim x$. So maybe *ranked* and even *sign_ranked*\ndata could work, and if it can work despite LINE assumptions not being\nsatisfied, that would be nice!\n\n## How does Sign-Rank data work?\n\nTBD: need to add some explanation here.\n\nSpearman correlation = Pearson correlation using the rank of the data\nobservations. Let's check how this holds for a our x and y1 data:\n\nSo the Linear Model for the Ranked Data would be:\n\n$$\n\\begin{aligned}\ny = \\beta_0 + \\beta_1 \\times rank(x)\\\\\n\\\\\nH_0: Null\\ Hypothesis\\ => \\beta_1 = 0\\\\\\\nH_a: Alternate\\ Hypothesis\\ => \\beta_1 \\ne 0\\\\\n\\end{aligned}\n$$\n\n### Code\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  term                    estimate p.value\n  <chr>                      <dbl>   <dbl>\n1 \"Spearman Correlation \"   -0.227   0.113\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 2\n  estimate p.value\n     <dbl>   <dbl>\n1   -0.227   0.114\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 2\n  estimate  p.value\n     <dbl>    <dbl>\n1   31.3   9.11e-10\n2   -0.227 1.14e- 1\n```\n:::\n:::\n\n\nNotes:\n\n1.  When ranks are used, the slope of the linear model ($\\beta_1$) has\n    the same value as the Spearman correlation coefficient ( $\\rho$ ).\n\n2.  Note that the slope from the linear model now has an intuitive\n    interpretation: **the number of ranks y changes for each change in\n    rank of x**. ( Ranks are \"independent\" of `sd` )\n\n### Example\n\nWe examine the `cars93` data, where the numeric variables of interest\nare `weight` and `price`.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-html/Spearman_example_1-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nLet us try a Spearman Correlation score for these variables, since the\ndata are not linearly related and the variance of `price` also is not\nconstant over `weight`\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 5\n  estimate statistic  p.value method                          alternative\n     <dbl>     <dbl>    <dbl> <chr>                           <chr>      \n1    0.883     3074. 1.07e-18 Spearman's rank correlation rho two.sided  \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = rank(price) ~ rank(weight), data = cars93)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-20.0676  -3.0135   0.7815   3.6926  20.4099 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   3.22074    2.05894   1.564    0.124    \nrank(weight)  0.88288    0.06514  13.554   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.46 on 52 degrees of freedom\nMultiple R-squared:  0.7794,\tAdjusted R-squared:  0.7751 \nF-statistic: 183.7 on 1 and 52 DF,  p-value: < 2.2e-16\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/Spearman_example_2-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nWe see that using ranks of the `price` variable, we obtain a Spearman's\n$\\rho = 0.882$ with a `p-value` that is very small. Hence we are able to\nreject the NULL hypothesis and state that there is a relationship\nbetween these two variables. The **linear** relationship is evaluated as\na correlation of `0.882`.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 4\n  estimate p.value conf.low conf.high\n     <dbl>   <dbl>    <dbl>     <dbl>\n1    0.133  0.0652 -0.00838     0.269\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 14\n  parameter1  paramet…¹ effec…² estim…³ conf.…⁴ conf.low conf.…⁵ stati…⁶ df.er…⁷\n  <chr>       <chr>     <chr>     <dbl>   <dbl>    <dbl>   <dbl>   <dbl>   <int>\n1 study_hours gpa       Pearso…   0.133    0.95 -0.00838   0.269    1.85     191\n# … with 5 more variables: p.value <dbl>, method <chr>, n.obs <int>,\n#   conf.method <chr>, expression <list>, and abbreviated variable names\n#   ¹​parameter2, ²​effectsize, ³​estimate, ⁴​conf.level, ⁵​conf.high, ⁶​statistic,\n#   ⁷​df.error\n```\n:::\n:::\n\n\n# References\n\n1.  *Common statistical tests are linear models (or: how to teach\n    stats)* by [Jonas Kristoffer\n    Lindeløv](https://lindeloev.github.io/tests-as-linear/)\n\n2.  [CheatSheet](https://lindeloev.github.io/tests-as-linear/linear_tests_cheat_sheet.pdf)\n\n3.  *Common statistical tests are linear models: a work through* by\n    [Steve Doogue](https://steverxd.github.io/Stat_tests/)\n\n4.  [Jeffrey Walker \"Elements of Statistical Modeling for Experimental\n    Biology\"](https://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/)\n\n5.  Diez, David M & Barr, Christopher D & Çetinkaya-Rundel, Mine:\n    [OpenIntro Statistics](https://www.openintro.org/book/os/)\n\n6.  Modern Statistics with R: From wrangling and exploring data to\n    inference and predictive modelling by [Måns\n    Thulin](http://www.modernstatisticswithr.com/)\n\n7.  [Jeffrey Walker \"A\n    linear-model-can-be-fit-to-data-with-continuous-discrete-or-categorical-x-variables\"](https://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/intro-linear-models.html#a-linear-model-can-be-fit-to-data-with-continuous-discrete-or-categorical-x-variables)\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}