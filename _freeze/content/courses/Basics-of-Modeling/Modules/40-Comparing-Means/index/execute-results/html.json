{
  "hash": "581d75e53030d9e9f2e7b036a658a5e1",
  "result": {
    "markdown": "---\ntitle: \"Comparing Means\"\nauthor: \"Arvind Venkatadri\"\ndate: 26/Nov/2022\ndate=modified: \"2023-01-12\"\ncode-fold: true\ncode-summary: \"Show the Code\"\ncode-line-numbers: true\ncode-tools: true\ncode-copy: true\nkeywords: Statistics ; Tests; p-value; Feynman Technique\nabstract: This module is intended to assist with making statistically significant insights that drive business decisions. This document deals with the basics of stats. The method followed is that of Jonas Lindoloev, wherein every stat test is treated as a linear model `y = mx + c`.\nexecute: \n  freeze: auto\n---\n\n\n\n\n# Structure of this document\n\nWe will follow the following structure: Each kind of Test is described\nin a separate Chapter. The Test *Model* is laid out in formula\n$y = mx + c$ and in *Code*.\n\n# The Linear Model\n\nThe premise here is that **many common statistical tests are special\ncases of the linear model**. So what *is* the linear model?\n\nA **linear model** estimates the relationship between dependent variable\nor \"response\" variable ($y$) and an explanatory variable or \"predictor\"\n($x$).\n\n(It is also possible that there is more than one explanatory variable:\nthis is **multiple regression.**. We will get there later).\n\nIt is assumed that the relationship is **linear**:\n\n$$\ny = \\beta_0 + \\beta_1 *x\n$$\n\n$\\beta_0$ is the *intercept* and $\\beta_1$ is the slope of the linear\nfit, that **predicts** the value of y based the value of x.\n\nEach prediction leaves a small \"residual\" error between the actual and\npredicted values. $\\beta_0$ and $\\beta_1$ are calculated based on\nminimizing the *sum of square*s of these residuals, and hence this\nmethod is called \"ordinary least squares\" regression.\n\n![Least Squares](images/OLS.png)\n\nThe net *area* of all the shaded squares is minimized in the calculation\nof $\\beta_0$ and $\\beta_1$.\n\n$$\ny = \\beta_0 + \\beta_1*x_1 + \\beta_2*x_2 ...+ \\beta_n*x_n\n$$\n\nwhere each of the $\\beta_i$ are slopes defining the relationship between\ny and $x_i$. Together, the RHS of that equation defines an n-dimensional\n*plane*.\n\nAs per Lindoloev, many statistical tests, going from one-sample t-tests\nto two-way ANOVA, are special cases of this system.\n\nAlso see [Jeffrey Walker \"A\nlinear-model-can-be-fit-to-data-with-continuous-discrete-or-categorical-x-variables\"](https://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/intro-linear-models.html#a-linear-model-can-be-fit-to-data-with-continuous-discrete-or-categorical-x-variables)\n\n## Linear Models as Hypothesis Tests\n\nUsing linear models is based on the idea of **Testing of Hypotheses**.\nThe Hypothesis Testing method typically defines a NULL Hypothesis where\nthe statements read as \"**there is no relationship**\" between the\nvariables at hand, explanatory and responses. The Alternative Hypothesis\ntypically states that there *is* a relationship between the variables.\n\nAccordingly, in fitting a linear model, we follow the process as\nfollows:\n\n1.  Make the following hypotheses: $$\n    y = \\beta_0 + \\beta_1 *x \\\\\n    NULL\\ Hypothesis\\ H_0 => x\\ and\\ y\\ are\\ unrelated.\\ (\\beta_1 = 0)\n    $$ $$\n    y = \\beta_0 + \\beta_1 *x \\\\\n    Alternate\\ Hypothesis\\ H_1 => x\\ and\\ y\\ are\\ linearly\\ related\\ (\\beta_1 \\ne 0)\n    $$\n2.  We \"assume\" that $H_0$ is true.\n3.  We calculate $\\beta_1$.\n4.  We then find probability **p** that \\[$\\beta_1 = Estimated\\ Value$\\]\n    **when the NULL Hypothesis** is **assumed** TRUE. This is the\n    **p-value**. If that probability is **p\\>=0.05**, we say we \"cannot\n    reject\" $H_0$ and there is unlikely to be significant linear\n    relationship.\n\nHowever, if **p\\<= 0.05** can we reject the NULL hypothesis, and say\nthat there could be a significant linear relationship,because\n$\\beta_1 = Estimated\\ Value$ by mere chance under $H_0$ is very small.\n\n## Assumptions in Linear Models\n\n1.  **L**: $\\color{blue}{linear}$ relationship\n2.  **I**: Errors are **independent** (across observations)\n3.  **N**: y is $\\color{red}{normally}$ distributed at each \"level\" of\n    x.\n4.  **E**: equal variance at all levels of x. No *heteroscedasticity*.\n    ![OLS Assumptions](images/ols_assumptions.png)\n\nLet us now see which standard statistical tests can be re-formulated as\nLinear Models.\n\n# Data\n\n## Sample Values\n\nMost examples in this exposition are based on three \"imaginary\" samples,\n$x, y, y2$. Each is normally distributed and made up of 50 observations.\nThe means are ($mu = c(0,0.3,0.5)$), and the sds ($sd = c(1,2,1.5)$) are\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 2\n     mu    sd\n  <dbl> <dbl>\n1   0     1  \n2   0.3   2  \n3   0.5   1.5\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\nLet us look at our toy data in three ways:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 50 × 3\n    x[,1] y1[,1] y2[,1]\n    <dbl>  <dbl>  <dbl>\n 1  0.427  1.98   0.103\n 2  0.444  1.27  -0.281\n 3 -0.828 -0.944  0.624\n 4 -0.799  0.103  5.43 \n 5 -0.323  2.19   0.168\n 6 -1.24   0.192 -0.549\n 7 -1.36   4.81   0.309\n 8  1.62   1.55  -0.990\n 9 -0.292  3.36  -0.172\n10 -1.25   1.92  -0.158\n# … with 40 more rows\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 150 × 2\n   group value[,1]\n   <chr>     <dbl>\n 1 x         0.427\n 2 y1        1.98 \n 3 y2        0.103\n 4 x         0.444\n 5 y1        1.27 \n 6 y2       -0.281\n 7 x        -0.828\n 8 y1       -0.944\n 9 y2        0.624\n10 x        -0.799\n# … with 140 more rows\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 100 × 2\n   group value[,1]\n   <chr>     <dbl>\n 1 y1        1.98 \n 2 y2        0.103\n 3 y1        1.27 \n 4 y2       -0.281\n 5 y1       -0.944\n 6 y2        0.624\n 7 y1        0.103\n 8 y2        5.43 \n 9 y1        2.19 \n10 y2        0.168\n# … with 90 more rows\n```\n:::\n:::\n\n\n## \"Signed Rank\" Values\n\nMost statistical tests use the **actual values** of the data variables.\nHowever, in some *non-parametric* statistical tests, the data are used\nin **rank-transformed** sense/order. In some cases the **signed-rank**\nof the data values is used instead of the data itself.\n\nSigned Rank is calculated as follows:\\\n1. Take the absolute value of each observation in a sample\\\n2. Place the <u>*ranks*</u> in order of (absolute magnitude). The\nsmallest number has *rank = 1* and so on.\\\n3. Give each of the ranks the sign of the original observation ( + or -\n)\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n## Plotting Original and Signed Rank Data\n\nA quick plot:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-html/data_plots-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n## How does Sign-Rank data work?\n\nTBD: need to add some explanation here.\n\n# Tests for Correlation {.tabset}\n\nCorrelation **r** is a measure of *strength* and *direction* of *linear\nassociation* between two variables. **r** is between \\[-1,+1\\], with 0\nimplying no association/correlation.\n\nFrom this definition, the *linear model* lends itself in a\nstraightforward way as a model to interpret *correlation*. Intuitively,\nthe slope of the linear model could be related to the correlation\nbetween y and x.\n\nNow we look at the numbers.\n\n## Pearson Correlation {.tabset}\n\n### Model\n\nThe model for Pearson Correlation tests is exactly the Linear Model:\n\n$$\ny = \\beta_0 + \\beta_1 * x \n\\\\\nH_0: \\beta_1 = 0\n$$\n\nSee the Code section for further insights into the relationship between\nthe Correlation Score and the Slope of the Linear Model.\n\n### Code\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 2\n  estimate p.value\n     <dbl>   <dbl>\n1   -0.232   0.105\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 2\n  estimate p.value\n     <dbl>   <dbl>\n1    0.3     0.286\n2   -0.464   0.105\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 2\n   estimate p.value\n      <dbl>   <dbl>\n1 -9.06e-17   1    \n2 -2.32e- 1   0.105\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 2\n   estimate p.value\n      <dbl>   <dbl>\n1 -2.32e- 1   0.105\n2  3   e- 1   0.286\n3 -4.64e- 1   0.105\n4 -9.06e-17   1    \n5 -2.32e- 1   0.105\n```\n:::\n:::\n\n\nNotes: 1. The *p-value* for Pearson Correlation and that for the *slope*\nin the linear model is the same ( 0.1053 ). Which means we cannot reject\nthe NULL hypothesis of \"no relationship\".\n\n2.  Here is the relationship between the slope and correlation:\n\n$$\nSlope\\ \\beta_1 = \\frac{sd_y}{sd_x} * r\n$$\n\nWhen both x and y have the same standard deviation, the slope and\ncorrelation are the same. Here, since x has twice the `sd` of y, the\nratio of **slope** = -0.4635533 to **r** = -0.2317767 is\n0.5. Hence a linear model using `scale()` for both variables will show\nslope = **r**.\n\nSlope_Scaled: -0.2317767 = Correlation: -0.2317767\n\n### Example\n\nWe choose to look at the `gpa_study_hours` dataset. It has two numeric\ncolumns `gpa` and `study_hours`:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\nRows: 193\nColumns: 2\n$ gpa         <dbl> 4.000, 3.800, 3.930, 3.400, 3.200, 3.520, 3.680, 3.400, 3.…\n$ study_hours <dbl> 10, 25, 45, 10, 4, 10, 24, 40, 10, 10, 30, 7, 15, 60, 10, …\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/Pearson_example_1-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nHmm...not normally distributed, and the relationship is also not linear,\nand there is some evidence of heterscedasticity, so Pearson correlation\nwould not be the best idea here.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = gpa ~ study_hours, data = gpa_study_hours)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.95130 -0.19456  0.03879  0.21708  0.73872 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 3.527997   0.037424  94.272   <2e-16 ***\nstudy_hours 0.003328   0.001794   1.855   0.0652 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2837 on 191 degrees of freedom\nMultiple R-squared:  0.01769,\tAdjusted R-squared:  0.01255 \nF-statistic:  3.44 on 1 and 191 DF,  p-value: 0.06517\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPearson's product-moment correlation\n\ndata:  gpa and study_hours\nt = 1.8548, df = 191, p-value = 0.06517\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.008383868  0.269196552\nsample estimates:\n      cor \n0.1330138 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 14\n  parameter1  paramet…¹ effec…² estim…³ conf.…⁴ conf.low conf.…⁵ stati…⁶ df.er…⁷\n  <chr>       <chr>     <chr>     <dbl>   <dbl>    <dbl>   <dbl>   <dbl>   <int>\n1 study_hours gpa       Pearso…   0.133    0.95 -0.00838   0.269    1.85     191\n# … with 5 more variables: p.value <dbl>, method <chr>, n.obs <int>,\n#   conf.method <chr>, expression <list>, and abbreviated variable names\n#   ¹​parameter2, ²​effectsize, ³​estimate, ⁴​conf.level, ⁵​conf.high, ⁶​statistic,\n#   ⁷​df.error\n```\n:::\n:::\n\n\nThe correlation estimate is $0.133$; the `p-value` is 0.065 and the\nconfidence interval includes 0. Hence we fail to reject the NULL\nhypothesis that `study_hours` and `gpa` have no relationship.\n\nWe can use a later package `ggstaplot` to plot this:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stderr}\n```\nRegistered S3 method overwritten by 'ggside':\n  method from   \n  +.gg   ggplot2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/Pearson_example_3-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n## Spearman Correlation\n\n### Model\n\nIn some cases the **LINE** assumptions may not hold. Nonlinear\nrelationships, non-normally distributed data ( with large outliers ) and\nworking with *ordinal* rather than continuous data: these situations\nnecessitate the use of Spearman's *ranked* correlation scores.\n(**Ranked**, not **sign-ranked**.)\n\n$$\nrank(y) = \\beta_0 + \\beta_1 * rank(x) \\\\\nH_0: \\beta_1 = 0\n$$\n\nSpearman correlation = Pearson correlation using the rank of the data\nobservations. Let's check how this holds for a our x and y data:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/Spearman_Plot-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nSlopes are almost identical, \\~ 0.25.\n\n### Code\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 2\n  estimate  p.value\n     <dbl>    <dbl>\n1   -0.227 1.13e- 1\n2   -0.227 1.14e- 1\n3   31.3   9.11e-10\n4   -0.227 1.14e- 1\n```\n:::\n:::\n\n\nNotes:\n\n1.  When ranks are used, the slope of the linear model ($\\beta_1$) has\n    the same value as the correlation coefficient ( $\\rho$ ).\n\n2.  Note that the slope from the linear model now has an intuitive\n    interpretation: **the number of ranks y changes for each change in\n    rank of x**. ( Ranks are \"independent\" of `sd` )\n\n### Example\n\nWe examine the `cars93` data, where the numeric variables of interest\nare `weight` and `price`.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/Spearman_example_1-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nLet us try a Spearman Correlation score for these variables, since the\ndata are not linearly related and the variance of `price` also is not\nconstant over `weight`\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stderr}\n```\nWarning in cor.test.default(x, y, ...): Cannot compute exact p-value with ties\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 5\n  estimate statistic  p.value method                          alternative\n     <dbl>     <dbl>    <dbl> <chr>                           <chr>      \n1    0.883     3074. 1.07e-18 Spearman's rank correlation rho two.sided  \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = rank(price) ~ rank(weight), data = cars93)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-20.0676  -3.0135   0.7815   3.6926  20.4099 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   3.22074    2.05894   1.564    0.124    \nrank(weight)  0.88288    0.06514  13.554   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.46 on 52 degrees of freedom\nMultiple R-squared:  0.7794,\tAdjusted R-squared:  0.7751 \nF-statistic: 183.7 on 1 and 52 DF,  p-value: < 2.2e-16\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/Spearman_example_2-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nWe see that using ranks of the `price` variable, we obtain a Spearman's\n$\\rho = 0.882$ with a `p-value` that is very small. Hence we are able to\nreject the NULL hypothesis and state that there is a relationship\nbetween these two variables. The **linear** relationship is evaluated as\na correlation of `0.882`.\n\n## Conclusion\n\nHopefully, interpreting Statistical Tests in terms of the Linear Model\nhas benefits of improved intuitive understanding.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}