{
  "hash": "d40d1dfcb227a03d264bfcff02b52730",
  "result": {
    "markdown": "---\ntitle: \"Random Forests\"\nauthor: \"Arvind Venkatadri\"\ndate: \"13/06/2020\"\noutput:\n  html_document:\n    df_print: paged\n    toc: yes\n    code_download: TRUE\neditor_options: \n  chunk_output_type: inline\n---\n\n\n\n\n## References:\n1. Machine Learning Basics - Random Forest at [Shirin's Playground](https://shirinsplayground.netlify.app/2018/10/ml_basics_rf/)\n2. \n\n## Penguin Random Forest Model with`randomForest`\n\nUsing the `penguins` dataset and Random Forest Classification.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_…¹ body_…² sex    year\n   <fct>   <fct>              <dbl>         <dbl>      <int>   <int> <fct> <int>\n 1 Adelie  Torgersen           39.1          18.7        181    3750 male   2007\n 2 Adelie  Torgersen           39.5          17.4        186    3800 fema…  2007\n 3 Adelie  Torgersen           40.3          18          195    3250 fema…  2007\n 4 Adelie  Torgersen           NA            NA           NA      NA <NA>   2007\n 5 Adelie  Torgersen           36.7          19.3        193    3450 fema…  2007\n 6 Adelie  Torgersen           39.3          20.6        190    3650 male   2007\n 7 Adelie  Torgersen           38.9          17.8        181    3625 fema…  2007\n 8 Adelie  Torgersen           39.2          19.6        195    4675 male   2007\n 9 Adelie  Torgersen           34.1          18.1        193    3475 <NA>   2007\n10 Adelie  Torgersen           42            20.2        190    4250 <NA>   2007\n# … with 334 more rows, and abbreviated variable names ¹​flipper_length_mm,\n#   ²​body_mass_g\n```\n:::\n\n```{.r .cell-code}\nsummary(penguins)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2                                 \n```\n:::\n\n```{.r .cell-code}\npenguins %>% skimr::skim()\n```\n\n::: {.cell-output-display}\nTable: Data summary\n\n|                         |           |\n|:------------------------|:----------|\n|Name                     |Piped data |\n|Number of rows           |344        |\n|Number of columns        |8          |\n|_______________________  |           |\n|Column type frequency:   |           |\n|factor                   |3          |\n|numeric                  |5          |\n|________________________ |           |\n|Group variables          |None       |\n\n\n**Variable type: factor**\n\n|skim_variable | n_missing| complete_rate|ordered | n_unique|top_counts                  |\n|:-------------|---------:|-------------:|:-------|--------:|:---------------------------|\n|species       |         0|          1.00|FALSE   |        3|Ade: 152, Gen: 124, Chi: 68 |\n|island        |         0|          1.00|FALSE   |        3|Bis: 168, Dre: 124, Tor: 52 |\n|sex           |        11|          0.97|FALSE   |        2|mal: 168, fem: 165          |\n\n\n**Variable type: numeric**\n\n|skim_variable     | n_missing| complete_rate|    mean|     sd|     p0|     p25|     p50|    p75|   p100|hist  |\n|:-----------------|---------:|-------------:|-------:|------:|------:|-------:|-------:|------:|------:|:-----|\n|bill_length_mm    |         2|          0.99|   43.92|   5.46|   32.1|   39.23|   44.45|   48.5|   59.6|▃▇▇▆▁ |\n|bill_depth_mm     |         2|          0.99|   17.15|   1.97|   13.1|   15.60|   17.30|   18.7|   21.5|▅▅▇▇▂ |\n|flipper_length_mm |         2|          0.99|  200.92|  14.06|  172.0|  190.00|  197.00|  213.0|  231.0|▂▇▃▅▂ |\n|body_mass_g       |         2|          0.99| 4201.75| 801.95| 2700.0| 3550.00| 4050.00| 4750.0| 6300.0|▃▇▆▃▂ |\n|year              |         0|          1.00| 2008.03|   0.82| 2007.0| 2007.00| 2008.00| 2009.0| 2009.0|▇▁▇▁▇ |\n:::\n\n```{.r .cell-code}\npenguins <- penguins %>% tidyr::drop_na()\n# Spent one hour trying to find `drop-na()` ( 14 June 2020)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# library(corrplot)\ncor <- penguins %>% select(is.numeric) %>% cor() \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Use of bare predicate functions was deprecated in tidyselect 1.1.0.\nℹ Please use wrap predicates in `where()` instead.\n  # Was:\n  data %>% select(is.numeric)\n\n  # Now:\n  data %>% select(where(is.numeric))\n```\n:::\n\n```{.r .cell-code}\ncor %>% corrplot(., method = \"ellipse\", order = \"hclust\",tl.cex = 0.5)\n```\n\n::: {.cell-output-display}\n![](Random-Forests_files/figure-html/EDA on penguins data-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# try these too:\n# cor %>% corrplot(., method = \"square\", order = \"hclust\",tl.cex = 0.5)\n# cor %>% corrplot(., method = \"color\", order = \"hclust\",tl.cex = 0.5)\n# cor %>% corrplot(., method = \"shade\", order = \"hclust\",tl.cex = 0.5)\n```\n:::\n\n\nNotes:\n- `flipper_length_mm` and `culmen_depth_mm` are negtively correlated at approx (-0.7)\n- `flipper_length_mm` and `body_mass_g` are positively correlated at approx 0.8\n\nSo we will use steps in the recipe to remove correlated variables. \n\n\n### Penguin Data Sampling and Recipe\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data Split\npenguin_split <- initial_split(penguins, prop = 0.6)\npenguin_train <- training(penguin_split)\npenguin_test <- testing(penguin_split)\npenguin_split\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Training/Testing/Total>\n<199/134/333>\n```\n:::\n\n```{.r .cell-code}\nhead(penguin_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 8\n  species   island bill_length_mm bill_depth_mm flipper_le…¹ body_…² sex    year\n  <fct>     <fct>           <dbl>         <dbl>        <int>   <int> <fct> <int>\n1 Adelie    Dream            37.3          17.8          191    3350 fema…  2008\n2 Chinstrap Dream            50.2          18.8          202    3800 male   2009\n3 Gentoo    Biscoe           44.5          14.7          214    4850 fema…  2009\n4 Gentoo    Biscoe           44.4          17.3          219    5250 male   2008\n5 Gentoo    Biscoe           45.2          14.8          212    5200 fema…  2009\n6 Chinstrap Dream            48.5          17.5          191    3400 male   2007\n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n```\n:::\n\n```{.r .cell-code}\n# Recipe\npenguin_recipe <- penguins %>% \n  recipe(species ~ .) %>% \n  step_normalize(all_numeric()) %>% # Scaling and Centering\n  step_corr(all_numeric()) %>%  # Handling correlated variables\n  prep()\n\n# Baking the data\npenguin_train_baked <-  penguin_train %>% \n  bake(object = penguin_recipe, new_data = .)\n\npenguin_test_baked <-  penguin_test %>% \n  bake(object = penguin_recipe, new_data = .)\n\nhead(penguin_train_baked)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 8\n  island bill_length_mm bill_depth_mm flipper_le…¹ body_…² sex      year species\n  <fct>           <dbl>         <dbl>        <dbl>   <dbl> <fct>   <dbl> <fct>  \n1 Dream         -1.22          0.323       -0.711   -1.06  fema… -0.0517 Adelie \n2 Dream          1.14          0.830        0.0737  -0.506 male   1.18   Chinst…\n3 Biscoe         0.0927       -1.25         0.930    0.798 fema…  1.18   Gentoo \n4 Biscoe         0.0745        0.0686       1.29     1.30  male  -0.0517 Gentoo \n5 Biscoe         0.221        -1.20         0.787    1.23  fema…  1.18   Gentoo \n6 Dream          0.824         0.170       -0.711   -1.00  male  -1.28   Chinst…\n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n```\n:::\n:::\n\n\n### Penguin Random Forest Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguin_model <- \n  rand_forest(trees = 100) %>% \n  set_engine(\"randomForest\") %>% \n  set_mode(\"classification\")\npenguin_model\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  trees = 100\n\nComputational engine: randomForest \n```\n:::\n\n```{.r .cell-code}\npenguin_fit <- \n  penguin_model %>% \n  fit(species ~ .,penguin_train_baked)\npenguin_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nparsnip model object\n\n\nCall:\n randomForest(x = maybe_data_frame(x), y = y, ntree = ~100) \n               Type of random forest: classification\n                     Number of trees: 100\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 2.01%\nConfusion matrix:\n          Adelie Chinstrap Gentoo class.error\nAdelie        93         1      1  0.02105263\nChinstrap      2        33      0  0.05714286\nGentoo         0         0     69  0.00000000\n```\n:::\n\n```{.r .cell-code}\n# iris_ranger <- \n#   rand_forest(trees = 100) %>% \n#   set_mode(\"classification\") %>% \n#   set_engine(\"ranger\") %>% \n#   fit(Species ~ ., data = iris_training_baked)\n```\n:::\n\n### Metrics for the Penguin Random Forest Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predictions\npredict(object = penguin_fit, new_data = penguin_test_baked) %>%  \n  dplyr::bind_cols(penguin_test_baked) %>% \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 134\nColumns: 9\n$ .pred_class       <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    <dbl> -0.8946955, -0.6752636, -0.8581235, -0.9312674, -1.7…\n$ bill_depth_mm     <dbl> 0.77955895, 0.42409105, 1.74440040, 0.32252879, 1.99…\n$ flipper_length_mm <dbl> -1.42460769, -0.42573251, -0.78247365, -1.42460769, …\n$ body_mass_g       <dbl> -0.567620576, -1.188572125, -0.691810886, -0.7228584…\n$ sex               <fct> male, female, male, female, male, female, female, ma…\n$ year              <dbl> -1.2818130, -1.2818130, -1.2818130, -1.2818130, -1.2…\n$ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n```\n:::\n\n```{.r .cell-code}\n# Prediction Accuracy Metrics\npredict(object = penguin_fit, new_data = penguin_test_baked) %>%  \n  dplyr::bind_cols(penguin_test_baked) %>% \n  yardstick::metrics(truth = species, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy multiclass         1\n2 kap      multiclass         1\n```\n:::\n\n```{.r .cell-code}\n# Prediction Probabilities\npenguin_fit_probs <- \n  predict(penguin_fit, penguin_test_baked, type = \"prob\") %>%\n  dplyr::bind_cols(penguin_test_baked)\nglimpse(penguin_fit_probs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 134\nColumns: 11\n$ .pred_Adelie      <dbl> 1.00, 1.00, 0.99, 1.00, 0.97, 1.00, 1.00, 0.51, 1.00…\n$ .pred_Chinstrap   <dbl> 0.00, 0.00, 0.01, 0.00, 0.03, 0.00, 0.00, 0.45, 0.00…\n$ .pred_Gentoo      <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.04, 0.00…\n$ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    <dbl> -0.8946955, -0.6752636, -0.8581235, -0.9312674, -1.7…\n$ bill_depth_mm     <dbl> 0.77955895, 0.42409105, 1.74440040, 0.32252879, 1.99…\n$ flipper_length_mm <dbl> -1.42460769, -0.42573251, -0.78247365, -1.42460769, …\n$ body_mass_g       <dbl> -0.567620576, -1.188572125, -0.691810886, -0.7228584…\n$ sex               <fct> male, female, male, female, male, female, female, ma…\n$ year              <dbl> -1.2818130, -1.2818130, -1.2818130, -1.2818130, -1.2…\n$ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n```\n:::\n\n```{.r .cell-code}\n# Confusion Matrix\npenguin_fit$fit$confusion %>% tidy()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: 'tidy.numeric' is deprecated.\nSee help(\"Deprecated\")\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 1\n  x[,\"Adelie\"] [,\"Chinstrap\"] [,\"Gentoo\"] [,\"class.error\"]\n         <dbl>          <dbl>       <dbl>            <dbl>\n1           93              1           1           0.0211\n2            2             33           0           0.0571\n3            0              0          69           0     \n```\n:::\n\n```{.r .cell-code}\n# Gain Curves\npenguin_fit_probs %>% \n  yardstick::gain_curve(species, .pred_Adelie:.pred_Gentoo) %>%\n  autoplot()\n```\n\n::: {.cell-output-display}\n![](Random-Forests_files/figure-html/Penguin Model Predictions and Metrics-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# ROC Plot\npenguin_fit_probs%>%\n  roc_curve(species, .pred_Adelie:.pred_Gentoo) %>%\n  autoplot()\n```\n\n::: {.cell-output-display}\n![](Random-Forests_files/figure-html/Penguin Model Predictions and Metrics-2.png){width=672}\n:::\n:::\n\n### Using `broom` on the penguin model\n\n::: {.cell}\n\n```{.r .cell-code}\npenguin_split\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Training/Testing/Total>\n<199/134/333>\n```\n:::\n\n```{.r .cell-code}\npenguin_split %>% broom::tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 333 × 2\n     Row Data    \n   <int> <chr>   \n 1     2 Analysis\n 2     4 Analysis\n 3     7 Analysis\n 4     8 Analysis\n 5     9 Analysis\n 6    12 Analysis\n 7    13 Analysis\n 8    17 Analysis\n 9    20 Analysis\n10    21 Analysis\n# … with 323 more rows\n```\n:::\n\n```{.r .cell-code}\npenguin_recipe %>% broom::tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 6\n  number operation type      trained skip  id             \n   <int> <chr>     <chr>     <lgl>   <lgl> <chr>          \n1      1 step      normalize TRUE    FALSE normalize_xH3M2\n2      2 step      corr      TRUE    FALSE corr_lZXmq     \n```\n:::\n\n```{.r .cell-code}\n# Following do not work for `random forest models` !! ;-()\n#penguin_model %>% tidy()\n#penguin_fit %>% tidy() \npenguin_model %>% str()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 7\n $ args                 :List of 3\n  ..$ mtry : language ~NULL\n  .. ..- attr(*, \".Environment\")=<environment: R_EmptyEnv> \n  ..$ trees: language ~100\n  .. ..- attr(*, \".Environment\")=<environment: R_EmptyEnv> \n  ..$ min_n: language ~NULL\n  .. ..- attr(*, \".Environment\")=<environment: R_EmptyEnv> \n $ eng_args             : Named list()\n  ..- attr(*, \"class\")= chr [1:2] \"quosures\" \"list\"\n $ mode                 : chr \"classification\"\n $ user_specified_mode  : logi TRUE\n $ method               : NULL\n $ engine               : chr \"randomForest\"\n $ user_specified_engine: logi TRUE\n - attr(*, \"class\")= chr [1:2] \"rand_forest\" \"model_spec\"\n```\n:::\n\n```{.r .cell-code}\npenguin_test_baked\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 134 × 8\n   island    bill_length_mm bill_depth_mm flipper…¹ body_m…² sex    year species\n   <fct>              <dbl>         <dbl>     <dbl>    <dbl> <fct> <dbl> <fct>  \n 1 Torgersen         -0.895         0.780    -1.42  -0.568   male  -1.28 Adelie \n 2 Torgersen         -0.675         0.424    -0.426 -1.19    fema… -1.28 Adelie \n 3 Torgersen         -0.858         1.74     -0.782 -0.692   male  -1.28 Adelie \n 4 Torgersen         -0.931         0.323    -1.42  -0.723   fema… -1.28 Adelie \n 5 Torgersen         -1.72          2.00     -0.212  0.240   male  -1.28 Adelie \n 6 Torgersen         -1.35          0.323    -1.14  -0.630   fema… -1.28 Adelie \n 7 Torgersen         -1.75          0.627    -1.21  -1.10    fema… -1.28 Adelie \n 8 Torgersen          0.367         2.20     -0.497 -0.00876 male  -1.28 Adelie \n 9 Biscoe            -1.13          0.576    -1.92  -1.00    fema… -1.28 Adelie \n10 Biscoe            -1.48          1.03     -0.854 -0.506   fema… -1.28 Adelie \n# … with 124 more rows, and abbreviated variable names ¹​flipper_length_mm,\n#   ²​body_mass_g\n```\n:::\n:::\n\n\n\n\n\n\n## Iris Random Forest Model with `ranger`\n\nUsing the `iris` dataset and Random Forest Classification.\nThis part uses `rsample` to split the data and the `recipes` to *prep* the data for model making. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n#set.seed(100)\niris_split <- rsample::initial_split(iris, prop = 0.6)\niris_split\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Training/Testing/Total>\n<90/60/150>\n```\n:::\n\n```{.r .cell-code}\niris_split %>% training() %>% glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 90\nColumns: 5\n$ Sepal.Length <dbl> 6.3, 4.5, 5.0, 4.8, 4.9, 6.5, 5.9, 6.5, 5.4, 6.3, 5.2, 5.…\n$ Sepal.Width  <dbl> 2.5, 2.3, 3.3, 3.1, 3.6, 3.2, 3.2, 3.0, 3.0, 2.5, 3.5, 3.…\n$ Petal.Length <dbl> 5.0, 1.3, 1.4, 1.6, 1.4, 5.1, 4.8, 5.8, 4.5, 4.9, 1.5, 4.…\n$ Petal.Width  <dbl> 1.9, 0.3, 0.2, 0.2, 0.1, 2.0, 1.8, 2.2, 1.5, 1.5, 0.2, 1.…\n$ Species      <fct> virginica, setosa, setosa, setosa, setosa, virginica, ver…\n```\n:::\n\n```{.r .cell-code}\niris_split %>% testing() %>% glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 60\nColumns: 5\n$ Sepal.Length <dbl> 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.9, 5.4, 5.4, 5.1, 5.…\n$ Sepal.Width  <dbl> 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 3.1, 3.7, 3.9, 3.5, 3.…\n$ Petal.Length <dbl> 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.5, 1.5, 1.3, 1.4, 1.…\n$ Petal.Width  <dbl> 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.1, 0.2, 0.4, 0.3, 0.…\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n```\n:::\n:::\n\n\n\n### Iris Data Pre-Processing: Creating the Recipe\n\nThe `recipes` package provides an interface that specializes in *data pre-processing*. Within the package, the functions that start, or execute, the data transformations are named after **cooking actions**. That makes the interface more user-friendly. For example:\n\n - `recipe()` - Starts a new set of transformations to be applied, similar to the `ggplot()` command. Its main argument is the model’s `formula`.\n\n - `prep()` - Executes the transformations on top of the data that is supplied (**typically, the training data**). Each data transformation is a `step()` function. ( Recall what we did with the `caret` package: *Centering, Scaling, Removing Correlated variables*...)\n\nNote that in order to avoid data leakage (e.g: transferring information from the train set into the test set), data should be “prepped” using the **train_tbl** only. <https://towardsdatascience.com/modelling-with-tidymodels-and-parsnip-bae2c01c131c>\nCRAN: The idea is that the preprocessing operations will all be **created** using the *training set* and then these steps will be **applied** to both the training and test set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Pre Processing the Training Data\n\niris_recipe <- \n  training(iris_split) %>% # Note: Using TRAINING data !!\n  recipe(Species ~ .)      # Note: Outcomes ~ Predictors !!\n\n# The data contained in the `data` argument need not be the training set; this data is only used to catalog the names of the variables and their types (e.g. numeric, etc.).\n```\n:::\n\n\nQ: How does the recipe \"figure\" out which are the outcomes and which are the predictors?\nA.The `recipe` command defines `Outcomes` and `Predictors` using the formula interface. ~~Not clear how this recipe \"figures\" out which are the outcomes and which are the predictors, when we have not yet specified them...~~\n\nQ. Why is the recipe not agnostic to data set? Is that a meaningful question?\nA. The use of the `training set` in the recipe command is just to declare the variables and specify the `roles` of the data, nothing else. `Roles` are open-ended and extensible. \nFrom <https://cran.r-project.org/web/packages/recipes/vignettes/Simple_Example.html> :\n\n> This document demonstrates some basic uses of recipes. First, some definitions are required:\n- **variables** are the original (raw) data columns in a data frame or tibble. For example, in a traditional formula Y ~ A + B + A:B, the variables are A, B, and Y.\n- **roles** define how variables will be used in the model. Examples are: `predictor` (independent variables), `response`, and `case weight`. This is meant to be open-ended and extensible.\n- **terms** are columns in a **design matrix** such as A, B, and A:B. These can be other derived entities that are grouped, such as a set of `principal components` or a set of columns, that define a `basis function` for a variable. These are synonymous with `features` in machine learning. Variables that have `predictor` roles would automatically be main `effect terms`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Apply the transformation steps\niris_recipe <- iris_recipe %>% \n  step_corr(all_predictors()) %>% \n  step_center(all_predictors(), -all_outcomes()) %>% \n  step_scale(all_predictors(), -all_outcomes()) %>% \n  prep()\n```\n:::\n\n\nThis has created the `recipe()` and prepped it too. \nWe now need to apply it to our datasets:\n\n- Take `training` data and `bake()` it to prepare it for modelling. \n- Do the same for the `testing` set. \n\n\n::: {.cell}\n\n```{.r .cell-code}\niris_training_baked <- \n  iris_split %>% \n  training() %>% \n  bake(iris_recipe,.)\niris_training_baked\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 90 × 4\n   Sepal.Length Sepal.Width Petal.Width Species   \n          <dbl>       <dbl>       <dbl> <fct>     \n 1      0.503       -1.10         0.859 virginica \n 2     -1.78        -1.53        -1.37  setosa    \n 3     -1.15         0.633       -1.51  setosa    \n 4     -1.40         0.200       -1.51  setosa    \n 5     -1.27         1.28        -1.65  setosa    \n 6      0.756        0.417        0.999 virginica \n 7     -0.00423      0.417        0.720 versicolor\n 8      0.756       -0.0169       1.28  virginica \n 9     -0.638       -0.0169       0.302 versicolor\n10      0.503       -1.10         0.302 versicolor\n# … with 80 more rows\n```\n:::\n\n```{.r .cell-code}\niris_testing_baked <- \n  iris_split %>% \n  testing() %>% \n  bake(iris_recipe,.)\niris_testing_baked \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 60 × 4\n   Sepal.Length Sepal.Width Petal.Width Species\n          <dbl>       <dbl>       <dbl> <fct>  \n 1       -1.27      -0.0169       -1.51 setosa \n 2       -1.53       0.417        -1.51 setosa \n 3       -1.65       0.200        -1.51 setosa \n 4       -1.15       1.28         -1.51 setosa \n 5       -0.638      1.93         -1.23 setosa \n 6       -1.65       0.850        -1.37 setosa \n 7       -1.15       0.850        -1.51 setosa \n 8       -1.27       0.200        -1.65 setosa \n 9       -0.638      1.50         -1.51 setosa \n10       -0.638      1.93         -1.23 setosa \n# … with 50 more rows\n```\n:::\n:::\n\n\n### Iris Model Training using `parsnip`\n\nDifferent ML packages provide different interfaces (APIs ) to do the same thing  (e.g random forests). The `tidymodels` package provides a consistent interface to invoke a wide variety of packages supporting a wide variety of models. \n\nThe `parsnip` package is a successor to `caret`. \n\nTo model with `parsnip`:\n1. Pick a `model` : \n2. Set the `engine`\n3. Set the `mode` (if needed): *Classification* or *Regression*\n\nCheck [here](https://tidymodels.github.io/parsnip/articles/articles/Models.html) for models available in `parsnip`. \n\n - Mode: *classification* and *regression* in `parsnip`, each using a variety of models. ( **Which Way**). This defines the form of the output. \n \n - Engine: The `engine` is the **R package** that is invoked by `parsnip` to execute the model. E.g `glm`, `glmnet`,`keras`.( **How** ) `parsnip` provides **wrappers** for models from these packages. \n \n - Model: is the **specific technique** used for the modelling task. E.g `linear_reg()`, `logistic_reg()`, `mars`, `decision_tree`, `nearest_neighbour`...(What model). \n \n and models have:\n - `hyperparameters`: that are numerical or factor variables that `tune` the model ( Like the alpha beta parameters for Bayesian priors)\n\nWe can use the `random forest` model to **classify** the iris into species. Here Species is the `Outcome` variable and the rest are `predictor` variables. The `random forest` model is provided by the `ranger` package, to which `tidymodels/parsnip` provides a simple and consistent interface.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ranger)\niris_ranger <- \n  rand_forest(trees = 100) %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"ranger\") %>% \n  fit(Species ~ ., data = iris_training_baked)\n```\n:::\n\n\n`ranger` can generate random forest models for `classification`, `regression`, `survival`( time series, time to event stuff). `Extreme Forests` are also supported, wherein all points in the dataset are used ( instead of bootstrap samples) along with `feature bagging`. \nWe can also run the same model using the `randomForest` package:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(randomForest,quietly = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nrandomForest 4.7-1.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nType rfNews() to see new features/changes/bug fixes.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'randomForest'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:ranger':\n\n    importance\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:ggplot2':\n\n    margin\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:dplyr':\n\n    combine\n```\n:::\n\n```{.r .cell-code}\niris_rf <- \n  rand_forest(trees = 100) %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"randomForest\") %>% \n  fit(Species ~ ., data = iris_training_baked)\n```\n:::\n\n### Iris Predictions\n\nThe `predict()` function run against a `parsnip` model returns a prediction `tibble`. By default, the prediction variable is called `.pred_class`. \n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(object = iris_ranger, new_data = iris_testing_baked) %>%  \n  dplyr::bind_cols(iris_testing_baked) %>% \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 60\nColumns: 5\n$ .pred_class  <fct> setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n$ Sepal.Length <dbl> -1.2718283, -1.5253489, -1.6521092, -1.1450680, -0.638026…\n$ Sepal.Width  <dbl> -0.01685809, 0.41663561, 0.19988876, 1.28362301, 1.933863…\n$ Petal.Width  <dbl> -1.509753, -1.509753, -1.509753, -1.509753, -1.231029, -1…\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n```\n:::\n:::\n\n\n### Iris Classification Model Validation\n\nWe use `metrics()` function from the `yardstick` package to evaluate how good the model is. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(iris_ranger, iris_testing_baked) %>%\n  dplyr::bind_cols(iris_testing_baked) %>% \n  yardstick::metrics(truth = Species, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy multiclass     0.983\n2 kap      multiclass     0.974\n```\n:::\n:::\n\n\nWe can also check the metrics for `randomForest` model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(iris_rf, iris_testing_baked) %>%\n  dplyr::bind_cols(iris_testing_baked) %>% \n  yardstick::metrics(truth = Species, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy multiclass     0.983\n2 kap      multiclass     0.974\n```\n:::\n:::\n\n\n### Iris Per-Classifier Metrics\n\nWe can use the parameter `type = \"prob\"` in the `predict()` function to obtain a probability score on each prediction. \n**TBD: How is this prob calculated?** Possible answer: the Random Forest model outputs its answer by majority voting across n trees. Each of the possible answers( i.e. predictions) for a particular test datum gets a share of the vote, that represents its probability. Hence each dataum in the test vector can show a probability for the \"winning\" answer. ( Quite possibly we can get the probabilities for *all possible outcomes* for **each test datum**)\n\n\n::: {.cell}\n\n```{.r .cell-code}\niris_ranger_probs <- \n  predict(iris_ranger, iris_testing_baked, type = \"prob\") %>%\n  dplyr::bind_cols(iris_testing_baked)\nglimpse(iris_ranger_probs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 60\nColumns: 7\n$ .pred_setosa     <dbl> 0.8063568, 0.9904167, 0.9673690, 0.9822222, 0.9605714…\n$ .pred_versicolor <dbl> 0.150555556, 0.006333333, 0.026523810, 0.013333333, 0…\n$ .pred_virginica  <dbl> 0.043087607, 0.003250000, 0.006107143, 0.004444444, 0…\n$ Sepal.Length     <dbl> -1.2718283, -1.5253489, -1.6521092, -1.1450680, -0.63…\n$ Sepal.Width      <dbl> -0.01685809, 0.41663561, 0.19988876, 1.28362301, 1.93…\n$ Petal.Width      <dbl> -1.509753, -1.509753, -1.509753, -1.509753, -1.231029…\n$ Species          <fct> setosa, setosa, setosa, setosa, setosa, setosa, setos…\n```\n:::\n\n```{.r .cell-code}\niris_rf_probs <- \n  predict(iris_rf, iris_testing_baked, type = \"prob\") %>%\n  dplyr::bind_cols(iris_testing_baked)\nglimpse(iris_rf_probs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 60\nColumns: 7\n$ .pred_setosa     <dbl> 0.80, 0.99, 0.99, 1.00, 0.99, 0.99, 0.99, 0.85, 0.99,…\n$ .pred_versicolor <dbl> 0.15, 0.01, 0.01, 0.00, 0.00, 0.00, 0.00, 0.12, 0.00,…\n$ .pred_virginica  <dbl> 0.05, 0.00, 0.00, 0.00, 0.01, 0.01, 0.01, 0.03, 0.01,…\n$ Sepal.Length     <dbl> -1.2718283, -1.5253489, -1.6521092, -1.1450680, -0.63…\n$ Sepal.Width      <dbl> -0.01685809, 0.41663561, 0.19988876, 1.28362301, 1.93…\n$ Petal.Width      <dbl> -1.509753, -1.509753, -1.509753, -1.509753, -1.231029…\n$ Species          <fct> setosa, setosa, setosa, setosa, setosa, setosa, setos…\n```\n:::\n\n```{.r .cell-code}\n# Tabulating the probabilities\nftable(iris_rf_probs$.pred_versicolor)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  0 0.01 0.02 0.04 0.05 0.06 0.07 0.08 0.1 0.12 0.13 0.14 0.15 0.16 0.18 0.5 0.55 0.69 0.7 0.74 0.75 0.78 0.79 0.81 0.83 0.91 0.95 0.96 0.97 0.98  1\n                                                                                                                                                    \n 17    6    2    1    2    1    1    1   2    2    2    2    2    1    1   1    1    2   1    1    1    1    1    1    1    1    1    1    1    1  1\n```\n:::\n\n```{.r .cell-code}\nftable(iris_rf_probs$.pred_virginica)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  0 0.01 0.02 0.03 0.04 0.05 0.09 0.12 0.17 0.18 0.19 0.2 0.22 0.23 0.31 0.45 0.78 0.79 0.84 0.85 0.86 0.9 0.91 0.92 0.93 0.94 0.95 0.99  1\n                                                                                                                                           \n  9   13    2    4    2    1    1    1    2    1    1   1    2    1    1    1    1    2    1    1    1   1    1    1    1    1    3    1  2\n```\n:::\n\n```{.r .cell-code}\nftable(iris_rf_probs$.pred_setosa)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  0 0.01 0.03 0.08 0.09 0.1 0.27 0.79 0.8 0.85 0.88 0.94 0.95 0.99  1\n                                                                     \n 21    1    2    6    2   1    1    1   1    2    1    1    1   15  4\n```\n:::\n:::\n\n```\n\n### Iris Classifier: Gain and ROC Curves\n\nWe can plot gain and ROC curves for each of these models\n\n::: {.cell}\n\n```{.r .cell-code}\niris_ranger_probs %>% \n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %>% \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 139\nColumns: 5\n$ .level          <chr> \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"set…\n$ .n              <dbl> 0, 1, 3, 4, 5, 6, 8, 10, 11, 12, 15, 18, 19, 20, 21, 2…\n$ .n_events       <dbl> 0, 1, 3, 4, 5, 6, 8, 10, 11, 12, 15, 18, 19, 20, 21, 2…\n$ .percent_tested <dbl> 0.000000, 1.666667, 5.000000, 6.666667, 8.333333, 10.0…\n$ .percent_found  <dbl> 0.000000, 3.846154, 11.538462, 15.384615, 19.230769, 2…\n```\n:::\n\n```{.r .cell-code}\niris_ranger_probs %>% \n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %>% \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](Random-Forests_files/figure-html/Gain and ROC Curves `ranger`-1.png){width=672}\n:::\n\n```{.r .cell-code}\niris_ranger_probs %>% \n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %>% \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 142\nColumns: 4\n$ .level      <chr> \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\"…\n$ .threshold  <dbl> -Inf, 0.0000000000, 0.0003846154, 0.0012500000, 0.00163461…\n$ specificity <dbl> 0.0000000, 0.0000000, 0.2058824, 0.2647059, 0.3235294, 0.3…\n$ sensitivity <dbl> 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0…\n```\n:::\n\n```{.r .cell-code}\niris_ranger_probs %>% \n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %>% \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](Random-Forests_files/figure-html/Gain and ROC Curves `ranger`-2.png){width=672}\n:::\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\niris_rf_probs %>% \n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %>% \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 78\nColumns: 5\n$ .level          <chr> \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"set…\n$ .n              <dbl> 0, 4, 19, 20, 21, 22, 24, 25, 26, 27, 28, 30, 36, 38, …\n$ .n_events       <dbl> 0, 4, 19, 20, 21, 22, 24, 25, 26, 26, 26, 26, 26, 26, …\n$ .percent_tested <dbl> 0.000000, 6.666667, 31.666667, 33.333333, 35.000000, 3…\n$ .percent_found  <dbl> 0.00000, 15.38462, 73.07692, 76.92308, 80.76923, 84.61…\n```\n:::\n\n```{.r .cell-code}\niris_rf_probs %>% \n  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %>% \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](Random-Forests_files/figure-html/Gain and ROC Curves `randomForest`-1.png){width=672}\n:::\n\n```{.r .cell-code}\niris_rf_probs %>% \n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %>% \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 81\nColumns: 4\n$ .level      <chr> \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"setosa\"…\n$ .threshold  <dbl> -Inf, 0.00, 0.01, 0.03, 0.08, 0.09, 0.10, 0.27, 0.79, 0.80…\n$ specificity <dbl> 0.0000000, 0.0000000, 0.6176471, 0.6470588, 0.7058824, 0.8…\n$ sensitivity <dbl> 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0…\n```\n:::\n\n```{.r .cell-code}\niris_rf_probs %>% \n  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %>% \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](Random-Forests_files/figure-html/Gain and ROC Curves `randomForest`-2.png){width=672}\n:::\n:::\n\n\n\n### Iris Classifier: Metrics\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(iris_ranger, iris_testing_baked, type = \"prob\") %>% \n  bind_cols(predict(iris_ranger,iris_testing_baked)) %>% \n  bind_cols(select(iris_testing_baked,Species)) %>% \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 60\nColumns: 5\n$ .pred_setosa     <dbl> 0.8063568, 0.9904167, 0.9673690, 0.9822222, 0.9605714…\n$ .pred_versicolor <dbl> 0.150555556, 0.006333333, 0.026523810, 0.013333333, 0…\n$ .pred_virginica  <dbl> 0.043087607, 0.003250000, 0.006107143, 0.004444444, 0…\n$ .pred_class      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setos…\n$ Species          <fct> setosa, setosa, setosa, setosa, setosa, setosa, setos…\n```\n:::\n\n```{.r .cell-code}\npredict(iris_ranger, iris_testing_baked, type = \"prob\") %>% \n  bind_cols(predict(iris_ranger,iris_testing_baked)) %>% \n  bind_cols(select(iris_testing_baked,Species)) %>% \n  yardstick::metrics(data = ., truth = Species, estimate = .pred_class, ... = .pred_setosa:.pred_virginica)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 3\n  .metric     .estimator .estimate\n  <chr>       <chr>          <dbl>\n1 accuracy    multiclass     0.983\n2 kap         multiclass     0.974\n3 mn_log_loss multiclass     0.152\n4 roc_auc     hand_till      0.994\n```\n:::\n\n```{.r .cell-code}\n# And for the `randomForest`method\n\npredict(iris_rf, iris_testing_baked, type = \"prob\") %>% \n  bind_cols(predict(iris_ranger,iris_testing_baked)) %>% \n  bind_cols(select(iris_testing_baked,Species)) %>% \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 60\nColumns: 5\n$ .pred_setosa     <dbl> 0.80, 0.99, 0.99, 1.00, 0.99, 0.99, 0.99, 0.85, 0.99,…\n$ .pred_versicolor <dbl> 0.15, 0.01, 0.01, 0.00, 0.00, 0.00, 0.00, 0.12, 0.00,…\n$ .pred_virginica  <dbl> 0.05, 0.00, 0.00, 0.00, 0.01, 0.01, 0.01, 0.03, 0.01,…\n$ .pred_class      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setos…\n$ Species          <fct> setosa, setosa, setosa, setosa, setosa, setosa, setos…\n```\n:::\n\n```{.r .cell-code}\npredict(iris_rf, iris_testing_baked, type = \"prob\") %>% \n  bind_cols(predict(iris_ranger,iris_testing_baked)) %>% \n  bind_cols(select(iris_testing_baked,Species)) %>% \n  yardstick::metrics(data = ., truth = Species, estimate = .pred_class, ... = .pred_setosa:.pred_virginica)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 3\n  .metric     .estimator .estimate\n  <chr>       <chr>          <dbl>\n1 accuracy    multiclass     0.983\n2 kap         multiclass     0.974\n3 mn_log_loss multiclass     0.139\n4 roc_auc     hand_till      0.994\n```\n:::\n:::\n",
    "supporting": [
      "Random-Forests_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}