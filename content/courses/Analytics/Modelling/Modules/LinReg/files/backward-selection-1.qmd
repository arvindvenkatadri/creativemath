---
title: "Tutorial: Multiple Linear Regression with Backward Selection"
author: "Arvind V."
date: 03/May/2023
date-modified: "`r Sys.Date()`"
categories: 
  - Linear Regression
  - Quantitative Predictor
  - Quantitative Response
  - Sum of Squares
  - Residuals
abstract: "Using Multiple Regression to predict Quantitative Target Variables"
bibliography: 
  - grateful-refs.bib
citation: true
#suppress-bibliography: true
---

## {{< iconify noto-v1 package >}} Setting up R Packages

```{r}
#| label: setup
#| message: true
#| warning: false
options(scipen = 1, digits = 3) #set to three decimal 
knitr::opts_chunk$set(echo = TRUE,warning = FALSE,message = FALSE)
library(tidyverse)
library(ggformula)
library(mosaic)
library(infer)

```

```{r}
#| label: Extra Pedagogical Packages
#| echo: false
#| message: false

library(checkdown)
library(epoxy)
library(TeachHist)
library(TeachingDemos)
library(grateful)

```

```{r}
#| label: plot theme
# Let us set a plot theme for Data visualisation

# my_theme <- function(){  # Creating a function
#   theme_classic() +  # Using pre-defined theme as base
#   theme(axis.text.x = element_text(size = 12, face = "bold"),  # Customizing axes text      
#         axis.text.y = element_text(size = 12, face = "bold"),
#         axis.title = element_text(size = 14, face = "bold"),  # Customizing axis title
#         panel.grid = element_blank(),  # Taking off the default grid
#         plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), units = , "cm"),
#         legend.text = element_text(size = 12, face = "italic"),  # Customizing legend text
#         legend.title = element_text(size = 12, face = "bold"),  # Customizing legend title
#         legend.position = "right",  # Customizing legend position
#         plot.caption = element_text(size = 12))  # Customizing plot caption
# }   

my_theme <- function(){  # Creating a function
  theme_classic() +  # Using pre-defined theme as base
  theme(plot.title = element_text(face = "bold", size = 14),
        axis.text.x = element_text(size = 10, face = "bold"),  
        # Customizing axes text      
        axis.text.y = element_text(size = 10, face = "bold"),
        axis.title = element_text(size = 12, face = "bold"),  
        # Customizing axis title
        panel.grid = element_blank(),  # Taking off the default grid
        plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), units = , "cm"),
        legend.text = element_text(size = 8, face = "italic"),  
        # Customizing legend text
        legend.title = element_text(size = 10, face = "bold"),  
        # Customizing legend title
        legend.position = "right",  # Customizing legend position
        plot.caption = element_text(size = 8))  # Customizing plot caption
}   

```

In this tutorial, we will use the Boston housing dataset. Our research
question is:

::: callout-note
## Research Question

How do we predict the price of a house in Boston, based on other
parameters Quantitative parameters such as area, location, rooms, and
crime-rate in the neighbourhood?

And how do we choose the "best" model, based on a tradeoff between Model
Complexity and Model Accuracy?
:::

## {{< iconify flat-color-icons workflow >}} Workflow: Read the Data

```{r}
data("BostonHousing2", package = "mlbench")
housing <- BostonHousing2
inspect(housing)

```

The original data are 506 observations on 14 variables, `medv` being the
target variable:

|         |                                                                       |
|:-------------|:---------------------------------------------------------|
| crim    | per capita crime rate by town                                         |
| zn      | proportion of residential land zoned for lots over 25,000 sq.ft       |
| indus   | proportion of non-retail business acres per town                      |
| chas    | Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) |
| nox     | nitric oxides concentration (parts per 10 million)                    |
| rm      | average number of rooms per dwelling                                  |
| age     | proportion of owner-occupied units built prior to 1940                |
| dis     | weighted distances to five Boston employment centres                  |
| rad     | index of accessibility to radial highways                             |
| tax     | full-value property-tax rate per USD 10,000                           |
| ptratio | pupil-teacher ratio by town                                           |
| b       | $1000(B - 0.63)^2$ where B is the proportion of Blacks by town        |
| lstat   | percentage of lower status of the population                          |
| medv    | median value of owner-occupied homes in USD 1000's                    |

The corrected data set has the following additional columns:

|       |                                                              |
|:------|:-------------------------------------------------------------|
| cmedv | corrected median value of owner-occupied homes in USD 1000's |
| town  | name of town                                                 |
| tract | census tract                                                 |
| lon   | longitude of census tract                                    |
| lat   | latitude of census tract                                     |

Our response variable is `cmedv`, the *corrected median value of
owner-occupied homes in USD 1000'*s. Their are many Quantitative feature
variables that we can use to predict `cmedv`. And there are two
Qualitative features, `chas` and `tax`.

## {{< iconify flat-color-icons workflow >}} Workflow: Correlations

We can use `purrr` to evaluate all pair-wise correlations in one shot:

```{r}
#| label: corrtest plots

all_corrs <- housing %>% 
  select(where(is.numeric)) %>% 
  
  # leave off cmedv/medv to get all the remaining ones
  select(- cmedv, -medv) %>%  
  
  # perform a cor.test for all variables against cmedv
  purrr::map(.x = .,
             .f = \(x) cor.test(x, housing$cmedv)) %>%
  
  # tidy up the cor.test outputs into a tidy data frame
  map_dfr(broom::tidy, .id = "predictor") 

all_corrs

all_corrs %>%
  gf_hline(yintercept = 0,
           color = "grey",
           linewidth = 2) %>%
  gf_errorbar(
    conf.high + conf.low ~ reorder(predictor, estimate),
    colour = ~ estimate,
    width = 0.5,
    linewidth = ~ -log10(p.value),
    caption = "Significance = -log10(p.value)"
  ) %>%
  gf_point(estimate ~ reorder(predictor, estimate)) %>%
  gf_labs(x = "Predictors", y = "Correlation with Median House Price") %>%
  gf_theme(my_theme()) %>%
  gf_theme(theme(axis.text.x = element_text(angle = 45, hjust = 1))) %>%
  gf_refine(
    scale_colour_distiller("Correlation", type = "div", palette = "RdBu"),
    scale_linewidth_continuous("Significance", range = c(0.25, 3)),
    guides(linewidth = guide_legend(reverse = TRUE)))

```

The variables `rm`, `lstat` seem to have high correlations with `cmedv`
which are also statistically significant.

## {{< iconify flat-color-icons workflow >}} Workflow: Maximal Multiple Regression Model

We will create a regression model for `cmedv` using **all** the other
numerical predictor features in the dataset.

```{r}
#| label: maximal model
housing_numeric <- housing %>% select(where(is.numeric), 
                                      
                    # remove medv
                    # an older version of cmedv
                                      -c(medv))
names(housing_numeric) # 16 variables, one target, 15 predictors

housing_maximal <- lm(cmedv ~ ., data = housing_numeric)
summary(housing_maximal)

```

The maximal model has an R.squared of $0.7426$ which is much better than
that we obtained for a simple model based on `rm` alone. How much can we
simplify this maximal model, without losing out on `R.squared`?

## {{< iconify flat-color-icons workflow >}} Workflow: Model Reduction

We now proceed naively by [removing one predictor after
another.]{.underline} We will resort to what may amount to **p-hacking**
by *sorting the predictors based on their p-value*[^1] in the maximal
model and removing them in decreasing order of their p-value.

We will also use some powerful features from the `purrr` package (also
part of the `tidyverse`), to create **all** these models all at once.
Then we will be able to plot their `R.squared` values together and
decide where we wish to trade off *Explainability vs Complexity* for our
model.

```{r}
#| label: Maximal Model and p-hacking

# No of Quant predictor variables in the dataset
n_vars <- housing %>%
  select(where(is.numeric), -c(cmedv, medv)) %>%
  length()

# Maximal Model, now tidied
housing_maximal_tidy <- 
  housing_maximal %>% 
  broom::tidy() %>% 
  
# Obviously remove "Intercept" ;-D
  filter(term != "(Intercept)") %>% 
  
# And horrors! Sort variables by p.value
  arrange(p.value)

housing_maximal_tidy

```

The last 5 variables are clearly statistically insignificant.

And now we unleash the `purrr` package to create all the simplified
models at once. We will construct a dataset containing three columns:

-   A list of all quantitative predictor variables
-   A sequence of numbers from 1 to `N(predictor)`
-   A "list" column containing the `housing` data frame itself

We will use the iteration capability of `purrr` to **sequentially drop
one variable at a time** from the maximal(15 predictor) model, build a
new reduced model each time, and compute the `r.squared`:

::: {layout="[ [1,1], [1]]"}
```{r}
#| label: All models at once

housing_model_set <- tibble(all_vars = 
                            list(housing_maximal_tidy$term), # p-hacked order!!
                            keep_vars = seq(1, n_vars),
                            data = list(housing_numeric))
housing_model_set

# Unleash purrr in a series of mutates
housing_model_set <- housing_model_set %>%
  
# list of predictor variables for each model
  mutate(mod_vars =
           pmap(
             .l = list(all_vars, keep_vars, data),
             .f = \(all_vars, keep_vars, data) all_vars[1:keep_vars]
           )) %>%
  
# build formulae with these for linear regression
  mutate(formula = 
           map(.x = mod_vars,
               .f = \(mod_vars) as.formula(paste(
                         "cmedv ~", paste(mod_vars, collapse = "+")
                       )))) %>%
  
# use the formulae to build multiple linear models
  mutate(models =
           pmap(
             .l = list(data, formula),
             .f = \(data, formula) lm(formula, data = data)
           ))
# Check everything after the operation
housing_model_set

# Tidy up the models using broom to expose their metrics
housing_models_tidy <- housing_model_set %>% 
  mutate(tidy_models =
           map(
             .x = models,
             .f = \(x) broom::glance(x,
                                     conf.int = TRUE,
                                     conf.lvel = 0.95)
           )) %>% 

  # Remove unwanted columns, keep model and predictor count
  select(keep_vars, tidy_models) %>%
  unnest(tidy_models)

housing_models_tidy %>%
  gf_line(
    r.squared ~ keep_vars,
    ylab = "R.Squared",
    xlab = "No. params in the Linear Model",
    title = "Model Explainability vs Complexity",
    subtitle = "Model r.squared vs No. of Predictors",
    data = .
  ) %>%
  
  # Plot r.squared vs predictor count
  gf_point(r.squared ~ keep_vars,
           size = 3.5, color = "grey") %>%
  
  # Show off the selected best model
  gf_point(
    r.squared ~ keep_vars,
    size = 3.5,
    color = "red",
    data = housing_models_tidy %>% filter(keep_vars == 4)
  ) %>%
  
  gf_hline(yintercept = 0.7, linetype = "dashed") %>%
  gf_theme(my_theme())


```
:::

At the loss of some 5% in the `r.squared`, we can drop our model
complexity from 15 predictors to say 4! Our final model will then be:

```{r}
#| label: Final Model
housing_model_final <- 
  housing_model_set %>% 
  
  # filter for best model, with 4 variables
  filter(keep_vars == 4) %>% 
  
  # tidy up the model
  mutate(tidy_models =
           map(
             .x = models,
             .f = \(x) broom::tidy(x,
                                     conf.int = TRUE,
                                     conf.lvel = 0.95)
           )) %>% 
  
  # Remove unwanted columns, keep model and predictor count
  select(keep_vars, models, tidy_models) %>%
  unnest(tidy_models)

housing_model_final
  
  
housing_model_final %>%  
  # And plot the model
  # Remove the intercept term
  filter(term != "(Intercept)") %>% 
  gf_col(estimate ~ term, fill = ~ term, width = 0.25) %>% 
  gf_hline(yintercept = 0) %>% 
  gf_errorbar(conf.low + conf.high ~ term, 
              width = 0.1, 
              title = "Multiple Regression",
              subtitle = "Model Estimates with Confidence Intervals") %>% 
  gf_theme(my_theme())


```

Our current best model can be stated as:

$$
\widehat{cmedv} \sim 24.459 - 0.563 * dis - 0.673 * lstat - 0.957 * ptratio  + 4.199 * rm
$$

## {{< iconify flat-color-icons workflow >}} Workflow: Diagnostics

Let us use `broom::augment` to calculate residuals and predictions to
arrive at a quick set of diagnostic plots.

```{r}
housing_model_final_augment <- 
  housing_model_set %>% 
  filter(keep_vars == 4) %>% 
  
# augment the model
  mutate(augment_models =
           map(
             .x = models,
             .f = \(x) broom::augment(x)
           )) %>% 
  unnest(augment_models) %>% 
  select(cmedv:last_col())

housing_model_final_augment

```

```{r}
#| label: Diagnostic Plots for Multiple Regression
#| layout-nrow: 2
#| message: false
#| warning: false
housing_model_final_augment %>% 
  gf_point(.resid ~ .fitted, title = "Residuals vs Fitted") %>%
  gf_smooth() %>% 
  gf_theme(my_theme)

housing_model_final_augment %>% 
  gf_qq(~ .std.resid, title = "Q-Q Residuals") %>% 
  gf_qqline() %>%
  gf_theme(my_theme)

housing_model_final_augment %>% 
  gf_point(sqrt(.std.resid) ~ .fitted, 
           title = "Scale-Location Plot") %>%
    gf_smooth() %>% 
  gf_theme(my_theme)

housing_model_final_augment %>% 
  gf_point(.std.resid ~ .hat, 
           title = "Residuals vs Leverage") %>%
    gf_smooth() %>% 
  gf_theme(my_theme)

```

The residuals plot shows a curved trend, and certainly does not resemble
the *stars at night*, so it is possible that we have left out some
possible richness in our model-making, a "systemic inadequacy".

The Q-Q plot of residuals also shows a `J-shape` which indicates a
non-normal distribution of residuals.

These could indicate that more complex model ( e.g. linear model with
**interactions** between variables ( i.e. product terms ) may be
necessary.

## {{< iconify fluent-mdl2 decision-solid >}} Conclusion

We have used a multiple-regression workflow that takes all predictor
variables into account in a linear model, and then systematically
simplified that model such that the performance was just adequate.

The models we chose were all linear of course, but without *interaction
terms* : each predictor was used only for its **main effect**. When the
diagnostic plots were examined, we did see some shortcomings in the
model. This could be overcome with a more complex model. These might
include selected interactions, transformations of target($cmedv^2$, or
$sqrt(cmedv)$) and some selected predictors.

## {{< iconify ooui references-rtl >}} References

::: {#refs style="font-size: 60%;"}
James, Witten, Hastie, Tibshirani, *An Introduction to Statistical
Learning. Chapter 3. Linear Regression.*
<https://hastie.su.domains/ISLR2/Labs/Rmarkdown_Notebooks/Ch3-linreg-lab.html>

###### {{< iconify lucide package-check >}} R Package Citations

```{r}
#| echo: false
#scan_packages()
cite_packages(
  output = "table",
  out.dir = ".",
  out.format = "html",
  pkgs = c("corrgram", "corrplot", "GGally","gt",
           "infer", "ISLR", "janitor", "reghelper")
) %>%
  knitr::kable(format = "simple")

```
:::

[^1]: James, Witten, Hastie, Tibshirani,*An Introduction to Statistical
    Learning*. Chapter 3. Linear Regression
    <https://hastie.su.domains/ISLR2/Labs/Rmarkdown_Notebooks/Ch3-linreg-lab.html>
