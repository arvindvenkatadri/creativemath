---
title: "Modelling Linear Regression"
author: "Arvind Venkatadri"
date: 13/Apr/2023
date-modified: "`r Sys.Date()`"
order: 20
image: featured.jpg
image-alt: ""
categories: 
  - Linear Regression
  - Quantitative Predictor
  - Quantitative Response
  - Sum of Squares
  - Residuals
abstract: "Using Regression to predict Quantitative Target Variables"
---

## {{< fa folder-open >}} Slides and Tutorials {#sec-tutorials}

|                                                                                                                  |     |                                                                                                             |     |
|------------------|-------------------|------------------|------------------|
| <a href="./files/multiple-regression.qmd"><i class="fa-brands        fa-r-project"></i>Multiple Regression</a>   |     | <a href="./files/lin-perm.qmd"> <i class="fa-bramds fa-r-project"></i> Permutation Test for Regression</a>  |     |

## {{< iconify noto-v1 package >}} Setting up R Packages

```{r}
#| label: setup
#| message: true
#| warning: false
knitr::opts_chunk$set(echo = TRUE,warning = FALSE,message = FALSE)
library(tidyverse)
library(ggformula)
library(mosaic)

```

```{r}
#| label: plot theme
# Let us set a plot theme for Data visualisation

my_theme <- function(){  # Creating a function
  theme_classic() +  # Using pre-defined theme as base
  theme(axis.text.x = element_text(size = 12, face = "bold"),  # Customizing axes text      
        axis.text.y = element_text(size = 12, face = "bold"),
        axis.title = element_text(size = 14, face = "bold"),  # Customizing axis title
        panel.grid = element_blank(),  # Taking off the default grid
        plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), units = , "cm"),
        legend.text = element_text(size = 12, face = "italic"),  # Customizing legend text
        legend.title = element_text(size = 12, face = "bold"),  # Customizing legend title
        legend.position = "right",  # Customizing legend position
        plot.caption = element_text(size = 12))  # Customizing plot caption
}                                                                         

```

## {{< iconify openmoji japanese-symbol-for-beginner >}} Introduction

One of the most common problems in Prediction Analytics is that of
predicting a Quantitative response variable, based on one or more
Quantitative predictor variables or *features*. This is called Linear
Regression. We will use the intuitions built up during our study of
ANOVA to develop our ideas about Linear Regression.

Suppose we have data on salaries in a Company, with years of study and
previous experience. Would we be able to predict the prospective salary
of a new candidate, based on their years of study and experience? Or
based on mileage done, could we predict the resale price of a used car?
These are typical problems in Linear Regression.

In this tutorial, we will use the Boston housing dataset. Our research
question is:

::: callout-note
## Research Question

How do we predict the price of a house in Boston, based on other
parameters Quantitative parameters such as area, location, rooms, and
crime-rate in the neighbourhood?
:::

### Read the Data

```{r}
data("BostonHousing2", package = "mlbench")
housing <- BostonHousing2
inspect(housing)

```

The original data are 506 observations on 14 variables, `medv` being the
target variable:

|         |                                                                       |
|:-------------------------|:---------------------------------------------|
| crim    | per capita crime rate by town                                         |
| zn      | proportion of residential land zoned for lots over 25,000 sq.ft       |
| indus   | proportion of non-retail business acres per town                      |
| chas    | Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) |
| nox     | nitric oxides concentration (parts per 10 million)                    |
| rm      | average number of rooms per dwelling                                  |
| age     | proportion of owner-occupied units built prior to 1940                |
| dis     | weighted distances to five Boston employment centres                  |
| rad     | index of accessibility to radial highways                             |
| tax     | full-value property-tax rate per USD 10,000                           |
| ptratio | pupil-teacher ratio by town                                           |
| b       | $1000(B - 0.63)^2$ where B is the proportion of Blacks by town        |
| lstat   | percentage of lower status of the population                          |
| medv    | median value of owner-occupied homes in USD 1000's                    |

The corrected data set has the following additional columns:

|       |                                                              |
|:------|:-------------------------------------------------------------|
| cmedv | corrected median value of owner-occupied homes in USD 1000's |
| town  | name of town                                                 |
| tract | census tract                                                 |
| lon   | longitude of census tract                                    |
| lat   | latitude of census tract                                     |

Our response variable is `cmedv`, the *corrected median value of
owner-occupied homes in USD 1000'*s. Their are many Quantitative feature
variables that we can use to predict `cmedv`. And there are two
Qualitative features, `chas` and `tax`.

## Workflow: EDA

Let us select a few sets of Quantitative and Qualitative features, along
with the target variable `cmedv` and do a pairs-plots with them:

```{r}
#| label: pairs plots 1
#| message: false
#| warning: false
housing %>% 
  # Target variable and Rooms / Age / Distance to City Centres / Radial Highway Access 
  select(cmedv, rm, age, dis) %>% 
  GGally::ggpairs()


```

Clearly, `rm` (avg. number of rooms) is a big determining feature for
median price `cmedv`. This we infer based on the large magnitudes of
correlations of `rm` with`cmedv`. The variable`age` (proportion of
owner-occupied units built prior to 1940) may also be a significant
influence on `cmedv`. (See top row)

```{r}
#| label: pairs plots 2
#| message: false
#| warning: false
housing %>% 
# Target variable and Access to Radial Highways, / Resid. Land Proportion / proportion of non-retail business acres / full-value property-tax rate per USD 10,000
  select(cmedv, rad, zn, indus, tax) %>% 
  GGally::ggpairs()

```

None of these Quant variables have a overly strong correlation with
`cmedv`. (See top row)

```{r}
#| label: pairs plots 3
#| message: false
#| warning: false
housing %>% 
  # Target variable and Crime Rate / Nitrous Oxide / Black Population / Lower Status Population
  select(cmedv, crim, nox, rad, b, lstat) %>% 
  GGally::ggpairs()

```

The variable `lstat` (proportion of lower classes in the neighbourhood)
as expected, has a strong (negative) correlation with `cmedv`;
`rad`(index of accessibility to radial highways), `nox`(nitrous oxide)
and `crim`(crime rate) also have fairly large correlations with `cmedv`.

Let us also check the Qualitative variables:

```{r}
#| label: pairs plots 4
#| message: false
#| warning: false
#| 
housing %>% 
  # Target variable and Access to Charles River
  select(cmedv,chas) %>% 
  GGally::ggpairs()


```

Access to the Charles river (`chas`) does seem to affect the prices
somewhat. Let us plot (again) scatter plots of Quant Variables that have
strong correlation with `cmedv`:

```{r}
#| layout-nrow: 1
#| 
gf_point(data = housing, 
             cmedv ~ age, 
         title = "Price vs Proportion of houses older than 1940",
         ylab = "Median Price",
         xlab = "Proportion of older-than-1940 buildings") %>% 
  gf_theme(theme = my_theme)

gf_point(data = housing,
         cmedv ~ rm, 
         title = "Price vs Average no. of Rooms",
         ylab = "(cmedv) Median Price",
         xlab = "(rm) Avg. No. of Rooms"
         ) %>% 
  gf_theme(theme = my_theme)


```

We have managed to get a decent idea which *predictor variables* might
be useful in modelling `cmedv`.

::: callout-note
## Simple Regression vs Multiple Regression

We would typically start with a **maximal model**[^1] and progressively
simplify the model by knocking off predictors that have the least impact
on model accuracy. We will do that in another tutorial (see above
@sec-tutorials) but for simplicity we will first use just one predictor
`rm`(Avg. no. of Rooms) to model housing prices. Later we will include
other predictor/features as well.
:::

## Workflow: Model Building

::: panel-tabset
### Model Code

We will first execute the `lm` test with code and evaluate the results.
Then we will do an intuitive walk through of the process and finally,
hand-calculate entire analysis for clear understanding. R offers a very
simple command to execute an Linear Model: Note the familiar `formula`
of stating the variables: ( $y \sim x$; where $y$ = target, $x$ =
predictor)

```{r}
#| label: Linear Model with code
housing_lm <- lm(cmedv ~ rm, data = housing)
summary(housing_lm)

```

The model for $\widehat{cmedv}$ , the prediction for `cmedv`can be
written in the form of $y = mx + c$, as:

$$
\widehat{cmedv} \sim -34.65924 + 9.09967* rm
$$ {#eq-rm-model}

The effect of `rm` on predicting `cmedv` is significant, a (slope) value
of $9.09967$ at p-value of $<2.2e-16$. The F-statistic for the Linear
Model is given by $474.3$, which is very high. The `R-squared` value is
48% which means that `rm` is able to explain about half of the trend in
`cmedv`; there is substantial variation in `cmedv` that is left to
explain, an indication that we should perhaps use a richer model, with
more predictors. We will explore this in the Tutorials. @sec-tutorials

We can plot the scatter plot of these two variables with the model also
over-plotted.

```{r}
#| label: model
#| echo: true

# Tidy Dataframe for the model using `broom`
housing_lm_tidy <- housing_lm %>% broom::tidy(conf.int= TRUE, 
                                              conf.level = 0.95)
housing_lm_tidy

housing_lm_augment <- housing_lm %>% broom::augment(se_fit = TRUE,
                                                    interval = "confidence")
housing_lm_augment

intercept <- housing_lm_tidy %>%
  filter(term == "(Intercept)") %>%
  select(estimate) %>%
  as.numeric()

slope <- housing_lm_tidy %>%
  filter(term == "rm") %>%
  select(estimate) %>%
  as.numeric()

gf_point(
  data = housing,
  cmedv ~ rm,
  title = "Price vs Average no. of Rooms",
  ylab = "Median Price",
  xlab = "Avg. No. of Rooms",
  alpha = 0.2
) %>%
  
  gf_abline(slope = slope,
            intercept = intercept,
            colour = "red")  %>%
  
  gf_segment(
    0 + 29 ~ 7 + 7,
    linetype = "dashed",
    color = "blue",
    arrow = arrow(
      angle = 30,
      length = unit(0.25, "inches"),
      ends = "last",
      type = "closed"
    ),
    data = housing_lm_augment
  ) %>%
  
  gf_segment(
    29 + 29 ~ 2.5 + 7,
    linetype = "dashed",
    arrow = arrow(
      angle = 30,
      length = unit(0.25, "inches"),
      ends = "first",
      type = "closed"
    ),
    color = "blue",
    data = housing_lm_augment
  ) %>%
  
  gf_refine(
    scale_x_continuous(limits = c(2.5, 10),
                       expand = c(0, 0)),
    # removes plot panel margins
    scale_y_continuous(limits = c(0, 55),
                       expand = c(0, 0))
  ) %>%
  gf_theme(theme = my_theme)

```

For any new value of `rm`, we go up to the red line and read off the
predicted median price. That is how the model is used.

In practice, we use the `broom` package functions to obtain a clear view
of the model parameters and predictions of `cmedv` for all *existing*
values of `rm`. To predict `cmedv` with *new* values of `rm`, we use
`predict`. We see estimates for the intercept and slope (`rm`) for the
linear model, along with the standard errors and p.values. And we see
the fitted values of `cmedv` for the existing `rm`; these values will
naturally line **on** the straight-line depicting the model. We will
examine this `augment`-ed data more in the @sec-diagnostics.

```{r}
#| label: predictions with new data
new <- data.frame(rm = seq(3, 10))
new %>% mutate(predictions =
                 stats::predict(
                   object = housing_lm,
                   newdata = .,
                   se.fit = FALSE
                 ))
```

Note that "negative values" for predicted `cmedv` would have no meaning!

### Linear Model Intuitive {#sec-lm-intuitive}

All that is very well, but what is happening under the hood of the `lm`
command? Consider the `cmedv` (target) variable and the `rm`
feature/predictor variable. What we do is:

1.  Plot a scatter plot `gf_point(cmedv ~ rm, housing)`
2.  Find a line that, in some way, gives us some prediction of `cmedv`
    for any given `rm`
3.  Calculate the errors in prediction and use those to find the "best"
    line.
4.  Use that "best" line henceforth as a model for prediction.

How does one fit the "best" line? Consider a choice of "lines" that we
can use to fit to the data. Here are 6 lines of varying slopes (and
intercepts ) that we can try as candidates for the best fit line:

```{r}
#| echo: false
set.seed(1234)
housing_sample <- housing_lm_augment %>% 
  slice_sample(n = 15)
mean_cmedv_sample <-
  mean( ~ cmedv, na.rm = TRUE, data = housing_sample)
mean_rm_sample <- mean( ~ rm, na.rm = TRUE, data = housing_sample)

lm_sample <- tibble(
  slope = slope + c(5, 2, 0, -2, -5, -slope),
  intercept = intercept + c(-30, -15, 0, 10, 30, -intercept + mean_cmedv_sample),
  # List column containing `housing_sample`
  # No repetition needed !!!
  # Auto recycle for each of slope + intercept
  sample = list(housing_sample)
)

lm_sample <- lm_sample %>% mutate(line =
                                    pmap(
                                      .l = list(intercept, slope, sample),
                                      .f = \(intercept, slope, sample) tibble(pred = intercept + slope * sample$rm,
       rm = sample$rm))) %>%
  
  mutate(graphs = pmap(
    .l = list(sample, line),
    .f = \(sample, line)
    gf_point(cmedv ~ rm, data = sample, size = 2) %>%
      gf_line(
        pred ~ rm,
        data = line ,
        color = "blue",
        linewidth = 2
      ) %>%
      gf_refine(
        scale_x_continuous(limits = c(2.5, 10),
                           expand = c(0, 0)),
        # removes plot panel margins
        scale_y_continuous(limits = c(0, 55),
                           expand = c(0, 0))
      ) %>%
      gf_theme(theme = my_theme)
  )) 


```

```{r}
#| echo: false
#| layout-ncol: 3
lm_sample %>% pluck("graphs",1)
lm_sample %>% pluck("graphs",2)
lm_sample %>% pluck("graphs",3)
lm_sample %>% pluck("graphs",4)
lm_sample %>% pluck("graphs",5)
lm_sample %>% pluck("graphs",6)


```

It should be apparent that while we cannot determine which line may be
the best, the **worst** line seems to be the one in the final plot,
which ignores the x-variable `rm` altogether. This corresponds to the
*NULL Hypothesis*, that there is *no relationship* between the two
variables. Any of the other lines could be a decent candidate, so how do
we decide?

```{r}
#| echo: false
p1 <- gf_point(cmedv ~ rm,
         data = housing_sample) %>% 
  gf_hline(yintercept =  ~ mean_cmedv_sample,
           color = "dodgerblue") %>% 
  gf_segment(data = housing_sample, 
             color = "green",
             mean_cmedv_sample + cmedv ~ rm + rm,
  title = "Price vs Average no. of Rooms",
  subtitle = "NULL Hypothesis",
  ylab = "cmedv (Median Price)",
  xlab = "rm (Avg. No. of Rooms)") %>%
  gf_theme(theme = my_theme)

p2 <- gf_point(cmedv ~ rm, 
         data = housing_sample) %>% 
  gf_abline(intercept = intercept, slope = slope) %>% 
  gf_segment(.fitted + cmedv ~ rm + rm, 
             color = "red",
  title = "Price vs Average no. of Rooms",
  subtitle = "Alternative Hypothesis",
  ylab = "cmedv (Median Price)",
  xlab = "rm (Avg. No. of Rooms)") %>%
  gf_abline(slope = slope,
            intercept = intercept,
            colour = "dodgerblue") %>%

  gf_theme(theme = my_theme)

p1 + p2 + patchwork::plot_annotation(tag_levels = c("A", "B"),)

```

In Fig A, the *horizontal* [blue line]{style="color: dodgerblue;"} is
the overall mean of `cmedv`, denoted as $\mu_{tot}$. The vertical [green
lines]{style="color: green;"} to the points show the departures of each
point from this overall mean, called
[**residuals**]{style="background-color: yellow;"}. The sum of *squares*
of these residuals in Fig A is called the [**Total Sum of Squares**
(SST)]{style="background-color: yellow;"}.

$$
SST = \Sigma (y - \mu_{tot})^2
$$ {#eq-SST}

In Fig B, the vertical [red lines]{style="color: red;"} are the
residuals of each point from the potential line of fit. The sum of the
*squares* of these lines is called the [**Total Error Sum of Squares**
(SSE)]{style="background-color: yellow;"}.

$$
SSE = \Sigma [(y - a - b * rm)^2]
$$ {#eq-SSE}

It should be apparent that if there is any positive linear relationship
between `cmedv` and `rm`,then $SSE < SST$.

How do we get the optimum slope + intercept? If we plot the $SSE$ as a
function of varying slope, we get

```{r}
#| label: slopes vs SSE
#| echo: false
tibble(b = slope + seq(-5,5),
                  a = intercept,
                  dat = list(tibble(cmedv = housing_sample$cmedv, 
                               rm = housing_sample$rm))) %>% 
  mutate(r_squared= pmap_dbl(
    .l = list(a,b,dat),
    .f = \(a,b, dat) sum((dat$cmedv - (b*dat$rm + a))^2))) %>% 
  gf_point(r_squared ~ b,data = .) %>% 
  gf_line(ylab = "SSE", xlab = "slope",title = "Error vs Slope") %>% 
  gf_theme(my_theme)

```

We see that there is a quadratic minimum $SSE$ at the optimum value of
slope and at all other slopes, the $SSE$ is higher. We can use this to
find the optimum slope, which is what the function `lm` does.

### Linear Models Manually Demonstrated (Apologies to Spinoza)

Let us hand-calculate the numbers so we know what the test is doing.
Here is the SST: we pretend that there is no relationship between
`cmedv` ans `rm` and compute a **NULL** model:

```{r}
#| label: SST Total Sum of Squares
# Calculate overall sum squares SST

SST <- deviance(lm(cmedv ~ 1, data = housing))
SST

```

And here is the SSE:

```{r}
#| label: SSE Within Group Sum of Squares

SSE <- deviance(housing_lm)
SSE

```

Given that the model leaves **unexplained** variations in `cmedv` to the
extent of $SSE$, we can compute the $SSR$, [the Regression Sum of
Squares]{style="background-color: yellow;"}, the amount of variation in
`cmedv` that the linear model **does** explain:

```{r}
#| label: SSR
SSR <- SST - SSE
SSR

```

We have $SST = 42577.74$, $SSE = 21934.39$ and therefore
$SSR = 20643.35$.

In order to calculate the F-Statistic, we need to compute the variances,
using these sum of squares. We obtain variances by dividing by their
*Degrees of Freedom*:

$$
F_{stat} = \frac{SSR / df_{SSR}}{SSE / df_{SSE}}
$$

where $df_{SSR}$ and $df_{SSE}$ are respectively the degrees of freedom
in SSR and SSE.

Let us calculate these Degrees of Freedom. If we have $n=$
`r dim(housing)[[1]]` observations of data, then:

-   $SST$ clearly has degree of freedom $n-1$, since it uses all
    observations but loses one degree to calculate the global mean.
-   $SSE$ was computed using the slope and intercept, so it has $(n-2)$
    as degrees of freedom.
-   And therefore $SSR$ has just $1$ degree of freedom.

Now we are ready to compute the F-statistic:

```{r}
n <- housing %>% count() %>% as.numeric()
df_SSR <- 1
df_SSE <- n -2
F_stat <- (SSR/df_SSR) / (SSE/df_SSE)
F_stat

```

The F-stat is compared with a **critical value** of the F-statistic,
which is computed using the formula for the f-distribution in R. As with
our hypothesis tests, we set the significance level to 0.95, and quote
the two relevant degrees of freedom as parameters to `qf()` which
computes the critical F value as a **quartile**:

```{r}
F_crit <-  qf(p = 0.95,     # Significance level is 5%
              df1 = df_SSR, # Numerator degrees of freedom 
              df2 = df_SSE) # Denominator degrees of freedom
F_crit
F_stat

```

The F_crit value can also be seen in a plot[^2]:

```{r}
mosaic::pdist(dist = "f",
              q = F_crit, 
              df1 = df_SSR, df2 = df_SSE)
```

Any value of F more than the $F_{crit}$ occurs with smaller probability
than 0.05. Our F_stat is much higher than $F_{crit}$, by orders of
magnitude! And so we can say with confidence that `rm` has a significant
effect on `cmedv`.

The value of `R.squared` is also calculated from the previously computed
sums of squares:

$$
R.squared = \frac{SSR}{SST} = \frac{SSY-SSE}{SST}
$$

```{r}
r_squared <- (SST - SSE)/SST
r_squared
# Also computable by
# mosaic::rsquared(housing_lm)

```

So `R.squared` = `r (SST - SSE)/SST`

The value of Slope and Intercept are computed using a maximum likelihood
derivation and the knowledge that the means square error is a minimum at
the optimum slope: for a linear model $y \sim mx + c$

$$
slope = \frac{\Sigma[(y - y_{mean})*(x - x_{mean})]}{\Sigma(x - x_{mean})^2}
$$

and

$$
Intercept = y_{mean} - slope * x_{mean}
$$

```{r}
#| label: slope and intercept
slope <- mosaic::cov(cmedv ~ rm, data = housing) / mosaic::var(~ rm, data = housing)
slope

intercept <- mosaic::mean(~ cmedv, data = housing) - slope * mosaic::mean(~ rm, data = housing)
intercept

```

So, there we are! All of this is done for us by one simple formula,
`lm()`!
:::

## Workflow: Model Checking and Diagnostics {#sec-diagnostics}

We will follow much of the treatment on Linear Model diagnostics, given
[here on the STHDA
website](http://www.sthda.com/english/articles/39-regression-model-diagnostics/161-linear-regression-assumptions-and-diagnostics-in-r-essentials/#homogeneity-of-variance).

> A first step of this regression diagnostic is to inspect the
> significance of the regression beta coefficients, as well as, the
> R.square that tells us how well the linear regression model fits to
> the data.
>
> For example, the linear regression model makes the assumption that the
> relationship between the predictors (x) and the outcome variable is
> linear. This might not be true. The relationship could be polynomial
> or logarithmic.
>
> Additionally, the data might contain some influential observations,
> such as outliers (or extreme values), that can affect the result of
> the regression.
>
> Therefore, the regression model must be closely diagnosed in order to
> detect potential problems and to check whether the assumptions made by
> the linear regression model are met or not. To do so, we generally
> examine the distribution of **residuals errors**, that can tell us
> more about our data.

#### Checks for Uncertainty

Let us first look at the uncertainties in the estimates of slope and
intercept. These are most easily read off from the `broom::tidy`-ed
model:

```{r}
#| label: uncertainty and confidence
# housing_lm_tidy <-  housing_lm %>% broom::tidy()
housing_lm_tidy

```

Plotting this is simple too:

```{r}
housing_lm_tidy %>%
  gf_col(estimate ~ term, fill = ~ term, width = 0.25) %>% 
  gf_hline(yintercept = 0) %>% 
  gf_errorbar(conf.low + conf.high ~ term, linewidth = 1, title = "Model Estimates with Confidence Intervals") %>% 
  gf_theme(my_theme())

```

#### Checks for Constant Variance/Heteroscedasticity

Linear Modelling makes 4 fundamental assumptions:("**LINE**")

1.  **Linear** relationship between y and x
2.  Observations are **independent**.
3.  Residuals are **normally** distributed
4.  Variance of the `y` variable is **equal** at all values of `x`.

We can check these using checks and graphs: Here we plot the residuals
against the independent/feature variable and see if there is a gross
variation in their range

```{r}
#| label: Check for Constant Variance
#| #| layout-nrow: 2
#| 
housing_lm_augment %>% 
  gf_point(.resid ~ .fitted, title = "Residuals vs Fitted") %>%
  gf_smooth() %>% 
  gf_theme(my_theme)

housing_lm_augment %>% 
  gf_qq(~ .std.resid, title = "Q-Q Residuals") %>% gf_qqline() %>%
  gf_theme(my_theme)

housing_lm_augment %>% 
  gf_point(sqrt(.std.resid) ~ .fitted, title = "Scale-Location Plot") %>%
    gf_smooth() %>% 
  gf_theme(my_theme)

housing_lm_augment %>% 
  gf_point(.std.resid ~ .hat, title = "Residuals vs Leverage") %>%
    gf_smooth() %>% 
  gf_theme(my_theme)
```

::: column-margin
Base R has a crisp command to plot these diagnostic graphs. But we will
continue to use `ggformula`.

```{r}
#| eval: false
plot(housing_lm)

```
:::

The residuals are not quite "like the night sky", i.e. random enough.
The Q-Q plot of residuals also has significant deviations from the
normal quartiles. These point to the need for a richer model, with more
predictors.

## Multiple Regression

One way of modelling is to take **all possible** predictor variables and
then selectively remove them, ensuring each time that the error
increases by an insignificant amount by the removal. We will follow this
method in another Tutorial.

## Conclusions

So our Linear Modelling workflow might look like this: we have not seen all stages yet, but that is for another course module or tutorial!

```{mermaid}
%%| echo: false
flowchart TD
    A[(A: Data)] -->|mosaic + ggformula|B[B:EDA] 
    B --> |corrplot + corrgram| C(C: Check Relationships)
    C --> D[D: Decide on Simple/Complex Model]
    D --> E{E: Is Simple Model Possible?}
    E --> |Yes| G[G: Build Model]
    E -->|Nope| F[F: Transform Variables]
    F --> D
    G --> H{H: Check Model Diagnostics}
    H --> |Problems| D
    H --> |All good| I(Interpret Your Model)
    I --> J(((Apply the Model for Predictions)))
```

## References

1.  The Boston Housing Dataset, corrected version. StatLib \@ CMU,
    [lib.stat.cmu.edu/datasets/boston_corrected.txt](http://lib.stat.cmu.edu/datasets/boston_corrected.txt)
2.  <https://feliperego.github.io/blog/2015/10/23/Interpreting-Model-Output-In-R>
3.  Michael Crawley, The R Book,second edition, 2013. Chapter 11.
4.  David C Howell, [Permutation Tests for Factorial ANOVA
    Designs](https://www.uvm.edu/~statdhtx/StatPages/Permutation%20Anova/PermTestsAnova.html)
5.  Marti Anderson, [Permutation tests for univariate or multivariate
    analysis of variance and
    regression](https://www.academia.edu/50056272/Permutation_tests_for_univariate_or_multivariate_analysis_of_variance_and_regression?auto=download)

[^1]: Michael Crawley, *The R Book, Third Edition 2023. Chapter 9.
    Statistical Modelling*

[^2]: Pruim R, Kaplan DT, Horton NJ (2017). "The mosaic Package: Helping
    Students to 'Think with Data' Using R." The R Journal, 9(1),
    77--102.
    https://journal.r-project.org/archive/2017/RJ-2017-024/index.html.
