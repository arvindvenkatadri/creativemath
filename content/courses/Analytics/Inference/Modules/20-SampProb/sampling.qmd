---
title: "\U0001F3B2 Samples, Populations, Statistics and Inference"
date: 25/Nov/2022
date-modified: "`r Sys.Date()`"
order: 20
abstract: "How much ~~Land~~ Data does a Man need?"
image: preview.jpg
categories:
- Sampling
- Central Limit Theorem
- Standard Error
- Confidence Intervals
bibliography: 
  - grateful-refs.bib
citation: true
filters: 
  - shinylive
---

## {{< fa folder-open >}} Slides and Tutorials

|                                                                                                 |     |     |                                                                                        |
|------------------|--------------------|------------------|------------------|
| <a href="./files/sampling-tutorial.qmd"><i class="fa-brands fa-r-project"></i> R Tutorial</a>   |     |     | <a href="./files/data/qdd-data.zip"> <i class="fa-solid fa-database"></i> Datasets</a> |

## {{< iconify noto-v1 package >}} Setting up R Packages

```{r}
#| label: setup
set.seed(123456) # TO get repeatable graphs!

library(tidyverse) # Data Processing in R
library(mosaic) # Our workhorse for stats, sampling
library(skimr) # Good to Examine data
library(ggformula) # Formula interface for graphs

# load the NHANES data library
library(NHANES)

library(cowplot) # ggplot themes and stacking of plots

```


```{r}
#| label: Extra Pedagogical Packages
#| echo: false
#| message: false

library(grateful)
library(checkdown)
library(epoxy)
library(TeachHist)
library(TeachingDemos)
library(smovie) # Stat Movies with code
library(visualize) # Plot Densities, Histograms and Probabilities as areas under the curve
library(regressinator) # Fake data populations we can sample from
library(shinylive)
```

```{r}
#| label: Plot Sizing and theming
#| echo: false
#| message: false
#| results: hide

# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto
extrafont::loadfonts(quiet = TRUE)

# Chunk options
knitr::opts_chunk$set(
 fig.width = 7,
 fig.asp = 0.618, # Golden Ratio
 #out.width = "80%",
 fig.align = "center",
 warning = FALSE
)
### Ggplot Theme
# library(ragg)
# library(showtext)
# library(thematic)
# extrafont::loadfonts(quiet = TRUE)

ggplot2::theme_set(ggplot2::theme_bw(base_size = 12)) + 
ggplot2::theme_update(
  panel.grid.minor = element_blank(),
  panel.background = element_blank(),
  plot.title = element_text(face = "bold"),
  plot.title.position = "plot",
)

# thematic_on(font = "Roboto Condensed",inherit = F,
#              bg = "#FFFFFF",fg = "#111111",
#              accent = "#DD1144")

```


```{r}
#| echo: FALSE
#| eval: false
#| fig.align: 'center'
#| fig.alt: "Photo by Anirudh on unsplash"
knitr::include_graphics("preview.jpg")

```

## {{< iconify clarity group-solid >}} What is a Population?

A *population* is a collection of individuals or observations we are
interested in. This is also commonly denoted as a study population. We
mathematically denote the population's size using upper-case `N`.

A *population parameter* is some numerical summary about the population
that is unknown but you wish you knew. For example, when this quantity
is a mean like *the mean height of all Bangaloreans*, the population
parameter of interest is the *population mean*.

A *census* is an exhaustive enumeration or counting of all N individuals
in the population. We do this in order to compute the population
parameter's value exactly. Of note is that as the number N of
individuals in our population increases, conducting a census gets more
expensive (in terms of time, energy, and money).

::: callout-important
## {{< iconify carbon parameter >}} Parameters

Populations *P*arameters are usually indicated by Greek Letters.
:::

## {{< iconify game-icons card-pickup >}} What is a Sample?

Sampling is the act of collecting a small subset from the population, 
which we generally do when we can't perform a **census**. We mathematically denote the sample size using lower case `n`, as opposed to upper case `N` which denotes the population's size. Typically the sample size `n` is much
smaller than the population size `N`. Thus sampling is a much cheaper
alternative than performing a census.

A **sample statistic**, also known as a *point estimate*, is a summary
statistic like a `mean` or `standard deviation` that is computed from a
sample.

::: callout-note
## Why do we sample?

Because we cannot conduct a census ( not always ) --- and **sometimes we won't even know how big the population is** --- we take samples. And we
*still* want to do useful work for/with the population, after
estimating its parameters, *an act of generalizing* from sample to
population. So the question is, **can we estimate useful parameters of the population, using just samples? Can point estimates serve as useful guides to population parameters?**

[This act of generalizing from sample to population is at the heart of
**statistical inference**.]{style="background-color: yellow;"}
:::

::: callout-important
## An Alliterative Mnemonic

NOTE: there is an
[*alliterative*](https://www.grammarly.com/blog/alliteration/)
[*mnemonic*](https://www.merriam-webster.com/dictionary/mnemonic) here:
**S**amples have **S**tatistics; **P**opulations have **P**arameters.
:::

## {{< iconify fluent-mdl2 test-parameter >}} Population Parameters and Sample Statistics

|                    | Population Parameter | Sample Statistic |
|--------------------|----------------------|------------------|
| Mean               | $\mu$                | $\bar{x}$        |
| Standard Deviation | $\sigma$             | s                |
| Proportion         | p                    | $\hat{p}$        |
| Correlation        | $\rho$               | r                |
| Slope (Regression) | $\beta_1$            | $b_1$            |

: Parameters and Statistics

::: callout-note
## Question

Q.1. What is the mean commute time for workers in a particular city?

A.1. The *parameter* is the mean commute time $\mu$ for a *population*
containing all workers who work in the city. We *estimate* it using
$\bar{x}$, the mean of the random sample of people who work in the city.
:::

::: callout-note
## Question

Q.2. What is the correlation between the size of dinner bills and the
size of tips at a restaurant?

A.2. The *parameter is* $\rho$ , the correlation between bill amount and
tip size for a *population* of all dinner bills at that restaurant. We
estimate it using r, the correlation from a random sample of dinner
bills.
:::

::: callout-note
## Question

Q.3. How much difference is there in the proportion of 30 to 39-year-old
residents who have only a cell phone (no land line phone) compared to 50
to 59-year-olds in the country?

A.3. The *population* is all citizens of the country, and the parameter
is $p_1 - p_2$, the difference in proportion of 30 to 39-year-old
residents who have only a cell phone ($p_1$) and the proportion with the
same property among all 50 to 59-year olds ($p_2$). We estimate it using
($\hat{p_1} - \hat{p_2}$), the difference in sample proportions computed
from random samples taken from each group.
:::

Sample statistics vary and in the following we will estimate this
uncertainty and decide how reliable they might be as estimates of
population parameters.

## {{< iconify pajamas issue-type-test-case >}} Case Study #1: Sampling the NHANES dataset

We will first execute some samples from a known dataset. We load up the
NHANES dataset and inspect it.

```{r}
#| column: body-outset-right
#| layout-ncol: 3
data("NHANES")
#mosaic::inspect(NHANES)
skimr::skim(NHANES)

```

Let us create a NHANES (sub)-dataset without duplicated IDs and only
adults:

```{r}
NHANES <-
  NHANES %>%
  distinct(ID, .keep_all = TRUE) 

#create a dataset of only adults
NHANES_adult <-  
  NHANES %>%
  filter(Age >= 18) %>% drop_na(Height)

```

### {{< iconify mdi head-thinking-outline >}} {{< iconify clarity group-solid >}} An "Assumed" Population

::: callout-important
## An "Assumed" Population

For now, we will **treat** this dataset as our **Population**. So each
variable in the dataset is a *population* for that particular
quantity/category, with appropriate *population parameters* such as
`mean`s, `sd`-s, and `proportions`.
:::

Let us calculate the **population parameters** for the `Height` data
from our "assumed" population:

```{r}
# NHANES_adult is assumed population
pop_mean_height <- mean(~ Height, data = NHANES_adult)
pop_sd_height <- sd(~ Height, data = NHANES_adult)

pop_mean_height
pop_sd_height

```

### {{< iconify game-icons card-pickup >}} Sampling

Now, we will sample **ONCE** from the NHANES `Height` variable. Let us
take a sample of `sample size` 50. We will compare **sample statistics**
with **population parameters** on the basis of this ONE sample of 50:

```{r}
#| layout: [[20,20,60]]
#| warning: false
sample_height <- sample(NHANES_adult, size = 50) %>% 
  select(Height)
sample_height

sample_mean_height <- mean(~ Height, data = sample_height)
sample_mean_height

# Plotting the histogram of this sample
sample_height %>% 
  gf_histogram(~ Height, bins = 10) %>% 
  
  gf_vline(xintercept = sample_mean_height, 
           color = "red") %>% 
  
  gf_vline(xintercept = pop_mean_height, 
           colour = "blue") %>% 
  
  gf_label(7 ~ (pop_mean_height + 8), 
          label = "Population Mean Height", 
          color = "blue") %>% 
  
  gf_label(7 ~ (sample_mean_height - 8), 
          label = "Sample Mean Height", color = "red") 


```

### {{< iconify ic baseline-loop >}} {{< iconify game-icons card-pickup >}} {{< iconify icon-park-outline average >}} Repeated Samples and Sample Means

OK, so the `sample_mean_height` is not too far from the
`pop_mean_height`. Is this always true? Let us check: we will create 500
samples each of size 50. And calculate their mean as the *sample
statistic*, giving us a data frame containing 500 `sample means`. We
will then see if these 500 means lie close to the `pop_mean_height`:

```{r}
#| layout: [[20,20,60]]
#| warning: false

sample_height_500 <- do(500) * {
  sample(NHANES_adult, size = 50) %>%
    select(Height) %>%
    summarise(
      sample_mean_500 = mean(Height),
      sample_min_500 = min(Height),
      sample_max_500 = max(Height))
}

head(sample_height_500)
dim(sample_height_500)

sample_height_500 %>%
  gf_point(.index ~ sample_mean_500, color = "red",
           title = "Sample Means are close to the Population Mean",
           subtitle = "Sample Means are Random!") %>%
  
  gf_segment(
    .index + .index ~ sample_min_500 + sample_max_500,
    color = "red",
    linewidth = 0.3,
    alpha = 0.3,
    ylab = "Sample Index (1-500)",
    xlab = "Sample Means"
  ) %>%
  
  gf_vline(xintercept = ~ pop_mean_height, 
           color = "blue") %>%
  
  gf_label(-15 ~ pop_mean_height, label = "Population Mean", 
           color = "blue") 

```

::: callout-note
### Sample Means are a Random Variable

The **sample-means** are a random variable! And hence they will have a
`mean` and `sd`. Do not get confused ;-D
:::

The `sample_mean`s (red dots), are themselves random because the samples
are random, of course. It appears that they are generally in the
vicinity of the `pop_mean` (blue line).

### {{< iconify game-icons card-pickup >}} {{< iconify tabler chart-histogram >}} Distribution of Sample-Means

Since the **sample-means** are themselves random variables, let's plot
the **distribution** of these 500 sample-means themselves, called **a
distribution of sample-means**. We will also plot the position of the
population mean `pop_mean_height` parameter, the mean of the `Height`
variable.

```{r}
#| label: Sampling-Mean-Distribution
#| layout-ncol: 2
#| warning: false
#| fig-cap: Distributions
#| fig-subcap: 
#|  - Sample
#|  - Sample and Population

sample_height_500 %>% 
  gf_dhistogram(~ sample_mean_500,bins = 30, xlab = "Height") %>% 
  
  gf_vline(xintercept = pop_mean_height, 
           color = "blue") %>% 
  
  gf_label(0.01 ~ pop_mean_height, 
            label = "Population Mean", 
            color = "blue") 


# How does this **distribution of sample-means** compare with the
# overall distribution of the population?
# 
sample_height_500 %>% 
  gf_dhistogram(~ sample_mean_500, bins = 30,xlab = "Height") %>% 
  
  gf_vline(xintercept = pop_mean_height, 
           color = "blue") %>% 
  
   gf_label(0.01 ~ pop_mean_height, 
            label = "Population Mean", 
            color = "blue") %>% 

  ## Add the population histogram
  gf_histogram(~ Height, data = NHANES_adult, 
               alpha = 0.2, fill = "blue", 
               bins = 30) %>% 
  
  gf_label(0.025 ~ (pop_mean_height + 20), 
           label = "Population Distribution", color = "blue") 


```

### {{< iconify carbon concept >}} Central Limit Theorem

We see in the Figure above that

-   the *distribution of sample-means* is centered around the
    `pop_mean`.
-   That the standard deviation of the *distribution of sample means* is
    less than that of the original population. But exactly what is it?
-   And what is the kind of distribution?

One more experiment.

Now let's repeatedly sample `Height` and compute the sample mean, and
look at the resulting histograms and [Q-Q
plots.](https://www.youtube.com/watch?v=okjYjClSjOg) (Q-Q plots check
whether a certain distribution is close to being normal or not.)

We will use sample sizes of `c(8, 16, ,32, 64)` and generate 1000
samples each time, take the means and plot these 1000 means:

```{r}

set.seed(12345)


samples_height_08 <- do(1000) * mean(resample(NHANES_adult$Height, size = 08))

samples_height_16 <- do(1000) * mean(resample(NHANES_adult$Height, size = 16))

samples_height_32 <- do(1000) * mean(resample(NHANES_adult$Height, size = 32))

samples_height_64 <- do(1000) * mean(resample(NHANES_adult$Height, size = 64))

# samples_height_128 <- do(1000) * mean(resample(NHANES_adult$Height, size = 128))

# Quick Check
head(samples_height_08)

```

Now let's create separate `Q-Q plots` for the different sample sizes.

```{r}
#| warning: false
# Now let's create separate Q-Q plots for the different sample sizes.
# ggplot2::theme_set(theme_classic())

p1 <- gf_qq( ~ mean,data = samples_height_08,
             title = "N = 8", 
             color = "dodgerblue") %>%
  gf_qqline()

p2 <- gf_qq( ~ mean,data = samples_height_16,
            title = "N = 16", 
            color = "sienna") %>%
  gf_qqline()

p3 <- gf_qq( ~ mean,data = samples_height_32,
            title = "N = 32", 
            color = "palegreen") %>%
  gf_qqline()

p4 <- gf_qq( ~ mean,data = samples_height_64,
            title = "N = 64", 
            color = "violetred") %>%
  gf_qqline()

cowplot::plot_grid(p1, p2, p3, p4)

```

Let us plot their individual histograms to compare them:

```{r}
#| warning: false
#| layout-ncol: 2
#| layout-nrow: 2

# Let us overlay their individual histograms to compare them:
p5 <- gf_dhistogram(~ mean,
              data = samples_height_08,
              color = "grey",
              fill = "dodgerblue",title = "N = 8") %>%
  gf_fitdistr() %>%
  gf_vline(xintercept = pop_mean_height, inherit = FALSE,
           color = "blue") %>%
  gf_label(-0.025 ~ pop_mean_height, 
           label = "Population Mean", 
           color = "blue") %>% 
  gf_theme(scale_y_continuous(expand = expansion(mult = c(0.08,0.02))))

p6 <- gf_dhistogram(~ mean,
              data = samples_height_16,
              color = "grey",
              fill = "sienna",title = "N = 16") %>%
  gf_fitdistr() %>%
  gf_vline(xintercept = pop_mean_height,
           color = "blue") %>%
  gf_label(-.025 ~ pop_mean_height, 
           label = "Population Mean", 
           color = "blue") %>% 
  gf_theme(scale_y_continuous(expand = expansion(mult = c(0.08,0.02))))

p7 <- gf_dhistogram(~ mean,
                    data = samples_height_32 ,
                    na.rm = TRUE,
                    color = "grey",
                    fill = "palegreen",title = "N =32") %>%
  gf_fitdistr() %>%
  gf_vline(xintercept = pop_mean_height,
           color = "blue") %>%
  gf_label(-.025 ~ pop_mean_height, 
           label = "Population Mean", color = "blue") %>% 
  gf_theme(scale_y_continuous(expand = expansion(mult = c(0.08,0.02))))

p8 <- gf_dhistogram(~ mean, 
                    data = samples_height_64,
                    na.rm = TRUE,
                    color = "grey",
                    fill = "violetred",title = "N = 64") %>% 
  gf_fitdistr() %>% 
  gf_vline(xintercept = pop_mean_height,
         color = "blue") %>%
  gf_label(-.025 ~ pop_mean_height, 
           label = "Population Mean", color = "blue") %>% 
  gf_theme(scale_y_continuous(expand = expansion(mult = c(0.08,0.02))))

#patchwork::wrap_plots(p5,p6,p7,p8)
p5
p6
p7
p8

```

And if we overlay the histograms:

```{r, echo=FALSE}
gf_dhistogram(~ mean,
              data = samples_height_08,
              color = "grey",
              fill = "dodgerblue",alpha = .5) %>% 
  # gf_label(0.15 ~ 172, inherit = FALSE,
  #          label = "N = 8", color = "dodgerblue") %>% 

gf_dhistogram(~ mean,
              data = samples_height_16,
              color = "grey",
              fill = "sienna",alpha = 0.5) %>% 
    # gf_label(0.2 ~ 172, 
    #        label = "N = 16", color = "sienna") %>% 

gf_dhistogram(~ mean,
              data = samples_height_32,
              color = "grey",
              fill = "palegreen",alpha = 0.5) %>% 
    # gf_label(0.3 ~ 172, 
    #        label = "N = 32", color = "palegreen") %>% 

gf_dhistogram(~ mean,
              data = samples_height_64,
              color = "grey",
              fill = "violetred",alpha = 0.5) %>% 
    # gf_label(0.4 ~ 172, 
    #        label = "N = 64", color = "violetred") %>% 

gf_fitdistr(
  ~ mean,
  data = samples_height_08,
  color = "dodgerblue",size = 2
) %>%
  
  gf_fitdistr(
    ~ mean,
    data = samples_height_16 ,
    color = "sienna",size = 2
  ) %>%
  
  gf_fitdistr(
    ~ mean,
    data = samples_height_32 ,
    na.rm = TRUE,
    color = "palegreen",size = 2
  ) %>%
  
  gf_fitdistr(
    ~ mean,
    data = samples_height_64 ,
    na.rm = TRUE,
    color = "violetred", size = 2
  ) %>%
  
  gf_vline(xintercept = pop_mean_height,
           color = "blue") %>%
  gf_label(-.02 ~ pop_mean_height, label = "Population Mean", 
           color = "blue") 

```

The QQ plots show that the results become more normally distributed
(i.e. following the straight line) as the samples get larger. From the
histograms we learn that the sample-means are normally distributed
around the *population mean*. [This feels intuitively right because when
we sample from the population, many values will be close to the
*population mean*, and values far away from the mean will be
increasingly scarce.]{style="background-color: yellow;"}

Let us calculate the mean of the sample-means:

```{r}
#| results: hold
mean(~ mean, data  = samples_height_08)
mean(~ mean, data  = samples_height_16)
mean(~ mean, data  = samples_height_32)
mean(~ mean, data  = samples_height_64)
pop_mean_height

```

And the sample `sd`s:

```{r}
#| results: hold
sd(~ mean, data  = samples_height_08)
sd(~ mean, data  = samples_height_16)
sd(~ mean, data  = samples_height_32)
sd(~ mean, data  = samples_height_64)

```

::: callout-note
## Central Limit Theorem

This is the **Central Limit Theorem (CLT)**

-   the [sample-means are normally distributed around the *population
    mean*]{style="background-color: yellow;"}.
-   the sample-means become "more normally distributed" with sample
    length, as shown by the (small but definite) improvements in the Q-Q
    plots with sample-size.
-   the sample-mean distributions narrow with sample length, i.e [the
    `sd` decreases with increasing sample
    size.]{style="background-color: yellow;"}
-   This is [regardless of the distribution of the *population*
    parameter]{style="background-color: yellow;"} itself.[^1]
:::

## {{< iconify dashicons code-standards >}} Standard Error

As we saw above, the standard deviations of the sample-mean
distributions reduce with sample size. In fact their SDs are defined by:

`sd = pop_sd/sqrt(sample_size)`[^2] where sample-size here is one of
`c(8, 16,32,64)`

The standard deviation of the **sample-mean distribution** is called the
**Standard Error**. This statistic derived from the sample, will help us
infer our population parameters with a precise estimate of the
*uncertainty* involved.

$$
Standard\ Error\ \pmb {se} = \frac{population\ sd}{\sqrt[]{sample\ size}} \\\
\pmb {se} = \frac{\sigma}{\sqrt[]{n}}
$$

In our sampling experiments, the Standard Errors evaluate to:

```{r}
#| results: hold
#| layout-ncol: 2
pop_sd_height <- sd(~ Height, data = NHANES_adult)

pop_sd_height/sqrt(8)
pop_sd_height/sqrt(16)
pop_sd_height/sqrt(32)
pop_sd_height/sqrt(64)


```

As seen, these are identical to the Standard Deviations of the
individual sample-mean distributions.

## {{< iconify fluent auto-fit-width-24-filled >}} Confidence intervals

When we work with samples, we want to be able to speak with a certain
degree of confidence about the **population mean**, based on the
evaluation of **one** sample mean, not a large number of them. Given
that sample-means are normally distributed around the **population means**, we can say that $68\%$ of *all possible sample-mean* lie within
$\pm SE$ of the *population mean*; and further that $95 \%$ of *all
possible sample-mean* lie within $\pm 2*SE$ of the *population mean*.

These two intervals $sample.mean \pm SE$ and $sample.mean \pm 1.5*SE$
are called the **confidence intervals** for the population mean, at
levels $68\%$ and $95 \%$ probability respectively.

## {{< iconify flat-color-icons workflow >}} Workflow

Thus if we want to estimate a *population parameter*:

-   we take one **random** sample from the population

-   we calculate the estimate from the sample

-   we calculate the sample-sd

-   we calculate the *Standard Error* as $\frac{sample-sd}{\sqrt[]{n}}$

-   we calculate 95% confidence intervals for the *population parameter*
    based on the formula

    $CI_{95\%}= sample.mean \pm 2*SE$.

-   Since Standard Error decreases with sample size, we need to make our
    sample of adequate size.( $n=30$ seems appropriate in most cases.
    Why?)
  
- And we do not have to worry about the distribution of the population. It need not be normal !!

## {{< iconify material-symbols interactive-space-outline >}} An interactive Sampling app

Here below is an interactive sampling app. Choose the number of samples you want from a **normally-distributed population** $N(\mu = 0, \sigma = 1)$. Vary the sample size to see how the sample mean varies around the tru population mean. 

```{shinylive-r}
#| standalone: true
#| viewerHeight: 600
#| viewerWidth: 1000
library(shiny)
library(bslib)

# Define UI for app that draws a histogram ----
ui <- page_sidebar(
  sidebar = sidebar(open = "open",
    numericInput("n", "Sample count", 100),
    checkboxInput("pause", "Pause", FALSE),
  ),
  plotOutput("plot", width=800)
)

server <- function(input, output, session) {
  data <- reactive({
    input$resample
    if (!isTRUE(input$pause)) {
      invalidateLater(1000)
    }
    rnorm(input$n)
  })
  
  output$plot <- renderPlot({
    hist(data(),
      breaks = 40,
      xlim = c(-3, 3),
      ylim = c(0, 1.5),
      lty = "blank",
      xlab = "value",
      freq = FALSE,
      main = ""
    )
    
    x <- seq(from = -3, to = 3, length.out = 500)
    y <- dnorm(x)
    lines(x, y, lwd=1.5)
    
    lwd <- 5
    abline(v=0, col="red", lwd=lwd, lty=2)
    abline(v=mean(data()), col="blue", lwd=lwd, lty=1)

    legend(legend = c("Normal", "Mean", "Sample mean"),
      col = c("black", "red", "blue"),
      lty = c(1, 2, 1),
      lwd = c(1, lwd, lwd),
      x = 0.25,
      y = 1.25
    )
  }, res=140)
}

# Create Shiny app ----
shinyApp(ui = ui, server = server)

```
What if we sample from *not* a normal distribution but say, a Poisson Distribution? Will we still see the sample mean hover around the population mean?

```{r}
#| echo: false
smovie::clt(distn = "gamma")

```

## An Interactive app for the CLT

<https://gallery.shinyapps.io/CLT_mean/>


## {{< iconify ooui references-rtl >}} References

1.  Diez, David M & Barr, Christopher D & Çetinkaya-Rundel, Mine,
    *OpenIntro Statistics*. <https://www.openintro.org/book/os/>

2.  Stats Test Wizard.
    <https://www.socscistatistics.com/tests/what_stats_test_wizard.aspx>

3.  Diez, David M & Barr, Christopher D & Çetinkaya-Rundel, Mine:
    *OpenIntro Statistics*. Available online
    <https://www.openintro.org/book/os/>

4.  Måns Thulin, *Modern Statistics with R: From wrangling and exploring
    data to inference and predictive modelling*
    <http://www.modernstatisticswithr.com/>

5.  Jonas Kristoffer Lindeløv, Common statistical tests are linear
    models (or: how to teach stats)
    <https://lindeloev.github.io/tests-as-linear/>

6.  CheatSheet
    <https://lindeloev.github.io/tests-as-linear/linear_tests_cheat_sheet.pdf>

7.  Common statistical tests are linear models: a work through by Steve
    Doogue <https://steverxd.github.io/Stat_tests/>

8.  Jeffrey Walker "Elements of Statistical Modeling for Experimental
    Biology".
    <https://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/>

9.  Adam Loy, Lendie Follett & Heike Hofmann (2016) Variations of
    *Q*--*Q* Plots: The Power of Our Eyes!, The American Statistician,
    70:2, 202-214, DOI:
    [10.1080/00031305.2015.1077728](https://doi.org/10.1080/00031305.2015.1077728)


[^1]: The \`Height\` variable seems to be normally distributed at
    population level. We will try other non-normal population variables
    as an exercise in the tutorials.

[^2]: Once `sample size = population`, we have complete access to the
    population and there is no question of estimation error! So
    `sample_sd = pop_sd`!

::: {#refs style="font-size: 60%;"}

###### {{< iconify lucide package-check >}} R Package Citations
```{r}
#| echo: false
#scan_packages()
cite_packages(
  output = "table",
  out.dir = ".",
  out.format = "html",
  pkgs = c("NHANES", "regressinator", "smovie", "TeachHist",
           "TeachingDemos", "visualize")
) %>%
  knitr::kable(format = "simple")

```
:::
