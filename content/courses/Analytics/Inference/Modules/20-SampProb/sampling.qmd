---
title: "\U0001F3B2 Samples, Populations, Statistics and Inference"
author: "Arvind Venkatadri"
date: 25/Nov/2022
date-modified: "`r Sys.Date()`"
order: 20
abstract: "How much ~~Land~~ Data does a Man need?"
image: preview.jpg
categories:
- Sampling
- Central Limit Theorem
- Standard Error
- Confidence Intervals
bibliography: 
  - grateful-refs.bib
citation: true
---

## {{< fa folder-open >}} Slides and Tutorials

|                                                                                                 |     |     |                                                                                        |
|------------------|--------------------|------------------|------------------|
| <a href="./files/sampling-tutorial.qmd"><i class="fa-brands fa-r-project"></i> R Tutorial</a>   |     |     | <a href="./files/data/qdd-data.zip"> <i class="fa-solid fa-database"></i> Datasets</a> |

## {{< iconify noto-v1 package >}} Setting up R Packages

```{r}
#| label: setup
set.seed(123456) # TO get repeatable graphs!

library(tidyverse) # Data Processing in R
library(mosaic) # Our workhorse for stats, sampling
library(skimr) # Good to Examine data
library(ggformula) # Formula interface for graphs

# load the NHANES data library
library(NHANES)

library(cowplot) # ggplot themes and stacking of plots

```


```{r}
#| label: Extra Pedagogical Packages
#| echo: false
#| message: false

library(checkdown)
library(epoxy)
library(TeachHist)
library(TeachingDemos)
library(grateful)
library(visualize) # Plot Densities, Histograms and Probabilities as areas under the curve

```

```{r}
#| echo: FALSE
#| eval: FALSE
#| fig.align: 'center'
#| fig.alt: "Photo by Anirudh on unsplash"
knitr::include_graphics("featured.jpg")

```

## {{< iconify clarity group-solid >}} What is a Population?

A *population* is a collection of individuals or observations we are
interested in. This is also commonly denoted as a study population. We
mathematically denote the population's size using upper-case `N`.

A *population parameter* is some numerical summary about the population
that is unknown but you wish you knew. For example, when this quantity
is a mean like the average height of all Bangaloreans, the population
parameter of interest is the population mean.

A *census* is an exhaustive enumeration or counting of all N individuals
in the population. We do this in order to compute the population
parameter's value exactly. Of note is that as the number N of
individuals in our population increases, conducting a census gets more
expensive (in terms of time, energy, and money).

::: callout-important
## {{< iconify carbon parameter >}} Parameters

Populations *P*arameters are usually indicated by Greek Letters.
:::

## {{< iconify game-icons card-pickup >}} What is a Sample?

Sampling is the act of collecting a sample from the population, which we
generally do when we can't perform a census. We mathematically denote
the sample size using lower case `n`, as opposed to upper case `N` which
denotes the population's size. Typically the sample size `n` is much
smaller than the population size `N`. Thus sampling is a much cheaper
alternative than performing a census.

A **sample statistic**, also known as a *point estimate*, is a summary
statistic like a `mean` or `standard deviation` that is computed from a
sample.

::: callout-note
## Why do we sample?

Because we cannot conduct a census ( not always ) --- and sometimes we
won't even know how big the population is --- we take samples. And we
*still* want to do useful work for/with the population, after
*estimating its parameters, an act of generalizing* from sample to
population. So the question is, **can we estimate useful parameters of
the population, using just samples? Can point estimates serve as useful
guides to population parameters?**

[This act of generalizing from sample to population is at the heart of
**statistical inference**.]{style="background-color: yellow;"}
:::

::: callout-important
## An Alliterative Mnemonic

NOTE: there is an
[*alliterative*](https://www.grammarly.com/blog/alliteration/)
[*mnemonic*](https://www.merriam-webster.com/dictionary/mnemonic) here:
**S**amples have **S**tatistics; **P**opulations have **P**arameters.
:::

## {{< iconify fluent-mdl2 test-parameter >}} Population Parameters and Sample Statistics

|                    | Population Parameter | Sample Statistic |
|--------------------|----------------------|------------------|
| Mean               | $\mu$                | $\bar{x}$        |
| Standard Deviation | $\sigma$             | s                |
| Proportion         | p                    | $\hat{p}$        |
| Correlation        | $\rho$               | r                |
| Slope (Regression) | $\beta_1$            | $b_1$            |

: Parameters and Statistics

::: callout-note
## Question

Q.1. What is the mean commute time for workers in a particular city?

A.1. The *parameter* is the mean commute time $\mu$ for a *population*
containing all workers who work in the city. We *estimate* it using
$\bar{x}$, the mean of the random sample of people who work in the city.
:::

::: callout-note
## Question

Q.2. What is the correlation between the size of dinner bills and the
size of tips at a restaurant?

A.2. The *parameter is* $\rho$ , the correlation between bill amount and
tip size for a *population* of all dinner bills at that restaurant. We
estimate it using r, the correlation from a random sample of dinner
bills.
:::

::: callout-note
## Question

Q.3. How much difference is there in the proportion of 30 to 39-year-old
residents who have only a cell phone (no land line phone) compared to 50
to 59-year-olds in the country?

A.3. The *population* is all citizens of the country, and the parameter
is $p_1 - p_2$, the difference in proportion of 30 to 39-year-old
residents who have only a cell phone ($p_1$) and the proportion with the
same property among all 50 to 59-year olds ($p_2$). We estimate it using
($\hat{p_1} - \hat{p_2}$), the difference in sample proportions computed
from random samples taken from each group.
:::

Sample statistics vary and in the following we will estimate this
uncertainty and decide how reliable they might be as estimates of
population parameters.

## {{< iconify pajamas issue-type-test-case >}} Case Study #1: Sampling the NHANES dataset

We will first execute some samples from a known dataset. We load up the
NHANES dataset and inspect it.

```{r}
#| column: body-outset-right
#| layout-ncol: 3
data("NHANES")
#mosaic::inspect(NHANES)
skimr::skim(NHANES)

```

Let us create a NHANES (sub)-dataset without duplicated IDs and only
adults:

```{r}
NHANES <-
  NHANES %>%
  distinct(ID, .keep_all = TRUE) 

#create a dataset of only adults
NHANES_adult <-  
  NHANES %>%
  filter(Age >= 18) %>% drop_na(Height)

```

### {{< iconify mdi head-thinking-outline >}} {{< iconify clarity group-solid >}} An "Assumed" Population

::: callout-important
## An "Assumed" Population

For now, we will **treat** this dataset as our **Population**. So each
variable in the dataset is a *population* for that particular
quantity/category, with appropriate *population parameters* such as
`mean`s, `sd`-s, and `proportions`.
:::

Let us calculate the **population parameters** for the `Height` data
from our "assumed" population:

```{r}
# NHANES_adult is assumed population
pop_mean_height <- mean(~ Height, data = NHANES_adult)
pop_sd_height <- sd(~ Height, data = NHANES_adult)

pop_mean_height
pop_sd_height

```

### {{< iconify game-icons card-pickup >}} Sampling

Now, we will sample **ONCE** from the NHANES `Height` variable. Let us
take a sample of `sample size` 50. We will compare **sample statistics**
with **population parameters** on the basis of this ONE sample of 50:

```{r}
#| layout: [[20,20,60]]
sample_height <- sample(NHANES_adult, size = 50) %>% 
  select(Height)
sample_height

sample_mean_height <- mean(~ Height, data = sample_height)
sample_mean_height

# Plotting the histogram of this sample
sample_height %>% 
  gf_histogram(~ Height, bins = 10) %>% 
  
  gf_vline(xintercept = sample_mean_height, 
           color = "red") %>% 
  
  gf_vline(xintercept = pop_mean_height, 
           colour = "blue") %>% 
  
  gf_label(7 ~ (pop_mean_height + 8), 
          label = "Population Mean Height", 
          color = "blue") %>% 
  
  gf_label(7 ~ (sample_mean_height - 8), 
          label = "Sample Mean Height", color = "red") %>% 
  gf_theme(theme_classic())


```

### {{< iconify ic baseline-loop >}} {{< iconify game-icons card-pickup >}} {{< iconify icon-park-outline average >}} Repeated Samples and Sample Means

OK, so the `sample_mean_height` is not too far from the
`pop_mean_height`. Is this always true? Let us check: we will create 500
samples each of size 50. And calculate their mean as the *sample
statistic*, giving us a data frame containing 500 `sample means`. We
will then see if these 500 means lie close to the `pop_mean_height`:

```{r}
#| layout: [[20,20,60]]

sample_height_500 <- do(500) * {
  sample(NHANES_adult, size = 50) %>%
    select(Height) %>%
    summarise(
      sample_mean_500 = mean(Height),
      sample_min_500 = min(Height),
      sample_max_500 = max(Height))
}

head(sample_height_500)
dim(sample_height_500)

sample_height_500 %>%
  gf_point(.index ~ sample_mean_500, color = "red",
           title = "Sample Means are close to the Population Mean",
           subtitle = "Sample Means are Random!") %>%
  
  gf_segment(
    .index + .index ~ sample_min_500 + sample_max_500,
    color = "red",
    size = 0.3,
    alpha = 0.3,
    ylab = "Sample Index (1-500)",
    xlab = "Sample Means"
  ) %>%
  
  gf_vline(xintercept = ~ pop_mean_height, 
           color = "blue") %>%
  
  gf_label(-15 ~ pop_mean_height, label = "Population Mean", 
           color = "blue") %>% 
  
  gf_theme(theme = theme_classic)

```

::: callout-note
### Sample Means are a Random Variable

The **sample-means** are a random variable! And hence they will have a
`mean` and `sd`. Do not get confused ;-D
:::

The `sample_mean`s (red dots), are themselves random because the samples
are random, of course. It appears that they are generally in the
vicinity of the `pop_mean` (blue line).

### {{< iconify game-icons card-pickup >}} {{< iconify tabler chart-histogram >}} Distribution of Sample-Means

Since the **sample-means** are themselves random variables, let's plot
the **distribution** of these 500 sample-means themselves, called **a
distribution of sample-means**. We will also plot the position of the
population mean `pop_mean_height` parameter, the mean of the `Height`
variable.

```{r}
#| label: Sampling-Mean-Distribution
#| layout-ncol: 2
#| fig-cap: Distributions
#| fig-subcap: 
#|  - Sample
#|  - Sample and Population

sample_height_500 %>% 
  gf_dhistogram(~ sample_mean_500,bins = 30, xlab = "Height") %>% 
  
  gf_vline(xintercept = pop_mean_height, 
           color = "blue") %>% 
  
  gf_label(0.01 ~ pop_mean_height, 
            label = "Population Mean", 
            color = "blue") %>% 
  gf_theme(theme_classic())

# How does this **distribution of sample-means** compare with the
# overall distribution of the population?
# 
sample_height_500 %>% 
  gf_dhistogram(~ sample_mean_500, bins = 30,xlab = "Height") %>% 
  
  gf_vline(xintercept = pop_mean_height, 
           color = "blue") %>% 
  
   gf_label(0.01 ~ pop_mean_height, 
            label = "Population Mean", 
            color = "blue") %>% 

  ## Add the population histogram
  gf_histogram(~ Height, data = NHANES_adult, 
               alpha = 0.2, fill = "blue", 
               bins = 30) %>% 
  
  gf_label(0.025 ~ (pop_mean_height + 20), 
           label = "Population Distribution", color = "blue") %>% 
  gf_theme(theme_classic())

```

### {{< iconify carbon concept >}} Central Limit Theorem

We see in the Figure above that

-   the *distribution of sample-means* is centered around the
    `pop_mean`.
-   That the standard deviation of the *distribution of sample means* is
    less than that of the original population. But exactly what is it?
-   And what is the kind of distribution?

One more experiment.

Now let's repeatedly sample `Height` and compute the sample mean, and
look at the resulting histograms and [Q-Q
plots.](https://www.youtube.com/watch?v=okjYjClSjOg) (Q-Q plots check
whether a certain distribution is close to being normal or not.)

We will use sample sizes of `c(16, 32, 64, 128)` and generate 1000
samples each time, take the means and plot these 1000 means:

```{r}

set.seed(12345)


samples_height_16 <- do(1000) * mean(resample(NHANES_adult$Height, size = 16))

samples_height_32 <- do(1000) * mean(resample(NHANES_adult$Height, size = 32))

samples_height_64 <- do(1000) * mean(resample(NHANES_adult$Height, size = 64))

samples_height_128 <- do(1000) * mean(resample(NHANES_adult$Height, size = 128))

# Quick Check
head(samples_height_16)

```

Now let's create separate `Q-Q plots` for the different sample sizes.

```{r}
#| warning: false
# Now let's create separate Q-Q plots for the different sample sizes.
#
p1 <- gf_qq( ~ mean,data = samples_height_16,
             title = "N = 16", 
             color = "cornsilk") %>%
  gf_qqline()

p2 <- gf_qq( ~ mean,data = samples_height_32,
            title = "N = 32", 
            color = "sienna") %>%
  gf_qqline()

p3 <- gf_qq( ~ mean,data = samples_height_32,
            title = "N = 64", 
            color = "tomato2") %>%
  gf_qqline()

p4 <- gf_qq( ~ mean,data = samples_height_128,
            title = "N = 128", 
            color = "violetred") %>%
  gf_qqline()

cowplot::plot_grid(p1, p2, p3, p4)

```

Let us plot their individual histograms to compare them:

```{r}

# Let us overlay their individual histograms to compare them:
p5 <- gf_dhistogram(~ mean,
              data = samples_height_16,
              color = "black",
              fill = "cornsilk",title = "N = 16") %>%
  gf_fitdistr() %>%
  gf_vline(xintercept = pop_mean_height,
           color = "blue") %>%
  gf_label(-0.01 ~ pop_mean_height, 
           label = "Population Mean", 
           color = "blue")

p6 <- gf_dhistogram(~ mean,
              data = samples_height_32,
              color = "black",
              fill = "sienna",title = "N = 32") %>%
  gf_fitdistr() %>%
  gf_vline(xintercept = pop_mean_height,
           color = "blue") %>%
  gf_label(-.01 ~ pop_mean_height, 
           label = "Population Mean", 
           color = "blue")

p7 <- gf_dhistogram(~ mean,
                    data = samples_height_64 ,
                    na.rm = TRUE,
                    color = "black",
                    fill = "tomato2",title = "N = 64") %>%
  gf_fitdistr() %>%
  gf_vline(xintercept = pop_mean_height,
           color = "blue") %>%
  gf_label(-.01 ~ pop_mean_height, 
           label = "Population Mean", color = "blue")

p8 <- gf_dhistogram(~ mean, 
                    data = samples_height_128,
                    na.rm = TRUE,
                    color = "black",
                    fill = "violetred",title = "N = 128") %>% 
  gf_fitdistr() %>% 
  gf_vline(xintercept = pop_mean_height,
         color = "blue") %>%
  gf_label(-.01 ~ pop_mean_height, 
           label = "Population Mean", color = "blue")

cowplot::plot_grid(p5,p6,p7,p8)


```

And if we overlay the histograms:

```{r, echo=FALSE}
gf_dhistogram(~ mean,
              data = samples_height_16,
              color = "black",
              fill = "cornsilk",alpha = .5) %>% 
  gf_label(0.15 ~ 172, 
           label = "N =16", fill = "cornsilk", color = "black") %>% 

gf_dhistogram(~ mean,
              data = samples_height_32,
              color = "black",
              fill = "sienna",alpha = 0.5) %>% 
    gf_label(0.2 ~ 172, 
           label = "N = 32", color = "sienna") %>% 

gf_dhistogram(~ mean,
              data = samples_height_64,
              color = "black",
              fill = "tomato2",alpha = 0.5) %>% 
    gf_label(0.3 ~ 172, 
           label = "N = 64", color = "tomato2") %>% 

gf_dhistogram(~ mean,
              data = samples_height_128,
              color = "black",
              fill = "violetred",alpha = 0.5) %>% 
    gf_label(0.4 ~ 172, 
           label = "N = 128", color = "violetred") %>% 

gf_fitdistr(
  ~ mean,
  data = samples_height_16,
  color = "cornsilk",size = 2
) %>%
  
  gf_fitdistr(
    ~ mean,
    data = samples_height_32 ,
    color = "sienna",size = 2
  ) %>%
  
  gf_fitdistr(
    ~ mean,
    data = samples_height_64 ,
    na.rm = TRUE,
    color = "tomato2",size = 2
  ) %>%
  
  gf_fitdistr(
    ~ mean,
    data = samples_height_128 ,
    na.rm = TRUE,
    color = "violetred", size = 2
  ) %>%
  
  gf_vline(xintercept = pop_mean_height,
           color = "blue") %>%
  gf_label(-.02 ~ pop_mean_height, label = "Population Mean", 
           color = "blue") %>% 
  gf_theme(ggplot2::theme_classic())

```

The QQ plots show that the results become more normally distributed
(i.e. following the straight line) as the samples get larger. From the
histograms we learn that the sample-means are normally distributed
around the *population mean*. [This feels intuitively right because when
we sample from the population, many values will be close to the
*population mean*, and values far away from the mean will be
increasingly scarce.]{style="background-color: yellow;"}

Let us calculate the mean of the sample-means:

```{r}
#| results: hold
mean(~ mean, data  = samples_height_16)
mean(~ mean, data  = samples_height_32)
mean(~ mean, data  = samples_height_64)
mean(~ mean, data  = samples_height_128)
pop_mean_height

```

And the sample `sd`s:

```{r}
#| results: hold
sd(~ mean, data  = samples_height_16)
sd(~ mean, data  = samples_height_32)
sd(~ mean, data  = samples_height_64)
sd(~ mean, data  = samples_height_128)

```

::: callout-note
## Central Limit Theorem

This is the **Central Limit Theorem (CLT)**

-   the [sample-means are normally distributed around the *population
    mean*]{style="background-color: yellow;"}.
-   the sample-means become "more normally distributed" with sample
    length, as shown by the (small but definite) improvements in the Q-Q
    plots with sample-size.
-   the sample-mean distributions narrow with sample length, i.e [the
    `sd` decreases with increasing sample
    size.]{style="background-color: yellow;"}
-   This is [regardless of the distribution of the *population*
    parameter]{style="background-color: yellow;"} itself.[^1]
:::

## {{< iconify dashicons code-standards >}} Standard Error

As we saw above, the standard deviations of the sample-mean
distributions reduce with sample size. In fact their SDs are defined by:

`sd = pop_sd/sqrt(sample_size)`[^2] where sample-size here is one of
`c(16,32,64,128)`

The standard deviation of the **sample-mean distribution** is called the
**Standard Error**. This statistic derived from the sample, will help us
infer our population parameters with a precise estimate of the
*uncertainty* involved.

$$
Standard\ Error\ \pmb {se} = \frac{population\ sd}{\sqrt[]{sample\ size}} \\\
\pmb {se} = \frac{\sigma}{\sqrt[]{n}}
$$

In our sampling experiments, the Standard Errors evaluate to:

```{r}
#| results: hold
#| layout-ncol: 2
pop_sd_height <- sd(~ Height, data = NHANES_adult)

pop_sd_height/sqrt(16)
pop_sd_height/sqrt(32)
pop_sd_height/sqrt(64)
pop_sd_height/sqrt(128)

```

As seen, these are identical to the Standard Deviations of the
individual sample-mean distributions.

## {{< iconify fluent auto-fit-width-24-filled >}} Confidence intervals

When we work with samples, we want to be able to speak with a certain
degree of confidence about the **population mean**, based on the
evaluation of **one** sample mean,not a whole large number of them. Give
that sample-means are normally distributed around the **population
means**, we can say that $68\%$ of *all possible sample-mean* lie within
$\pm SE$ of the *population mean*; and further that $95 \%$ of *all
possible sample-mean* lie within $\pm 2*SE$ of the *population mean*.

These two intervals $sample.mean \pm SE$ and $sample.mean \pm 1.5*SE$
are called the **confidence intervals** for the population mean, at
levels $68\%$ and $95 \%$ probability respectively.

## {{< iconify flat-color-icons workflow >}} Workflow

Thus if we want to estimate a *population parameter*:

-   we take one **random** sample from the population

-   we calculate the estimate from the sample

-   we calculate the sample-sd

-   we calculate the *Standard Error* as $\frac{sample-sd}{\sqrt[]{n}}$

-   we calculate 95% confidence intervals for the *population parameter*
    based on the formula

    $CI_{95\%}= sample.mean \pm 2*SE$.

-   Since Standard Error decreases with sample size, we need to make our
    sample of adequate size.( $n=30$ seems appropriate in most cases.
    Why?)

## {{< iconify ooui references-rtl >}} References

1.  Diez, David M & Barr, Christopher D & Çetinkaya-Rundel, Mine,
    *OpenIntro Statistics*. <https://www.openintro.org/book/os/>

2.  Stats Test Wizard.
    <https://www.socscistatistics.com/tests/what_stats_test_wizard.aspx>

3.  Diez, David M & Barr, Christopher D & Çetinkaya-Rundel, Mine:
    *OpenIntro Statistics*. Available online
    <https://www.openintro.org/book/os/>

4.  Måns Thulin, *Modern Statistics with R: From wrangling and exploring
    data to inference and predictive modelling*
    <http://www.modernstatisticswithr.com/>

5.  Jonas Kristoffer Lindeløv, Common statistical tests are linear
    models (or: how to teach stats)
    <https://lindeloev.github.io/tests-as-linear/>

6.  CheatSheet
    <https://lindeloev.github.io/tests-as-linear/linear_tests_cheat_sheet.pdf>

7.  Common statistical tests are linear models: a work through by Steve
    Doogue <https://steverxd.github.io/Stat_tests/>

8.  Jeffrey Walker "Elements of Statistical Modeling for Experimental
    Biology".
    <https://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/>

9.  Adam Loy, Lendie Follett & Heike Hofmann (2016) Variations of
    *Q*--*Q* Plots: The Power of Our Eyes!, The American Statistician,
    70:2, 202-214, DOI:
    [10.1080/00031305.2015.1077728](https://doi.org/10.1080/00031305.2015.1077728)

10. 

[^1]: The \`Height\` variable seems to be normally distributed at
    population level. We will try other non-normal population variables
    as an exercise in the tutorials.

[^2]: Once `sample size = population`, we have complete access to the
    population and there is no question of estimation error! So
    `sample_sd = pop_sd`!

## Package Citations
```{r}
#| echo: false
pkgs <- cite_packages(output = "table", out.dir = ".")
knitr::kable(pkgs)

```
