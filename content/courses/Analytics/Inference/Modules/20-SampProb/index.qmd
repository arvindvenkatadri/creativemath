---
title: "\U0001F3B2 Samples, Populations, Statistics and Inference"
date: 25/Nov/2022
date-modified: "`r Sys.Date()`"
order: 20
abstract: "How much ~~Land~~ Data does a Man need?"
image: preview.jpg
categories:
- Sampling
- Central Limit Theorem
- Standard Error
- Confidence Intervals
bibliography: 
  - grateful-refs.bib
citation: true
filters: 
  - shinylive
---

## {{< fa folder-open >}} Slides and Tutorials

|                                                                                                 |     |     |                                                                                        |
|------------------|--------------------|------------------|------------------|
| <a href="./files/sampling-tutorial.qmd"><i class="fa-brands fa-r-project"></i> R Tutorial</a>   |     |     | <a href="./files/data/qdd-data.zip"> <i class="fa-solid fa-database"></i> Datasets</a> |

:::: {.pa4}
::: {.athelas .ml0 .mt0 .pl4 .black-90 .bl .bw2 .b--blue}
["Truth, virtue, and courage are not necessarily enough, but they are our best bet."]{.f5 .f4-m .f3-l .lh-copy .measure .mt0}

[ --- Jordan B. Peterson]{.f6 .ttu .tracked .fs-normal}
:::
::::

## {{< iconify noto-v1 package >}} Setting up R Packages

```{r}
#| label: setup
library(tidyverse) # Data Processing in R
library(mosaic) # Our workhorse for stats, sampling
library(skimr) # Good to Examine data
library(ggformula) # Formula interface for graphs

# load the NHANES data library
library(NHANES)
library(infer)
library(cowplot) # ggplot themes and stacking of plots

```


```{r}
#| label: Extra Pedagogical Packages
#| echo: false
#| message: false

library(grateful)
library(checkdown)
library(epoxy)
library(TeachHist)
library(TeachingDemos)
library(smovie) # Stat Movies with code
library(visualize) # Plot Densities, Histograms and Probabilities as areas under the curve
library(regressinator) # Fake data populations we can sample from
library(shinylive)
```


```{r}
#| label: Plot Sizing and theming
#| echo: false
#| message: false
#| results: hide

# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto

# Chunk options
knitr::opts_chunk$set(
 fig.width = 7,
 fig.asp = 0.618, # Golden Ratio
 #out.width = "80%",
 fig.align = "center"
)
### Ggplot Theme
### https://rpubs.com/mclaire19/ggplot2-custom-themes

theme_custom <- function(){ 
    font <- "Roboto Condensed"   #assign font family up front
    
    theme_classic(base_size = 14) %+replace%    #replace elements we want to change
    
    theme(
      panel.grid.minor = element_blank(),    #strip minor gridlines
      text = element_text(family = font),
      #text elements
      plot.title = element_text(             #title
                   family = font,            #set font family
                   #size = 20,               #set font size
                   face = 'bold',            #bold typeface
                   hjust = 0,                #left align
                   #vjust = 2                #raise slightly
                   margin=margin(0,0,10,0)
),               
      
      plot.subtitle = element_text(          #subtitle
                   family = font,            #font family
                   #size = 14,                #font size
                   hjust = 0,
                   margin=margin(2,0,5,0)
),               
      
      plot.caption = element_text(           #caption
                   family = font,            #font family
                   size = 8,                 #font size
                   hjust = 1),               #right align
      
      axis.title = element_text(             #axis titles
                   family = font,            #font family
                   size = 10                 #font size
),
      
      axis.text = element_text(              #axis text
                   family = font,            #axis family
                   size = 8)               #font size
    )
}

# Set graph theme
theme_set(new = theme_custom())
#
```


```{r}
#| echo: FALSE
#| eval: false
#| fig.align: 'center'
#| fig.alt: "Photo by Anirudh on unsplash"
knitr::include_graphics("preview.jpg")

```

## {{< iconify clarity group-solid >}} What is a Population?

A *population* is a collection of individuals or observations we are
interested in. This is also commonly denoted as a study population. We
mathematically denote the population's size using upper-case `N`.

A *population parameter* is some numerical summary about the population
that is unknown but you wish you knew. For example, when this quantity
is a mean like *the mean height of all Bangaloreans*, the population
parameter of interest is the *population mean*.

A *census* is an exhaustive enumeration or counting of all N individuals
in the population. We do this in order to compute the population
parameter's value exactly. Of note is that as the number N of
individuals in our population increases, conducting a census gets more
expensive (in terms of time, energy, and money).

::: callout-important
## {{< iconify carbon parameter >}} Parameters

Populations *P*arameters are usually indicated by Greek Letters.
:::

## {{< iconify game-icons card-pickup >}} What is a Sample?

Sampling is the act of collecting a small subset from the population, 
which we generally do when we can't perform a **census**. We mathematically denote the sample size using lower case `n`, as opposed to upper case `N` which denotes the population's size. Typically the sample size `n` is much
smaller than the population size `N`. Thus sampling is a much cheaper
alternative than performing a census.

A **sample statistic**, also known as a *point estimate*, is a summary
statistic like a `mean` or `standard deviation` that is computed from a
sample.

::: callout-note
## Why do we sample?

Because we cannot conduct a census ( not always ) --- and **sometimes we won't even know how big the population is** --- we take samples. And we
*still* want to do useful work for/with the population, after
estimating its parameters, *an act of generalizing* from sample to
population. So the question is, **can we estimate useful parameters of the population, using just samples? Can point estimates serve as useful guides to population parameters?**

[This act of generalizing from sample to population is at the heart of
**statistical inference**.]{style="background-color: yellow;"}
:::

::: callout-important
## An Alliterative Mnemonic

NOTE: there is an
[*alliterative*](https://www.grammarly.com/blog/alliteration/)
[*mnemonic*](https://www.merriam-webster.com/dictionary/mnemonic) here:
**S**amples have **S**tatistics; **P**opulations have **P**arameters.
:::

## {{< iconify fluent-mdl2 test-parameter >}} Population Parameters and Sample Statistics

|                    | Population Parameter | Sample Statistic |
|--------------------|----------------------|------------------|
| Mean               | $\mu$                | $\bar{x}$        |
| Standard Deviation | $\sigma$             | s                |
| Proportion         | p                    | $\hat{p}$        |
| Correlation        | $\rho$               | r                |
| Slope (Regression) | $\beta_1$            | $b_1$            |

: Parameters and Statistics

::: callout-note
## Question

Q.1. What is the mean commute time for workers in a particular city?

A.1. The *parameter* is the mean commute time $\mu$ for a *population*
containing all workers who work in the city. We *estimate* it using
$\bar{x}$, the mean of the random sample of people who work in the city.
:::

::: callout-note
## Question

Q.2. What is the correlation between the size of dinner bills and the
size of tips at a restaurant?

A.2. The *parameter is* $\rho$ , the correlation between bill amount and
tip size for a *population* of all dinner bills at that restaurant. We
estimate it using r, the correlation from a random sample of dinner
bills.
:::

::: callout-note
## Question

Q.3. How much difference is there in the proportion of 30 to 39-year-old residents who have only a cell phone (no land line phone) compared to 50 to 59-year-olds in the country?

A.3. The *population* is all citizens of the country, and the parameter
is $p_1 - p_2$, the difference in proportion of 30 to 39-year-old
residents who have only a cell phone ($p_1$) and the proportion with the
same property among all 50 to 59-year olds ($p_2$). We estimate it using
($\hat{p_1} - \hat{p_2}$), the difference in sample proportions computed
from random samples taken from each group.
:::

Sample statistics vary and in the following we will estimate this
uncertainty and decide how reliable they might be as estimates of
population parameters.

## {{< iconify pajamas issue-type-test-case >}} Case Study #1: Sampling the NHANES dataset

We will first execute some samples from a known dataset. We load up the
NHANES dataset and inspect it.

```{r}
#| column: body-outset-right
#| layout-ncol: 3
data("NHANES")
#mosaic::inspect(NHANES)
skimr::skim(NHANES)

```

Let us create a NHANES (sub)-dataset without duplicated IDs and only
adults:

```{r}
NHANES <-
  NHANES %>%
  distinct(ID, .keep_all = TRUE) 

#create a dataset of only adults
NHANES_adult <-  
  NHANES %>%
  filter(Age >= 18) %>% drop_na(Height)

```

### {{< iconify mdi head-thinking-outline >}} {{< iconify clarity group-solid >}} An "Assumed" Population

::: callout-important
## An "Assumed" Population

Normally, we very rarely have access to a **population**. All we can do is sample it. However, for now, and in order to build up our intuition, we will **treat** this dataset as our **Population**. So each
variable in the dataset is a *population* for that particular
quantity/category, with appropriate *population parameters* such as
`mean`s, `sd`-s, and `proportions`.
:::

Let us calculate the **population parameters** for the `Height` data
from our "assumed" population:

```{r}
# NHANES_adult is assumed population
pop_mean_height <- mosaic::mean(~ Height, data = NHANES_adult)
pop_sd_height <- mosaic::sd(~ Height, data = NHANES_adult)

pop_mean_height
pop_sd_height

```

### {{< iconify game-icons card-pickup >}} Sampling

Now, we will sample **ONCE** from the NHANES `Height` variable. Let us
take a sample of `sample size` 50. We will compare **sample statistics**
with **population parameters** on the basis of this ONE sample of 50:

```{r}
#| layout: [[20,20,60]]
#| warning: false

# Set graph theme
theme_set(new = theme_custom())
#

sample_height <- sample(NHANES_adult, size = 50) %>% 
  select(Height)
sample_height

sample_mean_height <- mean(~ Height, data = sample_height)
sample_mean_height

# Plotting the histogram of this sample
sample_height %>% 
  gf_histogram(~ Height, bins = 10) %>% 
  
  gf_vline(xintercept = sample_mean_height, 
           color = "red") %>% 
  
  gf_vline(xintercept = pop_mean_height, 
           colour = "blue") %>% 
  
  gf_label(7 ~ (pop_mean_height + 8), 
          label = "Population Mean", 
          color = "blue") %>% 
  
  gf_label(7 ~ (sample_mean_height - 8), 
          label = "Sample Mean", color = "red") %>% 
  gf_labs(title = "Distribution and Mean of a Single Sample")


```

### {{< iconify ic baseline-loop >}} {{< iconify game-icons card-pickup >}} {{< iconify icon-park-outline average >}} Repeated Samples and Sample Means

OK, so the `sample_mean_height` is not too far from the
`pop_mean_height`. Is this always true? Let us check: we will create 500
samples each of size 50. And calculate their mean as the *sample statistic*, giving us a data frame containing 500 `sample means`. We
will then see if these 500 means lie close to the `pop_mean_height`:

```{r}
#| layout: [[20,20,60]]
#| warning: false

# Set graph theme
theme_set(new = theme_custom())
#

sample_height_500 <- do(500) * {
  sample(NHANES_adult, size = 50) %>%
    select(Height) %>%
    summarise(
      sample_mean_500 = mean(Height),
      sample_min_500 = min(Height),
      sample_max_500 = max(Height))
}

head(sample_height_500)
dim(sample_height_500)

sample_height_500 %>%
  gf_point(.index ~ sample_mean_500, color = "red",
           title = "Sample Means are close to the Population Mean",
           subtitle = "Sample Means are Random!",
           caption = "Red lines represent our 500 samples") %>%
  
  gf_segment(
    .index + .index ~ sample_min_500 + sample_max_500,
    color = "red",
    linewidth = 0.3,
    alpha = 0.3,
    ylab = "Sample Index (1-500)",
    xlab = "Sample Means"
  ) %>%
  
  gf_vline(xintercept = ~ pop_mean_height, 
           color = "blue") %>%
  
  gf_label(-25 ~ pop_mean_height, label = "Population Mean", 
           color = "blue") 

```

::: callout-note
### Sample Means are a Random Variable

The `sample_mean`s (red dots), are themselves random because the samples are random, of course. It appears that they are generally in the vicinity of the `pop_mean` (blue line). And hence they will have a `mean` and `sd` too `r emoji::emoji("scream")`. Do not get confused ;-D
:::


### {{< iconify game-icons card-pickup >}} {{< iconify tabler chart-histogram >}} Distribution of Sample-Means

Since the **sample-means** are themselves random variables, let's plot
the **distribution** of these 500 sample-means themselves, called **a distribution of sample-means**. We will also plot the position of the
population mean `pop_mean_height` parameter, the mean of the `Height`
variable.

```{r}
#| label: Sampling-Mean-Distribution
#| layout-ncol: 2
#| warning: false
#| fig-cap: Distributions
#| fig-subcap: 
#|  - Sample
#|  - Sample and Population

# Set graph theme
theme_set(new = theme_custom())
#

sample_height_500 %>% 
  gf_dhistogram(~ sample_mean_500,bins = 30, xlab = "Height") %>% 
  
  gf_vline(xintercept = pop_mean_height, 
           color = "blue") %>% 
  
  gf_label(0.01 ~ pop_mean_height, 
            label = "Population Mean", 
            color = "blue") %>% 
gf_labs(title = "Sampling Mean Distribution")


# How does this **distribution of sample-means** compare with the
# overall distribution of the population?
# 
sample_height_500 %>% 
  gf_dhistogram(~ sample_mean_500, bins = 30,xlab = "Height") %>% 
  
  gf_vline(xintercept = pop_mean_height, 
           color = "blue") %>% 
  
   gf_label(0.01 ~ pop_mean_height, 
            label = "Population Mean", 
            color = "blue") %>% 

  ## Add the population histogram
  gf_histogram(~ Height, data = NHANES_adult, 
               alpha = 0.2, fill = "blue", 
               bins = 30) %>% 
  
  gf_label(0.025 ~ (pop_mean_height + 20), 
           label = "Population Distribution", color = "blue") %>% 
gf_labs(title = "Sampling Mean Distribution", subtitle = "Original Population overlay")


```

### {{< iconify carbon concept >}} Central Limit Theorem

We see in the Figure above that

-   the *distribution of sample-means* is centered around the
    `pop_mean`.
-   That the "spread" of the *distribution of sample means* is
    less than that of the original population. But exactly what is it?
-   And what is the kind of distribution?

One more experiment.

Now let's repeatedly sample `Height` and compute the sample mean, and
look at the resulting histograms and [Q-Q
plots.](https://www.youtube.com/watch?v=okjYjClSjOg) (Q-Q plots check
whether a certain distribution is close to being normal or not.)

We will use sample sizes of `c(8, 16, ,32, 64)` and generate 1000
samples each time, take the means and plot these 1000 means:

```{r}
#set.seed(12345)

samples_height_08 <- do(1000) * mean(resample(NHANES_adult$Height, size = 08))

samples_height_16 <- do(1000) * mean(resample(NHANES_adult$Height, size = 16))

samples_height_32 <- do(1000) * mean(resample(NHANES_adult$Height, size = 32))

samples_height_64 <- do(1000) * mean(resample(NHANES_adult$Height, size = 64))

# samples_height_128 <- do(1000) * mean(resample(NHANES_adult$Height, size = 128))

# Quick Check
head(samples_height_08)

```


```{r}
#| warning: false
#| eval: false
# Now let's create separate Q-Q plots for the different sample sizes.

# Set graph theme
theme_set(new = theme_custom())
#

p1 <- gf_qq( ~ mean,data = samples_height_08,
             title = "N = 8", 
             color = "dodgerblue") %>%
  gf_qqline()

p2 <- gf_qq( ~ mean,data = samples_height_16,
            title = "N = 16", 
            color = "sienna") %>%
  gf_qqline()

p3 <- gf_qq( ~ mean,data = samples_height_32,
            title = "N = 32", 
            color = "palegreen") %>%
  gf_qqline()

p4 <- gf_qq( ~ mean,data = samples_height_64,
            title = "N = 64", 
            color = "violetred") %>%
  gf_qqline()

cowplot::plot_grid(p1, p2, p3, p4)

# The QQ plots show that the results become more normally distributed
# (i.e. following the straight line) as the samples get larger. 
```

Let us plot their individual histograms to compare them:

```{r}
#| warning: false
#| layout-ncol: 2
#| layout-nrow: 2

# Set graph theme
theme_set(new = theme_custom())
#

# Let us overlay their individual histograms to compare them:
p5 <- gf_dhistogram(~ mean,
              data = samples_height_08,
              color = "grey",
              fill = "dodgerblue",title = "N = 8") %>%
  gf_fitdistr(linewidth = 1) %>%
  gf_vline(xintercept = pop_mean_height, inherit = FALSE,
           color = "blue") %>%
  gf_label(-0.025 ~ pop_mean_height, 
           label = "Population Mean", 
           color = "blue") %>% 
  gf_theme(scale_y_continuous(expand = expansion(mult = c(0.08,0.02))))
##
p6 <- gf_dhistogram(~ mean,
              data = samples_height_16,
              color = "grey",
              fill = "sienna",title = "N = 16") %>%
  gf_fitdistr(linewidth = 1) %>%
  gf_vline(xintercept = pop_mean_height,
           color = "blue") %>%
  gf_label(-.025 ~ pop_mean_height, 
           label = "Population Mean", 
           color = "blue") %>% 
  gf_theme(scale_y_continuous(expand = expansion(mult = c(0.08,0.02))))
##
p7 <- gf_dhistogram(~ mean,
                    data = samples_height_32 ,
                    na.rm = TRUE,
                    color = "grey",
                    fill = "palegreen",title = "N = 32") %>%
  gf_fitdistr(linewidth = 1) %>%
  gf_vline(xintercept = pop_mean_height,
           color = "blue") %>%
  gf_label(-.025 ~ pop_mean_height, 
           label = "Population Mean", color = "blue") %>% 
  gf_theme(scale_y_continuous(expand = expansion(mult = c(0.08,0.02))))

p8 <- gf_dhistogram(~ mean, 
                    data = samples_height_64,
                    na.rm = TRUE,
                    color = "grey",
                    fill = "violetred",title = "N = 64") %>% 
  gf_fitdistr(linewidth = 1) %>% 
  gf_vline(xintercept = pop_mean_height,
         color = "blue") %>%
  gf_label(-.025 ~ pop_mean_height, 
           label = "Population Mean", color = "blue") %>% 
  gf_theme(scale_y_continuous(expand = expansion(mult = c(0.08,0.02))))

#patchwork::wrap_plots(p5,p6,p7,p8)
p5
p6
p7
p8

```

And if we overlay the histograms:

```{r, echo=FALSE}

# Set graph theme
theme_set(new = theme_custom())
#

gf_dhistogram(~ mean,
              data = samples_height_08,
              color = "grey",
              fill = "dodgerblue",alpha = .5) %>% 
  # gf_label(0.15 ~ 172, inherit = FALSE,
  #          label = "N = 8", color = "dodgerblue") %>% 

gf_dhistogram(~ mean,
              data = samples_height_16,
              color = "grey",
              fill = "sienna",alpha = 0.5) %>% 
    # gf_label(0.2 ~ 172, 
    #        label = "N = 16", color = "sienna") %>% 

gf_dhistogram(~ mean,
              data = samples_height_32,
              color = "grey",
              fill = "palegreen",alpha = 0.5) %>% 
    # gf_label(0.3 ~ 172, 
    #        label = "N = 32", color = "palegreen") %>% 

gf_dhistogram(~ mean,
              data = samples_height_64,
              color = "grey",
              fill = "violetred",alpha = 0.5) %>% 
    # gf_label(0.4 ~ 172, 
    #        label = "N = 64", color = "violetred") %>% 

gf_fitdistr(
  ~ mean,
  data = samples_height_08,
  color = "dodgerblue",size = 2
) %>%
  
  gf_fitdistr(
    ~ mean,
    data = samples_height_16 ,
    color = "sienna",size = 2
  ) %>%
  
  gf_fitdistr(
    ~ mean,
    data = samples_height_32 ,
    na.rm = TRUE,
    color = "palegreen",size = 2
  ) %>%
  
  gf_fitdistr(
    ~ mean,
    data = samples_height_64 ,
    na.rm = TRUE,
    color = "violetred", size = 2
  ) %>%
  
  gf_vline(xintercept = pop_mean_height,
           color = "blue") %>%
  gf_label(-.02 ~ pop_mean_height, label = "Population Mean", 
           color = "blue") 

```

From the
histograms we learn that the sample-means are normally distributed
around the *population mean*. [This feels intuitively right because when
we sample from the population, many values will be close to the
*population mean*, and values far away from the mean will be
increasingly scarce.]{style="background-color: yellow;"}

Let us calculate the mean of the sample-means:

:::: {.columns}
::: {.column width="60%"}
```{r}
#| label: sample-means
#| eval: false

mean(~ mean, data  = samples_height_08)
mean(~ mean, data  = samples_height_16)
mean(~ mean, data  = samples_height_32)
mean(~ mean, data  = samples_height_64)
pop_mean_height

```
:::
::: {.column width="40%"}

```{r}
#| ref.label: sample-means
#| echo: false
```

:::
::::

Consider the *variances* in each of the sample-groups: these are the cumulative squared-differences of each number in each sample with its individual mean ($var = \Sigma[{x_i - \bar{x}}]^2$)

:::: {.columns}
::: {.column width="60%"}
```{r}
#| label: sample-variances
#| eval: false
var(~ mean, data  = samples_height_08)
var(~ mean, data  = samples_height_16)
var(~ mean, data  = samples_height_32)
var(~ mean, data  = samples_height_64)

```
::: 
::: {.column width="40%"}

```{r}
#| ref.label: sample-variances
#| echo: false
```

:::
::::
These are also decreasing steadily with sample size.

::: callout-note
## Central Limit Theorem

Now we have enough to state the **Central Limit Theorem (CLT)**

-   the [sample-means are normally distributed around the *population
    mean*]{style="background-color: yellow;"}.
-   the sample-mean distributions narrow with sample length, i.e [the
    `var` decreases with increasing sample size.]{style="background-color: yellow;"}
-   This is [regardless of the distribution of the *population* parameter]{style="background-color: yellow;"} itself.[^1]
:::
This theorem underlies all our procedures and techniques for statistical inference, as we shall see.

## {{< iconify dashicons code-standards >}} The Standard Error
Consider the *variances* in each of the sample-sets that we have generated. As we saw these decrease with sample size. 

:::: {.columns}
::: {.column}
The variance is a **total sum** of all squared differences from the mean, in a sample. If we divide the variances by the sample lengths, we get a sort of *average variance*, or *per-capita* squared error from the mean. Taking the square root gives us an average error, which we would call, of course, the *standard deviation*.
:::

::: {.column}
```{r}
#| label: showing-variance-calculation
#| echo: false

# Set graph theme
theme_set(new = theme_custom())
##
sample_height %>% 
  rowid_to_column() %>% 
  gf_hline(yintercept = mean(sample_mean_height)) %>% 
  gf_segment(sample_mean_height + Height ~ rowid + rowid, colour = "red") %>% 
  gf_point(Height ~ rowid) %>% 
  gf_labs(x = "Observation Index", y = "Sampled Height",
  title = "Sum of Squares of Red lines = Variance",
subtitle = "Black Line = Mean")

```
:::

::::

`sd = variance_sample/sqrt(sample_size)`[^2] where sample-size here is one of `c(8, 16,32,64)`.

We reserve the term *Standard Deviation* for the population, and name this computed standard deviation of the **sample-mean distribution** the **Standard Error**. This statistic derived from the sample, will help us infer our population parameters with a precise estimate of the *uncertainty* involved.

$$
\begin{align}
Standard\ Error\ \pmb {se} &= \frac{sample\ sd}{\sqrt[]{sample\ size}}\\
&= \frac{s}{\sqrt[]{n}}
\end{align}
$$

In our sampling experiments, the Standard Errors evaluate to:

:::: {.columns}
::: {.column width="60%"}
```{r}
#| label: standard-error-computation
#| eval: false
pop_sd_height <- sd(~ Height, data = NHANES_adult)

sd(~ mean, data = samples_height_08)/sqrt(8)
sd(~ mean, data = samples_height_16)/sqrt(16)
sd(~ mean, data = samples_height_32)/sqrt(32)
sd(~ mean, data = samples_height_64)/sqrt(64)

```
:::
::: {.column width="40%"}
```{r}
#| ref.label: standard-error-computation
#| echo: false
```

:::
::::

As seen, these are identical to the Standard Deviations of the
individual sample-mean distributions.

## {{< iconify fluent auto-fit-width-24-filled >}} Confidence intervals

When we work with samples, we want to be able to speak with a certain
degree of confidence about the **population mean**, based on the
evaluation of **one** sample mean, not a large number of them. Given
that sample-means are normally distributed around the **population means**, we can say that $68\%$ of *all possible sample-mean* lie within
$\pm SE$ of the *population mean*; and further that $95 \%$ of *all
possible sample-mean* lie within $\pm 2*SE$ of the *population mean*.

These two intervals $sample.mean \pm SE$ and $sample.mean \pm 1.5*SE$
are called the **confidence intervals** for the population mean, at
levels $68\%$ and $95 \%$ probability respectively.

::: {.column-margin}

```{r}
#| echo: false
#| fig-cap: "Confidence Intervals and the Bell Curve"
 xqnorm(p = c(0.025, 0.975), 
        mean = mean(samples_height_08$mean),
        sd = sd(samples_height_08$mean),
        return = c("plot"), verbose = T,
        system = "gg") %>%  
   gf_labs(title = "Deciding Confidence Intervals", 
 x = "") %>% 
   gf_theme(theme_classic())

```

:::

:::: {.columns}
::: {.column width="60%"}
```{r}
#| label: confidence-interval-computation-1
#| eval: false

infer::get_ci(samples_height_08,
              level = 0.95)
```

:::

::: {.column width="40%"}
```{r}
#| ref.label: confidence-interval-computation-1
#| echo: false
```

:::
::::

:::: {.columns}
::: {.column width="60%"}
```{r}
#| label: confidence-interval-computation-2
#| eval: false

get_ci(samples_height_16,
      level = 0.95)


```
:::
::: {.column width="40%"}
```{r}
#| ref.label: confidence-interval-computation-2
#| echo: false
```

:::
::::

:::: {.columns}
::: {.column width="60%"}
```{r}
#| label: confidence-interval-computation-3
#| eval: false

get_ci(samples_height_32,
        level = 0.95)

```
:::
::: {.column width="40%"}
```{r}
#| ref.label: confidence-interval-computation-3
#| echo: false
```

:::
::::

:::: {.columns}
::: {.column width="60%"}
```{r}
#| label: confidence-interval-computation-4
#| eval: false

get_ci(samples_height_64,
      level = 0.95)

```
:::
::: {.column width="40%"}
```{r}
#| ref.label: confidence-interval-computation-4
#| echo: false
```

:::
::::

::: callout-important
## Confidence Intervals also Shrink!
Yes, with sample size? Why is that a good thing?
:::

## {{< iconify flat-color-icons workflow >}} Workflow

Thus if we want to estimate a *population parameter*:

-   we take one **random** sample from the population\
-   we calculate the estimate from the sample\
-   we calculate the sample-sd\
-   we calculate the *Standard Error* as $\frac{sample-sd}{\sqrt[]{n}}$\
-   we calculate 95% confidence intervals for the *population parameter*
    based on the formula

    $CI_{95\%}= sample.mean \pm 2*SE$.

-   Since Standard Error decreases with sample size, we need to make our
    sample of adequate size.( $n=30$ seems appropriate in most cases.
    Why?)
  
- And we do not have to worry about the distribution of the population. It need not be normal !!

## {{< iconify material-symbols interactive-space-outline >}} An interactive Sampling app

Here below is an interactive sampling app. Choose the number of samples you want from a **normally-distributed population** $N(\mu = 0, \sigma = 1)$. Vary the sample size to see how the sample mean varies around the tru population mean. 

```{shinylive-r}
#| standalone: true
#| viewerHeight: 600
#| viewerWidth: 1000
library(shiny)
library(bslib)

# Define UI for app that draws a histogram ----
ui <- page_sidebar(
  sidebar = sidebar(open = "open",
    numericInput("n", "Sample count", 100),
    checkboxInput("pause", "Pause", FALSE),
  ),
  plotOutput("plot", width=800)
)

server <- function(input, output, session) {
  data <- reactive({
    input$resample
    if (!isTRUE(input$pause)) {
      invalidateLater(1000)
    }
    rnorm(input$n)
  })
  
  output$plot <- renderPlot({
    hist(data(),
      breaks = 40,
      xlim = c(-3, 3),
      ylim = c(0, 1.5),
      lty = "blank",
      xlab = "value",
      freq = FALSE,
      main = ""
    )
    
    x <- seq(from = -3, to = 3, length.out = 500)
    y <- dnorm(x)
    lines(x, y, lwd=1.5)
    
    lwd <- 5
    abline(v=0, col="red", lwd=lwd, lty=2)
    abline(v=mean(data()), col="blue", lwd=lwd, lty=1)

    legend(legend = c("Normal", "Mean", "Sample mean"),
      col = c("black", "red", "blue"),
      lty = c(1, 2, 1),
      lwd = c(1, lwd, lwd),
      x = 0.25,
      y = 1.25
    )
  }, res=140)
}

# Create Shiny app ----
shinyApp(ui = ui, server = server)

```
What if we sample from *not* a normal distribution but say, a Poisson Distribution? Will we still see the sample mean hover around the population mean?

```{r}
#| echo: false
smovie::clt(distn = "gamma")

```

## An Interactive app for the CLT

<https://gallery.shinyapps.io/CLT_mean/>


## {{< iconify ooui references-rtl >}} References

1.  Diez, David M & Barr, Christopher D & Çetinkaya-Rundel, Mine,
    *OpenIntro Statistics*. <https://www.openintro.org/book/os/>

2.  Stats Test Wizard.
    <https://www.socscistatistics.com/tests/what_stats_test_wizard.aspx>

3.  Diez, David M & Barr, Christopher D & Çetinkaya-Rundel, Mine:
    *OpenIntro Statistics*. Available online
    <https://www.openintro.org/book/os/>

4.  Måns Thulin, *Modern Statistics with R: From wrangling and exploring
    data to inference and predictive modelling*
    <http://www.modernstatisticswithr.com/>

5.  Jonas Kristoffer Lindeløv, Common statistical tests are linear
    models (or: how to teach stats)
    <https://lindeloev.github.io/tests-as-linear/>

6.  CheatSheet
    <https://lindeloev.github.io/tests-as-linear/linear_tests_cheat_sheet.pdf>

7.  Common statistical tests are linear models: a work through by Steve
    Doogue <https://steverxd.github.io/Stat_tests/>

8.  Jeffrey Walker "Elements of Statistical Modeling for Experimental
    Biology".
    <https://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/>

9.  Adam Loy, Lendie Follett & Heike Hofmann (2016) Variations of
    *Q*--*Q* Plots: The Power of Our Eyes!, The American Statistician,
    70:2, 202-214, DOI:
    [10.1080/00031305.2015.1077728](https://doi.org/10.1080/00031305.2015.1077728)


[^1]: The \`Height\` variable seems to be normally distributed at
    population level. We will try other non-normal population variables
    as an exercise in the tutorials.

[^2]: Once `sample size = population`, we have complete access to the
    population and there is no question of estimation error! So
    `sample_sd = pop_sd`!

::: {#refs style="font-size: 60%;"}

###### {{< iconify lucide package-check >}} R Package Citations
```{r}
#| echo: false
#scan_packages()
cite_packages(
  output = "table",
  out.dir = ".",
  out.format = "html",
  pkgs = c("NHANES", "regressinator", "smovie", "TeachHist",
           "TeachingDemos", "visualize")
) %>%
  knitr::kable(format = "simple")

```
:::
