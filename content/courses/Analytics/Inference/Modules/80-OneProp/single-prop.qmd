---
title: "\U0001F0CF Testing a Single Proportion"
author: "Arvind Venkatadri"
date: 10/Nov/2022
date-modified: "`r Sys.Date()`"
abstract: "Using Permutation Tests to check the significance of a proportion"
order: 80
image: preview.jpg
image-alt: From The Internet Archive
categories:
- Permutation
- Monte Carlo Simulation
- Random Number Generation
- Distributions
- Generating Parallel Worlds
---


## {{< iconify noto-v1 package >}} Setting up R packages

```{r}
#| label: setup
#| include: true
#| message: false
#| warning: false
knitr::opts_chunk$set(echo = TRUE,message = TRUE,warning = TRUE, fig.align = "center")
options(digits=2)
library(tidyverse)
library(mosaic)
library(ggformula)
library(ggmosaic) # plotting mosaic plots for Categorical Data
library(vcd) # Creating Tables and plotting mosaic charts

## Datasets from Chihara and Hesterberg's book (Second Edition)
library(resampledata)

## Datasets from Cetinkaya-Rundel and Hardin's book (First Edition)
library(openintro)

```

## Introduction

We saw from the diagram created by Allen Downey that *there is only one
test*! We will now use this philosophy to develop a technique that
allows us to mechanize several *Statistical Models* in that way, with
nearly identical code.

We will use two packages in R, `mosaic` to develop our intuition for
what are called **permutation** based statistical tests. (There is also
a more recent package called `infer` in R which can do pretty much all
of this, including visualization. In my opinion, the code is a little
too high-level and does not offer quite the detailed insight that the
`mosaic` package does).

## Permutation Visually Demonstrated

We will look visually at a permutation exercise. We will create dummy
data that contains the following case study:

> A set of identical resumes was sent to male and female evaluators. The
> candidates in the resumes were of both genders. We wish to see if
> there was difference in the way resumes were evaluated, by male and
> female evaluators. (We use just *one* male and *one* female evaluator
> here, to keep things simple!)

```{r}
#| label: Artificial Data
#| echo: false
set.seed(123456)
data1 <- tibble(evaluator = rep(x = "F", times = 24),
               candidate = sample(c(0,1), 
                               size = 24, 
                               replace = T, 
                                prob = c(0.1, 0.9)))

data2 <- tibble(evaluator = rep(x = "M", times = 24),
               candidate = sample(c(0,1), size = 24, 
                                replace = T, 
                                prob = c(0.6, 0.4)))
#data1
#data2
               
data <-  rbind(data1, data2) %>% 
  
  # Create a 4*12 matrix of integer coordinates
  cbind(expand.grid(x = 1:4, y = seq(4, 48,4))) %>% 
  mutate(evaluator = as_factor(evaluator))
data %>% select(evaluator, candidate)
summary <- data %>% 
  group_by(evaluator) %>% 
  summarise(selection_ratio = mean(candidate == "0"), 
            count = count(candidate == "0"),
            n = n())
summary
obs_difference <- diff(mean(candidate ~ evaluator, data = data))
obs_difference

```

So, we have a solid disparity in percentage of selection between the two
evaluators! Let us first plot this:

```{r}
#| warning: false
#| echo: false

p0 <- data %>% 
  
  # knock off the coordinates prior to shuffle
  select(evaluator, candidate) %>% 
  #mutate(candidate = shuffle(candidate, replace = FALSE)) %>% 
  arrange(evaluator, candidate) %>% 
  
  # reassign coordinates
  cbind(., expand.grid(x = 1:4, y = seq(4, 24,4))) %>% # Not 48!!
  
  ggplot(data = ., aes(x = x, y = y, 
             group = evaluator, 
             fill = as_factor(candidate))) + 
  geom_point(shape = 21, size = 8) + 
  scale_fill_manual(name = NULL,
                    values = c("red", "blue"),
                    labels = c("Rejected","Selected")) + 
  
  # Very important to have scales = "free" !!
  facet_wrap(~ evaluator, ncol = 1, scales = "free") +
  
  theme_void() + 
  theme(legend.position = "top", 
                       strip.background = element_blank(),
                       strip.text.x = element_blank(),
                       plot.title = element_text(hjust = 0.5)) + 
 expand_limits(y = c(0,28)) +
  
  # Need to give stat_brace two pairs x values and y values 
  # as there are two facets
  ggbrace::stat_brace(aes(#x = c(4.5, 5.5, 4.5, 5.5), # What a hack this is!!
                          #y = c(4, 24, 5, 24),
                      label = evaluator),
                      rotate = 90,labelrotate = 0,
                      labelsize = 4,
                      inherit.aes = TRUE) 
p0
```

Now we pretend that *there is no difference between the selections made
by either set of evaluators*. So we can just:

-   Pool up all the evaluations

-   Arbitrarily re-assign a given candidate(selected or rejected) to
    either of the two sets of evaluators, by permutation.

    How would that pooled shuffled set of evaluations look like?

    ```{r}
    #| label: Shuffle the data once
    #| echo: false

    data_shuffled <- data %>%
      mutate(evaluator = shuffle(evaluator)) 
    #data_shuffled %>% select(evaluator, candidate)
    data_shuffled %>% 
      group_by(evaluator) %>% 
      summarise(selection_ratio = mean(candidate == "0"), 
            count = count(candidate == "0"),
            n = n())
    ```

    ```{r}
    #| echo: false
    p1 <- data_shuffled %>% 
      group_by(evaluator, candidate) %>% 
      ggplot(aes(x = x, y = y, 
                 group = evaluator, 
                 fill = as_factor(candidate))) + 
      geom_point(shape = 21,size = 8) + 
      scale_fill_manual(name = NULL,
                        values = c("red", "blue"),
                        labels = c("Rejected","Selected")) + 
      theme_void() + theme(legend.position = "top") +
      ggtitle(label = "Pooled Evaluations, Permuted")

    ```

```{r}
#| warning: false
#| echo: false
p2 <- data_shuffled %>% 
  
  # knock off the coordinates
  # Do not use "group_by"!! Why not?
  select(evaluator, candidate) %>% 
  arrange(evaluator,candidate) %>% 
  
  # reassign coordinates
  cbind(., expand.grid(x = 1:4, y = seq(4, 24,4))) %>% # Not 48!!
  
  ggplot(data = ., aes(x = x, y = y, 
             group = evaluator, 
             fill = as_factor(candidate))) + 
  geom_point(shape = 21, size = 8) + 
  scale_fill_manual(name = NULL,
                    values = c("red", "blue"),
                    labels = c("Rejected","Selected")) + 
  
  # Very important to have scales = "free" !!
  facet_wrap(~ evaluator, ncol = 1, scales = "free") +
   expand_limits(y = c(0,28)) +

  ggbrace::stat_brace(aes(label = evaluator),
                      rotate = 90,
                      labelrotate = 0,
                      labelsize = 4,
                      inherit.aes = TRUE) +
  theme_void() + 
  theme(legend.position = "top", 
                       strip.background = element_blank(),
                       strip.text.x = element_blank(),
                       plot.title = element_text(hjust = 0.5)) +
  ggtitle(label = "Permuted Evaluations, Grouped")

```

```{r}
#| echo: false
#| message: false
#| warning: false

library(patchwork)
p1 + p2 + plot_annotation(tag_levels = "A") + plot_layout(widths = unit(c(7, 10), c('cm', 'cm')))

```

As can be seen, the ratio is different! We can now do this many many
times, to check out our Hypothesis that there is *no* bias. We can plot
the distribution of the differences in selection ratio and see how that
artificially created distribution compares with mother-nature, the
originally observed figure.

```{r}
null_dist <- do(10000) * diff(mean(candidate ~ shuffle(evaluator), data = data))
null_dist %>% names()
null_dist %>% gf_histogram( ~ M, 
                            fill = ~ (M <= obs_difference)) %>% 
  gf_vline(xintercept = ~ obs_difference, color = "red" )

mean(~ M<= obs_difference, data = null_dist)

```

We see that the artificial data can hardly ever (\$ p = 0.012\$) mimic
what the real world experiment is showing. Hence we had good reason to
reject our Hypothesis that there is no bias.

## An Example

## Conclusion

## References

1.  [OpenIntro Modern Statistics: Chapter
    17](https://openintro-ims.netlify.app/inference-one-prop.html)
2.  Chihara and Hesterberg
