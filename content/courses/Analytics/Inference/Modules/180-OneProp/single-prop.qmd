---
title: "\U0001F0CF Testing a Single Proportion"
author: "Arvind V."
date: 10/Nov/2022
date-modified: "`r Sys.Date()`"
abstract: "Inference Tests for the significance of a Proportion"
order: 180
image: preview.jpg
image-alt: From The Internet Archive
categories:
- Permutation
- Monte Carlo Simulation
- Random Number Generation
- Distributions
- Generating Parallel Worlds
bibliography: 
  - grateful-refs.bib
citation: true
---

## {{< iconify noto-v1 package >}} Setting up R packages

```{r}
#| label: setup
#| include: true
#| message: false
#| warning: false
knitr::opts_chunk$set(echo = TRUE,message = TRUE,warning = TRUE, fig.align = "center")
options(digits=2)
library(tidyverse)
library(mosaic)
library(ggformula)


## Datasets from Chihara and Hesterberg's book (Second Edition)
library(resampledata)

## Datasets from Cetinkaya-Rundel and Hardin's book (First Edition)
library(openintro)

```


```{r}
#| label: Extra Pedagogical Packages
#| echo: false
#| message: false

library(checkdown)
library(epoxy)
library(ggbrace)
library(TeachHist)
library(TeachingDemos)
library(grateful)

```

## {{< iconify openmoji japanese-symbol-for-beginner >}} Introduction

We saw from the diagram created by Allen Downey that [there is only
one test](http://allendowney.blogspot.com/2016/06/there-is-still-only-one-test.html)!
We will now use this philosophy to develop a technique that allows us to
mechanize several *Statistical Models* in that way, with nearly
identical code.

We will use two packages in R, `mosaic` to develop our intuition for
what are called **permutation** based statistical tests. (There is also
a more recent package called `infer` in R which can do pretty much all
of this, including visualization. In my opinion, the code is a little
too high-level and does not offer quite the detailed insight that the
`mosaic` package does).

## {{< iconify svg-spinners blocks-shuffle-3 >}} Permutation Visually Demonstrated

We will look visually at a permutation exercise. We will create dummy
data that contains the following case study:

> A set of identical resumes was sent to male and female evaluators. The
> candidates in the resumes were of both genders. We wish to see if
> there was difference in the way resumes were evaluated, by male and
> female evaluators. (We use just *one* male and *one* female evaluator
> here, to keep things simple!)

```{r}
#| label: Artificial Data
#| echo: false
#| layout: [[30, 40, 10, 40]]
set.seed(123456)
data1 <- tibble(evaluator = rep(x = "F", times = 24),
               candidate = sample(c(0,1), 
                               size = 24, 
                               replace = T, 
                                prob = c(0.1, 0.9)))

data2 <- tibble(evaluator = rep(x = "M", times = 24),
               candidate = sample(c(0,1), size = 24, 
                                replace = T, 
                                prob = c(0.6, 0.4)))
#data1
#data2
               
data <-  rbind(data1, data2) %>% 
  
  # Create a 4*12 matrix of integer coordinates
  cbind(expand.grid(x = 1:4, y = seq(4, 48,4))) %>% 
  mutate(evaluator = as_factor(evaluator))
data %>% select(evaluator, candidate)
summary <- data %>% 
  group_by(evaluator) %>% 
  summarise(selection_ratio = mean(candidate == "0"), 
            count = count(candidate == "0"),
            n = n())
summary
obs_difference <- diff(mean(candidate ~ evaluator, data = data))
obs_difference


p0 <- data %>% 
  
  # knock off the coordinates prior to shuffle
  select(evaluator, candidate) %>% 
  #mutate(candidate = shuffle(candidate, replace = FALSE)) %>% 
  arrange(evaluator, candidate) %>% 
  
  # reassign coordinates
  cbind(., expand.grid(x = 1:4, y = seq(4, 24,4))) %>% # Not 48!!
  
  ggplot(data = ., aes(x = x, y = y, 
             group = evaluator, 
             fill = as_factor(candidate))) + 
  geom_point(shape = 21, size = 8) + 
  scale_fill_manual(name = NULL,
                    values = c("red", "blue"),
                    labels = c("Rejected","Selected")) + 
  
  # Very important to have scales = "free" !!
  facet_wrap(~ evaluator, ncol = 1, scales = "free") +
  
  theme_void() + 
  theme(legend.position = "top", 
                       strip.background = element_blank(),
                       strip.text.x = element_blank(),
                       plot.title = element_text(hjust = 0.5)) + 
 expand_limits(y = c(0,28)) +
  
  # Need to give stat_brace two pairs x values and y values 
  # as there are two facets
  ggbrace::stat_brace(aes(#x = c(4.5, 5.5, 4.5, 5.5), # What a hack this is!!
                          #y = c(4, 24, 5, 24),
                      label = evaluator),
                      rotate = 90,labelrotate = 0,
                      labelsize = 4,
                      inherit.aes = TRUE) 
p0

```

So, we have a solid disparity in percentage of selection between the two
evaluators!

### {{< iconify mdi cards-playing-outline >}} Permutation

Now we pretend that *there is no difference between the selections made
by either set of evaluators*. So we can just:

-   Pool up all the evaluations\
-   Arbitrarily re-assign a given candidate(selected or rejected) to
    either of the two sets of evaluators, by permutation.\

How would that pooled shuffled set of evaluations look like?

```{r}
#| label: Shuffle the data once
#| echo: false

data_shuffled <- data %>%
  mutate(evaluator = shuffle(evaluator))
#data_shuffled %>% select(evaluator, candidate)
data_shuffled %>%
  group_by(evaluator) %>%
  summarise(
    selection_ratio = mean(candidate == "0"),
    count = count(candidate == "0"),
    n = n()
  )
```

```{r}
#| echo: false
#| layout: [[45,-10, 45]]
data_shuffled %>% 
      group_by(evaluator, candidate) %>% 
      ggplot(aes(x = x, y = y, 
                 group = evaluator, 
                 fill = as_factor(candidate))) + 
      geom_point(shape = 21,size = 8) + 
      scale_fill_manual(name = NULL,
                        values = c("red", "blue"),
                        labels = c("Rejected","Selected")) + 
      theme_void() + theme(legend.position = "top") +
      ggtitle(label = "Pooled Evaluations, Permuted")


data_shuffled %>% 
  
  # knock off the coordinates
  # Do not use "group_by"!! Why not?
  select(evaluator, candidate) %>% 
  arrange(evaluator,candidate) %>% 
  
  # reassign coordinates
  cbind(., expand.grid(x = 1:4, y = seq(4, 24,4))) %>% # Not 48!!
  
  ggplot(data = ., aes(x = x, y = y, 
             group = evaluator, 
             fill = as_factor(candidate))) + 
  geom_point(shape = 21, size = 8) + 
  scale_fill_manual(name = NULL,
                    values = c("red", "blue"),
                    labels = c("Rejected","Selected")) + 
  
  # Very important to have scales = "free" !!
  facet_wrap(~ evaluator, ncol = 1, scales = "free") +
   expand_limits(y = c(0,28)) +

  ggbrace::stat_brace(aes(label = evaluator),
                      rotate = 90,
                      labelrotate = 0,
                      labelsize = 4,
                      inherit.aes = TRUE) +
  theme_void() + 
  theme(legend.position = "top", 
                       strip.background = element_blank(),
                       strip.text.x = element_blank(),
                       plot.title = element_text(hjust = 0.5)) +
  ggtitle(label = "Permuted Evaluations, Grouped")

```

```{r}
#| label: Old code retained
#| echo: false
#| message: false
#| warning: false
#| eval: false

library(patchwork)
p1 + p2 + plot_annotation(tag_levels = "A") + plot_layout(widths = unit(c(7, 10), c('cm', 'cm')))

```

As can be seen, the ratio is different! 

We can now check out our Hypothesis that there is *no* bias. We can shuffle the data many many times, calculating the ratio each time, and plot the *distribution of the differences in selection ratio* and see how that artificially created distribution compares with the originally observed figure from Mother Nature.

```{r}
#| layout: [[60, -10, 30]]
null_dist <- do(5000) * diff(mean(candidate ~ shuffle(evaluator), 
                                   data = data))
# null_dist %>% names()
null_dist %>% gf_histogram( ~ M, 
                  fill = ~ (M <= obs_difference), 
                  bins = 25,show.legend = FALSE,
                  xlab = "Bias Proportion", 
                  ylab = "How Often?",
                  title = "Permutation Test on Diffence between Groups",
                  subtitle = "") %>% 
  gf_vline(xintercept = ~ obs_difference, color = "red" ) %>% 
  gf_label(500 ~ obs_difference, label = "Observed\n Bias", 
           show.legend = FALSE) %>% 
  gf_theme(theme_classic())

mean(~ M<= obs_difference, data = null_dist)

```

We see that the artificial data can hardly ever ($p = 0.012$) mimic what
the real world experiment is showing. Hence we had good reason to reject
our NULL Hypothesis that there is no bias.

# {{< iconify pajamas issue-type-test-case >}} Case Study #1: TBD

# {{< lordicons hbjlznlo >}} Case Study #2: Weight vs Exercise in the YRBSS Survey

## {{< iconify fluent-mdl2 decision-solid >}} Conclusion

## {{< iconify ooui references-ltr >}} References

1.  [OpenIntro Modern Statistics: Chapter
    17](https://openintro-ims.netlify.app/inference-one-prop.html)
2.  Chihara and Hesterberg

## Package Citations
```{r}
#| echo: false
pkgs <- cite_packages(output = "table", out.dir = ".")
knitr::kable(pkgs)

```
