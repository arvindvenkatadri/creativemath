---
title: ML - Clustering
date: 19/July/2022
date-modified: "`r Sys.Date()`"
abstract: "We will look at the basic models for Clustering of Data."
order: 40
tags:
  - Machine Learning
  - Orange
  - kNN Algorithm
  - k Means Algorithm
  - Hierarchical Clustering
  - Agglomerative Clustering
  - Distances
---

## Introduction

Quoting from <http://baoqiang.org/?p=579>

### k-Nearest-Neighbour and K-Means clustering

These two are arguably the two commonly used cluster methods. One of the
reasons is that they are easy to use and also somehow straightforward.
So how do they work?

**k-Nearest-Neighbour**: Provide N n-dimension entries with known
associated classes for each entry, the number of classes is k, that is,
$$
\{\vec{x_i}, y_i\} ,\ \vec{x_i} \in\ {\Re^{n}}\ , y_i\ = \{c_1,...c_k\}, 
i = 1...N
$$

For a new entry $\vec{v_j}$, to which class should it belong? We need
use a distance measure to get the k closest entries of the new entry
\vec{v_{j}}, the final decision is *simple majority vote* based the
closest k neighbors. The distance metric could be euclidean or other
similar ones.

<iframe width="100%" height="735" frameborder="0" src="https://observablehq.com/embed/16bc2b3dcb13d1cd@289?cells=viewof+numTrain%2Cviewof+k%2CPlot">

</iframe>


<iframe width="100%" height="500" frameborder="0"
  src="https://observablehq.com/embed/5f5821f1971c7749?cell=*"></iframe>
  
  
**K-means**: Given N n-dimension entries and classify them in k classes.
At first, we *randomly* choose k entries and assign them to k clusters.
They are the seed classes. Then we calculate the distance between each
entry and each class. Each entry will be assigned into one class in
terms of the its distance to each class, i.e., assign the entry to its
closest class. After the assignment is complete, we then calculate the
centroid of each class based on their new members. After the centroid
calculation, we go back to the distance calculation and therefore new
round classification. We stop the iteration when there is
convergence,i.e,, no new centroid and classification.

The two methods are all *semi-supervised learning algorithms* because
they do need we provide the number of clusters prior the clustering.

<iframe width="100%" height="853" frameborder="0" src="https://observablehq.com/embed/ab4e983a61997013?cells=viewof+seed%2Cviewof+spread%2Cviewof+num_centroids%2Cviewof+selection%2Cviewof+stepslider">

</iframe>

## Workflow using Orange

## Workflow using Radiant

## Workflow using R

## Conclusion

## References

1.  K-means Cluster Analysis. [UC Business Analytics R Programming
    Guide](https://uc-r.github.io/)
    <https://uc-r.github.io/kmeans_clustering#optimal>

2.  Thean C Lim. Clustering: k-means, k-means ++ and gganimate.
    <https://theanlim.rbind.io/post/clustering-k-means-k-means-and-gganimate/>


3. https://www.datacamp.com/tutorial/hierarchical-clustering-R

4. https://www.datacamp.com/tutorial/k-means-clustering-r

5. Michele Coscia. 2019. *Who will Cluster the Cluster Makers?* <https://www.michelecoscia.com/?p=1709> Accessed 12 Jan 2024. 


