---
date-modified: "`r Sys.Date()`"
date: 20/Nov/2024
title: "The Perceptron"
order: 182
summary: 
tags:
- Neural Nets
- Perceptron
- Weighted Average

---


```{r setup, include=FALSE}
library(blogdown)
library(tidyverse)
library(ggformula)
library(mosaicCalc) # Analytic Calculus
library(ambient) # Noise generation in R
library(plot3D) # 3D plots for explanation
library(caracas)
library(downloadthis)
library(knitr)
library(kableExtra)
## Markdown boiler plate stuff!!
# ![An Elephant](elephant.png){#fig-elephant}
# This is illustrated well by @fig-elephant.
# 
# ### Figure Panel Divs
#     ::: {#fig-elephants layout-ncol=2}
#     ![Surus](surus.png){#fig-surus}
#     ![Hanno](hanno.png){#fig-hanno}
#      Famous Elephants
#     :::
#     Adding download buttons
#     data that has been read in
#     {{< downloadthis ../../../../materials/Data/housing_train.csv dname="house_prices" label="Download the House Prices Dataset" icon="database-fill-down" type="info" >}}
#    existing file
#    {{< downloadthis Orange/grouped-summaries.ows dname="grouped_summaries" label="Download the Orange Workflow" icon="database-fill-down" type="info" >}} 

```

```{r}
#| label: Extra Pedagogical Packages
#| echo: false
#| message: false

library(checkdown)
library(epoxy)
library(grateful)
library(MKdescr)
library(shinylive) # To create a Shiny app in a Quarto HTML doc
# Will not work if webr is also used in the SAME Quarto doc!
library(sysfonts)
library(gfonts)
library(kableExtra)
# library(conflicted)
# conflicted::conflicts_prefer(dplyr::filter, dplyr::count, dplyr::last, dplyr::glimpse, base::max)
library(downloadthis)
#devtools::install_github("mccarthy-m-g/embedr")
library(embedr) # Embed multimedia in HTML files
```

```{r}
#| label: Plot Sizing and theming
#| echo: false
#| message: false
#| results: hide

# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto

# Chunk options
knitr::opts_chunk$set(
 fig.width = 7,
 fig.asp = 0.618, # Golden Ratio
 #out.width = "80%",
 fig.align = "center"
)
### Ggplot Theme
### https://rpubs.com/mclaire19/ggplot2-custom-themes

theme_custom <- function(){ 
    font <- "Roboto Condensed"   #assign font family up front
    
    theme_classic(base_size = 14) %+replace%    #replace elements we want to change
    
    theme(
      panel.grid.minor = element_blank(),    #strip minor gridlines
      text = element_text(family = font),
      #text elements
      plot.title = element_text(             #title
                   family = font,            #set font family
                   #size = 20,               #set font size
                   face = 'bold',            #bold typeface
                   hjust = 0,                #left align
                   #vjust = 2                #raise slightly
                   margin=margin(0,0,10,0)
),               
      
      plot.subtitle = element_text(          #subtitle
                   family = font,            #font family
                   #size = 14,                #font size
                   hjust = 0,
                   margin=margin(2,0,5,0)
),               
      
      plot.caption = element_text(           #caption
                   family = font,            #font family
                   size = 8,                 #font size
                   hjust = 1),               #right align
      
      axis.title = element_text(             #axis titles
                   family = font,            #font family
                   size = 10                 #font size
),
      
      axis.text = element_text(              #axis text
                   family = font,            #axis family
                   size = 8)               #font size
    )
}

# Set graph theme
theme_set(new = theme_custom())
#
```


```{r,echo = FALSE, eval = FALSE, fig.alt="Petr Slováček on Unsplash", fig.align='center'}
knitr::include_graphics("featured.jpg")

```

## {{< iconify icons8 idea >}} Inspiration


## What is a Perceptron?

The [perceptron](../../../../materials/pdfs/Rosenblatt1958.pdf) was invented by Frank Rosenblatt is considered one of the foundational pieces of neural network structures. The output is viewed as a *decision* from the neuron and is usually propagated as an input to other neurons inside the neural network.

![Perceptron](../../../../materials/images/png/Neurons.png){height=360}

#### Math Intuition

- We can imagine this as a set of inputs that *averaged in weighted fashion*. 
- Since the inputs are this effectively acts like a **linear transformation** of the input data.
- If we imagine 2D-input (i.e inputs representing X and Y directions/quantities), then the multiplications scale/stretch/compress the X-axis and Y-axis of the 2D-plane, like a rubber sheet. (But do not **fold it**.)
- Metaphorically, it seems like the neuron is consulting each of the inputs, asking for their opinion, and then making a decision by attaching different amounts of significance to each opinion.
- The Structure should remind you of [Linear Regression](../../../Analytics/Modelling/Modules/LinReg/index.qmd)!

#### Why "Linear"?
Why are (almost) all operations linear operations in a NN? 

- Well, weighted sums *are* a linear operation!
- Moreover, we wish to be able to set-up analytic functions for performance of the DLNN, and be able to differentiate them to be able to optimize them. 
- Non-linear blocks, such as *threshold blocks/signum-function based slicers* are not differentiable and we are unable to set up such analysis. 
- Note the title of [this reference](https://www.sscardapane.it/alice-book/).

#### Why is there a Bias input?

- We want the weighted sum of the inputs to *mean something* significant, before we accept it. The bias is *subtracted* from the weighted sum of inputs, and the bias input could also (notionally) have a weight. 
- The *bias* is like a threshold which the weighted sum has to exceed; if it does, the neuron is said to **fire**.  

#### What is the Activation Block?

- We said earlier that the weighting and adding is a linear operation. 
- While this is great, simple linear translations of data are not capable of generating what we might call learning or generalization ability. 
- We need to have some non-linear block to allow the data to create nonlinear transformations of the data space, such as curving it, or folding it, or creating bumps, depressions, twists, and so on. 

![Activation](../../../../materials/images/png/Activation_Functions.png){height=360}

- This nonlinear function needs to be chosen with care so that it is both differentiable and keeps the math analysis tractable. (More later)
- Such a nonlinear mathematical function is implemented in the Activation Block.
- See this example: red and blue areas, which we wish to **separate and classify these** with our DLNN, are not separable unless we fold and curve our 2D data space. 
- The separation is achieved using a linear operation, i.e. a LINE!!(More later)

![From Colah Blog, used sadly without permission](https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/img/spiral.1-2.2-2-2-2-2-2.gif)


#### What is the Sigmoid Function?

So how do we implement this nonlinear Activation Block?

- One of the popular functions used in the Activation Block is a function based on the exponential function $e^x$.
- Why? Because this function retains is identity when differentiated! This is a very convenient property!

![Sigmoid Activation](../../../../materials/images/png/Sigmoid.png){height=360}



::: callout-note
#### Remembering Logistic Regression
Recall your study of [Logistic Regression](../../../Analytics/Modelling/Modules/LogReg/index.qmd). There, the Sigmoid function was used to model the odds of the (Qualitative) target variable against the (Quantitative) predictor.
:::

2. Deep Learning Networks
  - Input Layers
  - Output Layers
  - Hidden Layers
  - Activation
  
3. Adaptation and Training
  - Backpropagation
  - Error Functions and Surfaces
  
4. Working
  - "Repeated Weighted Averaging with Thresholding"
  - How does that end up "learning"? Is there an intuitive explanation?


## Neural Nets in Code


## References

1. The Neural Network Zoo - The Asimov Institute. <http://www.asimovinstitute.org/neural-network-zoo/>

1. It’s just a linear model:  neural  networks edition. <https://lucy.shinyapps.io/neural-net-linear/>

1. Neural Network Playground. <https://playground.tensorflow.org/>

1. Rohit Patel (20 Oct 2024). *Understanding LLMs from Scratch Using Middle School Math: A self-contained, full explanation to inner workings of an LLM*. <https://towardsdatascience.com/understanding-llms-from-scratch-using-middle-school-math-e602d27ec876>

1. Machine Learning Tokyo: Interactive Tools for ML/DL, and Math. <https://github.com/Machine-Learning-Tokyo/Interactive_Tool>

1. *Anyone Can Learn AI Using This Blog*. <https://colab.research.google.com/drive/1g5fj7W6QMER4-03jtou7k1t7zMVE9TVt#scrollTo=V8Vq_6Q3zivl>

1. Neural Networks Visual with [vcubingx](https://youtube.com/@vcubingx?feature=shared)

  - Part 1. <https://youtu.be/UOvPeC8WOt8>
  
  - Part 2. <https://www.youtube.com/watch?v=-at7SLoVK_I>
  
1. Practical Deep Learning for Coders: An Online Free Course.<https://course.fast.ai>


#### Text Books
1. Michael Nielsen. *Neural Networks and Deep Learning*, a free online book. <http://neuralnetworksanddeeplearning.com/index.html>

1. Simone Scardapane. (2024) *Alice’s Adventures in a differentiable Wonderland*. https://www.sscardapane.it/alice-book/


#### Using R for DL

1. David Selby (9 January 2018). Tea and Stats Blog. *Building a neural network from scratch in R*. <https://selbydavid.com/2018/01/09/neural-network/>

1. *torch for R: An open source machine learning framework based on PyTorch.* <https://torch.mlverse.org>
1. Akshaj Verma. (2020-07-24). *Building A Neural Net from Scratch Using R - Part 1 and Part 2*.
<https://rviews.rstudio.com/2020/07/20/shallow-neural-net-from-scratch-using-r-part-1/> and 
<https://rviews.rstudio.com/2020/07/24/building-a-neural-net-from-scratch-using-r-part-2/>
 
 
#### Maths

1.  Parr and Howard (2018). *The Matrix Calculus You Need for Deep Learning*.<https://arxiv.org/abs/1802.01528>


