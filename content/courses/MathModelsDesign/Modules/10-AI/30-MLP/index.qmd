---
date: 23/Nov/2024
date-modified: "`r Sys.Date()`"
title: "The Multilayer Perceptron"
order: 30
summary: 
tags:
- Neural Nets
- Hidden Layers
filters:
  - d2
d2:
  layout: elk
  theme: "CoolClassics"
  sketch: true

---


```{r setup, include=FALSE}
library(blogdown)
library(tidyverse)
library(ggformula)
library(mosaicCalc) # Analytic Calculus
library(neuralnet) # Backpropagation training
library(plot3D) # 3D plots for explanation
library(DiagrammeR)
library(DiagrammeRsvg)
##
#pak::pkg_install("elipousson/d2r")
library(d2r)
#remotes::install_github("wjschne/ggdiagram")
library(ggdiagram)

## Markdown boiler plate stuff!!
# ![An Elephant](elephant.png){#fig-elephant}
# This is illustrated well by @fig-elephant.
# 
# ### Figure Panel Divs
#     ::: {#fig-elephants layout-ncol=2}
#     ![Surus](surus.png){#fig-surus}
#     ![Hanno](hanno.png){#fig-hanno}
#      Famous Elephants
#     :::
#     Adding download buttons
#     data that has been read in
#     {{< downloadthis ../../../../materials/Data/housing_train.csv dname="house_prices" label="Download the House Prices Dataset" icon="database-fill-down" type="info" >}}
#    existing file
#    {{< downloadthis Orange/grouped-summaries.ows dname="grouped_summaries" label="Download the Orange Workflow" icon="database-fill-down" type="info" >}} 

```

```{r}
#| label: Extra Pedagogical Packages
#| echo: false
#| message: false

library(checkdown)
library(epoxy)
library(grateful)
library(MKdescr)
library(shinylive) # To create a Shiny app in a Quarto HTML doc
# Will not work if webr is also used in the SAME Quarto doc!
library(sysfonts)
library(gfonts)
library(kableExtra)
# library(conflicted)
# conflicted::conflicts_prefer(dplyr::filter, dplyr::count, dplyr::last, dplyr::glimpse, base::max)
library(downloadthis)
#devtools::install_github("mccarthy-m-g/embedr")
library(embedr) # Embed multimedia in HTML files
```

```{r}
#| label: Plot Sizing and theming
#| echo: false
#| message: false
#| results: hide

# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto

# Chunk options
knitr::opts_chunk$set(
 fig.width = 7,
 fig.asp = 0.618, # Golden Ratio
 #out.width = "80%",
 fig.align = "center"
)
### Ggplot Theme
### https://rpubs.com/mclaire19/ggplot2-custom-themes

theme_custom <- function(){ 
    font <- "Roboto Condensed"   #assign font family up front
    
    theme_classic(base_size = 14) %+replace%    #replace elements we want to change
    
    theme(
      panel.grid.minor = element_blank(),    #strip minor gridlines
      text = element_text(family = font),
      #text elements
      plot.title = element_text(             #title
                   family = font,            #set font family
                   #size = 20,               #set font size
                   face = 'bold',            #bold typeface
                   hjust = 0,                #left align
                   #vjust = 2                #raise slightly
                   margin=margin(0,0,10,0)
),               
      
      plot.subtitle = element_text(          #subtitle
                   family = font,            #font family
                   #size = 14,                #font size
                   hjust = 0,
                   margin=margin(2,0,5,0)
),               
      
      plot.caption = element_text(           #caption
                   family = font,            #font family
                   size = 8,                 #font size
                   hjust = 1),               #right align
      
      axis.title = element_text(             #axis titles
                   family = font,            #font family
                   size = 10                 #font size
),
      
      axis.text = element_text(              #axis text
                   family = font,            #axis family
                   size = 8)               #font size
    )
}

# Set graph theme
theme_set(new = theme_custom())
#
```


```{r,echo = FALSE, eval = FALSE, fig.alt="Petr Slováček on Unsplash", fig.align='center'}
knitr::include_graphics("featured.jpg")

```

## {{< iconify icons8 idea >}} What is a Multilayer Perceptron?

This was our bare bones Perceptron, or neuron as we will refer to it henceforth:

```{d2}
direction: right
grid-gap: 10
title: The Perceptron {
  shape: text
  near: top-center
  style: {
    font-size: 30
    italic: true
  }
}
in1.style.opacity: 0
in2.style.opacity: 0
in3.style.opacity: 0
in4.style.opacity: 0
in5.style.opacity: 0
in6.style.opacity: 0
1.shape: circle
2.shape: circle
3.shape: circle
4.shape: circle
5.shape: circle
6.shape: circle
act.shape: circle
act {
  icon: ./carbon--chart-logistic-regression.svg
}
out.style.opacity: 0
###
in1 -> 1
in2 -> 2
in3 -> 3
in4 -> 4
in5 -> 5
in6 -> 6

1 -> act
2 -> act
3 -> act
4 -> act
5 -> act
6 -> act

act -> out


```

<br>
We add (one or more ) **hidden layers** to setup a *Multilayer Perceptron*:

```{d2}
title: Multilayer Perceptron {
  shape: text
  near: top-center
  style: {
    font-size: 30
    italic: true
  }
}
direction: right
grid-columns: 5
grid-rows: 3
###
in1: "in1" {shape: circle
     style: {
      font-size: 25
      stroke: white
      fill: white
    }}
in2: "in2" {shape: circle
     style: {
      font-size: 25
      stroke: white
      fill: white
    }}
in3: "in3" {shape: circle
     style: {
      font-size: 25
      stroke: white
      fill: white
    }}
i1: {shape: circle
     style: {
      font-size: 25
      fill: white
    }}
i2: {shape: circle
     style: {
      font-size: 25
      fill: white
    }}
i3: {shape: circle
     style: {
      font-size: 25
      fill: white
    }}
h1: {shape: circle
     style: {
      stroke: blue
      font-color: blue
      font-size: 25
      stroke-dash: 2
      stroke-width: 6
      fill: white
    }}
h2: {shape: circle
     style: {
      stroke: blue
      font-color: blue
      font-size: 25
      stroke-dash: 2
      stroke-width: 6
      fill: white
    }}
h3: {shape: circle
     style: {
      stroke: blue
      font-color: blue
      font-size: 25
      stroke-dash: 2
      stroke-width: 6
      fill: white
    }}
o1: {shape: circle
     style: {
      font-size: 25
      fill: white
    }}
o2: {shape: circle
     style: {
      font-size: 25
      fill: white
    }}
o3: {shape: circle
     style: {
      font-size: 25
      fill: white
    }}
out1: "out1" {shape: circle
     style: {
      font-size: 25
      stroke: white
      fill: white
    }}
out2: "out2" {shape: circle
     style: {
      font-size: 25
      stroke: white
      fill: white
    }}
out3: "out3" {shape: circle
     style: {
      font-size: 25
      stroke: white
      fill: white
    }}
###
in1 -> i1
in2 -> i2
in3 -> i3
i1 -> h1
i1 -> h2
i1 -> h3
i2 -> h1
i2 -> h2
i2 -> h3
i3 -> h1
i3 -> h2
i3 -> h3
h1 -> o1
h2 -> o1
h3 -> o1
h1 -> o2
h2 -> o2
h3 -> o2
h1 -> o3
h2 -> o3
h3 -> o3

o1 -> out1
o2 -> out2
o3 -> out3

```

<br>

- Here, `i1`, `i2`, and `i3` are *input neurons*: they are simply inputs and are drawn as circles in the literature. 
- The `h1`, `h2`, `h3` are neuron in the so-called **hidden layer**; *hidden* because they are not inputs! 
- The neurons `o1`, `o2`, and `o3` are **output neurons**.
- The signals/information flows from left to right in the diagram. And we have shown every neuron connected to everyone in the next layer downstream.

How do we mathematically, and concisely, express the operation of the MLP? Let us setup a notation for the MLP weights. 

  - $l$ : layer index; 
  - $j$, $k$ : neuron index in two adjacent layers
  - $W^l_{jk}$ : weight from $k$th neuron / $(l−1)$th layer to $j$th neuron / $l$th layer;
  - $b^l_j$ : *bias* of the $j$th neuron in the $l$th layer.
  - $a^l_j$ : activation of $j$th neuron / $l$th layer. 

```{d2}
title: Data Processing Between Layers {
  shape: text
  near: top-center
  style: {
    font-size: 30
    italic: true
  }
}
direction: right
grid-columns: 3

layer-1: {
  grid-columns: 1
  1.shape: circle
  2.shape: circle
  3.shape: circle
}
layer-2: {
  grid-columns: 1
  h1.shape: circle
  h2.shape: circle
  h3.shape: circle
}
layer-3: {
  grid-columns: 1
  style: {
    opacity: 0
  }
    a21: "a21" {shape: circle
     style: {
      stroke: white
      fill: white
    }}
    a22: "a22" {shape: circle
     style: {
      stroke: white
      fill: white
    }}
    a23: "a23" {shape: circle
     style: {
      stroke: white
      fill: white
     }
     }
}
layer-1.1 -> layer-2.h1: W11 {
  source-arrowhead.label: a11
  style: {
    fill: LightBlue
    stroke: FireBrick
    stroke-width: 4
    animated: true
  }
}
layer-1.1 -> layer-2.h2
layer-1.1 -> layer-2.h3
layer-1.2 -> layer-2.h1: W21 {
  source-arrowhead.label: a12
  style: {
    fill: LightBlue
    stroke: FireBrick
    stroke-width: 4
    animated: true
  }
}
layer-1.2 -> layer-2.h2
layer-1.2 -> layer-2.h3
layer-1.3 -> layer-2.h1: W31 {
  source-arrowhead.label: a13
  style: {
    fill: LightBlue
    stroke: FireBrick
    stroke-width: 4
    animated: true
  }
}
layer-1.3 -> layer-2.h2
layer-1.3 -> layer-2.h3

layer-2.h1 -> layer-3.a21
layer-2.h2 -> layer-3.a22
layer-2.h3 -> layer-3.a23

```



```{text}
#| include: false
title: Input + Hidden Layers {
  shape: text
  near: top-center
  style: {
    font-size: 30
    italic: true
  }
}
direction: right
grid-columns: 4
grid-rows: 3
grid-gap: 200
###
in1: "in1" {shape: circle
     style: {
      font-size: 25
      stroke: white
      fill: white
    }}
in2: "in2" {shape: circle
     style: {
      font-size: 25
      stroke: white
      fill: white
    }}
in3: "in3" {shape: circle
     style: {
      font-size: 25
      stroke: white
      fill: white
    }}
i1: {shape: circle
     style: {
      font-size: 25
      fill: white
    }}
i2: {shape: circle
     style: {
      font-size: 25
      fill: white
    }}
i3: {shape: circle
     style: {
      font-size: 25
      fill: white
    }}
h1: {shape: circle
     style: {
      stroke: blue
      font-color: blue
      font-size: 25
      stroke-dash: 2
      stroke-width: 6
      fill: white
    }}
h2: {shape: circle
     style: {
      stroke: blue
      font-color: blue
      font-size: 25
      stroke-dash: 2
      stroke-width: 6
      fill: white
    }}
h3: {shape: circle
     style: {
      stroke: blue
      font-color: blue
      font-size: 25
      stroke-dash: 2
      stroke-width: 6
      fill: white
    }}
oh1: "oh1" {shape: circle
     style: {
      font-size: 25
      stroke: white
      fill: white
    }}
oh2: "oh2" {shape: circle
     style: {
      font-size: 25
      stroke: white
      fill: white
    }}
oh3: "oh3" {shape: circle
     style: {
      font-size: 25
      stroke: white
      fill: white
    }}
###
in1 -> i1
in2 -> i2
in3 -> i3

i1 -> h1: {
  source-arrowhead.label: a11
  target-arrowhead.label: W11
  style: {
  fill: LightBlue
  font-size: 18
  stroke: FireBrick
  stroke-width: 6
  animated: true
  }
}
i1 -> h2
i1 -> h3
i2 -> h1: {
  source-arrowhead.label: a12
  target-arrowhead.label: W21
  style: {
  fill: LightBlue
  font-size: 18
  stroke: FireBrick
  stroke-width: 6
  animated: true
  }
}
i2 -> h2
i2 -> h3
i3 -> h1: {
  source-arrowhead.label: a13
  target-arrowhead.label: W31
  style: {
  fill: LightBlue
  font-size: 18
  stroke: FireBrick
  stroke-width: 6
  animated: true
  }
}
i3 -> h2
i3 -> h3
h1 -> oh1
h2 -> oh2
h3 -> oh3

```
<br>
We can write the outputs of the `layer-2` as:


$$
\begin{align}
a^2_1 = sigmoid~(~\color{red}{W^2_{11}*a^1_1} + \color{skyblue}{W^2_{21}*a^1_2} + \color{forestgreen}{W^2_{31}*a^1_3}~ + b^2_1)\\
a^2_2 = sigmoid~(~W^2_{12}*a^1_1 + W^2_{22}*a^1_2 + W^2_{32}*a^1_3~ + b^2_2 )\\
a^2_3 = sigmoid~(~W^2_{13}*a^1_1 + W^2_{23}*a^1_2 + W^2_{33}*a^1_3~ + b^2_3)\\
\end{align}
$$
In (dreaded?) matrix notation :

$$
\begin{bmatrix}
a^2_1\\
a^2_2\\
a^2_3\\
\end{bmatrix} = 
sigmoid~\Bigg(
\begin{bmatrix}
\color{red}{W^2_{11}} & \color{skyblue}{W^2_{21}} & \color{forestgreen}{W^2_{31}}\\
W^2_{12} & W^2_{22} & W^2_{32}\\
W^2_{13} & W^2_{23} & W^2_{33}\\
\end{bmatrix} * 
\begin{bmatrix}
\color{red}{a^1_1}\\
\color{skyblue}{a^1_2}\\
\color{forestgreen}{a^1_3}\\
\end{bmatrix} +
\begin{bmatrix}
b^2_1\\
b^2_2\\
b^2_3\\
\end{bmatrix}
\Bigg)
$$
In compact notation we write, in general:

$$
a^l_j=σ(\sum_kW^l_{jk} * a^{l−1}_k+b^l_j)
$${#eq-forward-prop}





### How does and MLP Learn?

See how the conections between neurons are marked by **weights**: these multiply the signal from the previous neuron. The multiplied/weighted products are added up in the neuron, and the sum is given to the activation block therein. 

So learning?

The only controllable variables in a neural network are these weights! So learning involves adapting these wwights so that they can perform a sueful function. 

### What is the Training Process?

The process of adapting the weights of a neural network can be described in the following steps:

- **Training Set**: Training is over several known input-output pairs ("training data")
- **Training Epoch**: For each input, the signals propagate forward until we have an output
- **Error Calculation**: Output is compared with **desired output**, to calculate *error*
- **Backpropagation**: Errors need to be *sent backward from the output* to input, where we unravel the error from layer $l$ to layer $l-1$. (like apportioning blame !!). 
- **Error-to-Cost**: How does error at a given neuron relate to overall Cost?
- **Differentiate**: Evaluate the *effect* of each weight/bias on ~~the (apportioned) error~~ overall Cost. (Slope!!)
- **Gradient Descent**: Adapt the weights/biases with a small step in the **opposite direction** to the slope


### Assumptions in the Training Process

1. **Training Error**: We can calculate overall training Cost as the average Cost taken over all/several input samples.
$$
C = \frac{1}{n}*\sum_{x}C_x
$$
In practice, the input samples are not presented one by one, but in batches, called *minbatches*. This is cheaper by way of computation since we adapt the weights on a per-batch basis, and also allows to perform *averaging* of errors over each batch. However is also seems to complicate the housekeeping necessary to manage batches. ( We will not, in the interest of time, deal with this idea more. It is dealt with in the references below. )

2. **Cost Function**: We will minimize the Cost function which is assumed to be a function of (all) outputs of a NN.

3. **Global Minimum**: Cost function has a global minimum! (Bowl shaped surface which we can descend)


## Here Comes the ~~Rain~~ Maths Again!

1. Rosenblatt-Nielsen's Demon: 
  - messes/perturbs with input to the sigmoid function at a neuron. (Weighted Sum)
  - Error = Slope * perturbation  
  - However, **Error ~= Slope** when we allow that the perturbation is a fixed amplitude.
  - Still a product of slopes  -O
  
$$
{\delta_j}^L = \frac{\delta C}{\delta {a_j}^L} * \sigma ({z_j}^L)
$${#eq-bp1}

:::: {.columns}

::: {.column width="48%"}
{{< video https://youtu.be/Ilg3gGewQ5U?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi >}}
:::

::: {.column width="4%"}
:::

::: {.column width="48%"}
{{< video https://youtu.be/tIeHLnjs5U8?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi >}}
:::
::::



## MLPs in Code

::: {.panel-tabset .nav-pills style="background: whitesmoke "}

### Using p5.js


### Using R
Using `torch`.

:::


## References

1. Tariq Rashid. *Make your own Neural Network*. [PDF Online](https://github.com/harshitkgupta/StudyMaterial/blob/master/Make%20Your%20Own%20Neural%20Network%20(Tariq%20Rashid)%20-%20%7BCHB%20Books%7D.pdf)
1. Mathoverflow. *Intuitive Crutches for Higher Dimensional Thinking*. <https://mathoverflow.net/questions/25983/intuitive-crutches-for-higher-dimensional-thinking>

