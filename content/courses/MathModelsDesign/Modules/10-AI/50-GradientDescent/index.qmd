---
date: 23/Nov/2024
date-modified: "`r Sys.Date()`"
title: "Gradient Descent"
order: 50
summary: 
tags:
- Neural Nets
- Back Propagation
- Gradient
filters:
  - d2
d2:
  layout: elk
  theme: "CoolClassics"
  sketch: true

---


```{r setup, include=FALSE}
library(blogdown)
library(tidyverse)
library(ggformula)
library(mosaicCalc) # Analytic Calculus
library(neuralnet) # Backpropagation training
library(plot3D) # 3D plots for explanation
library(DiagrammeR)
library(DiagrammeRsvg)
##
#pak::pkg_install("elipousson/d2r")
library(d2r)

## Markdown boiler plate stuff!!
# ![An Elephant](elephant.png){#fig-elephant}
# This is illustrated well by @fig-elephant.
# 
# ### Figure Panel Divs
#     ::: {#fig-elephants layout-ncol=2}
#     ![Surus](surus.png){#fig-surus}
#     ![Hanno](hanno.png){#fig-hanno}
#      Famous Elephants
#     :::
#     Adding download buttons
#     data that has been read in
#     {{< downloadthis ../../../../materials/Data/housing_train.csv dname="house_prices" label="Download the House Prices Dataset" icon="database-fill-down" type="info" >}}
#    existing file
#    {{< downloadthis Orange/grouped-summaries.ows dname="grouped_summaries" label="Download the Orange Workflow" icon="database-fill-down" type="info" >}} 

```

```{r}
#| label: Extra Pedagogical Packages
#| echo: false
#| message: false

library(checkdown)
library(epoxy)
library(grateful)
library(MKdescr)
library(shinylive) # To create a Shiny app in a Quarto HTML doc
# Will not work if webr is also used in the SAME Quarto doc!
library(sysfonts)
library(gfonts)
library(kableExtra)
# library(conflicted)
# conflicted::conflicts_prefer(dplyr::filter, dplyr::count, dplyr::last, dplyr::glimpse, base::max)
library(downloadthis)
#devtools::install_github("mccarthy-m-g/embedr")
library(embedr) # Embed multimedia in HTML files
```

```{r}
#| label: Plot Sizing and theming
#| echo: false
#| message: false
#| results: hide

# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto

# Chunk options
knitr::opts_chunk$set(
 fig.width = 7,
 fig.asp = 0.618, # Golden Ratio
 #out.width = "80%",
 fig.align = "center"
)
### Ggplot Theme
### https://rpubs.com/mclaire19/ggplot2-custom-themes

theme_custom <- function(){ 
    font <- "Roboto Condensed"   #assign font family up front
    
    theme_classic(base_size = 14) %+replace%    #replace elements we want to change
    
    theme(
      panel.grid.minor = element_blank(),    #strip minor gridlines
      text = element_text(family = font),
      #text elements
      plot.title = element_text(             #title
                   family = font,            #set font family
                   #size = 20,               #set font size
                   face = 'bold',            #bold typeface
                   hjust = 0,                #left align
                   #vjust = 2                #raise slightly
                   margin=margin(0,0,10,0)
),               
      
      plot.subtitle = element_text(          #subtitle
                   family = font,            #font family
                   #size = 14,                #font size
                   hjust = 0,
                   margin=margin(2,0,5,0)
),               
      
      plot.caption = element_text(           #caption
                   family = font,            #font family
                   size = 8,                 #font size
                   hjust = 1),               #right align
      
      axis.title = element_text(             #axis titles
                   family = font,            #font family
                   size = 10                 #font size
),
      
      axis.text = element_text(              #axis text
                   family = font,            #axis family
                   size = 8)               #font size
    )
}

# Set graph theme
theme_set(new = theme_custom())
#
```


```{r,echo = FALSE, eval = FALSE, fig.alt="Petr Slováček on Unsplash", fig.align='center'}
knitr::include_graphics("featured.jpg")

```


## Learning: Adapting the Weights

We obtained the backpropagated error for each layer:

$$
\begin{bmatrix}
e_{11}\\
e_{21}\\
e_{31}\\
\end{bmatrix} \pmb{\sim}
\begin{bmatrix}
W_{11} & W_{12} & W_{13} \\
W_{21} & W_{22}  & W_{23} \\
W_{31} & W_{32} & W_{33} \\
\end{bmatrix} * 
\begin{bmatrix}
{e_{12}}\\
{e_{22}}\\
{e_{32}}\\
\end{bmatrix}
$$

And the matrix form: 

$$
e^{l-1} ~ \pmb{\sim} ~ {W^l}^{\pmb{\color{red}{T}}}* e^{l}
$${#eq-Back-Prop}


Now what? How do we use all these errors, from the output right up to those backpropagated backwards up to the first ($l=1$) layer? To adapt the weights of the NN using these backpropagated errors, here are the steps:

1. **Per-Weight Cost Gradient**: We are looking for something like $\large{\pmb{\color{red}{\frac{dC}{W_{jk}}}}}$ for all possible combos of $jk$.
1. **Learn**: Adapt the Weights in the opposite direction to its Cost-Gradient. (Why?)

 Are you ready? ;-D Let us do this !


### Cost-Gradient for each Weight

1. The cost function was the squared error averaged over all $n$ neurons:

$$
\begin{align}
C(W, b) &= \frac{1}{2n}\sum^{n ~ neurons}_{i=1}e^2(i)\\
\end{align}
$${#eq-cost-function}



2. ***Serious Magic***: We want to differentiate this sum for **each Weight**. Before we calculate $\frac{dC}{dW^l_{jk}}$, we realize that *any weight* $W^l_{jk}$ connects only as input to **one neuron** $k$, which outputs $a_k$. No other neuron-terms in the above summation depend upon this specific Weight, so the summation becomes *just one term*, pertaining to activation-output, say $a_k$!

$$
\begin{align}
\frac{d~C}{d~\color{orange}{\pmb{W^l_{jk}}}} &= \large\frac{d}{d~\color{orange}{\pmb{W^l_{jk}}}}\Bigg({\frac{1}{2n}\sum^{all~n~neurons}_{i=1}(e_i)^2}~\Bigg)\\
\\
&= \frac{\color{skyblue}{\large{e^l_k}} }{n} ~ * \frac{d}{d~\color{orange}{\pmb{W^l_{jk}}}} ~ \Bigg(\pmb{\color{red}{\Large{{e^{l}_k}}}} ~ \Bigg) ~~only~~k^{th}~neuron~l^{th}~layer\\
\\
&= \frac{\color{skyblue}{\large{e^l_k}} }{n} ~ * ~ {\frac{d}{d~\color{orange}{\pmb{W^l_{jk}}}}\bigg(\Large{\color{red}{a^{l}_k - d^l_k}}}\bigg)
\end{align}
$$

3. Now, the relationship between $a^{l}_k$ and $W^l_{jk}$ involves the sigmoid function. (And $d_k$ is not dependent upon anything!)

$$
\begin{align}
\color{red}{\pmb{a^l_k}} ~ &= \sigma~\bigg(\sum^{neurons~in~l-1}_{j=1} \pmb{\color{orange}{W^l_{jk}}} ~ * ~{a^{l-1}_j + b^l_j}\bigg)\\
&= \color{red}{\sigma(everything)}\\
\end{align}
$$

4. We also know 
$$
\large{\frac{d\sigma(x)}{dx}} = \sigma(x) * \big(1 - \sigma(x)\big)
$$ 

5. **Final Leap**: Using the great [chain rule for differentiation](https://en.wikipedia.org/wiki/Derivative#Rules_for_basic_functions), we obtain:

::: column-screen-inset-right
$$
\begin{align}
\frac{d~C}{d~\color{orange}{\pmb{W^l_{jk}}}} &= \frac{\color{skyblue}{\large{e^l_k}} }{n} ~ * ~ {\frac{d}{d~\color{orange}{\pmb{W^l_{jk}}}}\bigg(\Large{\color{red}{a^{l}_k - d^l_k}}}\bigg)\\
&= \frac{\color{skyblue}{\large{e^l_k}} }{n} ~ * ~\frac{d~\color{red}{\pmb{a^l_k}}}{d~\color{orange}{\pmb{W^l_{jk}}}}\\
&= \frac{\color{skyblue}{\large{e^l_k}} }{n} ~ *\frac{d~ \color{red}{\sigma(everything)}}{d~\color{orange}{\pmb{W^l_{jk}}}}\\
\\
&= \frac{\color{skyblue}{\large{e^l_k}} }{n} ~ * \sigma(everything) * (1 -\sigma(everything)) * \frac{d(everything)}{d~\color{orange}{\pmb{W^l_{jk}}}}~~ \text{Applying Chain Rule!}\\
&= \huge{\frac{\color{skyblue}{\large{e^l_k}} }{n} ~ * \color{red}{~a^{l-1}_k} * ~\\
\large{\sigma~\bigg(\sum^{neurons~in~l-1}_{j=1} \pmb{\color{orange}{W^l_{jk}}} ~ * ~ {a^{l-1}_j + b^l_j}\bigg) * \\
\bigg(1 - \sigma~\bigg(\sum^{neurons~in~l-1}_{j=1} \pmb{\color{orange}{W^l_{jk}}} ~ * ~ {a^{l-1}_j + b^l_j}\bigg)\bigg)}}
\end{align}
$${#eq-gradient}

:::



How to understand this monster equation intuitively? Let us first draw a diagram to visualize the components:

```{d2}
title: Cost Gradient Interpreted {
  shape: text
  near: top-center
  style: {
    font-size: 60
    italic: true
  }
}
direction: right
grid-columns: 3
grid-gap: 400

layer-1: Layer: L-1 Index-j {
  grid-columns: 1
  grid-gap: 100
  j {shape: circle
     style: {
      font-size: 60
      fill: orange
      stroke-width:6
    }}
  2 {shape: circle
     style: {
      font-size: 45
    }}
  3 {shape: circle
     style: {
      font-size: 45
    }}
}
layer-2: Layer: L Index-k {
  grid-columns: 1
  grid-gap: 100
  h1 {shape: circle
     style: {
      font-size: 45
    }}
  k {shape: circle
     style: {
      font-size: 60
      fill: orange
      stroke-width:6
    }}
  h3 {shape: circle
     style: {
      font-size: 45
    }}
}

layer-3: {
  grid-columns: 1
  grid-gap: 100
  style: {
    opacity: 0
  }
    e12: "e12" {shape: circle
     style: {
      font-size: 45
      stroke: white
      fill: white
    }}
    e22: "ekL" {shape: circle
     style: {
      font-size: 80
      stroke: white
      fill: lightblue
    }}
    e32: "e32" {shape: circle
     style: {
      font-size: 45
      stroke: white
      fill: white
     }
     }
}
layer-1.j -> layer-2.k: Wjk {
  source-arrowhead: {
    label: a(L-1)j
    style {
    font-color: red
    }
  }
  style: {
    fill: orange
    font-size: 80
    stroke: FireBrick
    stroke-width: 9
    animated: true
  }
}
#layer-1.j -> layer-2.h1
#layer-1.j -> layer-2.h3
layer-1.2 -> layer-2.k: W2k {
  source-arrowhead.label: a(L-1)2
  style: {
    font-size: 45
    fill: LightBlue
    stroke: FireBrick
    stroke-width: 9
    animated: true
  }
}
#layer-1.2 -> layer-2.h1
#layer-1.2 -> layer-2.h3
layer-1.3 -> layer-2.k: W3k {
  source-arrowhead.label: a(L-1)3
  style: {
    font-size: 45
    fill: LightBlue
    stroke: FireBrick
    stroke-width: 9
    animated: true
  }
}
#layer-1.3 -> layer-2.h1
#layer-1.3 -> layer-2.h3

layer-2.k -> layer-3.e22 { style: {stroke-width: 9
         stroke: FireBrick}}
layer-2.h1 -> layer-3.e12
layer-2.h3 -> layer-3.e32

```

Let us take the *Weight* $Wjk$. It connects neuron $j^{l-1}$ with neuron $k^l$, using the activation $a^{l-1}_j$. The relevant output *error* ( that contributes to the Cost function) is $e^l_{k}$.

- The product $\large{\color{red}{a^{l-1}_j} ~ * ~ \color{lightblue}{e^l_k}}$ is **like a correlation product** of the two quanties at the input and output of the neuron $k$. This product contributes to a sense of *slope*: the larger either of these, larger is the Cost-slope going from neuron $j$ to $k$.
- How do we account for the magnitude of the *Weight* $Wjk$ itself? Surely that matters! Yes, but note that $Wjk$ is entwined with the remaining inputs and weights via the $\sigma$ function term!  We must differentiate that and put that differential into the product! That gives is the two other product terms in the formula above which involve the sigmoid function.

So, monster as it is, the formula is quite intuitive and even beautiful!



### What does this Gradient Look Like?


```{r}
#| label: quad-surface
#| eval: false
#| echo: false
#| message: false
#| warn: false


library(plotly)

df <- tibble(x = seq(1,100,1), 
             y = seq(1,100,1), 
             z = (x^2 + y^2)/1000 + 0.5)

df %>% plot_ly(x = ~x, y = ~y, z = ~z, color = ~z, 
               type = "surface") %>% 
  add_markers()

```

This gradient is calculated (in vector fashion) for **all** weights. 

### How Does the NN Use this Gradient?

So now that we have the gradient of *Cost vs $W^l_{jk}$*, we can adapt $W^l_{jk}$ by moving a small tuning step in the **opposite direction**:

$$
W^l_{jk}~|~new = W^l_{jk}~|~old - \alpha * gradient
$${#eq-Wljk-adaptation}

and we adapt all weights in opposition to their individual cost gradient. The parameter $\alpha$ is called the **learning rate**.

Yes, but not all neurons have a *desired output*; so what do we use for error?? Only the output neurons have a desired output!!

The **backpropagated error**, peasants! Each neuron has already "received" its share of error, which is converted to Cost, whose gradient wrt all input weights of the specific neuron is calculated using @eq-gradient, and each weight thusly adapted using @eq-Wljk-adaptation.


## Here Comes the ~~Rain~~ Maths Again!

Now, we are ready (maybe?) to watch these two very beautifully made videos on Backpropagation. One is of course from Dan Shiffman, and the other from Grant Sanderson a.k.a. 3Blue1Brown.

:::: {.columns}

::: {.column width="48%"}
{{< video https://youtu.be/QJoa0JYaX1I?list=PLRqwX-V7Uu6Y7MdSCaIfsxc561QI0U0Tb >}}
:::

::: {.column width="4%"}
:::

::: {.column width="48%"}
{{< video https://youtu.be/tIeHLnjs5U8?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi >}}
:::
::::

## Gradient Descent in Code

::: {.panel-tabset .nav-pills style="background: whitesmoke "}

### Using p5.js


### Using R
Using `torch`.

:::


## References

1. Tariq Rashid. *Make your own Neural Network*. [PDF Online](https://github.com/harshitkgupta/StudyMaterial/blob/master/Make%20Your%20Own%20Neural%20Network%20(Tariq%20Rashid)%20-%20%7BCHB%20Books%7D.pdf)
1. Mathoverflow. *Intuitive Crutches for Higher Dimensional Thinking*. <https://mathoverflow.net/questions/25983/intuitive-crutches-for-higher-dimensional-thinking>
1. Interactive Backpropagation Explainer <https://xnought.github.io/backprop-explainer/>

