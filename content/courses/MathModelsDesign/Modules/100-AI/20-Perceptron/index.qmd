---
title: "The Perceptron"
date-modified: "`r Sys.Date()`"
date: 20/Nov/2024
order: 20
summary: 
tags:
- Neural Nets
- Perceptron
- Weighted Average
---

```{r setup, include=FALSE}
library(blogdown)
library(tidyverse)
library(ggformula)
library(mosaicCalc) # Analytic Calculus
library(ambient) # Noise generation in R
library(plot3D) # 3D plots for explanation
library(caracas)
library(downloadthis)
library(knitr)
library(kableExtra)
library(neuralnet) 
## Markdown boiler plate stuff!!
# ![An Elephant](elephant.png){#fig-elephant}
# This is illustrated well by @fig-elephant.
# 
# ### Figure Panel Divs
#     ::: {#fig-elephants layout-ncol=2}
#     ![Surus](surus.png){#fig-surus}
#     ![Hanno](hanno.png){#fig-hanno}
#      Famous Elephants
#     :::
#     Adding download buttons
#     data that has been read in
#     {{< downloadthis ../../../../materials/Data/housing_train.csv dname="house_prices" label="Download the House Prices Dataset" icon="database-fill-down" type="info" >}}
#    existing file
#    {{< downloadthis Orange/grouped-summaries.ows dname="grouped_summaries" label="Download the Orange Workflow" icon="database-fill-down" type="info" >}} 

```

```{r}
#| label: Extra Pedagogical Packages
#| echo: false
#| message: false

library(checkdown)
library(epoxy)
library(grateful)
library(MKdescr)
library(shinylive) # To create a Shiny app in a Quarto HTML doc
# Will not work if webr is also used in the SAME Quarto doc!
library(sysfonts)
library(gfonts)
library(kableExtra)
# library(conflicted)
# conflicted::conflicts_prefer(dplyr::filter, dplyr::count, dplyr::last, dplyr::glimpse, base::max)
library(downloadthis)
#devtools::install_github("mccarthy-m-g/embedr")
library(embedr) # Embed multimedia in HTML files
```

```{r}
#| label: Plot Sizing and theming
#| echo: false
#| message: false
#| results: hide

# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto

# Chunk options
knitr::opts_chunk$set(
 fig.width = 7,
 fig.asp = 0.618, # Golden Ratio
 #out.width = "80%",
 fig.align = "center"
)
### Ggplot Theme
### https://rpubs.com/mclaire19/ggplot2-custom-themes

theme_custom <- function(){ 
    font <- "Roboto Condensed"   #assign font family up front
    
    theme_classic(base_size = 14) %+replace%    #replace elements we want to change
    
    theme(
      panel.grid.minor = element_blank(),    #strip minor gridlines
      text = element_text(family = font),
      #text elements
      plot.title = element_text(             #title
                   family = font,            #set font family
                   #size = 20,               #set font size
                   face = 'bold',            #bold typeface
                   hjust = 0,                #left align
                   #vjust = 2                #raise slightly
                   margin=margin(0,0,10,0)
),               
      
      plot.subtitle = element_text(          #subtitle
                   family = font,            #font family
                   #size = 14,                #font size
                   hjust = 0,
                   margin=margin(2,0,5,0)
),               
      
      plot.caption = element_text(           #caption
                   family = font,            #font family
                   size = 8,                 #font size
                   hjust = 1),               #right align
      
      axis.title = element_text(             #axis titles
                   family = font,            #font family
                   size = 10                 #font size
),
      
      axis.text = element_text(              #axis text
                   family = font,            #axis family
                   size = 8)               #font size
    )
}

# Set graph theme
theme_set(new = theme_custom())
#
```

```{r,echo = FALSE, eval = FALSE, fig.alt="Petr Slováček on Unsplash", fig.align='center'}
knitr::include_graphics("featured.jpg")

```

## {{< iconify icons8 idea >}} Inspiration

## What is a Perceptron?

The [perceptron](../../../../../materials/pdfs/Rosenblatt1958.pdf) was
invented by Frank Rosenblatt is considered one of the foundational
pieces of neural network structures. The output is viewed as a
*decision* from the neuron and is usually propagated as an input to
other neurons inside the neural network.

![Perceptron](../../../../../materials/images/png/Neurons.png){height="360"}


#### Math Intuition

-   We can imagine this as a set of inputs that *averaged in weighted
    fashion*.

$$
y_k = sign~(~\sum_{k=1}^n W_k*x_k + b~)
$${#eq-perceptron}

-   Since the inputs are added with linear weighting, this effectively
    acts like a **linear transformation** of the input data.
    -   A linear equation of this sort is the [general equation of an
        n-dimensional plane](https://www.geeksforgeeks.org/equation-of-plane/).
    -   If we imagine the input as representing the n-coordinates in a
        plane, then the multiplications scale/stretch/compress the
        plane, like a rubber sheet. (But do not **fold it**.)
    -   If there were only 2 inputs, we could mentally picture this with a handkerchief.
-   More metaphorically, it seems like the neuron is consulting each of
    the inputs, asking for their opinion, and then making a decision by
    attaching *different amounts of significance* to each opinion.
-   The Structure should remind you of [Linear
    Regression](../../../../Analytics/Modelling/Modules/LinReg/index.qmd)
    !

So how does it work? Consider the interactive diagram below:

<iframe height=480 width=680
src="https://editor.p5js.org/arvindv/full/qx7rMp3RI">
</iframe>

- The coordinate axes are as shown [X]{.red}, [Y]{.green}, and [Z]{.blue} 

- The [grey]{.black .bg-gray} and [yellow]{.black .bg-yellow} points are the data we wish to classify into two categories, unsurprisingly "yellow" and grey".
- The [Weight vector]{.black .bg-red} line is a vector of all the weights in the Perceptron. 

- Now, as per the [point-normal form of an n-dimensional plane](https://www.geeksforgeeks.org/equation-of-plane/), the multiplication of the input data with the weight vector is like taking a ***vector dot product*** ( aka inner product) ! And: every point on the plane has a dot product of ZERO. See the [purple vector]{.bg-purple} which is **normal** to the Weight vector: its dot product with the Weight vector is zero. 
- Data points that are off this "normal plane" in either direction (above and below) will have dot-products which will be positive or negative depending upon the direction!
- Hence we can use the dot-product POLARITY to decide if a point is above or below the plane defined by the Weight vector. Which is what is done in the threshold-based activation!
- The bias $b$ defines the POSITION of the plane; and the Weights define the direction. Together, they classify the points based on the @eq-perceptron. 
- Try to move the slider to get an intuition of how the plane moves with the bias.  Clearly, the bias is very influential in deciding the POLARITY of the dot-products. When it aligns with the purple vector ($dot product = 0$), it works best. 


#### Why "Linear"?

Why are (almost) all operations linear operations in a NN?

-   We said that the weighted sums *are* a linear operation, but why is
    this so?
-   We wish to be able to set-up analytic functions for performance of
    the NN, and **be able to differentiate them** to be able to optimize
    them.
-   Non-linear blocks, such as *threshold blocks/signum-function based
    slicers* are not differentiable and we are unable to set up such
    analysis.
-   Note the title of [this
    reference](https://www.sscardapane.it/alice-book/).

#### Why is there a Bias input?

-   We want the weighted sum of the inputs to *mean something*
    significant, before we accept it.
-   The bias is *subtracted* from the weighted sum of inputs, and the
    bias input could also (notionally) have a weight.
-   The *bias* is like a threshold which the weighted sum has to exceed;
    if it does, the neuron is said to **fire**.


So with all that vocabulary, we *might* want to watch this longish video
by the great Dan Shiffman:

{{< video https://youtu.be/ntKn5TPHHAk?list=PLRqwX-V7Uu6Y7MdSCaIfsxc561QI0U0Tb >}}

## Perceptrons in Code

::: {.panel-tabset .nav-pills style="background: whitesmoke;"}
### {{< iconify la r-project >}} R

Let us try a simple single layer NN in R. We will use the R package
`neuralnet`.

```{r}
#| label: nn-with-r
#| code-fold: true
#| message: false
#| warning: false

# Load the package 
#library(neuralnet) 

# Use iris
# Create Training and Testing Datasets
df_train <- iris %>% slice_sample(n = 100)
df_test <- iris %>% anti_join(df_train)
head(iris)
# Create a simle Neural Net
nn = neuralnet(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, 
               data = df_train, 
               hidden = 0, 
               #act.fct = "logistic", # Sigmoid
               linear.output = TRUE) # TRUE to ignore activation function

# str(nn)

# Plot 
plot(nn) 

# Predictions
# Predict <- compute(nn, df_test) 
# Predict
# cat("Predicted values:\n") 
# print(Predict$net.result) 
# 
# probability <- Predict$net.result 
# pred <- ifelse(probability > 0.5, 1, 0)
# cat("Result in binary values:\n") 
# pred %>% as_tibble()

```

![](nn.png)

### {{< iconify skill-icons p5js>}} p5.js

To Be Written Up.
:::

## References

1.  The Neural Network Zoo - The Asimov Institute.
    <http://www.asimovinstitute.org/neural-network-zoo/>
2.  It’s just a linear model: neural networks edition.
    <https://lucy.shinyapps.io/neural-net-linear/>
3.  Neural Network Playground. <https://playground.tensorflow.org/>
4.  Rohit Patel (20 Oct 2024). *Understanding LLMs from Scratch Using
    Middle School Math: A self-contained, full explanation to inner
    workings of an LLM*.
    <https://towardsdatascience.com/understanding-llms-from-scratch-using-middle-school-math-e602d27ec876>
5.  Machine Learning Tokyo: Interactive Tools for ML/DL, and Math.
    <https://github.com/Machine-Learning-Tokyo/Interactive_Tool>
6.  *Anyone Can Learn AI Using This Blog*.
    <https://colab.research.google.com/drive/1g5fj7W6QMER4-03jtou7k1t7zMVE9TVt#scrollTo=V8Vq_6Q3zivl>
7.  Neural Networks Visual with
    [vcubingx](https://youtube.com/@vcubingx?feature=shared)
    -   Part 1. <https://youtu.be/UOvPeC8WOt8>
    -   Part 2. <https://www.youtube.com/watch?v=-at7SLoVK_I>
8.  Practical Deep Learning for Coders: An Online Free
    Course.<https://course.fast.ai>

#### Text Books

1.  Michael Nielsen. *Neural Networks and Deep Learning*, a free online
    book. <http://neuralnetworksanddeeplearning.com/index.html>
2.  Simone Scardapane. (2024) *Alice’s Adventures in a differentiable
    Wonderland*.<https://www.sscardapane.it/alice-book/>

#### Using R for DL

1.  David Selby (9 January 2018). Tea and Stats Blog. *Building a neural
    network from scratch in R*.
    <https://selbydavid.com/2018/01/09/neural-network/>
2.  *torch for R: An open source machine learning framework based on
    PyTorch.* <https://torch.mlverse.org>
3.  Akshaj Verma. (2020-07-24). *Building A Neural Net from Scratch
    Using R - Part 1 and Part 2*.
    <https://rviews.rstudio.com/2020/07/20/shallow-neural-net-from-scratch-using-r-part-1/>
    and
    <https://rviews.rstudio.com/2020/07/24/building-a-neural-net-from-scratch-using-r-part-2/>

#### Maths

1.  Parr and Howard (2018). *The Matrix Calculus You Need for Deep
    Learning*.<https://arxiv.org/abs/1802.01528>

::: {#refs style="font-size: 60%;"}
###### {{< iconify lucide package-check >}} R Package Citations

```{r}
#| echo: false
#scan_packages()
grateful::cite_packages(
  output = "table",
  out.dir = ".",
  out.format = "html",
  pkgs = c("neuralnet") ) %>%
  knitr::kable(format = "simple")

```
:::
