---
date: 23/Nov/2024
date-modified: "`r Sys.Date()`"
title: "The Multilayer Perceptron"
order: 30
summary: 
tags:
- Neural Nets
- Hidden Layers
filters:
  - d2
d2:
  layout: elk
  theme: "CoolClassics"
  sketch: true
---

```{r setup, include=FALSE}
library(blogdown)
library(tidyverse)
library(ggformula)
library(mosaicCalc) # Analytic Calculus
library(neuralnet) # Backpropagation training
library(plot3D) # 3D plots for explanation
library(DiagrammeR)
library(DiagrammeRsvg)
##
#pak::pkg_install("elipousson/d2r")
library(d2r)
#remotes::install_github("wjschne/ggdiagram")
library(ggdiagram)

## Markdown boiler plate stuff!!
# ![An Elephant](elephant.png){#fig-elephant}
# This is illustrated well by @fig-elephant.
# 
# ### Figure Panel Divs
#     ::: {#fig-elephants layout-ncol=2}
#     ![Surus](surus.png){#fig-surus}
#     ![Hanno](hanno.png){#fig-hanno}
#      Famous Elephants
#     :::
#     Adding download buttons
#     data that has been read in
#     {{< downloadthis ../../../../materials/Data/housing_train.csv dname="house_prices" label="Download the House Prices Dataset" icon="database-fill-down" type="info" >}}
#    existing file
#    {{< downloadthis Orange/grouped-summaries.ows dname="grouped_summaries" label="Download the Orange Workflow" icon="database-fill-down" type="info" >}} 

```

```{r}
#| label: Extra Pedagogical Packages
#| echo: false
#| message: false

library(checkdown)
library(epoxy)
library(grateful)
library(MKdescr)
library(shinylive) # To create a Shiny app in a Quarto HTML doc
# Will not work if webr is also used in the SAME Quarto doc!
library(sysfonts)
library(gfonts)
library(kableExtra)
# library(conflicted)
# conflicted::conflicts_prefer(dplyr::filter, dplyr::count, dplyr::last, dplyr::glimpse, base::max)
library(downloadthis)
#devtools::install_github("mccarthy-m-g/embedr")
library(embedr) # Embed multimedia in HTML files
```

```{r}
#| label: Plot Sizing and theming
#| echo: false
#| message: false
#| results: hide

# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto

# Chunk options
knitr::opts_chunk$set(
 fig.width = 7,
 fig.asp = 0.618, # Golden Ratio
 #out.width = "80%",
 fig.align = "center"
)
### Ggplot Theme
### https://rpubs.com/mclaire19/ggplot2-custom-themes

theme_custom <- function(){ 
    font <- "Roboto Condensed"   #assign font family up front
    
    theme_classic(base_size = 14) %+replace%    #replace elements we want to change
    
    theme(
      panel.grid.minor = element_blank(),    #strip minor gridlines
      text = element_text(family = font),
      #text elements
      plot.title = element_text(             #title
                   family = font,            #set font family
                   #size = 20,               #set font size
                   face = 'bold',            #bold typeface
                   hjust = 0,                #left align
                   #vjust = 2                #raise slightly
                   margin=margin(0,0,10,0)
),               
      
      plot.subtitle = element_text(          #subtitle
                   family = font,            #font family
                   #size = 14,                #font size
                   hjust = 0,
                   margin=margin(2,0,5,0)
),               
      
      plot.caption = element_text(           #caption
                   family = font,            #font family
                   size = 8,                 #font size
                   hjust = 1),               #right align
      
      axis.title = element_text(             #axis titles
                   family = font,            #font family
                   size = 10                 #font size
),
      
      axis.text = element_text(              #axis text
                   family = font,            #axis family
                   size = 8)               #font size
    )
}

# Set graph theme
theme_set(new = theme_custom())
#
```

```{r,echo = FALSE, eval = FALSE, fig.alt="Petr Slováček on Unsplash", fig.align='center'}
knitr::include_graphics("featured.jpg")

```

## {{< iconify icons8 idea >}} What is a Multilayer Perceptron?

This was our bare bones Perceptron, or neuron as we will refer to it
henceforth:

$$
y_k = sign~(~\sum_{k=1}^n W_k*x_k + b~)
$$

```{d2}
direction: right
grid-gap: 10
title: The Perceptron {
  shape: text
  near: top-center
  style: {
    font-size: 30
    italic: true
  }
}
in1.style.opacity: 0
in2.style.opacity: 0
in3.style.opacity: 0
in4.style.opacity: 0
in5.style.opacity: 0
in6.style.opacity: 0
1.shape: circle
2.shape: circle
3.shape: circle
4.shape: circle
5.shape: circle
6.shape: circle
act.shape: circle
act {
  icon: ./carbon--chart-logistic-regression.svg
}
out.style.opacity: 0
###
in1 -> 1
in2 -> 2
in3 -> 3
in4 -> 4
in5 -> 5
in6 -> 6

1 -> act
2 -> act
3 -> act
4 -> act
5 -> act
6 -> act

act -> out


```


For the multi-layer perceptron, two changes were made:

- Changing the hard-threshold activation into a more soft sigmoid activation

- addition of (one or more ) **hidden layers**.

Let us discuss these changes in detail. 

#### What is the Activation Block?

-   We said earlier that the weighting and adding is a linear operation.
-   While this is great, simple linear translations of data are not
    capable of generating what we might call learning or generalization
    ability.
-   The outout of the perceptron is a "learning decision" that is made by deciding if the combined output is greater or smaller than a ***threshold***. 
-   We need to have some non-linear block to allow the data to create
    nonlinear transformations of the data space, such as *curving it, or
    folding it, or creating bumps, depressions, twists*, and so on.

![Activation](../../../../../materials/images/png/Activation_Functions.png){height="360"}

-   This nonlinear function needs to be chosen with care so that it is
    both differentiable and keeps the math analysis tractable. (More
    later)
-   Such a nonlinear mathematical function is implemented in the
    **Activation Block**.
-   See this example: red and blue areas, which we wish to **separate
    and classify these** with our DLNN, are not separable unless we fold
    and curve our 2D data space.
-   The separation is achieved using a linear operation, i.e. a LINE!!

![From [Colah
Blog](https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/img/spiral.1-2.2-2-2-2-2-2.gif),
used sadly without
permission](https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/img/spiral.1-2.2-2-2-2-2-2.gif){#fig-manifolds-and-neural-networks}

-   For instance in @fig-manifolds-and-neural-networks, no amount of
    stretching or compressing of the surface can separate the two sets (
    blue and red ) using a line or plane, **unless** the surface can be
    warped into **another dimension** by folding.
    

#### What is the Sigmoid Function?

The hard-threshold used in the [Perceptron](../20-Perceptron/index.qmd) allowed us to make certain decisions based on **linear combinations** of the input data. But what is the dataset possesses *classes* that are not separable in a linear way? What if different categories of points are intertwined with a curved boundary between classes?

We need to have some non-linear block to allow the data to create nonlinear transformations of the data space, such as *curving it, or folding it, or creating bumps, depressions, twists*, and so on. 

![Activation](../../../../../materials/images/png/Activation_Functions.png){height="360"}

-   This nonlinear function needs to be chosen with care so that it is
    both differentiable and keeps the math analysis tractable. (More
    later)
-   Such a nonlinear mathematical function is implemented in the
    **Activation Block**.
-   See this example: red and blue areas, which we wish to **separate
    and classify these** with our DLNN, are not separable unless we fold
    and curve our 2D data space.
-   The separation is achieved using a linear operation, i.e. a LINE!!

![From [Colah
Blog](https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/img/spiral.1-2.2-2-2-2-2-2.gif),
used sadly without
permission](https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/img/spiral.1-2.2-2-2-2-2-2.gif){#fig-manifolds-and-neural-networks}

-   For instance in @fig-manifolds-and-neural-networks, no amount of
    stretching or compressing of the surface can separate the two sets (
    blue and red ) using a line or plane, **unless** the surface can be
    warped into **another dimension** by folding.

So how do we implement this nonlinear Activation Block?

-   One of the popular functions used in the Activation Block is a
    function based on the exponential function $e^x$.
-   Why? Because this function retains is identity when differentiated!
    This is a very convenient property!

![Sigmoid
Activation](../../../../../materials/images/png/Sigmoid.png){height="360"}

::: callout-note
#### Remembering Logistic Regression

Recall your study of [Logistic
Regression](../../../../Analytics/Modelling/Modules/LogReg/index.qmd).
There, the Sigmoid function was used to model the odds of the
(Qualitative) target variable against the (Quantitative) predictor.
:::

::: callout-note
#### But Why Sigmoid?

Because [the Sigmoid function is
differentiable](https://math.stackexchange.com/questions/78575/derivative-of-sigmoid-function-sigma-x-frac11e-x).
And linear in the mid ranges. Oh, and remember the [Chain
Rule](https://en.wikipedia.org/wiki/Derivative#Rules_for_basic_functions)?

$$
\begin{align}
\frac{df(x)}{dx}
&= \frac{d}{dx} * \frac{1}{1 + e^{-x}} \\\
&= -(1 + e^{-x})^{-2} * \frac{d}{dx}(1 + e^{-x})~~\text{(Using Chain Rule)}\\
&= -(1 + e^{-x})^{-2} * (-e^{-x})\\
&=  \frac{e^{-x}}{(1 + e^{-x})^{2}}\\
&= \frac{(1 + e^{-x}) -1}{(1 + e^{-x})^{2}}\\
&= \frac{1}{1 + e^{-x}} * \Bigg({\frac{1 + e^{-x}}{1 + e^{-x}}} - \frac{1}{1 + e^{-x}}\Bigg)\\\
&\text{ and therefore}\\\
\Large{\frac{df(x)}{dx}} &= \Large{f(x) * (1 - f(x))}\\
\end{align}
$$
:::


#### What are Hidden Layers?

The MLP adds several layers of perceptrons, in **layers**, as shown below:

```{d2}
title: Multilayer Perceptron {
  shape: text
  near: top-center
  style: {
    font-size: 30
    italic: true
  }
}
direction: right
grid-columns: 5
grid-rows: 3
###
in1: "in1" {shape: circle
     style: {
      font-size: 25
      stroke: white
      fill: white
    }}
in2: "in2" {shape: circle
     style: {
      font-size: 25
      stroke: white
      fill: white
    }}
in3: "in3" {shape: circle
     style: {
      font-size: 25
      stroke: white
      fill: white
    }}
i1: {shape: circle
     style: {
      font-size: 25
      fill: white
    }}
i2: {shape: circle
     style: {
      font-size: 25
      fill: white
    }}
i3: {shape: circle
     style: {
      font-size: 25
      fill: white
    }}
h1: {shape: circle
     style: {
      stroke: blue
      font-color: blue
      font-size: 25
      stroke-dash: 2
      stroke-width: 6
      fill: white
    }}
h2: {shape: circle
     style: {
      stroke: blue
      font-color: blue
      font-size: 25
      stroke-dash: 2
      stroke-width: 6
      fill: white
    }}
h3: {shape: circle
     style: {
      stroke: blue
      font-color: blue
      font-size: 25
      stroke-dash: 2
      stroke-width: 6
      fill: white
    }}
o1: {shape: circle
     style: {
      font-size: 25
      fill: white
    }}
o2: {shape: circle
     style: {
      font-size: 25
      fill: white
    }}
o3: {shape: circle
     style: {
      font-size: 25
      fill: white
    }}
out1: "out1" {shape: circle
     style: {
      font-size: 25
      stroke: white
      fill: white
    }}
out2: "out2" {shape: circle
     style: {
      font-size: 25
      stroke: white
      fill: white
    }}
out3: "out3" {shape: circle
     style: {
      font-size: 25
      stroke: white
      fill: white
    }}
###
in1 -> i1
in2 -> i2
in3 -> i3
i1 -> h1
i1 -> h2
i1 -> h3
i2 -> h1
i2 -> h2
i2 -> h3
i3 -> h1
i3 -> h2
i3 -> h3
h1 -> o1
h2 -> o1
h3 -> o1
h1 -> o2
h2 -> o2
h3 -> o2
h1 -> o3
h2 -> o3
h3 -> o3

o1 -> out1
o2 -> out2
o3 -> out3

```

<br>

-   Here, `i1`, `i2`, and `i3` are *input neurons*: they are simply
    inputs and are drawn as circles in the literature.
-   The `h1`, `h2`, `h3` are neuron in the so-called **hidden layer**;
    *hidden* because they are not inputs!
-   The neurons `o1`, `o2`, and `o3` are **output neurons**.
-   The signals/information flows from left to right in the diagram. And
    we have shown every neuron connected to everyone in the next layer
    downstream.

How do we mathematically, and concisely, express the operation of the
MLP? Let us setup a notation for the MLP weights.

-   $l$ : layer index;
-   $j$, $k$ : neuron index in two adjacent layers
-   $W^l_{jk}$ (i.e. $W^{layer}_{{source}~{destn}}$) : weight from $j$th neuron / $(l−1)$th layer to $k$th
    neuron / $l$th layer;
-   $b^l_k$ : *bias* of the $k$th neuron in the $l$th layer.
-   $a^l_k$ : activation (output) of $k$th neuron / $l$th layer.


```{d2}
title: Data Processing between Layers {
  shape: text
  near: top-center
  style: {
    font-size: 60
    italic: true
  }
}

direction: right
grid-columns: 3
grid-gap: 400

layer-1: "Layer-1 : Index(j)" {
  grid-columns: 1
  grid-gap: 100
  style: {
      font-size: 30
    }
  1 {shape: circle
     style: {
      font-size: 45
    }}
  2 {shape: circle
     style: {
      font-size: 45
    }}
  3 {shape: circle
     style: {
      font-size: 45
    }}
}
layer-2: "Layer-2 : Index(k)" {
  grid-columns: 1
  grid-gap: 100
  style: {
      font-size: 30
    }
  h1 {shape: circle
     style: {
      font-size: 45
    }}
  h2 {shape: circle
     style: {
      font-size: 45
    }}
  h3 {shape: circle
     style: {
      font-size: 45
    }}
}

layer-3: {
  grid-columns: 1
  grid-gap: 100
  style: {
    opacity: 0
  }
    a12: "a12" {shape: circle
     style: {
      font-size: 45
      stroke: white
      fill: white
    }}
    a22: "a22" {shape: circle
     style: {
      font-size: 45
      stroke: white
      fill: white
    }}
    a32: "a32" {shape: circle
     style: {
      font-size: 45
      stroke: white
      fill: white
     }
     }
}
layer-1.1 -> layer-2.h1: W11 {
  source-arrowhead.label: a11
  style: {
    font-size: 45
    fill: LightBlue
    stroke: FireBrick
    stroke-width: 9
    animated: true
  }
}
layer-1.1 -> layer-2.h2 {style:{stroke: grey}}
layer-1.1 -> layer-2.h3 {style:{stroke: grey}}
layer-1.2 -> layer-2.h1: W21 {
  source-arrowhead.label: a21
  style: {
    font-size: 45
    fill: LightBlue
    stroke: FireBrick
    stroke-width: 9
    animated: true
  }
}
layer-1.2 -> layer-2.h2 {style:{stroke: grey}}
layer-1.2 -> layer-2.h3 {style:{stroke: grey}}
layer-1.3 -> layer-2.h1: W31 {
  source-arrowhead.label: a31
  style: {
    font-size: 45
    fill: LightBlue
    stroke: FireBrick
    stroke-width: 9
    animated: true
  }
}
layer-1.3 -> layer-2.h2 {style:{stroke: grey}}
layer-1.3 -> layer-2.h3 {style:{stroke: grey}}

layer-2.h1 -> layer-3.a12 { style: {stroke-width: 9
         stroke: FireBrick}}
layer-2.h2 -> layer-3.a22 {style:{stroke: grey}}
layer-2.h3 -> layer-3.a32 {style:{stroke: grey}}

```


<br> We can write the outputs of the `layer-2` as:

$$
\begin{align}
(k = 1): ~ a_{12} = sigmoid~(~\color{red}{W^2_{11}*a_{11}} + \color{skyblue}{W^2_{21}*a_{21}} + \color{forestgreen}{W^2_{31}*a_{31}} ~ + b_{12})\\
(k = 2): ~ a_{22} = sigmoid~(~W^2_{12}*a_{11} + W^2_{22}*a_{21} + W^2_{32}*a_{31}~ + b_{22} )\\
(k = 3): ~ a_{32} = sigmoid~(~W^2_{13}*a_{11} + W^2_{23}*a_{21} + W^2_{33}*a_{31}~ + b_{32})\\
\end{align}
$$ 

In (dreaded?) matrix notation :

$$
\begin{bmatrix}
a_{12}\\
a_{22}\\
a_{32}\\
\end{bmatrix} = 
sigmoid~\Bigg(
\begin{bmatrix}
\color{red}{W^2_{11}} & \color{skyblue}{W^2_{21}} & \color{forestgreen}{W^2_{31}}\\
W^2_{12} & W^2_{22} & W^2_{32}\\
W^2_{13} & W^2_{23} & W^2_{33}\\
\end{bmatrix} * 
\begin{bmatrix}
\color{red}{a_{11}}\\
\color{skyblue}{a_{21}}\\
\color{forestgreen}{a_{31}}\\
\end{bmatrix} +
\begin{bmatrix}
b_{12}\\
b_{22}\\
b_{32}\\
\end{bmatrix}
\Bigg)
$$ 


In compact notation we write, in general:

$$
A^l = \sigma\Bigg(W^lA^{l-1} + B^l\Bigg)
$$



$$
a^l_j=σ(\sum_kW^l_{jk} * a^{l−1}_k+b^l_j)
$$ {#eq-forward-prop}


## {{< iconify mingcute thought-line >}} Wait, But Why?

- The "vanilla" perceptron was big advance in AI and learning. However, it was realized that this can only make classification decisions with data that are [linearly separable](https://www.wikiwand.com/en/articles/Linear_separability).
- Including a differentiable non-linearity in the activation block allows us to deform the coordinate space in which the data points are mapped.
- This deformation may permit unique views of the data wherein the categories of data are separable by an n-dimensional plane. 
- This idea is also used in a machine learning algorithm called [Support Vector Machines.](../../../../ML4Artists/Modules/35-SVM/index.qmd)


## MLPs in Code

::: {.panel-tabset .nav-pills style="background: whitesmoke "}
### Using p5.js

### Using R

Using `torch`.
:::

## References

1.  Tariq Rashid. *Make your own Neural Network*. [PDF
    Online](https://github.com/harshitkgupta/StudyMaterial/blob/master/Make%20Your%20Own%20Neural%20Network%20(Tariq%20Rashid)%20-%20%7BCHB%20Books%7D.pdf)
1.  Mathoverflow. *Intuitive Crutches for Higher Dimensional Thinking*.
    <https://mathoverflow.net/questions/25983/intuitive-crutches-for-higher-dimensional-thinking>
1. 3D MatMul Visualizer<https://bhosmer.github.io/mm/ref.html>
